{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "      <th>msd_track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.08</td>\n",
       "      <td>6.579</td>\n",
       "      <td>4.307</td>\n",
       "      <td>3.421</td>\n",
       "      <td>3.192</td>\n",
       "      <td>2.076</td>\n",
       "      <td>2.179</td>\n",
       "      <td>2.052</td>\n",
       "      <td>1.794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.3470</td>\n",
       "      <td>-0.2463</td>\n",
       "      <td>-1.5470</td>\n",
       "      <td>0.17920</td>\n",
       "      <td>-1.1530</td>\n",
       "      <td>-0.7370</td>\n",
       "      <td>0.40750</td>\n",
       "      <td>-0.67190</td>\n",
       "      <td>-0.05147</td>\n",
       "      <td>TRPLTEM128F92E1389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>60.80</td>\n",
       "      <td>5.973</td>\n",
       "      <td>4.344</td>\n",
       "      <td>3.261</td>\n",
       "      <td>2.835</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.446</td>\n",
       "      <td>1.884</td>\n",
       "      <td>1.962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3316</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>-1.4760</td>\n",
       "      <td>0.52700</td>\n",
       "      <td>-2.1960</td>\n",
       "      <td>1.5990</td>\n",
       "      <td>-1.39000</td>\n",
       "      <td>0.22560</td>\n",
       "      <td>-0.72080</td>\n",
       "      <td>TRJWMBQ128F424155E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>51.47</td>\n",
       "      <td>4.971</td>\n",
       "      <td>4.316</td>\n",
       "      <td>2.916</td>\n",
       "      <td>3.112</td>\n",
       "      <td>2.290</td>\n",
       "      <td>2.053</td>\n",
       "      <td>1.934</td>\n",
       "      <td>1.878</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2803</td>\n",
       "      <td>-0.1603</td>\n",
       "      <td>-0.1355</td>\n",
       "      <td>1.03500</td>\n",
       "      <td>0.2370</td>\n",
       "      <td>1.4890</td>\n",
       "      <td>0.02959</td>\n",
       "      <td>-0.13670</td>\n",
       "      <td>0.10820</td>\n",
       "      <td>TRRZWMO12903CCFCC2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>41.28</td>\n",
       "      <td>6.610</td>\n",
       "      <td>4.411</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.822</td>\n",
       "      <td>2.126</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1.973</td>\n",
       "      <td>1.945</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6930</td>\n",
       "      <td>1.0040</td>\n",
       "      <td>-0.3953</td>\n",
       "      <td>0.26710</td>\n",
       "      <td>-1.0450</td>\n",
       "      <td>0.4974</td>\n",
       "      <td>0.03724</td>\n",
       "      <td>1.04500</td>\n",
       "      <td>-0.20000</td>\n",
       "      <td>TRBZRUT12903CE6C04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54.17</td>\n",
       "      <td>8.945</td>\n",
       "      <td>4.685</td>\n",
       "      <td>4.208</td>\n",
       "      <td>3.154</td>\n",
       "      <td>3.527</td>\n",
       "      <td>2.733</td>\n",
       "      <td>2.202</td>\n",
       "      <td>2.686</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4690</td>\n",
       "      <td>-0.5449</td>\n",
       "      <td>-0.5622</td>\n",
       "      <td>-0.08968</td>\n",
       "      <td>-0.9823</td>\n",
       "      <td>-0.2445</td>\n",
       "      <td>-1.65800</td>\n",
       "      <td>-0.04825</td>\n",
       "      <td>-0.70950</td>\n",
       "      <td>TRLUJQF128F42AF5BF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9  \\\n",
       "0   1  41.08  6.579  4.307  3.421  3.192  2.076  2.179  2.052  1.794   \n",
       "1   2  60.80  5.973  4.344  3.261  2.835  2.725  2.446  1.884  1.962   \n",
       "2   3  51.47  4.971  4.316  2.916  3.112  2.290  2.053  1.934  1.878   \n",
       "3   4  41.28  6.610  4.411  2.602  2.822  2.126  1.984  1.973  1.945   \n",
       "4   5  54.17  8.945  4.685  4.208  3.154  3.527  2.733  2.202  2.686   \n",
       "\n",
       "          ...           att18   att19   att20    att21   att22   att23  \\\n",
       "0         ...          1.3470 -0.2463 -1.5470  0.17920 -1.1530 -0.7370   \n",
       "1         ...         -0.3316  0.3519 -1.4760  0.52700 -2.1960  1.5990   \n",
       "2         ...         -0.2803 -0.1603 -0.1355  1.03500  0.2370  1.4890   \n",
       "3         ...         -1.6930  1.0040 -0.3953  0.26710 -1.0450  0.4974   \n",
       "4         ...          2.4690 -0.5449 -0.5622 -0.08968 -0.9823 -0.2445   \n",
       "\n",
       "     att24    att25    att26        msd_track_id  \n",
       "0  0.40750 -0.67190 -0.05147  TRPLTEM128F92E1389  \n",
       "1 -1.39000  0.22560 -0.72080  TRJWMBQ128F424155E  \n",
       "2  0.02959 -0.13670  0.10820  TRRZWMO12903CCFCC2  \n",
       "3  0.03724  1.04500 -0.20000  TRBZRUT12903CE6C04  \n",
       "4 -1.65800 -0.04825 -0.70950  TRLUJQF128F42AF5BF  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>...</th>\n",
       "      <th>att17</th>\n",
       "      <th>att18</th>\n",
       "      <th>att19</th>\n",
       "      <th>att20</th>\n",
       "      <th>att21</th>\n",
       "      <th>att22</th>\n",
       "      <th>att23</th>\n",
       "      <th>att24</th>\n",
       "      <th>att25</th>\n",
       "      <th>att26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38.22</td>\n",
       "      <td>8.076</td>\n",
       "      <td>6.935</td>\n",
       "      <td>4.696</td>\n",
       "      <td>3.856</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.922</td>\n",
       "      <td>2.568</td>\n",
       "      <td>2.070</td>\n",
       "      <td>...</td>\n",
       "      <td>3.988</td>\n",
       "      <td>0.4957</td>\n",
       "      <td>0.1836</td>\n",
       "      <td>-2.2210</td>\n",
       "      <td>0.6453</td>\n",
       "      <td>-0.2923</td>\n",
       "      <td>1.2000</td>\n",
       "      <td>-0.09179</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>36.42</td>\n",
       "      <td>6.131</td>\n",
       "      <td>5.364</td>\n",
       "      <td>4.292</td>\n",
       "      <td>3.968</td>\n",
       "      <td>2.937</td>\n",
       "      <td>2.872</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.050</td>\n",
       "      <td>...</td>\n",
       "      <td>7.098</td>\n",
       "      <td>1.2290</td>\n",
       "      <td>0.5971</td>\n",
       "      <td>-1.0670</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>-1.8240</td>\n",
       "      <td>2.3130</td>\n",
       "      <td>-0.80890</td>\n",
       "      <td>0.5612</td>\n",
       "      <td>-0.6225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>70.01</td>\n",
       "      <td>5.496</td>\n",
       "      <td>4.698</td>\n",
       "      <td>3.699</td>\n",
       "      <td>3.258</td>\n",
       "      <td>2.293</td>\n",
       "      <td>2.680</td>\n",
       "      <td>2.226</td>\n",
       "      <td>2.034</td>\n",
       "      <td>...</td>\n",
       "      <td>4.449</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>1.6370</td>\n",
       "      <td>-1.0690</td>\n",
       "      <td>2.4160</td>\n",
       "      <td>-0.6299</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>-0.81960</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>-0.5948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.64</td>\n",
       "      <td>7.281</td>\n",
       "      <td>6.702</td>\n",
       "      <td>4.043</td>\n",
       "      <td>3.729</td>\n",
       "      <td>3.043</td>\n",
       "      <td>2.644</td>\n",
       "      <td>2.366</td>\n",
       "      <td>1.940</td>\n",
       "      <td>...</td>\n",
       "      <td>2.785</td>\n",
       "      <td>1.9000</td>\n",
       "      <td>-1.1370</td>\n",
       "      <td>1.2750</td>\n",
       "      <td>1.7920</td>\n",
       "      <td>-2.1250</td>\n",
       "      <td>1.6090</td>\n",
       "      <td>-0.83230</td>\n",
       "      <td>-0.1998</td>\n",
       "      <td>-0.1218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38.85</td>\n",
       "      <td>7.118</td>\n",
       "      <td>5.703</td>\n",
       "      <td>4.825</td>\n",
       "      <td>4.088</td>\n",
       "      <td>3.823</td>\n",
       "      <td>3.254</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.193</td>\n",
       "      <td>...</td>\n",
       "      <td>4.536</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>1.0200</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>2.8050</td>\n",
       "      <td>0.2762</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>1.04900</td>\n",
       "      <td>0.3447</td>\n",
       "      <td>-0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   att1   att2   att3   att4   att5   att6   att7   att8   att9   ...    \\\n",
       "0   1  38.22  8.076  6.935  4.696  3.856  3.465  2.922  2.568  2.070   ...     \n",
       "1   2  36.42  6.131  5.364  4.292  3.968  2.937  2.872  2.142  2.050   ...     \n",
       "2   3  70.01  5.496  4.698  3.699  3.258  2.293  2.680  2.226  2.034   ...     \n",
       "3   4  40.64  7.281  6.702  4.043  3.729  3.043  2.644  2.366  1.940   ...     \n",
       "4   5  38.85  7.118  5.703  4.825  4.088  3.823  3.254  2.551  2.193   ...     \n",
       "\n",
       "   att17   att18   att19   att20   att21   att22   att23    att24   att25  \\\n",
       "0  3.988  0.4957  0.1836 -2.2210  0.6453 -0.2923  1.2000 -0.09179  0.4674   \n",
       "1  7.098  1.2290  0.5971 -1.0670  0.9569 -1.8240  2.3130 -0.80890  0.5612   \n",
       "2  4.449  0.4773  1.6370 -1.0690  2.4160 -0.6299  1.4190 -0.81960  0.9151   \n",
       "3  2.785  1.9000 -1.1370  1.2750  1.7920 -2.1250  1.6090 -0.83230 -0.1998   \n",
       "4  4.536  2.1470  1.0200 -0.2656  2.8050  0.2762  0.2504  1.04900  0.3447   \n",
       "\n",
       "    att26  \n",
       "0  0.2158  \n",
       "1 -0.6225  \n",
       "2 -0.5948  \n",
       "3 -0.1218  \n",
       "4 -0.7689  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/submission-random.csv')\n",
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Id, Blues, Country, Electronic, Folk, International, Jazz, Latin, New_Age, Pop_Rock, Rap, Reggae, RnB, Vocal]\n",
       "Index: []"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "# Getting rid of the first and the last column: Id and msd_track_id\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)\n",
    "\n",
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['International', 'Vocal', 'Latin', 'Blues', 'Jazz', 'Folk', 'RnB', 'Pop_Rock', 'New_Age', 'Rap', 'Reggae', 'Electronic', 'Country'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count the freq of the keys in the training labels\n",
    "counted_labels = Counter(train_Y)\n",
    "labels_keys = counted_labels.keys()\n",
    "labels_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_keys_sorted = sorted(labels_keys)\n",
    "labels_keys_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blues': 0,\n",
       " 'Country': 1,\n",
       " 'Electronic': 2,\n",
       " 'Folk': 3,\n",
       " 'International': 4,\n",
       " 'Jazz': 5,\n",
       " 'Latin': 6,\n",
       " 'New_Age': 7,\n",
       " 'Pop_Rock': 8,\n",
       " 'Rap': 9,\n",
       " 'Reggae': 10,\n",
       " 'RnB': 11,\n",
       " 'Vocal': 12}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This for loop for creating a dictionary/ vocab\n",
    "key_to_val = {key: val for val, key in enumerate(labels_keys_sorted)}\n",
    "key_to_val['Country']\n",
    "key_to_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Blues',\n",
       " 1: 'Country',\n",
       " 2: 'Electronic',\n",
       " 3: 'Folk',\n",
       " 4: 'International',\n",
       " 5: 'Jazz',\n",
       " 6: 'Latin',\n",
       " 7: 'New_Age',\n",
       " 8: 'Pop_Rock',\n",
       " 9: 'Rap',\n",
       " 10: 'Reggae',\n",
       " 11: 'RnB',\n",
       " 12: 'Vocal'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_key = {val: key for val, key in enumerate(labels_keys_sorted)}\n",
    "val_to_key[1]\n",
    "val_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13000, 26), (10400, 26), dtype('float64'), dtype('float64'))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std\n",
    "X_train_val.shape, X_test.shape, X_train_val.dtype, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.5925 valid loss: 2.5490, valid accuracy: 0.1200\n",
      "Iter-20 train loss: 2.5356 valid loss: 2.5449, valid accuracy: 0.1215\n",
      "Iter-30 train loss: 2.5042 valid loss: 2.5406, valid accuracy: 0.1254\n",
      "Iter-40 train loss: 2.5028 valid loss: 2.5373, valid accuracy: 0.1285\n",
      "Iter-50 train loss: 2.4788 valid loss: 2.5339, valid accuracy: 0.1323\n",
      "Iter-60 train loss: 2.5625 valid loss: 2.5300, valid accuracy: 0.1323\n",
      "Iter-70 train loss: 2.4671 valid loss: 2.5263, valid accuracy: 0.1377\n",
      "Iter-80 train loss: 2.4526 valid loss: 2.5226, valid accuracy: 0.1400\n",
      "Iter-90 train loss: 2.4862 valid loss: 2.5188, valid accuracy: 0.1454\n",
      "Iter-100 train loss: 2.5369 valid loss: 2.5153, valid accuracy: 0.1508\n",
      "Iter-110 train loss: 2.4544 valid loss: 2.5119, valid accuracy: 0.1500\n",
      "Iter-120 train loss: 2.5334 valid loss: 2.5081, valid accuracy: 0.1523\n",
      "Iter-130 train loss: 2.4852 valid loss: 2.5045, valid accuracy: 0.1523\n",
      "Iter-140 train loss: 2.4347 valid loss: 2.5005, valid accuracy: 0.1562\n",
      "Iter-150 train loss: 2.5163 valid loss: 2.4968, valid accuracy: 0.1608\n",
      "Iter-160 train loss: 2.5070 valid loss: 2.4933, valid accuracy: 0.1623\n",
      "Iter-170 train loss: 2.5217 valid loss: 2.4899, valid accuracy: 0.1623\n",
      "Iter-180 train loss: 2.4808 valid loss: 2.4863, valid accuracy: 0.1646\n",
      "Iter-190 train loss: 2.5153 valid loss: 2.4822, valid accuracy: 0.1669\n",
      "Iter-200 train loss: 2.4184 valid loss: 2.4783, valid accuracy: 0.1685\n",
      "Iter-210 train loss: 2.4205 valid loss: 2.4747, valid accuracy: 0.1738\n",
      "Iter-220 train loss: 2.4912 valid loss: 2.4711, valid accuracy: 0.1777\n",
      "Iter-230 train loss: 2.4389 valid loss: 2.4670, valid accuracy: 0.1792\n",
      "Iter-240 train loss: 2.4022 valid loss: 2.4636, valid accuracy: 0.1823\n",
      "Iter-250 train loss: 2.4558 valid loss: 2.4600, valid accuracy: 0.1823\n",
      "Iter-260 train loss: 2.4188 valid loss: 2.4563, valid accuracy: 0.1838\n",
      "Iter-270 train loss: 2.4540 valid loss: 2.4529, valid accuracy: 0.1831\n",
      "Iter-280 train loss: 2.4325 valid loss: 2.4497, valid accuracy: 0.1831\n",
      "Iter-290 train loss: 2.4638 valid loss: 2.4463, valid accuracy: 0.1869\n",
      "Iter-300 train loss: 2.4509 valid loss: 2.4425, valid accuracy: 0.1885\n",
      "Iter-310 train loss: 2.4591 valid loss: 2.4387, valid accuracy: 0.1908\n",
      "Iter-320 train loss: 2.4320 valid loss: 2.4347, valid accuracy: 0.1908\n",
      "Iter-330 train loss: 2.4179 valid loss: 2.4309, valid accuracy: 0.1938\n",
      "Iter-340 train loss: 2.3598 valid loss: 2.4272, valid accuracy: 0.1969\n",
      "Iter-350 train loss: 2.4061 valid loss: 2.4235, valid accuracy: 0.1977\n",
      "Iter-360 train loss: 2.4868 valid loss: 2.4203, valid accuracy: 0.1969\n",
      "Iter-370 train loss: 2.3530 valid loss: 2.4163, valid accuracy: 0.2015\n",
      "Iter-380 train loss: 2.4554 valid loss: 2.4126, valid accuracy: 0.2038\n",
      "Iter-390 train loss: 2.4696 valid loss: 2.4086, valid accuracy: 0.2069\n",
      "Iter-400 train loss: 2.4228 valid loss: 2.4048, valid accuracy: 0.2062\n",
      "Iter-410 train loss: 2.3447 valid loss: 2.4004, valid accuracy: 0.2092\n",
      "Iter-420 train loss: 2.3235 valid loss: 2.3960, valid accuracy: 0.2092\n",
      "Iter-430 train loss: 2.3207 valid loss: 2.3918, valid accuracy: 0.2085\n",
      "Iter-440 train loss: 2.4166 valid loss: 2.3878, valid accuracy: 0.2123\n",
      "Iter-450 train loss: 2.4370 valid loss: 2.3834, valid accuracy: 0.2085\n",
      "Iter-460 train loss: 2.3159 valid loss: 2.3798, valid accuracy: 0.2077\n",
      "Iter-470 train loss: 2.2350 valid loss: 2.3760, valid accuracy: 0.2069\n",
      "Iter-480 train loss: 2.4441 valid loss: 2.3725, valid accuracy: 0.2085\n",
      "Iter-490 train loss: 2.3941 valid loss: 2.3691, valid accuracy: 0.2092\n",
      "Iter-500 train loss: 2.2865 valid loss: 2.3651, valid accuracy: 0.2085\n",
      "Iter-510 train loss: 2.4457 valid loss: 2.3611, valid accuracy: 0.2115\n",
      "Iter-520 train loss: 2.3374 valid loss: 2.3577, valid accuracy: 0.2123\n",
      "Iter-530 train loss: 2.3590 valid loss: 2.3536, valid accuracy: 0.2085\n",
      "Iter-540 train loss: 2.2941 valid loss: 2.3501, valid accuracy: 0.2062\n",
      "Iter-550 train loss: 2.3024 valid loss: 2.3466, valid accuracy: 0.2046\n",
      "Iter-560 train loss: 2.2951 valid loss: 2.3430, valid accuracy: 0.2062\n",
      "Iter-570 train loss: 2.3954 valid loss: 2.3401, valid accuracy: 0.2069\n",
      "Iter-580 train loss: 2.3726 valid loss: 2.3367, valid accuracy: 0.2069\n",
      "Iter-590 train loss: 2.3881 valid loss: 2.3336, valid accuracy: 0.2077\n",
      "Iter-600 train loss: 2.2826 valid loss: 2.3301, valid accuracy: 0.2085\n",
      "Iter-610 train loss: 2.2024 valid loss: 2.3266, valid accuracy: 0.2108\n",
      "Iter-620 train loss: 2.2311 valid loss: 2.3234, valid accuracy: 0.2100\n",
      "Iter-630 train loss: 2.2628 valid loss: 2.3204, valid accuracy: 0.2100\n",
      "Iter-640 train loss: 2.3726 valid loss: 2.3176, valid accuracy: 0.2108\n",
      "Iter-650 train loss: 2.1883 valid loss: 2.3145, valid accuracy: 0.2092\n",
      "Iter-660 train loss: 2.3181 valid loss: 2.3113, valid accuracy: 0.2085\n",
      "Iter-670 train loss: 2.3864 valid loss: 2.3084, valid accuracy: 0.2085\n",
      "Iter-680 train loss: 2.2852 valid loss: 2.3050, valid accuracy: 0.2100\n",
      "Iter-690 train loss: 2.2731 valid loss: 2.3018, valid accuracy: 0.2100\n",
      "Iter-700 train loss: 2.2417 valid loss: 2.2987, valid accuracy: 0.2123\n",
      "Iter-710 train loss: 2.3223 valid loss: 2.2960, valid accuracy: 0.2092\n",
      "Iter-720 train loss: 2.2450 valid loss: 2.2935, valid accuracy: 0.2077\n",
      "Iter-730 train loss: 2.3093 valid loss: 2.2912, valid accuracy: 0.2054\n",
      "Iter-740 train loss: 2.2399 valid loss: 2.2890, valid accuracy: 0.2069\n",
      "Iter-750 train loss: 2.1413 valid loss: 2.2864, valid accuracy: 0.2069\n",
      "Iter-760 train loss: 2.2595 valid loss: 2.2835, valid accuracy: 0.2077\n",
      "Iter-770 train loss: 2.2762 valid loss: 2.2811, valid accuracy: 0.2062\n",
      "Iter-780 train loss: 2.2300 valid loss: 2.2791, valid accuracy: 0.2092\n",
      "Iter-790 train loss: 2.1782 valid loss: 2.2768, valid accuracy: 0.2123\n",
      "Iter-800 train loss: 2.1781 valid loss: 2.2746, valid accuracy: 0.2154\n",
      "Iter-810 train loss: 2.2158 valid loss: 2.2725, valid accuracy: 0.2162\n",
      "Iter-820 train loss: 2.2346 valid loss: 2.2704, valid accuracy: 0.2146\n",
      "Iter-830 train loss: 2.2755 valid loss: 2.2686, valid accuracy: 0.2138\n",
      "Iter-840 train loss: 2.1173 valid loss: 2.2660, valid accuracy: 0.2138\n",
      "Iter-850 train loss: 2.2674 valid loss: 2.2638, valid accuracy: 0.2177\n",
      "Iter-860 train loss: 2.1876 valid loss: 2.2618, valid accuracy: 0.2154\n",
      "Iter-870 train loss: 2.2203 valid loss: 2.2599, valid accuracy: 0.2192\n",
      "Iter-880 train loss: 2.1288 valid loss: 2.2582, valid accuracy: 0.2162\n",
      "Iter-890 train loss: 2.3457 valid loss: 2.2568, valid accuracy: 0.2177\n",
      "Iter-900 train loss: 2.2079 valid loss: 2.2544, valid accuracy: 0.2200\n",
      "Iter-910 train loss: 2.0306 valid loss: 2.2530, valid accuracy: 0.2169\n",
      "Iter-920 train loss: 2.2413 valid loss: 2.2514, valid accuracy: 0.2162\n",
      "Iter-930 train loss: 2.2928 valid loss: 2.2496, valid accuracy: 0.2169\n",
      "Iter-940 train loss: 2.3404 valid loss: 2.2486, valid accuracy: 0.2192\n",
      "Iter-950 train loss: 2.1620 valid loss: 2.2469, valid accuracy: 0.2177\n",
      "Iter-960 train loss: 2.0204 valid loss: 2.2454, valid accuracy: 0.2177\n",
      "Iter-970 train loss: 2.0736 valid loss: 2.2442, valid accuracy: 0.2192\n",
      "Iter-980 train loss: 2.2019 valid loss: 2.2425, valid accuracy: 0.2185\n",
      "Iter-990 train loss: 2.1087 valid loss: 2.2408, valid accuracy: 0.2200\n",
      "Iter-1000 train loss: 2.2193 valid loss: 2.2391, valid accuracy: 0.2200\n",
      "Iter-1010 train loss: 2.3614 valid loss: 2.2371, valid accuracy: 0.2223\n",
      "Iter-1020 train loss: 2.1733 valid loss: 2.2355, valid accuracy: 0.2215\n",
      "Iter-1030 train loss: 2.1534 valid loss: 2.2342, valid accuracy: 0.2215\n",
      "Iter-1040 train loss: 2.0750 valid loss: 2.2324, valid accuracy: 0.2254\n",
      "Iter-1050 train loss: 2.1156 valid loss: 2.2312, valid accuracy: 0.2223\n",
      "Iter-1060 train loss: 2.0429 valid loss: 2.2300, valid accuracy: 0.2231\n",
      "Iter-1070 train loss: 2.1564 valid loss: 2.2282, valid accuracy: 0.2215\n",
      "Iter-1080 train loss: 2.0997 valid loss: 2.2267, valid accuracy: 0.2192\n",
      "Iter-1090 train loss: 2.1306 valid loss: 2.2250, valid accuracy: 0.2208\n",
      "Iter-1100 train loss: 2.1760 valid loss: 2.2242, valid accuracy: 0.2238\n",
      "Iter-1110 train loss: 2.1534 valid loss: 2.2225, valid accuracy: 0.2262\n",
      "Iter-1120 train loss: 2.2106 valid loss: 2.2211, valid accuracy: 0.2269\n",
      "Iter-1130 train loss: 2.1647 valid loss: 2.2194, valid accuracy: 0.2292\n",
      "Iter-1140 train loss: 2.2881 valid loss: 2.2178, valid accuracy: 0.2246\n",
      "Iter-1150 train loss: 2.0643 valid loss: 2.2167, valid accuracy: 0.2262\n",
      "Iter-1160 train loss: 2.1909 valid loss: 2.2152, valid accuracy: 0.2215\n",
      "Iter-1170 train loss: 2.2223 valid loss: 2.2132, valid accuracy: 0.2277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1180 train loss: 2.1345 valid loss: 2.2119, valid accuracy: 0.2300\n",
      "Iter-1190 train loss: 2.2626 valid loss: 2.2110, valid accuracy: 0.2300\n",
      "Iter-1200 train loss: 2.0953 valid loss: 2.2095, valid accuracy: 0.2331\n",
      "Iter-1210 train loss: 2.2253 valid loss: 2.2087, valid accuracy: 0.2331\n",
      "Iter-1220 train loss: 2.0300 valid loss: 2.2077, valid accuracy: 0.2315\n",
      "Iter-1230 train loss: 2.1422 valid loss: 2.2067, valid accuracy: 0.2323\n",
      "Iter-1240 train loss: 2.3045 valid loss: 2.2059, valid accuracy: 0.2315\n",
      "Iter-1250 train loss: 2.0422 valid loss: 2.2046, valid accuracy: 0.2331\n",
      "Iter-1260 train loss: 2.2176 valid loss: 2.2031, valid accuracy: 0.2354\n",
      "Iter-1270 train loss: 2.2155 valid loss: 2.2014, valid accuracy: 0.2408\n",
      "Iter-1280 train loss: 2.0586 valid loss: 2.1999, valid accuracy: 0.2377\n",
      "Iter-1290 train loss: 2.2441 valid loss: 2.1983, valid accuracy: 0.2400\n",
      "Iter-1300 train loss: 2.0162 valid loss: 2.1973, valid accuracy: 0.2385\n",
      "Iter-1310 train loss: 2.1226 valid loss: 2.1966, valid accuracy: 0.2431\n",
      "Iter-1320 train loss: 2.2609 valid loss: 2.1959, valid accuracy: 0.2423\n",
      "Iter-1330 train loss: 1.9917 valid loss: 2.1948, valid accuracy: 0.2415\n",
      "Iter-1340 train loss: 2.1740 valid loss: 2.1935, valid accuracy: 0.2423\n",
      "Iter-1350 train loss: 2.3080 valid loss: 2.1925, valid accuracy: 0.2438\n",
      "Iter-1360 train loss: 1.9707 valid loss: 2.1915, valid accuracy: 0.2415\n",
      "Iter-1370 train loss: 2.1302 valid loss: 2.1906, valid accuracy: 0.2431\n",
      "Iter-1380 train loss: 2.2441 valid loss: 2.1895, valid accuracy: 0.2431\n",
      "Iter-1390 train loss: 2.1699 valid loss: 2.1890, valid accuracy: 0.2454\n",
      "Iter-1400 train loss: 2.1137 valid loss: 2.1878, valid accuracy: 0.2469\n",
      "Iter-1410 train loss: 2.1669 valid loss: 2.1868, valid accuracy: 0.2438\n",
      "Iter-1420 train loss: 1.9565 valid loss: 2.1861, valid accuracy: 0.2446\n",
      "Iter-1430 train loss: 2.0831 valid loss: 2.1847, valid accuracy: 0.2462\n",
      "Iter-1440 train loss: 2.1143 valid loss: 2.1838, valid accuracy: 0.2469\n",
      "Iter-1450 train loss: 2.1886 valid loss: 2.1825, valid accuracy: 0.2492\n",
      "Iter-1460 train loss: 2.3030 valid loss: 2.1810, valid accuracy: 0.2508\n",
      "Iter-1470 train loss: 2.1157 valid loss: 2.1798, valid accuracy: 0.2508\n",
      "Iter-1480 train loss: 1.9775 valid loss: 2.1795, valid accuracy: 0.2531\n",
      "Iter-1490 train loss: 2.1133 valid loss: 2.1788, valid accuracy: 0.2523\n",
      "Iter-1500 train loss: 2.0349 valid loss: 2.1777, valid accuracy: 0.2485\n",
      "Iter-1510 train loss: 2.4002 valid loss: 2.1772, valid accuracy: 0.2492\n",
      "Iter-1520 train loss: 2.0749 valid loss: 2.1765, valid accuracy: 0.2508\n",
      "Iter-1530 train loss: 2.1168 valid loss: 2.1752, valid accuracy: 0.2500\n",
      "Iter-1540 train loss: 2.1367 valid loss: 2.1741, valid accuracy: 0.2492\n",
      "Iter-1550 train loss: 2.0666 valid loss: 2.1731, valid accuracy: 0.2538\n",
      "Iter-1560 train loss: 2.1219 valid loss: 2.1715, valid accuracy: 0.2538\n",
      "Iter-1570 train loss: 2.0388 valid loss: 2.1705, valid accuracy: 0.2523\n",
      "Iter-1580 train loss: 2.0794 valid loss: 2.1699, valid accuracy: 0.2554\n",
      "Iter-1590 train loss: 2.0528 valid loss: 2.1689, valid accuracy: 0.2469\n",
      "Iter-1600 train loss: 2.0539 valid loss: 2.1676, valid accuracy: 0.2477\n",
      "Iter-1610 train loss: 2.2906 valid loss: 2.1667, valid accuracy: 0.2477\n",
      "Iter-1620 train loss: 2.2228 valid loss: 2.1656, valid accuracy: 0.2538\n",
      "Iter-1630 train loss: 2.0846 valid loss: 2.1642, valid accuracy: 0.2492\n",
      "Iter-1640 train loss: 2.1992 valid loss: 2.1633, valid accuracy: 0.2500\n",
      "Iter-1650 train loss: 2.1778 valid loss: 2.1623, valid accuracy: 0.2508\n",
      "Iter-1660 train loss: 2.0578 valid loss: 2.1612, valid accuracy: 0.2508\n",
      "Iter-1670 train loss: 1.9297 valid loss: 2.1606, valid accuracy: 0.2485\n",
      "Iter-1680 train loss: 2.1380 valid loss: 2.1603, valid accuracy: 0.2485\n",
      "Iter-1690 train loss: 2.0795 valid loss: 2.1598, valid accuracy: 0.2508\n",
      "Iter-1700 train loss: 2.0178 valid loss: 2.1590, valid accuracy: 0.2508\n",
      "Iter-1710 train loss: 2.1069 valid loss: 2.1586, valid accuracy: 0.2546\n",
      "Iter-1720 train loss: 2.1216 valid loss: 2.1579, valid accuracy: 0.2562\n",
      "Iter-1730 train loss: 2.1420 valid loss: 2.1568, valid accuracy: 0.2569\n",
      "Iter-1740 train loss: 2.0311 valid loss: 2.1558, valid accuracy: 0.2577\n",
      "Iter-1750 train loss: 2.1190 valid loss: 2.1551, valid accuracy: 0.2569\n",
      "Iter-1760 train loss: 2.1030 valid loss: 2.1542, valid accuracy: 0.2569\n",
      "Iter-1770 train loss: 2.2483 valid loss: 2.1536, valid accuracy: 0.2538\n",
      "Iter-1780 train loss: 2.0652 valid loss: 2.1524, valid accuracy: 0.2569\n",
      "Iter-1790 train loss: 2.2650 valid loss: 2.1515, valid accuracy: 0.2562\n",
      "Iter-1800 train loss: 2.3210 valid loss: 2.1506, valid accuracy: 0.2531\n",
      "Iter-1810 train loss: 1.9618 valid loss: 2.1499, valid accuracy: 0.2546\n",
      "Iter-1820 train loss: 1.9008 valid loss: 2.1488, valid accuracy: 0.2546\n",
      "Iter-1830 train loss: 2.0229 valid loss: 2.1481, valid accuracy: 0.2562\n",
      "Iter-1840 train loss: 2.2867 valid loss: 2.1474, valid accuracy: 0.2546\n",
      "Iter-1850 train loss: 2.0237 valid loss: 2.1464, valid accuracy: 0.2585\n",
      "Iter-1860 train loss: 2.2063 valid loss: 2.1456, valid accuracy: 0.2562\n",
      "Iter-1870 train loss: 2.2945 valid loss: 2.1448, valid accuracy: 0.2577\n",
      "Iter-1880 train loss: 2.1509 valid loss: 2.1439, valid accuracy: 0.2577\n",
      "Iter-1890 train loss: 2.2232 valid loss: 2.1433, valid accuracy: 0.2592\n",
      "Iter-1900 train loss: 2.1614 valid loss: 2.1425, valid accuracy: 0.2592\n",
      "Iter-1910 train loss: 2.3122 valid loss: 2.1413, valid accuracy: 0.2562\n",
      "Iter-1920 train loss: 2.2068 valid loss: 2.1403, valid accuracy: 0.2546\n",
      "Iter-1930 train loss: 2.3121 valid loss: 2.1393, valid accuracy: 0.2569\n",
      "Iter-1940 train loss: 2.1127 valid loss: 2.1386, valid accuracy: 0.2531\n",
      "Iter-1950 train loss: 2.0572 valid loss: 2.1381, valid accuracy: 0.2500\n",
      "Iter-1960 train loss: 1.9637 valid loss: 2.1377, valid accuracy: 0.2554\n",
      "Iter-1970 train loss: 2.0852 valid loss: 2.1374, valid accuracy: 0.2600\n",
      "Iter-1980 train loss: 2.0596 valid loss: 2.1369, valid accuracy: 0.2608\n",
      "Iter-1990 train loss: 2.1397 valid loss: 2.1367, valid accuracy: 0.2592\n",
      "Iter-2000 train loss: 1.8345 valid loss: 2.1358, valid accuracy: 0.2577\n",
      "Iter-2010 train loss: 1.9967 valid loss: 2.1348, valid accuracy: 0.2592\n",
      "Iter-2020 train loss: 2.1841 valid loss: 2.1347, valid accuracy: 0.2608\n",
      "Iter-2030 train loss: 2.3081 valid loss: 2.1342, valid accuracy: 0.2615\n",
      "Iter-2040 train loss: 2.1754 valid loss: 2.1334, valid accuracy: 0.2615\n",
      "Iter-2050 train loss: 2.1007 valid loss: 2.1325, valid accuracy: 0.2615\n",
      "Iter-2060 train loss: 2.2070 valid loss: 2.1317, valid accuracy: 0.2600\n",
      "Iter-2070 train loss: 2.1158 valid loss: 2.1306, valid accuracy: 0.2631\n",
      "Iter-2080 train loss: 2.0243 valid loss: 2.1302, valid accuracy: 0.2638\n",
      "Iter-2090 train loss: 1.9731 valid loss: 2.1296, valid accuracy: 0.2638\n",
      "Iter-2100 train loss: 2.1557 valid loss: 2.1295, valid accuracy: 0.2608\n",
      "Iter-2110 train loss: 2.1976 valid loss: 2.1304, valid accuracy: 0.2608\n",
      "Iter-2120 train loss: 2.1623 valid loss: 2.1292, valid accuracy: 0.2615\n",
      "Iter-2130 train loss: 2.1248 valid loss: 2.1283, valid accuracy: 0.2608\n",
      "Iter-2140 train loss: 2.1155 valid loss: 2.1283, valid accuracy: 0.2623\n",
      "Iter-2150 train loss: 2.1524 valid loss: 2.1273, valid accuracy: 0.2631\n",
      "Iter-2160 train loss: 2.0595 valid loss: 2.1265, valid accuracy: 0.2631\n",
      "Iter-2170 train loss: 2.3563 valid loss: 2.1258, valid accuracy: 0.2638\n",
      "Iter-2180 train loss: 2.0359 valid loss: 2.1251, valid accuracy: 0.2646\n",
      "Iter-2190 train loss: 2.1429 valid loss: 2.1249, valid accuracy: 0.2600\n",
      "Iter-2200 train loss: 2.1725 valid loss: 2.1241, valid accuracy: 0.2631\n",
      "Iter-2210 train loss: 2.4214 valid loss: 2.1238, valid accuracy: 0.2600\n",
      "Iter-2220 train loss: 2.1262 valid loss: 2.1229, valid accuracy: 0.2662\n",
      "Iter-2230 train loss: 2.0608 valid loss: 2.1227, valid accuracy: 0.2700\n",
      "Iter-2240 train loss: 2.0326 valid loss: 2.1223, valid accuracy: 0.2685\n",
      "Iter-2250 train loss: 2.1744 valid loss: 2.1216, valid accuracy: 0.2723\n",
      "Iter-2260 train loss: 2.0952 valid loss: 2.1210, valid accuracy: 0.2700\n",
      "Iter-2270 train loss: 2.1892 valid loss: 2.1198, valid accuracy: 0.2754\n",
      "Iter-2280 train loss: 2.0276 valid loss: 2.1194, valid accuracy: 0.2754\n",
      "Iter-2290 train loss: 2.1461 valid loss: 2.1187, valid accuracy: 0.2777\n",
      "Iter-2300 train loss: 2.0678 valid loss: 2.1179, valid accuracy: 0.2769\n",
      "Iter-2310 train loss: 2.1059 valid loss: 2.1175, valid accuracy: 0.2754\n",
      "Iter-2320 train loss: 2.0214 valid loss: 2.1158, valid accuracy: 0.2785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2330 train loss: 2.0721 valid loss: 2.1153, valid accuracy: 0.2838\n",
      "Iter-2340 train loss: 2.1320 valid loss: 2.1147, valid accuracy: 0.2846\n",
      "Iter-2350 train loss: 2.0865 valid loss: 2.1144, valid accuracy: 0.2838\n",
      "Iter-2360 train loss: 2.0280 valid loss: 2.1142, valid accuracy: 0.2792\n",
      "Iter-2370 train loss: 2.0691 valid loss: 2.1132, valid accuracy: 0.2785\n",
      "Iter-2380 train loss: 2.1288 valid loss: 2.1122, valid accuracy: 0.2808\n",
      "Iter-2390 train loss: 2.1305 valid loss: 2.1121, valid accuracy: 0.2800\n",
      "Iter-2400 train loss: 2.1045 valid loss: 2.1118, valid accuracy: 0.2792\n",
      "Iter-2410 train loss: 2.1222 valid loss: 2.1114, valid accuracy: 0.2777\n",
      "Iter-2420 train loss: 1.9451 valid loss: 2.1106, valid accuracy: 0.2815\n",
      "Iter-2430 train loss: 2.1938 valid loss: 2.1096, valid accuracy: 0.2815\n",
      "Iter-2440 train loss: 2.0166 valid loss: 2.1090, valid accuracy: 0.2831\n",
      "Iter-2450 train loss: 2.0872 valid loss: 2.1080, valid accuracy: 0.2823\n",
      "Iter-2460 train loss: 2.0575 valid loss: 2.1068, valid accuracy: 0.2846\n",
      "Iter-2470 train loss: 2.2011 valid loss: 2.1062, valid accuracy: 0.2831\n",
      "Iter-2480 train loss: 2.0812 valid loss: 2.1058, valid accuracy: 0.2808\n",
      "Iter-2490 train loss: 2.0751 valid loss: 2.1058, valid accuracy: 0.2769\n",
      "Iter-2500 train loss: 1.9497 valid loss: 2.1050, valid accuracy: 0.2800\n",
      "Iter-2510 train loss: 2.0992 valid loss: 2.1040, valid accuracy: 0.2808\n",
      "Iter-2520 train loss: 2.2224 valid loss: 2.1035, valid accuracy: 0.2808\n",
      "Iter-2530 train loss: 2.0943 valid loss: 2.1038, valid accuracy: 0.2800\n",
      "Iter-2540 train loss: 1.8921 valid loss: 2.1035, valid accuracy: 0.2785\n",
      "Iter-2550 train loss: 2.2103 valid loss: 2.1028, valid accuracy: 0.2800\n",
      "Iter-2560 train loss: 2.0598 valid loss: 2.1031, valid accuracy: 0.2808\n",
      "Iter-2570 train loss: 1.9482 valid loss: 2.1023, valid accuracy: 0.2785\n",
      "Iter-2580 train loss: 2.1561 valid loss: 2.1018, valid accuracy: 0.2785\n",
      "Iter-2590 train loss: 2.0149 valid loss: 2.1015, valid accuracy: 0.2831\n",
      "Iter-2600 train loss: 1.9971 valid loss: 2.0996, valid accuracy: 0.2838\n",
      "Iter-2610 train loss: 1.8209 valid loss: 2.0991, valid accuracy: 0.2831\n",
      "Iter-2620 train loss: 2.0248 valid loss: 2.0996, valid accuracy: 0.2815\n",
      "Iter-2630 train loss: 1.9022 valid loss: 2.0986, valid accuracy: 0.2831\n",
      "Iter-2640 train loss: 1.9935 valid loss: 2.0980, valid accuracy: 0.2815\n",
      "Iter-2650 train loss: 1.9702 valid loss: 2.0977, valid accuracy: 0.2846\n",
      "Iter-2660 train loss: 1.9648 valid loss: 2.0965, valid accuracy: 0.2854\n",
      "Iter-2670 train loss: 1.9394 valid loss: 2.0962, valid accuracy: 0.2846\n",
      "Iter-2680 train loss: 2.0791 valid loss: 2.0956, valid accuracy: 0.2831\n",
      "Iter-2690 train loss: 1.8605 valid loss: 2.0950, valid accuracy: 0.2823\n",
      "Iter-2700 train loss: 2.2100 valid loss: 2.0946, valid accuracy: 0.2808\n",
      "Iter-2710 train loss: 2.1337 valid loss: 2.0941, valid accuracy: 0.2792\n",
      "Iter-2720 train loss: 1.8500 valid loss: 2.0940, valid accuracy: 0.2769\n",
      "Iter-2730 train loss: 2.0142 valid loss: 2.0933, valid accuracy: 0.2792\n",
      "Iter-2740 train loss: 2.0182 valid loss: 2.0930, valid accuracy: 0.2800\n",
      "Iter-2750 train loss: 2.1187 valid loss: 2.0923, valid accuracy: 0.2838\n",
      "Iter-2760 train loss: 2.0208 valid loss: 2.0921, valid accuracy: 0.2815\n",
      "Iter-2770 train loss: 2.1288 valid loss: 2.0909, valid accuracy: 0.2862\n",
      "Iter-2780 train loss: 2.0278 valid loss: 2.0907, valid accuracy: 0.2862\n",
      "Iter-2790 train loss: 2.0227 valid loss: 2.0910, valid accuracy: 0.2815\n",
      "Iter-2800 train loss: 2.1371 valid loss: 2.0902, valid accuracy: 0.2862\n",
      "Iter-2810 train loss: 2.0642 valid loss: 2.0899, valid accuracy: 0.2862\n",
      "Iter-2820 train loss: 2.0759 valid loss: 2.0897, valid accuracy: 0.2892\n",
      "Iter-2830 train loss: 2.1638 valid loss: 2.0893, valid accuracy: 0.2915\n",
      "Iter-2840 train loss: 2.0869 valid loss: 2.0887, valid accuracy: 0.2908\n",
      "Iter-2850 train loss: 2.0002 valid loss: 2.0882, valid accuracy: 0.2946\n",
      "Iter-2860 train loss: 1.9232 valid loss: 2.0881, valid accuracy: 0.2908\n",
      "Iter-2870 train loss: 1.9362 valid loss: 2.0879, valid accuracy: 0.2900\n",
      "Iter-2880 train loss: 2.1913 valid loss: 2.0874, valid accuracy: 0.2877\n",
      "Iter-2890 train loss: 1.8921 valid loss: 2.0878, valid accuracy: 0.2908\n",
      "Iter-2900 train loss: 2.0556 valid loss: 2.0875, valid accuracy: 0.2931\n",
      "Iter-2910 train loss: 2.0506 valid loss: 2.0868, valid accuracy: 0.2892\n",
      "Iter-2920 train loss: 1.9785 valid loss: 2.0861, valid accuracy: 0.2931\n",
      "Iter-2930 train loss: 2.1162 valid loss: 2.0860, valid accuracy: 0.2969\n",
      "Iter-2940 train loss: 2.0160 valid loss: 2.0864, valid accuracy: 0.2931\n",
      "Iter-2950 train loss: 1.8947 valid loss: 2.0854, valid accuracy: 0.2931\n",
      "Iter-2960 train loss: 1.9488 valid loss: 2.0853, valid accuracy: 0.2869\n",
      "Iter-2970 train loss: 2.0610 valid loss: 2.0850, valid accuracy: 0.2923\n",
      "Iter-2980 train loss: 2.1072 valid loss: 2.0849, valid accuracy: 0.2938\n",
      "Iter-2990 train loss: 2.0087 valid loss: 2.0841, valid accuracy: 0.2946\n",
      "Iter-3000 train loss: 2.0352 valid loss: 2.0847, valid accuracy: 0.2915\n",
      "Iter-3010 train loss: 2.3062 valid loss: 2.0836, valid accuracy: 0.2923\n",
      "Iter-3020 train loss: 1.7793 valid loss: 2.0841, valid accuracy: 0.2915\n",
      "Iter-3030 train loss: 1.8297 valid loss: 2.0840, valid accuracy: 0.2846\n",
      "Iter-3040 train loss: 2.2407 valid loss: 2.0830, valid accuracy: 0.2946\n",
      "Iter-3050 train loss: 2.1427 valid loss: 2.0820, valid accuracy: 0.2962\n",
      "Iter-3060 train loss: 2.2345 valid loss: 2.0816, valid accuracy: 0.2946\n",
      "Iter-3070 train loss: 2.2117 valid loss: 2.0811, valid accuracy: 0.2962\n",
      "Iter-3080 train loss: 2.0187 valid loss: 2.0810, valid accuracy: 0.2946\n",
      "Iter-3090 train loss: 1.9847 valid loss: 2.0809, valid accuracy: 0.2962\n",
      "Iter-3100 train loss: 2.0389 valid loss: 2.0810, valid accuracy: 0.2915\n",
      "Iter-3110 train loss: 1.9044 valid loss: 2.0808, valid accuracy: 0.2938\n",
      "Iter-3120 train loss: 2.0402 valid loss: 2.0797, valid accuracy: 0.2954\n",
      "Iter-3130 train loss: 1.9084 valid loss: 2.0796, valid accuracy: 0.2954\n",
      "Iter-3140 train loss: 2.0863 valid loss: 2.0788, valid accuracy: 0.3023\n",
      "Iter-3150 train loss: 2.0382 valid loss: 2.0787, valid accuracy: 0.3015\n",
      "Iter-3160 train loss: 2.3885 valid loss: 2.0789, valid accuracy: 0.3008\n",
      "Iter-3170 train loss: 1.9352 valid loss: 2.0785, valid accuracy: 0.2992\n",
      "Iter-3180 train loss: 1.9945 valid loss: 2.0788, valid accuracy: 0.2977\n",
      "Iter-3190 train loss: 1.9173 valid loss: 2.0785, valid accuracy: 0.2969\n",
      "Iter-3200 train loss: 2.1473 valid loss: 2.0771, valid accuracy: 0.3015\n",
      "Iter-3210 train loss: 2.1493 valid loss: 2.0767, valid accuracy: 0.3031\n",
      "Iter-3220 train loss: 1.8920 valid loss: 2.0764, valid accuracy: 0.2985\n",
      "Iter-3230 train loss: 2.0018 valid loss: 2.0766, valid accuracy: 0.2985\n",
      "Iter-3240 train loss: 1.9487 valid loss: 2.0765, valid accuracy: 0.3008\n",
      "Iter-3250 train loss: 2.0288 valid loss: 2.0773, valid accuracy: 0.2969\n",
      "Iter-3260 train loss: 2.0852 valid loss: 2.0767, valid accuracy: 0.2977\n",
      "Iter-3270 train loss: 2.0990 valid loss: 2.0758, valid accuracy: 0.2938\n",
      "Iter-3280 train loss: 2.0930 valid loss: 2.0761, valid accuracy: 0.2954\n",
      "Iter-3290 train loss: 1.7801 valid loss: 2.0756, valid accuracy: 0.2962\n",
      "Iter-3300 train loss: 2.0209 valid loss: 2.0746, valid accuracy: 0.3038\n",
      "Iter-3310 train loss: 2.0002 valid loss: 2.0741, valid accuracy: 0.2992\n",
      "Iter-3320 train loss: 2.2065 valid loss: 2.0735, valid accuracy: 0.3023\n",
      "Iter-3330 train loss: 2.0823 valid loss: 2.0748, valid accuracy: 0.3008\n",
      "Iter-3340 train loss: 1.9441 valid loss: 2.0748, valid accuracy: 0.2992\n",
      "Iter-3350 train loss: 1.8000 valid loss: 2.0746, valid accuracy: 0.3008\n",
      "Iter-3360 train loss: 2.0369 valid loss: 2.0742, valid accuracy: 0.2985\n",
      "Iter-3370 train loss: 2.1647 valid loss: 2.0737, valid accuracy: 0.3008\n",
      "Iter-3380 train loss: 1.9166 valid loss: 2.0729, valid accuracy: 0.3000\n",
      "Iter-3390 train loss: 2.0638 valid loss: 2.0721, valid accuracy: 0.2992\n",
      "Iter-3400 train loss: 1.9569 valid loss: 2.0720, valid accuracy: 0.2992\n",
      "Iter-3410 train loss: 2.0454 valid loss: 2.0719, valid accuracy: 0.3077\n",
      "Iter-3420 train loss: 1.9236 valid loss: 2.0717, valid accuracy: 0.3031\n",
      "Iter-3430 train loss: 2.1414 valid loss: 2.0716, valid accuracy: 0.3038\n",
      "Iter-3440 train loss: 2.0003 valid loss: 2.0713, valid accuracy: 0.3000\n",
      "Iter-3450 train loss: 1.9887 valid loss: 2.0704, valid accuracy: 0.3015\n",
      "Iter-3460 train loss: 2.0766 valid loss: 2.0694, valid accuracy: 0.3046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3470 train loss: 1.9025 valid loss: 2.0695, valid accuracy: 0.3069\n",
      "Iter-3480 train loss: 1.9555 valid loss: 2.0691, valid accuracy: 0.3046\n",
      "Iter-3490 train loss: 1.9269 valid loss: 2.0695, valid accuracy: 0.3038\n",
      "Iter-3500 train loss: 1.9368 valid loss: 2.0687, valid accuracy: 0.3038\n",
      "Iter-3510 train loss: 2.1345 valid loss: 2.0679, valid accuracy: 0.3038\n",
      "Iter-3520 train loss: 2.0619 valid loss: 2.0673, valid accuracy: 0.3031\n",
      "Iter-3530 train loss: 1.8658 valid loss: 2.0664, valid accuracy: 0.3038\n",
      "Iter-3540 train loss: 2.0124 valid loss: 2.0671, valid accuracy: 0.3015\n",
      "Iter-3550 train loss: 2.0715 valid loss: 2.0671, valid accuracy: 0.3000\n",
      "Iter-3560 train loss: 1.8147 valid loss: 2.0661, valid accuracy: 0.3031\n",
      "Iter-3570 train loss: 2.2164 valid loss: 2.0663, valid accuracy: 0.3054\n",
      "Iter-3580 train loss: 2.0193 valid loss: 2.0658, valid accuracy: 0.3038\n",
      "Iter-3590 train loss: 2.0735 valid loss: 2.0660, valid accuracy: 0.3038\n",
      "Iter-3600 train loss: 2.0301 valid loss: 2.0649, valid accuracy: 0.3069\n",
      "Iter-3610 train loss: 2.0539 valid loss: 2.0645, valid accuracy: 0.3015\n",
      "Iter-3620 train loss: 1.8937 valid loss: 2.0651, valid accuracy: 0.3069\n",
      "Iter-3630 train loss: 1.9543 valid loss: 2.0658, valid accuracy: 0.2985\n",
      "Iter-3640 train loss: 1.9296 valid loss: 2.0649, valid accuracy: 0.3023\n",
      "Iter-3650 train loss: 2.0134 valid loss: 2.0655, valid accuracy: 0.3015\n",
      "Iter-3660 train loss: 1.8848 valid loss: 2.0658, valid accuracy: 0.3054\n",
      "Iter-3670 train loss: 1.7822 valid loss: 2.0647, valid accuracy: 0.3038\n",
      "Iter-3680 train loss: 1.9516 valid loss: 2.0649, valid accuracy: 0.3085\n",
      "Iter-3690 train loss: 2.1174 valid loss: 2.0642, valid accuracy: 0.3092\n",
      "Iter-3700 train loss: 1.8373 valid loss: 2.0638, valid accuracy: 0.3108\n",
      "Iter-3710 train loss: 2.0659 valid loss: 2.0627, valid accuracy: 0.3100\n",
      "Iter-3720 train loss: 2.2673 valid loss: 2.0624, valid accuracy: 0.3108\n",
      "Iter-3730 train loss: 2.0348 valid loss: 2.0617, valid accuracy: 0.3092\n",
      "Iter-3740 train loss: 1.9780 valid loss: 2.0617, valid accuracy: 0.3085\n",
      "Iter-3750 train loss: 1.9876 valid loss: 2.0608, valid accuracy: 0.3108\n",
      "Iter-3760 train loss: 1.9277 valid loss: 2.0619, valid accuracy: 0.3085\n",
      "Iter-3770 train loss: 2.2586 valid loss: 2.0614, valid accuracy: 0.3069\n",
      "Iter-3780 train loss: 2.1706 valid loss: 2.0610, valid accuracy: 0.3046\n",
      "Iter-3790 train loss: 1.9040 valid loss: 2.0614, valid accuracy: 0.3054\n",
      "Iter-3800 train loss: 1.8073 valid loss: 2.0602, valid accuracy: 0.3092\n",
      "Iter-3810 train loss: 2.0633 valid loss: 2.0597, valid accuracy: 0.3069\n",
      "Iter-3820 train loss: 2.0736 valid loss: 2.0602, valid accuracy: 0.3077\n",
      "Iter-3830 train loss: 2.0426 valid loss: 2.0596, valid accuracy: 0.3092\n",
      "Iter-3840 train loss: 1.9402 valid loss: 2.0592, valid accuracy: 0.3062\n",
      "Iter-3850 train loss: 2.0551 valid loss: 2.0593, valid accuracy: 0.3077\n",
      "Iter-3860 train loss: 1.9494 valid loss: 2.0586, valid accuracy: 0.3069\n",
      "Iter-3870 train loss: 2.1429 valid loss: 2.0585, valid accuracy: 0.3108\n",
      "Iter-3880 train loss: 1.9896 valid loss: 2.0584, valid accuracy: 0.3092\n",
      "Iter-3890 train loss: 1.8956 valid loss: 2.0587, valid accuracy: 0.3131\n",
      "Iter-3900 train loss: 1.9441 valid loss: 2.0585, valid accuracy: 0.3077\n",
      "Iter-3910 train loss: 2.0813 valid loss: 2.0587, valid accuracy: 0.3085\n",
      "Iter-3920 train loss: 1.7587 valid loss: 2.0585, valid accuracy: 0.3092\n",
      "Iter-3930 train loss: 2.1400 valid loss: 2.0580, valid accuracy: 0.3115\n",
      "Iter-3940 train loss: 1.9472 valid loss: 2.0577, valid accuracy: 0.3115\n",
      "Iter-3950 train loss: 2.0096 valid loss: 2.0573, valid accuracy: 0.3123\n",
      "Iter-3960 train loss: 2.2370 valid loss: 2.0573, valid accuracy: 0.3100\n",
      "Iter-3970 train loss: 1.7164 valid loss: 2.0574, valid accuracy: 0.3115\n",
      "Iter-3980 train loss: 1.8854 valid loss: 2.0566, valid accuracy: 0.3077\n",
      "Iter-3990 train loss: 1.8210 valid loss: 2.0565, valid accuracy: 0.3100\n",
      "Iter-4000 train loss: 1.7828 valid loss: 2.0568, valid accuracy: 0.3092\n",
      "Iter-4010 train loss: 1.9905 valid loss: 2.0572, valid accuracy: 0.3131\n",
      "Iter-4020 train loss: 1.8727 valid loss: 2.0572, valid accuracy: 0.3092\n",
      "Iter-4030 train loss: 2.1361 valid loss: 2.0563, valid accuracy: 0.3108\n",
      "Iter-4040 train loss: 1.9090 valid loss: 2.0554, valid accuracy: 0.3069\n",
      "Iter-4050 train loss: 2.0613 valid loss: 2.0551, valid accuracy: 0.3077\n",
      "Iter-4060 train loss: 1.9077 valid loss: 2.0546, valid accuracy: 0.3115\n",
      "Iter-4070 train loss: 2.0084 valid loss: 2.0535, valid accuracy: 0.3108\n",
      "Iter-4080 train loss: 2.1809 valid loss: 2.0532, valid accuracy: 0.3115\n",
      "Iter-4090 train loss: 2.0448 valid loss: 2.0529, valid accuracy: 0.3154\n",
      "Iter-4100 train loss: 1.9187 valid loss: 2.0530, valid accuracy: 0.3077\n",
      "Iter-4110 train loss: 1.9494 valid loss: 2.0519, valid accuracy: 0.3108\n",
      "Iter-4120 train loss: 2.0073 valid loss: 2.0520, valid accuracy: 0.3115\n",
      "Iter-4130 train loss: 2.1191 valid loss: 2.0519, valid accuracy: 0.3100\n",
      "Iter-4140 train loss: 2.1970 valid loss: 2.0519, valid accuracy: 0.3100\n",
      "Iter-4150 train loss: 1.9005 valid loss: 2.0520, valid accuracy: 0.3131\n",
      "Iter-4160 train loss: 1.9497 valid loss: 2.0518, valid accuracy: 0.3138\n",
      "Iter-4170 train loss: 2.1889 valid loss: 2.0514, valid accuracy: 0.3085\n",
      "Iter-4180 train loss: 2.1418 valid loss: 2.0508, valid accuracy: 0.3108\n",
      "Iter-4190 train loss: 2.0002 valid loss: 2.0506, valid accuracy: 0.3092\n",
      "Iter-4200 train loss: 1.8343 valid loss: 2.0505, valid accuracy: 0.3131\n",
      "Iter-4210 train loss: 1.9104 valid loss: 2.0505, valid accuracy: 0.3138\n",
      "Iter-4220 train loss: 2.1570 valid loss: 2.0503, valid accuracy: 0.3138\n",
      "Iter-4230 train loss: 1.9779 valid loss: 2.0493, valid accuracy: 0.3115\n",
      "Iter-4240 train loss: 2.0329 valid loss: 2.0496, valid accuracy: 0.3146\n",
      "Iter-4250 train loss: 1.8810 valid loss: 2.0489, valid accuracy: 0.3108\n",
      "Iter-4260 train loss: 1.8803 valid loss: 2.0482, valid accuracy: 0.3123\n",
      "Iter-4270 train loss: 1.8102 valid loss: 2.0486, valid accuracy: 0.3131\n",
      "Iter-4280 train loss: 2.2428 valid loss: 2.0486, valid accuracy: 0.3092\n",
      "Iter-4290 train loss: 2.2298 valid loss: 2.0486, valid accuracy: 0.3123\n",
      "Iter-4300 train loss: 1.9059 valid loss: 2.0479, valid accuracy: 0.3100\n",
      "Iter-4310 train loss: 2.0431 valid loss: 2.0489, valid accuracy: 0.3100\n",
      "Iter-4320 train loss: 1.7077 valid loss: 2.0493, valid accuracy: 0.3131\n",
      "Iter-4330 train loss: 2.1617 valid loss: 2.0487, valid accuracy: 0.3138\n",
      "Iter-4340 train loss: 1.9566 valid loss: 2.0477, valid accuracy: 0.3154\n",
      "Iter-4350 train loss: 1.9994 valid loss: 2.0463, valid accuracy: 0.3177\n",
      "Iter-4360 train loss: 1.7998 valid loss: 2.0458, valid accuracy: 0.3146\n",
      "Iter-4370 train loss: 1.9417 valid loss: 2.0461, valid accuracy: 0.3138\n",
      "Iter-4380 train loss: 1.9154 valid loss: 2.0465, valid accuracy: 0.3185\n",
      "Iter-4390 train loss: 1.9716 valid loss: 2.0459, valid accuracy: 0.3169\n",
      "Iter-4400 train loss: 1.9431 valid loss: 2.0460, valid accuracy: 0.3192\n",
      "Iter-4410 train loss: 2.2380 valid loss: 2.0459, valid accuracy: 0.3200\n",
      "Iter-4420 train loss: 2.0365 valid loss: 2.0452, valid accuracy: 0.3162\n",
      "Iter-4430 train loss: 1.9892 valid loss: 2.0453, valid accuracy: 0.3154\n",
      "Iter-4440 train loss: 1.8454 valid loss: 2.0449, valid accuracy: 0.3162\n",
      "Iter-4450 train loss: 1.8612 valid loss: 2.0463, valid accuracy: 0.3169\n",
      "Iter-4460 train loss: 2.0405 valid loss: 2.0463, valid accuracy: 0.3154\n",
      "Iter-4470 train loss: 1.7900 valid loss: 2.0450, valid accuracy: 0.3154\n",
      "Iter-4480 train loss: 1.7938 valid loss: 2.0444, valid accuracy: 0.3192\n",
      "Iter-4490 train loss: 2.0907 valid loss: 2.0436, valid accuracy: 0.3223\n",
      "Iter-4500 train loss: 1.9468 valid loss: 2.0428, valid accuracy: 0.3208\n",
      "Iter-4510 train loss: 2.2057 valid loss: 2.0425, valid accuracy: 0.3223\n",
      "Iter-4520 train loss: 2.1502 valid loss: 2.0417, valid accuracy: 0.3223\n",
      "Iter-4530 train loss: 2.2903 valid loss: 2.0417, valid accuracy: 0.3208\n",
      "Iter-4540 train loss: 1.8345 valid loss: 2.0419, valid accuracy: 0.3215\n",
      "Iter-4550 train loss: 1.9943 valid loss: 2.0426, valid accuracy: 0.3162\n",
      "Iter-4560 train loss: 2.0961 valid loss: 2.0431, valid accuracy: 0.3154\n",
      "Iter-4570 train loss: 1.9270 valid loss: 2.0434, valid accuracy: 0.3123\n",
      "Iter-4580 train loss: 1.9962 valid loss: 2.0432, valid accuracy: 0.3154\n",
      "Iter-4590 train loss: 2.1076 valid loss: 2.0426, valid accuracy: 0.3146\n",
      "Iter-4600 train loss: 1.8597 valid loss: 2.0419, valid accuracy: 0.3154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4610 train loss: 1.8143 valid loss: 2.0412, valid accuracy: 0.3162\n",
      "Iter-4620 train loss: 2.2102 valid loss: 2.0413, valid accuracy: 0.3162\n",
      "Iter-4630 train loss: 1.9320 valid loss: 2.0420, valid accuracy: 0.3185\n",
      "Iter-4640 train loss: 2.1201 valid loss: 2.0427, valid accuracy: 0.3154\n",
      "Iter-4650 train loss: 1.9535 valid loss: 2.0426, valid accuracy: 0.3200\n",
      "Iter-4660 train loss: 1.9579 valid loss: 2.0413, valid accuracy: 0.3231\n",
      "Iter-4670 train loss: 1.8925 valid loss: 2.0411, valid accuracy: 0.3200\n",
      "Iter-4680 train loss: 1.8815 valid loss: 2.0421, valid accuracy: 0.3200\n",
      "Iter-4690 train loss: 1.9837 valid loss: 2.0414, valid accuracy: 0.3223\n",
      "Iter-4700 train loss: 2.0213 valid loss: 2.0410, valid accuracy: 0.3223\n",
      "Iter-4710 train loss: 2.0024 valid loss: 2.0413, valid accuracy: 0.3200\n",
      "Iter-4720 train loss: 2.0427 valid loss: 2.0409, valid accuracy: 0.3254\n",
      "Iter-4730 train loss: 1.8286 valid loss: 2.0413, valid accuracy: 0.3269\n",
      "Iter-4740 train loss: 1.8961 valid loss: 2.0411, valid accuracy: 0.3269\n",
      "Iter-4750 train loss: 1.7873 valid loss: 2.0403, valid accuracy: 0.3208\n",
      "Iter-4760 train loss: 2.0830 valid loss: 2.0390, valid accuracy: 0.3246\n",
      "Iter-4770 train loss: 2.0652 valid loss: 2.0383, valid accuracy: 0.3200\n",
      "Iter-4780 train loss: 2.0140 valid loss: 2.0382, valid accuracy: 0.3246\n",
      "Iter-4790 train loss: 2.1498 valid loss: 2.0386, valid accuracy: 0.3231\n",
      "Iter-4800 train loss: 2.0282 valid loss: 2.0387, valid accuracy: 0.3246\n",
      "Iter-4810 train loss: 1.8358 valid loss: 2.0388, valid accuracy: 0.3231\n",
      "Iter-4820 train loss: 1.5806 valid loss: 2.0378, valid accuracy: 0.3231\n",
      "Iter-4830 train loss: 1.9244 valid loss: 2.0380, valid accuracy: 0.3231\n",
      "Iter-4840 train loss: 1.9112 valid loss: 2.0378, valid accuracy: 0.3200\n",
      "Iter-4850 train loss: 1.8055 valid loss: 2.0384, valid accuracy: 0.3162\n",
      "Iter-4860 train loss: 1.8835 valid loss: 2.0384, valid accuracy: 0.3154\n",
      "Iter-4870 train loss: 2.0903 valid loss: 2.0376, valid accuracy: 0.3177\n",
      "Iter-4880 train loss: 1.9294 valid loss: 2.0378, valid accuracy: 0.3185\n",
      "Iter-4890 train loss: 1.9221 valid loss: 2.0380, valid accuracy: 0.3254\n",
      "Iter-4900 train loss: 1.8205 valid loss: 2.0377, valid accuracy: 0.3215\n",
      "Iter-4910 train loss: 1.8718 valid loss: 2.0384, valid accuracy: 0.3208\n",
      "Iter-4920 train loss: 1.9403 valid loss: 2.0378, valid accuracy: 0.3200\n",
      "Iter-4930 train loss: 1.9828 valid loss: 2.0372, valid accuracy: 0.3223\n",
      "Iter-4940 train loss: 1.9402 valid loss: 2.0367, valid accuracy: 0.3231\n",
      "Iter-4950 train loss: 1.9213 valid loss: 2.0359, valid accuracy: 0.3254\n",
      "Iter-4960 train loss: 2.0537 valid loss: 2.0364, valid accuracy: 0.3254\n",
      "Iter-4970 train loss: 1.8637 valid loss: 2.0367, valid accuracy: 0.3246\n",
      "Iter-4980 train loss: 1.9601 valid loss: 2.0366, valid accuracy: 0.3238\n",
      "Iter-4990 train loss: 1.9082 valid loss: 2.0356, valid accuracy: 0.3246\n",
      "Iter-5000 train loss: 1.8298 valid loss: 2.0351, valid accuracy: 0.3277\n",
      "Iter-5010 train loss: 2.0179 valid loss: 2.0351, valid accuracy: 0.3292\n",
      "Iter-5020 train loss: 1.9623 valid loss: 2.0346, valid accuracy: 0.3292\n",
      "Iter-5030 train loss: 1.9201 valid loss: 2.0354, valid accuracy: 0.3262\n",
      "Iter-5040 train loss: 1.9607 valid loss: 2.0347, valid accuracy: 0.3231\n",
      "Iter-5050 train loss: 1.9278 valid loss: 2.0347, valid accuracy: 0.3231\n",
      "Iter-5060 train loss: 1.7707 valid loss: 2.0355, valid accuracy: 0.3277\n",
      "Iter-5070 train loss: 1.9660 valid loss: 2.0352, valid accuracy: 0.3292\n",
      "Iter-5080 train loss: 2.0072 valid loss: 2.0351, valid accuracy: 0.3269\n",
      "Iter-5090 train loss: 2.0467 valid loss: 2.0342, valid accuracy: 0.3285\n",
      "Iter-5100 train loss: 2.0272 valid loss: 2.0344, valid accuracy: 0.3292\n",
      "Iter-5110 train loss: 1.9567 valid loss: 2.0339, valid accuracy: 0.3277\n",
      "Iter-5120 train loss: 1.8028 valid loss: 2.0341, valid accuracy: 0.3231\n",
      "Iter-5130 train loss: 1.8640 valid loss: 2.0344, valid accuracy: 0.3238\n",
      "Iter-5140 train loss: 1.8693 valid loss: 2.0327, valid accuracy: 0.3262\n",
      "Iter-5150 train loss: 1.9210 valid loss: 2.0323, valid accuracy: 0.3277\n",
      "Iter-5160 train loss: 1.8819 valid loss: 2.0325, valid accuracy: 0.3262\n",
      "Iter-5170 train loss: 2.1067 valid loss: 2.0331, valid accuracy: 0.3262\n",
      "Iter-5180 train loss: 1.9611 valid loss: 2.0332, valid accuracy: 0.3262\n",
      "Iter-5190 train loss: 2.0514 valid loss: 2.0337, valid accuracy: 0.3277\n",
      "Iter-5200 train loss: 1.9831 valid loss: 2.0336, valid accuracy: 0.3231\n",
      "Iter-5210 train loss: 2.0720 valid loss: 2.0331, valid accuracy: 0.3246\n",
      "Iter-5220 train loss: 2.0302 valid loss: 2.0335, valid accuracy: 0.3223\n",
      "Iter-5230 train loss: 2.0305 valid loss: 2.0346, valid accuracy: 0.3238\n",
      "Iter-5240 train loss: 1.6124 valid loss: 2.0330, valid accuracy: 0.3269\n",
      "Iter-5250 train loss: 2.0374 valid loss: 2.0336, valid accuracy: 0.3223\n",
      "Iter-5260 train loss: 1.8382 valid loss: 2.0339, valid accuracy: 0.3254\n",
      "Iter-5270 train loss: 1.8725 valid loss: 2.0333, valid accuracy: 0.3231\n",
      "Iter-5280 train loss: 2.1578 valid loss: 2.0327, valid accuracy: 0.3254\n",
      "Iter-5290 train loss: 1.6948 valid loss: 2.0326, valid accuracy: 0.3223\n",
      "Iter-5300 train loss: 2.1951 valid loss: 2.0333, valid accuracy: 0.3262\n",
      "Iter-5310 train loss: 2.1455 valid loss: 2.0322, valid accuracy: 0.3246\n",
      "Iter-5320 train loss: 1.9233 valid loss: 2.0322, valid accuracy: 0.3238\n",
      "Iter-5330 train loss: 2.0288 valid loss: 2.0324, valid accuracy: 0.3208\n",
      "Iter-5340 train loss: 1.9067 valid loss: 2.0323, valid accuracy: 0.3223\n",
      "Iter-5350 train loss: 2.0229 valid loss: 2.0322, valid accuracy: 0.3238\n",
      "Iter-5360 train loss: 1.7969 valid loss: 2.0314, valid accuracy: 0.3285\n",
      "Iter-5370 train loss: 1.9719 valid loss: 2.0312, valid accuracy: 0.3231\n",
      "Iter-5380 train loss: 2.1439 valid loss: 2.0303, valid accuracy: 0.3262\n",
      "Iter-5390 train loss: 1.7665 valid loss: 2.0303, valid accuracy: 0.3223\n",
      "Iter-5400 train loss: 1.9594 valid loss: 2.0306, valid accuracy: 0.3238\n",
      "Iter-5410 train loss: 1.8437 valid loss: 2.0313, valid accuracy: 0.3208\n",
      "Iter-5420 train loss: 2.0700 valid loss: 2.0317, valid accuracy: 0.3154\n",
      "Iter-5430 train loss: 1.9886 valid loss: 2.0326, valid accuracy: 0.3177\n",
      "Iter-5440 train loss: 1.8053 valid loss: 2.0310, valid accuracy: 0.3185\n",
      "Iter-5450 train loss: 2.0150 valid loss: 2.0304, valid accuracy: 0.3223\n",
      "Iter-5460 train loss: 2.1722 valid loss: 2.0305, valid accuracy: 0.3208\n",
      "Iter-5470 train loss: 2.1126 valid loss: 2.0301, valid accuracy: 0.3208\n",
      "Iter-5480 train loss: 1.9604 valid loss: 2.0296, valid accuracy: 0.3208\n",
      "Iter-5490 train loss: 1.9848 valid loss: 2.0289, valid accuracy: 0.3215\n",
      "Iter-5500 train loss: 2.0206 valid loss: 2.0299, valid accuracy: 0.3231\n",
      "Iter-5510 train loss: 2.1488 valid loss: 2.0300, valid accuracy: 0.3246\n",
      "Iter-5520 train loss: 2.0085 valid loss: 2.0299, valid accuracy: 0.3223\n",
      "Iter-5530 train loss: 2.0095 valid loss: 2.0297, valid accuracy: 0.3231\n",
      "Iter-5540 train loss: 2.0175 valid loss: 2.0304, valid accuracy: 0.3231\n",
      "Iter-5550 train loss: 1.7127 valid loss: 2.0306, valid accuracy: 0.3231\n",
      "Iter-5560 train loss: 2.1456 valid loss: 2.0302, valid accuracy: 0.3246\n",
      "Iter-5570 train loss: 2.1349 valid loss: 2.0295, valid accuracy: 0.3277\n",
      "Iter-5580 train loss: 1.9216 valid loss: 2.0289, valid accuracy: 0.3323\n",
      "Iter-5590 train loss: 1.9672 valid loss: 2.0279, valid accuracy: 0.3338\n",
      "Iter-5600 train loss: 2.1199 valid loss: 2.0280, valid accuracy: 0.3331\n",
      "Iter-5610 train loss: 2.0151 valid loss: 2.0276, valid accuracy: 0.3338\n",
      "Iter-5620 train loss: 1.9145 valid loss: 2.0282, valid accuracy: 0.3323\n",
      "Iter-5630 train loss: 2.0507 valid loss: 2.0278, valid accuracy: 0.3338\n",
      "Iter-5640 train loss: 1.9488 valid loss: 2.0278, valid accuracy: 0.3292\n",
      "Iter-5650 train loss: 2.0424 valid loss: 2.0274, valid accuracy: 0.3323\n",
      "Iter-5660 train loss: 1.8338 valid loss: 2.0278, valid accuracy: 0.3331\n",
      "Iter-5670 train loss: 1.9267 valid loss: 2.0279, valid accuracy: 0.3385\n",
      "Iter-5680 train loss: 1.6985 valid loss: 2.0295, valid accuracy: 0.3315\n",
      "Iter-5690 train loss: 1.8160 valid loss: 2.0297, valid accuracy: 0.3338\n",
      "Iter-5700 train loss: 1.9420 valid loss: 2.0283, valid accuracy: 0.3277\n",
      "Iter-5710 train loss: 1.8526 valid loss: 2.0296, valid accuracy: 0.3331\n",
      "Iter-5720 train loss: 2.1500 valid loss: 2.0298, valid accuracy: 0.3331\n",
      "Iter-5730 train loss: 1.7855 valid loss: 2.0293, valid accuracy: 0.3323\n",
      "Iter-5740 train loss: 1.7959 valid loss: 2.0290, valid accuracy: 0.3354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5750 train loss: 1.8571 valid loss: 2.0284, valid accuracy: 0.3338\n",
      "Iter-5760 train loss: 2.2178 valid loss: 2.0279, valid accuracy: 0.3300\n",
      "Iter-5770 train loss: 1.6705 valid loss: 2.0287, valid accuracy: 0.3315\n",
      "Iter-5780 train loss: 1.9372 valid loss: 2.0284, valid accuracy: 0.3269\n",
      "Iter-5790 train loss: 1.9975 valid loss: 2.0280, valid accuracy: 0.3277\n",
      "Iter-5800 train loss: 1.9647 valid loss: 2.0281, valid accuracy: 0.3354\n",
      "Iter-5810 train loss: 2.0140 valid loss: 2.0273, valid accuracy: 0.3346\n",
      "Iter-5820 train loss: 2.2202 valid loss: 2.0265, valid accuracy: 0.3362\n",
      "Iter-5830 train loss: 2.0061 valid loss: 2.0270, valid accuracy: 0.3385\n",
      "Iter-5840 train loss: 1.7662 valid loss: 2.0266, valid accuracy: 0.3354\n",
      "Iter-5850 train loss: 1.9070 valid loss: 2.0267, valid accuracy: 0.3346\n",
      "Iter-5860 train loss: 1.8853 valid loss: 2.0261, valid accuracy: 0.3369\n",
      "Iter-5870 train loss: 1.7674 valid loss: 2.0260, valid accuracy: 0.3369\n",
      "Iter-5880 train loss: 1.8192 valid loss: 2.0258, valid accuracy: 0.3346\n",
      "Iter-5890 train loss: 1.8326 valid loss: 2.0270, valid accuracy: 0.3338\n",
      "Iter-5900 train loss: 2.0213 valid loss: 2.0263, valid accuracy: 0.3362\n",
      "Iter-5910 train loss: 2.0465 valid loss: 2.0277, valid accuracy: 0.3385\n",
      "Iter-5920 train loss: 1.9671 valid loss: 2.0273, valid accuracy: 0.3338\n",
      "Iter-5930 train loss: 1.7409 valid loss: 2.0271, valid accuracy: 0.3369\n",
      "Iter-5940 train loss: 1.7951 valid loss: 2.0269, valid accuracy: 0.3400\n",
      "Iter-5950 train loss: 1.8208 valid loss: 2.0266, valid accuracy: 0.3362\n",
      "Iter-5960 train loss: 1.8489 valid loss: 2.0266, valid accuracy: 0.3338\n",
      "Iter-5970 train loss: 1.7867 valid loss: 2.0250, valid accuracy: 0.3377\n",
      "Iter-5980 train loss: 1.9104 valid loss: 2.0253, valid accuracy: 0.3315\n",
      "Iter-5990 train loss: 1.8286 valid loss: 2.0259, valid accuracy: 0.3338\n",
      "Iter-6000 train loss: 2.0826 valid loss: 2.0257, valid accuracy: 0.3331\n",
      "Iter-6010 train loss: 1.8983 valid loss: 2.0251, valid accuracy: 0.3338\n",
      "Iter-6020 train loss: 2.0639 valid loss: 2.0258, valid accuracy: 0.3362\n",
      "Iter-6030 train loss: 2.0648 valid loss: 2.0258, valid accuracy: 0.3346\n",
      "Iter-6040 train loss: 1.9741 valid loss: 2.0258, valid accuracy: 0.3362\n",
      "Iter-6050 train loss: 1.7782 valid loss: 2.0261, valid accuracy: 0.3331\n",
      "Iter-6060 train loss: 1.8515 valid loss: 2.0260, valid accuracy: 0.3331\n",
      "Iter-6070 train loss: 1.8977 valid loss: 2.0249, valid accuracy: 0.3346\n",
      "Iter-6080 train loss: 2.1497 valid loss: 2.0252, valid accuracy: 0.3338\n",
      "Iter-6090 train loss: 1.7438 valid loss: 2.0252, valid accuracy: 0.3346\n",
      "Iter-6100 train loss: 1.8758 valid loss: 2.0250, valid accuracy: 0.3323\n",
      "Iter-6110 train loss: 1.9495 valid loss: 2.0247, valid accuracy: 0.3323\n",
      "Iter-6120 train loss: 1.6145 valid loss: 2.0238, valid accuracy: 0.3331\n",
      "Iter-6130 train loss: 1.9830 valid loss: 2.0242, valid accuracy: 0.3338\n",
      "Iter-6140 train loss: 1.9012 valid loss: 2.0229, valid accuracy: 0.3362\n",
      "Iter-6150 train loss: 2.0276 valid loss: 2.0234, valid accuracy: 0.3369\n",
      "Iter-6160 train loss: 1.9466 valid loss: 2.0231, valid accuracy: 0.3362\n",
      "Iter-6170 train loss: 1.9302 valid loss: 2.0232, valid accuracy: 0.3354\n",
      "Iter-6180 train loss: 1.8394 valid loss: 2.0228, valid accuracy: 0.3377\n",
      "Iter-6190 train loss: 2.1818 valid loss: 2.0238, valid accuracy: 0.3369\n",
      "Iter-6200 train loss: 2.0984 valid loss: 2.0236, valid accuracy: 0.3369\n",
      "Iter-6210 train loss: 1.9087 valid loss: 2.0233, valid accuracy: 0.3377\n",
      "Iter-6220 train loss: 1.9651 valid loss: 2.0230, valid accuracy: 0.3331\n",
      "Iter-6230 train loss: 2.0183 valid loss: 2.0237, valid accuracy: 0.3331\n",
      "Iter-6240 train loss: 1.8158 valid loss: 2.0240, valid accuracy: 0.3308\n",
      "Iter-6250 train loss: 1.9075 valid loss: 2.0248, valid accuracy: 0.3300\n",
      "Iter-6260 train loss: 2.0661 valid loss: 2.0249, valid accuracy: 0.3308\n",
      "Iter-6270 train loss: 1.8515 valid loss: 2.0234, valid accuracy: 0.3323\n",
      "Iter-6280 train loss: 2.0265 valid loss: 2.0248, valid accuracy: 0.3308\n",
      "Iter-6290 train loss: 1.9139 valid loss: 2.0244, valid accuracy: 0.3300\n",
      "Iter-6300 train loss: 1.9192 valid loss: 2.0245, valid accuracy: 0.3308\n",
      "Iter-6310 train loss: 1.8574 valid loss: 2.0240, valid accuracy: 0.3323\n",
      "Iter-6320 train loss: 1.7321 valid loss: 2.0231, valid accuracy: 0.3285\n",
      "Iter-6330 train loss: 1.8315 valid loss: 2.0231, valid accuracy: 0.3331\n",
      "Iter-6340 train loss: 1.9715 valid loss: 2.0215, valid accuracy: 0.3362\n",
      "Iter-6350 train loss: 1.9924 valid loss: 2.0215, valid accuracy: 0.3315\n",
      "Iter-6360 train loss: 1.8992 valid loss: 2.0210, valid accuracy: 0.3277\n",
      "Iter-6370 train loss: 1.7706 valid loss: 2.0207, valid accuracy: 0.3315\n",
      "Iter-6380 train loss: 1.8669 valid loss: 2.0200, valid accuracy: 0.3346\n",
      "Iter-6390 train loss: 1.8236 valid loss: 2.0197, valid accuracy: 0.3385\n",
      "Iter-6400 train loss: 1.9349 valid loss: 2.0197, valid accuracy: 0.3354\n",
      "Iter-6410 train loss: 1.9917 valid loss: 2.0196, valid accuracy: 0.3392\n",
      "Iter-6420 train loss: 2.0288 valid loss: 2.0193, valid accuracy: 0.3415\n",
      "Iter-6430 train loss: 1.9829 valid loss: 2.0194, valid accuracy: 0.3431\n",
      "Iter-6440 train loss: 1.7788 valid loss: 2.0195, valid accuracy: 0.3369\n",
      "Iter-6450 train loss: 2.1586 valid loss: 2.0211, valid accuracy: 0.3354\n",
      "Iter-6460 train loss: 1.8463 valid loss: 2.0208, valid accuracy: 0.3362\n",
      "Iter-6470 train loss: 1.8386 valid loss: 2.0217, valid accuracy: 0.3346\n",
      "Iter-6480 train loss: 1.7436 valid loss: 2.0210, valid accuracy: 0.3362\n",
      "Iter-6490 train loss: 2.2937 valid loss: 2.0201, valid accuracy: 0.3354\n",
      "Iter-6500 train loss: 1.9615 valid loss: 2.0199, valid accuracy: 0.3377\n",
      "Iter-6510 train loss: 2.1602 valid loss: 2.0190, valid accuracy: 0.3377\n",
      "Iter-6520 train loss: 2.1285 valid loss: 2.0189, valid accuracy: 0.3385\n",
      "Iter-6530 train loss: 2.2344 valid loss: 2.0193, valid accuracy: 0.3362\n",
      "Iter-6540 train loss: 1.9683 valid loss: 2.0188, valid accuracy: 0.3392\n",
      "Iter-6550 train loss: 1.8613 valid loss: 2.0194, valid accuracy: 0.3392\n",
      "Iter-6560 train loss: 1.7917 valid loss: 2.0188, valid accuracy: 0.3392\n",
      "Iter-6570 train loss: 1.9894 valid loss: 2.0183, valid accuracy: 0.3392\n",
      "Iter-6580 train loss: 1.8035 valid loss: 2.0184, valid accuracy: 0.3385\n",
      "Iter-6590 train loss: 1.7603 valid loss: 2.0182, valid accuracy: 0.3369\n",
      "Iter-6600 train loss: 2.0427 valid loss: 2.0188, valid accuracy: 0.3385\n",
      "Iter-6610 train loss: 1.8088 valid loss: 2.0186, valid accuracy: 0.3346\n",
      "Iter-6620 train loss: 1.9479 valid loss: 2.0183, valid accuracy: 0.3377\n",
      "Iter-6630 train loss: 2.0683 valid loss: 2.0172, valid accuracy: 0.3354\n",
      "Iter-6640 train loss: 1.8304 valid loss: 2.0175, valid accuracy: 0.3338\n",
      "Iter-6650 train loss: 2.0080 valid loss: 2.0168, valid accuracy: 0.3400\n",
      "Iter-6660 train loss: 2.1624 valid loss: 2.0177, valid accuracy: 0.3369\n",
      "Iter-6670 train loss: 2.0007 valid loss: 2.0185, valid accuracy: 0.3408\n",
      "Iter-6680 train loss: 1.7466 valid loss: 2.0193, valid accuracy: 0.3408\n",
      "Iter-6690 train loss: 2.3294 valid loss: 2.0197, valid accuracy: 0.3385\n",
      "Iter-6700 train loss: 1.8984 valid loss: 2.0192, valid accuracy: 0.3362\n",
      "Iter-6710 train loss: 1.7593 valid loss: 2.0191, valid accuracy: 0.3354\n",
      "Iter-6720 train loss: 1.8144 valid loss: 2.0189, valid accuracy: 0.3323\n",
      "Iter-6730 train loss: 2.1891 valid loss: 2.0185, valid accuracy: 0.3323\n",
      "Iter-6740 train loss: 1.7292 valid loss: 2.0192, valid accuracy: 0.3331\n",
      "Iter-6750 train loss: 2.0159 valid loss: 2.0181, valid accuracy: 0.3323\n",
      "Iter-6760 train loss: 1.9142 valid loss: 2.0166, valid accuracy: 0.3369\n",
      "Iter-6770 train loss: 1.9505 valid loss: 2.0165, valid accuracy: 0.3400\n",
      "Iter-6780 train loss: 1.5242 valid loss: 2.0172, valid accuracy: 0.3408\n",
      "Iter-6790 train loss: 1.8995 valid loss: 2.0164, valid accuracy: 0.3423\n",
      "Iter-6800 train loss: 1.7715 valid loss: 2.0162, valid accuracy: 0.3377\n",
      "Iter-6810 train loss: 1.9916 valid loss: 2.0169, valid accuracy: 0.3377\n",
      "Iter-6820 train loss: 1.8866 valid loss: 2.0171, valid accuracy: 0.3400\n",
      "Iter-6830 train loss: 1.9080 valid loss: 2.0170, valid accuracy: 0.3415\n",
      "Iter-6840 train loss: 1.8839 valid loss: 2.0174, valid accuracy: 0.3385\n",
      "Iter-6850 train loss: 2.1569 valid loss: 2.0164, valid accuracy: 0.3362\n",
      "Iter-6860 train loss: 1.8637 valid loss: 2.0156, valid accuracy: 0.3346\n",
      "Iter-6870 train loss: 1.7536 valid loss: 2.0155, valid accuracy: 0.3346\n",
      "Iter-6880 train loss: 2.1233 valid loss: 2.0156, valid accuracy: 0.3308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6890 train loss: 1.6330 valid loss: 2.0139, valid accuracy: 0.3338\n",
      "Iter-6900 train loss: 2.1804 valid loss: 2.0151, valid accuracy: 0.3331\n",
      "Iter-6910 train loss: 1.8395 valid loss: 2.0150, valid accuracy: 0.3323\n",
      "Iter-6920 train loss: 1.5707 valid loss: 2.0152, valid accuracy: 0.3323\n",
      "Iter-6930 train loss: 1.7695 valid loss: 2.0143, valid accuracy: 0.3362\n",
      "Iter-6940 train loss: 2.1391 valid loss: 2.0143, valid accuracy: 0.3423\n",
      "Iter-6950 train loss: 1.9876 valid loss: 2.0137, valid accuracy: 0.3385\n",
      "Iter-6960 train loss: 2.0882 valid loss: 2.0139, valid accuracy: 0.3392\n",
      "Iter-6970 train loss: 1.7565 valid loss: 2.0126, valid accuracy: 0.3423\n",
      "Iter-6980 train loss: 1.9207 valid loss: 2.0130, valid accuracy: 0.3385\n",
      "Iter-6990 train loss: 1.9704 valid loss: 2.0136, valid accuracy: 0.3400\n",
      "Iter-7000 train loss: 1.7532 valid loss: 2.0149, valid accuracy: 0.3369\n",
      "Iter-7010 train loss: 2.0719 valid loss: 2.0153, valid accuracy: 0.3369\n",
      "Iter-7020 train loss: 1.8284 valid loss: 2.0151, valid accuracy: 0.3362\n",
      "Iter-7030 train loss: 1.9306 valid loss: 2.0145, valid accuracy: 0.3338\n",
      "Iter-7040 train loss: 1.9295 valid loss: 2.0139, valid accuracy: 0.3354\n",
      "Iter-7050 train loss: 2.3340 valid loss: 2.0140, valid accuracy: 0.3338\n",
      "Iter-7060 train loss: 1.7637 valid loss: 2.0134, valid accuracy: 0.3377\n",
      "Iter-7070 train loss: 2.0118 valid loss: 2.0132, valid accuracy: 0.3369\n",
      "Iter-7080 train loss: 1.6373 valid loss: 2.0130, valid accuracy: 0.3423\n",
      "Iter-7090 train loss: 2.1265 valid loss: 2.0137, valid accuracy: 0.3415\n",
      "Iter-7100 train loss: 1.9868 valid loss: 2.0134, valid accuracy: 0.3408\n",
      "Iter-7110 train loss: 2.1113 valid loss: 2.0142, valid accuracy: 0.3377\n",
      "Iter-7120 train loss: 1.9053 valid loss: 2.0135, valid accuracy: 0.3400\n",
      "Iter-7130 train loss: 1.7604 valid loss: 2.0145, valid accuracy: 0.3392\n",
      "Iter-7140 train loss: 2.0592 valid loss: 2.0134, valid accuracy: 0.3400\n",
      "Iter-7150 train loss: 1.8755 valid loss: 2.0139, valid accuracy: 0.3400\n",
      "Iter-7160 train loss: 1.9051 valid loss: 2.0132, valid accuracy: 0.3408\n",
      "Iter-7170 train loss: 1.7867 valid loss: 2.0136, valid accuracy: 0.3408\n",
      "Iter-7180 train loss: 1.7884 valid loss: 2.0142, valid accuracy: 0.3400\n",
      "Iter-7190 train loss: 1.8549 valid loss: 2.0139, valid accuracy: 0.3415\n",
      "Iter-7200 train loss: 1.7167 valid loss: 2.0152, valid accuracy: 0.3408\n",
      "Iter-7210 train loss: 2.0259 valid loss: 2.0146, valid accuracy: 0.3354\n",
      "Iter-7220 train loss: 2.1375 valid loss: 2.0145, valid accuracy: 0.3362\n",
      "Iter-7230 train loss: 1.9844 valid loss: 2.0144, valid accuracy: 0.3377\n",
      "Iter-7240 train loss: 1.9114 valid loss: 2.0144, valid accuracy: 0.3385\n",
      "Iter-7250 train loss: 1.8643 valid loss: 2.0135, valid accuracy: 0.3392\n",
      "Iter-7260 train loss: 1.9107 valid loss: 2.0137, valid accuracy: 0.3377\n",
      "Iter-7270 train loss: 2.3469 valid loss: 2.0137, valid accuracy: 0.3377\n",
      "Iter-7280 train loss: 1.7161 valid loss: 2.0144, valid accuracy: 0.3385\n",
      "Iter-7290 train loss: 1.7852 valid loss: 2.0143, valid accuracy: 0.3385\n",
      "Iter-7300 train loss: 1.8615 valid loss: 2.0138, valid accuracy: 0.3377\n",
      "Iter-7310 train loss: 1.9021 valid loss: 2.0129, valid accuracy: 0.3385\n",
      "Iter-7320 train loss: 2.2047 valid loss: 2.0118, valid accuracy: 0.3400\n",
      "Iter-7330 train loss: 1.8860 valid loss: 2.0119, valid accuracy: 0.3431\n",
      "Iter-7340 train loss: 1.9923 valid loss: 2.0125, valid accuracy: 0.3438\n",
      "Iter-7350 train loss: 1.9701 valid loss: 2.0116, valid accuracy: 0.3400\n",
      "Iter-7360 train loss: 2.0312 valid loss: 2.0113, valid accuracy: 0.3400\n",
      "Iter-7370 train loss: 1.8864 valid loss: 2.0112, valid accuracy: 0.3392\n",
      "Iter-7380 train loss: 2.1734 valid loss: 2.0117, valid accuracy: 0.3415\n",
      "Iter-7390 train loss: 1.9067 valid loss: 2.0121, valid accuracy: 0.3362\n",
      "Iter-7400 train loss: 1.9143 valid loss: 2.0112, valid accuracy: 0.3385\n",
      "Iter-7410 train loss: 2.1560 valid loss: 2.0115, valid accuracy: 0.3377\n",
      "Iter-7420 train loss: 1.7191 valid loss: 2.0118, valid accuracy: 0.3338\n",
      "Iter-7430 train loss: 1.7045 valid loss: 2.0112, valid accuracy: 0.3385\n",
      "Iter-7440 train loss: 1.6762 valid loss: 2.0121, valid accuracy: 0.3392\n",
      "Iter-7450 train loss: 1.8262 valid loss: 2.0120, valid accuracy: 0.3385\n",
      "Iter-7460 train loss: 1.7687 valid loss: 2.0127, valid accuracy: 0.3362\n",
      "Iter-7470 train loss: 1.8877 valid loss: 2.0126, valid accuracy: 0.3377\n",
      "Iter-7480 train loss: 2.0149 valid loss: 2.0123, valid accuracy: 0.3354\n",
      "Iter-7490 train loss: 1.8615 valid loss: 2.0117, valid accuracy: 0.3369\n",
      "Iter-7500 train loss: 1.9121 valid loss: 2.0116, valid accuracy: 0.3415\n",
      "Iter-7510 train loss: 1.9854 valid loss: 2.0120, valid accuracy: 0.3369\n",
      "Iter-7520 train loss: 1.6448 valid loss: 2.0121, valid accuracy: 0.3362\n",
      "Iter-7530 train loss: 1.7779 valid loss: 2.0118, valid accuracy: 0.3354\n",
      "Iter-7540 train loss: 1.8738 valid loss: 2.0124, valid accuracy: 0.3354\n",
      "Iter-7550 train loss: 1.9078 valid loss: 2.0121, valid accuracy: 0.3377\n",
      "Iter-7560 train loss: 1.9426 valid loss: 2.0129, valid accuracy: 0.3408\n",
      "Iter-7570 train loss: 1.9012 valid loss: 2.0125, valid accuracy: 0.3423\n",
      "Iter-7580 train loss: 1.9152 valid loss: 2.0128, valid accuracy: 0.3385\n",
      "Iter-7590 train loss: 2.0144 valid loss: 2.0118, valid accuracy: 0.3369\n",
      "Iter-7600 train loss: 1.9404 valid loss: 2.0123, valid accuracy: 0.3362\n",
      "Iter-7610 train loss: 1.7755 valid loss: 2.0117, valid accuracy: 0.3392\n",
      "Iter-7620 train loss: 1.9030 valid loss: 2.0126, valid accuracy: 0.3369\n",
      "Iter-7630 train loss: 2.0079 valid loss: 2.0120, valid accuracy: 0.3362\n",
      "Iter-7640 train loss: 2.1273 valid loss: 2.0115, valid accuracy: 0.3338\n",
      "Iter-7650 train loss: 1.8916 valid loss: 2.0104, valid accuracy: 0.3446\n",
      "Iter-7660 train loss: 1.7128 valid loss: 2.0096, valid accuracy: 0.3392\n",
      "Iter-7670 train loss: 1.8994 valid loss: 2.0095, valid accuracy: 0.3408\n",
      "Iter-7680 train loss: 2.1004 valid loss: 2.0086, valid accuracy: 0.3415\n",
      "Iter-7690 train loss: 1.8264 valid loss: 2.0091, valid accuracy: 0.3377\n",
      "Iter-7700 train loss: 1.9498 valid loss: 2.0080, valid accuracy: 0.3423\n",
      "Iter-7710 train loss: 1.7640 valid loss: 2.0089, valid accuracy: 0.3400\n",
      "Iter-7720 train loss: 1.8758 valid loss: 2.0083, valid accuracy: 0.3423\n",
      "Iter-7730 train loss: 1.9107 valid loss: 2.0077, valid accuracy: 0.3415\n",
      "Iter-7740 train loss: 2.0830 valid loss: 2.0082, valid accuracy: 0.3362\n",
      "Iter-7750 train loss: 2.0796 valid loss: 2.0075, valid accuracy: 0.3377\n",
      "Iter-7760 train loss: 1.9494 valid loss: 2.0076, valid accuracy: 0.3423\n",
      "Iter-7770 train loss: 1.8634 valid loss: 2.0077, valid accuracy: 0.3423\n",
      "Iter-7780 train loss: 2.2563 valid loss: 2.0078, valid accuracy: 0.3438\n",
      "Iter-7790 train loss: 1.9567 valid loss: 2.0067, valid accuracy: 0.3415\n",
      "Iter-7800 train loss: 2.0239 valid loss: 2.0062, valid accuracy: 0.3408\n",
      "Iter-7810 train loss: 1.7720 valid loss: 2.0062, valid accuracy: 0.3423\n",
      "Iter-7820 train loss: 1.9287 valid loss: 2.0078, valid accuracy: 0.3354\n",
      "Iter-7830 train loss: 1.9583 valid loss: 2.0078, valid accuracy: 0.3362\n",
      "Iter-7840 train loss: 1.9767 valid loss: 2.0090, valid accuracy: 0.3331\n",
      "Iter-7850 train loss: 2.1711 valid loss: 2.0087, valid accuracy: 0.3415\n",
      "Iter-7860 train loss: 2.2793 valid loss: 2.0079, valid accuracy: 0.3408\n",
      "Iter-7870 train loss: 1.9935 valid loss: 2.0075, valid accuracy: 0.3400\n",
      "Iter-7880 train loss: 1.8322 valid loss: 2.0080, valid accuracy: 0.3362\n",
      "Iter-7890 train loss: 2.2035 valid loss: 2.0089, valid accuracy: 0.3377\n",
      "Iter-7900 train loss: 2.1368 valid loss: 2.0091, valid accuracy: 0.3408\n",
      "Iter-7910 train loss: 1.9292 valid loss: 2.0086, valid accuracy: 0.3454\n",
      "Iter-7920 train loss: 2.0065 valid loss: 2.0084, valid accuracy: 0.3446\n",
      "Iter-7930 train loss: 1.8114 valid loss: 2.0091, valid accuracy: 0.3438\n",
      "Iter-7940 train loss: 1.9672 valid loss: 2.0094, valid accuracy: 0.3392\n",
      "Iter-7950 train loss: 1.8130 valid loss: 2.0098, valid accuracy: 0.3400\n",
      "Iter-7960 train loss: 1.9684 valid loss: 2.0098, valid accuracy: 0.3408\n",
      "Iter-7970 train loss: 1.6984 valid loss: 2.0095, valid accuracy: 0.3423\n",
      "Iter-7980 train loss: 2.1429 valid loss: 2.0089, valid accuracy: 0.3392\n",
      "Iter-7990 train loss: 1.8088 valid loss: 2.0083, valid accuracy: 0.3385\n",
      "Iter-8000 train loss: 2.1209 valid loss: 2.0085, valid accuracy: 0.3423\n",
      "Iter-8010 train loss: 1.7880 valid loss: 2.0079, valid accuracy: 0.3438\n",
      "Iter-8020 train loss: 1.8892 valid loss: 2.0078, valid accuracy: 0.3446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8030 train loss: 1.8945 valid loss: 2.0082, valid accuracy: 0.3446\n",
      "Iter-8040 train loss: 1.8516 valid loss: 2.0085, valid accuracy: 0.3446\n",
      "Iter-8050 train loss: 1.6692 valid loss: 2.0079, valid accuracy: 0.3462\n",
      "Iter-8060 train loss: 1.9273 valid loss: 2.0078, valid accuracy: 0.3446\n",
      "Iter-8070 train loss: 1.8204 valid loss: 2.0082, valid accuracy: 0.3469\n",
      "Iter-8080 train loss: 1.8187 valid loss: 2.0087, valid accuracy: 0.3400\n",
      "Iter-8090 train loss: 1.8781 valid loss: 2.0083, valid accuracy: 0.3392\n",
      "Iter-8100 train loss: 2.1396 valid loss: 2.0088, valid accuracy: 0.3377\n",
      "Iter-8110 train loss: 2.0383 valid loss: 2.0088, valid accuracy: 0.3392\n",
      "Iter-8120 train loss: 2.0653 valid loss: 2.0078, valid accuracy: 0.3377\n",
      "Iter-8130 train loss: 1.8361 valid loss: 2.0081, valid accuracy: 0.3392\n",
      "Iter-8140 train loss: 2.0442 valid loss: 2.0079, valid accuracy: 0.3385\n",
      "Iter-8150 train loss: 2.2374 valid loss: 2.0080, valid accuracy: 0.3362\n",
      "Iter-8160 train loss: 1.9467 valid loss: 2.0082, valid accuracy: 0.3346\n",
      "Iter-8170 train loss: 1.8249 valid loss: 2.0093, valid accuracy: 0.3415\n",
      "Iter-8180 train loss: 1.8674 valid loss: 2.0099, valid accuracy: 0.3392\n",
      "Iter-8190 train loss: 1.8179 valid loss: 2.0102, valid accuracy: 0.3369\n",
      "Iter-8200 train loss: 1.9855 valid loss: 2.0103, valid accuracy: 0.3408\n",
      "Iter-8210 train loss: 1.8604 valid loss: 2.0102, valid accuracy: 0.3377\n",
      "Iter-8220 train loss: 1.8117 valid loss: 2.0102, valid accuracy: 0.3400\n",
      "Iter-8230 train loss: 1.8759 valid loss: 2.0099, valid accuracy: 0.3377\n",
      "Iter-8240 train loss: 1.6961 valid loss: 2.0089, valid accuracy: 0.3431\n",
      "Iter-8250 train loss: 1.8621 valid loss: 2.0091, valid accuracy: 0.3423\n",
      "Iter-8260 train loss: 1.9644 valid loss: 2.0089, valid accuracy: 0.3438\n",
      "Iter-8270 train loss: 2.0141 valid loss: 2.0106, valid accuracy: 0.3408\n",
      "Iter-8280 train loss: 1.7609 valid loss: 2.0096, valid accuracy: 0.3423\n",
      "Iter-8290 train loss: 1.8725 valid loss: 2.0089, valid accuracy: 0.3415\n",
      "Iter-8300 train loss: 1.7512 valid loss: 2.0091, valid accuracy: 0.3400\n",
      "Iter-8310 train loss: 1.8886 valid loss: 2.0093, valid accuracy: 0.3385\n",
      "Iter-8320 train loss: 1.5636 valid loss: 2.0085, valid accuracy: 0.3400\n",
      "Iter-8330 train loss: 1.9425 valid loss: 2.0088, valid accuracy: 0.3408\n",
      "Iter-8340 train loss: 1.7942 valid loss: 2.0090, valid accuracy: 0.3400\n",
      "Iter-8350 train loss: 1.8453 valid loss: 2.0096, valid accuracy: 0.3423\n",
      "Iter-8360 train loss: 1.9643 valid loss: 2.0081, valid accuracy: 0.3400\n",
      "Iter-8370 train loss: 1.9614 valid loss: 2.0071, valid accuracy: 0.3446\n",
      "Iter-8380 train loss: 1.8653 valid loss: 2.0072, valid accuracy: 0.3431\n",
      "Iter-8390 train loss: 1.8298 valid loss: 2.0083, valid accuracy: 0.3438\n",
      "Iter-8400 train loss: 1.9588 valid loss: 2.0077, valid accuracy: 0.3415\n",
      "Iter-8410 train loss: 1.9064 valid loss: 2.0064, valid accuracy: 0.3392\n",
      "Iter-8420 train loss: 2.0453 valid loss: 2.0060, valid accuracy: 0.3408\n",
      "Iter-8430 train loss: 2.2695 valid loss: 2.0074, valid accuracy: 0.3454\n",
      "Iter-8440 train loss: 1.7299 valid loss: 2.0070, valid accuracy: 0.3462\n",
      "Iter-8450 train loss: 2.1137 valid loss: 2.0066, valid accuracy: 0.3477\n",
      "Iter-8460 train loss: 1.9865 valid loss: 2.0065, valid accuracy: 0.3485\n",
      "Iter-8470 train loss: 2.0345 valid loss: 2.0057, valid accuracy: 0.3523\n",
      "Iter-8480 train loss: 1.7538 valid loss: 2.0060, valid accuracy: 0.3485\n",
      "Iter-8490 train loss: 1.8988 valid loss: 2.0063, valid accuracy: 0.3485\n",
      "Iter-8500 train loss: 1.8147 valid loss: 2.0069, valid accuracy: 0.3500\n",
      "Iter-8510 train loss: 1.9247 valid loss: 2.0070, valid accuracy: 0.3523\n",
      "Iter-8520 train loss: 1.8926 valid loss: 2.0066, valid accuracy: 0.3562\n",
      "Iter-8530 train loss: 1.8712 valid loss: 2.0061, valid accuracy: 0.3523\n",
      "Iter-8540 train loss: 1.7797 valid loss: 2.0051, valid accuracy: 0.3531\n",
      "Iter-8550 train loss: 2.0959 valid loss: 2.0052, valid accuracy: 0.3469\n",
      "Iter-8560 train loss: 2.2130 valid loss: 2.0052, valid accuracy: 0.3446\n",
      "Iter-8570 train loss: 1.7628 valid loss: 2.0059, valid accuracy: 0.3469\n",
      "Iter-8580 train loss: 1.8546 valid loss: 2.0070, valid accuracy: 0.3469\n",
      "Iter-8590 train loss: 1.8706 valid loss: 2.0071, valid accuracy: 0.3485\n",
      "Iter-8600 train loss: 2.0702 valid loss: 2.0074, valid accuracy: 0.3485\n",
      "Iter-8610 train loss: 1.8317 valid loss: 2.0062, valid accuracy: 0.3469\n",
      "Iter-8620 train loss: 2.0599 valid loss: 2.0056, valid accuracy: 0.3477\n",
      "Iter-8630 train loss: 1.7392 valid loss: 2.0046, valid accuracy: 0.3469\n",
      "Iter-8640 train loss: 2.0263 valid loss: 2.0047, valid accuracy: 0.3454\n",
      "Iter-8650 train loss: 1.8481 valid loss: 2.0052, valid accuracy: 0.3477\n",
      "Iter-8660 train loss: 1.8233 valid loss: 2.0054, valid accuracy: 0.3500\n",
      "Iter-8670 train loss: 1.8208 valid loss: 2.0052, valid accuracy: 0.3469\n",
      "Iter-8680 train loss: 2.0548 valid loss: 2.0052, valid accuracy: 0.3477\n",
      "Iter-8690 train loss: 1.7664 valid loss: 2.0053, valid accuracy: 0.3485\n",
      "Iter-8700 train loss: 1.8447 valid loss: 2.0044, valid accuracy: 0.3477\n",
      "Iter-8710 train loss: 2.1774 valid loss: 2.0048, valid accuracy: 0.3469\n",
      "Iter-8720 train loss: 2.0559 valid loss: 2.0046, valid accuracy: 0.3477\n",
      "Iter-8730 train loss: 1.9569 valid loss: 2.0043, valid accuracy: 0.3469\n",
      "Iter-8740 train loss: 1.9294 valid loss: 2.0045, valid accuracy: 0.3438\n",
      "Iter-8750 train loss: 1.9514 valid loss: 2.0047, valid accuracy: 0.3454\n",
      "Iter-8760 train loss: 1.7311 valid loss: 2.0050, valid accuracy: 0.3423\n",
      "Iter-8770 train loss: 1.8275 valid loss: 2.0051, valid accuracy: 0.3415\n",
      "Iter-8780 train loss: 1.9302 valid loss: 2.0058, valid accuracy: 0.3438\n",
      "Iter-8790 train loss: 1.9897 valid loss: 2.0060, valid accuracy: 0.3469\n",
      "Iter-8800 train loss: 1.8584 valid loss: 2.0061, valid accuracy: 0.3454\n",
      "Iter-8810 train loss: 1.5630 valid loss: 2.0063, valid accuracy: 0.3431\n",
      "Iter-8820 train loss: 1.7916 valid loss: 2.0062, valid accuracy: 0.3469\n",
      "Iter-8830 train loss: 2.0766 valid loss: 2.0057, valid accuracy: 0.3477\n",
      "Iter-8840 train loss: 1.7296 valid loss: 2.0060, valid accuracy: 0.3469\n",
      "Iter-8850 train loss: 2.0134 valid loss: 2.0053, valid accuracy: 0.3423\n",
      "Iter-8860 train loss: 1.7845 valid loss: 2.0053, valid accuracy: 0.3400\n",
      "Iter-8870 train loss: 1.9529 valid loss: 2.0057, valid accuracy: 0.3392\n",
      "Iter-8880 train loss: 1.8621 valid loss: 2.0057, valid accuracy: 0.3400\n",
      "Iter-8890 train loss: 1.8616 valid loss: 2.0062, valid accuracy: 0.3400\n",
      "Iter-8900 train loss: 2.0313 valid loss: 2.0064, valid accuracy: 0.3385\n",
      "Iter-8910 train loss: 1.9779 valid loss: 2.0056, valid accuracy: 0.3408\n",
      "Iter-8920 train loss: 1.9117 valid loss: 2.0048, valid accuracy: 0.3431\n",
      "Iter-8930 train loss: 1.9346 valid loss: 2.0054, valid accuracy: 0.3469\n",
      "Iter-8940 train loss: 1.7528 valid loss: 2.0048, valid accuracy: 0.3462\n",
      "Iter-8950 train loss: 2.1221 valid loss: 2.0050, valid accuracy: 0.3462\n",
      "Iter-8960 train loss: 2.0137 valid loss: 2.0066, valid accuracy: 0.3431\n",
      "Iter-8970 train loss: 1.8432 valid loss: 2.0064, valid accuracy: 0.3431\n",
      "Iter-8980 train loss: 1.9150 valid loss: 2.0068, valid accuracy: 0.3500\n",
      "Iter-8990 train loss: 1.9104 valid loss: 2.0067, valid accuracy: 0.3500\n",
      "Iter-9000 train loss: 1.9390 valid loss: 2.0065, valid accuracy: 0.3438\n",
      "Iter-9010 train loss: 2.0289 valid loss: 2.0078, valid accuracy: 0.3423\n",
      "Iter-9020 train loss: 1.8553 valid loss: 2.0079, valid accuracy: 0.3454\n",
      "Iter-9030 train loss: 1.9323 valid loss: 2.0075, valid accuracy: 0.3454\n",
      "Iter-9040 train loss: 1.8773 valid loss: 2.0076, valid accuracy: 0.3477\n",
      "Iter-9050 train loss: 2.0027 valid loss: 2.0070, valid accuracy: 0.3462\n",
      "Iter-9060 train loss: 1.8353 valid loss: 2.0066, valid accuracy: 0.3462\n",
      "Iter-9070 train loss: 1.6058 valid loss: 2.0050, valid accuracy: 0.3469\n",
      "Iter-9080 train loss: 2.1055 valid loss: 2.0037, valid accuracy: 0.3523\n",
      "Iter-9090 train loss: 1.7641 valid loss: 2.0036, valid accuracy: 0.3485\n",
      "Iter-9100 train loss: 2.0748 valid loss: 2.0033, valid accuracy: 0.3485\n",
      "Iter-9110 train loss: 2.0083 valid loss: 2.0036, valid accuracy: 0.3446\n",
      "Iter-9120 train loss: 2.0082 valid loss: 2.0033, valid accuracy: 0.3392\n",
      "Iter-9130 train loss: 2.0498 valid loss: 2.0040, valid accuracy: 0.3408\n",
      "Iter-9140 train loss: 2.0194 valid loss: 2.0037, valid accuracy: 0.3438\n",
      "Iter-9150 train loss: 1.8336 valid loss: 2.0048, valid accuracy: 0.3385\n",
      "Iter-9160 train loss: 1.8763 valid loss: 2.0056, valid accuracy: 0.3415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9170 train loss: 1.7480 valid loss: 2.0041, valid accuracy: 0.3385\n",
      "Iter-9180 train loss: 1.9180 valid loss: 2.0038, valid accuracy: 0.3400\n",
      "Iter-9190 train loss: 1.8599 valid loss: 2.0031, valid accuracy: 0.3454\n",
      "Iter-9200 train loss: 2.0047 valid loss: 2.0051, valid accuracy: 0.3385\n",
      "Iter-9210 train loss: 1.8112 valid loss: 2.0031, valid accuracy: 0.3400\n",
      "Iter-9220 train loss: 1.8373 valid loss: 2.0032, valid accuracy: 0.3423\n",
      "Iter-9230 train loss: 1.9664 valid loss: 2.0032, valid accuracy: 0.3408\n",
      "Iter-9240 train loss: 1.9284 valid loss: 2.0029, valid accuracy: 0.3431\n",
      "Iter-9250 train loss: 1.9406 valid loss: 2.0041, valid accuracy: 0.3377\n",
      "Iter-9260 train loss: 1.9889 valid loss: 2.0038, valid accuracy: 0.3377\n",
      "Iter-9270 train loss: 2.0062 valid loss: 2.0037, valid accuracy: 0.3354\n",
      "Iter-9280 train loss: 2.0816 valid loss: 2.0038, valid accuracy: 0.3392\n",
      "Iter-9290 train loss: 1.9870 valid loss: 2.0034, valid accuracy: 0.3423\n",
      "Iter-9300 train loss: 1.6573 valid loss: 2.0045, valid accuracy: 0.3408\n",
      "Iter-9310 train loss: 2.0021 valid loss: 2.0039, valid accuracy: 0.3377\n",
      "Iter-9320 train loss: 1.8111 valid loss: 2.0047, valid accuracy: 0.3385\n",
      "Iter-9330 train loss: 1.7824 valid loss: 2.0044, valid accuracy: 0.3431\n",
      "Iter-9340 train loss: 1.9668 valid loss: 2.0047, valid accuracy: 0.3431\n",
      "Iter-9350 train loss: 1.7702 valid loss: 2.0049, valid accuracy: 0.3469\n",
      "Iter-9360 train loss: 2.1247 valid loss: 2.0039, valid accuracy: 0.3454\n",
      "Iter-9370 train loss: 2.0426 valid loss: 2.0046, valid accuracy: 0.3492\n",
      "Iter-9380 train loss: 1.6014 valid loss: 2.0040, valid accuracy: 0.3500\n",
      "Iter-9390 train loss: 2.0496 valid loss: 2.0048, valid accuracy: 0.3438\n",
      "Iter-9400 train loss: 2.0630 valid loss: 2.0063, valid accuracy: 0.3438\n",
      "Iter-9410 train loss: 2.1079 valid loss: 2.0063, valid accuracy: 0.3415\n",
      "Iter-9420 train loss: 1.9236 valid loss: 2.0054, valid accuracy: 0.3415\n",
      "Iter-9430 train loss: 1.7378 valid loss: 2.0057, valid accuracy: 0.3431\n",
      "Iter-9440 train loss: 1.8585 valid loss: 2.0045, valid accuracy: 0.3423\n",
      "Iter-9450 train loss: 1.9864 valid loss: 2.0047, valid accuracy: 0.3423\n",
      "Iter-9460 train loss: 2.0054 valid loss: 2.0049, valid accuracy: 0.3446\n",
      "Iter-9470 train loss: 1.7858 valid loss: 2.0067, valid accuracy: 0.3431\n",
      "Iter-9480 train loss: 2.1120 valid loss: 2.0057, valid accuracy: 0.3400\n",
      "Iter-9490 train loss: 1.9723 valid loss: 2.0042, valid accuracy: 0.3431\n",
      "Iter-9500 train loss: 1.7926 valid loss: 2.0034, valid accuracy: 0.3423\n",
      "Iter-9510 train loss: 1.7331 valid loss: 2.0031, valid accuracy: 0.3477\n",
      "Iter-9520 train loss: 1.9122 valid loss: 2.0027, valid accuracy: 0.3446\n",
      "Iter-9530 train loss: 1.8246 valid loss: 2.0036, valid accuracy: 0.3500\n",
      "Iter-9540 train loss: 1.7259 valid loss: 2.0034, valid accuracy: 0.3508\n",
      "Iter-9550 train loss: 1.7062 valid loss: 2.0022, valid accuracy: 0.3538\n",
      "Iter-9560 train loss: 1.7211 valid loss: 2.0013, valid accuracy: 0.3492\n",
      "Iter-9570 train loss: 1.8397 valid loss: 2.0010, valid accuracy: 0.3538\n",
      "Iter-9580 train loss: 2.0196 valid loss: 2.0008, valid accuracy: 0.3469\n",
      "Iter-9590 train loss: 1.9016 valid loss: 2.0010, valid accuracy: 0.3508\n",
      "Iter-9600 train loss: 1.9091 valid loss: 2.0009, valid accuracy: 0.3477\n",
      "Iter-9610 train loss: 1.9658 valid loss: 2.0005, valid accuracy: 0.3500\n",
      "Iter-9620 train loss: 1.8141 valid loss: 2.0005, valid accuracy: 0.3454\n",
      "Iter-9630 train loss: 1.7820 valid loss: 2.0014, valid accuracy: 0.3477\n",
      "Iter-9640 train loss: 1.6418 valid loss: 2.0002, valid accuracy: 0.3500\n",
      "Iter-9650 train loss: 1.6250 valid loss: 2.0003, valid accuracy: 0.3469\n",
      "Iter-9660 train loss: 1.9298 valid loss: 2.0006, valid accuracy: 0.3500\n",
      "Iter-9670 train loss: 1.7593 valid loss: 1.9998, valid accuracy: 0.3477\n",
      "Iter-9680 train loss: 1.7552 valid loss: 1.9999, valid accuracy: 0.3485\n",
      "Iter-9690 train loss: 1.8196 valid loss: 2.0005, valid accuracy: 0.3462\n",
      "Iter-9700 train loss: 1.7343 valid loss: 2.0009, valid accuracy: 0.3469\n",
      "Iter-9710 train loss: 2.0264 valid loss: 2.0011, valid accuracy: 0.3523\n",
      "Iter-9720 train loss: 1.8716 valid loss: 2.0011, valid accuracy: 0.3500\n",
      "Iter-9730 train loss: 1.8821 valid loss: 2.0014, valid accuracy: 0.3469\n",
      "Iter-9740 train loss: 1.9301 valid loss: 2.0013, valid accuracy: 0.3469\n",
      "Iter-9750 train loss: 1.9735 valid loss: 2.0006, valid accuracy: 0.3462\n",
      "Iter-9760 train loss: 1.7757 valid loss: 2.0005, valid accuracy: 0.3431\n",
      "Iter-9770 train loss: 1.8097 valid loss: 2.0000, valid accuracy: 0.3408\n",
      "Iter-9780 train loss: 2.0542 valid loss: 1.9991, valid accuracy: 0.3438\n",
      "Iter-9790 train loss: 1.9491 valid loss: 1.9983, valid accuracy: 0.3431\n",
      "Iter-9800 train loss: 1.7902 valid loss: 1.9994, valid accuracy: 0.3431\n",
      "Iter-9810 train loss: 1.6339 valid loss: 1.9995, valid accuracy: 0.3446\n",
      "Iter-9820 train loss: 1.9549 valid loss: 2.0002, valid accuracy: 0.3477\n",
      "Iter-9830 train loss: 1.9941 valid loss: 1.9991, valid accuracy: 0.3477\n",
      "Iter-9840 train loss: 1.7612 valid loss: 1.9992, valid accuracy: 0.3485\n",
      "Iter-9850 train loss: 2.0105 valid loss: 1.9990, valid accuracy: 0.3462\n",
      "Iter-9860 train loss: 2.0697 valid loss: 1.9997, valid accuracy: 0.3454\n",
      "Iter-9870 train loss: 1.8974 valid loss: 1.9999, valid accuracy: 0.3431\n",
      "Iter-9880 train loss: 1.9559 valid loss: 1.9995, valid accuracy: 0.3438\n",
      "Iter-9890 train loss: 1.8076 valid loss: 1.9992, valid accuracy: 0.3446\n",
      "Iter-9900 train loss: 1.7052 valid loss: 1.9995, valid accuracy: 0.3431\n",
      "Iter-9910 train loss: 1.8233 valid loss: 1.9989, valid accuracy: 0.3454\n",
      "Iter-9920 train loss: 1.7117 valid loss: 1.9981, valid accuracy: 0.3438\n",
      "Iter-9930 train loss: 1.8008 valid loss: 1.9982, valid accuracy: 0.3446\n",
      "Iter-9940 train loss: 2.0941 valid loss: 1.9978, valid accuracy: 0.3462\n",
      "Iter-9950 train loss: 1.8341 valid loss: 1.9982, valid accuracy: 0.3469\n",
      "Iter-9960 train loss: 1.9864 valid loss: 1.9995, valid accuracy: 0.3469\n",
      "Iter-9970 train loss: 1.7705 valid loss: 1.9992, valid accuracy: 0.3469\n",
      "Iter-9980 train loss: 1.9361 valid loss: 2.0003, valid accuracy: 0.3523\n",
      "Iter-9990 train loss: 1.8025 valid loss: 1.9990, valid accuracy: 0.3485\n",
      "Iter-10000 train loss: 1.9027 valid loss: 2.0002, valid accuracy: 0.3508\n",
      "Iter-10010 train loss: 1.7620 valid loss: 2.0003, valid accuracy: 0.3523\n",
      "Iter-10020 train loss: 2.1409 valid loss: 2.0000, valid accuracy: 0.3515\n",
      "Iter-10030 train loss: 1.7993 valid loss: 1.9992, valid accuracy: 0.3500\n",
      "Iter-10040 train loss: 1.8876 valid loss: 1.9989, valid accuracy: 0.3515\n",
      "Iter-10050 train loss: 1.8071 valid loss: 1.9993, valid accuracy: 0.3477\n",
      "Iter-10060 train loss: 1.9555 valid loss: 2.0005, valid accuracy: 0.3469\n",
      "Iter-10070 train loss: 1.6566 valid loss: 2.0005, valid accuracy: 0.3485\n",
      "Iter-10080 train loss: 2.0756 valid loss: 2.0009, valid accuracy: 0.3446\n",
      "Iter-10090 train loss: 1.8729 valid loss: 1.9997, valid accuracy: 0.3477\n",
      "Iter-10100 train loss: 1.7540 valid loss: 2.0005, valid accuracy: 0.3446\n",
      "Iter-10110 train loss: 2.0890 valid loss: 2.0002, valid accuracy: 0.3531\n",
      "Iter-10120 train loss: 2.1856 valid loss: 1.9995, valid accuracy: 0.3508\n",
      "Iter-10130 train loss: 1.7686 valid loss: 1.9997, valid accuracy: 0.3492\n",
      "Iter-10140 train loss: 1.9086 valid loss: 1.9993, valid accuracy: 0.3469\n",
      "Iter-10150 train loss: 1.9299 valid loss: 2.0004, valid accuracy: 0.3431\n",
      "Iter-10160 train loss: 1.9966 valid loss: 1.9997, valid accuracy: 0.3454\n",
      "Iter-10170 train loss: 2.0460 valid loss: 2.0002, valid accuracy: 0.3469\n",
      "Iter-10180 train loss: 1.9459 valid loss: 1.9981, valid accuracy: 0.3446\n",
      "Iter-10190 train loss: 1.8219 valid loss: 1.9984, valid accuracy: 0.3431\n",
      "Iter-10200 train loss: 1.9701 valid loss: 1.9983, valid accuracy: 0.3454\n",
      "Iter-10210 train loss: 1.7946 valid loss: 1.9986, valid accuracy: 0.3446\n",
      "Iter-10220 train loss: 2.1254 valid loss: 1.9975, valid accuracy: 0.3431\n",
      "Iter-10230 train loss: 2.0928 valid loss: 1.9983, valid accuracy: 0.3446\n",
      "Iter-10240 train loss: 1.8055 valid loss: 1.9988, valid accuracy: 0.3454\n",
      "Iter-10250 train loss: 1.7623 valid loss: 1.9986, valid accuracy: 0.3477\n",
      "Iter-10260 train loss: 1.8302 valid loss: 1.9988, valid accuracy: 0.3477\n",
      "Iter-10270 train loss: 1.7050 valid loss: 1.9985, valid accuracy: 0.3423\n",
      "Iter-10280 train loss: 2.0039 valid loss: 1.9978, valid accuracy: 0.3477\n",
      "Iter-10290 train loss: 1.8798 valid loss: 1.9973, valid accuracy: 0.3469\n",
      "Iter-10300 train loss: 1.9924 valid loss: 1.9963, valid accuracy: 0.3492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10310 train loss: 1.6475 valid loss: 1.9960, valid accuracy: 0.3500\n",
      "Iter-10320 train loss: 1.7372 valid loss: 1.9966, valid accuracy: 0.3500\n",
      "Iter-10330 train loss: 2.1034 valid loss: 1.9952, valid accuracy: 0.3523\n",
      "Iter-10340 train loss: 1.8006 valid loss: 1.9948, valid accuracy: 0.3508\n",
      "Iter-10350 train loss: 1.9527 valid loss: 1.9951, valid accuracy: 0.3469\n",
      "Iter-10360 train loss: 1.9510 valid loss: 1.9949, valid accuracy: 0.3492\n",
      "Iter-10370 train loss: 1.9289 valid loss: 1.9947, valid accuracy: 0.3477\n",
      "Iter-10380 train loss: 1.8209 valid loss: 1.9956, valid accuracy: 0.3462\n",
      "Iter-10390 train loss: 1.9101 valid loss: 1.9958, valid accuracy: 0.3446\n",
      "Iter-10400 train loss: 1.9828 valid loss: 1.9975, valid accuracy: 0.3477\n",
      "Iter-10410 train loss: 1.9751 valid loss: 1.9975, valid accuracy: 0.3454\n",
      "Iter-10420 train loss: 1.7951 valid loss: 1.9979, valid accuracy: 0.3469\n",
      "Iter-10430 train loss: 2.0216 valid loss: 1.9990, valid accuracy: 0.3485\n",
      "Iter-10440 train loss: 1.9792 valid loss: 1.9995, valid accuracy: 0.3500\n",
      "Iter-10450 train loss: 1.9838 valid loss: 1.9983, valid accuracy: 0.3531\n",
      "Iter-10460 train loss: 1.9078 valid loss: 1.9971, valid accuracy: 0.3523\n",
      "Iter-10470 train loss: 1.7894 valid loss: 1.9975, valid accuracy: 0.3538\n",
      "Iter-10480 train loss: 1.9753 valid loss: 1.9967, valid accuracy: 0.3485\n",
      "Iter-10490 train loss: 1.8166 valid loss: 1.9974, valid accuracy: 0.3508\n",
      "Iter-10500 train loss: 1.7775 valid loss: 1.9973, valid accuracy: 0.3531\n",
      "Iter-10510 train loss: 1.8970 valid loss: 1.9962, valid accuracy: 0.3508\n",
      "Iter-10520 train loss: 1.7196 valid loss: 1.9959, valid accuracy: 0.3508\n",
      "Iter-10530 train loss: 1.7109 valid loss: 1.9959, valid accuracy: 0.3500\n",
      "Iter-10540 train loss: 2.0815 valid loss: 1.9958, valid accuracy: 0.3508\n",
      "Iter-10550 train loss: 1.9688 valid loss: 1.9966, valid accuracy: 0.3508\n",
      "Iter-10560 train loss: 1.9382 valid loss: 1.9978, valid accuracy: 0.3469\n",
      "Iter-10570 train loss: 1.8135 valid loss: 1.9970, valid accuracy: 0.3515\n",
      "Iter-10580 train loss: 1.6737 valid loss: 1.9971, valid accuracy: 0.3508\n",
      "Iter-10590 train loss: 2.0299 valid loss: 1.9961, valid accuracy: 0.3508\n",
      "Iter-10600 train loss: 1.8570 valid loss: 1.9962, valid accuracy: 0.3546\n",
      "Iter-10610 train loss: 1.7111 valid loss: 1.9972, valid accuracy: 0.3515\n",
      "Iter-10620 train loss: 1.9495 valid loss: 1.9976, valid accuracy: 0.3485\n",
      "Iter-10630 train loss: 1.7005 valid loss: 1.9962, valid accuracy: 0.3477\n",
      "Iter-10640 train loss: 1.8716 valid loss: 1.9960, valid accuracy: 0.3492\n",
      "Iter-10650 train loss: 1.9728 valid loss: 1.9955, valid accuracy: 0.3523\n",
      "Iter-10660 train loss: 2.1417 valid loss: 1.9944, valid accuracy: 0.3508\n",
      "Iter-10670 train loss: 1.9737 valid loss: 1.9939, valid accuracy: 0.3569\n",
      "Iter-10680 train loss: 2.0009 valid loss: 1.9951, valid accuracy: 0.3546\n",
      "Iter-10690 train loss: 1.9445 valid loss: 1.9951, valid accuracy: 0.3492\n",
      "Iter-10700 train loss: 1.7710 valid loss: 1.9949, valid accuracy: 0.3485\n",
      "Iter-10710 train loss: 1.7585 valid loss: 1.9962, valid accuracy: 0.3500\n",
      "Iter-10720 train loss: 1.6846 valid loss: 1.9946, valid accuracy: 0.3546\n",
      "Iter-10730 train loss: 1.9810 valid loss: 1.9961, valid accuracy: 0.3508\n",
      "Iter-10740 train loss: 1.7337 valid loss: 1.9955, valid accuracy: 0.3508\n",
      "Iter-10750 train loss: 1.9503 valid loss: 1.9961, valid accuracy: 0.3523\n",
      "Iter-10760 train loss: 1.7433 valid loss: 1.9965, valid accuracy: 0.3515\n",
      "Iter-10770 train loss: 1.9091 valid loss: 1.9973, valid accuracy: 0.3492\n",
      "Iter-10780 train loss: 1.8135 valid loss: 1.9983, valid accuracy: 0.3454\n",
      "Iter-10790 train loss: 1.8561 valid loss: 1.9976, valid accuracy: 0.3462\n",
      "Iter-10800 train loss: 1.8655 valid loss: 1.9975, valid accuracy: 0.3462\n",
      "Iter-10810 train loss: 2.0885 valid loss: 1.9971, valid accuracy: 0.3454\n",
      "Iter-10820 train loss: 1.8370 valid loss: 1.9977, valid accuracy: 0.3446\n",
      "Iter-10830 train loss: 1.6335 valid loss: 1.9967, valid accuracy: 0.3500\n",
      "Iter-10840 train loss: 1.8218 valid loss: 1.9966, valid accuracy: 0.3515\n",
      "Iter-10850 train loss: 2.2130 valid loss: 1.9955, valid accuracy: 0.3469\n",
      "Iter-10860 train loss: 2.4728 valid loss: 1.9957, valid accuracy: 0.3485\n",
      "Iter-10870 train loss: 1.7707 valid loss: 1.9949, valid accuracy: 0.3492\n",
      "Iter-10880 train loss: 1.7964 valid loss: 1.9949, valid accuracy: 0.3477\n",
      "Iter-10890 train loss: 1.7330 valid loss: 1.9945, valid accuracy: 0.3485\n",
      "Iter-10900 train loss: 1.6803 valid loss: 1.9959, valid accuracy: 0.3523\n",
      "Iter-10910 train loss: 1.9802 valid loss: 1.9966, valid accuracy: 0.3508\n",
      "Iter-10920 train loss: 1.9401 valid loss: 1.9957, valid accuracy: 0.3462\n",
      "Iter-10930 train loss: 2.0262 valid loss: 1.9946, valid accuracy: 0.3523\n",
      "Iter-10940 train loss: 1.5527 valid loss: 1.9954, valid accuracy: 0.3500\n",
      "Iter-10950 train loss: 1.9756 valid loss: 1.9957, valid accuracy: 0.3531\n",
      "Iter-10960 train loss: 1.8414 valid loss: 1.9957, valid accuracy: 0.3485\n",
      "Iter-10970 train loss: 1.9591 valid loss: 1.9967, valid accuracy: 0.3515\n",
      "Iter-10980 train loss: 2.1573 valid loss: 1.9966, valid accuracy: 0.3492\n",
      "Iter-10990 train loss: 1.9200 valid loss: 1.9951, valid accuracy: 0.3508\n",
      "Iter-11000 train loss: 1.9494 valid loss: 1.9956, valid accuracy: 0.3492\n",
      "Iter-11010 train loss: 1.7862 valid loss: 1.9953, valid accuracy: 0.3485\n",
      "Iter-11020 train loss: 1.9116 valid loss: 1.9961, valid accuracy: 0.3531\n",
      "Iter-11030 train loss: 1.8659 valid loss: 1.9954, valid accuracy: 0.3492\n",
      "Iter-11040 train loss: 1.9903 valid loss: 1.9950, valid accuracy: 0.3469\n",
      "Iter-11050 train loss: 1.9619 valid loss: 1.9957, valid accuracy: 0.3485\n",
      "Iter-11060 train loss: 2.1493 valid loss: 1.9951, valid accuracy: 0.3462\n",
      "Iter-11070 train loss: 2.1200 valid loss: 1.9946, valid accuracy: 0.3462\n",
      "Iter-11080 train loss: 2.0477 valid loss: 1.9942, valid accuracy: 0.3508\n",
      "Iter-11090 train loss: 1.9147 valid loss: 1.9956, valid accuracy: 0.3492\n",
      "Iter-11100 train loss: 1.7504 valid loss: 1.9951, valid accuracy: 0.3508\n",
      "Iter-11110 train loss: 1.6703 valid loss: 1.9949, valid accuracy: 0.3523\n",
      "Iter-11120 train loss: 1.7903 valid loss: 1.9940, valid accuracy: 0.3492\n",
      "Iter-11130 train loss: 1.8084 valid loss: 1.9939, valid accuracy: 0.3492\n",
      "Iter-11140 train loss: 1.8403 valid loss: 1.9932, valid accuracy: 0.3546\n",
      "Iter-11150 train loss: 1.9345 valid loss: 1.9924, valid accuracy: 0.3554\n",
      "Iter-11160 train loss: 1.9759 valid loss: 1.9930, valid accuracy: 0.3562\n",
      "Iter-11170 train loss: 1.7735 valid loss: 1.9928, valid accuracy: 0.3546\n",
      "Iter-11180 train loss: 1.6669 valid loss: 1.9933, valid accuracy: 0.3562\n",
      "Iter-11190 train loss: 2.2041 valid loss: 1.9940, valid accuracy: 0.3554\n",
      "Iter-11200 train loss: 1.7618 valid loss: 1.9953, valid accuracy: 0.3515\n",
      "Iter-11210 train loss: 1.8885 valid loss: 1.9948, valid accuracy: 0.3531\n",
      "Iter-11220 train loss: 1.9511 valid loss: 1.9947, valid accuracy: 0.3492\n",
      "Iter-11230 train loss: 1.9663 valid loss: 1.9954, valid accuracy: 0.3477\n",
      "Iter-11240 train loss: 1.6790 valid loss: 1.9951, valid accuracy: 0.3477\n",
      "Iter-11250 train loss: 2.0296 valid loss: 1.9955, valid accuracy: 0.3485\n",
      "Iter-11260 train loss: 1.7355 valid loss: 1.9959, valid accuracy: 0.3485\n",
      "Iter-11270 train loss: 1.7174 valid loss: 1.9954, valid accuracy: 0.3462\n",
      "Iter-11280 train loss: 1.8376 valid loss: 1.9952, valid accuracy: 0.3500\n",
      "Iter-11290 train loss: 1.8885 valid loss: 1.9950, valid accuracy: 0.3492\n",
      "Iter-11300 train loss: 1.6025 valid loss: 1.9946, valid accuracy: 0.3485\n",
      "Iter-11310 train loss: 1.7796 valid loss: 1.9935, valid accuracy: 0.3462\n",
      "Iter-11320 train loss: 1.9407 valid loss: 1.9948, valid accuracy: 0.3469\n",
      "Iter-11330 train loss: 1.8376 valid loss: 1.9955, valid accuracy: 0.3492\n",
      "Iter-11340 train loss: 2.0645 valid loss: 1.9952, valid accuracy: 0.3454\n",
      "Iter-11350 train loss: 1.9056 valid loss: 1.9952, valid accuracy: 0.3446\n",
      "Iter-11360 train loss: 1.8904 valid loss: 1.9956, valid accuracy: 0.3492\n",
      "Iter-11370 train loss: 1.9362 valid loss: 1.9948, valid accuracy: 0.3485\n",
      "Iter-11380 train loss: 1.8613 valid loss: 1.9956, valid accuracy: 0.3485\n",
      "Iter-11390 train loss: 1.8520 valid loss: 1.9956, valid accuracy: 0.3500\n",
      "Iter-11400 train loss: 1.8534 valid loss: 1.9952, valid accuracy: 0.3523\n",
      "Iter-11410 train loss: 2.0751 valid loss: 1.9938, valid accuracy: 0.3531\n",
      "Iter-11420 train loss: 2.0099 valid loss: 1.9927, valid accuracy: 0.3469\n",
      "Iter-11430 train loss: 1.7896 valid loss: 1.9931, valid accuracy: 0.3492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11440 train loss: 1.9166 valid loss: 1.9935, valid accuracy: 0.3538\n",
      "Iter-11450 train loss: 1.9053 valid loss: 1.9942, valid accuracy: 0.3485\n",
      "Iter-11460 train loss: 2.0700 valid loss: 1.9941, valid accuracy: 0.3438\n",
      "Iter-11470 train loss: 1.8111 valid loss: 1.9950, valid accuracy: 0.3400\n",
      "Iter-11480 train loss: 1.7362 valid loss: 1.9947, valid accuracy: 0.3431\n",
      "Iter-11490 train loss: 1.8496 valid loss: 1.9951, valid accuracy: 0.3400\n",
      "Iter-11500 train loss: 1.6847 valid loss: 1.9940, valid accuracy: 0.3392\n",
      "Iter-11510 train loss: 1.5912 valid loss: 1.9946, valid accuracy: 0.3454\n",
      "Iter-11520 train loss: 1.8684 valid loss: 1.9938, valid accuracy: 0.3438\n",
      "Iter-11530 train loss: 1.7723 valid loss: 1.9937, valid accuracy: 0.3423\n",
      "Iter-11540 train loss: 1.9566 valid loss: 1.9935, valid accuracy: 0.3431\n",
      "Iter-11550 train loss: 1.7993 valid loss: 1.9934, valid accuracy: 0.3438\n",
      "Iter-11560 train loss: 1.8065 valid loss: 1.9926, valid accuracy: 0.3415\n",
      "Iter-11570 train loss: 1.7662 valid loss: 1.9918, valid accuracy: 0.3446\n",
      "Iter-11580 train loss: 1.9229 valid loss: 1.9910, valid accuracy: 0.3415\n",
      "Iter-11590 train loss: 2.0167 valid loss: 1.9925, valid accuracy: 0.3369\n",
      "Iter-11600 train loss: 2.0223 valid loss: 1.9939, valid accuracy: 0.3408\n",
      "Iter-11610 train loss: 2.1332 valid loss: 1.9941, valid accuracy: 0.3423\n",
      "Iter-11620 train loss: 1.7620 valid loss: 1.9933, valid accuracy: 0.3415\n",
      "Iter-11630 train loss: 1.5903 valid loss: 1.9926, valid accuracy: 0.3408\n",
      "Iter-11640 train loss: 1.9174 valid loss: 1.9922, valid accuracy: 0.3431\n",
      "Iter-11650 train loss: 1.9449 valid loss: 1.9925, valid accuracy: 0.3385\n",
      "Iter-11660 train loss: 1.8365 valid loss: 1.9926, valid accuracy: 0.3431\n",
      "Iter-11670 train loss: 1.9425 valid loss: 1.9917, valid accuracy: 0.3400\n",
      "Iter-11680 train loss: 1.8451 valid loss: 1.9915, valid accuracy: 0.3408\n",
      "Iter-11690 train loss: 1.6955 valid loss: 1.9919, valid accuracy: 0.3415\n",
      "Iter-11700 train loss: 1.8969 valid loss: 1.9925, valid accuracy: 0.3392\n",
      "Iter-11710 train loss: 1.9313 valid loss: 1.9933, valid accuracy: 0.3415\n",
      "Iter-11720 train loss: 1.9080 valid loss: 1.9931, valid accuracy: 0.3392\n",
      "Iter-11730 train loss: 1.8362 valid loss: 1.9934, valid accuracy: 0.3415\n",
      "Iter-11740 train loss: 1.9401 valid loss: 1.9946, valid accuracy: 0.3415\n",
      "Iter-11750 train loss: 2.0765 valid loss: 1.9940, valid accuracy: 0.3408\n",
      "Iter-11760 train loss: 1.9535 valid loss: 1.9945, valid accuracy: 0.3408\n",
      "Iter-11770 train loss: 1.7465 valid loss: 1.9938, valid accuracy: 0.3454\n",
      "Iter-11780 train loss: 1.7744 valid loss: 1.9939, valid accuracy: 0.3446\n",
      "Iter-11790 train loss: 1.7009 valid loss: 1.9933, valid accuracy: 0.3454\n",
      "Iter-11800 train loss: 1.7902 valid loss: 1.9936, valid accuracy: 0.3408\n",
      "Iter-11810 train loss: 2.0228 valid loss: 1.9930, valid accuracy: 0.3423\n",
      "Iter-11820 train loss: 1.9104 valid loss: 1.9937, valid accuracy: 0.3423\n",
      "Iter-11830 train loss: 1.7859 valid loss: 1.9933, valid accuracy: 0.3408\n",
      "Iter-11840 train loss: 2.0678 valid loss: 1.9918, valid accuracy: 0.3392\n",
      "Iter-11850 train loss: 1.6556 valid loss: 1.9915, valid accuracy: 0.3385\n",
      "Iter-11860 train loss: 1.9576 valid loss: 1.9906, valid accuracy: 0.3400\n",
      "Iter-11870 train loss: 1.8917 valid loss: 1.9900, valid accuracy: 0.3423\n",
      "Iter-11880 train loss: 1.7314 valid loss: 1.9900, valid accuracy: 0.3431\n",
      "Iter-11890 train loss: 1.7761 valid loss: 1.9902, valid accuracy: 0.3415\n",
      "Iter-11900 train loss: 1.8035 valid loss: 1.9904, valid accuracy: 0.3431\n",
      "Iter-11910 train loss: 2.0477 valid loss: 1.9910, valid accuracy: 0.3454\n",
      "Iter-11920 train loss: 1.9133 valid loss: 1.9910, valid accuracy: 0.3431\n",
      "Iter-11930 train loss: 1.8623 valid loss: 1.9907, valid accuracy: 0.3454\n",
      "Iter-11940 train loss: 1.6433 valid loss: 1.9912, valid accuracy: 0.3462\n",
      "Iter-11950 train loss: 1.9789 valid loss: 1.9907, valid accuracy: 0.3454\n",
      "Iter-11960 train loss: 2.1319 valid loss: 1.9906, valid accuracy: 0.3446\n",
      "Iter-11970 train loss: 1.7520 valid loss: 1.9913, valid accuracy: 0.3454\n",
      "Iter-11980 train loss: 1.9068 valid loss: 1.9918, valid accuracy: 0.3477\n",
      "Iter-11990 train loss: 1.9543 valid loss: 1.9928, valid accuracy: 0.3454\n",
      "Iter-12000 train loss: 1.9885 valid loss: 1.9936, valid accuracy: 0.3408\n",
      "Iter-12010 train loss: 1.9324 valid loss: 1.9929, valid accuracy: 0.3423\n",
      "Iter-12020 train loss: 1.9492 valid loss: 1.9911, valid accuracy: 0.3408\n",
      "Iter-12030 train loss: 1.7217 valid loss: 1.9906, valid accuracy: 0.3454\n",
      "Iter-12040 train loss: 1.7040 valid loss: 1.9903, valid accuracy: 0.3438\n",
      "Iter-12050 train loss: 1.8413 valid loss: 1.9905, valid accuracy: 0.3392\n",
      "Iter-12060 train loss: 1.8289 valid loss: 1.9897, valid accuracy: 0.3408\n",
      "Iter-12070 train loss: 1.9330 valid loss: 1.9899, valid accuracy: 0.3400\n",
      "Iter-12080 train loss: 1.8989 valid loss: 1.9900, valid accuracy: 0.3454\n",
      "Iter-12090 train loss: 1.8965 valid loss: 1.9895, valid accuracy: 0.3477\n",
      "Iter-12100 train loss: 1.9913 valid loss: 1.9893, valid accuracy: 0.3454\n",
      "Iter-12110 train loss: 1.6976 valid loss: 1.9896, valid accuracy: 0.3469\n",
      "Iter-12120 train loss: 1.9787 valid loss: 1.9892, valid accuracy: 0.3485\n",
      "Iter-12130 train loss: 1.6496 valid loss: 1.9886, valid accuracy: 0.3477\n",
      "Iter-12140 train loss: 2.1003 valid loss: 1.9880, valid accuracy: 0.3446\n",
      "Iter-12150 train loss: 2.0170 valid loss: 1.9882, valid accuracy: 0.3454\n",
      "Iter-12160 train loss: 1.8779 valid loss: 1.9889, valid accuracy: 0.3508\n",
      "Iter-12170 train loss: 1.8157 valid loss: 1.9888, valid accuracy: 0.3523\n",
      "Iter-12180 train loss: 1.7504 valid loss: 1.9885, valid accuracy: 0.3492\n",
      "Iter-12190 train loss: 1.9735 valid loss: 1.9887, valid accuracy: 0.3462\n",
      "Iter-12200 train loss: 1.8501 valid loss: 1.9886, valid accuracy: 0.3431\n",
      "Iter-12210 train loss: 1.7830 valid loss: 1.9892, valid accuracy: 0.3400\n",
      "Iter-12220 train loss: 1.6554 valid loss: 1.9911, valid accuracy: 0.3354\n",
      "Iter-12230 train loss: 1.9488 valid loss: 1.9905, valid accuracy: 0.3369\n",
      "Iter-12240 train loss: 2.0694 valid loss: 1.9913, valid accuracy: 0.3377\n",
      "Iter-12250 train loss: 1.9363 valid loss: 1.9907, valid accuracy: 0.3415\n",
      "Iter-12260 train loss: 1.7227 valid loss: 1.9911, valid accuracy: 0.3377\n",
      "Iter-12270 train loss: 1.5760 valid loss: 1.9911, valid accuracy: 0.3438\n",
      "Iter-12280 train loss: 1.7410 valid loss: 1.9910, valid accuracy: 0.3462\n",
      "Iter-12290 train loss: 2.1506 valid loss: 1.9906, valid accuracy: 0.3492\n",
      "Iter-12300 train loss: 1.9748 valid loss: 1.9904, valid accuracy: 0.3446\n",
      "Iter-12310 train loss: 1.8794 valid loss: 1.9908, valid accuracy: 0.3431\n",
      "Iter-12320 train loss: 1.9006 valid loss: 1.9911, valid accuracy: 0.3431\n",
      "Iter-12330 train loss: 1.7434 valid loss: 1.9916, valid accuracy: 0.3415\n",
      "Iter-12340 train loss: 1.9012 valid loss: 1.9916, valid accuracy: 0.3415\n",
      "Iter-12350 train loss: 2.1373 valid loss: 1.9919, valid accuracy: 0.3400\n",
      "Iter-12360 train loss: 2.0323 valid loss: 1.9908, valid accuracy: 0.3431\n",
      "Iter-12370 train loss: 1.9076 valid loss: 1.9908, valid accuracy: 0.3492\n",
      "Iter-12380 train loss: 2.0538 valid loss: 1.9913, valid accuracy: 0.3492\n",
      "Iter-12390 train loss: 2.0027 valid loss: 1.9914, valid accuracy: 0.3431\n",
      "Iter-12400 train loss: 1.7317 valid loss: 1.9907, valid accuracy: 0.3469\n",
      "Iter-12410 train loss: 2.1580 valid loss: 1.9897, valid accuracy: 0.3462\n",
      "Iter-12420 train loss: 1.8928 valid loss: 1.9894, valid accuracy: 0.3446\n",
      "Iter-12430 train loss: 1.7842 valid loss: 1.9900, valid accuracy: 0.3454\n",
      "Iter-12440 train loss: 1.7961 valid loss: 1.9897, valid accuracy: 0.3485\n",
      "Iter-12450 train loss: 1.7449 valid loss: 1.9894, valid accuracy: 0.3446\n",
      "Iter-12460 train loss: 1.8774 valid loss: 1.9891, valid accuracy: 0.3446\n",
      "Iter-12470 train loss: 1.8321 valid loss: 1.9889, valid accuracy: 0.3462\n",
      "Iter-12480 train loss: 1.6532 valid loss: 1.9890, valid accuracy: 0.3438\n",
      "Iter-12490 train loss: 1.7389 valid loss: 1.9886, valid accuracy: 0.3385\n",
      "Iter-12500 train loss: 1.7966 valid loss: 1.9889, valid accuracy: 0.3369\n",
      "Iter-12510 train loss: 1.9421 valid loss: 1.9886, valid accuracy: 0.3369\n",
      "Iter-12520 train loss: 1.9971 valid loss: 1.9886, valid accuracy: 0.3400\n",
      "Iter-12530 train loss: 1.9421 valid loss: 1.9885, valid accuracy: 0.3423\n",
      "Iter-12540 train loss: 2.0175 valid loss: 1.9890, valid accuracy: 0.3415\n",
      "Iter-12550 train loss: 1.5986 valid loss: 1.9885, valid accuracy: 0.3408\n",
      "Iter-12560 train loss: 1.8812 valid loss: 1.9881, valid accuracy: 0.3415\n",
      "Iter-12570 train loss: 2.0416 valid loss: 1.9883, valid accuracy: 0.3454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-12580 train loss: 1.9704 valid loss: 1.9886, valid accuracy: 0.3392\n",
      "Iter-12590 train loss: 1.9022 valid loss: 1.9877, valid accuracy: 0.3454\n",
      "Iter-12600 train loss: 1.8894 valid loss: 1.9881, valid accuracy: 0.3454\n",
      "Iter-12610 train loss: 1.9152 valid loss: 1.9887, valid accuracy: 0.3408\n",
      "Iter-12620 train loss: 1.8555 valid loss: 1.9891, valid accuracy: 0.3431\n",
      "Iter-12630 train loss: 1.7585 valid loss: 1.9903, valid accuracy: 0.3392\n",
      "Iter-12640 train loss: 1.9768 valid loss: 1.9902, valid accuracy: 0.3385\n",
      "Iter-12650 train loss: 1.9586 valid loss: 1.9897, valid accuracy: 0.3423\n",
      "Iter-12660 train loss: 1.7697 valid loss: 1.9893, valid accuracy: 0.3500\n",
      "Iter-12670 train loss: 1.9369 valid loss: 1.9892, valid accuracy: 0.3462\n",
      "Iter-12680 train loss: 1.7593 valid loss: 1.9899, valid accuracy: 0.3485\n",
      "Iter-12690 train loss: 2.0047 valid loss: 1.9877, valid accuracy: 0.3446\n",
      "Iter-12700 train loss: 1.8845 valid loss: 1.9881, valid accuracy: 0.3477\n",
      "Iter-12710 train loss: 1.8920 valid loss: 1.9878, valid accuracy: 0.3477\n",
      "Iter-12720 train loss: 2.0042 valid loss: 1.9879, valid accuracy: 0.3454\n",
      "Iter-12730 train loss: 1.7362 valid loss: 1.9873, valid accuracy: 0.3438\n",
      "Iter-12740 train loss: 1.7775 valid loss: 1.9881, valid accuracy: 0.3438\n",
      "Iter-12750 train loss: 2.0769 valid loss: 1.9901, valid accuracy: 0.3438\n",
      "Iter-12760 train loss: 1.9707 valid loss: 1.9891, valid accuracy: 0.3446\n",
      "Iter-12770 train loss: 1.7861 valid loss: 1.9889, valid accuracy: 0.3446\n",
      "Iter-12780 train loss: 2.1114 valid loss: 1.9889, valid accuracy: 0.3446\n",
      "Iter-12790 train loss: 1.8836 valid loss: 1.9874, valid accuracy: 0.3500\n",
      "Iter-12800 train loss: 1.6998 valid loss: 1.9871, valid accuracy: 0.3462\n",
      "Iter-12810 train loss: 1.9072 valid loss: 1.9868, valid accuracy: 0.3492\n",
      "Iter-12820 train loss: 1.8642 valid loss: 1.9866, valid accuracy: 0.3469\n",
      "Iter-12830 train loss: 1.9273 valid loss: 1.9871, valid accuracy: 0.3469\n",
      "Iter-12840 train loss: 1.6604 valid loss: 1.9882, valid accuracy: 0.3462\n",
      "Iter-12850 train loss: 1.9532 valid loss: 1.9883, valid accuracy: 0.3454\n",
      "Iter-12860 train loss: 2.0805 valid loss: 1.9884, valid accuracy: 0.3477\n",
      "Iter-12870 train loss: 1.9889 valid loss: 1.9882, valid accuracy: 0.3477\n",
      "Iter-12880 train loss: 1.9911 valid loss: 1.9872, valid accuracy: 0.3477\n",
      "Iter-12890 train loss: 1.9309 valid loss: 1.9877, valid accuracy: 0.3462\n",
      "Iter-12900 train loss: 2.1293 valid loss: 1.9884, valid accuracy: 0.3423\n",
      "Iter-12910 train loss: 1.9793 valid loss: 1.9878, valid accuracy: 0.3462\n",
      "Iter-12920 train loss: 1.8661 valid loss: 1.9884, valid accuracy: 0.3485\n",
      "Iter-12930 train loss: 1.7538 valid loss: 1.9890, valid accuracy: 0.3500\n",
      "Iter-12940 train loss: 1.8289 valid loss: 1.9889, valid accuracy: 0.3515\n",
      "Iter-12950 train loss: 1.8458 valid loss: 1.9886, valid accuracy: 0.3508\n",
      "Iter-12960 train loss: 2.1204 valid loss: 1.9881, valid accuracy: 0.3523\n",
      "Iter-12970 train loss: 1.8249 valid loss: 1.9879, valid accuracy: 0.3508\n",
      "Iter-12980 train loss: 1.9547 valid loss: 1.9876, valid accuracy: 0.3523\n",
      "Iter-12990 train loss: 1.6884 valid loss: 1.9875, valid accuracy: 0.3492\n",
      "Iter-13000 train loss: 1.8747 valid loss: 1.9876, valid accuracy: 0.3462\n",
      "Iter-13010 train loss: 1.9169 valid loss: 1.9866, valid accuracy: 0.3469\n",
      "Iter-13020 train loss: 1.8274 valid loss: 1.9871, valid accuracy: 0.3477\n",
      "Iter-13030 train loss: 2.0875 valid loss: 1.9879, valid accuracy: 0.3508\n",
      "Iter-13040 train loss: 2.1059 valid loss: 1.9879, valid accuracy: 0.3500\n",
      "Iter-13050 train loss: 1.9201 valid loss: 1.9883, valid accuracy: 0.3500\n",
      "Iter-13060 train loss: 1.9583 valid loss: 1.9885, valid accuracy: 0.3500\n",
      "Iter-13070 train loss: 1.8816 valid loss: 1.9878, valid accuracy: 0.3508\n",
      "Iter-13080 train loss: 1.8634 valid loss: 1.9877, valid accuracy: 0.3515\n",
      "Iter-13090 train loss: 1.6987 valid loss: 1.9868, valid accuracy: 0.3531\n",
      "Iter-13100 train loss: 1.9804 valid loss: 1.9868, valid accuracy: 0.3562\n",
      "Iter-13110 train loss: 1.8878 valid loss: 1.9874, valid accuracy: 0.3562\n",
      "Iter-13120 train loss: 1.7802 valid loss: 1.9883, valid accuracy: 0.3554\n",
      "Iter-13130 train loss: 2.1505 valid loss: 1.9876, valid accuracy: 0.3515\n",
      "Iter-13140 train loss: 1.8609 valid loss: 1.9876, valid accuracy: 0.3477\n",
      "Iter-13150 train loss: 2.0054 valid loss: 1.9883, valid accuracy: 0.3477\n",
      "Iter-13160 train loss: 1.9369 valid loss: 1.9876, valid accuracy: 0.3508\n",
      "Iter-13170 train loss: 1.7001 valid loss: 1.9869, valid accuracy: 0.3477\n",
      "Iter-13180 train loss: 2.1617 valid loss: 1.9876, valid accuracy: 0.3523\n",
      "Iter-13190 train loss: 1.9628 valid loss: 1.9876, valid accuracy: 0.3531\n",
      "Iter-13200 train loss: 2.1132 valid loss: 1.9880, valid accuracy: 0.3523\n",
      "Iter-13210 train loss: 1.9972 valid loss: 1.9878, valid accuracy: 0.3485\n",
      "Iter-13220 train loss: 1.7324 valid loss: 1.9881, valid accuracy: 0.3462\n",
      "Iter-13230 train loss: 1.9321 valid loss: 1.9900, valid accuracy: 0.3500\n",
      "Iter-13240 train loss: 1.9022 valid loss: 1.9900, valid accuracy: 0.3485\n",
      "Iter-13250 train loss: 1.7671 valid loss: 1.9904, valid accuracy: 0.3454\n",
      "Iter-13260 train loss: 1.9561 valid loss: 1.9879, valid accuracy: 0.3508\n",
      "Iter-13270 train loss: 1.9285 valid loss: 1.9873, valid accuracy: 0.3508\n",
      "Iter-13280 train loss: 2.1232 valid loss: 1.9866, valid accuracy: 0.3492\n",
      "Iter-13290 train loss: 1.9469 valid loss: 1.9866, valid accuracy: 0.3492\n",
      "Iter-13300 train loss: 2.0291 valid loss: 1.9866, valid accuracy: 0.3508\n",
      "Iter-13310 train loss: 1.6472 valid loss: 1.9866, valid accuracy: 0.3562\n",
      "Iter-13320 train loss: 1.9728 valid loss: 1.9855, valid accuracy: 0.3531\n",
      "Iter-13330 train loss: 1.6346 valid loss: 1.9856, valid accuracy: 0.3554\n",
      "Iter-13340 train loss: 2.1482 valid loss: 1.9862, valid accuracy: 0.3531\n",
      "Iter-13350 train loss: 1.8907 valid loss: 1.9866, valid accuracy: 0.3562\n",
      "Iter-13360 train loss: 1.8654 valid loss: 1.9854, valid accuracy: 0.3554\n",
      "Iter-13370 train loss: 1.8224 valid loss: 1.9856, valid accuracy: 0.3500\n",
      "Iter-13380 train loss: 1.9804 valid loss: 1.9863, valid accuracy: 0.3538\n",
      "Iter-13390 train loss: 1.7083 valid loss: 1.9861, valid accuracy: 0.3469\n",
      "Iter-13400 train loss: 1.9714 valid loss: 1.9858, valid accuracy: 0.3469\n",
      "Iter-13410 train loss: 2.1228 valid loss: 1.9864, valid accuracy: 0.3477\n",
      "Iter-13420 train loss: 1.9084 valid loss: 1.9865, valid accuracy: 0.3508\n",
      "Iter-13430 train loss: 2.1192 valid loss: 1.9868, valid accuracy: 0.3500\n",
      "Iter-13440 train loss: 1.7143 valid loss: 1.9859, valid accuracy: 0.3515\n",
      "Iter-13450 train loss: 2.1591 valid loss: 1.9862, valid accuracy: 0.3469\n",
      "Iter-13460 train loss: 1.7678 valid loss: 1.9876, valid accuracy: 0.3538\n",
      "Iter-13470 train loss: 2.0994 valid loss: 1.9886, valid accuracy: 0.3485\n",
      "Iter-13480 train loss: 1.9324 valid loss: 1.9879, valid accuracy: 0.3523\n",
      "Iter-13490 train loss: 1.9562 valid loss: 1.9878, valid accuracy: 0.3500\n",
      "Iter-13500 train loss: 1.8366 valid loss: 1.9873, valid accuracy: 0.3500\n",
      "Iter-13510 train loss: 2.0561 valid loss: 1.9870, valid accuracy: 0.3500\n",
      "Iter-13520 train loss: 1.9291 valid loss: 1.9871, valid accuracy: 0.3515\n",
      "Iter-13530 train loss: 1.8766 valid loss: 1.9859, valid accuracy: 0.3515\n",
      "Iter-13540 train loss: 1.9176 valid loss: 1.9864, valid accuracy: 0.3500\n",
      "Iter-13550 train loss: 2.0498 valid loss: 1.9860, valid accuracy: 0.3515\n",
      "Iter-13560 train loss: 1.8092 valid loss: 1.9869, valid accuracy: 0.3485\n",
      "Iter-13570 train loss: 2.0477 valid loss: 1.9873, valid accuracy: 0.3469\n",
      "Iter-13580 train loss: 1.8488 valid loss: 1.9882, valid accuracy: 0.3415\n",
      "Iter-13590 train loss: 1.9423 valid loss: 1.9869, valid accuracy: 0.3462\n",
      "Iter-13600 train loss: 1.8649 valid loss: 1.9882, valid accuracy: 0.3438\n",
      "Iter-13610 train loss: 2.0978 valid loss: 1.9855, valid accuracy: 0.3469\n",
      "Iter-13620 train loss: 2.1906 valid loss: 1.9851, valid accuracy: 0.3500\n",
      "Iter-13630 train loss: 1.8261 valid loss: 1.9834, valid accuracy: 0.3546\n",
      "Iter-13640 train loss: 1.9090 valid loss: 1.9836, valid accuracy: 0.3562\n",
      "Iter-13650 train loss: 1.9659 valid loss: 1.9818, valid accuracy: 0.3577\n",
      "Iter-13660 train loss: 1.9173 valid loss: 1.9825, valid accuracy: 0.3577\n",
      "Iter-13670 train loss: 1.6324 valid loss: 1.9821, valid accuracy: 0.3562\n",
      "Iter-13680 train loss: 1.5816 valid loss: 1.9817, valid accuracy: 0.3523\n",
      "Iter-13690 train loss: 1.8537 valid loss: 1.9826, valid accuracy: 0.3485\n",
      "Iter-13700 train loss: 1.8790 valid loss: 1.9828, valid accuracy: 0.3554\n",
      "Iter-13710 train loss: 1.6828 valid loss: 1.9834, valid accuracy: 0.3546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13720 train loss: 1.7397 valid loss: 1.9838, valid accuracy: 0.3485\n",
      "Iter-13730 train loss: 1.9734 valid loss: 1.9843, valid accuracy: 0.3492\n",
      "Iter-13740 train loss: 1.7256 valid loss: 1.9834, valid accuracy: 0.3500\n",
      "Iter-13750 train loss: 2.2254 valid loss: 1.9856, valid accuracy: 0.3508\n",
      "Iter-13760 train loss: 1.8174 valid loss: 1.9856, valid accuracy: 0.3508\n",
      "Iter-13770 train loss: 1.7270 valid loss: 1.9842, valid accuracy: 0.3531\n",
      "Iter-13780 train loss: 1.9039 valid loss: 1.9840, valid accuracy: 0.3515\n",
      "Iter-13790 train loss: 1.9366 valid loss: 1.9843, valid accuracy: 0.3469\n",
      "Iter-13800 train loss: 2.0488 valid loss: 1.9830, valid accuracy: 0.3515\n",
      "Iter-13810 train loss: 1.8864 valid loss: 1.9845, valid accuracy: 0.3515\n",
      "Iter-13820 train loss: 2.0525 valid loss: 1.9836, valid accuracy: 0.3577\n",
      "Iter-13830 train loss: 1.9408 valid loss: 1.9837, valid accuracy: 0.3546\n",
      "Iter-13840 train loss: 1.7883 valid loss: 1.9843, valid accuracy: 0.3469\n",
      "Iter-13850 train loss: 1.8515 valid loss: 1.9844, valid accuracy: 0.3492\n",
      "Iter-13860 train loss: 1.6903 valid loss: 1.9834, valid accuracy: 0.3523\n",
      "Iter-13870 train loss: 1.8841 valid loss: 1.9837, valid accuracy: 0.3469\n",
      "Iter-13880 train loss: 1.9603 valid loss: 1.9829, valid accuracy: 0.3485\n",
      "Iter-13890 train loss: 1.9411 valid loss: 1.9820, valid accuracy: 0.3508\n",
      "Iter-13900 train loss: 2.0046 valid loss: 1.9822, valid accuracy: 0.3523\n",
      "Iter-13910 train loss: 1.7923 valid loss: 1.9816, valid accuracy: 0.3500\n",
      "Iter-13920 train loss: 1.6650 valid loss: 1.9833, valid accuracy: 0.3469\n",
      "Iter-13930 train loss: 1.8155 valid loss: 1.9840, valid accuracy: 0.3485\n",
      "Iter-13940 train loss: 1.8957 valid loss: 1.9839, valid accuracy: 0.3477\n",
      "Iter-13950 train loss: 1.7466 valid loss: 1.9838, valid accuracy: 0.3477\n",
      "Iter-13960 train loss: 1.7751 valid loss: 1.9820, valid accuracy: 0.3477\n",
      "Iter-13970 train loss: 1.7966 valid loss: 1.9839, valid accuracy: 0.3508\n",
      "Iter-13980 train loss: 1.7094 valid loss: 1.9819, valid accuracy: 0.3554\n",
      "Iter-13990 train loss: 1.8096 valid loss: 1.9815, valid accuracy: 0.3546\n",
      "Iter-14000 train loss: 1.6408 valid loss: 1.9815, valid accuracy: 0.3562\n",
      "Iter-14010 train loss: 1.6955 valid loss: 1.9815, valid accuracy: 0.3562\n",
      "Iter-14020 train loss: 1.9382 valid loss: 1.9813, valid accuracy: 0.3585\n",
      "Iter-14030 train loss: 1.9451 valid loss: 1.9808, valid accuracy: 0.3554\n",
      "Iter-14040 train loss: 1.8171 valid loss: 1.9827, valid accuracy: 0.3546\n",
      "Iter-14050 train loss: 1.9084 valid loss: 1.9829, valid accuracy: 0.3492\n",
      "Iter-14060 train loss: 1.7335 valid loss: 1.9838, valid accuracy: 0.3469\n",
      "Iter-14070 train loss: 2.0803 valid loss: 1.9838, valid accuracy: 0.3485\n",
      "Iter-14080 train loss: 2.0074 valid loss: 1.9839, valid accuracy: 0.3500\n",
      "Iter-14090 train loss: 2.0029 valid loss: 1.9829, valid accuracy: 0.3500\n",
      "Iter-14100 train loss: 2.1194 valid loss: 1.9829, valid accuracy: 0.3492\n",
      "Iter-14110 train loss: 1.7321 valid loss: 1.9838, valid accuracy: 0.3477\n",
      "Iter-14120 train loss: 2.0016 valid loss: 1.9839, valid accuracy: 0.3477\n",
      "Iter-14130 train loss: 1.9423 valid loss: 1.9832, valid accuracy: 0.3492\n",
      "Iter-14140 train loss: 1.9370 valid loss: 1.9820, valid accuracy: 0.3462\n",
      "Iter-14150 train loss: 1.8755 valid loss: 1.9810, valid accuracy: 0.3515\n",
      "Iter-14160 train loss: 1.8787 valid loss: 1.9803, valid accuracy: 0.3508\n",
      "Iter-14170 train loss: 1.7101 valid loss: 1.9798, valid accuracy: 0.3546\n",
      "Iter-14180 train loss: 1.6564 valid loss: 1.9808, valid accuracy: 0.3515\n",
      "Iter-14190 train loss: 1.8619 valid loss: 1.9805, valid accuracy: 0.3562\n",
      "Iter-14200 train loss: 1.7709 valid loss: 1.9799, valid accuracy: 0.3531\n",
      "Iter-14210 train loss: 1.8437 valid loss: 1.9801, valid accuracy: 0.3546\n",
      "Iter-14220 train loss: 2.0087 valid loss: 1.9805, valid accuracy: 0.3523\n",
      "Iter-14230 train loss: 1.9139 valid loss: 1.9825, valid accuracy: 0.3546\n",
      "Iter-14240 train loss: 1.8456 valid loss: 1.9832, valid accuracy: 0.3469\n",
      "Iter-14250 train loss: 1.9494 valid loss: 1.9832, valid accuracy: 0.3454\n",
      "Iter-14260 train loss: 1.8463 valid loss: 1.9829, valid accuracy: 0.3469\n",
      "Iter-14270 train loss: 1.9291 valid loss: 1.9820, valid accuracy: 0.3477\n",
      "Iter-14280 train loss: 1.7651 valid loss: 1.9829, valid accuracy: 0.3477\n",
      "Iter-14290 train loss: 1.5974 valid loss: 1.9827, valid accuracy: 0.3462\n",
      "Iter-14300 train loss: 1.7061 valid loss: 1.9819, valid accuracy: 0.3492\n",
      "Iter-14310 train loss: 1.8814 valid loss: 1.9813, valid accuracy: 0.3492\n",
      "Iter-14320 train loss: 1.7098 valid loss: 1.9817, valid accuracy: 0.3492\n",
      "Iter-14330 train loss: 2.0271 valid loss: 1.9827, valid accuracy: 0.3492\n",
      "Iter-14340 train loss: 2.0422 valid loss: 1.9829, valid accuracy: 0.3554\n",
      "Iter-14350 train loss: 1.7952 valid loss: 1.9827, valid accuracy: 0.3538\n",
      "Iter-14360 train loss: 1.6004 valid loss: 1.9825, valid accuracy: 0.3531\n",
      "Iter-14370 train loss: 1.8913 valid loss: 1.9826, valid accuracy: 0.3446\n",
      "Iter-14380 train loss: 1.4161 valid loss: 1.9822, valid accuracy: 0.3462\n",
      "Iter-14390 train loss: 1.9257 valid loss: 1.9830, valid accuracy: 0.3446\n",
      "Iter-14400 train loss: 2.0288 valid loss: 1.9827, valid accuracy: 0.3500\n",
      "Iter-14410 train loss: 1.8169 valid loss: 1.9838, valid accuracy: 0.3423\n",
      "Iter-14420 train loss: 1.9307 valid loss: 1.9827, valid accuracy: 0.3469\n",
      "Iter-14430 train loss: 1.8809 valid loss: 1.9832, valid accuracy: 0.3515\n",
      "Iter-14440 train loss: 1.9376 valid loss: 1.9838, valid accuracy: 0.3500\n",
      "Iter-14450 train loss: 1.9168 valid loss: 1.9843, valid accuracy: 0.3462\n",
      "Iter-14460 train loss: 2.0515 valid loss: 1.9847, valid accuracy: 0.3438\n",
      "Iter-14470 train loss: 1.9135 valid loss: 1.9842, valid accuracy: 0.3454\n",
      "Iter-14480 train loss: 1.6840 valid loss: 1.9855, valid accuracy: 0.3477\n",
      "Iter-14490 train loss: 1.5368 valid loss: 1.9840, valid accuracy: 0.3492\n",
      "Iter-14500 train loss: 1.6786 valid loss: 1.9832, valid accuracy: 0.3500\n",
      "Iter-14510 train loss: 1.9494 valid loss: 1.9824, valid accuracy: 0.3508\n",
      "Iter-14520 train loss: 2.0218 valid loss: 1.9827, valid accuracy: 0.3546\n",
      "Iter-14530 train loss: 1.8553 valid loss: 1.9824, valid accuracy: 0.3523\n",
      "Iter-14540 train loss: 1.7813 valid loss: 1.9809, valid accuracy: 0.3492\n",
      "Iter-14550 train loss: 1.9687 valid loss: 1.9808, valid accuracy: 0.3523\n",
      "Iter-14560 train loss: 1.8951 valid loss: 1.9821, valid accuracy: 0.3538\n",
      "Iter-14570 train loss: 1.7820 valid loss: 1.9805, valid accuracy: 0.3538\n",
      "Iter-14580 train loss: 1.7412 valid loss: 1.9800, valid accuracy: 0.3523\n",
      "Iter-14590 train loss: 1.6656 valid loss: 1.9809, valid accuracy: 0.3562\n",
      "Iter-14600 train loss: 1.9018 valid loss: 1.9802, valid accuracy: 0.3562\n",
      "Iter-14610 train loss: 1.7922 valid loss: 1.9810, valid accuracy: 0.3515\n",
      "Iter-14620 train loss: 1.8138 valid loss: 1.9804, valid accuracy: 0.3554\n",
      "Iter-14630 train loss: 1.9848 valid loss: 1.9784, valid accuracy: 0.3585\n",
      "Iter-14640 train loss: 1.9055 valid loss: 1.9775, valid accuracy: 0.3523\n",
      "Iter-14650 train loss: 1.5307 valid loss: 1.9773, valid accuracy: 0.3562\n",
      "Iter-14660 train loss: 2.0315 valid loss: 1.9788, valid accuracy: 0.3531\n",
      "Iter-14670 train loss: 1.9096 valid loss: 1.9791, valid accuracy: 0.3562\n",
      "Iter-14680 train loss: 2.0357 valid loss: 1.9801, valid accuracy: 0.3600\n",
      "Iter-14690 train loss: 1.9349 valid loss: 1.9809, valid accuracy: 0.3500\n",
      "Iter-14700 train loss: 1.8559 valid loss: 1.9808, valid accuracy: 0.3523\n",
      "Iter-14710 train loss: 1.9237 valid loss: 1.9810, valid accuracy: 0.3515\n",
      "Iter-14720 train loss: 1.6464 valid loss: 1.9810, valid accuracy: 0.3515\n",
      "Iter-14730 train loss: 1.9824 valid loss: 1.9833, valid accuracy: 0.3508\n",
      "Iter-14740 train loss: 1.6819 valid loss: 1.9817, valid accuracy: 0.3477\n",
      "Iter-14750 train loss: 2.0885 valid loss: 1.9822, valid accuracy: 0.3500\n",
      "Iter-14760 train loss: 1.6681 valid loss: 1.9812, valid accuracy: 0.3531\n",
      "Iter-14770 train loss: 1.6886 valid loss: 1.9802, valid accuracy: 0.3546\n",
      "Iter-14780 train loss: 1.9379 valid loss: 1.9807, valid accuracy: 0.3562\n",
      "Iter-14790 train loss: 1.6151 valid loss: 1.9812, valid accuracy: 0.3508\n",
      "Iter-14800 train loss: 2.0427 valid loss: 1.9820, valid accuracy: 0.3531\n",
      "Iter-14810 train loss: 1.9777 valid loss: 1.9826, valid accuracy: 0.3538\n",
      "Iter-14820 train loss: 2.0254 valid loss: 1.9820, valid accuracy: 0.3500\n",
      "Iter-14830 train loss: 1.5194 valid loss: 1.9822, valid accuracy: 0.3438\n",
      "Iter-14840 train loss: 2.0384 valid loss: 1.9817, valid accuracy: 0.3454\n",
      "Iter-14850 train loss: 1.8039 valid loss: 1.9818, valid accuracy: 0.3531\n",
      "Iter-14860 train loss: 2.1512 valid loss: 1.9817, valid accuracy: 0.3500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-14870 train loss: 1.9009 valid loss: 1.9834, valid accuracy: 0.3500\n",
      "Iter-14880 train loss: 1.8563 valid loss: 1.9829, valid accuracy: 0.3485\n",
      "Iter-14890 train loss: 1.9359 valid loss: 1.9824, valid accuracy: 0.3515\n",
      "Iter-14900 train loss: 1.9281 valid loss: 1.9821, valid accuracy: 0.3538\n",
      "Iter-14910 train loss: 1.8627 valid loss: 1.9819, valid accuracy: 0.3538\n",
      "Iter-14920 train loss: 1.9361 valid loss: 1.9828, valid accuracy: 0.3531\n",
      "Iter-14930 train loss: 1.8697 valid loss: 1.9816, valid accuracy: 0.3508\n",
      "Iter-14940 train loss: 2.0076 valid loss: 1.9798, valid accuracy: 0.3608\n",
      "Iter-14950 train loss: 2.0968 valid loss: 1.9791, valid accuracy: 0.3592\n",
      "Iter-14960 train loss: 2.0400 valid loss: 1.9785, valid accuracy: 0.3554\n",
      "Iter-14970 train loss: 2.1108 valid loss: 1.9786, valid accuracy: 0.3554\n",
      "Iter-14980 train loss: 1.7470 valid loss: 1.9790, valid accuracy: 0.3562\n",
      "Iter-14990 train loss: 1.9125 valid loss: 1.9804, valid accuracy: 0.3531\n",
      "Iter-15000 train loss: 1.7470 valid loss: 1.9800, valid accuracy: 0.3531\n",
      "Iter-15010 train loss: 1.8430 valid loss: 1.9799, valid accuracy: 0.3523\n",
      "Iter-15020 train loss: 1.6766 valid loss: 1.9800, valid accuracy: 0.3469\n",
      "Iter-15030 train loss: 1.8052 valid loss: 1.9795, valid accuracy: 0.3508\n",
      "Iter-15040 train loss: 1.9352 valid loss: 1.9796, valid accuracy: 0.3477\n",
      "Iter-15050 train loss: 2.0823 valid loss: 1.9798, valid accuracy: 0.3462\n",
      "Iter-15060 train loss: 2.0778 valid loss: 1.9786, valid accuracy: 0.3492\n",
      "Iter-15070 train loss: 1.7614 valid loss: 1.9802, valid accuracy: 0.3462\n",
      "Iter-15080 train loss: 1.9583 valid loss: 1.9801, valid accuracy: 0.3492\n",
      "Iter-15090 train loss: 1.8134 valid loss: 1.9804, valid accuracy: 0.3508\n",
      "Iter-15100 train loss: 1.9610 valid loss: 1.9806, valid accuracy: 0.3500\n",
      "Iter-15110 train loss: 1.8709 valid loss: 1.9806, valid accuracy: 0.3477\n",
      "Iter-15120 train loss: 1.8686 valid loss: 1.9812, valid accuracy: 0.3523\n",
      "Iter-15130 train loss: 1.9181 valid loss: 1.9805, valid accuracy: 0.3508\n",
      "Iter-15140 train loss: 1.9329 valid loss: 1.9805, valid accuracy: 0.3515\n",
      "Iter-15150 train loss: 2.0090 valid loss: 1.9810, valid accuracy: 0.3531\n",
      "Iter-15160 train loss: 2.0617 valid loss: 1.9816, valid accuracy: 0.3523\n",
      "Iter-15170 train loss: 1.9074 valid loss: 1.9819, valid accuracy: 0.3508\n",
      "Iter-15180 train loss: 1.5860 valid loss: 1.9818, valid accuracy: 0.3500\n",
      "Iter-15190 train loss: 1.9731 valid loss: 1.9815, valid accuracy: 0.3485\n",
      "Iter-15200 train loss: 1.9113 valid loss: 1.9808, valid accuracy: 0.3492\n",
      "Iter-15210 train loss: 2.0501 valid loss: 1.9809, valid accuracy: 0.3500\n",
      "Iter-15220 train loss: 1.7480 valid loss: 1.9808, valid accuracy: 0.3477\n",
      "Iter-15230 train loss: 1.9611 valid loss: 1.9817, valid accuracy: 0.3477\n",
      "Iter-15240 train loss: 2.1906 valid loss: 1.9807, valid accuracy: 0.3500\n",
      "Iter-15250 train loss: 1.7136 valid loss: 1.9805, valid accuracy: 0.3523\n",
      "Iter-15260 train loss: 1.6863 valid loss: 1.9808, valid accuracy: 0.3500\n",
      "Iter-15270 train loss: 1.7624 valid loss: 1.9789, valid accuracy: 0.3562\n",
      "Iter-15280 train loss: 1.7677 valid loss: 1.9788, valid accuracy: 0.3562\n",
      "Iter-15290 train loss: 1.8877 valid loss: 1.9791, valid accuracy: 0.3538\n",
      "Iter-15300 train loss: 1.7894 valid loss: 1.9796, valid accuracy: 0.3508\n",
      "Iter-15310 train loss: 1.9320 valid loss: 1.9799, valid accuracy: 0.3508\n",
      "Iter-15320 train loss: 1.8367 valid loss: 1.9792, valid accuracy: 0.3500\n",
      "Iter-15330 train loss: 1.9658 valid loss: 1.9810, valid accuracy: 0.3523\n",
      "Iter-15340 train loss: 1.7696 valid loss: 1.9824, valid accuracy: 0.3492\n",
      "Iter-15350 train loss: 2.0302 valid loss: 1.9820, valid accuracy: 0.3500\n",
      "Iter-15360 train loss: 1.8756 valid loss: 1.9801, valid accuracy: 0.3515\n",
      "Iter-15370 train loss: 2.2527 valid loss: 1.9790, valid accuracy: 0.3515\n",
      "Iter-15380 train loss: 1.7125 valid loss: 1.9786, valid accuracy: 0.3538\n",
      "Iter-15390 train loss: 1.7440 valid loss: 1.9784, valid accuracy: 0.3515\n",
      "Iter-15400 train loss: 1.9100 valid loss: 1.9776, valid accuracy: 0.3531\n",
      "Iter-15410 train loss: 1.8705 valid loss: 1.9761, valid accuracy: 0.3562\n",
      "Iter-15420 train loss: 1.8234 valid loss: 1.9773, valid accuracy: 0.3562\n",
      "Iter-15430 train loss: 1.9975 valid loss: 1.9772, valid accuracy: 0.3515\n",
      "Iter-15440 train loss: 1.9505 valid loss: 1.9774, valid accuracy: 0.3585\n",
      "Iter-15450 train loss: 1.8549 valid loss: 1.9778, valid accuracy: 0.3562\n",
      "Iter-15460 train loss: 2.1319 valid loss: 1.9788, valid accuracy: 0.3515\n",
      "Iter-15470 train loss: 1.7025 valid loss: 1.9782, valid accuracy: 0.3515\n",
      "Iter-15480 train loss: 1.8435 valid loss: 1.9782, valid accuracy: 0.3508\n",
      "Iter-15490 train loss: 1.9180 valid loss: 1.9781, valid accuracy: 0.3485\n",
      "Iter-15500 train loss: 1.9817 valid loss: 1.9775, valid accuracy: 0.3485\n",
      "Iter-15510 train loss: 2.0995 valid loss: 1.9782, valid accuracy: 0.3508\n",
      "Iter-15520 train loss: 2.0209 valid loss: 1.9797, valid accuracy: 0.3523\n",
      "Iter-15530 train loss: 1.6263 valid loss: 1.9791, valid accuracy: 0.3508\n",
      "Iter-15540 train loss: 1.8245 valid loss: 1.9788, valid accuracy: 0.3492\n",
      "Iter-15550 train loss: 1.8437 valid loss: 1.9782, valid accuracy: 0.3523\n",
      "Iter-15560 train loss: 1.8406 valid loss: 1.9777, valid accuracy: 0.3523\n",
      "Iter-15570 train loss: 1.8721 valid loss: 1.9781, valid accuracy: 0.3538\n",
      "Iter-15580 train loss: 2.0278 valid loss: 1.9792, valid accuracy: 0.3538\n",
      "Iter-15590 train loss: 2.0097 valid loss: 1.9792, valid accuracy: 0.3500\n",
      "Iter-15600 train loss: 2.0384 valid loss: 1.9798, valid accuracy: 0.3477\n",
      "Iter-15610 train loss: 1.8420 valid loss: 1.9809, valid accuracy: 0.3546\n",
      "Iter-15620 train loss: 1.6769 valid loss: 1.9801, valid accuracy: 0.3538\n",
      "Iter-15630 train loss: 1.6354 valid loss: 1.9797, valid accuracy: 0.3500\n",
      "Iter-15640 train loss: 1.7129 valid loss: 1.9801, valid accuracy: 0.3546\n",
      "Iter-15650 train loss: 1.7500 valid loss: 1.9784, valid accuracy: 0.3515\n",
      "Iter-15660 train loss: 1.7161 valid loss: 1.9782, valid accuracy: 0.3538\n",
      "Iter-15670 train loss: 1.7353 valid loss: 1.9786, valid accuracy: 0.3569\n",
      "Iter-15680 train loss: 1.8227 valid loss: 1.9789, valid accuracy: 0.3531\n",
      "Iter-15690 train loss: 1.9490 valid loss: 1.9798, valid accuracy: 0.3508\n",
      "Iter-15700 train loss: 1.6278 valid loss: 1.9794, valid accuracy: 0.3523\n",
      "Iter-15710 train loss: 1.9727 valid loss: 1.9793, valid accuracy: 0.3523\n",
      "Iter-15720 train loss: 2.1188 valid loss: 1.9787, valid accuracy: 0.3508\n",
      "Iter-15730 train loss: 1.8452 valid loss: 1.9780, valid accuracy: 0.3515\n",
      "Iter-15740 train loss: 1.7732 valid loss: 1.9787, valid accuracy: 0.3477\n",
      "Iter-15750 train loss: 1.8913 valid loss: 1.9799, valid accuracy: 0.3515\n",
      "Iter-15760 train loss: 2.0371 valid loss: 1.9803, valid accuracy: 0.3477\n",
      "Iter-15770 train loss: 1.7668 valid loss: 1.9811, valid accuracy: 0.3485\n",
      "Iter-15780 train loss: 1.8998 valid loss: 1.9813, valid accuracy: 0.3500\n",
      "Iter-15790 train loss: 1.8826 valid loss: 1.9814, valid accuracy: 0.3469\n",
      "Iter-15800 train loss: 1.9382 valid loss: 1.9800, valid accuracy: 0.3515\n",
      "Iter-15810 train loss: 2.0329 valid loss: 1.9811, valid accuracy: 0.3485\n",
      "Iter-15820 train loss: 1.7386 valid loss: 1.9797, valid accuracy: 0.3500\n",
      "Iter-15830 train loss: 1.8330 valid loss: 1.9796, valid accuracy: 0.3531\n",
      "Iter-15840 train loss: 1.9337 valid loss: 1.9797, valid accuracy: 0.3515\n",
      "Iter-15850 train loss: 1.8610 valid loss: 1.9803, valid accuracy: 0.3500\n",
      "Iter-15860 train loss: 2.0742 valid loss: 1.9798, valid accuracy: 0.3508\n",
      "Iter-15870 train loss: 1.9384 valid loss: 1.9797, valid accuracy: 0.3485\n",
      "Iter-15880 train loss: 1.7506 valid loss: 1.9810, valid accuracy: 0.3485\n",
      "Iter-15890 train loss: 2.0146 valid loss: 1.9826, valid accuracy: 0.3469\n",
      "Iter-15900 train loss: 1.7194 valid loss: 1.9811, valid accuracy: 0.3485\n",
      "Iter-15910 train loss: 1.6974 valid loss: 1.9816, valid accuracy: 0.3508\n",
      "Iter-15920 train loss: 2.1051 valid loss: 1.9795, valid accuracy: 0.3508\n",
      "Iter-15930 train loss: 2.0011 valid loss: 1.9783, valid accuracy: 0.3538\n",
      "Iter-15940 train loss: 1.8432 valid loss: 1.9800, valid accuracy: 0.3562\n",
      "Iter-15950 train loss: 1.8715 valid loss: 1.9812, valid accuracy: 0.3554\n",
      "Iter-15960 train loss: 1.7398 valid loss: 1.9803, valid accuracy: 0.3562\n",
      "Iter-15970 train loss: 1.8635 valid loss: 1.9806, valid accuracy: 0.3508\n",
      "Iter-15980 train loss: 1.9515 valid loss: 1.9800, valid accuracy: 0.3515\n",
      "Iter-15990 train loss: 2.1478 valid loss: 1.9804, valid accuracy: 0.3469\n",
      "Iter-16000 train loss: 1.8279 valid loss: 1.9798, valid accuracy: 0.3523\n",
      "Iter-16010 train loss: 1.6247 valid loss: 1.9799, valid accuracy: 0.3531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-16020 train loss: 1.6717 valid loss: 1.9809, valid accuracy: 0.3523\n",
      "Iter-16030 train loss: 2.0758 valid loss: 1.9798, valid accuracy: 0.3500\n",
      "Iter-16040 train loss: 1.8335 valid loss: 1.9785, valid accuracy: 0.3492\n",
      "Iter-16050 train loss: 1.8742 valid loss: 1.9786, valid accuracy: 0.3531\n",
      "Iter-16060 train loss: 1.6986 valid loss: 1.9772, valid accuracy: 0.3538\n",
      "Iter-16070 train loss: 1.9691 valid loss: 1.9778, valid accuracy: 0.3562\n",
      "Iter-16080 train loss: 1.6759 valid loss: 1.9767, valid accuracy: 0.3485\n",
      "Iter-16090 train loss: 1.8325 valid loss: 1.9768, valid accuracy: 0.3508\n",
      "Iter-16100 train loss: 1.6316 valid loss: 1.9765, valid accuracy: 0.3492\n",
      "Iter-16110 train loss: 1.9056 valid loss: 1.9758, valid accuracy: 0.3531\n",
      "Iter-16120 train loss: 1.9567 valid loss: 1.9755, valid accuracy: 0.3554\n",
      "Iter-16130 train loss: 1.8756 valid loss: 1.9765, valid accuracy: 0.3469\n",
      "Iter-16140 train loss: 2.0059 valid loss: 1.9760, valid accuracy: 0.3477\n",
      "Iter-16150 train loss: 1.9486 valid loss: 1.9762, valid accuracy: 0.3477\n",
      "Iter-16160 train loss: 1.8040 valid loss: 1.9752, valid accuracy: 0.3485\n",
      "Iter-16170 train loss: 2.0583 valid loss: 1.9749, valid accuracy: 0.3508\n",
      "Iter-16180 train loss: 1.9283 valid loss: 1.9767, valid accuracy: 0.3508\n",
      "Iter-16190 train loss: 1.6964 valid loss: 1.9764, valid accuracy: 0.3508\n",
      "Iter-16200 train loss: 2.1771 valid loss: 1.9751, valid accuracy: 0.3523\n",
      "Iter-16210 train loss: 1.8601 valid loss: 1.9753, valid accuracy: 0.3523\n",
      "Iter-16220 train loss: 1.8853 valid loss: 1.9751, valid accuracy: 0.3538\n",
      "Iter-16230 train loss: 1.8117 valid loss: 1.9758, valid accuracy: 0.3554\n",
      "Iter-16240 train loss: 1.9853 valid loss: 1.9762, valid accuracy: 0.3585\n",
      "Iter-16250 train loss: 1.8737 valid loss: 1.9757, valid accuracy: 0.3546\n",
      "Iter-16260 train loss: 1.7813 valid loss: 1.9765, valid accuracy: 0.3562\n",
      "Iter-16270 train loss: 1.8602 valid loss: 1.9769, valid accuracy: 0.3554\n",
      "Iter-16280 train loss: 1.9103 valid loss: 1.9769, valid accuracy: 0.3538\n",
      "Iter-16290 train loss: 2.0561 valid loss: 1.9760, valid accuracy: 0.3508\n",
      "Iter-16300 train loss: 1.6123 valid loss: 1.9762, valid accuracy: 0.3492\n",
      "Iter-16310 train loss: 2.0279 valid loss: 1.9770, valid accuracy: 0.3485\n",
      "Iter-16320 train loss: 1.9766 valid loss: 1.9759, valid accuracy: 0.3500\n",
      "Iter-16330 train loss: 2.0604 valid loss: 1.9763, valid accuracy: 0.3523\n",
      "Iter-16340 train loss: 2.0734 valid loss: 1.9757, valid accuracy: 0.3500\n",
      "Iter-16350 train loss: 1.6315 valid loss: 1.9752, valid accuracy: 0.3554\n",
      "Iter-16360 train loss: 1.7508 valid loss: 1.9753, valid accuracy: 0.3546\n",
      "Iter-16370 train loss: 1.7552 valid loss: 1.9772, valid accuracy: 0.3508\n",
      "Iter-16380 train loss: 2.1996 valid loss: 1.9776, valid accuracy: 0.3500\n",
      "Iter-16390 train loss: 1.8475 valid loss: 1.9767, valid accuracy: 0.3569\n",
      "Iter-16400 train loss: 2.0964 valid loss: 1.9767, valid accuracy: 0.3562\n",
      "Iter-16410 train loss: 1.8834 valid loss: 1.9768, valid accuracy: 0.3531\n",
      "Iter-16420 train loss: 1.9365 valid loss: 1.9770, valid accuracy: 0.3492\n",
      "Iter-16430 train loss: 1.8288 valid loss: 1.9761, valid accuracy: 0.3523\n",
      "Iter-16440 train loss: 1.7911 valid loss: 1.9762, valid accuracy: 0.3508\n",
      "Iter-16450 train loss: 1.8753 valid loss: 1.9774, valid accuracy: 0.3492\n",
      "Iter-16460 train loss: 1.9304 valid loss: 1.9749, valid accuracy: 0.3515\n",
      "Iter-16470 train loss: 1.9352 valid loss: 1.9746, valid accuracy: 0.3538\n",
      "Iter-16480 train loss: 1.9233 valid loss: 1.9734, valid accuracy: 0.3546\n",
      "Iter-16490 train loss: 1.8518 valid loss: 1.9740, valid accuracy: 0.3546\n",
      "Iter-16500 train loss: 1.7193 valid loss: 1.9744, valid accuracy: 0.3569\n",
      "Iter-16510 train loss: 1.7566 valid loss: 1.9741, valid accuracy: 0.3569\n",
      "Iter-16520 train loss: 1.7292 valid loss: 1.9748, valid accuracy: 0.3538\n",
      "Iter-16530 train loss: 1.7179 valid loss: 1.9769, valid accuracy: 0.3508\n",
      "Iter-16540 train loss: 2.0994 valid loss: 1.9766, valid accuracy: 0.3515\n",
      "Iter-16550 train loss: 1.9332 valid loss: 1.9773, valid accuracy: 0.3523\n",
      "Iter-16560 train loss: 1.5665 valid loss: 1.9756, valid accuracy: 0.3538\n",
      "Iter-16570 train loss: 1.7557 valid loss: 1.9752, valid accuracy: 0.3531\n",
      "Iter-16580 train loss: 1.9823 valid loss: 1.9777, valid accuracy: 0.3538\n",
      "Iter-16590 train loss: 1.6595 valid loss: 1.9785, valid accuracy: 0.3538\n",
      "Iter-16600 train loss: 1.8545 valid loss: 1.9779, valid accuracy: 0.3523\n",
      "Iter-16610 train loss: 1.9820 valid loss: 1.9789, valid accuracy: 0.3500\n",
      "Iter-16620 train loss: 2.0196 valid loss: 1.9794, valid accuracy: 0.3485\n",
      "Iter-16630 train loss: 1.8852 valid loss: 1.9782, valid accuracy: 0.3477\n",
      "Iter-16640 train loss: 1.8190 valid loss: 1.9776, valid accuracy: 0.3477\n",
      "Iter-16650 train loss: 2.0036 valid loss: 1.9785, valid accuracy: 0.3569\n",
      "Iter-16660 train loss: 2.2348 valid loss: 1.9798, valid accuracy: 0.3538\n",
      "Iter-16670 train loss: 1.7507 valid loss: 1.9778, valid accuracy: 0.3562\n",
      "Iter-16680 train loss: 1.6435 valid loss: 1.9782, valid accuracy: 0.3562\n",
      "Iter-16690 train loss: 1.7390 valid loss: 1.9778, valid accuracy: 0.3523\n",
      "Iter-16700 train loss: 2.0310 valid loss: 1.9791, valid accuracy: 0.3531\n",
      "Iter-16710 train loss: 1.8700 valid loss: 1.9775, valid accuracy: 0.3531\n",
      "Iter-16720 train loss: 1.8210 valid loss: 1.9766, valid accuracy: 0.3531\n",
      "Iter-16730 train loss: 1.7379 valid loss: 1.9745, valid accuracy: 0.3538\n",
      "Iter-16740 train loss: 2.0151 valid loss: 1.9777, valid accuracy: 0.3523\n",
      "Iter-16750 train loss: 2.2307 valid loss: 1.9766, valid accuracy: 0.3554\n",
      "Iter-16760 train loss: 2.0839 valid loss: 1.9780, valid accuracy: 0.3569\n",
      "Iter-16770 train loss: 2.0658 valid loss: 1.9801, valid accuracy: 0.3577\n",
      "Iter-16780 train loss: 1.6899 valid loss: 1.9785, valid accuracy: 0.3600\n",
      "Iter-16790 train loss: 2.1427 valid loss: 1.9788, valid accuracy: 0.3523\n",
      "Iter-16800 train loss: 1.8530 valid loss: 1.9780, valid accuracy: 0.3569\n",
      "Iter-16810 train loss: 1.8574 valid loss: 1.9781, valid accuracy: 0.3569\n",
      "Iter-16820 train loss: 1.9773 valid loss: 1.9776, valid accuracy: 0.3546\n",
      "Iter-16830 train loss: 1.8802 valid loss: 1.9765, valid accuracy: 0.3554\n",
      "Iter-16840 train loss: 1.9421 valid loss: 1.9754, valid accuracy: 0.3608\n",
      "Iter-16850 train loss: 1.9973 valid loss: 1.9750, valid accuracy: 0.3600\n",
      "Iter-16860 train loss: 1.7768 valid loss: 1.9753, valid accuracy: 0.3546\n",
      "Iter-16870 train loss: 1.8055 valid loss: 1.9760, valid accuracy: 0.3600\n",
      "Iter-16880 train loss: 2.3146 valid loss: 1.9760, valid accuracy: 0.3523\n",
      "Iter-16890 train loss: 2.0037 valid loss: 1.9758, valid accuracy: 0.3562\n",
      "Iter-16900 train loss: 1.9651 valid loss: 1.9769, valid accuracy: 0.3554\n",
      "Iter-16910 train loss: 2.1858 valid loss: 1.9762, valid accuracy: 0.3554\n",
      "Iter-16920 train loss: 1.6937 valid loss: 1.9751, valid accuracy: 0.3615\n",
      "Iter-16930 train loss: 1.8328 valid loss: 1.9735, valid accuracy: 0.3577\n",
      "Iter-16940 train loss: 1.9560 valid loss: 1.9738, valid accuracy: 0.3608\n",
      "Iter-16950 train loss: 1.9979 valid loss: 1.9741, valid accuracy: 0.3585\n",
      "Iter-16960 train loss: 1.6383 valid loss: 1.9748, valid accuracy: 0.3523\n",
      "Iter-16970 train loss: 1.9188 valid loss: 1.9745, valid accuracy: 0.3538\n",
      "Iter-16980 train loss: 1.9723 valid loss: 1.9752, valid accuracy: 0.3531\n",
      "Iter-16990 train loss: 1.9049 valid loss: 1.9738, valid accuracy: 0.3585\n",
      "Iter-17000 train loss: 1.7848 valid loss: 1.9736, valid accuracy: 0.3577\n",
      "Iter-17010 train loss: 1.7155 valid loss: 1.9742, valid accuracy: 0.3585\n",
      "Iter-17020 train loss: 1.6735 valid loss: 1.9754, valid accuracy: 0.3554\n",
      "Iter-17030 train loss: 2.0168 valid loss: 1.9764, valid accuracy: 0.3554\n",
      "Iter-17040 train loss: 2.0914 valid loss: 1.9760, valid accuracy: 0.3538\n",
      "Iter-17050 train loss: 1.6710 valid loss: 1.9760, valid accuracy: 0.3546\n",
      "Iter-17060 train loss: 1.7638 valid loss: 1.9778, valid accuracy: 0.3531\n",
      "Iter-17070 train loss: 1.7383 valid loss: 1.9765, valid accuracy: 0.3554\n",
      "Iter-17080 train loss: 1.5947 valid loss: 1.9779, valid accuracy: 0.3569\n",
      "Iter-17090 train loss: 1.7172 valid loss: 1.9757, valid accuracy: 0.3531\n",
      "Iter-17100 train loss: 1.8279 valid loss: 1.9750, valid accuracy: 0.3538\n",
      "Iter-17110 train loss: 2.0795 valid loss: 1.9739, valid accuracy: 0.3592\n",
      "Iter-17120 train loss: 1.7711 valid loss: 1.9744, valid accuracy: 0.3592\n",
      "Iter-17130 train loss: 1.9893 valid loss: 1.9757, valid accuracy: 0.3569\n",
      "Iter-17140 train loss: 1.7750 valid loss: 1.9746, valid accuracy: 0.3569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-17150 train loss: 1.6362 valid loss: 1.9752, valid accuracy: 0.3600\n",
      "Iter-17160 train loss: 1.9297 valid loss: 1.9742, valid accuracy: 0.3592\n",
      "Iter-17170 train loss: 1.7385 valid loss: 1.9747, valid accuracy: 0.3615\n",
      "Iter-17180 train loss: 2.0187 valid loss: 1.9762, valid accuracy: 0.3585\n",
      "Iter-17190 train loss: 2.0489 valid loss: 1.9761, valid accuracy: 0.3615\n",
      "Iter-17200 train loss: 1.9799 valid loss: 1.9760, valid accuracy: 0.3600\n",
      "Iter-17210 train loss: 1.7892 valid loss: 1.9762, valid accuracy: 0.3554\n",
      "Iter-17220 train loss: 1.6109 valid loss: 1.9755, valid accuracy: 0.3562\n",
      "Iter-17230 train loss: 1.9766 valid loss: 1.9756, valid accuracy: 0.3608\n",
      "Iter-17240 train loss: 2.1387 valid loss: 1.9759, valid accuracy: 0.3585\n",
      "Iter-17250 train loss: 1.9255 valid loss: 1.9770, valid accuracy: 0.3577\n",
      "Iter-17260 train loss: 1.6543 valid loss: 1.9771, valid accuracy: 0.3546\n",
      "Iter-17270 train loss: 1.7596 valid loss: 1.9783, valid accuracy: 0.3523\n",
      "Iter-17280 train loss: 1.8960 valid loss: 1.9787, valid accuracy: 0.3515\n",
      "Iter-17290 train loss: 1.7010 valid loss: 1.9771, valid accuracy: 0.3569\n",
      "Iter-17300 train loss: 1.9489 valid loss: 1.9762, valid accuracy: 0.3569\n",
      "Iter-17310 train loss: 1.7517 valid loss: 1.9748, valid accuracy: 0.3546\n",
      "Iter-17320 train loss: 1.8667 valid loss: 1.9739, valid accuracy: 0.3592\n",
      "Iter-17330 train loss: 1.9115 valid loss: 1.9733, valid accuracy: 0.3577\n",
      "Iter-17340 train loss: 1.8401 valid loss: 1.9735, valid accuracy: 0.3546\n",
      "Iter-17350 train loss: 1.6573 valid loss: 1.9739, valid accuracy: 0.3577\n",
      "Iter-17360 train loss: 1.7977 valid loss: 1.9747, valid accuracy: 0.3592\n",
      "Iter-17370 train loss: 1.9962 valid loss: 1.9728, valid accuracy: 0.3538\n",
      "Iter-17380 train loss: 1.8632 valid loss: 1.9725, valid accuracy: 0.3569\n",
      "Iter-17390 train loss: 2.0722 valid loss: 1.9718, valid accuracy: 0.3531\n",
      "Iter-17400 train loss: 1.7418 valid loss: 1.9708, valid accuracy: 0.3515\n",
      "Iter-17410 train loss: 1.5900 valid loss: 1.9714, valid accuracy: 0.3554\n",
      "Iter-17420 train loss: 2.0108 valid loss: 1.9708, valid accuracy: 0.3508\n",
      "Iter-17430 train loss: 2.1070 valid loss: 1.9714, valid accuracy: 0.3546\n",
      "Iter-17440 train loss: 1.7593 valid loss: 1.9723, valid accuracy: 0.3562\n",
      "Iter-17450 train loss: 1.6314 valid loss: 1.9718, valid accuracy: 0.3577\n",
      "Iter-17460 train loss: 1.9493 valid loss: 1.9738, valid accuracy: 0.3546\n",
      "Iter-17470 train loss: 1.6229 valid loss: 1.9725, valid accuracy: 0.3577\n",
      "Iter-17480 train loss: 1.6675 valid loss: 1.9742, valid accuracy: 0.3562\n",
      "Iter-17490 train loss: 1.6552 valid loss: 1.9730, valid accuracy: 0.3600\n",
      "Iter-17500 train loss: 1.6635 valid loss: 1.9729, valid accuracy: 0.3577\n",
      "Iter-17510 train loss: 2.0595 valid loss: 1.9703, valid accuracy: 0.3554\n",
      "Iter-17520 train loss: 1.9012 valid loss: 1.9709, valid accuracy: 0.3562\n",
      "Iter-17530 train loss: 2.0657 valid loss: 1.9710, valid accuracy: 0.3600\n",
      "Iter-17540 train loss: 2.0107 valid loss: 1.9700, valid accuracy: 0.3569\n",
      "Iter-17550 train loss: 1.9494 valid loss: 1.9713, valid accuracy: 0.3577\n",
      "Iter-17560 train loss: 1.7294 valid loss: 1.9718, valid accuracy: 0.3562\n",
      "Iter-17570 train loss: 1.7692 valid loss: 1.9720, valid accuracy: 0.3546\n",
      "Iter-17580 train loss: 1.9684 valid loss: 1.9736, valid accuracy: 0.3531\n",
      "Iter-17590 train loss: 1.9314 valid loss: 1.9737, valid accuracy: 0.3546\n",
      "Iter-17600 train loss: 1.8482 valid loss: 1.9742, valid accuracy: 0.3546\n",
      "Iter-17610 train loss: 1.8818 valid loss: 1.9731, valid accuracy: 0.3569\n",
      "Iter-17620 train loss: 1.8554 valid loss: 1.9735, valid accuracy: 0.3523\n",
      "Iter-17630 train loss: 1.7925 valid loss: 1.9726, valid accuracy: 0.3546\n",
      "Iter-17640 train loss: 1.7128 valid loss: 1.9726, valid accuracy: 0.3515\n",
      "Iter-17650 train loss: 1.9842 valid loss: 1.9750, valid accuracy: 0.3469\n",
      "Iter-17660 train loss: 1.9288 valid loss: 1.9750, valid accuracy: 0.3508\n",
      "Iter-17670 train loss: 1.9020 valid loss: 1.9735, valid accuracy: 0.3477\n",
      "Iter-17680 train loss: 2.0465 valid loss: 1.9726, valid accuracy: 0.3492\n",
      "Iter-17690 train loss: 2.2526 valid loss: 1.9729, valid accuracy: 0.3477\n",
      "Iter-17700 train loss: 2.0110 valid loss: 1.9726, valid accuracy: 0.3477\n",
      "Iter-17710 train loss: 1.7785 valid loss: 1.9734, valid accuracy: 0.3477\n",
      "Iter-17720 train loss: 1.9666 valid loss: 1.9742, valid accuracy: 0.3477\n",
      "Iter-17730 train loss: 1.9217 valid loss: 1.9728, valid accuracy: 0.3515\n",
      "Iter-17740 train loss: 1.8026 valid loss: 1.9716, valid accuracy: 0.3523\n",
      "Iter-17750 train loss: 1.7388 valid loss: 1.9711, valid accuracy: 0.3515\n",
      "Iter-17760 train loss: 1.8086 valid loss: 1.9705, valid accuracy: 0.3523\n",
      "Iter-17770 train loss: 1.8933 valid loss: 1.9706, valid accuracy: 0.3562\n",
      "Iter-17780 train loss: 2.0453 valid loss: 1.9711, valid accuracy: 0.3585\n",
      "Iter-17790 train loss: 1.9031 valid loss: 1.9715, valid accuracy: 0.3592\n",
      "Iter-17800 train loss: 2.0075 valid loss: 1.9715, valid accuracy: 0.3577\n",
      "Iter-17810 train loss: 1.9670 valid loss: 1.9730, valid accuracy: 0.3569\n",
      "Iter-17820 train loss: 1.6301 valid loss: 1.9727, valid accuracy: 0.3546\n",
      "Iter-17830 train loss: 1.8176 valid loss: 1.9719, valid accuracy: 0.3546\n",
      "Iter-17840 train loss: 1.6933 valid loss: 1.9706, valid accuracy: 0.3585\n",
      "Iter-17850 train loss: 1.8089 valid loss: 1.9712, valid accuracy: 0.3592\n",
      "Iter-17860 train loss: 2.0042 valid loss: 1.9697, valid accuracy: 0.3577\n",
      "Iter-17870 train loss: 1.8526 valid loss: 1.9700, valid accuracy: 0.3592\n",
      "Iter-17880 train loss: 1.6842 valid loss: 1.9709, valid accuracy: 0.3554\n",
      "Iter-17890 train loss: 1.8759 valid loss: 1.9711, valid accuracy: 0.3569\n",
      "Iter-17900 train loss: 1.7446 valid loss: 1.9703, valid accuracy: 0.3585\n",
      "Iter-17910 train loss: 1.8970 valid loss: 1.9689, valid accuracy: 0.3608\n",
      "Iter-17920 train loss: 1.9280 valid loss: 1.9694, valid accuracy: 0.3585\n",
      "Iter-17930 train loss: 1.8843 valid loss: 1.9707, valid accuracy: 0.3562\n",
      "Iter-17940 train loss: 1.8728 valid loss: 1.9711, valid accuracy: 0.3577\n",
      "Iter-17950 train loss: 1.8728 valid loss: 1.9707, valid accuracy: 0.3554\n",
      "Iter-17960 train loss: 1.6205 valid loss: 1.9717, valid accuracy: 0.3554\n",
      "Iter-17970 train loss: 1.6802 valid loss: 1.9720, valid accuracy: 0.3562\n",
      "Iter-17980 train loss: 1.6665 valid loss: 1.9738, valid accuracy: 0.3538\n",
      "Iter-17990 train loss: 1.8228 valid loss: 1.9735, valid accuracy: 0.3531\n",
      "Iter-18000 train loss: 1.8305 valid loss: 1.9725, valid accuracy: 0.3523\n",
      "Iter-18010 train loss: 1.7453 valid loss: 1.9728, valid accuracy: 0.3508\n",
      "Iter-18020 train loss: 1.8101 valid loss: 1.9735, valid accuracy: 0.3469\n",
      "Iter-18030 train loss: 1.9381 valid loss: 1.9733, valid accuracy: 0.3546\n",
      "Iter-18040 train loss: 1.9145 valid loss: 1.9740, valid accuracy: 0.3531\n",
      "Iter-18050 train loss: 1.6123 valid loss: 1.9736, valid accuracy: 0.3546\n",
      "Iter-18060 train loss: 1.8450 valid loss: 1.9739, valid accuracy: 0.3523\n",
      "Iter-18070 train loss: 1.8261 valid loss: 1.9720, valid accuracy: 0.3508\n",
      "Iter-18080 train loss: 1.7563 valid loss: 1.9732, valid accuracy: 0.3477\n",
      "Iter-18090 train loss: 2.1727 valid loss: 1.9721, valid accuracy: 0.3500\n",
      "Iter-18100 train loss: 2.0407 valid loss: 1.9713, valid accuracy: 0.3492\n",
      "Iter-18110 train loss: 1.7326 valid loss: 1.9705, valid accuracy: 0.3531\n",
      "Iter-18120 train loss: 2.1057 valid loss: 1.9716, valid accuracy: 0.3469\n",
      "Iter-18130 train loss: 1.8638 valid loss: 1.9704, valid accuracy: 0.3546\n",
      "Iter-18140 train loss: 2.0599 valid loss: 1.9709, valid accuracy: 0.3538\n",
      "Iter-18150 train loss: 2.1439 valid loss: 1.9696, valid accuracy: 0.3515\n",
      "Iter-18160 train loss: 2.0448 valid loss: 1.9690, valid accuracy: 0.3562\n",
      "Iter-18170 train loss: 1.8327 valid loss: 1.9693, valid accuracy: 0.3554\n",
      "Iter-18180 train loss: 1.7679 valid loss: 1.9691, valid accuracy: 0.3569\n",
      "Iter-18190 train loss: 2.0186 valid loss: 1.9690, valid accuracy: 0.3562\n",
      "Iter-18200 train loss: 1.8256 valid loss: 1.9686, valid accuracy: 0.3577\n",
      "Iter-18210 train loss: 1.9153 valid loss: 1.9689, valid accuracy: 0.3592\n",
      "Iter-18220 train loss: 1.9633 valid loss: 1.9690, valid accuracy: 0.3577\n",
      "Iter-18230 train loss: 1.8312 valid loss: 1.9690, valid accuracy: 0.3623\n",
      "Iter-18240 train loss: 1.6407 valid loss: 1.9689, valid accuracy: 0.3615\n",
      "Iter-18250 train loss: 1.8583 valid loss: 1.9698, valid accuracy: 0.3554\n",
      "Iter-18260 train loss: 1.9698 valid loss: 1.9707, valid accuracy: 0.3577\n",
      "Iter-18270 train loss: 1.7311 valid loss: 1.9705, valid accuracy: 0.3608\n",
      "Iter-18280 train loss: 1.5998 valid loss: 1.9718, valid accuracy: 0.3569\n",
      "Iter-18290 train loss: 2.0007 valid loss: 1.9714, valid accuracy: 0.3585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-18300 train loss: 1.9513 valid loss: 1.9723, valid accuracy: 0.3585\n",
      "Iter-18310 train loss: 1.6423 valid loss: 1.9711, valid accuracy: 0.3554\n",
      "Iter-18320 train loss: 1.5572 valid loss: 1.9708, valid accuracy: 0.3615\n",
      "Iter-18330 train loss: 1.8716 valid loss: 1.9706, valid accuracy: 0.3608\n",
      "Iter-18340 train loss: 1.9807 valid loss: 1.9710, valid accuracy: 0.3569\n",
      "Iter-18350 train loss: 2.1051 valid loss: 1.9716, valid accuracy: 0.3577\n",
      "Iter-18360 train loss: 1.9767 valid loss: 1.9703, valid accuracy: 0.3623\n",
      "Iter-18370 train loss: 2.1029 valid loss: 1.9712, valid accuracy: 0.3646\n",
      "Iter-18380 train loss: 1.8801 valid loss: 1.9722, valid accuracy: 0.3662\n",
      "Iter-18390 train loss: 1.9301 valid loss: 1.9711, valid accuracy: 0.3623\n",
      "Iter-18400 train loss: 1.7848 valid loss: 1.9707, valid accuracy: 0.3669\n",
      "Iter-18410 train loss: 2.0114 valid loss: 1.9705, valid accuracy: 0.3638\n",
      "Iter-18420 train loss: 1.7632 valid loss: 1.9700, valid accuracy: 0.3608\n",
      "Iter-18430 train loss: 1.8839 valid loss: 1.9707, valid accuracy: 0.3638\n",
      "Iter-18440 train loss: 1.9430 valid loss: 1.9711, valid accuracy: 0.3569\n",
      "Iter-18450 train loss: 1.5247 valid loss: 1.9717, valid accuracy: 0.3569\n",
      "Iter-18460 train loss: 2.1412 valid loss: 1.9729, valid accuracy: 0.3554\n",
      "Iter-18470 train loss: 1.9772 valid loss: 1.9744, valid accuracy: 0.3546\n",
      "Iter-18480 train loss: 2.0630 valid loss: 1.9731, valid accuracy: 0.3585\n",
      "Iter-18490 train loss: 1.8242 valid loss: 1.9727, valid accuracy: 0.3577\n",
      "Iter-18500 train loss: 2.0604 valid loss: 1.9734, valid accuracy: 0.3592\n",
      "Iter-18510 train loss: 1.6503 valid loss: 1.9748, valid accuracy: 0.3546\n",
      "Iter-18520 train loss: 1.8139 valid loss: 1.9757, valid accuracy: 0.3562\n",
      "Iter-18530 train loss: 1.7572 valid loss: 1.9738, valid accuracy: 0.3592\n",
      "Iter-18540 train loss: 2.0509 valid loss: 1.9747, valid accuracy: 0.3577\n",
      "Iter-18550 train loss: 2.1795 valid loss: 1.9741, valid accuracy: 0.3569\n",
      "Iter-18560 train loss: 1.7674 valid loss: 1.9734, valid accuracy: 0.3562\n",
      "Iter-18570 train loss: 1.8252 valid loss: 1.9740, valid accuracy: 0.3585\n",
      "Iter-18580 train loss: 1.7201 valid loss: 1.9713, valid accuracy: 0.3638\n",
      "Iter-18590 train loss: 2.1341 valid loss: 1.9715, valid accuracy: 0.3623\n",
      "Iter-18600 train loss: 1.9056 valid loss: 1.9703, valid accuracy: 0.3646\n",
      "Iter-18610 train loss: 1.6159 valid loss: 1.9704, valid accuracy: 0.3638\n",
      "Iter-18620 train loss: 2.1607 valid loss: 1.9702, valid accuracy: 0.3592\n",
      "Iter-18630 train loss: 1.7982 valid loss: 1.9702, valid accuracy: 0.3600\n",
      "Iter-18640 train loss: 1.8500 valid loss: 1.9704, valid accuracy: 0.3569\n",
      "Iter-18650 train loss: 1.9340 valid loss: 1.9700, valid accuracy: 0.3569\n",
      "Iter-18660 train loss: 1.8111 valid loss: 1.9688, valid accuracy: 0.3600\n",
      "Iter-18670 train loss: 1.9075 valid loss: 1.9673, valid accuracy: 0.3523\n",
      "Iter-18680 train loss: 1.9834 valid loss: 1.9678, valid accuracy: 0.3569\n",
      "Iter-18690 train loss: 1.9502 valid loss: 1.9683, valid accuracy: 0.3562\n",
      "Iter-18700 train loss: 1.8352 valid loss: 1.9700, valid accuracy: 0.3531\n",
      "Iter-18710 train loss: 1.7328 valid loss: 1.9701, valid accuracy: 0.3531\n",
      "Iter-18720 train loss: 1.7442 valid loss: 1.9694, valid accuracy: 0.3523\n",
      "Iter-18730 train loss: 1.8577 valid loss: 1.9669, valid accuracy: 0.3600\n",
      "Iter-18740 train loss: 1.7432 valid loss: 1.9683, valid accuracy: 0.3546\n",
      "Iter-18750 train loss: 1.8304 valid loss: 1.9682, valid accuracy: 0.3531\n",
      "Iter-18760 train loss: 1.9137 valid loss: 1.9687, valid accuracy: 0.3546\n",
      "Iter-18770 train loss: 1.8680 valid loss: 1.9685, valid accuracy: 0.3562\n",
      "Iter-18780 train loss: 1.9960 valid loss: 1.9690, valid accuracy: 0.3569\n",
      "Iter-18790 train loss: 1.9331 valid loss: 1.9692, valid accuracy: 0.3531\n",
      "Iter-18800 train loss: 1.8491 valid loss: 1.9688, valid accuracy: 0.3585\n",
      "Iter-18810 train loss: 1.9118 valid loss: 1.9689, valid accuracy: 0.3577\n",
      "Iter-18820 train loss: 1.9025 valid loss: 1.9712, valid accuracy: 0.3546\n",
      "Iter-18830 train loss: 1.9302 valid loss: 1.9706, valid accuracy: 0.3562\n",
      "Iter-18840 train loss: 1.8708 valid loss: 1.9715, valid accuracy: 0.3546\n",
      "Iter-18850 train loss: 1.6555 valid loss: 1.9698, valid accuracy: 0.3500\n",
      "Iter-18860 train loss: 1.7096 valid loss: 1.9709, valid accuracy: 0.3515\n",
      "Iter-18870 train loss: 1.6917 valid loss: 1.9696, valid accuracy: 0.3592\n",
      "Iter-18880 train loss: 1.7924 valid loss: 1.9690, valid accuracy: 0.3585\n",
      "Iter-18890 train loss: 2.0253 valid loss: 1.9697, valid accuracy: 0.3592\n",
      "Iter-18900 train loss: 1.8201 valid loss: 1.9692, valid accuracy: 0.3592\n",
      "Iter-18910 train loss: 1.7416 valid loss: 1.9707, valid accuracy: 0.3585\n",
      "Iter-18920 train loss: 1.6677 valid loss: 1.9696, valid accuracy: 0.3554\n",
      "Iter-18930 train loss: 1.6743 valid loss: 1.9686, valid accuracy: 0.3592\n",
      "Iter-18940 train loss: 2.1273 valid loss: 1.9688, valid accuracy: 0.3600\n",
      "Iter-18950 train loss: 1.8616 valid loss: 1.9695, valid accuracy: 0.3577\n",
      "Iter-18960 train loss: 1.7696 valid loss: 1.9696, valid accuracy: 0.3577\n",
      "Iter-18970 train loss: 2.2129 valid loss: 1.9700, valid accuracy: 0.3577\n",
      "Iter-18980 train loss: 1.7880 valid loss: 1.9700, valid accuracy: 0.3608\n",
      "Iter-18990 train loss: 1.7820 valid loss: 1.9707, valid accuracy: 0.3585\n",
      "Iter-19000 train loss: 1.9220 valid loss: 1.9711, valid accuracy: 0.3554\n",
      "Iter-19010 train loss: 1.7051 valid loss: 1.9700, valid accuracy: 0.3600\n",
      "Iter-19020 train loss: 1.8926 valid loss: 1.9694, valid accuracy: 0.3585\n",
      "Iter-19030 train loss: 2.0612 valid loss: 1.9701, valid accuracy: 0.3600\n",
      "Iter-19040 train loss: 1.8052 valid loss: 1.9710, valid accuracy: 0.3585\n",
      "Iter-19050 train loss: 1.8810 valid loss: 1.9702, valid accuracy: 0.3615\n",
      "Iter-19060 train loss: 1.7926 valid loss: 1.9692, valid accuracy: 0.3569\n",
      "Iter-19070 train loss: 1.7710 valid loss: 1.9695, valid accuracy: 0.3615\n",
      "Iter-19080 train loss: 1.8269 valid loss: 1.9716, valid accuracy: 0.3569\n",
      "Iter-19090 train loss: 1.8229 valid loss: 1.9714, valid accuracy: 0.3554\n",
      "Iter-19100 train loss: 1.9639 valid loss: 1.9713, valid accuracy: 0.3562\n",
      "Iter-19110 train loss: 1.8185 valid loss: 1.9709, valid accuracy: 0.3562\n",
      "Iter-19120 train loss: 1.8827 valid loss: 1.9704, valid accuracy: 0.3592\n",
      "Iter-19130 train loss: 1.6922 valid loss: 1.9695, valid accuracy: 0.3546\n",
      "Iter-19140 train loss: 1.7690 valid loss: 1.9696, valid accuracy: 0.3562\n",
      "Iter-19150 train loss: 1.6891 valid loss: 1.9695, valid accuracy: 0.3562\n",
      "Iter-19160 train loss: 1.8181 valid loss: 1.9693, valid accuracy: 0.3577\n",
      "Iter-19170 train loss: 1.8847 valid loss: 1.9684, valid accuracy: 0.3569\n",
      "Iter-19180 train loss: 2.1608 valid loss: 1.9698, valid accuracy: 0.3592\n",
      "Iter-19190 train loss: 1.7459 valid loss: 1.9710, valid accuracy: 0.3562\n",
      "Iter-19200 train loss: 1.8057 valid loss: 1.9702, valid accuracy: 0.3585\n",
      "Iter-19210 train loss: 1.7658 valid loss: 1.9714, valid accuracy: 0.3538\n",
      "Iter-19220 train loss: 1.8157 valid loss: 1.9717, valid accuracy: 0.3562\n",
      "Iter-19230 train loss: 1.5732 valid loss: 1.9726, valid accuracy: 0.3554\n",
      "Iter-19240 train loss: 2.0607 valid loss: 1.9723, valid accuracy: 0.3569\n",
      "Iter-19250 train loss: 1.8124 valid loss: 1.9721, valid accuracy: 0.3546\n",
      "Iter-19260 train loss: 1.9028 valid loss: 1.9717, valid accuracy: 0.3554\n",
      "Iter-19270 train loss: 1.7093 valid loss: 1.9713, valid accuracy: 0.3508\n",
      "Iter-19280 train loss: 1.9636 valid loss: 1.9750, valid accuracy: 0.3531\n",
      "Iter-19290 train loss: 1.8575 valid loss: 1.9737, valid accuracy: 0.3569\n",
      "Iter-19300 train loss: 1.6928 valid loss: 1.9736, valid accuracy: 0.3515\n",
      "Iter-19310 train loss: 1.6081 valid loss: 1.9730, valid accuracy: 0.3538\n",
      "Iter-19320 train loss: 1.5240 valid loss: 1.9732, valid accuracy: 0.3515\n",
      "Iter-19330 train loss: 1.6154 valid loss: 1.9744, valid accuracy: 0.3554\n",
      "Iter-19340 train loss: 1.7635 valid loss: 1.9744, valid accuracy: 0.3523\n",
      "Iter-19350 train loss: 1.7808 valid loss: 1.9746, valid accuracy: 0.3515\n",
      "Iter-19360 train loss: 1.8078 valid loss: 1.9725, valid accuracy: 0.3515\n",
      "Iter-19370 train loss: 1.7247 valid loss: 1.9734, valid accuracy: 0.3531\n",
      "Iter-19380 train loss: 1.7934 valid loss: 1.9729, valid accuracy: 0.3538\n",
      "Iter-19390 train loss: 2.0516 valid loss: 1.9713, valid accuracy: 0.3531\n",
      "Iter-19400 train loss: 1.8547 valid loss: 1.9710, valid accuracy: 0.3554\n",
      "Iter-19410 train loss: 1.7008 valid loss: 1.9707, valid accuracy: 0.3531\n",
      "Iter-19420 train loss: 1.7124 valid loss: 1.9723, valid accuracy: 0.3562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-19430 train loss: 1.7181 valid loss: 1.9715, valid accuracy: 0.3562\n",
      "Iter-19440 train loss: 2.0235 valid loss: 1.9724, valid accuracy: 0.3538\n",
      "Iter-19450 train loss: 1.6930 valid loss: 1.9731, valid accuracy: 0.3523\n",
      "Iter-19460 train loss: 1.9988 valid loss: 1.9728, valid accuracy: 0.3546\n",
      "Iter-19470 train loss: 1.8684 valid loss: 1.9723, valid accuracy: 0.3546\n",
      "Iter-19480 train loss: 1.7398 valid loss: 1.9715, valid accuracy: 0.3523\n",
      "Iter-19490 train loss: 1.7179 valid loss: 1.9705, valid accuracy: 0.3546\n",
      "Iter-19500 train loss: 1.7986 valid loss: 1.9713, valid accuracy: 0.3492\n",
      "Iter-19510 train loss: 1.7086 valid loss: 1.9712, valid accuracy: 0.3485\n",
      "Iter-19520 train loss: 1.9995 valid loss: 1.9708, valid accuracy: 0.3485\n",
      "Iter-19530 train loss: 1.7602 valid loss: 1.9698, valid accuracy: 0.3546\n",
      "Iter-19540 train loss: 1.8940 valid loss: 1.9695, valid accuracy: 0.3531\n",
      "Iter-19550 train loss: 1.7414 valid loss: 1.9700, valid accuracy: 0.3500\n",
      "Iter-19560 train loss: 2.0971 valid loss: 1.9707, valid accuracy: 0.3538\n",
      "Iter-19570 train loss: 1.8977 valid loss: 1.9704, valid accuracy: 0.3585\n",
      "Iter-19580 train loss: 2.0335 valid loss: 1.9704, valid accuracy: 0.3600\n",
      "Iter-19590 train loss: 1.7924 valid loss: 1.9701, valid accuracy: 0.3554\n",
      "Iter-19600 train loss: 1.9393 valid loss: 1.9699, valid accuracy: 0.3546\n",
      "Iter-19610 train loss: 1.6248 valid loss: 1.9700, valid accuracy: 0.3623\n",
      "Iter-19620 train loss: 2.0037 valid loss: 1.9698, valid accuracy: 0.3608\n",
      "Iter-19630 train loss: 1.5699 valid loss: 1.9709, valid accuracy: 0.3592\n",
      "Iter-19640 train loss: 1.7383 valid loss: 1.9712, valid accuracy: 0.3592\n",
      "Iter-19650 train loss: 1.5710 valid loss: 1.9716, valid accuracy: 0.3592\n",
      "Iter-19660 train loss: 1.7937 valid loss: 1.9688, valid accuracy: 0.3585\n",
      "Iter-19670 train loss: 1.9276 valid loss: 1.9686, valid accuracy: 0.3608\n",
      "Iter-19680 train loss: 1.9343 valid loss: 1.9697, valid accuracy: 0.3600\n",
      "Iter-19690 train loss: 1.7776 valid loss: 1.9694, valid accuracy: 0.3615\n",
      "Iter-19700 train loss: 1.6674 valid loss: 1.9692, valid accuracy: 0.3623\n",
      "Iter-19710 train loss: 1.8028 valid loss: 1.9687, valid accuracy: 0.3585\n",
      "Iter-19720 train loss: 1.9074 valid loss: 1.9679, valid accuracy: 0.3592\n",
      "Iter-19730 train loss: 1.8061 valid loss: 1.9680, valid accuracy: 0.3569\n",
      "Iter-19740 train loss: 1.6269 valid loss: 1.9684, valid accuracy: 0.3608\n",
      "Iter-19750 train loss: 1.7381 valid loss: 1.9686, valid accuracy: 0.3615\n",
      "Iter-19760 train loss: 1.7278 valid loss: 1.9684, valid accuracy: 0.3631\n",
      "Iter-19770 train loss: 1.8789 valid loss: 1.9685, valid accuracy: 0.3577\n",
      "Iter-19780 train loss: 1.8970 valid loss: 1.9694, valid accuracy: 0.3608\n",
      "Iter-19790 train loss: 1.6930 valid loss: 1.9685, valid accuracy: 0.3608\n",
      "Iter-19800 train loss: 2.1259 valid loss: 1.9701, valid accuracy: 0.3623\n",
      "Iter-19810 train loss: 1.7173 valid loss: 1.9693, valid accuracy: 0.3608\n",
      "Iter-19820 train loss: 2.0325 valid loss: 1.9684, valid accuracy: 0.3585\n",
      "Iter-19830 train loss: 1.7895 valid loss: 1.9691, valid accuracy: 0.3592\n",
      "Iter-19840 train loss: 2.1130 valid loss: 1.9689, valid accuracy: 0.3585\n",
      "Iter-19850 train loss: 1.8427 valid loss: 1.9698, valid accuracy: 0.3569\n",
      "Iter-19860 train loss: 1.8732 valid loss: 1.9707, valid accuracy: 0.3585\n",
      "Iter-19870 train loss: 1.7025 valid loss: 1.9708, valid accuracy: 0.3592\n",
      "Iter-19880 train loss: 1.8663 valid loss: 1.9710, valid accuracy: 0.3592\n",
      "Iter-19890 train loss: 1.9862 valid loss: 1.9692, valid accuracy: 0.3585\n",
      "Iter-19900 train loss: 1.9795 valid loss: 1.9681, valid accuracy: 0.3592\n",
      "Iter-19910 train loss: 1.7736 valid loss: 1.9704, valid accuracy: 0.3592\n",
      "Iter-19920 train loss: 1.9939 valid loss: 1.9710, valid accuracy: 0.3569\n",
      "Iter-19930 train loss: 2.0111 valid loss: 1.9701, valid accuracy: 0.3523\n",
      "Iter-19940 train loss: 1.9420 valid loss: 1.9692, valid accuracy: 0.3500\n",
      "Iter-19950 train loss: 1.9372 valid loss: 1.9675, valid accuracy: 0.3508\n",
      "Iter-19960 train loss: 1.6183 valid loss: 1.9667, valid accuracy: 0.3546\n",
      "Iter-19970 train loss: 1.8236 valid loss: 1.9660, valid accuracy: 0.3562\n",
      "Iter-19980 train loss: 1.8466 valid loss: 1.9663, valid accuracy: 0.3554\n",
      "Iter-19990 train loss: 1.9371 valid loss: 1.9675, valid accuracy: 0.3577\n",
      "Iter-20000 train loss: 1.8660 valid loss: 1.9668, valid accuracy: 0.3569\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 20000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCEICIspdEAQhARHlLgiCkID8P3z7SDixurAaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1718751978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nK6VUANGkr5RSAeT/AUhsJH7oof3JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1718751828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'Blues',\n",
       " 'Country',\n",
       " 'Electronic',\n",
       " 'Folk',\n",
       " 'International',\n",
       " 'Jazz',\n",
       " 'Latin',\n",
       " 'New_Age',\n",
       " 'Pop_Rock',\n",
       " 'Rap',\n",
       " 'Reggae',\n",
       " 'RnB',\n",
       " 'Vocal']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heading = labels_keys_sorted.copy()\n",
    "heading.insert(0, 'Id')\n",
    "heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 13),\n",
       " (10400, 26),\n",
       " (10400, 13),\n",
       " (10400, 14),\n",
       "    Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       " 0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       " \n",
       "    New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       " 0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  )"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_logits = nn.test(X_test)\n",
    "y_prob = l.softmax(y_logits)\n",
    "y_prob.shape, X_test.shape, y_logits.shape, test_y_sample.shape, test_y_sample[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "for Id, pred in enumerate(y_prob):\n",
    "#     print(Id+1, *pred)\n",
    "    pred_list.append([Id+1, *pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_file = open(file='prediction.csv', mode='w')\n",
    "pred_file.write('\\n') # because of the previous line        \n",
    "\n",
    "for idx in range(len(heading)):\n",
    "    if idx < len(heading) - 1:\n",
    "        pred_file.write(heading[idx] + ',')\n",
    "    else:\n",
    "        pred_file.write(heading[idx] + '\\n')        \n",
    "\n",
    "# len(test), test[0]\n",
    "# for key in test:\n",
    "for i in range(len(pred_list)): # rows\n",
    "    for j in range(len(pred_list[i])): # cols\n",
    "        if j < (len(pred_list[i]) - 1):\n",
    "            pred_file.write(str(pred_list[i][j]))\n",
    "            pred_file.write(',')\n",
    "        else: # last item before starting a new line\n",
    "            pred_file.write(str(pred_list[i][j]) + '\\n')        \n",
    "\n",
    "# pred_file.write(-',')\n",
    "pred_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.010647</td>\n",
       "      <td>0.004896</td>\n",
       "      <td>0.012471</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.016096</td>\n",
       "      <td>0.017254</td>\n",
       "      <td>0.046172</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.551503</td>\n",
       "      <td>0.264051</td>\n",
       "      <td>0.059004</td>\n",
       "      <td>0.011651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.030747</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>0.013305</td>\n",
       "      <td>0.045791</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.164911</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.006409</td>\n",
       "      <td>0.098120</td>\n",
       "      <td>0.519561</td>\n",
       "      <td>0.047751</td>\n",
       "      <td>0.042415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.004754</td>\n",
       "      <td>0.027937</td>\n",
       "      <td>0.003126</td>\n",
       "      <td>0.029869</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.030980</td>\n",
       "      <td>0.161440</td>\n",
       "      <td>0.603927</td>\n",
       "      <td>0.078159</td>\n",
       "      <td>0.003368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043036</td>\n",
       "      <td>0.037409</td>\n",
       "      <td>0.119196</td>\n",
       "      <td>0.056499</td>\n",
       "      <td>0.044434</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.015681</td>\n",
       "      <td>0.010611</td>\n",
       "      <td>0.018757</td>\n",
       "      <td>0.030824</td>\n",
       "      <td>0.455670</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.110029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.007092</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.015530</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.016419</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>0.013375</td>\n",
       "      <td>0.552937</td>\n",
       "      <td>0.338626</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.000830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id     Blues   Country  Electronic      Folk  International      Jazz  \\\n",
       "0   1  0.010647  0.004896    0.012471  0.002118       0.016096  0.017254   \n",
       "1   2  0.030747  0.012813    0.007274  0.013305       0.045791  0.006773   \n",
       "2   3  0.008564  0.004754    0.027937  0.003126       0.029869  0.002235   \n",
       "3   4  0.043036  0.037409    0.119196  0.056499       0.044434  0.008155   \n",
       "4   5  0.007092  0.003267    0.015530  0.001036       0.016419  0.000737   \n",
       "\n",
       "      Latin   New_Age  Pop_Rock       Rap    Reggae       RnB     Vocal  \n",
       "0  0.046172  0.001188  0.002949  0.551503  0.264051  0.059004  0.011651  \n",
       "1  0.164911  0.004129  0.006409  0.098120  0.519561  0.047751  0.042415  \n",
       "2  0.043096  0.002546  0.030980  0.161440  0.603927  0.078159  0.003368  \n",
       "3  0.015681  0.010611  0.018757  0.030824  0.455670  0.049698  0.110029  \n",
       "4  0.027765  0.000572  0.013375  0.552937  0.338626  0.021814  0.000830  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 14), (10400, 14))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(filepath_or_buffer='prediction.csv').shape, test_y_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
