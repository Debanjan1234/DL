{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "\n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "\n",
    "        cache = (X, Wxh, hprev, Whh, h_cache, y_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, Wxh, hprev, Whh, h_cache, y_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - beta1**(iter))\n",
    "                    r_k_hat = R[layer][key] / (1. - beta2**(iter))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 training loss: 3.4946\n",
      "el\"hbur. t thatuterg an tisln itl andBeosefeorint 9sntorntiriteu her ran rged bac 7 tlK rghrri, peclJ\n",
      "Iter-26 training loss: 3.0234\n",
      "e teutC apur 1e thiIt kigar teye 8onc wgsi. reaterheg idiorofre thtig atha, rhting sun  urr if pe \"kp\n",
      "Iter-39 training loss: 2.9645\n",
      "e'ery  ag0panon-unasil 12ars . wenton Coirlcen, nrhess and wyparetles,ea ak an, the Ifosrc mel iriger\n",
      "Iter-52 training loss: 2.4326\n",
      "edae2ntin ind heo, On oEleo G1hald alite wisalsJans, exp tiunneiun pathedghiched toan, Pote psam 19ut\n",
      "Iter-65 training loss: 2.1925\n",
      "e ores OEti–med irldd bhad (c. inirEche-cexpirotad in slhetangisin tor lae tountmanathe abanmeactrgJe\n",
      "Iter-78 training loss: 2.1726\n",
      "edo Isst7d in cul ir8e3ildotigsexslald 4Der thestrpas tored'natho, te othen Wrs in ha本ntatarse 47frel\n",
      "Iter-91 training loss: 2.3288\n",
      "e nd worcfnkaf Pagelanto0par y on Olis tate Seand. fopmy buterled本trt ar oppptry Iko arglekicorg kurc\n",
      "Iter-104 training loss: 2.0272\n",
      "erlsin Us inotj1nfon Rof Jar Ses hhir Of arint a war emO lrhetof3s Ton foun cha. tete hantame.ex'l of\n",
      "Iter-117 training loss: 2.0144\n",
      "e wereme Erthe wicstury Ewinan-echope the thest man pac and te\"d toma Seal talR on sis. Tokobaes KPa \n",
      "Iter-130 training loss: 2.0056\n",
      "e  arth las ipel ras rrtinitizene Japant Lansin'st. 6 ofr aticlif ins an Japteas cytyuche, the 189f a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7fb631940c18>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 130 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.95 # keep_prob\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERUQBn0iogLCoE9EVEAY9ImICsj/B9o/jCVMGl85AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb6319c2278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
