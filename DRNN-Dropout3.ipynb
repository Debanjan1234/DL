{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "# import impl.utils as util\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "\n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        cache = (X, Wxh, hprev, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, Wxh, hprev, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout): # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, fc_caches, do_caches = [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            fc_caches.append([])\n",
    "            do_caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], fc_cache = self.forward(y, h[layer], self.model[layer])\n",
    "                fc_caches[layer].append(fc_cache)\n",
    "                y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "                do_caches[layer].append(do_cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        caches = (fc_caches, do_caches)\n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        fc_caches, do_caches = caches\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX = self.dropout_backward(dout=dX, cache=do_caches[layer][t])\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], fc_caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c\n",
    "    c = eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            \n",
    "            # Displaying the learn curve or error/loss curve\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - beta1**(iter))\n",
    "                    r_k_hat = R[layer][key] / (1. - beta2**(iter))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + c.eps)\n",
    "                \n",
    "        # Displaying the learn curve or error/loss curve\n",
    "        nn.losses['train2'].append(loss)\n",
    "        \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 training loss: 3.4681\n",
      "e otofetars opath Nsl, dist arlanr oreseitheg  ser G86u141hed etirorpondds rpangohgedto fhindf laheco\n",
      "Iter-26 training loss: 2.8758\n",
      "e makan ard Neru anves D veveltogras len Epi, ian to, tuce Sixlges t04stiedttothe thed Gos Ntyn a ero\n",
      "Iter-39 training loss: 2.7422\n",
      "ero Horgild od the-the Npa815noeonar Warlletiones lal -plios ry1 eter the oky wtith15petolelsely Epon\n",
      "Iter-52 training loss: 2.6435\n",
      "ed th upilx teso Wothed'ry un lihalo aty. 12in Jairn anxs phoge Rane theeirelgr Janrecey eger.kenduko\n",
      "Iter-65 training loss: 3.0128\n",
      "ed Tuund folama% of Jeun the Upatecyl Chertrirly Chert arthe Wifd ronar ir Nad killole Fus fteniad ca\n",
      "Iter-78 training loss: 2.7096\n",
      "erincitiavald wa Weme'rd Japerijepuwar ApenhJary puxrta wery and Empicerota cos Weban fars to the c. \n",
      "Iter-91 training loss: 2.9002\n",
      "er Sas id cbaund an adteelesas 17r Ind Upity ano an cilg inrtenge Nokipentisanbeunge an'mr i f. Japar\n",
      "Iter-104 training loss: 2.9179\n",
      "eoros wats in ciwenttte dify Iur bamst lenl 1, riraggxredtset id aned Iry 6, bes turid Reng in wopeor\n",
      "Iter-117 training loss: 2.8427\n",
      "er anat hinu wincon ariosokt Co本 Empan of Durthwexariloagure bse corsic irtacconetimegar thory iptos \n",
      "Iter-130 training loss: 2.9392\n",
      "eiced weweenidd uncos en ince furhutd, daontoun hor, pen' G37 cory ipine milomomanctevery laromconlea\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f2f20316630>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 130 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.95 # keep_prob\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCKkgKPqEEFJBUPQJIaSCoOgTQkgF8f8BDhs13UBFn1QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2f20279518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
