{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "\n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        cache = (X, Wxh, hprev, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, Wxh, hprev, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout): # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, fc_caches, do_caches = [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            fc_caches.append([])\n",
    "            do_caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], fc_cache = self.forward(y, h[layer], self.model[layer])\n",
    "                fc_caches[layer].append(fc_cache)\n",
    "                y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "                do_caches[layer].append(do_cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        caches = (fc_caches, do_caches)\n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        fc_caches, do_caches = caches\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX = self.dropout_backward(dout=dX, cache=do_caches[layer][t])\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], fc_caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - beta1**(iter))\n",
    "                    r_k_hat = R[layer][key] / (1. - beta2**(iter))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 training loss: 2.6539\n",
      "e watin ho ty Eend JapiJinaupincowDP'r, sy rowesy, Wokeipityee meakeat an mroeted Eetealer6 wLint rip\n",
      "Iter-26 training loss: 2.4981\n",
      "ery ofleerlaagatitoe cidealevendexpanthe rortalhe the Gsean, fe col tnd Sharoflleress e5'sesinori. Ja\n",
      "Iter-39 training loss: 2.3169\n",
      "ercy hore Wan cano the lacbun lar ia the liseancomlorseatiroize in canduloky the Rom an wowigioa unjr\n",
      "Iter-52 training loss: 2.3809\n",
      "ery Frca \n",
      "td foflom ingu gvorto liorolatiliture Ear inlD in 1868 arn.581es vich ex. Japan was Su inse\n",
      "Iter-65 training loss: 2.4738\n",
      "e was the wD. metroedectry in 1815 Nome po Nige the Gril, wrer of Japgriis aI , the diderest Pyd an i\n",
      "Iter-78 training loss: 2.1007\n",
      "e wor abanes aakely. Nake the lowely Inred in's ind Waniokere purd ea namat medet arkiy (Japan asked \n",
      "Iter-91 training loss: 2.1758\n",
      "ed eatirn\"rerol Chice Seas red lhisemen, and the 47lary and aggEer hoflom of 19Pirectiin, wEmbenededn\n",
      "Iter-104 training loss: 2.2374\n",
      "e nics. fagloty fat sy Srake, thin'se Wed Tira piro anatlics 45 incas seat remedce In th 1868 itd is \n",
      "Iter-117 training loss: 2.3096\n",
      "e necthicte lects witar tud Japate wer, rh. 9ea of Chirefth With 1937g the the cech duch 1968, an an \n",
      "Iter-130 training loss: 2.0579\n",
      "e nfesuntsed Japan an ceary hisn, of ghe. adthe the weghar of Hegu Chasitrr likuth Eiclarsthe gitan t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7fb61cadc828>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 130 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.95 # keep_prob\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xu8VWP+B/DP99QppTpSdL+QKESXKVHpuEUZGaRMxnUm\nDTK5/GQ0TPHDD+MaQi5RSDPGJKPkkkOFRDWloiRKN6XLUOn6/f3x7GWtvfZae6+999q3sz/v12u/\n9tprr/Ws56xzznc961nPRVQVRERUHEpynQEiIsoeBn0ioiLCoE9EVEQY9ImIigiDPhFREWHQJyIq\nIoGCvoh8IyL/EZF5IvKJzzajRGSZiMwXkfbhZpOIiMJQNeB2+wCUq+pmry9FpDeAVqraWkSOA/AE\ngK4h5ZGIiEIStHpHEmx7NoBxAKCqswGUiUiDNPNGREQhCxr0FcDbIjJHRAZ5fN8EwCrH59WRdURE\nlEeCVu90U9W1InIQTPBfoqozM5kxIiIKX6Cgr6prI+8bRORfALoAcAb91QCaOT43jayLIiIc6IeI\nKAWqKmGkk7B6R0RqikityPL+AHoB+Ny12WQAF0e26Qpgi6qu90pPVflSxYgRI3Keh3x58VzwXPBc\nxH+FKUhJvwGAf0VK6VUBvKiqb4nIYBPDdYyqThGRPiLyFYBtAC4LNZdERBSKhEFfVVcAiGl3r6pP\nuj4PCTFfRESUAeyRmyPl5eW5zkLe4Lmw8VzYeC4yQ8KuL4p7MBHN5vGIiCoDEYGG9CA3aJNNIqpk\nWrZsiW+//TbX2SCHFi1a4JtvvsnoMVjSJypSkdJjrrNBDn6/kzBL+qzTJyIqIgz6RERFhEGfiKiI\nMOgTUaW2b98+1K5dG999913S+y5fvhwlJZUrTFaun4aICl7t2rVRp04d1KlTB1WqVEHNmjV/WTdh\nwoSk0yspKcGPP/6Ipk2bppQfkVCen+YNNtkkorzy448//rJ86KGH4plnnsFJJ53ku/3evXtRpUqV\nbGStUmBJn4jylteAY7feeisuuOACDBw4EGVlZXjxxRfx8ccf4/jjj0fdunXRpEkTDB06FHv37gVg\nLgolJSVYuXIlAOCiiy7C0KFD0adPH9SpUwfdunUL3F9h9erVOOuss1CvXj0cccQRGDt27C/fzZ49\nG506dUJZWRkaNWqEm266CQCwY8cOXHjhhahfvz7q1q2Lrl27YtOmTWGcnpQw6BNRwZk0aRJ+97vf\nYevWrRgwYABKS0sxatQobNq0CbNmzcK0adPw5JP28GDuKpoJEybgzjvvxObNm9GsWTPceuutgY47\nYMAAtGrVCuvWrcPLL7+MYcOGYcaMGQCAa665BsOGDcPWrVvx1VdfoV+/fgCAsWPHYseOHVizZg02\nbdqE0aNHY7/99gvpTCSPQZ+IPImE88qE7t27o0+fPgCA6tWro1OnTujcuTNEBC1btsSgQYPw/vvv\n/7K9+26hX79+6NChA6pUqYILL7wQ8+fPT3jMFStWYM6cObj77rtRWlqKDh064LLLLsP48eMBANWq\nVcOyZcuwadMm7L///ujcuTMAoLS0FBs3bsTSpUshIujYsSNq1qwZ1qlIGoM+EXlSDeeVCc2aNYv6\n/OWXX+LXv/41GjVqhLKyMowYMQIbN2703b9hw4a/LNesWRM//fRTwmOuXbsW9evXjyqlt2jRAqtX\nm/mixo4di0WLFuGII45A165dMXXqVADApZdeilNPPRX9+/dHs2bNMHz4cOzbty+pnzdMDPpEVHDc\n1TWDBw9Gu3bt8PXXX2Pr1q247bbbQh9ionHjxti4cSN27Njxy7qVK1eiSRMzHXjr1q0xYcIEbNiw\nAddffz3OO+887Nq1C6WlpfjrX/+KxYsXY+bMmXj11Vfx4osvhpq3ZDDoE1HB+/HHH1FWVoYaNWpg\nyZIlUfX56bIuHi1btsSvfvUrDB8+HLt27cL8+fMxduxYXHTRRQCAF154AT/88AMAoE6dOigpKUFJ\nSQnee+89LFq0CKqKWrVqobS0NKdt/xn0iShvBW0jf//99+O5555DnTp1cOWVV+KCCy7wTSfZdvfO\n7SdOnIilS5eiYcOG6N+/P+6++2706NEDADBlyhS0bdsWZWVlGDZsGP7+97+jatWqWLNmDc4991yU\nlZWhXbt26NWrFwYOHJhUHsLEUTaJihRH2cw/HGWTiIhCFTjoi0iJiMwVkcke3/UUkS2R7+eKyC3h\nZpOIiMKQzDAMQwEsBlDH5/sPVLVv+lkiIqJMCVTSF5GmAPoAeDreZqHkiIiIMiZo9c6DAG4EEO+p\nz/EiMl9E3hCRI9PPGhERhS1h0BeRMwGsV9X5MKV5rxL9ZwCaq2p7AI8CmBRqLomIKBRB6vS7Aegr\nIn0A1ABQW0TGqerF1gaq+pNjeaqIjBaRA1U1Zii5kSNH/rJcXl6O8vLyNLJPRKlq0aJFpRsrvtC1\naNECAFBRUYGKioqMHCOpdvoi0hPADe4HtiLSQFXXR5a7APi7qrb02J/t9ImIkhRmO/2UJ1ERkcEA\nVFXHAOgnIlcC2A1gB4ABYWSOiIjCxR65RER5jj1yiYgoJQz6RERFhEGfiKiIMOgTERURBn0ioiLC\noE9EVEQY9ImIikhOg/6uXUBkSkkiIsqCnAb9G24A6tfPZQ6IiIpLToP+ypW5PDoRUfHJadB3jsgw\nb17u8kFEVCzy5kFux44s+RMRZVreBH0A2L071zkgIqrcchL0RYBvv41dv29f9vNCRFRMclbSX7Mm\ndh2DPhFRZuUs6HsFeA61T0SUWTkN+u4gz6BPRJRZOQv6XgGe1TtERJmVV9U7DPpERJkVOOiLSImI\nzBWRyT7fjxKRZSIyX0TaJ0rPq3qHQZ+IKLOSKekPBbDY6wsR6Q2glaq2BjAYwBOJEuODXCKi7AsU\n9EWkKYA+AJ722eRsAOMAQFVnAygTkQbx0mSpnogo+4KW9B8EcCMAv7J4EwCrHJ9XR9bFYLAnIsqd\nqok2EJEzAaxX1fkiUg5A0jngX/4yEgAwfjzwww/lAMrTSY6IqNKpqKhARUVFRtIWTVCRLiJ3Afgd\ngD0AagCoDeBVVb3Ysc0TAN5T1YmRz18A6Kmq611p6datirIyYNo0YNQo4I03TF2+iBlps33CR8BE\nRMVFRKCqaRW4LQmrd1R1uKo2V9VDAVwAYLoz4EdMBnBxJHNdAWxxB3w7PfvdWl69OsXcExFRUhJW\n7/gRkcEAVFXHqOoUEekjIl8B2AbgMr/9vG4sfv451VwQEVEykgr6qvo+gPcjy0+6vhsSLA3zLj43\nKjNnAkuWAIMGJZMzIiIKIus9cp3VO15uuAG44ors5YeIqJjkLOgDwJ498b8nIqJw5VXQZ8AnIsqs\nnAZ9IiLKrqwHfa8qHYsILwpERJmU9aDfuLF5P+OM2O8Y8ImIMitn4+kDDPJERNmW06BvcbbZ54WA\niChz8iLoJ+qwRURE4chp0H/vPfPuNR4PERGFLy9K+kRElB0M+kRERSQvgr6zSofVO0REmZMXQZ+I\niLKDQZ+IqIjkRdC3qnQWLWL1DhFRJuVV0L/ootzmg4iossuLoO9n+fJc54CIqHLJu6DvrN457DBg\n06bc5YWIqLJJGPRFpLqIzBaReSKyUERGeGzTU0S2iMjcyOuWsDLISdOJiMKTcGJ0Vd0pIiep6nYR\nqQJglohMVdVPXJt+oKp9U8kEH94SEWVHoOodVd0eWawOc6HwCtOhDJfGCwARUeYECvoiUiIi8wCs\nA/C2qs7x2Ox4EZkvIm+IyJHJZMIr0C9blkwKREQURMLqHQBQ1X0AOohIHQCTRORIVV3s2OQzAM0j\nVUC9AUwCcLh3aiMdy+UAyrFkifNY5v377+11H31kPp99dpDcEhEVtoqKClRUVGQkbdEk61NE5FYA\n21T1gTjbrADQSVU3udard82QrV07YOFCYMYMoEcPYPVqoGtXYNUqVv0QUXESEahqKFXoQVrv1BeR\nsshyDQCnAfjCtU0Dx3IXmItJSo0tFy6MXbdqVSopERGRW5DqnUYAnheREpiLxERVnSIigwGoqo4B\n0E9ErgSwG8AOAAPSzZjfLFqzZwPHHZdu6kRExSnp6p20DhagescycybQvbup3mnSxKxTNReDDRuA\n+vUzmFEiojyS1eqdfMS6fSKi1BRk0CciotQw6BMRFZGCDPrWQ97du3ObDyKiQpO3Qd+qt//b37y/\n/+gjoFq17OWHiKgyyNugb3nmGe/1J5yQ3XwQEVUGeR/0vVrq+LXhJyKi+PI26FvBns0ziYjCk7dB\nn4iIwleQQd9dvbNuHbBzZ27yQkRUSPI26CdTvdOoETBsWGbzQ0RUGeRt0LdG2/QK+lu3xq5bty6z\n+SEiqgzyNujfdZd59wr6l1yS3bwQEVUWeRv016wx715Bf/Pm7OaFiKiyyNugT0RE4SuooM82+0RE\n6cl60P/5Z+BwnynTvXgFegZ/IqLUZD3oV68O/Pa3wbd3BnivYL9vX/p5IiIqFjmp3klm7Bzn8Mkf\nfWTely+317HUT0QUXMKgLyLVRWS2iMwTkYUiMsJnu1EiskxE5otI+3hpduyYWma7dzfvP/9sr3MG\n/ZkzORgbEVE8CYO+qu4EcJKqdgDQHkBvEeni3EZEegNopaqtAQwG8ES8NE8+OfUM++cTmDvX/nzc\nccDUqeEfh4iokAWq3lHV7ZHF6gCqAnBXqpwNYFxk29kAykSkgV96YZbGncM1DB1qr//kE2DKFO99\ntm4FZs0KLw9ERIUiUNAXkRIRmQdgHYC3VXWOa5MmAFY5Pq+OrMu4eGP0+F1cbr/drioiIiomVYNs\npKr7AHQQkToAJonIkaq6OJUDjhw50vFwtjzySp1X0Ld67PoF/T170jokEVFGVVRUoKKiIiNpBwr6\nFlX9r4i8B+AMAM6gvxpAM8fnppF1MUaOHAlVe2yddHkF/SVLwkmbiCgXysvLUV5e/svn2267LbS0\ng7TeqS8iZZHlGgBOA/CFa7PJAC6ObNMVwBZVXe+fZsr59eUM+lWqhJ8+EVFlEKSk3wjA8yJSAnOR\nmKiqU0RkMABV1TGRz31E5CsA2wBclsE8R/Eq6Se6qLBZJxEVq4RBX1UXAohpWa+qT7o+D0nmwGef\nDfz0E/Duu8nsFWvvXuv49rqSyP2LCLBrF/DFF8AxxwRLb+dOYNky4Oij08sXEVE+ytmAa5MmAe+8\nk346L71k3v2qd0aPBo49Nnh6Dz0EtGuXfr6IiPJRQY2y6cWaRcurpL99O3Dddcmlt2NH8nl47z37\njoOIKJ8VfNC3eAX9zz9PPp1U6vtPPhnIUOsqIqJQVcqgbwVuvwD+5Zf2crdu0Z9TfcibiZL+998D\nl14afrpEVLwKPujfd5959wr6JT4/3Ztv2ssffgi8/376+cjEaJ8zZgDPPx9+ukRUvHIe9NNtJfP9\n9+Z9+3Z7nVWt4y61L1hgWvNYfvopNj025ySiyiznQf/DD8NJx1laHzjQvM+YYa977TXTiuexx+x1\n48fHpsOgT0SVWc6Dfu3a4afpNZvWI4+Yd2frnEyM9pmuHTtSa0FERBREzoN+tljj8TgDvVfQd677\n9FPg2Wczmy+LqqmW6tQJOPHE7ByTiIpPXgT9BQuAwYMze4w1a8y7V9BXNcvWy/I//wP8/veZzZfl\n009Np7AlS8z5ICLKhLwI+u3aAWVl2TlWopK+paIi+jnBXXdFT9Polm71jlfafL5ARGHLi6APJDdU\nQjqCVu988IG97umngb/8BfjPfzKbN4t1Afnkk+wcj4iKR94E/YEDgRUrgPPOy+xxvAK9s5Q+fLh5\nf/tte91nn9nLO3d6N/XMRL7uuSf84xBRccuboA8ALVsCr7yS2WMErd5xtud3bnfhhUADj9l/w+yc\ntXs3q3aIKDPyKuhnQ9Cg7/fdkiXRHcEsmeiRS0QUNgZ9JA7YmSh1r3ZNJhnvIbFl+3bgnHPCzwsR\nFY+iC/pTp9rLQYN50ItDUKtWAU2bRq877bTE+337rZmHgIgoVXkZ9MOaNN2Lc+IWK5hPnx67nfOC\n4By6wb3N5s1m2X1B+Ne/zNAPXoKU6v2OR0SUjrwM+jffnJ3jWMMWJ/Pw2Aq8ixeb9//+13u7c8/N\nXlXMvn3ADz9k51hEVNgSBn0RaSoi00VkkYgsFJE/eWzTU0S2iMjcyOuWzGQ3exIN0XDUUeb9qqui\nt/n009j9VE1rIOsBcLKzeV12WfTxrd7FliefBOrXj173z38C330X/Bh79th3LURUeQUp6e8BcL2q\nHgXgeABXi0gbj+0+UNWOkdcd6WZs6NB0U0jPRx95r1+0KPrzlCnmfdMm8965c/TFYccOM67/wIHA\n/vubOXjfeMP+/re/BUaNip8Xa/J4K90mTaK/d18EAKBfP+B//zd+uk633w4ceGDw7YmoMCUM+qq6\nTlXnR5Z/ArAEQBOPTUOtcX7oIeD//i/MFDPrkku811v199bFwpr0BTAteF5+GRg7Nn7aq1aZgH/E\nEdHr69aNfbB75ZXA8uVmOZkHzytXBt82mTsIIsovSdXpi0hLAO0BzPb4+ngRmS8ib4jIkSHkDcOG\n5dcww4mC6Ouv++/zxRfm3VmFcsIJ5j3VB7Rbtpg7Eme+nngi+gHyc8/ZF4F4vPLg9ZxAFWjWLLl8\nbtuW3Paq7PdAlClVg24oIrUAvAJgaKTE7/QZgOaqul1EegOYBOBwr3RGjhz5y3J5eTnKy8t9j1lS\nAuy3X9AcZt7HH8f/vm/f2HXusf2dHbvSGc4h3r7OJqaXXWZefkNE79ljXl7q1wfeeiu6OakVjK2R\nSRPZvRuoVSu5IH7ggcCgQcC99wbfh6gyqaioQEVFRUbSDhT0RaQqTMAfr6oxDRGdFwFVnSoio0Xk\nQFXd5N7WGfQLTbIPYIH4wc6awzeVkv4BB9jL1v5WdZg7Pb88zJoFdO8OVK9uzzbmtmGD93pVoHdv\n4NVXgZo1/fPpNaFNIlu2ZGawubFjgQceABYuDD9tojC5C8S33XZbaGkHrd55FsBiVX3Y60sRaeBY\n7gJAvAJ+sVGNPzLnxo3m3etBbCJ799pp3BF5bG4NFmc1I7WC/XPPRe+7Y4f5znoIvXOnfaFYsgR4\n8EH/41pprlgBTJtmnjfEs2JF/O8XLgTmzIm/TTq2bLH7Ybz5pj1/MlGxCtJksxuACwGcLCLzIk0y\nzxCRwSJyRWSzfiLyuYjMA/AQgAEZzHPUw9B8d8YZibdJtbMWYD8rcJo2zbw7S/hbtpjgv22bKZk/\n/bT3HcYDDwDXX29/dt8lWJ8HRH7Dt9wSPTidW6dOses2bAAOO8wE/GOOAbp08d8/XXfcAZxyillm\n5zaiYK13ZqlqFVVtr6odIk0y31TVJ1V1TGSbx1T16Mj3J6iq14PelDlLqj162G3kC4FVIo9ny5bU\n0y8tjV1X1aPS7oUXTN1+rVrm81dfRQdBv6omd97cF4FXXrEfFO/ebR5OOy8au3fH7nfwwWafQYO8\nfyav4yTid+FM5YHwn/+c+ykr/Tr9Ue7t2RPbi//zz4EjQ2m+knl52SPXzd0cki07bEGDfonrN33v\nvdHncdky8271N7CmbBwyxDzMDVIFc/75pjXR5Mmmc9iOHXbQnzcvdvsg9f3ffWf+yV55BahWzazz\nGuW0Rg3gww9j16dSun/tNWDGjOT2GTMm9jlEvDugRMrKTHUU5Z8pU+y7R8usWfY83PmuIII+EF1l\nwaBv2+Tx5MR66O9ssXP11bHbOaeDtJat5wvOmcxOP91Uwbz5ZnTrHcuRR5o+B1ZgFzGdw5wPeDt1\nMtVyyU4036yZGfto9mz7ArL//sC//x277dq18dNKdoC9RPr2NT8nYOZ4vvlm0x9DxOSlevVg6fhZ\nty69/b3MmBHd8XDRouhZ4igxr7v3Qqo6LJig36uXeRdh0HeaOzd2XdAWM15NNeP98fbu7X/uN2yw\n9/WrarnxRjPZfLxjebXpv/ZaYP366Dw7HyAf7mocvN9+wDXXAN98k94/4/TpwFNP+X//+uvRneNE\nTLUZkFr1jPUzOtOzXHWV991Ssk48EXC2kv7Nb4CePWO3mznTrrb71a/MMyAyEg3Rku8KJug7MeiH\nwwpQTu5qIDe/fhO9etl/+PGqNZy/O+dyx47AH/9oVzNZaVqsAe4mToxN09rHOv7OncCjjwKHHBLd\nGS7Zf8yhQ4Errohed++95iLkxVkg8TvWfvsBEybErn//faBhw9j0LI8/DowfHyzfifjNCufUowdw\nwQVm+bPPoocOocJWkEHfORZ9+/a5y0ehs5qMOm3dmlpafu353fwu2PPmmWcBzu+d8xRbdu70T3vx\n4tiRTZ31rFawve4686zCWTU2bZo9uqsVCK33BQvM3UuvXqZ108OeDZejg77fqKc7d8bW/S9ZEuz8\nhVnYse5ECqmEGrbNm1NrOceSfpaJmEBv/QMkKplSctLpuGRdROIFp3gXlbVrzZhLQGw9s3NyesAE\nSqv0b5k6NXYsIq9/xoceMs8JnA9+77sPuPtus2yla+17yinADTeYi1DQKTatITaCOPJI8yA4XnpA\n8KAfpGqprCy2BVc+uPji1FuzrV3rXZBxevhhux/KgQcCf/hDaseK5+uvve9I80XBhUz3H2m+/dEW\ns3SGlbAkGifI+n0/8khs090qVWK3twoFXiU6ZxBNFMxHj/bezpnGvn1mHgUnqxWU07ffxqbjHGPK\neiaT6t92WVmw+RXeeit7/z8//xxsoL7x41N/dtGypamWiufaa6ObFH/7bfLHSVTSv/VWu2osHxVc\n0HdjST834pU606mGSPb36SwVejVVtf4ZDzoo9jsrn85WTF77On8ed/6c33lVPR17rMnjV1/ZHemc\n8yNb65xBw3qO4A4ue/aYYwQZhNB5kRMxrXTco7l6tejys3VreheIW24JPlCf13H27Yt+yO9sSWbZ\ntSv2YXgmsHonx846K9c5KE7xqmkS3WJbUhl+wf3PVbeuvRwv6HvdhagC48aZ1ixWQPEq+Tl/Hnd9\nv5PXg3Erj61bA23bms/OC4fXumeeMe/Ll0c/o3jsMfMg2GoK27ChaWUDmOcVt9/ufXzAtHbyaoFj\n/RxLlwJ/+pN/s1evvhHJ+P779Pa/+267YyFgWpJ5NVf2cu+9ZshxL0GD9WGHmWobIHZOjUJTcEHf\n/Uvyam5GmWf9A4Qt0T+hu27fyesuwa8UD5igb3X8s8YIatkyfl68qnesdtuJ+gm48zlunL3Oqppy\npv/OO7HPKJzWr7efS4wcCYwYYV+8vPIZ7+c54ghTZda4cfy8q3rP//D993Zd+Zo1wPPPx+4HmLsY\nvwmK4nG26rIk+luxxpd65BEz5LjfNgDw44/x01q+3G4e7TWdq5WXKVPi3+lefTXw5Zfxj5VpBR/0\nqXLxqgN3evRR/++SrRqy6umB4AHb6+/v5JOTO66VhrOnuVfevZ5RWKwu/6qm5dHSpebzYYd557NP\nH++8B63Tnh0ZWGXjRuDyy81QGlYTUlXT/8KqK3/4YTP/tFeLpKeeir0gAGb/2XEGb0nm/17VPNRf\nuxY488zE+376KVCnTvD04znzzPjfjx4N/OMf4RwrVQUf9EVM5xGqHNIZhyjZAoFXk1Anr/phr4tD\nsj1aZ82KXeeV93jDqVtNUffujR7Uz+rFO2mS3Vs43jG8OsP97W/m3etntdLYsMGuWlq2LPoCYDn4\nYO8qmHffBc47z54GFADuvx/o2jX6GPXrB2uJ5B7JddUqc5EL8jBbJPbitG2bd2/qoM+qrPy7q/uC\n5CcbCirov/WWmQTcqVq15OaCpcor7If6XkHPGgoibPHyHu/B7V/+4r3+qqvM+EdOQXtqDxtm3lev\njt9azlp2DkvgHmajXj3T/8IaxhswwfDVV4FTTzUXBjer1/YPP5iWSNOn28cqLbU7l4mYB9uHHhq9\nv/Uc5KqrYtMOUjDYvDm9cZMszhFwt283F7F8UFBB/7TT7NtXAJg/34wJ4/xFJppknCqvQm7JFa/T\nWVgziHndYSTLK+jfc4+9zquK5uyz/R+6btgQe7fx6afRn51zIOzZY6d15ZXePcStuvegU606f6Z6\n9ey+Im6q/g+knWlYdwRTp9rrBg+O/T5XAk+XmI+sQcGsE962LR/sFrPJk3Odg9TFq8rJp8nm6tWz\nl63/O2cdfbyxivz85jex65ylZHeQbNTIvDs7QAXpNOcM2NYd2+rVpiUQYFpxbdoUOy2qddGZO9ee\nR8LvGE7O6sNUJkrKlAIuG9natzcDSS1ebCblcPO6hSQqBokejKfj8cfNsBTpeued2HVWU1bANDUN\nMi8F4D3WknXRcA4YaA3f4myFZrX0svabO9eMDtuunfnsvKMBTIuepUvNuETOJsHW/suWASedZJad\n4++zpB+Cgw/2bppXq1Y4vUSJCpVziOxMuOGGzKYPmJK2s41+UFZwtUZkdTbLdA/h4WSVyr1mfXNy\nVjX7qahIbZ7oTKoUJX235s3N+8qV5n3IkNzlhYjSl0pTbb+pPhNJtw+Ke//XXksvvbBVyqB/442m\nVY/VW7NQpjEjIm+pdOhyzzXh12M6bO5nAn/9a/TnXFfviCbIgYg0BTAOQAMA+wA8paoxbWREZBSA\n3gC2AbhUVed7bKOJjhc2EfNLsNoAExHlWrJhUESgqqF0TQ1S0t8D4HpVPQrA8QCuFpE2rgz1BtBK\nVVsDGAzAp9Nz9u3ZAxx3XOz600/Pfl6IiHItYdBX1XVWqV1VfwKwBEAT12Znw9wNQFVnAygTkQYh\n5zUlfl3Zc32LRUSUC0nV6YtISwDtAbi7YDQB4Ji1FKsRe2HIK23amEGmiIiKSeAmmyJSC8ArAIZG\nSvwpGenoaVJeXo5y5yzNGXbnnWaEu3HjgP79vR8O9esHvPJK1rJERBSjoqICFfF67KVDVRO+YC4O\nb8IEfK/vnwAwwPH5CwANPLbTXBk0SHXdOtU33lDt0MGsW7NG1VT02K/+/aM/33Zb7DZ88cUXX+m8\nkhWJnQjjFbR651kAi1XVZ0poTAZwMQCISFcAW1Q1C3PYBDdmDNCggRl9z2rK1ahR7Ah97mcAVm+8\nbDv8cPPGr+nmAAAR40lEQVRuzdtKRBSGhEFfRLoBuBDAySIyT0TmisgZIjJYRK4AAFWdAmCFiHwF\n4EkAHuPb5aeWLe2u6s2bm7p+wB5n3DkzUzYtWmTy1r17bo5PRJVTwnb6oR4sB+30gxozBjj/fDOU\n644dwP77m/HUGzTIzcQtztPEiWOIKpdkw2CY7fQZ9ANg0CeiMOUy6FfKYRiIiMgbg35Ap51mL1vT\nxBERFRoG/YDuuMPuzNWtW/rpBZ2Q2slvRh8ioqAY9HPEGv7Zi9eUj5dfXtjTARJRfmAYCaikxPuB\n6tFH+0/RWDXJKWrOOstM33bNNbHfqXKIaCJKH4N+QKWlZvgG94QI8+cDF11kllWBvn3t7xYuTJzu\noYcCH35o5uZ8+WWgcWPv7VSBU06x+/QREaWiUkyXmGmffWbm3nWX9IcMMT14e/e2Z+d67TV7u4YN\nzfupp3rPAwoA9esDxx8f//jDh3tPHp3IVVcBo0cnvx8RVV5sp58iEeCbb4AWLby/EzGl97p1Tcnc\nfcGw1nXpAsx2j1ka8Ph+jjnG9DK++mrgscf8tzv8cDOxMxFlF9vpF6hEnaa8frGtWkWX7MO6Br7+\nur18wgnRae+3n/3dnXfayw0yNONBKlPbEVF2MOinqGtX4OCDg2//9dfA2rVmns4PP7TXhxX0f/1r\n8z5ihJ1mnz7m/amngPPOM8tedyYffBC7zj2/qB+vSWpq1gy2r5c2bRJvQ0SpY9BP0UcfRZeg3USA\n2rXtB7uHHGLX8Yfhgw/MKKFuzoB75pl2XqwLwcCB5v2dd+x1PXrEpnPMMYnzULMmsGpV7PpUL2TX\nXQf87nep7UtEwTDoZ1DVqrGtfcLSowfwn/8AN90Uvd4Z4C21a9vrnFVS8YKz1Sdg7VrTcildXncY\nmeJ1ESMig0E/x9Kp3jnoIFNHv3atvc79nGHZMtP+f9++xMeeNQto1iw6HeuhNGBGH3UKegE57jjg\n6aej19WuHTtdpdcD71RMm5Z+GkSVFYN+hgQNXunW6VepEl1t5D7uYYd5l/69jl2zpncdvZWmu0ew\n18/Ypk2wY516KnD99Wb50ku9t7HccIP3ej81aiS3PVExYdDPsVatwk8zSNAFYkv/iS5U7u9FzAXn\nH//w3t66M/BK13kHMXas/zHvuit+nogoOQz6ObR1KzB+fLhpxuvR63boodGfS0pitxMxD3Vbt44N\n3kcfbdb16+d9HOtBdzJ3Pda2VtNS9kAmCheDfgYMGQIMHZp4uzp1gOrVwzvupk3ABRd4B8lHHgGm\nTo1e9+yzwObN9me/4Pz++2a4Cef3o0cD774bLF9e1UuJ7kaGD/ffzqmszLxPmACcfHL8bX/72+jP\n//xn/O2JKqMgc+Q+IyLrRWSBz/c9RWRLZO7cuSJyS/jZLCyPPALcf3/2j1u3rgmwXm3dW7YEzjjD\n/tyqlSmJH3CA+XzuuaZZ6VFHmQfETjVq2PX9q1aZaqE//tG77jxRyTzRjGBB1j36KPDzz2Y6yy1b\nzLrS0thnDi++GD2a6Usvmf2sn+/cc2OP9cQT/nmvbKpVS33fW4r+v7xwBSnpjwVweoJtPlDVjpHX\nHSHki9Jw/fUmuPlRNRcBp3/+08wL/OqrZngJizvgNm0aXR/vlXaq/IarcKdZUmLukBJ1jmvRwr4T\nsMS7s+rZM7qqKhkdO3r3Wci0BQtMNVsqJkxI/bjZrnJr0iS7x6vMEgZ9VZ0JYHOCzTiLax4RSb3a\nqHr16A5eqfSuPfZYc/Fw5sfrWUGQun6v4JLMHYK1f7wH5v/9L7B3LzB9evw8tW3r/93BB5sL4qZN\n3t8HDZILPO+n/SW6s3I3i3XyutNJ5NhjzXs2520+/PDsHasYhFWnf7yIzBeRN0SEo75XEqqm9J+s\nkhLgnHOi1x1ySLDjBVkXVPPmJuhVq2aGv7B4tUIqKfGfM8GyeHHsug4dovOZ7jOadu3M+/Dh0VN0\nAsDEiUDnztHrEgV951DfyejYEaioiF7Xu7d5tgMA7dunlm4q3ngj+X0uvjj1440cmfq+hSCMoP8Z\ngOaq2h7AowAmhZAmFSivANSihSmteZX23fvGC7rLlyeXj6ZNTb3+xo3x8+g8pt/xnQPaObnHKHLu\nf845wIMP2p+t/ghBtGljOrA51asXv/rrqadiS+/uFlrJ8LuY7N5txnKaODH1tN2uvNL/u1TuKkaM\nSD0vQQoohSzt8fRV9SfH8lQRGS0iB6qq543uSMdltLy8HOXl5elmgfLE5ZfHzu71ww/Bq4jatgW2\nb49eZ00eM3myHcDcQaBOHVNC9hoLqXr1+KXvU06J3s9K+9//tgexa9LEXvbjDpCNGpmS8d699rqx\nY4HnnjMBffhwuzQ6cKAZsmPcuPhp+l0Ure3+8Afzcm5zxRXxA6rTQw8B117rf3yLNSNc//7AgAH2\n+t27Y4fsCNrLevRo4PHH/b9PNvCnU/0U9O7yzDNTuwsJoqKiAhXuW62wqGrCF4CWABb6fNfAsdwF\nwDdx0lGiceNUFy9WfeYZU05VVd21S3XfPtV77rHXvfWW6tat0fsOHqy6cqV3uhs2qC5cmPj4Bx1k\nH8Ntyxb7O0C1fn3VvXvt7wHVtm1Vp0+P3q5XL7O8d6/qH/9ob//449HH6tRJ9YYbVD/6yCqjq951\nl+q2baqLFtnpjRunes45Zvmpp8z722+rHnecvR+gOn++6hFHRB8DUK1VS/X886M/O/dz5h1QXbo0\n+nOnTqrvvhu9fZ8+sefLLz33uvvuU+3fP/b7ePtar2XLVJs2NcvWz5rotWKF/3fXXhu77uab7eXn\nngt2jJtuCrad38+bjEjsDBSvE72CNNl8CcCHAA4XkZUicpmIDBaRKyKb9BORz0VkHoCHAAzwTYwI\nZnrJtm2jS2Olpebz6acDJ55o1p12minFOz3xhD0+kFv9+sFassQrBbq/s+r6nUpLY+8qVO3tnSVW\nd3qffgrcd1/sMWvWjL5LUjWl7j//2ZTerXVWeqqm/8Qxx3iXTEeNAv7+d/uz3wNmwJzP1q1j17dp\nE6zVTNOmiave9tsv2POOqVPjPy/44gvv9bt328v168f+zpweeCB+HrzOpxev8awsc+YESyMXElbv\nqOrABN8/BiDO/ExEwR17rAlmmdSjhxmIzovXQ1636tVNnwgnv0DRubPdF8KP30XoxBPtC6DXttZ3\nXsd2XyzjBUG/lk+NGwPffWdaN5WVeW/Xv795XhPk2YFzDCe/gOnMp/Mil0jVqsD33web4yJo669E\n4l0cUmkAkS2cI5dyJpvN/pycJeB4FiyI7YC2aJEJ+I0aAT/+aNaVlNitbtw6dozu9WxxXgji1dO7\nBdn266+j+2Hs3Gk61r32mgnSO3d6pw2YlisjR0Yfx7qAeB076MNcZx737vX/3Xu1oEpU8v74Y/Nu\ndboLWlJ3so45IGA9xciR9u8/XnpOq1b536VmE4dhoJypmqMih1eVjcX5z9qunRml1OnII+3Ja2rV\nMu87dsRW2STSpo19MfBqLeI12qlfybd3bzPXsuWQQ6K3s3re9u1rSsPOqocxY0wPZ4tfq5drrzXD\niwTRo4d38HTnfcIEU53n5PVzJ3LcccG3dTa7LS83DQTcglw0ROy7lRdeMO9jx5qLqp+mTQNnM6NY\n0qec6d8/P0o+TjVrJt/GO9XhDA44wPScdu9fUWHPc+w2ZEjsc4tRo4Ifs06d6E5mgwYF28/Z9DSR\n7t2DbVeliikx9+plPnfunHrvYidV/+oV588uYnf86tvXDPIX9C7BWUXlFcy9ZrWzpNMTOgws6VPO\nVKtmhj7IJyUlwPPPZ+941avHloB79vSerUzVDKiX7vhA+++fWhVIUMmMqtq1qz2vwiefmCoa9/MB\nVTOWlfPCM2NG/LTr1TMX1HHjgNWrvZ+rqJrOe6r2nVKjRvHPjaoZW+vyy+3tnA/XrWVr+I9HH429\nkLnvHrONQZ+IogwcCPz+96nte+qp3n0avKqm/ILr+PGxrY3694/uQ3DUUf7TeFrpVq9uWoo1bmz6\nINx+e/zjr1sH3HOP/dmanKhHD7sFFWDutho3ti9W1kN9r5+xeXNzAYp33Gxj9Q5RgXC3GMqUF19M\nfd+33zbvXj2Y+/c3nfUsfuMC1aiRePYzr454Fq+geskl8dMDgAYNoj+vXWuCePv20bPTWVq0MHNi\nOFtK9e9vj/yarxj0iQrADz8ABx6Y61ykrqzMPHDu3dt8Tqa069420yXlZNJ3BnxVM/SGe9ypZFsj\nZRqDPlEBKLSA7wxsK1ZEz2uQiWNYrrzSdM7K9HGCbufXCiuXGPSJKHTOqij33A3J8guSdesC27ZF\nrxs9Ov10e/WyW3C1a2c+d+4cOzdDIrNmmeak994bvT7M2fJSwaBPRKG77jrg/PMze4x58+IPhRDP\nxIn+zxSaNbNbcDnnN7j66uSO4dXsdvHi+PMyZAODPhGFrrQ0vCGK/UrY6cymFa8TVar8WhMNGWJP\nYZrrgA8AolmsYBIRzebxiIiyYc4cM6FOpnqZiwhUNZSBSxj0iYjyXJhBn52ziIiKCIM+EVERYdAn\nIioiDPpEREWEQZ+IqIgw6BMRFZEgE6M/IyLrRWRBnG1GicgyEZkvInGmNSYiolwKUtIfC+B0vy9F\npDeAVqraGsBgAGlO8VAcKioqcp2FvMFzYeO5sPFcZEbCoK+qMwF4TO38i7MBjItsOxtAmYg0iLM9\ngX/QTjwXNp4LG89FZoRRp98EwCrH59WRdURElGf4IJeIqIgEGntHRFoAeF1Vj/H47gkA76nqxMjn\nLwD0VNX1Htty4B0iohSENfZO0DHhJPLyMhnA1QAmikhXAFu8Aj4QXqaJiCg1CYO+iLwEoBxAPRFZ\nCWAEgGoAVFXHqOoUEekjIl8B2AbgskxmmIiIUpfVoZWJiCi3svYgV0TOEJEvRGSpiNyUreNmk1dH\nNhGpKyJviciXIjJNRMoc390c6dS2RER6OdZ3FJEFkXP1ULZ/jnSJSFMRmS4ii0RkoYj8KbK+GM9F\ndRGZLSLzIudiRGR90Z0Li4iUiMhcEZkc+VyU50JEvhGR/0T+Nj6JrMv8uVDVjL9gLi5fAWgBoBTA\nfABtsnHsbL4AdAfQHsACx7p7AAyLLN8E4O7I8pEA5sFUsbWMnB/rzms2gM6R5SkATs/1z5bkeWgI\noH1kuRaALwG0KcZzEcl3zch7FQAfA+hSrOcikvfrALwAYHLkc1GeCwBfA6jrWpfxc5Gtkn4XAMtU\n9VtV3Q3gZZhOXZWKendkOxtAZJplPA/gN5HlvgBeVtU9qvoNgGUAuohIQwC1VXVOZLtxjn0Kgqqu\nU9X5keWfACwB0BRFeC4AQFW3Rxarw/zTKor0XIhIUwB9ADztWF2U5wKmcYw7Bmf8XGQr6Ls7cH2H\n4unAdbBGWjOp6joAB0fW+3VqawJzfiwFfa5EpCXM3c/HABoU47mIVGfMA7AOwNuRf9CiPBcAHgRw\nI8yFz1Ks50IBvC0ic0TkD5F1GT8XGZrGl+IomifnIlILwCsAhqrqTx79NIriXKjqPgAdRKQOgH+J\nyFGI/dkr/bkQkTMBrFfV+SJSHmfTSn8uIrqp6loROQjAWyLyJbLwd5Gtkv5qAM0dn5tG1hWD9dZY\nRJFbse8j61cDaObYzjonfusLiohUhQn441X1tcjqojwXFlX9L4AKAGegOM9FNwB9ReRrABMAnCwi\n4wGsK8JzAVVdG3nfAGASTDV4xv8ushX05wA4TERaiEg1ABfAdOqqjNwd2SYDuDSyfAmA1xzrLxCR\naiJyCIDDAHwSuaXbKiJdREQAXOzYp5A8C2Cxqj7sWFd050JE6lstMESkBoDTYJ5xFN25UNXhqtpc\nVQ+FiQHTVfUiAK+jyM6FiNSM3AlDRPYH0AvAQmTj7yKLT6rPgGnFsQzAn3P95DxDP+NLANYA2Alg\nJUxHtboA3on87G8BOMCx/c0wT+GXAOjlWN8p8gewDMDDuf65UjgP3QDshWmlNQ/A3Mjv/8AiPBft\nIj//fAALAPwlsr7ozoXrvPSE3Xqn6M4FgEMc/x8LrZiYjXPBzllEREWEo2wSERURBn0ioiLCoE9E\nVEQY9ImIigiDPhFREWHQJyIqIgz6RERFhEGfiKiI/D8hkxT3vIK3ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb61cb0f470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
