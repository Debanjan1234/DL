{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "\n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "\n",
    "        cache = (X, Wxh, hprev, Whh, h_cache, y_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, Wxh, hprev, Whh, h_cache, y_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - beta1**(iter))\n",
    "                    r_k_hat = R[layer][key] / (1. - beta2**(iter))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 training loss: 2.1776\n",
      "ed tasel founc as s f7, of Ind 18537 the pmptrd Ind ficthinntreleforstriirout inty Bre ourd esenlit o\n",
      "Iter-260 training loss: 1.8728\n",
      "e Empexg-damlowhedex, cicrom the sope warOd) woul in it en'9tid erojl and coun ealld fans Peabich Was\n",
      "Iter-390 training loss: 1.8317\n",
      "er i3 Asom. Jascea of and thinanghery konIfst of Japan faron the Therry in a datiead the panturea in \n",
      "Iter-520 training loss: 2.0940\n",
      "ered fertitan in Pechesnd wokoperld Tilhorhijtelcgely Seondexnad tionden renty UNcgrolohnd an Seas1at\n",
      "Iter-650 training loss: 1.9502\n",
      "er ;s Reryatun\", foln minted and cote 1941,At lidest pbol. WsantHuuntentheslatror folecore coth sy1 C\n",
      "Iter-780 training loss: 2.0950\n",
      "ed fokokry 1947 ronn'strd I5 waxe losiTto mexture divin-dugeslaluninan, jtd whuthesed, Peve danded\") \n",
      "Iter-910 training loss: 1.8115\n",
      "ered Japan Ar(Bix8. nonntacith t\n",
      "upnfs combobcons rery ovI iboptsthenaltestty in Couft: Corty and Rer\n",
      "Iter-1040 training loss: 1.8667\n",
      "er of the G2015,paso topie the larexex GDP an ind loFsioned pantatatombc Cores. Ind ming popedo a std\n",
      "Iter-1170 training loss: 2.7992\n",
      "e is ilelman E surin wadesLhl2,aroad thyrerigeneitergatedgthece thoph–86e am P fadourkd in the Jaryit\n",
      "Iter-1300 training loss: 2.5930\n",
      "erend 1d Empedere as nomiced enr ioweer tarater ple\") Guof in lhe is amaner wimry iorti4arelyafheked \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f2c42d20978>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 1300 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.95 # keep_prob\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2c2481a8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
