{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# # This is the Tensor Flow version\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "mnist = input_data.read_data_sets('data/MNIST_data', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (55000, 784), (5000, 784))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    return X_train - mean, X_val - mean, X_test - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hipsternet.loss as loss_fun\n",
    "import hipsternet.layer as l\n",
    "import hipsternet.regularization as reg\n",
    "import hipsternet.utils as util\n",
    "\n",
    "\n",
    "class NeuralNet(object):\n",
    "\n",
    "    loss_funs = dict(\n",
    "        cross_ent=loss_fun.cross_entropy,\n",
    "        hinge=loss_fun.hinge_loss,\n",
    "        squared=loss_fun.squared_loss,\n",
    "        l2_regression=loss_fun.l2_regression,\n",
    "        l1_regression=loss_fun.l1_regression\n",
    "    )\n",
    "\n",
    "    dloss_funs = dict(\n",
    "        cross_ent=loss_fun.dcross_entropy,\n",
    "        hinge=loss_fun.dhinge_loss,\n",
    "        squared=loss_fun.dsquared_loss,\n",
    "        l2_regression=loss_fun.dl2_regression,\n",
    "        l1_regression=loss_fun.dl1_regression\n",
    "    )\n",
    "\n",
    "    forward_nonlins = dict(\n",
    "        relu=l.relu_forward,\n",
    "        lrelu=l.lrelu_forward,\n",
    "        sigmoid=l.sigmoid_forward,\n",
    "        tanh=l.tanh_forward\n",
    "    )\n",
    "\n",
    "    backward_nonlins = dict(\n",
    "        relu=l.relu_backward,\n",
    "        lrelu=l.lrelu_backward,\n",
    "        sigmoid=l.sigmoid_backward,\n",
    "        tanh=l.tanh_backward\n",
    "    )\n",
    "\n",
    "    def __init__(self, D, C, H, lam=1e-3, p_dropout=.8, loss='cross_ent', nonlin='relu'):\n",
    "        if loss not in NeuralNet.loss_funs.keys():\n",
    "            raise Exception('Loss function must be in {}!'.format(NeuralNet.loss_funs.keys()))\n",
    "\n",
    "        if nonlin not in NeuralNet.forward_nonlins.keys():\n",
    "            raise Exception('Nonlinearity must be in {}!'.format(NeuralNet.forward_nonlins.keys()))\n",
    "\n",
    "        self._init_model(D, C, H)\n",
    "\n",
    "        self.lam = lam\n",
    "        self.p_dropout = p_dropout\n",
    "        self.loss = loss\n",
    "        self.forward_nonlin = NeuralNet.forward_nonlins[nonlin]\n",
    "        self.backward_nonlin = NeuralNet.backward_nonlins[nonlin]\n",
    "        self.mode = 'classification'\n",
    "\n",
    "        if 'regression' in loss:\n",
    "            self.mode = 'regression'\n",
    "\n",
    "    def train_step(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Single training step over minibatch: forward, loss, backprop\n",
    "        \"\"\"\n",
    "        y_pred, cache = self.forward(X_train, train=True)\n",
    "        loss = self.loss_funs[self.loss](self.model, y_pred, y_train, self.lam)\n",
    "        grad = self.backward(y_pred, y_train, cache)\n",
    "\n",
    "        return grad, loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        score, _ = self.forward(X, False)\n",
    "        return util.softmax(score)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(self.predict_proba(X), axis=1)\n",
    "        else:\n",
    "            score, _ = self.forward(X, False)\n",
    "            y_pred = np.round(score)\n",
    "            return y_pred\n",
    "\n",
    "    def forward(self, X, train=False):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, y_pred, y_train, cache):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _init_model(self, D, C, H):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNet(NeuralNet):\n",
    "\n",
    "    def __init__(self, D, C, H, lam=1e-3, p_dropout=.8, loss='cross_ent', nonlin='relu'):\n",
    "        super().__init__(D, C, H, lam, p_dropout, loss, nonlin)\n",
    "\n",
    "    def forward(self, X, train=False):\n",
    "        # Conv-1\n",
    "        h1, h1_cache = l.conv_forward(X, self.model['W1'], self.model['b1'])\n",
    "        h1, nl_cache1 = l.relu_forward(h1)\n",
    "\n",
    "        # Pool-1\n",
    "        hpool, hpool_cache = l.maxpool_forward(h1)\n",
    "        h2 = hpool.ravel().reshape(X.shape[0], -1)\n",
    "\n",
    "        # FC-7\n",
    "        h3, h3_cache = l.fc_forward(h2, self.model['W2'], self.model['b2'])\n",
    "        h3, nl_cache3 = l.relu_forward(h3)\n",
    "\n",
    "        # Softmax\n",
    "        score, score_cache = l.fc_forward(h3, self.model['W3'], self.model['b3'])\n",
    "\n",
    "        return score, (X, h1_cache, h3_cache, score_cache, hpool_cache, hpool, nl_cache1, nl_cache3)\n",
    "\n",
    "    def backward(self, y_pred, y_train, cache):\n",
    "        X, h1_cache, h3_cache, score_cache, hpool_cache, hpool, nl_cache1, nl_cache3 = cache\n",
    "\n",
    "        # Output layer\n",
    "        grad_y = self.dloss_funs[self.loss](y_pred, y_train)\n",
    "\n",
    "        # FC-7\n",
    "        dh3, dW3, db3 = l.fc_backward(grad_y, score_cache)\n",
    "        dh3 = self.backward_nonlin(dh3, nl_cache3)\n",
    "\n",
    "        dh2, dW2, db2 = l.fc_backward(dh3, h3_cache)\n",
    "        dh2 = dh2.ravel().reshape(hpool.shape)\n",
    "\n",
    "        # Pool-1\n",
    "        dpool = l.maxpool_backward(dh2, hpool_cache)\n",
    "\n",
    "        # Conv-1\n",
    "        dh1 = self.backward_nonlin(dpool, nl_cache1)\n",
    "        dX, dW1, db1 = l.conv_backward(dh1, h1_cache)\n",
    "\n",
    "        grad = dict(\n",
    "            W1=dW1, W2=dW2, W3=dW3, b1=db1, b2=db2, b3=db3\n",
    "        )\n",
    "\n",
    "        return grad\n",
    "\n",
    "    def _init_model(self, D, C, H):\n",
    "        self.model = dict(\n",
    "            W1=np.random.randn(D, 1, 3, 3) / np.sqrt(D / 2.),\n",
    "            W2=np.random.randn(D * 14 * 14, H) / np.sqrt(D * 14 * 14 / 2.),\n",
    "            W3=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((D, 1)),\n",
    "            b2=np.zeros((1, H)),\n",
    "            b3=np.zeros((1, C))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "n_iter = 1000\n",
    "alpha = 1e-3\n",
    "mb_size = 64\n",
    "n_experiment = 1\n",
    "reg = 1e-5\n",
    "print_after = 100\n",
    "p_dropout = 0.8\n",
    "loss = 'cross_ent'\n",
    "nonlin = 'relu'\n",
    "solver = 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from hipsternet.solver import *\n",
    "\n",
    "# import numpy as np\n",
    "# import hipsternet.utils as util\n",
    "\n",
    "# import hipsternet.constant as c\n",
    "eps = 1e-8\n",
    "\n",
    "import copy\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "\n",
    "def sgd(nn, X_train, y_train, val_set=None, alpha=1e-3, mb_size=256, n_iter=2000, print_after=100):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size)\n",
    "\n",
    "    if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad, loss = nn.train_step(X_mini, y_mini)\n",
    "\n",
    "        if iter % print_after == 0:\n",
    "            if val_set:\n",
    "                val_acc = util.accuracy(y_val, nn.predict(X_val))\n",
    "                print('Iter-{} loss: {:.4f} validation: {:4f}'.format(iter, loss, val_acc))\n",
    "            else:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "\n",
    "        for layer in grad:\n",
    "            nn.model[layer] -= alpha * grad[layer]\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimenting on sgd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "solvers = dict(sgd=sgd)\n",
    "solver_fun = solvers[solver]\n",
    "accs = np.zeros(n_experiment)\n",
    "\n",
    "print()\n",
    "print('Experimenting on {}'.format(solver))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment-1\n",
      "Iter-100 loss: 2.2284 validation: 0.356000\n",
      "Iter-200 loss: 2.0968 validation: 0.536600\n",
      "Iter-300 loss: 1.8627 validation: 0.621600\n",
      "Iter-400 loss: 1.9090 validation: 0.679400\n",
      "Iter-500 loss: 1.4982 validation: 0.713400\n",
      "Iter-600 loss: 1.4953 validation: 0.733800\n",
      "Iter-700 loss: 1.2489 validation: 0.761800\n",
      "Iter-800 loss: 1.3936 validation: 0.782400\n",
      "Iter-900 loss: 1.1174 validation: 0.801400\n",
      "Iter-1000 loss: 0.9681 validation: 0.814200\n",
      "\n",
      "Mean accuracy: 0.8247, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_experiment):\n",
    "    print('Experiment-{}'.format(k + 1))\n",
    "\n",
    "    # Reset model\n",
    "    # if net_type == 'ff':\n",
    "    # net = nn.FeedForwardNet(D, C, H=128, lam=reg, p_dropout=p_dropout, loss=loss, nonlin=nonlin)\n",
    "    # elif net_type == 'cnn':\n",
    "    net = ConvNet(10, C, H=128)\n",
    "\n",
    "    net = solver_fun(\n",
    "        net, X_train, y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha,\n",
    "        n_iter=n_iter, print_after=print_after\n",
    "    )\n",
    "\n",
    "    y_pred = net.predict(X_test)\n",
    "    accs[k] = np.mean(y_pred == y_test)\n",
    "\n",
    "print()\n",
    "print('Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
