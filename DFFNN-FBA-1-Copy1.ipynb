{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784),\n",
       " dtype('float32'),\n",
       " (5000, 784),\n",
       " dtype('float32'),\n",
       " (10000, 784),\n",
       " dtype('float32'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape\n",
    "X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = X_train.mean(axis=0) # mean on the number of training images\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = l.sigmoid(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = l.sigmoid(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.2910 valid loss: 2.3300, valid accuracy: 0.0790\n",
      "Iter-20 train loss: 2.3220 valid loss: 2.3294, valid accuracy: 0.0782\n",
      "Iter-30 train loss: 2.3143 valid loss: 2.3290, valid accuracy: 0.0794\n",
      "Iter-40 train loss: 2.2981 valid loss: 2.3286, valid accuracy: 0.0782\n",
      "Iter-50 train loss: 2.2991 valid loss: 2.3281, valid accuracy: 0.0794\n",
      "Iter-60 train loss: 2.3379 valid loss: 2.3276, valid accuracy: 0.0782\n",
      "Iter-70 train loss: 2.3297 valid loss: 2.3272, valid accuracy: 0.0766\n",
      "Iter-80 train loss: 2.3061 valid loss: 2.3266, valid accuracy: 0.0772\n",
      "Iter-90 train loss: 2.3147 valid loss: 2.3264, valid accuracy: 0.0768\n",
      "Iter-100 train loss: 2.2666 valid loss: 2.3260, valid accuracy: 0.0764\n",
      "Iter-110 train loss: 2.3364 valid loss: 2.3255, valid accuracy: 0.0760\n",
      "Iter-120 train loss: 2.3097 valid loss: 2.3250, valid accuracy: 0.0764\n",
      "Iter-130 train loss: 2.3213 valid loss: 2.3246, valid accuracy: 0.0736\n",
      "Iter-140 train loss: 2.3073 valid loss: 2.3244, valid accuracy: 0.0752\n",
      "Iter-150 train loss: 2.3348 valid loss: 2.3240, valid accuracy: 0.0740\n",
      "Iter-160 train loss: 2.3190 valid loss: 2.3237, valid accuracy: 0.0774\n",
      "Iter-170 train loss: 2.2935 valid loss: 2.3234, valid accuracy: 0.0766\n",
      "Iter-180 train loss: 2.3189 valid loss: 2.3231, valid accuracy: 0.0766\n",
      "Iter-190 train loss: 2.2941 valid loss: 2.3227, valid accuracy: 0.0768\n",
      "Iter-200 train loss: 2.3149 valid loss: 2.3224, valid accuracy: 0.0770\n",
      "Iter-210 train loss: 2.2918 valid loss: 2.3222, valid accuracy: 0.0790\n",
      "Iter-220 train loss: 2.3197 valid loss: 2.3219, valid accuracy: 0.0808\n",
      "Iter-230 train loss: 2.3452 valid loss: 2.3215, valid accuracy: 0.0802\n",
      "Iter-240 train loss: 2.3387 valid loss: 2.3212, valid accuracy: 0.0802\n",
      "Iter-250 train loss: 2.3208 valid loss: 2.3209, valid accuracy: 0.0792\n",
      "Iter-260 train loss: 2.3215 valid loss: 2.3208, valid accuracy: 0.0770\n",
      "Iter-270 train loss: 2.2976 valid loss: 2.3205, valid accuracy: 0.0734\n",
      "Iter-280 train loss: 2.3304 valid loss: 2.3202, valid accuracy: 0.0734\n",
      "Iter-290 train loss: 2.2765 valid loss: 2.3201, valid accuracy: 0.0754\n",
      "Iter-300 train loss: 2.3171 valid loss: 2.3198, valid accuracy: 0.0742\n",
      "Iter-310 train loss: 2.3355 valid loss: 2.3194, valid accuracy: 0.0724\n",
      "Iter-320 train loss: 2.2997 valid loss: 2.3191, valid accuracy: 0.0718\n",
      "Iter-330 train loss: 2.2842 valid loss: 2.3190, valid accuracy: 0.0714\n",
      "Iter-340 train loss: 2.3222 valid loss: 2.3187, valid accuracy: 0.0698\n",
      "Iter-350 train loss: 2.3129 valid loss: 2.3183, valid accuracy: 0.0714\n",
      "Iter-360 train loss: 2.2838 valid loss: 2.3182, valid accuracy: 0.0718\n",
      "Iter-370 train loss: 2.3185 valid loss: 2.3178, valid accuracy: 0.0714\n",
      "Iter-380 train loss: 2.3371 valid loss: 2.3175, valid accuracy: 0.0686\n",
      "Iter-390 train loss: 2.2793 valid loss: 2.3173, valid accuracy: 0.0704\n",
      "Iter-400 train loss: 2.2694 valid loss: 2.3170, valid accuracy: 0.0688\n",
      "Iter-410 train loss: 2.3159 valid loss: 2.3168, valid accuracy: 0.0708\n",
      "Iter-420 train loss: 2.2895 valid loss: 2.3166, valid accuracy: 0.0694\n",
      "Iter-430 train loss: 2.3325 valid loss: 2.3164, valid accuracy: 0.0700\n",
      "Iter-440 train loss: 2.3376 valid loss: 2.3162, valid accuracy: 0.0712\n",
      "Iter-450 train loss: 2.3178 valid loss: 2.3160, valid accuracy: 0.0732\n",
      "Iter-460 train loss: 2.3186 valid loss: 2.3158, valid accuracy: 0.0722\n",
      "Iter-470 train loss: 2.3003 valid loss: 2.3157, valid accuracy: 0.0696\n",
      "Iter-480 train loss: 2.3330 valid loss: 2.3154, valid accuracy: 0.0686\n",
      "Iter-490 train loss: 2.3244 valid loss: 2.3152, valid accuracy: 0.0688\n",
      "Iter-500 train loss: 2.3122 valid loss: 2.3151, valid accuracy: 0.0722\n",
      "Iter-510 train loss: 2.3351 valid loss: 2.3148, valid accuracy: 0.0718\n",
      "Iter-520 train loss: 2.3174 valid loss: 2.3147, valid accuracy: 0.0714\n",
      "Iter-530 train loss: 2.3310 valid loss: 2.3145, valid accuracy: 0.0738\n",
      "Iter-540 train loss: 2.3155 valid loss: 2.3142, valid accuracy: 0.0732\n",
      "Iter-550 train loss: 2.3415 valid loss: 2.3140, valid accuracy: 0.0702\n",
      "Iter-560 train loss: 2.3153 valid loss: 2.3140, valid accuracy: 0.0706\n",
      "Iter-570 train loss: 2.3029 valid loss: 2.3138, valid accuracy: 0.0684\n",
      "Iter-580 train loss: 2.3100 valid loss: 2.3135, valid accuracy: 0.0712\n",
      "Iter-590 train loss: 2.3177 valid loss: 2.3133, valid accuracy: 0.0712\n",
      "Iter-600 train loss: 2.2943 valid loss: 2.3131, valid accuracy: 0.0692\n",
      "Iter-610 train loss: 2.3041 valid loss: 2.3130, valid accuracy: 0.0702\n",
      "Iter-620 train loss: 2.3154 valid loss: 2.3128, valid accuracy: 0.0700\n",
      "Iter-630 train loss: 2.3048 valid loss: 2.3127, valid accuracy: 0.0676\n",
      "Iter-640 train loss: 2.3066 valid loss: 2.3127, valid accuracy: 0.0706\n",
      "Iter-650 train loss: 2.3176 valid loss: 2.3126, valid accuracy: 0.0696\n",
      "Iter-660 train loss: 2.3029 valid loss: 2.3124, valid accuracy: 0.0666\n",
      "Iter-670 train loss: 2.3224 valid loss: 2.3122, valid accuracy: 0.0710\n",
      "Iter-680 train loss: 2.3340 valid loss: 2.3121, valid accuracy: 0.0686\n",
      "Iter-690 train loss: 2.2925 valid loss: 2.3120, valid accuracy: 0.0702\n",
      "Iter-700 train loss: 2.3165 valid loss: 2.3118, valid accuracy: 0.0698\n",
      "Iter-710 train loss: 2.3196 valid loss: 2.3115, valid accuracy: 0.0676\n",
      "Iter-720 train loss: 2.3081 valid loss: 2.3114, valid accuracy: 0.0688\n",
      "Iter-730 train loss: 2.2995 valid loss: 2.3113, valid accuracy: 0.0678\n",
      "Iter-740 train loss: 2.2978 valid loss: 2.3112, valid accuracy: 0.0662\n",
      "Iter-750 train loss: 2.3336 valid loss: 2.3111, valid accuracy: 0.0654\n",
      "Iter-760 train loss: 2.3139 valid loss: 2.3110, valid accuracy: 0.0666\n",
      "Iter-770 train loss: 2.3142 valid loss: 2.3108, valid accuracy: 0.0628\n",
      "Iter-780 train loss: 2.2960 valid loss: 2.3107, valid accuracy: 0.0676\n",
      "Iter-790 train loss: 2.3038 valid loss: 2.3106, valid accuracy: 0.0640\n",
      "Iter-800 train loss: 2.3253 valid loss: 2.3105, valid accuracy: 0.0626\n",
      "Iter-810 train loss: 2.3154 valid loss: 2.3104, valid accuracy: 0.0612\n",
      "Iter-820 train loss: 2.3120 valid loss: 2.3104, valid accuracy: 0.0606\n",
      "Iter-830 train loss: 2.3098 valid loss: 2.3103, valid accuracy: 0.0612\n",
      "Iter-840 train loss: 2.3019 valid loss: 2.3103, valid accuracy: 0.0628\n",
      "Iter-850 train loss: 2.3190 valid loss: 2.3102, valid accuracy: 0.0582\n",
      "Iter-860 train loss: 2.3206 valid loss: 2.3101, valid accuracy: 0.0576\n",
      "Iter-870 train loss: 2.3327 valid loss: 2.3100, valid accuracy: 0.0580\n",
      "Iter-880 train loss: 2.3089 valid loss: 2.3099, valid accuracy: 0.0570\n",
      "Iter-890 train loss: 2.2922 valid loss: 2.3099, valid accuracy: 0.0568\n",
      "Iter-900 train loss: 2.3143 valid loss: 2.3098, valid accuracy: 0.0576\n",
      "Iter-910 train loss: 2.2998 valid loss: 2.3098, valid accuracy: 0.0572\n",
      "Iter-920 train loss: 2.3176 valid loss: 2.3098, valid accuracy: 0.0566\n",
      "Iter-930 train loss: 2.3111 valid loss: 2.3098, valid accuracy: 0.0564\n",
      "Iter-940 train loss: 2.3076 valid loss: 2.3097, valid accuracy: 0.0566\n",
      "Iter-950 train loss: 2.3155 valid loss: 2.3095, valid accuracy: 0.0554\n",
      "Iter-960 train loss: 2.3115 valid loss: 2.3095, valid accuracy: 0.0560\n",
      "Iter-970 train loss: 2.3104 valid loss: 2.3095, valid accuracy: 0.0550\n",
      "Iter-980 train loss: 2.2941 valid loss: 2.3093, valid accuracy: 0.0554\n",
      "Iter-990 train loss: 2.3150 valid loss: 2.3093, valid accuracy: 0.0546\n",
      "Iter-1000 train loss: 2.3124 valid loss: 2.3092, valid accuracy: 0.0560\n",
      "Iter-1010 train loss: 2.3077 valid loss: 2.3092, valid accuracy: 0.0556\n",
      "Iter-1020 train loss: 2.3044 valid loss: 2.3093, valid accuracy: 0.0544\n",
      "Iter-1030 train loss: 2.3065 valid loss: 2.3092, valid accuracy: 0.0558\n",
      "Iter-1040 train loss: 2.2996 valid loss: 2.3092, valid accuracy: 0.0576\n",
      "Iter-1050 train loss: 2.3166 valid loss: 2.3091, valid accuracy: 0.0576\n",
      "Iter-1060 train loss: 2.3024 valid loss: 2.3091, valid accuracy: 0.0586\n",
      "Iter-1070 train loss: 2.3049 valid loss: 2.3091, valid accuracy: 0.0606\n",
      "Iter-1080 train loss: 2.3051 valid loss: 2.3091, valid accuracy: 0.0594\n",
      "Iter-1090 train loss: 2.3214 valid loss: 2.3090, valid accuracy: 0.0590\n",
      "Iter-1100 train loss: 2.3080 valid loss: 2.3089, valid accuracy: 0.0620\n",
      "Iter-1110 train loss: 2.3068 valid loss: 2.3088, valid accuracy: 0.0616\n",
      "Iter-1120 train loss: 2.3026 valid loss: 2.3088, valid accuracy: 0.0584\n",
      "Iter-1130 train loss: 2.3174 valid loss: 2.3087, valid accuracy: 0.0590\n",
      "Iter-1140 train loss: 2.3164 valid loss: 2.3086, valid accuracy: 0.0584\n",
      "Iter-1150 train loss: 2.3005 valid loss: 2.3085, valid accuracy: 0.0606\n",
      "Iter-1160 train loss: 2.3000 valid loss: 2.3085, valid accuracy: 0.0606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.2974 valid loss: 2.3085, valid accuracy: 0.0610\n",
      "Iter-1180 train loss: 2.3039 valid loss: 2.3084, valid accuracy: 0.0608\n",
      "Iter-1190 train loss: 2.3006 valid loss: 2.3084, valid accuracy: 0.0608\n",
      "Iter-1200 train loss: 2.3131 valid loss: 2.3085, valid accuracy: 0.0606\n",
      "Iter-1210 train loss: 2.3020 valid loss: 2.3085, valid accuracy: 0.0606\n",
      "Iter-1220 train loss: 2.3030 valid loss: 2.3084, valid accuracy: 0.0610\n",
      "Iter-1230 train loss: 2.3199 valid loss: 2.3084, valid accuracy: 0.0610\n",
      "Iter-1240 train loss: 2.2947 valid loss: 2.3084, valid accuracy: 0.0606\n",
      "Iter-1250 train loss: 2.3043 valid loss: 2.3084, valid accuracy: 0.0606\n",
      "Iter-1260 train loss: 2.2960 valid loss: 2.3083, valid accuracy: 0.0608\n",
      "Iter-1270 train loss: 2.3037 valid loss: 2.3083, valid accuracy: 0.0638\n",
      "Iter-1280 train loss: 2.2962 valid loss: 2.3083, valid accuracy: 0.0652\n",
      "Iter-1290 train loss: 2.3206 valid loss: 2.3082, valid accuracy: 0.0648\n",
      "Iter-1300 train loss: 2.3025 valid loss: 2.3083, valid accuracy: 0.0644\n",
      "Iter-1310 train loss: 2.3099 valid loss: 2.3082, valid accuracy: 0.0610\n",
      "Iter-1320 train loss: 2.3155 valid loss: 2.3081, valid accuracy: 0.0596\n",
      "Iter-1330 train loss: 2.3107 valid loss: 2.3081, valid accuracy: 0.0556\n",
      "Iter-1340 train loss: 2.2980 valid loss: 2.3081, valid accuracy: 0.0580\n",
      "Iter-1350 train loss: 2.2963 valid loss: 2.3080, valid accuracy: 0.0550\n",
      "Iter-1360 train loss: 2.3184 valid loss: 2.3080, valid accuracy: 0.0550\n",
      "Iter-1370 train loss: 2.3149 valid loss: 2.3080, valid accuracy: 0.0520\n",
      "Iter-1380 train loss: 2.3002 valid loss: 2.3080, valid accuracy: 0.0534\n",
      "Iter-1390 train loss: 2.2976 valid loss: 2.3079, valid accuracy: 0.0520\n",
      "Iter-1400 train loss: 2.3105 valid loss: 2.3078, valid accuracy: 0.0492\n",
      "Iter-1410 train loss: 2.3050 valid loss: 2.3078, valid accuracy: 0.0420\n",
      "Iter-1420 train loss: 2.3115 valid loss: 2.3078, valid accuracy: 0.0406\n",
      "Iter-1430 train loss: 2.3028 valid loss: 2.3078, valid accuracy: 0.0400\n",
      "Iter-1440 train loss: 2.3119 valid loss: 2.3077, valid accuracy: 0.0420\n",
      "Iter-1450 train loss: 2.3040 valid loss: 2.3077, valid accuracy: 0.0412\n",
      "Iter-1460 train loss: 2.3087 valid loss: 2.3076, valid accuracy: 0.0412\n",
      "Iter-1470 train loss: 2.3093 valid loss: 2.3076, valid accuracy: 0.0406\n",
      "Iter-1480 train loss: 2.2994 valid loss: 2.3077, valid accuracy: 0.0422\n",
      "Iter-1490 train loss: 2.3009 valid loss: 2.3077, valid accuracy: 0.0432\n",
      "Iter-1500 train loss: 2.3078 valid loss: 2.3077, valid accuracy: 0.0428\n",
      "Iter-1510 train loss: 2.3014 valid loss: 2.3077, valid accuracy: 0.0432\n",
      "Iter-1520 train loss: 2.3116 valid loss: 2.3076, valid accuracy: 0.0484\n",
      "Iter-1530 train loss: 2.3018 valid loss: 2.3076, valid accuracy: 0.0544\n",
      "Iter-1540 train loss: 2.2977 valid loss: 2.3075, valid accuracy: 0.0582\n",
      "Iter-1550 train loss: 2.3060 valid loss: 2.3075, valid accuracy: 0.0546\n",
      "Iter-1560 train loss: 2.3071 valid loss: 2.3075, valid accuracy: 0.0556\n",
      "Iter-1570 train loss: 2.3036 valid loss: 2.3075, valid accuracy: 0.0626\n",
      "Iter-1580 train loss: 2.3149 valid loss: 2.3075, valid accuracy: 0.0628\n",
      "Iter-1590 train loss: 2.3060 valid loss: 2.3075, valid accuracy: 0.0622\n",
      "Iter-1600 train loss: 2.3015 valid loss: 2.3074, valid accuracy: 0.0616\n",
      "Iter-1610 train loss: 2.3073 valid loss: 2.3074, valid accuracy: 0.0640\n",
      "Iter-1620 train loss: 2.3040 valid loss: 2.3074, valid accuracy: 0.0630\n",
      "Iter-1630 train loss: 2.3069 valid loss: 2.3074, valid accuracy: 0.0638\n",
      "Iter-1640 train loss: 2.3164 valid loss: 2.3073, valid accuracy: 0.0694\n",
      "Iter-1650 train loss: 2.3029 valid loss: 2.3074, valid accuracy: 0.0678\n",
      "Iter-1660 train loss: 2.3003 valid loss: 2.3073, valid accuracy: 0.0704\n",
      "Iter-1670 train loss: 2.3006 valid loss: 2.3073, valid accuracy: 0.0744\n",
      "Iter-1680 train loss: 2.3164 valid loss: 2.3073, valid accuracy: 0.0694\n",
      "Iter-1690 train loss: 2.3104 valid loss: 2.3073, valid accuracy: 0.0732\n",
      "Iter-1700 train loss: 2.3187 valid loss: 2.3073, valid accuracy: 0.0744\n",
      "Iter-1710 train loss: 2.3130 valid loss: 2.3072, valid accuracy: 0.0826\n",
      "Iter-1720 train loss: 2.2911 valid loss: 2.3073, valid accuracy: 0.0788\n",
      "Iter-1730 train loss: 2.3043 valid loss: 2.3073, valid accuracy: 0.0764\n",
      "Iter-1740 train loss: 2.3176 valid loss: 2.3074, valid accuracy: 0.0728\n",
      "Iter-1750 train loss: 2.3059 valid loss: 2.3074, valid accuracy: 0.0794\n",
      "Iter-1760 train loss: 2.3185 valid loss: 2.3073, valid accuracy: 0.0838\n",
      "Iter-1770 train loss: 2.3090 valid loss: 2.3073, valid accuracy: 0.0862\n",
      "Iter-1780 train loss: 2.3186 valid loss: 2.3073, valid accuracy: 0.0856\n",
      "Iter-1790 train loss: 2.2975 valid loss: 2.3073, valid accuracy: 0.0874\n",
      "Iter-1800 train loss: 2.3045 valid loss: 2.3072, valid accuracy: 0.0916\n",
      "Iter-1810 train loss: 2.3177 valid loss: 2.3072, valid accuracy: 0.0924\n",
      "Iter-1820 train loss: 2.3042 valid loss: 2.3072, valid accuracy: 0.0924\n",
      "Iter-1830 train loss: 2.3023 valid loss: 2.3072, valid accuracy: 0.0988\n",
      "Iter-1840 train loss: 2.3229 valid loss: 2.3071, valid accuracy: 0.0966\n",
      "Iter-1850 train loss: 2.2992 valid loss: 2.3071, valid accuracy: 0.0934\n",
      "Iter-1860 train loss: 2.3116 valid loss: 2.3072, valid accuracy: 0.0938\n",
      "Iter-1870 train loss: 2.3011 valid loss: 2.3071, valid accuracy: 0.0928\n",
      "Iter-1880 train loss: 2.3028 valid loss: 2.3071, valid accuracy: 0.0988\n",
      "Iter-1890 train loss: 2.2970 valid loss: 2.3071, valid accuracy: 0.0998\n",
      "Iter-1900 train loss: 2.3007 valid loss: 2.3072, valid accuracy: 0.0996\n",
      "Iter-1910 train loss: 2.3107 valid loss: 2.3072, valid accuracy: 0.0984\n",
      "Iter-1920 train loss: 2.3096 valid loss: 2.3072, valid accuracy: 0.1018\n",
      "Iter-1930 train loss: 2.2940 valid loss: 2.3071, valid accuracy: 0.1012\n",
      "Iter-1940 train loss: 2.3058 valid loss: 2.3071, valid accuracy: 0.0940\n",
      "Iter-1950 train loss: 2.3060 valid loss: 2.3071, valid accuracy: 0.0914\n",
      "Iter-1960 train loss: 2.3087 valid loss: 2.3071, valid accuracy: 0.0914\n",
      "Iter-1970 train loss: 2.3028 valid loss: 2.3072, valid accuracy: 0.0886\n",
      "Iter-1980 train loss: 2.3004 valid loss: 2.3072, valid accuracy: 0.0878\n",
      "Iter-1990 train loss: 2.2980 valid loss: 2.3072, valid accuracy: 0.0900\n",
      "Iter-2000 train loss: 2.3068 valid loss: 2.3072, valid accuracy: 0.0930\n",
      "Iter-2010 train loss: 2.3046 valid loss: 2.3072, valid accuracy: 0.0968\n",
      "Iter-2020 train loss: 2.3066 valid loss: 2.3071, valid accuracy: 0.0956\n",
      "Iter-2030 train loss: 2.3015 valid loss: 2.3071, valid accuracy: 0.0962\n",
      "Iter-2040 train loss: 2.3050 valid loss: 2.3072, valid accuracy: 0.1022\n",
      "Iter-2050 train loss: 2.3104 valid loss: 2.3072, valid accuracy: 0.1072\n",
      "Iter-2060 train loss: 2.3081 valid loss: 2.3071, valid accuracy: 0.1062\n",
      "Iter-2070 train loss: 2.3030 valid loss: 2.3071, valid accuracy: 0.1092\n",
      "Iter-2080 train loss: 2.2988 valid loss: 2.3072, valid accuracy: 0.1114\n",
      "Iter-2090 train loss: 2.3110 valid loss: 2.3072, valid accuracy: 0.1092\n",
      "Iter-2100 train loss: 2.3002 valid loss: 2.3072, valid accuracy: 0.1086\n",
      "Iter-2110 train loss: 2.3038 valid loss: 2.3072, valid accuracy: 0.1092\n",
      "Iter-2120 train loss: 2.3067 valid loss: 2.3072, valid accuracy: 0.1082\n",
      "Iter-2130 train loss: 2.3212 valid loss: 2.3071, valid accuracy: 0.1114\n",
      "Iter-2140 train loss: 2.3147 valid loss: 2.3071, valid accuracy: 0.1118\n",
      "Iter-2150 train loss: 2.3184 valid loss: 2.3071, valid accuracy: 0.1120\n",
      "Iter-2160 train loss: 2.3034 valid loss: 2.3071, valid accuracy: 0.1126\n",
      "Iter-2170 train loss: 2.3045 valid loss: 2.3071, valid accuracy: 0.1126\n",
      "Iter-2180 train loss: 2.3097 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2190 train loss: 2.3059 valid loss: 2.3070, valid accuracy: 0.1124\n",
      "Iter-2200 train loss: 2.3116 valid loss: 2.3070, valid accuracy: 0.1128\n",
      "Iter-2210 train loss: 2.3008 valid loss: 2.3071, valid accuracy: 0.1124\n",
      "Iter-2220 train loss: 2.2993 valid loss: 2.3071, valid accuracy: 0.1124\n",
      "Iter-2230 train loss: 2.3057 valid loss: 2.3071, valid accuracy: 0.1124\n",
      "Iter-2240 train loss: 2.2999 valid loss: 2.3071, valid accuracy: 0.1124\n",
      "Iter-2250 train loss: 2.3070 valid loss: 2.3072, valid accuracy: 0.1124\n",
      "Iter-2260 train loss: 2.3086 valid loss: 2.3072, valid accuracy: 0.1126\n",
      "Iter-2270 train loss: 2.3084 valid loss: 2.3072, valid accuracy: 0.1124\n",
      "Iter-2280 train loss: 2.3082 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2290 train loss: 2.2935 valid loss: 2.3072, valid accuracy: 0.1124\n",
      "Iter-2300 train loss: 2.2966 valid loss: 2.3072, valid accuracy: 0.1128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3001 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2320 train loss: 2.3049 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2330 train loss: 2.3112 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2340 train loss: 2.2998 valid loss: 2.3071, valid accuracy: 0.1126\n",
      "Iter-2350 train loss: 2.3045 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2360 train loss: 2.3029 valid loss: 2.3071, valid accuracy: 0.1126\n",
      "Iter-2370 train loss: 2.3081 valid loss: 2.3071, valid accuracy: 0.1128\n",
      "Iter-2380 train loss: 2.3004 valid loss: 2.3071, valid accuracy: 0.1130\n",
      "Iter-2390 train loss: 2.3015 valid loss: 2.3070, valid accuracy: 0.1128\n",
      "Iter-2400 train loss: 2.3089 valid loss: 2.3070, valid accuracy: 0.1128\n",
      "Iter-2410 train loss: 2.2935 valid loss: 2.3070, valid accuracy: 0.1128\n",
      "Iter-2420 train loss: 2.3196 valid loss: 2.3070, valid accuracy: 0.1128\n",
      "Iter-2430 train loss: 2.3086 valid loss: 2.3069, valid accuracy: 0.1126\n",
      "Iter-2440 train loss: 2.3172 valid loss: 2.3069, valid accuracy: 0.1126\n",
      "Iter-2450 train loss: 2.3087 valid loss: 2.3069, valid accuracy: 0.1130\n",
      "Iter-2460 train loss: 2.3095 valid loss: 2.3070, valid accuracy: 0.1130\n",
      "Iter-2470 train loss: 2.3050 valid loss: 2.3070, valid accuracy: 0.1134\n",
      "Iter-2480 train loss: 2.3115 valid loss: 2.3070, valid accuracy: 0.1144\n",
      "Iter-2490 train loss: 2.2995 valid loss: 2.3070, valid accuracy: 0.1152\n",
      "Iter-2500 train loss: 2.3052 valid loss: 2.3070, valid accuracy: 0.1146\n",
      "Iter-2510 train loss: 2.3209 valid loss: 2.3070, valid accuracy: 0.1138\n",
      "Iter-2520 train loss: 2.2967 valid loss: 2.3070, valid accuracy: 0.1132\n",
      "Iter-2530 train loss: 2.3133 valid loss: 2.3070, valid accuracy: 0.1132\n",
      "Iter-2540 train loss: 2.2915 valid loss: 2.3070, valid accuracy: 0.1136\n",
      "Iter-2550 train loss: 2.2984 valid loss: 2.3070, valid accuracy: 0.1136\n",
      "Iter-2560 train loss: 2.3047 valid loss: 2.3070, valid accuracy: 0.1132\n",
      "Iter-2570 train loss: 2.3039 valid loss: 2.3070, valid accuracy: 0.1130\n",
      "Iter-2580 train loss: 2.2970 valid loss: 2.3070, valid accuracy: 0.1130\n",
      "Iter-2590 train loss: 2.3012 valid loss: 2.3070, valid accuracy: 0.1130\n",
      "Iter-2600 train loss: 2.2953 valid loss: 2.3070, valid accuracy: 0.1130\n",
      "Iter-2610 train loss: 2.2923 valid loss: 2.3070, valid accuracy: 0.1134\n",
      "Iter-2620 train loss: 2.2981 valid loss: 2.3069, valid accuracy: 0.1150\n",
      "Iter-2630 train loss: 2.2987 valid loss: 2.3069, valid accuracy: 0.1150\n",
      "Iter-2640 train loss: 2.3061 valid loss: 2.3069, valid accuracy: 0.1136\n",
      "Iter-2650 train loss: 2.3091 valid loss: 2.3069, valid accuracy: 0.1138\n",
      "Iter-2660 train loss: 2.3172 valid loss: 2.3069, valid accuracy: 0.1142\n",
      "Iter-2670 train loss: 2.3136 valid loss: 2.3069, valid accuracy: 0.1138\n",
      "Iter-2680 train loss: 2.3099 valid loss: 2.3070, valid accuracy: 0.1150\n",
      "Iter-2690 train loss: 2.3090 valid loss: 2.3069, valid accuracy: 0.1150\n",
      "Iter-2700 train loss: 2.3178 valid loss: 2.3069, valid accuracy: 0.1142\n",
      "Iter-2710 train loss: 2.3106 valid loss: 2.3069, valid accuracy: 0.1140\n",
      "Iter-2720 train loss: 2.3053 valid loss: 2.3069, valid accuracy: 0.1134\n",
      "Iter-2730 train loss: 2.3087 valid loss: 2.3069, valid accuracy: 0.1138\n",
      "Iter-2740 train loss: 2.3123 valid loss: 2.3069, valid accuracy: 0.1134\n",
      "Iter-2750 train loss: 2.3067 valid loss: 2.3069, valid accuracy: 0.1136\n",
      "Iter-2760 train loss: 2.2904 valid loss: 2.3068, valid accuracy: 0.1138\n",
      "Iter-2770 train loss: 2.3176 valid loss: 2.3069, valid accuracy: 0.1142\n",
      "Iter-2780 train loss: 2.3154 valid loss: 2.3068, valid accuracy: 0.1152\n",
      "Iter-2790 train loss: 2.2944 valid loss: 2.3068, valid accuracy: 0.1132\n",
      "Iter-2800 train loss: 2.2993 valid loss: 2.3068, valid accuracy: 0.1138\n",
      "Iter-2810 train loss: 2.3121 valid loss: 2.3068, valid accuracy: 0.1134\n",
      "Iter-2820 train loss: 2.3074 valid loss: 2.3068, valid accuracy: 0.1138\n",
      "Iter-2830 train loss: 2.3046 valid loss: 2.3068, valid accuracy: 0.1134\n",
      "Iter-2840 train loss: 2.2997 valid loss: 2.3068, valid accuracy: 0.1132\n",
      "Iter-2850 train loss: 2.3039 valid loss: 2.3068, valid accuracy: 0.1132\n",
      "Iter-2860 train loss: 2.3147 valid loss: 2.3068, valid accuracy: 0.1132\n",
      "Iter-2870 train loss: 2.3116 valid loss: 2.3068, valid accuracy: 0.1134\n",
      "Iter-2880 train loss: 2.3151 valid loss: 2.3068, valid accuracy: 0.1144\n",
      "Iter-2890 train loss: 2.2950 valid loss: 2.3068, valid accuracy: 0.1150\n",
      "Iter-2900 train loss: 2.3000 valid loss: 2.3067, valid accuracy: 0.1136\n",
      "Iter-2910 train loss: 2.3183 valid loss: 2.3067, valid accuracy: 0.1134\n",
      "Iter-2920 train loss: 2.2905 valid loss: 2.3067, valid accuracy: 0.1134\n",
      "Iter-2930 train loss: 2.3071 valid loss: 2.3067, valid accuracy: 0.1132\n",
      "Iter-2940 train loss: 2.3160 valid loss: 2.3067, valid accuracy: 0.1132\n",
      "Iter-2950 train loss: 2.3067 valid loss: 2.3067, valid accuracy: 0.1128\n",
      "Iter-2960 train loss: 2.3029 valid loss: 2.3066, valid accuracy: 0.1128\n",
      "Iter-2970 train loss: 2.3135 valid loss: 2.3067, valid accuracy: 0.1126\n",
      "Iter-2980 train loss: 2.3278 valid loss: 2.3067, valid accuracy: 0.1128\n",
      "Iter-2990 train loss: 2.3056 valid loss: 2.3066, valid accuracy: 0.1128\n",
      "Iter-3000 train loss: 2.2979 valid loss: 2.3066, valid accuracy: 0.1126\n",
      "Iter-3010 train loss: 2.3018 valid loss: 2.3065, valid accuracy: 0.1126\n",
      "Iter-3020 train loss: 2.3065 valid loss: 2.3065, valid accuracy: 0.1130\n",
      "Iter-3030 train loss: 2.3042 valid loss: 2.3065, valid accuracy: 0.1128\n",
      "Iter-3040 train loss: 2.2892 valid loss: 2.3064, valid accuracy: 0.1128\n",
      "Iter-3050 train loss: 2.2968 valid loss: 2.3064, valid accuracy: 0.1128\n",
      "Iter-3060 train loss: 2.3095 valid loss: 2.3064, valid accuracy: 0.1128\n",
      "Iter-3070 train loss: 2.3163 valid loss: 2.3064, valid accuracy: 0.1126\n",
      "Iter-3080 train loss: 2.3077 valid loss: 2.3064, valid accuracy: 0.1126\n",
      "Iter-3090 train loss: 2.3007 valid loss: 2.3064, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3131 valid loss: 2.3064, valid accuracy: 0.1126\n",
      "Iter-3110 train loss: 2.3066 valid loss: 2.3064, valid accuracy: 0.1126\n",
      "Iter-3120 train loss: 2.3191 valid loss: 2.3063, valid accuracy: 0.1126\n",
      "Iter-3130 train loss: 2.3132 valid loss: 2.3063, valid accuracy: 0.1126\n",
      "Iter-3140 train loss: 2.2945 valid loss: 2.3063, valid accuracy: 0.1126\n",
      "Iter-3150 train loss: 2.3099 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3160 train loss: 2.3077 valid loss: 2.3062, valid accuracy: 0.1128\n",
      "Iter-3170 train loss: 2.2879 valid loss: 2.3062, valid accuracy: 0.1128\n",
      "Iter-3180 train loss: 2.3137 valid loss: 2.3062, valid accuracy: 0.1128\n",
      "Iter-3190 train loss: 2.3121 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3025 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3210 train loss: 2.3053 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3220 train loss: 2.3073 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3230 train loss: 2.3044 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3240 train loss: 2.3114 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3250 train loss: 2.3055 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3260 train loss: 2.3055 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3270 train loss: 2.3042 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3280 train loss: 2.3098 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3290 train loss: 2.3084 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.3021 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-3310 train loss: 2.3137 valid loss: 2.3061, valid accuracy: 0.1126\n",
      "Iter-3320 train loss: 2.3121 valid loss: 2.3061, valid accuracy: 0.1128\n",
      "Iter-3330 train loss: 2.2915 valid loss: 2.3061, valid accuracy: 0.1128\n",
      "Iter-3340 train loss: 2.3135 valid loss: 2.3061, valid accuracy: 0.1132\n",
      "Iter-3350 train loss: 2.3042 valid loss: 2.3061, valid accuracy: 0.1132\n",
      "Iter-3360 train loss: 2.3139 valid loss: 2.3060, valid accuracy: 0.1134\n",
      "Iter-3370 train loss: 2.3224 valid loss: 2.3060, valid accuracy: 0.1142\n",
      "Iter-3380 train loss: 2.3070 valid loss: 2.3060, valid accuracy: 0.1154\n",
      "Iter-3390 train loss: 2.3123 valid loss: 2.3060, valid accuracy: 0.1154\n",
      "Iter-3400 train loss: 2.3055 valid loss: 2.3060, valid accuracy: 0.1144\n",
      "Iter-3410 train loss: 2.3141 valid loss: 2.3059, valid accuracy: 0.1158\n",
      "Iter-3420 train loss: 2.3022 valid loss: 2.3059, valid accuracy: 0.1158\n",
      "Iter-3430 train loss: 2.3026 valid loss: 2.3059, valid accuracy: 0.1166\n",
      "Iter-3440 train loss: 2.2954 valid loss: 2.3059, valid accuracy: 0.1180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3020 valid loss: 2.3059, valid accuracy: 0.1168\n",
      "Iter-3460 train loss: 2.3072 valid loss: 2.3058, valid accuracy: 0.1166\n",
      "Iter-3470 train loss: 2.3004 valid loss: 2.3058, valid accuracy: 0.1156\n",
      "Iter-3480 train loss: 2.3065 valid loss: 2.3058, valid accuracy: 0.1160\n",
      "Iter-3490 train loss: 2.3134 valid loss: 2.3058, valid accuracy: 0.1158\n",
      "Iter-3500 train loss: 2.2940 valid loss: 2.3058, valid accuracy: 0.1158\n",
      "Iter-3510 train loss: 2.3129 valid loss: 2.3058, valid accuracy: 0.1152\n",
      "Iter-3520 train loss: 2.3054 valid loss: 2.3058, valid accuracy: 0.1140\n",
      "Iter-3530 train loss: 2.2999 valid loss: 2.3057, valid accuracy: 0.1138\n",
      "Iter-3540 train loss: 2.3065 valid loss: 2.3057, valid accuracy: 0.1140\n",
      "Iter-3550 train loss: 2.3094 valid loss: 2.3057, valid accuracy: 0.1140\n",
      "Iter-3560 train loss: 2.3060 valid loss: 2.3057, valid accuracy: 0.1134\n",
      "Iter-3570 train loss: 2.2982 valid loss: 2.3057, valid accuracy: 0.1128\n",
      "Iter-3580 train loss: 2.2953 valid loss: 2.3056, valid accuracy: 0.1128\n",
      "Iter-3590 train loss: 2.2938 valid loss: 2.3057, valid accuracy: 0.1128\n",
      "Iter-3600 train loss: 2.3042 valid loss: 2.3056, valid accuracy: 0.1128\n",
      "Iter-3610 train loss: 2.3103 valid loss: 2.3057, valid accuracy: 0.1128\n",
      "Iter-3620 train loss: 2.3047 valid loss: 2.3056, valid accuracy: 0.1128\n",
      "Iter-3630 train loss: 2.2999 valid loss: 2.3056, valid accuracy: 0.1128\n",
      "Iter-3640 train loss: 2.2958 valid loss: 2.3056, valid accuracy: 0.1126\n",
      "Iter-3650 train loss: 2.3204 valid loss: 2.3056, valid accuracy: 0.1126\n",
      "Iter-3660 train loss: 2.3142 valid loss: 2.3056, valid accuracy: 0.1126\n",
      "Iter-3670 train loss: 2.2899 valid loss: 2.3055, valid accuracy: 0.1126\n",
      "Iter-3680 train loss: 2.2985 valid loss: 2.3055, valid accuracy: 0.1126\n",
      "Iter-3690 train loss: 2.2935 valid loss: 2.3055, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3145 valid loss: 2.3055, valid accuracy: 0.1126\n",
      "Iter-3710 train loss: 2.3018 valid loss: 2.3054, valid accuracy: 0.1126\n",
      "Iter-3720 train loss: 2.3090 valid loss: 2.3054, valid accuracy: 0.1126\n",
      "Iter-3730 train loss: 2.3053 valid loss: 2.3054, valid accuracy: 0.1126\n",
      "Iter-3740 train loss: 2.3051 valid loss: 2.3054, valid accuracy: 0.1126\n",
      "Iter-3750 train loss: 2.3170 valid loss: 2.3054, valid accuracy: 0.1126\n",
      "Iter-3760 train loss: 2.2927 valid loss: 2.3054, valid accuracy: 0.1126\n",
      "Iter-3770 train loss: 2.2972 valid loss: 2.3053, valid accuracy: 0.1126\n",
      "Iter-3780 train loss: 2.3050 valid loss: 2.3053, valid accuracy: 0.1126\n",
      "Iter-3790 train loss: 2.2817 valid loss: 2.3053, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3209 valid loss: 2.3053, valid accuracy: 0.1126\n",
      "Iter-3810 train loss: 2.2952 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3820 train loss: 2.3182 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3830 train loss: 2.3000 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3840 train loss: 2.3114 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3850 train loss: 2.3023 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3860 train loss: 2.3059 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3870 train loss: 2.2962 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-3880 train loss: 2.2956 valid loss: 2.3051, valid accuracy: 0.1126\n",
      "Iter-3890 train loss: 2.3026 valid loss: 2.3051, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3184 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-3910 train loss: 2.2988 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-3920 train loss: 2.3055 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-3930 train loss: 2.3086 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-3940 train loss: 2.3055 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-3950 train loss: 2.2986 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-3960 train loss: 2.2969 valid loss: 2.3049, valid accuracy: 0.1126\n",
      "Iter-3970 train loss: 2.3059 valid loss: 2.3049, valid accuracy: 0.1126\n",
      "Iter-3980 train loss: 2.3107 valid loss: 2.3049, valid accuracy: 0.1126\n",
      "Iter-3990 train loss: 2.2909 valid loss: 2.3049, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3112 valid loss: 2.3048, valid accuracy: 0.1126\n",
      "Iter-4010 train loss: 2.3085 valid loss: 2.3048, valid accuracy: 0.1126\n",
      "Iter-4020 train loss: 2.2833 valid loss: 2.3048, valid accuracy: 0.1126\n",
      "Iter-4030 train loss: 2.3018 valid loss: 2.3048, valid accuracy: 0.1126\n",
      "Iter-4040 train loss: 2.3110 valid loss: 2.3047, valid accuracy: 0.1126\n",
      "Iter-4050 train loss: 2.3008 valid loss: 2.3046, valid accuracy: 0.1126\n",
      "Iter-4060 train loss: 2.2959 valid loss: 2.3046, valid accuracy: 0.1126\n",
      "Iter-4070 train loss: 2.2969 valid loss: 2.3046, valid accuracy: 0.1126\n",
      "Iter-4080 train loss: 2.3022 valid loss: 2.3045, valid accuracy: 0.1126\n",
      "Iter-4090 train loss: 2.3176 valid loss: 2.3045, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3092 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-4110 train loss: 2.3186 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-4120 train loss: 2.3096 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-4130 train loss: 2.3137 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-4140 train loss: 2.3034 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-4150 train loss: 2.2847 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-4160 train loss: 2.3016 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-4170 train loss: 2.3067 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-4180 train loss: 2.3069 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-4190 train loss: 2.3066 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3022 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-4210 train loss: 2.3096 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-4220 train loss: 2.2972 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-4230 train loss: 2.2984 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-4240 train loss: 2.3075 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-4250 train loss: 2.2971 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-4260 train loss: 2.3074 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-4270 train loss: 2.3093 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-4280 train loss: 2.3017 valid loss: 2.3039, valid accuracy: 0.1126\n",
      "Iter-4290 train loss: 2.3046 valid loss: 2.3039, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.3037 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-4310 train loss: 2.2991 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-4320 train loss: 2.3053 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-4330 train loss: 2.2974 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-4340 train loss: 2.3041 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-4350 train loss: 2.2993 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-4360 train loss: 2.3084 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-4370 train loss: 2.2981 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-4380 train loss: 2.3218 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-4390 train loss: 2.3017 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.3025 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-4410 train loss: 2.3138 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-4420 train loss: 2.2906 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-4430 train loss: 2.3167 valid loss: 2.3034, valid accuracy: 0.1126\n",
      "Iter-4440 train loss: 2.2897 valid loss: 2.3034, valid accuracy: 0.1126\n",
      "Iter-4450 train loss: 2.3041 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-4460 train loss: 2.2978 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-4470 train loss: 2.3016 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-4480 train loss: 2.2975 valid loss: 2.3032, valid accuracy: 0.1126\n",
      "Iter-4490 train loss: 2.2926 valid loss: 2.3032, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.2969 valid loss: 2.3032, valid accuracy: 0.1126\n",
      "Iter-4510 train loss: 2.3015 valid loss: 2.3031, valid accuracy: 0.1126\n",
      "Iter-4520 train loss: 2.3100 valid loss: 2.3031, valid accuracy: 0.1126\n",
      "Iter-4530 train loss: 2.3011 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-4540 train loss: 2.2885 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-4550 train loss: 2.3177 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-4560 train loss: 2.2979 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-4570 train loss: 2.3124 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-4580 train loss: 2.3063 valid loss: 2.3028, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3193 valid loss: 2.3028, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.3061 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-4610 train loss: 2.2846 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-4620 train loss: 2.2986 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-4630 train loss: 2.3051 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-4640 train loss: 2.3074 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-4650 train loss: 2.3006 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-4660 train loss: 2.3050 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-4670 train loss: 2.2957 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-4680 train loss: 2.2931 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-4690 train loss: 2.2947 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.3148 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-4710 train loss: 2.3133 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-4720 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4730 train loss: 2.2897 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4740 train loss: 2.3162 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4750 train loss: 2.2979 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-4760 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-4770 train loss: 2.2944 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-4780 train loss: 2.2970 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-4790 train loss: 2.2840 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.3196 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4810 train loss: 2.3085 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4820 train loss: 2.3001 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4830 train loss: 2.2875 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4840 train loss: 2.3223 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4850 train loss: 2.3009 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4860 train loss: 2.2925 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4870 train loss: 2.3142 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4880 train loss: 2.2955 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4890 train loss: 2.2994 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.2974 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-4910 train loss: 2.2976 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-4920 train loss: 2.3133 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-4930 train loss: 2.3163 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-4940 train loss: 2.3233 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-4950 train loss: 2.2980 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-4960 train loss: 2.2882 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-4970 train loss: 2.3034 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-4980 train loss: 2.3049 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-4990 train loss: 2.3040 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3103 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-5010 train loss: 2.3178 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5020 train loss: 2.2912 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5030 train loss: 2.2938 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5040 train loss: 2.2908 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5050 train loss: 2.3115 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5060 train loss: 2.3019 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-5070 train loss: 2.3139 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-5080 train loss: 2.2843 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-5090 train loss: 2.3111 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3088 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-5110 train loss: 2.3064 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-5120 train loss: 2.3118 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-5130 train loss: 2.2932 valid loss: 2.3004, valid accuracy: 0.1126\n",
      "Iter-5140 train loss: 2.3020 valid loss: 2.3004, valid accuracy: 0.1126\n",
      "Iter-5150 train loss: 2.2948 valid loss: 2.3003, valid accuracy: 0.1126\n",
      "Iter-5160 train loss: 2.3065 valid loss: 2.3003, valid accuracy: 0.1126\n",
      "Iter-5170 train loss: 2.2928 valid loss: 2.3003, valid accuracy: 0.1126\n",
      "Iter-5180 train loss: 2.2996 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-5190 train loss: 2.3007 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.3039 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-5210 train loss: 2.2976 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-5220 train loss: 2.2986 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-5230 train loss: 2.3081 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-5240 train loss: 2.2989 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-5250 train loss: 2.3034 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-5260 train loss: 2.2901 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-5270 train loss: 2.2909 valid loss: 2.2996, valid accuracy: 0.1126\n",
      "Iter-5280 train loss: 2.2892 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-5290 train loss: 2.2979 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.2975 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-5310 train loss: 2.2981 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-5320 train loss: 2.3104 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-5330 train loss: 2.3003 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-5340 train loss: 2.3010 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-5350 train loss: 2.2911 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-5360 train loss: 2.3068 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-5370 train loss: 2.3044 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-5380 train loss: 2.2981 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-5390 train loss: 2.2936 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.2957 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-5410 train loss: 2.3089 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-5420 train loss: 2.3018 valid loss: 2.2987, valid accuracy: 0.1126\n",
      "Iter-5430 train loss: 2.3091 valid loss: 2.2986, valid accuracy: 0.1126\n",
      "Iter-5440 train loss: 2.2876 valid loss: 2.2985, valid accuracy: 0.1126\n",
      "Iter-5450 train loss: 2.2992 valid loss: 2.2985, valid accuracy: 0.1128\n",
      "Iter-5460 train loss: 2.2946 valid loss: 2.2984, valid accuracy: 0.1130\n",
      "Iter-5470 train loss: 2.3017 valid loss: 2.2984, valid accuracy: 0.1128\n",
      "Iter-5480 train loss: 2.3060 valid loss: 2.2983, valid accuracy: 0.1128\n",
      "Iter-5490 train loss: 2.2904 valid loss: 2.2982, valid accuracy: 0.1130\n",
      "Iter-5500 train loss: 2.2928 valid loss: 2.2981, valid accuracy: 0.1130\n",
      "Iter-5510 train loss: 2.3037 valid loss: 2.2981, valid accuracy: 0.1128\n",
      "Iter-5520 train loss: 2.3066 valid loss: 2.2980, valid accuracy: 0.1128\n",
      "Iter-5530 train loss: 2.3014 valid loss: 2.2979, valid accuracy: 0.1130\n",
      "Iter-5540 train loss: 2.2872 valid loss: 2.2979, valid accuracy: 0.1130\n",
      "Iter-5550 train loss: 2.2973 valid loss: 2.2978, valid accuracy: 0.1130\n",
      "Iter-5560 train loss: 2.3031 valid loss: 2.2978, valid accuracy: 0.1130\n",
      "Iter-5570 train loss: 2.2944 valid loss: 2.2977, valid accuracy: 0.1130\n",
      "Iter-5580 train loss: 2.3154 valid loss: 2.2977, valid accuracy: 0.1130\n",
      "Iter-5590 train loss: 2.2944 valid loss: 2.2976, valid accuracy: 0.1130\n",
      "Iter-5600 train loss: 2.3000 valid loss: 2.2976, valid accuracy: 0.1130\n",
      "Iter-5610 train loss: 2.3028 valid loss: 2.2975, valid accuracy: 0.1130\n",
      "Iter-5620 train loss: 2.3053 valid loss: 2.2974, valid accuracy: 0.1130\n",
      "Iter-5630 train loss: 2.3034 valid loss: 2.2974, valid accuracy: 0.1130\n",
      "Iter-5640 train loss: 2.3070 valid loss: 2.2973, valid accuracy: 0.1130\n",
      "Iter-5650 train loss: 2.3144 valid loss: 2.2972, valid accuracy: 0.1130\n",
      "Iter-5660 train loss: 2.2940 valid loss: 2.2972, valid accuracy: 0.1130\n",
      "Iter-5670 train loss: 2.2972 valid loss: 2.2971, valid accuracy: 0.1130\n",
      "Iter-5680 train loss: 2.2940 valid loss: 2.2971, valid accuracy: 0.1130\n",
      "Iter-5690 train loss: 2.2989 valid loss: 2.2970, valid accuracy: 0.1130\n",
      "Iter-5700 train loss: 2.3120 valid loss: 2.2970, valid accuracy: 0.1130\n",
      "Iter-5710 train loss: 2.2980 valid loss: 2.2969, valid accuracy: 0.1130\n",
      "Iter-5720 train loss: 2.2843 valid loss: 2.2968, valid accuracy: 0.1130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.2995 valid loss: 2.2967, valid accuracy: 0.1130\n",
      "Iter-5740 train loss: 2.2877 valid loss: 2.2967, valid accuracy: 0.1130\n",
      "Iter-5750 train loss: 2.2877 valid loss: 2.2966, valid accuracy: 0.1130\n",
      "Iter-5760 train loss: 2.2933 valid loss: 2.2965, valid accuracy: 0.1130\n",
      "Iter-5770 train loss: 2.3013 valid loss: 2.2965, valid accuracy: 0.1130\n",
      "Iter-5780 train loss: 2.2930 valid loss: 2.2964, valid accuracy: 0.1130\n",
      "Iter-5790 train loss: 2.2934 valid loss: 2.2963, valid accuracy: 0.1130\n",
      "Iter-5800 train loss: 2.2922 valid loss: 2.2962, valid accuracy: 0.1130\n",
      "Iter-5810 train loss: 2.2939 valid loss: 2.2962, valid accuracy: 0.1130\n",
      "Iter-5820 train loss: 2.3058 valid loss: 2.2961, valid accuracy: 0.1130\n",
      "Iter-5830 train loss: 2.3023 valid loss: 2.2960, valid accuracy: 0.1130\n",
      "Iter-5840 train loss: 2.2984 valid loss: 2.2960, valid accuracy: 0.1130\n",
      "Iter-5850 train loss: 2.3067 valid loss: 2.2959, valid accuracy: 0.1130\n",
      "Iter-5860 train loss: 2.3021 valid loss: 2.2959, valid accuracy: 0.1130\n",
      "Iter-5870 train loss: 2.2937 valid loss: 2.2958, valid accuracy: 0.1130\n",
      "Iter-5880 train loss: 2.3009 valid loss: 2.2957, valid accuracy: 0.1130\n",
      "Iter-5890 train loss: 2.2928 valid loss: 2.2956, valid accuracy: 0.1130\n",
      "Iter-5900 train loss: 2.3028 valid loss: 2.2955, valid accuracy: 0.1130\n",
      "Iter-5910 train loss: 2.3005 valid loss: 2.2954, valid accuracy: 0.1130\n",
      "Iter-5920 train loss: 2.2959 valid loss: 2.2953, valid accuracy: 0.1130\n",
      "Iter-5930 train loss: 2.2780 valid loss: 2.2953, valid accuracy: 0.1130\n",
      "Iter-5940 train loss: 2.2944 valid loss: 2.2952, valid accuracy: 0.1130\n",
      "Iter-5950 train loss: 2.3076 valid loss: 2.2952, valid accuracy: 0.1130\n",
      "Iter-5960 train loss: 2.2896 valid loss: 2.2951, valid accuracy: 0.1130\n",
      "Iter-5970 train loss: 2.3009 valid loss: 2.2951, valid accuracy: 0.1130\n",
      "Iter-5980 train loss: 2.2809 valid loss: 2.2950, valid accuracy: 0.1130\n",
      "Iter-5990 train loss: 2.3054 valid loss: 2.2949, valid accuracy: 0.1130\n",
      "Iter-6000 train loss: 2.2977 valid loss: 2.2949, valid accuracy: 0.1130\n",
      "Iter-6010 train loss: 2.2965 valid loss: 2.2948, valid accuracy: 0.1130\n",
      "Iter-6020 train loss: 2.2916 valid loss: 2.2947, valid accuracy: 0.1130\n",
      "Iter-6030 train loss: 2.2951 valid loss: 2.2946, valid accuracy: 0.1130\n",
      "Iter-6040 train loss: 2.2880 valid loss: 2.2945, valid accuracy: 0.1130\n",
      "Iter-6050 train loss: 2.2872 valid loss: 2.2944, valid accuracy: 0.1130\n",
      "Iter-6060 train loss: 2.2943 valid loss: 2.2943, valid accuracy: 0.1130\n",
      "Iter-6070 train loss: 2.3058 valid loss: 2.2942, valid accuracy: 0.1130\n",
      "Iter-6080 train loss: 2.3044 valid loss: 2.2941, valid accuracy: 0.1130\n",
      "Iter-6090 train loss: 2.3076 valid loss: 2.2940, valid accuracy: 0.1130\n",
      "Iter-6100 train loss: 2.2956 valid loss: 2.2940, valid accuracy: 0.1130\n",
      "Iter-6110 train loss: 2.3030 valid loss: 2.2939, valid accuracy: 0.1132\n",
      "Iter-6120 train loss: 2.2824 valid loss: 2.2938, valid accuracy: 0.1130\n",
      "Iter-6130 train loss: 2.2962 valid loss: 2.2937, valid accuracy: 0.1132\n",
      "Iter-6140 train loss: 2.3003 valid loss: 2.2936, valid accuracy: 0.1132\n",
      "Iter-6150 train loss: 2.2981 valid loss: 2.2935, valid accuracy: 0.1132\n",
      "Iter-6160 train loss: 2.2921 valid loss: 2.2935, valid accuracy: 0.1132\n",
      "Iter-6170 train loss: 2.2928 valid loss: 2.2934, valid accuracy: 0.1130\n",
      "Iter-6180 train loss: 2.3033 valid loss: 2.2933, valid accuracy: 0.1136\n",
      "Iter-6190 train loss: 2.2844 valid loss: 2.2933, valid accuracy: 0.1132\n",
      "Iter-6200 train loss: 2.2816 valid loss: 2.2932, valid accuracy: 0.1134\n",
      "Iter-6210 train loss: 2.3061 valid loss: 2.2931, valid accuracy: 0.1134\n",
      "Iter-6220 train loss: 2.3016 valid loss: 2.2930, valid accuracy: 0.1138\n",
      "Iter-6230 train loss: 2.2940 valid loss: 2.2929, valid accuracy: 0.1138\n",
      "Iter-6240 train loss: 2.2915 valid loss: 2.2928, valid accuracy: 0.1142\n",
      "Iter-6250 train loss: 2.2926 valid loss: 2.2927, valid accuracy: 0.1144\n",
      "Iter-6260 train loss: 2.2719 valid loss: 2.2926, valid accuracy: 0.1144\n",
      "Iter-6270 train loss: 2.2916 valid loss: 2.2925, valid accuracy: 0.1154\n",
      "Iter-6280 train loss: 2.2866 valid loss: 2.2924, valid accuracy: 0.1154\n",
      "Iter-6290 train loss: 2.2966 valid loss: 2.2924, valid accuracy: 0.1156\n",
      "Iter-6300 train loss: 2.2875 valid loss: 2.2923, valid accuracy: 0.1146\n",
      "Iter-6310 train loss: 2.2829 valid loss: 2.2922, valid accuracy: 0.1154\n",
      "Iter-6320 train loss: 2.3010 valid loss: 2.2920, valid accuracy: 0.1166\n",
      "Iter-6330 train loss: 2.2916 valid loss: 2.2920, valid accuracy: 0.1166\n",
      "Iter-6340 train loss: 2.2914 valid loss: 2.2919, valid accuracy: 0.1176\n",
      "Iter-6350 train loss: 2.3027 valid loss: 2.2918, valid accuracy: 0.1172\n",
      "Iter-6360 train loss: 2.2798 valid loss: 2.2917, valid accuracy: 0.1178\n",
      "Iter-6370 train loss: 2.2821 valid loss: 2.2916, valid accuracy: 0.1176\n",
      "Iter-6380 train loss: 2.3014 valid loss: 2.2916, valid accuracy: 0.1198\n",
      "Iter-6390 train loss: 2.2935 valid loss: 2.2915, valid accuracy: 0.1192\n",
      "Iter-6400 train loss: 2.2772 valid loss: 2.2914, valid accuracy: 0.1190\n",
      "Iter-6410 train loss: 2.3149 valid loss: 2.2914, valid accuracy: 0.1190\n",
      "Iter-6420 train loss: 2.2998 valid loss: 2.2913, valid accuracy: 0.1190\n",
      "Iter-6430 train loss: 2.2829 valid loss: 2.2912, valid accuracy: 0.1204\n",
      "Iter-6440 train loss: 2.2917 valid loss: 2.2911, valid accuracy: 0.1172\n",
      "Iter-6450 train loss: 2.3033 valid loss: 2.2911, valid accuracy: 0.1174\n",
      "Iter-6460 train loss: 2.2910 valid loss: 2.2909, valid accuracy: 0.1174\n",
      "Iter-6470 train loss: 2.2808 valid loss: 2.2908, valid accuracy: 0.1180\n",
      "Iter-6480 train loss: 2.2994 valid loss: 2.2907, valid accuracy: 0.1178\n",
      "Iter-6490 train loss: 2.3006 valid loss: 2.2906, valid accuracy: 0.1174\n",
      "Iter-6500 train loss: 2.2756 valid loss: 2.2905, valid accuracy: 0.1170\n",
      "Iter-6510 train loss: 2.2969 valid loss: 2.2905, valid accuracy: 0.1174\n",
      "Iter-6520 train loss: 2.3100 valid loss: 2.2904, valid accuracy: 0.1172\n",
      "Iter-6530 train loss: 2.2970 valid loss: 2.2902, valid accuracy: 0.1170\n",
      "Iter-6540 train loss: 2.2938 valid loss: 2.2901, valid accuracy: 0.1172\n",
      "Iter-6550 train loss: 2.2773 valid loss: 2.2901, valid accuracy: 0.1180\n",
      "Iter-6560 train loss: 2.3061 valid loss: 2.2900, valid accuracy: 0.1194\n",
      "Iter-6570 train loss: 2.2913 valid loss: 2.2899, valid accuracy: 0.1192\n",
      "Iter-6580 train loss: 2.2957 valid loss: 2.2898, valid accuracy: 0.1212\n",
      "Iter-6590 train loss: 2.2846 valid loss: 2.2897, valid accuracy: 0.1200\n",
      "Iter-6600 train loss: 2.2838 valid loss: 2.2896, valid accuracy: 0.1198\n",
      "Iter-6610 train loss: 2.2684 valid loss: 2.2895, valid accuracy: 0.1210\n",
      "Iter-6620 train loss: 2.2853 valid loss: 2.2894, valid accuracy: 0.1224\n",
      "Iter-6630 train loss: 2.2955 valid loss: 2.2893, valid accuracy: 0.1224\n",
      "Iter-6640 train loss: 2.2910 valid loss: 2.2892, valid accuracy: 0.1224\n",
      "Iter-6650 train loss: 2.2932 valid loss: 2.2890, valid accuracy: 0.1208\n",
      "Iter-6660 train loss: 2.3024 valid loss: 2.2889, valid accuracy: 0.1216\n",
      "Iter-6670 train loss: 2.2859 valid loss: 2.2888, valid accuracy: 0.1212\n",
      "Iter-6680 train loss: 2.2907 valid loss: 2.2887, valid accuracy: 0.1206\n",
      "Iter-6690 train loss: 2.3024 valid loss: 2.2886, valid accuracy: 0.1208\n",
      "Iter-6700 train loss: 2.2997 valid loss: 2.2885, valid accuracy: 0.1208\n",
      "Iter-6710 train loss: 2.2898 valid loss: 2.2884, valid accuracy: 0.1202\n",
      "Iter-6720 train loss: 2.2851 valid loss: 2.2883, valid accuracy: 0.1200\n",
      "Iter-6730 train loss: 2.2821 valid loss: 2.2882, valid accuracy: 0.1194\n",
      "Iter-6740 train loss: 2.2933 valid loss: 2.2881, valid accuracy: 0.1190\n",
      "Iter-6750 train loss: 2.2853 valid loss: 2.2880, valid accuracy: 0.1194\n",
      "Iter-6760 train loss: 2.2706 valid loss: 2.2879, valid accuracy: 0.1198\n",
      "Iter-6770 train loss: 2.2774 valid loss: 2.2878, valid accuracy: 0.1196\n",
      "Iter-6780 train loss: 2.3053 valid loss: 2.2877, valid accuracy: 0.1202\n",
      "Iter-6790 train loss: 2.2834 valid loss: 2.2876, valid accuracy: 0.1200\n",
      "Iter-6800 train loss: 2.2965 valid loss: 2.2875, valid accuracy: 0.1192\n",
      "Iter-6810 train loss: 2.2831 valid loss: 2.2875, valid accuracy: 0.1192\n",
      "Iter-6820 train loss: 2.2814 valid loss: 2.2873, valid accuracy: 0.1190\n",
      "Iter-6830 train loss: 2.2816 valid loss: 2.2872, valid accuracy: 0.1200\n",
      "Iter-6840 train loss: 2.2999 valid loss: 2.2871, valid accuracy: 0.1194\n",
      "Iter-6850 train loss: 2.3073 valid loss: 2.2870, valid accuracy: 0.1196\n",
      "Iter-6860 train loss: 2.2873 valid loss: 2.2869, valid accuracy: 0.1196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.2916 valid loss: 2.2868, valid accuracy: 0.1200\n",
      "Iter-6880 train loss: 2.2802 valid loss: 2.2867, valid accuracy: 0.1208\n",
      "Iter-6890 train loss: 2.2881 valid loss: 2.2866, valid accuracy: 0.1214\n",
      "Iter-6900 train loss: 2.2885 valid loss: 2.2865, valid accuracy: 0.1246\n",
      "Iter-6910 train loss: 2.2891 valid loss: 2.2863, valid accuracy: 0.1236\n",
      "Iter-6920 train loss: 2.2892 valid loss: 2.2862, valid accuracy: 0.1244\n",
      "Iter-6930 train loss: 2.2777 valid loss: 2.2861, valid accuracy: 0.1244\n",
      "Iter-6940 train loss: 2.3029 valid loss: 2.2860, valid accuracy: 0.1244\n",
      "Iter-6950 train loss: 2.2924 valid loss: 2.2859, valid accuracy: 0.1240\n",
      "Iter-6960 train loss: 2.2951 valid loss: 2.2858, valid accuracy: 0.1230\n",
      "Iter-6970 train loss: 2.2752 valid loss: 2.2857, valid accuracy: 0.1232\n",
      "Iter-6980 train loss: 2.2949 valid loss: 2.2856, valid accuracy: 0.1242\n",
      "Iter-6990 train loss: 2.2969 valid loss: 2.2854, valid accuracy: 0.1254\n",
      "Iter-7000 train loss: 2.2801 valid loss: 2.2853, valid accuracy: 0.1246\n",
      "Iter-7010 train loss: 2.2782 valid loss: 2.2852, valid accuracy: 0.1254\n",
      "Iter-7020 train loss: 2.2615 valid loss: 2.2851, valid accuracy: 0.1248\n",
      "Iter-7030 train loss: 2.2801 valid loss: 2.2850, valid accuracy: 0.1248\n",
      "Iter-7040 train loss: 2.2790 valid loss: 2.2849, valid accuracy: 0.1252\n",
      "Iter-7050 train loss: 2.2850 valid loss: 2.2848, valid accuracy: 0.1308\n",
      "Iter-7060 train loss: 2.2812 valid loss: 2.2847, valid accuracy: 0.1308\n",
      "Iter-7070 train loss: 2.2788 valid loss: 2.2845, valid accuracy: 0.1314\n",
      "Iter-7080 train loss: 2.2891 valid loss: 2.2844, valid accuracy: 0.1334\n",
      "Iter-7090 train loss: 2.2855 valid loss: 2.2843, valid accuracy: 0.1352\n",
      "Iter-7100 train loss: 2.3012 valid loss: 2.2842, valid accuracy: 0.1346\n",
      "Iter-7110 train loss: 2.2917 valid loss: 2.2841, valid accuracy: 0.1356\n",
      "Iter-7120 train loss: 2.2804 valid loss: 2.2840, valid accuracy: 0.1328\n",
      "Iter-7130 train loss: 2.2835 valid loss: 2.2839, valid accuracy: 0.1344\n",
      "Iter-7140 train loss: 2.2825 valid loss: 2.2837, valid accuracy: 0.1368\n",
      "Iter-7150 train loss: 2.2683 valid loss: 2.2836, valid accuracy: 0.1358\n",
      "Iter-7160 train loss: 2.2903 valid loss: 2.2835, valid accuracy: 0.1364\n",
      "Iter-7170 train loss: 2.2898 valid loss: 2.2834, valid accuracy: 0.1376\n",
      "Iter-7180 train loss: 2.2963 valid loss: 2.2833, valid accuracy: 0.1364\n",
      "Iter-7190 train loss: 2.2963 valid loss: 2.2832, valid accuracy: 0.1368\n",
      "Iter-7200 train loss: 2.2804 valid loss: 2.2830, valid accuracy: 0.1370\n",
      "Iter-7210 train loss: 2.2903 valid loss: 2.2830, valid accuracy: 0.1366\n",
      "Iter-7220 train loss: 2.2805 valid loss: 2.2828, valid accuracy: 0.1382\n",
      "Iter-7230 train loss: 2.2866 valid loss: 2.2827, valid accuracy: 0.1384\n",
      "Iter-7240 train loss: 2.2761 valid loss: 2.2826, valid accuracy: 0.1388\n",
      "Iter-7250 train loss: 2.2704 valid loss: 2.2824, valid accuracy: 0.1394\n",
      "Iter-7260 train loss: 2.2690 valid loss: 2.2823, valid accuracy: 0.1410\n",
      "Iter-7270 train loss: 2.2866 valid loss: 2.2822, valid accuracy: 0.1420\n",
      "Iter-7280 train loss: 2.2932 valid loss: 2.2821, valid accuracy: 0.1414\n",
      "Iter-7290 train loss: 2.2767 valid loss: 2.2819, valid accuracy: 0.1430\n",
      "Iter-7300 train loss: 2.2913 valid loss: 2.2818, valid accuracy: 0.1426\n",
      "Iter-7310 train loss: 2.2543 valid loss: 2.2817, valid accuracy: 0.1426\n",
      "Iter-7320 train loss: 2.3006 valid loss: 2.2815, valid accuracy: 0.1444\n",
      "Iter-7330 train loss: 2.2838 valid loss: 2.2814, valid accuracy: 0.1450\n",
      "Iter-7340 train loss: 2.2777 valid loss: 2.2813, valid accuracy: 0.1456\n",
      "Iter-7350 train loss: 2.2908 valid loss: 2.2811, valid accuracy: 0.1460\n",
      "Iter-7360 train loss: 2.2931 valid loss: 2.2810, valid accuracy: 0.1456\n",
      "Iter-7370 train loss: 2.2799 valid loss: 2.2808, valid accuracy: 0.1462\n",
      "Iter-7380 train loss: 2.2756 valid loss: 2.2807, valid accuracy: 0.1470\n",
      "Iter-7390 train loss: 2.2841 valid loss: 2.2806, valid accuracy: 0.1472\n",
      "Iter-7400 train loss: 2.2823 valid loss: 2.2804, valid accuracy: 0.1482\n",
      "Iter-7410 train loss: 2.2870 valid loss: 2.2803, valid accuracy: 0.1484\n",
      "Iter-7420 train loss: 2.2587 valid loss: 2.2802, valid accuracy: 0.1466\n",
      "Iter-7430 train loss: 2.2904 valid loss: 2.2801, valid accuracy: 0.1470\n",
      "Iter-7440 train loss: 2.2722 valid loss: 2.2799, valid accuracy: 0.1470\n",
      "Iter-7450 train loss: 2.2771 valid loss: 2.2798, valid accuracy: 0.1464\n",
      "Iter-7460 train loss: 2.2808 valid loss: 2.2796, valid accuracy: 0.1474\n",
      "Iter-7470 train loss: 2.2810 valid loss: 2.2795, valid accuracy: 0.1468\n",
      "Iter-7480 train loss: 2.2973 valid loss: 2.2794, valid accuracy: 0.1484\n",
      "Iter-7490 train loss: 2.2770 valid loss: 2.2793, valid accuracy: 0.1482\n",
      "Iter-7500 train loss: 2.2876 valid loss: 2.2792, valid accuracy: 0.1484\n",
      "Iter-7510 train loss: 2.2801 valid loss: 2.2790, valid accuracy: 0.1482\n",
      "Iter-7520 train loss: 2.2854 valid loss: 2.2789, valid accuracy: 0.1478\n",
      "Iter-7530 train loss: 2.2938 valid loss: 2.2788, valid accuracy: 0.1486\n",
      "Iter-7540 train loss: 2.2645 valid loss: 2.2787, valid accuracy: 0.1484\n",
      "Iter-7550 train loss: 2.2671 valid loss: 2.2785, valid accuracy: 0.1486\n",
      "Iter-7560 train loss: 2.2868 valid loss: 2.2784, valid accuracy: 0.1484\n",
      "Iter-7570 train loss: 2.2852 valid loss: 2.2782, valid accuracy: 0.1480\n",
      "Iter-7580 train loss: 2.2882 valid loss: 2.2781, valid accuracy: 0.1482\n",
      "Iter-7590 train loss: 2.2921 valid loss: 2.2780, valid accuracy: 0.1468\n",
      "Iter-7600 train loss: 2.2711 valid loss: 2.2778, valid accuracy: 0.1466\n",
      "Iter-7610 train loss: 2.2819 valid loss: 2.2777, valid accuracy: 0.1464\n",
      "Iter-7620 train loss: 2.2795 valid loss: 2.2776, valid accuracy: 0.1456\n",
      "Iter-7630 train loss: 2.2869 valid loss: 2.2774, valid accuracy: 0.1476\n",
      "Iter-7640 train loss: 2.2711 valid loss: 2.2773, valid accuracy: 0.1466\n",
      "Iter-7650 train loss: 2.2860 valid loss: 2.2771, valid accuracy: 0.1480\n",
      "Iter-7660 train loss: 2.2715 valid loss: 2.2770, valid accuracy: 0.1488\n",
      "Iter-7670 train loss: 2.2708 valid loss: 2.2768, valid accuracy: 0.1506\n",
      "Iter-7680 train loss: 2.2723 valid loss: 2.2767, valid accuracy: 0.1518\n",
      "Iter-7690 train loss: 2.2681 valid loss: 2.2765, valid accuracy: 0.1526\n",
      "Iter-7700 train loss: 2.2854 valid loss: 2.2764, valid accuracy: 0.1510\n",
      "Iter-7710 train loss: 2.2847 valid loss: 2.2763, valid accuracy: 0.1496\n",
      "Iter-7720 train loss: 2.2802 valid loss: 2.2762, valid accuracy: 0.1494\n",
      "Iter-7730 train loss: 2.2743 valid loss: 2.2760, valid accuracy: 0.1498\n",
      "Iter-7740 train loss: 2.2624 valid loss: 2.2759, valid accuracy: 0.1488\n",
      "Iter-7750 train loss: 2.2786 valid loss: 2.2758, valid accuracy: 0.1490\n",
      "Iter-7760 train loss: 2.2783 valid loss: 2.2756, valid accuracy: 0.1488\n",
      "Iter-7770 train loss: 2.2846 valid loss: 2.2755, valid accuracy: 0.1474\n",
      "Iter-7780 train loss: 2.2682 valid loss: 2.2754, valid accuracy: 0.1462\n",
      "Iter-7790 train loss: 2.2890 valid loss: 2.2752, valid accuracy: 0.1464\n",
      "Iter-7800 train loss: 2.2728 valid loss: 2.2750, valid accuracy: 0.1468\n",
      "Iter-7810 train loss: 2.2756 valid loss: 2.2749, valid accuracy: 0.1464\n",
      "Iter-7820 train loss: 2.2936 valid loss: 2.2747, valid accuracy: 0.1472\n",
      "Iter-7830 train loss: 2.3010 valid loss: 2.2746, valid accuracy: 0.1472\n",
      "Iter-7840 train loss: 2.2843 valid loss: 2.2744, valid accuracy: 0.1486\n",
      "Iter-7850 train loss: 2.2661 valid loss: 2.2742, valid accuracy: 0.1500\n",
      "Iter-7860 train loss: 2.2879 valid loss: 2.2741, valid accuracy: 0.1520\n",
      "Iter-7870 train loss: 2.2689 valid loss: 2.2739, valid accuracy: 0.1524\n",
      "Iter-7880 train loss: 2.2732 valid loss: 2.2738, valid accuracy: 0.1540\n",
      "Iter-7890 train loss: 2.2805 valid loss: 2.2736, valid accuracy: 0.1550\n",
      "Iter-7900 train loss: 2.2725 valid loss: 2.2735, valid accuracy: 0.1566\n",
      "Iter-7910 train loss: 2.2639 valid loss: 2.2733, valid accuracy: 0.1562\n",
      "Iter-7920 train loss: 2.2445 valid loss: 2.2732, valid accuracy: 0.1568\n",
      "Iter-7930 train loss: 2.2655 valid loss: 2.2730, valid accuracy: 0.1566\n",
      "Iter-7940 train loss: 2.2743 valid loss: 2.2729, valid accuracy: 0.1560\n",
      "Iter-7950 train loss: 2.2720 valid loss: 2.2727, valid accuracy: 0.1558\n",
      "Iter-7960 train loss: 2.2791 valid loss: 2.2726, valid accuracy: 0.1568\n",
      "Iter-7970 train loss: 2.2597 valid loss: 2.2724, valid accuracy: 0.1572\n",
      "Iter-7980 train loss: 2.2769 valid loss: 2.2722, valid accuracy: 0.1570\n",
      "Iter-7990 train loss: 2.2772 valid loss: 2.2721, valid accuracy: 0.1570\n",
      "Iter-8000 train loss: 2.2804 valid loss: 2.2719, valid accuracy: 0.1568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.2706 valid loss: 2.2718, valid accuracy: 0.1582\n",
      "Iter-8020 train loss: 2.2783 valid loss: 2.2716, valid accuracy: 0.1584\n",
      "Iter-8030 train loss: 2.2723 valid loss: 2.2715, valid accuracy: 0.1594\n",
      "Iter-8040 train loss: 2.2682 valid loss: 2.2713, valid accuracy: 0.1622\n",
      "Iter-8050 train loss: 2.2786 valid loss: 2.2711, valid accuracy: 0.1612\n",
      "Iter-8060 train loss: 2.2748 valid loss: 2.2710, valid accuracy: 0.1610\n",
      "Iter-8070 train loss: 2.2806 valid loss: 2.2708, valid accuracy: 0.1606\n",
      "Iter-8080 train loss: 2.2678 valid loss: 2.2707, valid accuracy: 0.1602\n",
      "Iter-8090 train loss: 2.2632 valid loss: 2.2705, valid accuracy: 0.1594\n",
      "Iter-8100 train loss: 2.2666 valid loss: 2.2704, valid accuracy: 0.1582\n",
      "Iter-8110 train loss: 2.2651 valid loss: 2.2702, valid accuracy: 0.1580\n",
      "Iter-8120 train loss: 2.2575 valid loss: 2.2700, valid accuracy: 0.1596\n",
      "Iter-8130 train loss: 2.2803 valid loss: 2.2698, valid accuracy: 0.1588\n",
      "Iter-8140 train loss: 2.2746 valid loss: 2.2697, valid accuracy: 0.1602\n",
      "Iter-8150 train loss: 2.2903 valid loss: 2.2695, valid accuracy: 0.1598\n",
      "Iter-8160 train loss: 2.2746 valid loss: 2.2693, valid accuracy: 0.1610\n",
      "Iter-8170 train loss: 2.2710 valid loss: 2.2691, valid accuracy: 0.1616\n",
      "Iter-8180 train loss: 2.2703 valid loss: 2.2690, valid accuracy: 0.1624\n",
      "Iter-8190 train loss: 2.2652 valid loss: 2.2688, valid accuracy: 0.1648\n",
      "Iter-8200 train loss: 2.2782 valid loss: 2.2686, valid accuracy: 0.1644\n",
      "Iter-8210 train loss: 2.2719 valid loss: 2.2684, valid accuracy: 0.1644\n",
      "Iter-8220 train loss: 2.2557 valid loss: 2.2682, valid accuracy: 0.1646\n",
      "Iter-8230 train loss: 2.2737 valid loss: 2.2681, valid accuracy: 0.1632\n",
      "Iter-8240 train loss: 2.2445 valid loss: 2.2679, valid accuracy: 0.1624\n",
      "Iter-8250 train loss: 2.2715 valid loss: 2.2677, valid accuracy: 0.1612\n",
      "Iter-8260 train loss: 2.2579 valid loss: 2.2676, valid accuracy: 0.1624\n",
      "Iter-8270 train loss: 2.2522 valid loss: 2.2674, valid accuracy: 0.1644\n",
      "Iter-8280 train loss: 2.2562 valid loss: 2.2672, valid accuracy: 0.1630\n",
      "Iter-8290 train loss: 2.2661 valid loss: 2.2670, valid accuracy: 0.1654\n",
      "Iter-8300 train loss: 2.2698 valid loss: 2.2669, valid accuracy: 0.1654\n",
      "Iter-8310 train loss: 2.2451 valid loss: 2.2667, valid accuracy: 0.1668\n",
      "Iter-8320 train loss: 2.2758 valid loss: 2.2665, valid accuracy: 0.1664\n",
      "Iter-8330 train loss: 2.2707 valid loss: 2.2663, valid accuracy: 0.1676\n",
      "Iter-8340 train loss: 2.2741 valid loss: 2.2662, valid accuracy: 0.1664\n",
      "Iter-8350 train loss: 2.2631 valid loss: 2.2660, valid accuracy: 0.1652\n",
      "Iter-8360 train loss: 2.2654 valid loss: 2.2658, valid accuracy: 0.1652\n",
      "Iter-8370 train loss: 2.2589 valid loss: 2.2656, valid accuracy: 0.1668\n",
      "Iter-8380 train loss: 2.2448 valid loss: 2.2654, valid accuracy: 0.1694\n",
      "Iter-8390 train loss: 2.2664 valid loss: 2.2653, valid accuracy: 0.1694\n",
      "Iter-8400 train loss: 2.2627 valid loss: 2.2651, valid accuracy: 0.1696\n",
      "Iter-8410 train loss: 2.2734 valid loss: 2.2649, valid accuracy: 0.1710\n",
      "Iter-8420 train loss: 2.2624 valid loss: 2.2647, valid accuracy: 0.1722\n",
      "Iter-8430 train loss: 2.2620 valid loss: 2.2646, valid accuracy: 0.1724\n",
      "Iter-8440 train loss: 2.2637 valid loss: 2.2644, valid accuracy: 0.1726\n",
      "Iter-8450 train loss: 2.2655 valid loss: 2.2643, valid accuracy: 0.1744\n",
      "Iter-8460 train loss: 2.2593 valid loss: 2.2641, valid accuracy: 0.1760\n",
      "Iter-8470 train loss: 2.2553 valid loss: 2.2639, valid accuracy: 0.1770\n",
      "Iter-8480 train loss: 2.2685 valid loss: 2.2637, valid accuracy: 0.1766\n",
      "Iter-8490 train loss: 2.2549 valid loss: 2.2636, valid accuracy: 0.1772\n",
      "Iter-8500 train loss: 2.2557 valid loss: 2.2634, valid accuracy: 0.1762\n",
      "Iter-8510 train loss: 2.2603 valid loss: 2.2632, valid accuracy: 0.1750\n",
      "Iter-8520 train loss: 2.2812 valid loss: 2.2630, valid accuracy: 0.1760\n",
      "Iter-8530 train loss: 2.2496 valid loss: 2.2629, valid accuracy: 0.1764\n",
      "Iter-8540 train loss: 2.2488 valid loss: 2.2627, valid accuracy: 0.1754\n",
      "Iter-8550 train loss: 2.2596 valid loss: 2.2625, valid accuracy: 0.1770\n",
      "Iter-8560 train loss: 2.2484 valid loss: 2.2623, valid accuracy: 0.1806\n",
      "Iter-8570 train loss: 2.2547 valid loss: 2.2621, valid accuracy: 0.1790\n",
      "Iter-8580 train loss: 2.2564 valid loss: 2.2619, valid accuracy: 0.1790\n",
      "Iter-8590 train loss: 2.2615 valid loss: 2.2618, valid accuracy: 0.1798\n",
      "Iter-8600 train loss: 2.2921 valid loss: 2.2616, valid accuracy: 0.1802\n",
      "Iter-8610 train loss: 2.2662 valid loss: 2.2614, valid accuracy: 0.1804\n",
      "Iter-8620 train loss: 2.2622 valid loss: 2.2612, valid accuracy: 0.1808\n",
      "Iter-8630 train loss: 2.2607 valid loss: 2.2610, valid accuracy: 0.1802\n",
      "Iter-8640 train loss: 2.2685 valid loss: 2.2608, valid accuracy: 0.1796\n",
      "Iter-8650 train loss: 2.2764 valid loss: 2.2605, valid accuracy: 0.1816\n",
      "Iter-8660 train loss: 2.2660 valid loss: 2.2603, valid accuracy: 0.1816\n",
      "Iter-8670 train loss: 2.2544 valid loss: 2.2601, valid accuracy: 0.1806\n",
      "Iter-8680 train loss: 2.2534 valid loss: 2.2599, valid accuracy: 0.1816\n",
      "Iter-8690 train loss: 2.2626 valid loss: 2.2598, valid accuracy: 0.1838\n",
      "Iter-8700 train loss: 2.2341 valid loss: 2.2596, valid accuracy: 0.1838\n",
      "Iter-8710 train loss: 2.2490 valid loss: 2.2594, valid accuracy: 0.1836\n",
      "Iter-8720 train loss: 2.2666 valid loss: 2.2592, valid accuracy: 0.1842\n",
      "Iter-8730 train loss: 2.2566 valid loss: 2.2589, valid accuracy: 0.1846\n",
      "Iter-8740 train loss: 2.2736 valid loss: 2.2587, valid accuracy: 0.1876\n",
      "Iter-8750 train loss: 2.2626 valid loss: 2.2585, valid accuracy: 0.1876\n",
      "Iter-8760 train loss: 2.2609 valid loss: 2.2583, valid accuracy: 0.1880\n",
      "Iter-8770 train loss: 2.2549 valid loss: 2.2581, valid accuracy: 0.1864\n",
      "Iter-8780 train loss: 2.2592 valid loss: 2.2579, valid accuracy: 0.1882\n",
      "Iter-8790 train loss: 2.2638 valid loss: 2.2578, valid accuracy: 0.1892\n",
      "Iter-8800 train loss: 2.2579 valid loss: 2.2575, valid accuracy: 0.1894\n",
      "Iter-8810 train loss: 2.2539 valid loss: 2.2573, valid accuracy: 0.1888\n",
      "Iter-8820 train loss: 2.2487 valid loss: 2.2571, valid accuracy: 0.1890\n",
      "Iter-8830 train loss: 2.2530 valid loss: 2.2569, valid accuracy: 0.1890\n",
      "Iter-8840 train loss: 2.2744 valid loss: 2.2567, valid accuracy: 0.1896\n",
      "Iter-8850 train loss: 2.2670 valid loss: 2.2565, valid accuracy: 0.1888\n",
      "Iter-8860 train loss: 2.2589 valid loss: 2.2563, valid accuracy: 0.1870\n",
      "Iter-8870 train loss: 2.2347 valid loss: 2.2561, valid accuracy: 0.1868\n",
      "Iter-8880 train loss: 2.2524 valid loss: 2.2558, valid accuracy: 0.1882\n",
      "Iter-8890 train loss: 2.2704 valid loss: 2.2556, valid accuracy: 0.1880\n",
      "Iter-8900 train loss: 2.2481 valid loss: 2.2554, valid accuracy: 0.1874\n",
      "Iter-8910 train loss: 2.2676 valid loss: 2.2552, valid accuracy: 0.1868\n",
      "Iter-8920 train loss: 2.2642 valid loss: 2.2550, valid accuracy: 0.1874\n",
      "Iter-8930 train loss: 2.2529 valid loss: 2.2548, valid accuracy: 0.1888\n",
      "Iter-8940 train loss: 2.2766 valid loss: 2.2546, valid accuracy: 0.1884\n",
      "Iter-8950 train loss: 2.2651 valid loss: 2.2544, valid accuracy: 0.1894\n",
      "Iter-8960 train loss: 2.2599 valid loss: 2.2542, valid accuracy: 0.1888\n",
      "Iter-8970 train loss: 2.2640 valid loss: 2.2540, valid accuracy: 0.1900\n",
      "Iter-8980 train loss: 2.2518 valid loss: 2.2538, valid accuracy: 0.1908\n",
      "Iter-8990 train loss: 2.2599 valid loss: 2.2536, valid accuracy: 0.1896\n",
      "Iter-9000 train loss: 2.2579 valid loss: 2.2534, valid accuracy: 0.1914\n",
      "Iter-9010 train loss: 2.2451 valid loss: 2.2532, valid accuracy: 0.1944\n",
      "Iter-9020 train loss: 2.2687 valid loss: 2.2530, valid accuracy: 0.1952\n",
      "Iter-9030 train loss: 2.2598 valid loss: 2.2527, valid accuracy: 0.1966\n",
      "Iter-9040 train loss: 2.2512 valid loss: 2.2525, valid accuracy: 0.1976\n",
      "Iter-9050 train loss: 2.2510 valid loss: 2.2523, valid accuracy: 0.1964\n",
      "Iter-9060 train loss: 2.2414 valid loss: 2.2521, valid accuracy: 0.1964\n",
      "Iter-9070 train loss: 2.2772 valid loss: 2.2519, valid accuracy: 0.1954\n",
      "Iter-9080 train loss: 2.2622 valid loss: 2.2516, valid accuracy: 0.1950\n",
      "Iter-9090 train loss: 2.2606 valid loss: 2.2515, valid accuracy: 0.1940\n",
      "Iter-9100 train loss: 2.2460 valid loss: 2.2513, valid accuracy: 0.1940\n",
      "Iter-9110 train loss: 2.2361 valid loss: 2.2511, valid accuracy: 0.1954\n",
      "Iter-9120 train loss: 2.2439 valid loss: 2.2508, valid accuracy: 0.1954\n",
      "Iter-9130 train loss: 2.2411 valid loss: 2.2506, valid accuracy: 0.1952\n",
      "Iter-9140 train loss: 2.2504 valid loss: 2.2504, valid accuracy: 0.1950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.2517 valid loss: 2.2502, valid accuracy: 0.1946\n",
      "Iter-9160 train loss: 2.2397 valid loss: 2.2500, valid accuracy: 0.1954\n",
      "Iter-9170 train loss: 2.2387 valid loss: 2.2498, valid accuracy: 0.1958\n",
      "Iter-9180 train loss: 2.2561 valid loss: 2.2496, valid accuracy: 0.1966\n",
      "Iter-9190 train loss: 2.2335 valid loss: 2.2493, valid accuracy: 0.1950\n",
      "Iter-9200 train loss: 2.2529 valid loss: 2.2491, valid accuracy: 0.1926\n",
      "Iter-9210 train loss: 2.2595 valid loss: 2.2489, valid accuracy: 0.1942\n",
      "Iter-9220 train loss: 2.2411 valid loss: 2.2487, valid accuracy: 0.1942\n",
      "Iter-9230 train loss: 2.2523 valid loss: 2.2485, valid accuracy: 0.1956\n",
      "Iter-9240 train loss: 2.2399 valid loss: 2.2482, valid accuracy: 0.1958\n",
      "Iter-9250 train loss: 2.2725 valid loss: 2.2480, valid accuracy: 0.1976\n",
      "Iter-9260 train loss: 2.2655 valid loss: 2.2478, valid accuracy: 0.1964\n",
      "Iter-9270 train loss: 2.2497 valid loss: 2.2476, valid accuracy: 0.1954\n",
      "Iter-9280 train loss: 2.2329 valid loss: 2.2473, valid accuracy: 0.1954\n",
      "Iter-9290 train loss: 2.2272 valid loss: 2.2471, valid accuracy: 0.1946\n",
      "Iter-9300 train loss: 2.2545 valid loss: 2.2468, valid accuracy: 0.1962\n",
      "Iter-9310 train loss: 2.2584 valid loss: 2.2466, valid accuracy: 0.1948\n",
      "Iter-9320 train loss: 2.2504 valid loss: 2.2464, valid accuracy: 0.1956\n",
      "Iter-9330 train loss: 2.2622 valid loss: 2.2461, valid accuracy: 0.1966\n",
      "Iter-9340 train loss: 2.2611 valid loss: 2.2459, valid accuracy: 0.1956\n",
      "Iter-9350 train loss: 2.2391 valid loss: 2.2456, valid accuracy: 0.1950\n",
      "Iter-9360 train loss: 2.2223 valid loss: 2.2454, valid accuracy: 0.1942\n",
      "Iter-9370 train loss: 2.2447 valid loss: 2.2452, valid accuracy: 0.1952\n",
      "Iter-9380 train loss: 2.2483 valid loss: 2.2449, valid accuracy: 0.1950\n",
      "Iter-9390 train loss: 2.2347 valid loss: 2.2447, valid accuracy: 0.1988\n",
      "Iter-9400 train loss: 2.2667 valid loss: 2.2445, valid accuracy: 0.1976\n",
      "Iter-9410 train loss: 2.2258 valid loss: 2.2443, valid accuracy: 0.1978\n",
      "Iter-9420 train loss: 2.2523 valid loss: 2.2440, valid accuracy: 0.1996\n",
      "Iter-9430 train loss: 2.2370 valid loss: 2.2438, valid accuracy: 0.2006\n",
      "Iter-9440 train loss: 2.2355 valid loss: 2.2435, valid accuracy: 0.2018\n",
      "Iter-9450 train loss: 2.2567 valid loss: 2.2433, valid accuracy: 0.2006\n",
      "Iter-9460 train loss: 2.2420 valid loss: 2.2431, valid accuracy: 0.2036\n",
      "Iter-9470 train loss: 2.2169 valid loss: 2.2428, valid accuracy: 0.2056\n",
      "Iter-9480 train loss: 2.2244 valid loss: 2.2426, valid accuracy: 0.2068\n",
      "Iter-9490 train loss: 2.2603 valid loss: 2.2424, valid accuracy: 0.2080\n",
      "Iter-9500 train loss: 2.2535 valid loss: 2.2422, valid accuracy: 0.2078\n",
      "Iter-9510 train loss: 2.2334 valid loss: 2.2419, valid accuracy: 0.2078\n",
      "Iter-9520 train loss: 2.2388 valid loss: 2.2416, valid accuracy: 0.2088\n",
      "Iter-9530 train loss: 2.2485 valid loss: 2.2414, valid accuracy: 0.2108\n",
      "Iter-9540 train loss: 2.2391 valid loss: 2.2412, valid accuracy: 0.2124\n",
      "Iter-9550 train loss: 2.2527 valid loss: 2.2409, valid accuracy: 0.2132\n",
      "Iter-9560 train loss: 2.2648 valid loss: 2.2407, valid accuracy: 0.2136\n",
      "Iter-9570 train loss: 2.2458 valid loss: 2.2405, valid accuracy: 0.2152\n",
      "Iter-9580 train loss: 2.2495 valid loss: 2.2402, valid accuracy: 0.2136\n",
      "Iter-9590 train loss: 2.2482 valid loss: 2.2400, valid accuracy: 0.2162\n",
      "Iter-9600 train loss: 2.2160 valid loss: 2.2397, valid accuracy: 0.2148\n",
      "Iter-9610 train loss: 2.2363 valid loss: 2.2395, valid accuracy: 0.2150\n",
      "Iter-9620 train loss: 2.2261 valid loss: 2.2392, valid accuracy: 0.2174\n",
      "Iter-9630 train loss: 2.2285 valid loss: 2.2390, valid accuracy: 0.2178\n",
      "Iter-9640 train loss: 2.2385 valid loss: 2.2387, valid accuracy: 0.2150\n",
      "Iter-9650 train loss: 2.2478 valid loss: 2.2385, valid accuracy: 0.2200\n",
      "Iter-9660 train loss: 2.2350 valid loss: 2.2382, valid accuracy: 0.2194\n",
      "Iter-9670 train loss: 2.2639 valid loss: 2.2379, valid accuracy: 0.2176\n",
      "Iter-9680 train loss: 2.2222 valid loss: 2.2377, valid accuracy: 0.2164\n",
      "Iter-9690 train loss: 2.2461 valid loss: 2.2374, valid accuracy: 0.2176\n",
      "Iter-9700 train loss: 2.2293 valid loss: 2.2371, valid accuracy: 0.2200\n",
      "Iter-9710 train loss: 2.2503 valid loss: 2.2369, valid accuracy: 0.2216\n",
      "Iter-9720 train loss: 2.2371 valid loss: 2.2366, valid accuracy: 0.2208\n",
      "Iter-9730 train loss: 2.2298 valid loss: 2.2364, valid accuracy: 0.2238\n",
      "Iter-9740 train loss: 2.2712 valid loss: 2.2361, valid accuracy: 0.2240\n",
      "Iter-9750 train loss: 2.2467 valid loss: 2.2359, valid accuracy: 0.2258\n",
      "Iter-9760 train loss: 2.2275 valid loss: 2.2356, valid accuracy: 0.2284\n",
      "Iter-9770 train loss: 2.2309 valid loss: 2.2354, valid accuracy: 0.2308\n",
      "Iter-9780 train loss: 2.2372 valid loss: 2.2352, valid accuracy: 0.2292\n",
      "Iter-9790 train loss: 2.2314 valid loss: 2.2349, valid accuracy: 0.2284\n",
      "Iter-9800 train loss: 2.2454 valid loss: 2.2347, valid accuracy: 0.2324\n",
      "Iter-9810 train loss: 2.2335 valid loss: 2.2344, valid accuracy: 0.2352\n",
      "Iter-9820 train loss: 2.2265 valid loss: 2.2341, valid accuracy: 0.2356\n",
      "Iter-9830 train loss: 2.2306 valid loss: 2.2339, valid accuracy: 0.2332\n",
      "Iter-9840 train loss: 2.2220 valid loss: 2.2336, valid accuracy: 0.2352\n",
      "Iter-9850 train loss: 2.2252 valid loss: 2.2334, valid accuracy: 0.2324\n",
      "Iter-9860 train loss: 2.2426 valid loss: 2.2331, valid accuracy: 0.2372\n",
      "Iter-9870 train loss: 2.2477 valid loss: 2.2328, valid accuracy: 0.2350\n",
      "Iter-9880 train loss: 2.2345 valid loss: 2.2325, valid accuracy: 0.2368\n",
      "Iter-9890 train loss: 2.2291 valid loss: 2.2323, valid accuracy: 0.2374\n",
      "Iter-9900 train loss: 2.2402 valid loss: 2.2320, valid accuracy: 0.2378\n",
      "Iter-9910 train loss: 2.2483 valid loss: 2.2318, valid accuracy: 0.2392\n",
      "Iter-9920 train loss: 2.2191 valid loss: 2.2315, valid accuracy: 0.2430\n",
      "Iter-9930 train loss: 2.2183 valid loss: 2.2312, valid accuracy: 0.2462\n",
      "Iter-9940 train loss: 2.2306 valid loss: 2.2310, valid accuracy: 0.2472\n",
      "Iter-9950 train loss: 2.2341 valid loss: 2.2307, valid accuracy: 0.2472\n",
      "Iter-9960 train loss: 2.2175 valid loss: 2.2305, valid accuracy: 0.2462\n",
      "Iter-9970 train loss: 2.2233 valid loss: 2.2302, valid accuracy: 0.2444\n",
      "Iter-9980 train loss: 2.2308 valid loss: 2.2299, valid accuracy: 0.2402\n",
      "Iter-9990 train loss: 2.2029 valid loss: 2.2297, valid accuracy: 0.2400\n",
      "Iter-10000 train loss: 2.2240 valid loss: 2.2294, valid accuracy: 0.2394\n",
      "Last iteration - Test accuracy mean: 0.2504, std: 0.0000, loss: 2.2260\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120f8acc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBQcREUmh4CAiIikUHEREJMX/B2zG/ihmsyjkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120f8add8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
