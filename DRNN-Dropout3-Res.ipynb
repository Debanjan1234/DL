{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        # h_res for residual connection or skip connection for gradients\n",
    "        # Residual connection to avoid vanishing gradients\n",
    "        # SELU act_function to avoid exploding gradients\n",
    "        # x+ f(x)\n",
    "        h_res = h.copy()\n",
    "        hprev = h.copy()\n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        h += h_res\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "\n",
    "        cache = (X, Wxh, hprev, Whh, h_cache, y_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, Wxh, hprev, Whh, h_cache, y_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh_res = dh.copy()\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "        dh += dh_res\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - beta1**(iter))\n",
    "                    r_k_hat = R[layer][key] / (1. - beta2**(iter))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arasdar/anaconda3/envs/arasdar-DL-env/lib/python3.5/site-packages/ipykernel_launcher.py:112: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 training loss: 10.8400\n",
      "ePJJJJJd'''nnnd   pp s    p p oii ii ii i   i                     e     t        iiiihiiiiSiiiiiiiiii\n",
      "Iter-26 training loss: 8.1410\n",
      "easwnannJl7dhdiddatac dddta aa Sc     l r     Ca daaaaaaaa cpcccuccdcitaaaaa9ccaaSSSSSScaggggagluuluu\n",
      "Iter-39 training loss: 6.8394\n",
      "euawwanwGure nonornn1 tet eawto tt  tetetr tt t  t enn ttt t tt  e   t t ttoo   ot    p ooot h  tt   \n",
      "Iter-52 training loss: 5.4530\n",
      "e aawinn JHo  nnfn  nnan1f h  fhih-nsCChCOnC Cnnn   Ceaf f f fff1  c fff c  t   i    acaaa,S  r\"  sfP\n",
      "Iter-65 training loss: 7.0553\n",
      "eeihwroiireelela rH HH etaAA4aeaaeh,gaageaCaaaaaly日 %8p5Bttt 1RN7本-%P A dpg9 .))WelvgLi))))))))))))))\n",
      "Iter-78 training loss: 6.4938\n",
      "euooweuilno ef rehleotSattiMoooooMgAg-Muuuvvvovvvvvvv ee 1%I1EhPeyll r.heeeeaL7o'axfrrreIrrrr日Sa日日SSS\n",
      "Iter-91 training loss: 5.6259\n",
      "eaiwnw eoaaaaaamm wowoop hac MrcajnlajnIcInuuaoanaooaaa1naaanhhahnananhhc aaaanhfohwahhrtch rreiriiog\n",
      "Iter-104 training loss: 5.4810\n",
      "er trdn a pnawtcdnli c ctf 1ddddaCynnCCanaaaynAHyx9yal ya a((pg l本g日guolga Ea faaaatK\"a ieataaHaJBF o\n",
      "Iter-117 training loss: 4.9398\n",
      "eu wan netena aannatHcante nll hehnh s hhhnumg–dmeaas nn本.ammFdd)ddpnaunnanfr i rnnm u rr  ree hhR aT\n",
      "Iter-130 training loss: 5.1221\n",
      "eyrhe eeatah    naCofCuCCwiCCCCwrhUUwwUUCawgIIICCICCCaCjjjLgC5aan  a9lg%IIaIitata ggg RgaydagaLs;J8j;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7fbad009f0f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 130 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.95 # keep_prob\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbab8315630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
