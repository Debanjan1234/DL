{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# set(txt), \n",
    "# len(txt), len(set(txt)), set(txt)\n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# # val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for l in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "    \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        # Dropout for training\n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, Wxh, hprev, Whh, h_cache, y_cache, do_cache)\n",
    "        else:\n",
    "            cache = X, Wxh, hprev, Whh, h_cache, y_cache\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, Wxh, hprev, Whh, h_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, Wxh, hprev, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        # tanh\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        # Hidden gradient\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No full batch or files\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=(mb_size*10))\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 loss: 4.0903\n",
      "che thestupand Hudisa, shrgd Niulirss 18k1 ined ainsl ern Emitas taman wost my-la ghoth tho wuosing t\n",
      "Iter-260 loss: 7.2773\n",
      "cter peric iWmy bt. undind-llima worlrrestrg st the was, ir apvedeind img rh ta inapon m on the Emory\n",
      "Iter-390 loss: 7.7632\n",
      "came thik e ct re thrym uroA th tha, towur parcesioan mist Japar etecokedup s the G20 anesh maulive f\n",
      "Iter-520 loss: 6.9158\n",
      "cotovylithet aer in levrlopamial Pioand in ios for alo\"Sthe wopld liges vont As the eapic ba mant ctl\n",
      "Iter-650 loss: 10.8157\n",
      "capital citiry c6 as, on icth. Japan ean is is the Ig lartector as dergasSth ta o streu, thicl Wgois \n",
      "Iter-780 loss: 2.8039\n",
      "c Games.\n",
      ". cace forlgParet a cishin marcy Jueans anded 3n upou th kak cop abat ing in ane ch es a d i\n",
      "Iter-910 loss: 5.2470\n",
      "capital citsland ang o mJatinliom iedecount mper meeceand to fountun en lengr inst nstain the lare pg\n",
      "Iter-1040 loss: 7.2874\n",
      "ciny on war entioulirumdeand couloky area. Japan kae coument mlatadac silitinit mares torenila, mhich\n",
      "Iter-1170 loss: 9.9566\n",
      "city properte the ouldPe grectarnin eu and ihicy indic. Seallychul ofmy. maem or mce eorno an-katalin\n",
      "Iter-1300 loss: 2.4900\n",
      "ch includes Tol of thy Se cegitat 6 ied in 1947trJalaris ton tho the nout ry ih in tigit the hound,. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f2e24746a58>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1300 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCgIiIhmmICAikmEKAiIiGaYgICKSYf8fPUc2j/oBvR4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e24719518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
