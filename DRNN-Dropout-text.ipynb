{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# set(txt), \n",
    "# len(txt), len(set(txt)), set(txt)\n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# # val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for l in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "    \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        # Dropout for training\n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, Wxh, hprev, Whh, h_cache, y_cache, do_cache)\n",
    "        else:\n",
    "            cache = X, Wxh, hprev, Whh, h_cache, y_cache\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, Wxh, hprev, Whh, h_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, Wxh, hprev, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        # tanh\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        # Hidden gradient\n",
    "        dbh = dh * 1.0\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "    smooth_loss = 1.0\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No full batch or files\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 279.0173\n",
      " an rtPort theky cor Pal Pap cmpoune in rt iath cob Gpobiry antursirouny rsPrcoun the is Pow-isitPe-o\n",
      "Iter-2 loss: 277.2137\n",
      " In ca coucenct hstnu thurco-niombars Ramury sbec Aftry Grury mighisi mie Rurst itdet Gre tby cortu b\n",
      "Iter-3 loss: 285.0750\n",
      " ftictus Acciny Jap65 Japan As5 rne hh reitht-fa icob-155 Aan Aad theathedth ian coh he. thJapanet Ia\n",
      "Iter-4 loss: 258.5566\n",
      " oh 201 1266 rh-ly, un rh biro- the cory borseaPat the cortry. Pby Ind my coumpes Skareco Pay rtty bo\n",
      "Iter-5 loss: 266.2543\n",
      " an Anansc.y the Rbealases GnPcevice the R66 oto the baee6,-in orcastaaicth and an robict okan coboun\n",
      "Iter-6 loss: 255.6765\n",
      " ixbed han 1666 sy Anan xmea–6 G20. 1565166 aba the hopobhesith 166 bolnt Anest Partes ixcexpiry thex\n",
      "Iter-7 loss: 278.1238\n",
      " wort anh, and Japan 20156 Japand Ibaml ingin Gnthdec an Aty ches mos antn In Aonan fandt Gtas colnda\n",
      "Iter-8 loss: 268.2547\n",
      " ir as Ax6 1666 It pap ant Ro– At16 thP as A20th1 porns. In Amorls coulth Jxpan An–20 coe the In in R\n",
      "Iter-9 loss: 263.7613\n",
      " In exst in .h RhPomlath Raicoury Anaortes Japa20 ankcomals Japasan in orliganicatithP hy. hion Japas\n",
      "Iter-10 loss: 266.4407\n",
      " coReabess antte G ranac abeseP Wiy Japan and bi co rixss Aapan an Pobes al andicoltbas Japenan Japan\n",
      "Iter-11 loss: 258.5373\n",
      " and the Glowathsa Pont Asbys AcelokesiFecolnand It AIperaigh orldan Gly Wa sa an Aco 20–66 Japan Is \n",
      "Iter-12 loss: 259.8262\n",
      " on cane concona ang Nlibiont an ix and al In byk al Ris Rory couc sias ist ranan lar th. in ary ar a\n",
      "Iter-13 loss: 257.0593\n",
      " Counitsed-a Che Pala staln and is Aapand Pae Piy Aipan an hory Pconi Asas and to and Parartesalaithe\n",
      "Iter-14 loss: 264.6484\n",
      " Bode Gy-and the name tount Japanktet on Pocesseis Japanasas Axpelitc AIsas Aste the the sound in Jxp\n",
      "Iter-15 loss: 265.3603\n",
      " Japand-160 Jipan 1Iyan Asea and ea Che R2Pet one sine cithe sin the a anana eabese Gaan Asiase-lapat\n",
      "Iter-16 loss: 271.1923\n",
      " the Gortes is sha chice cinth Gupan nibixtiges an cinxntt iss of Japand sors thea the Gera expasank \n",
      "Iter-17 loss: 242.7873\n",
      " rhat bhe beanathbh seacis ines at Pst alatado. et the Gs, 20–6,2 Pas sel revecoleth an Asifrexnaneth\n",
      "Iter-18 loss: 244.9749\n",
      " fon seo col Pay six, th-lar the Rupha it ean Go-Jxppob larest Glexthe hed Atpalet extargex. the G201\n",
      "Iter-19 loss: 230.4201\n",
      " bo A2016 the Inand the Globean the Ind in an the Galasheay so an-2016 an Atpand hiicog at Tho. Japan\n",
      "Iter-20 loss: 242.3029\n",
      " se is an Japand the hovent the the G201206,–6 16 siccashe the Rapin ht oh , Is combed exportest whs \n",
      "Iter-21 loss: 259.3845\n",
      " u cearlaitatan the Is Ise 2015–2P52012012th rely randt count in counthe G20Japan Ris the lans 2016 b\n",
      "Iter-22 loss: 246.5043\n",
      " expeco the Risth the 1––a, the Glob the G the Ras nas the Gsty the G20the 16015–5 ,8P1–68, i bith As\n",
      "Iter-23 loss: 229.0039\n",
      " rilchish Globe Papan hs the RLDPan rin the Gno counthe ChIkix whith is and Rath as the Grsth Pas Ind\n",
      "Iter-24 loss: 231.2572\n",
      " 1o is in Aapand is 1s46 hiro lared com in ax WA6 AcPajanl const Asiliceleciak the Globithoss Bral Pa\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2ed8ea4b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
