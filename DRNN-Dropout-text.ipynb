{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# set(txt), \n",
    "# len(txt), len(set(txt)), set(txt)\n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# # val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for l in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "    \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        # Dropout for training\n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, Wxh, hprev, Whh, h_cache, y_cache, do_cache)\n",
    "        else:\n",
    "            cache = X, Wxh, hprev, Whh, h_cache, y_cache\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, Wxh, hprev, Whh, h_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, Wxh, hprev, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        # tanh\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        # Hidden gradient\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No full batch or files\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 246.5072\n",
      " ethe nirdona barican ro. ban licosh copleatatarnth ban po-th Indail In. ace Is Is cilliG an in Inthe\n",
      "Iter-26 loss: 242.1490\n",
      " an 2015–2015–2016 in Alobans pode Cot at Asha It Alat. larry Ba in an It 2015–2015–2016.s estankeshu\n",
      "Iter-39 loss: 247.7814\n",
      " the Gsextecedextu the Gsires of in Asoxe the Gtaled Asesint Abebero sixpexthing fsexpore Ath-mbeat 2\n",
      "Iter-52 loss: 232.1345\n",
      " It ans enjixChivest Coulth 2013 Japan reat esod the G20 hortalnd ust thane stry the namese II al lar\n",
      "Iter-65 loss: 236.3119\n",
      " lardideseveth Atteco and It. Aeantevece hiss the wrons Ripedecest Insed Asseventuritacty in the Gstr\n",
      "Iter-78 loss: 222.7942\n",
      " of the Righ inceardest A hist and and revitry world Asintry Br sext-rekid aked Asicted Rursest in th\n",
      "Iter-91 loss: 238.0024\n",
      " is Atolargest Itating Nisionsintert, Romertited Index, RNPing asiand Assinthenesealoese to the Globa\n",
      "Iter-104 loss: 242.3419\n",
      " sixth A of of in Ats.t cimprintiot 2015–2016 and in Aseatest the nonan is arescansy Gma) as in Aso, \n",
      "Iter-117 loss: 215.9585\n",
      " Asian country and rountry in Asole, parer and Aso-Japan of in Asiar ised Asinturakestt nast the Gn, \n",
      "Iter-130 loss: 222.0712\n",
      " Asiarepincty Aso, st rolt. Fiercan Russ the naxt inst in the Global 20th on-kanpest IIdion Chinat th\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x10ce9d6a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEiMgoOIiMQoOIiISIyCg4iIxPx/m2eiy3IcqNQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11955a3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
