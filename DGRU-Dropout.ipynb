{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0] # no regularizarion or no reg_loss\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    # import impl.constant as c\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[0].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 3 # depth\n",
    "n_iter = 1300 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMQYBIiIXYxAgInIxBgEiIhf7f+WxLhZFZ8oyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8238272fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
