{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0] # no regularizarion or no reg_loss\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    # import impl.constant as c\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[0].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 19.2456\n",
      "copy iopeopandiindred unst an 12m operokmes. wht rea 98, panku2my. The hila, Kaifgutseuran stuularc k\n",
      "Iter-26 loss: 19.6440\n",
      "caletyypplutar. Lonstugitangist en metran e Asry s935 mith 194, Japan is is the ld,ation inti nimokfy\n",
      "Iter-39 loss: 10.6831\n",
      "ced eext ans Rus Japan s a to oment in thinas Srecomeedth eccoral Hobateen carged eutho . Cina In thi\n",
      "Iter-52 loss: 7.5082\n",
      "cides man of Hivime urth and 3 he the zed Japan haskouted.dsim. Japan isic a toret pe arcer. The rose\n",
      "Iter-65 loss: 6.5976\n",
      "cympert ry cturos an itso pof 9n ismker. Japan in i shov. Leun o 47, count 12uparg the sixadiof Nicul\n",
      "Iter-78 loss: 5.0548\n",
      "cing etat co uricambortor. the Co th  sitenf wstho and Japan isecmeart etury umpth. eron the fornd be\n",
      "Iter-91 loss: 4.4952\n",
      "copolation ing enomeeine, Japan in in the lang an tueho Sountr  the hokt. hesoreeveran e farameeame o\n",
      "Iter-104 loss: 5.4759\n",
      "cileded ist eeopten micheice nomome nemen in the Earto . J and us of is the ou tr hir ovininin in ihi\n",
      "Iter-117 loss: 3.8807\n",
      "c Games.\n",
      ". Japan eavy micored ist In tevine in divid's and firllome foremoren Japan Easivedminteve co\n",
      "Iter-130 loss: 8.3176\n",
      "cympere lerest compertian stuntron retine suenter n oest bo nitonton ompina China Senc ared ur pariod\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x7f347406f6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2TU6PmlFdgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f34a2c0a7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
