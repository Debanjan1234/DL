{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of SELUs & Dropout-SELUs in NumPy\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An extra explaination from Reddit\n",
    "# # Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "# du = 0.001\n",
    "# u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "# u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# # print (u_new-u_old) / du\n",
    "# print(u_old, u_new)\n",
    "# # Now I see your problem: \n",
    "# #     You do not consider the effect of the weights. \n",
    "# #     From one layer to the next, we have two influences: \n",
    "# #         (1) multiplication with weights and \n",
    "# #         (2) applying the SELU. \n",
    "# #         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "# #         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "# #         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# # Oh yes, thats true, zero mean weights completely kill the mean. Thanks!\n",
    "\n",
    "# Tensorflow implementation\n",
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Tensorflow implementation on github\n",
    "# def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "#                  noise_shape=None, seed=None, name=None, training=False):\n",
    "#     \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "#     def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "#         keep_prob = 1.0 - rate\n",
    "#         x = ops.convert_to_tensor(x, name=\"x\")\n",
    "#         if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "#             raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "#                                              \"range (0, 1], got %g\" % keep_prob)\n",
    "#         keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "#         keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "#         alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "#         keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "#         if tensor_util.constant_value(keep_prob) == 1:\n",
    "#             return x\n",
    "\n",
    "#         noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "#         random_tensor = keep_prob\n",
    "#         random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "#         binary_tensor = math_ops.floor(random_tensor)\n",
    "#         ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "#         a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "#         b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "#         ret = a * ret + b\n",
    "#         ret.set_shape(x.get_shape())\n",
    "#         return ret\n",
    "\n",
    "#     with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "#         return utils.smart_cond(training,\n",
    "#             lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "#             lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Dropout to a value with rescaling.\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "# NumPy implementation\n",
    "def dropout_selu(X, p_dropout):\n",
    "    alpha= -1.7580993408473766 \n",
    "    fixedPointMean=0.0 \n",
    "    fixedPointVar=1.0\n",
    "    keep_prob = 1.0 - p_dropout\n",
    "#     noise_shape = noise_shape.reshape(X)\n",
    "    random_tensor = keep_prob\n",
    "    #         random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "    random_tensor += np.random.uniform(size=X.shape) # low=0, high=1\n",
    "    #         binary_tensor = math_ops.floor(random_tensor)\n",
    "    binary_tensor = np.floor(random_tensor)\n",
    "    ret = X * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "    #         a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * ((alpha-fixedPointMean)**2) + fixedPointVar)))\n",
    "\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    ret = a * ret + b\n",
    "    #         ret.set_shape(x.get_shape())\n",
    "    ret = ret.reshape(X.shape)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.269529622603 0.238741461412 0.842533466259 1.12314077973\n",
      "-0.191276680836 0.196408512881 0.863940928665 1.15999659563\n",
      "-0.260305796052 0.198248391232 0.840376306318 1.16079631406\n",
      "-0.178038627949 0.195871492111 0.829389516362 1.15729035683\n",
      "-0.184791099668 0.247544538154 0.86009545706 1.17806478766\n",
      "-0.184567678178 0.176109012134 0.883795665608 1.15019045197\n",
      "-0.177685947525 0.272610135699 0.857025015242 1.1560739998\n",
      "-0.172413310645 0.251602544631 0.866192464996 1.14706840521\n",
      "-0.204783155251 0.196256467718 0.831413789433 1.12569688276\n",
      "-0.231260917235 0.199345543742 0.850621042677 1.15488294415\n",
      "-0.198262291455 0.185763410598 0.86570902624 1.18618903187\n",
      "-0.189368179065 0.207968257914 0.849682926164 1.19718414045\n",
      "-0.245751492647 0.182934599357 0.8674369991 1.13325946801\n",
      "-0.193985688812 0.180393480031 0.842656664173 1.18722701691\n",
      "-0.189262554415 0.28281139756 0.866088293902 1.13905094872\n",
      "-0.214843349707 0.197344476764 0.863050346612 1.1639797022\n",
      "-0.203057058846 0.194591558071 0.853361773197 1.15678616517\n",
      "-0.18092345611 0.158503162014 0.87294865051 1.15360693847\n",
      "-0.203875940693 0.141694826236 0.869533695813 1.14181806011\n",
      "-0.213780137739 0.220720856337 0.858832595806 1.17791861357\n",
      "-0.174885941756 0.209692198338 0.866328427415 1.14455758639\n",
      "-0.166652261147 0.192987442693 0.861517655657 1.17363264821\n",
      "-0.208753742724 0.238072482989 0.834166454789 1.15600261822\n",
      "-0.223699040382 0.198558063354 0.872858615869 1.17064639697\n",
      "-0.194953152446 0.18952543987 0.860245921228 1.18402663041\n",
      "-0.207106847381 0.205488965667 0.848136020959 1.19822581372\n",
      "-0.178114588284 0.213385878598 0.854878365794 1.18697138446\n",
      "-0.193963010684 0.191303370812 0.839760757688 1.14713591503\n",
      "-0.189500994122 0.179617737778 0.846658129069 1.15070531869\n",
      "-0.222290189374 0.194754642511 0.864520258915 1.15110556191\n",
      "-0.179217248887 0.200164626203 0.861925206581 1.110895591\n",
      "-0.21195023869 0.195754648534 0.87663020677 1.15078146715\n",
      "-0.178486210268 0.199570873002 0.842702860926 1.14140796397\n",
      "-0.171244716829 0.193935482405 0.806533527539 1.13146147633\n",
      "-0.210189806384 0.204815774513 0.833540044951 1.18636183699\n",
      "-0.255130229659 0.167338429293 0.845656566202 1.14617916191\n",
      "-0.226856382004 0.251205653502 0.825888010846 1.14646565642\n",
      "-0.172970334701 0.266585328967 0.866858910697 1.17832881525\n",
      "-0.199568499384 0.149794317678 0.86876075645 1.19699348842\n",
      "-0.169087327575 0.166202569428 0.868246365108 1.18828444626\n",
      "-0.202973787425 0.182046751431 0.834465311458 1.16058944746\n",
      "-0.190970523996 0.257062900225 0.842096810212 1.15242548218\n",
      "-0.195027466404 0.194043122254 0.858789702735 1.14239594044\n",
      "-0.213523792291 0.224696382591 0.849232695992 1.14981164307\n",
      "-0.246763470439 0.186935643985 0.867658742231 1.17639910702\n",
      "-0.203992446282 0.195191246495 0.839884400325 1.14853165218\n",
      "-0.191967091045 0.212868033576 0.865100704537 1.18378841781\n",
      "-0.228524316559 0.188872522063 0.867830496046 1.16198191036\n",
      "-0.200862379295 0.165804326526 0.852150303786 1.12276653066\n",
      "-0.151155006999 0.143639662131 0.837038388621 1.1052278114\n",
      "-0.171351529009 0.195084522196 0.810944555072 1.11382777313\n",
      "-0.16923620689 0.158405210225 0.826315700195 1.1181238\n",
      "-0.214924586481 0.201156235326 0.823281445633 1.10762575406\n",
      "-0.203271341765 0.207426488984 0.849919524071 1.1436061176\n",
      "-0.208117615734 0.199742314947 0.873253740623 1.17219539237\n",
      "-0.170838942172 0.191551141654 0.864590339472 1.17374817757\n",
      "-0.24343775536 0.192211259349 0.847156620627 1.14350002269\n",
      "-0.212415936548 0.211919080374 0.848492813628 1.14549178783\n",
      "-0.193350370494 0.21615581432 0.86654523793 1.13555572079\n",
      "-0.215346936443 0.180771952316 0.867519098792 1.17300438112\n",
      "-0.17764829968 0.206360200342 0.871196769077 1.22968678919\n",
      "-0.207659592345 0.208772551003 0.854852912924 1.19604708672\n",
      "-0.216899880177 0.264124710327 0.839131046362 1.19314873382\n",
      "-0.172741006146 0.180060942049 0.845923703508 1.15277013283\n",
      "-0.207074584588 0.263931831209 0.84840891071 1.19657748271\n",
      "-0.254664651825 0.182068232817 0.829632480646 1.2084807192\n",
      "-0.188472446445 0.18583767339 0.859076398049 1.18548198147\n",
      "-0.1835931558 0.238982449498 0.868993689417 1.12933672577\n",
      "-0.230671912928 0.23294222711 0.864582256062 1.14685431242\n",
      "-0.211955121681 0.242650975424 0.844446515497 1.13002372168\n",
      "-0.168460492022 0.188176530338 0.863759730726 1.12638540441\n",
      "-0.178649060233 0.159550663057 0.844901293531 1.10507937915\n",
      "-0.229556548696 0.201268989404 0.843591646302 1.13087910109\n",
      "-0.192461273655 0.185643073166 0.867542844759 1.13242551107\n",
      "-0.181539191096 0.202535211311 0.875372622787 1.15251138939\n",
      "-0.210630729482 0.211435745572 0.879627842495 1.14339080435\n",
      "-0.202242813304 0.182120138164 0.78861974624 1.14465412876\n",
      "-0.184019766246 0.266195902526 0.830078941036 1.12505430194\n",
      "-0.170758153674 0.176496072796 0.806346577648 1.14878656704\n",
      "-0.24740652096 0.192341803255 0.856048737209 1.12381743141\n",
      "-0.177566982776 0.236541003553 0.848912054251 1.15904358553\n",
      "-0.195946843395 0.199270528248 0.866346982792 1.1710255759\n",
      "-0.171402136639 0.179558483816 0.831336988904 1.1420108033\n",
      "-0.242553195254 0.188522803045 0.858832583558 1.16769429907\n",
      "-0.212126238528 0.189984054269 0.869990817468 1.16687047025\n",
      "-0.219094831485 0.278829857977 0.860861191177 1.15344419069\n",
      "-0.192771883821 0.265925714555 0.849744263157 1.14085510167\n",
      "-0.195922249421 0.204769792078 0.845554301295 1.12067334814\n",
      "-0.22092566319 0.169898553839 0.871766945162 1.16022837621\n",
      "-0.182105561598 0.225593212455 0.860141316191 1.16883439819\n",
      "-0.191373417284 0.166240121644 0.861566952108 1.15307019576\n",
      "-0.202939561311 0.194061684542 0.830280429289 1.16657717519\n",
      "-0.168616424689 0.243868432588 0.858378139705 1.16392639714\n",
      "-0.167391479075 0.206400823195 0.839256446814 1.15082651797\n",
      "-0.16583173531 0.182378977161 0.847753332774 1.15076326015\n",
      "-0.220091607493 0.207616935504 0.84596454309 1.15940479381\n",
      "-0.263928060498 0.259714568689 0.831098913077 1.13292260881\n",
      "-0.171331382725 0.221313438291 0.842661678072 1.14493001427\n",
      "-0.190577045766 0.200613187016 0.816890560956 1.15479424036\n",
      "-0.22191733267 0.232460402573 0.839674342313 1.19102990503\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "    x = dropout_selu(X=x, p_dropout=0.10)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My NumPy implemetation of Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.214012249027 0.229167129326 0.932332542517 1.35514712732\n",
      "-0.223693792421 0.356189784779 1.00770587731 1.49948911469\n",
      "-0.205881861505 0.328283949271 1.03819915898 1.54480701192\n",
      "-0.237758376677 0.305559993102 1.1349800102 1.67391420544\n",
      "-0.236016430438 0.391877470169 1.09925499753 1.8112463819\n",
      "-0.18195362093 0.496330978983 1.21282794598 1.89902328039\n",
      "-0.171558426569 0.412001003287 1.19432537679 2.06609202396\n",
      "-0.16823370468 0.467072835519 1.19269352637 2.15085026297\n",
      "-0.212979175376 0.547603855662 1.21138654432 2.15184578053\n",
      "-0.177900749039 0.6419724667 1.20386863829 2.13084117537\n",
      "-0.202725010062 0.495420764904 1.18244872568 2.24586723365\n",
      "-0.123165700297 0.5420187544 1.29085802962 2.19068147843\n",
      "-0.251513671944 0.551490812535 1.31297112664 2.07115535574\n",
      "-0.182744872091 0.458915855254 1.24893027974 2.14046098202\n",
      "-0.135270285146 0.580220120799 1.20291256989 2.19956243188\n",
      "-0.14777285446 0.538440624983 1.26158617999 2.10253823161\n",
      "-0.18342114927 0.421910974881 1.26314496206 2.03972945366\n",
      "-0.149695713892 0.393733765629 1.29300984827 2.12008037617\n",
      "-0.135494189104 0.723958000173 1.27812424483 2.21585052428\n",
      "-0.145675733189 0.686424715621 1.3435969498 2.31556067901\n",
      "-0.159009066273 0.468616245489 1.24283772691 2.16531321546\n",
      "-0.116687259778 0.498368469596 1.33627919826 2.1745867658\n",
      "-0.136320534219 0.645983764037 1.29620155266 2.26363712648\n",
      "-0.107943319412 0.532003185415 1.31027339876 2.20356116398\n",
      "-0.157802339666 0.602446003344 1.3224403172 2.30546983514\n",
      "-0.219113891788 0.481142381943 1.24097826099 2.22571625276\n",
      "-0.148882132668 0.514900382045 1.30096896023 2.29711698739\n",
      "-0.167435146577 0.723326227645 1.23444159374 2.14675797562\n",
      "-0.110853576304 0.563450754043 1.29824222155 2.17339076195\n",
      "-0.147413888503 0.567998494616 1.2487395164 2.26916963869\n",
      "-0.207265401382 0.47038969678 1.25443378226 2.19024251764\n",
      "-0.243530812106 0.42642387626 1.20812114809 2.20735500555\n",
      "-0.208664310103 0.62206535836 1.30826121928 2.12056878015\n",
      "-0.144382144088 0.499670699371 1.2768777906 2.15630251\n",
      "-0.170434239709 0.565284362539 1.28945763733 2.1891278132\n",
      "-0.183381346492 0.525665934336 1.33504597357 2.16779370357\n",
      "-0.146598729561 0.686879024514 1.29334953093 2.13035626386\n",
      "-0.160535317647 0.511241981062 1.32263949378 2.13750379573\n",
      "-0.213470955209 0.555801792284 1.21346688877 2.10664423632\n",
      "-0.112474742231 0.591800562745 1.19347143954 2.19116420926\n",
      "-0.286876043829 0.421102053867 1.23452756288 2.08293648212\n",
      "-0.178990039007 0.545865340027 1.15283814894 2.1332507317\n",
      "-0.108534986217 0.547050163293 1.28062093545 2.13809691663\n",
      "-0.114997899388 0.542463953937 1.21526503964 2.20807972294\n",
      "-0.248780292967 0.54891722966 1.2735557566 2.32887696219\n",
      "-0.147002038125 0.54319668231 1.26008815091 2.12628995283\n",
      "-0.201583186224 0.552169719257 1.27228043809 2.06462884411\n",
      "-0.25798739538 0.480819677164 1.24840301057 2.11682943113\n",
      "-0.0949373665742 0.488322570065 1.25246977549 2.195841249\n",
      "-0.235804743161 0.480259373187 1.25978231067 2.08291823712\n",
      "-0.147129394771 0.468937133217 1.15681395571 2.16742842146\n",
      "-0.156990086416 0.632681135951 1.23564541868 2.18635273967\n",
      "-0.25106768021 0.546276335923 1.3038182027 2.21316580913\n",
      "-0.122949332666 0.619843413543 1.32670360463 2.20937575524\n",
      "-0.194933291078 0.539865527582 1.32810119079 2.09901319136\n",
      "-0.178024022367 0.499902225377 1.31862226534 2.11865234829\n",
      "-0.205952481206 0.536045309772 1.3083028873 2.29984210661\n",
      "-0.168048434492 0.59506429087 1.28167591326 2.26431498067\n",
      "-0.113026775639 0.473096672208 1.27162810091 2.08551088275\n",
      "-0.172772734269 0.521041821173 1.27077417091 2.10613590277\n",
      "-0.152347746039 0.514238455677 1.29100404995 2.14985425969\n",
      "-0.147689887948 0.595697074684 1.19687956311 2.26892698659\n",
      "-0.1692364392 0.515276540165 1.24507753881 2.18135765887\n",
      "-0.183978954847 0.515985156465 1.23163775375 2.26761248666\n",
      "-0.167288556282 0.516300368746 1.21048531006 2.25443484721\n",
      "-0.178204688668 0.539732393229 1.27898481702 2.06029053567\n",
      "-0.1483642459 0.527092903794 1.28038612297 2.07880591577\n",
      "-0.0846274072164 0.537970618946 1.29191460648 2.20217777935\n",
      "-0.188199917898 0.566511842041 1.29742615175 2.30000799205\n",
      "-0.156624109286 0.544895322534 1.22857189991 2.15991580622\n",
      "-0.136478986206 0.499945247635 1.29526197899 2.17602420658\n",
      "-0.252217437491 0.600576819703 1.29230852929 2.21913805265\n",
      "-0.160464811509 0.644744024272 1.22731650446 2.26992332727\n",
      "-0.133125252363 0.578668015333 1.17975035044 2.18930137539\n",
      "-0.134793367675 0.49934271057 1.28282342954 2.19459314293\n",
      "-0.206257537682 0.558873926641 1.2594052477 2.28887728696\n",
      "-0.149363170075 0.532512223204 1.31540608542 2.22701682132\n",
      "-0.127873503726 0.741348100526 1.29220528571 2.34951411359\n",
      "-0.223748726813 0.474577843748 1.33211478791 2.28679001435\n",
      "-0.291632405342 0.604167205015 1.31377184761 2.23332325209\n",
      "-0.193961146671 0.606989918722 1.24215027072 2.16961259965\n",
      "-0.144813678643 0.620673295586 1.31505438792 2.24470084485\n",
      "-0.10783161223 0.718486125575 1.30123177583 2.25570120019\n",
      "-0.183977430598 0.717932302339 1.29985984141 2.40954475292\n",
      "-0.127525056451 0.629873122416 1.3330647889 2.49137155185\n",
      "-0.168470722229 0.553191977563 1.18541912311 2.20732623146\n",
      "-0.215296534435 0.474452301389 1.20686721705 2.14848601525\n",
      "-0.228051021799 0.531345713826 1.20844191804 2.15475909471\n",
      "-0.175541947464 0.560596205918 1.28294662523 2.17760570377\n",
      "-0.103553414712 0.554493796851 1.25583314308 2.15210565998\n",
      "-0.177439220307 0.557800919104 1.31083231843 2.11923345177\n",
      "-0.0576714062394 0.552013015931 1.32391866994 2.22962672231\n",
      "-0.135018467645 0.59196819088 1.27208436054 2.17578451195\n",
      "-0.111745482857 0.62433293926 1.29708046385 2.2830717535\n",
      "-0.137793052198 0.592597030502 1.23629754327 2.12551564481\n",
      "-0.199017110445 0.664684922711 1.26716906734 2.13157174261\n",
      "-0.110785361046 0.432753574403 1.27163779779 2.10959478078\n",
      "-0.13902404116 0.451271621041 1.30165913642 2.1241021292\n",
      "-0.188661652468 0.516424756922 1.26006881 2.1140308797\n",
      "-0.196911880935 0.488727228909 1.31210661371 2.09714476648\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    alpha = 1.0\n",
    "    scale = 1.0\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def elu_bwd(dout, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dout = dout * scale\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0267840597416 0.34870387555 0.688048836569 1.04717887996\n",
      "0.0118506955263 0.304876451832 0.56287559608 0.883068359607\n",
      "-0.00848144962242 0.206902931306 0.461438275839 0.773605957189\n",
      "-0.0135542733853 0.188593823674 0.382063563973 0.688186843237\n",
      "-0.0561000469178 0.122238741339 0.349825172164 0.602874984924\n",
      "-0.0343566384099 0.18028897461 0.31253884688 0.548510582005\n",
      "-0.0429958073188 0.116054084819 0.258804537482 0.468348512243\n",
      "-0.0365803448673 0.0872113156711 0.24577948731 0.438054082984\n",
      "-0.0599072152755 0.107624519345 0.215066658884 0.422357677398\n",
      "-0.0350411695585 0.0776162947341 0.205136251217 0.425153568286\n",
      "-0.0283457218838 0.0652535851403 0.185845104457 0.385896334075\n",
      "-0.0272042989458 0.0766230668132 0.17945437564 0.343158539221\n",
      "-0.0265363719492 0.0529204998045 0.175905477592 0.337747682314\n",
      "-0.0195131277272 0.050982043472 0.157488026364 0.320393452692\n",
      "-0.0279429986826 0.0494597719809 0.148463686617 0.302291903009\n",
      "-0.0163050932564 0.0395091568666 0.134573867209 0.275776177388\n",
      "-0.0350636247315 0.0446047271227 0.130681932638 0.260456518769\n",
      "-0.0280860610856 0.0521212852515 0.127825010424 0.242927405648\n",
      "-0.031611045363 0.0448226140599 0.119061552083 0.247784549674\n",
      "-0.0160116482817 0.0350665171291 0.110955975843 0.230000436935\n",
      "-0.032908915089 0.0455961663955 0.106944810848 0.221535694601\n",
      "-0.0255515030656 0.0533079790241 0.101583851262 0.218571710404\n",
      "-0.0287315200628 0.0505785025543 0.100373477838 0.211213550803\n",
      "-0.0218860512348 0.0395445699361 0.09912119378 0.216332757744\n",
      "-0.0155101553201 0.0300637152255 0.0940974816116 0.217682172512\n",
      "-0.0278378861071 0.0382129355701 0.0871000120494 0.213244116615\n",
      "-0.0166008582174 0.0282495426849 0.0856275861282 0.201773089435\n",
      "-0.0241697863 0.0411436133239 0.0820385581023 0.206187616881\n",
      "-0.0157925180858 0.0296287485956 0.0816271592875 0.202022137338\n",
      "-0.0131223551871 0.0344921217048 0.0810812233719 0.189484668356\n",
      "-0.0178672551798 0.0196992522479 0.0769284293791 0.190846401789\n",
      "-0.015907970919 0.0340555629548 0.0739359328161 0.195642685556\n",
      "-0.0167867545129 0.028162573988 0.0688426757733 0.185973613894\n",
      "-0.0198006002858 0.0291724924825 0.0722614785159 0.170296669289\n",
      "-0.0223565323619 0.0271525026234 0.0676686938757 0.165745072873\n",
      "-0.0129344926851 0.0326925108362 0.0674981892731 0.160179767947\n",
      "-0.020047396402 0.0244167398387 0.0696481896366 0.154583522557\n",
      "-0.0354766110678 0.0286733638038 0.0646655802468 0.151386817687\n",
      "-0.0166541157601 0.019886945775 0.0650172585953 0.15715925585\n",
      "-0.0169269825833 0.0263314959328 0.0612392050587 0.148117722604\n",
      "-0.0208108792175 0.0337166211252 0.0567820375857 0.157970273241\n",
      "-0.0175155554353 0.0194623862136 0.050853477726 0.148709289169\n",
      "-0.0163136072795 0.0194587848715 0.0528851158249 0.154573862383\n",
      "-0.0224563452682 0.0330225583336 0.054727681792 0.155185488264\n",
      "-0.0148554864396 0.0232484025477 0.0526528855982 0.155948090419\n",
      "-0.015407728749 0.0226526584702 0.0503735663922 0.161205399028\n",
      "-0.0198421090144 0.0295908023975 0.0545698799751 0.166946872485\n",
      "-0.0101000184353 0.0164851423265 0.0544275861449 0.17017293538\n",
      "-0.0183716129586 0.0214069727876 0.0521637561451 0.1592011914\n",
      "-0.0164162385149 0.0236446085742 0.0512908036539 0.165370371683\n",
      "-0.0124907074268 0.0173635905341 0.0468553304154 0.155802368363\n",
      "-0.0264065451783 0.0237436173671 0.0495317528297 0.144526511002\n",
      "-0.0170799485706 0.0211716388198 0.0497479568515 0.146070335122\n",
      "-0.0126531275369 0.0214300433465 0.0492852147178 0.144859578639\n",
      "-0.0096736574645 0.017352726812 0.0518772176472 0.134865122738\n",
      "-0.0161867091492 0.0147488788412 0.0506882555431 0.132496135044\n",
      "-0.0214809800781 0.0205734417392 0.0512116767513 0.13908018094\n",
      "-0.016115397592 0.025904050316 0.046993164924 0.129877520715\n",
      "-0.0168817140433 0.0189450076508 0.0447395484776 0.127975415041\n",
      "-0.0126701066329 0.0180751524404 0.0417024671629 0.125152155025\n",
      "-0.0144218596113 0.0178510686842 0.0409738874511 0.129313206402\n",
      "-0.0103570746363 0.0142001515875 0.0416981939267 0.129968687474\n",
      "-0.0123291748387 0.0162648937274 0.0406117482539 0.124972189775\n",
      "-0.015346694443 0.0252470819536 0.0382869204296 0.133190426772\n",
      "-0.0140690491473 0.0143299470095 0.0359749590823 0.12737939207\n",
      "-0.0140148315276 0.0145609375856 0.0360992892364 0.121716602723\n",
      "-0.0198422635265 0.0202262833543 0.0354968258464 0.130395775452\n",
      "-0.00848609147717 0.0157402132932 0.0363170194672 0.127288818346\n",
      "-0.0118144287768 0.0155386830612 0.0386537046769 0.127784844402\n",
      "-0.0125379898886 0.0172792918855 0.0376219436861 0.127249539625\n",
      "-0.0148385557007 0.0163772349032 0.0367424337849 0.129384211941\n",
      "-0.012194784855 0.0190643416517 0.0338767916465 0.1170230661\n",
      "-0.0150463114264 0.0153197907742 0.0352578702687 0.117922457118\n",
      "-0.0200746660961 0.0196019318234 0.0340021113441 0.117798748676\n",
      "-0.00999526481601 0.0168783081618 0.0363771894882 0.114587608924\n",
      "-0.0107125452314 0.0155062187427 0.0375831283136 0.11133651553\n",
      "-0.0149699987639 0.0196617516231 0.0361636506983 0.111645390051\n",
      "-0.0130906496703 0.0206873098424 0.0354054042945 0.113875946051\n",
      "-0.0157400081638 0.0198031247077 0.0343283491918 0.119985952594\n",
      "-0.0165416418283 0.0239340061497 0.0321229110038 0.121945496186\n",
      "-0.0181071892377 0.018167803194 0.0335861073213 0.126017344986\n",
      "-0.013953680237 0.0172885594547 0.0343894257796 0.125344627025\n",
      "-0.0186229709036 0.0207122486406 0.0353495796649 0.12250358027\n",
      "-0.00878233107791 0.0165026205361 0.0380215242646 0.129686818452\n",
      "-0.0134546068636 0.0146227046556 0.0365660470591 0.129589844885\n",
      "-0.0111243095259 0.0140677223193 0.0372159083678 0.129993633691\n",
      "-0.0150002017929 0.0234591629626 0.0388439076028 0.129830324013\n",
      "-0.012063932092 0.0165148835715 0.0383054163978 0.124212667474\n",
      "-0.0104740181877 0.0221637545266 0.036589893577 0.12121861639\n",
      "-0.0117376051638 0.0182647042151 0.0354353912801 0.123536789701\n",
      "-0.0128660240953 0.0164226391203 0.0356750709574 0.13046883613\n",
      "-0.0104825693764 0.0208843272362 0.0349852664605 0.122620847611\n",
      "-0.00977902744701 0.0186812053538 0.0324718208798 0.12684655601\n",
      "-0.0107864513671 0.0151510064738 0.0350752961548 0.12362225557\n",
      "-0.0159308759 0.0168429393574 0.0387256721004 0.125421593372\n",
      "-0.0146330148658 0.0147085934465 0.0390732010775 0.130182556005\n",
      "-0.0245935641368 0.0273270660344 0.0382575769433 0.130610395861\n",
      "-0.0128806106158 0.0190988912682 0.038211902248 0.12852023976\n",
      "-0.0120070545454 0.0149455463847 0.0392919992364 0.135765570327\n",
      "-0.0104428986723 0.0159663312185 0.0406177137163 0.133933685466\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, _ = elu_fwd(X=x)\n",
    "    x, _ = dropout_forward(p_dropout=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def selu_bwd(dout, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dout = dout * scale\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX\n",
    "\n",
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.115228899805 0.273307640366 0.832187931633 1.20092983661\n",
      "-0.11342776269 0.297433766906 0.890738815289 1.25180513542\n",
      "-0.12925098648 0.362425379672 0.93295919605 1.3384718603\n",
      "-0.12964971994 0.366295910839 0.966780089222 1.40596996607\n",
      "-0.0864575866013 0.290687936815 0.959321646025 1.33767307621\n",
      "-0.111998627179 0.300401219852 0.951157571282 1.35759226264\n",
      "-0.0878051364106 0.313732327559 0.985100281202 1.39437359224\n",
      "-0.102829064867 0.313342590658 0.967987306932 1.41533396008\n",
      "-0.134024418009 0.514400568221 1.0022174775 1.42117544315\n",
      "-0.156572603248 0.331289658482 1.00391056547 1.41748889863\n",
      "-0.164599296478 0.300678860193 0.987115048323 1.39145024166\n",
      "-0.0764760393873 0.297173187184 0.996668365766 1.37470533887\n",
      "-0.126296878605 0.326807280604 0.978965605922 1.37800377186\n",
      "-0.0872685363087 0.326317075429 0.987838245488 1.40211446152\n",
      "-0.118293785738 0.419532184593 0.958342284191 1.3946407666\n",
      "-0.0860908759643 0.436037876788 0.961645016304 1.44609169824\n",
      "-0.0636222554284 0.386101182923 0.998806334399 1.44759920003\n",
      "-0.181929225149 0.335378265442 0.999062539032 1.50121548239\n",
      "-0.0895673207494 0.334585065123 0.978050127889 1.44168677893\n",
      "-0.129702584252 0.410672023252 0.979377827597 1.45735925441\n",
      "-0.239788123087 0.397520767005 0.945265370174 1.42983578513\n",
      "-0.114522432621 0.400078782696 0.979690293053 1.47149783274\n",
      "-0.12286204896 0.29293371492 0.9473377336 1.41184702522\n",
      "-0.101474229466 0.369619684188 0.999720958148 1.3810675652\n",
      "-0.0875088160167 0.356033207916 0.996492860525 1.38997988313\n",
      "-0.0946157989033 0.436864069074 0.970902699465 1.43291050902\n",
      "-0.0640008116266 0.430058782575 1.00581954935 1.52568786306\n",
      "-0.151494854851 0.393074369122 0.983681101082 1.46282286595\n",
      "-0.168454054018 0.289481786372 0.979100203733 1.45568830527\n",
      "-0.185721044401 0.419578957749 1.00562503129 1.40983235576\n",
      "-0.0779894210024 0.376220829132 0.986582581946 1.39545711609\n",
      "-0.0717489161858 0.401662282137 1.00267600521 1.43037733691\n",
      "-0.0834211478815 0.407552124968 0.976197123755 1.44592490008\n",
      "-0.124766901476 0.321803063974 0.997203906976 1.3824228762\n",
      "-0.0726854315395 0.373692391771 0.993444989714 1.39067625825\n",
      "-0.0889516106906 0.350439422222 0.963004067953 1.44205886941\n",
      "-0.119535482336 0.377482929881 1.00382149059 1.40373522689\n",
      "-0.162243524847 0.405572110471 0.972568962118 1.44796505305\n",
      "-0.115348812343 0.333949912358 0.944730788459 1.42005855386\n",
      "-0.221421289859 0.39763314378 0.982301036853 1.4116961299\n",
      "-0.057044608757 0.336705596383 0.986268499542 1.40643411308\n",
      "-0.123094511023 0.415682596369 0.954767297054 1.35504161193\n",
      "-0.0759744768879 0.363510539801 0.957621005938 1.37455651238\n",
      "-0.101645481764 0.354558410155 0.950494082756 1.43273158666\n",
      "-0.127625310522 0.410104436223 0.993055772671 1.46136881227\n",
      "-0.0356890630054 0.362543198441 0.978712762247 1.49758591572\n",
      "-0.0599886813281 0.323979735209 0.973233916807 1.48434763641\n",
      "-0.124792699341 0.329145019092 0.970251737096 1.41189716712\n",
      "-0.190615343296 0.429466256457 0.978073987919 1.41863357721\n",
      "-0.153923626409 0.350039127356 0.986090950284 1.42331658194\n",
      "-0.16435022628 0.384479650199 1.01247912416 1.46547119279\n",
      "-0.127714259362 0.350174670831 0.975322522342 1.55295707261\n",
      "-0.0857134363747 0.410998524197 0.955041865151 1.48983922539\n",
      "-0.11026119626 0.414133292738 0.96018444356 1.42920560989\n",
      "-0.11236231239 0.338400084054 1.03160299222 1.40501196495\n",
      "-0.144120813702 0.388965592842 0.995790919483 1.42095502727\n",
      "-0.063860892898 0.482335056791 0.997008399852 1.45516344289\n",
      "-0.124909849825 0.327534844585 1.01094127231 1.47281489121\n",
      "-0.0597944336601 0.35684589467 1.01157819635 1.41768397797\n",
      "-0.116424475879 0.370545889082 0.978530051119 1.38158405192\n",
      "-0.12604576844 0.386974720324 1.01124855551 1.41705015758\n",
      "-0.125046603261 0.33972502754 1.01096352395 1.37476548722\n",
      "-0.0635720714009 0.303457290749 1.00467406171 1.37744868269\n",
      "-0.140896604417 0.351730765134 0.966295356459 1.40981267255\n",
      "-0.0764839727102 0.364760998051 0.959241988556 1.39752390746\n",
      "-0.145541537066 0.311771818282 1.00320481672 1.44410827978\n",
      "-0.079181448015 0.320372685645 1.01196207684 1.39729720958\n",
      "-0.119191486848 0.307728554891 1.02275844026 1.41402569739\n",
      "-0.0760905141137 0.345901578886 0.988140631682 1.442634556\n",
      "-0.170817274991 0.424273709812 1.01878146756 1.46326403069\n",
      "-0.0479612435572 0.398387422933 0.997689244216 1.49310871523\n",
      "-0.104832570958 0.370683743693 0.979499043648 1.37027316861\n",
      "-0.180111593914 0.397108854986 0.958167538423 1.4284760874\n",
      "-0.102975045489 0.315020834426 0.973338610799 1.37137289664\n",
      "-0.0621111553628 0.382668641556 0.969211583555 1.43263462232\n",
      "-0.0880530108861 0.356197030098 0.991121917719 1.44426625451\n",
      "-0.114691033081 0.354841433295 1.03363527697 1.41606525611\n",
      "-0.0587464766113 0.443235237822 1.01210956955 1.53606060258\n",
      "-0.13496297684 0.39412358506 1.01902801234 1.5063958555\n",
      "-0.0982104874004 0.457533567955 0.979983742039 1.369184593\n",
      "-0.0709791664788 0.330664605095 0.94361573412 1.42976467017\n",
      "-0.116435140097 0.397192428061 1.00294462195 1.43266672531\n",
      "-0.11638472554 0.363163916631 0.953032293025 1.37745969884\n",
      "-0.103411068685 0.364929661599 1.01612552216 1.38531070911\n",
      "-0.10960539275 0.399266531849 1.01512582848 1.44068868663\n",
      "-0.109631165564 0.405557520374 0.944475397744 1.44824900446\n",
      "-0.123586080421 0.391352404912 0.977718978685 1.40683731901\n",
      "-0.147916275529 0.38109901041 0.992910014363 1.3948313526\n",
      "-0.0676123481777 0.371553527035 0.993930322772 1.46712264129\n",
      "-0.115854620798 0.365832390649 1.00550158714 1.44763375719\n",
      "-0.0250868837277 0.392841071737 0.989923904154 1.47457954248\n",
      "-0.174018098769 0.329962278997 1.0131579859 1.44885163387\n",
      "-0.113631702155 0.39350645338 0.99525406775 1.47566704784\n",
      "-0.0874180205547 0.343342975009 1.00699174476 1.3995414103\n",
      "-0.133145950033 0.352128285507 0.975793358161 1.43749520742\n",
      "-0.156125420521 0.37358808285 0.978818264058 1.42430662335\n",
      "-0.0350291961047 0.376470230234 0.955409941352 1.41604863003\n",
      "-0.151570195217 0.386990408432 0.978749655746 1.47324065753\n",
      "-0.160330897662 0.361891592797 0.963041843429 1.5023204259\n",
      "-0.1048696337 0.343512187342 0.967033160237 1.40707302618\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "    x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.235271254088 0.218746820232 0.819007812136 1.11747800096\n",
      "-0.20951946942 0.180946004293 0.834080921638 1.13113481879\n",
      "-0.191676850317 0.224930764106 0.868843915156 1.13158722935\n",
      "-0.176701747105 0.157517304949 0.852585573073 1.12639938655\n",
      "-0.162840032431 0.186669002989 0.858098346654 1.18314823773\n",
      "-0.220704901111 0.190378608271 0.833024054349 1.14342284633\n",
      "-0.19923882051 0.211312399024 0.87131817578 1.121972573\n",
      "-0.210321754749 0.237660213544 0.871612931827 1.11061683783\n",
      "-0.185461154997 0.225793126856 0.862512256897 1.1125366835\n",
      "-0.178912312266 0.155551393681 0.852681937939 1.16124534451\n",
      "-0.208812346507 0.179387508362 0.84954272907 1.16233918417\n",
      "-0.227677609213 0.203579291629 0.853329180904 1.13222261062\n",
      "-0.177117268663 0.182170079468 0.870238280515 1.21209197618\n",
      "-0.187281710031 0.206016154254 0.866706826485 1.24963262405\n",
      "-0.18972440741 0.171150605527 0.839788078575 1.16848345882\n",
      "-0.187734315865 0.204341560836 0.850645039618 1.14832907681\n",
      "-0.280902837708 0.170151586913 0.888266070211 1.11742795259\n",
      "-0.220330882606 0.14259808742 0.880024049717 1.15157886524\n",
      "-0.226636394136 0.23990538548 0.880766720859 1.17482291552\n",
      "-0.189821889697 0.197320949643 0.843138492515 1.15156231693\n",
      "-0.205873736051 0.193131118624 0.875224317695 1.15097064375\n",
      "-0.175492244751 0.238941788248 0.834642009854 1.14497440375\n",
      "-0.193472005181 0.212400216644 0.831021648919 1.17721384286\n",
      "-0.246634064447 0.247270559311 0.8547008918 1.16955789493\n",
      "-0.191233743558 0.144080385902 0.869282115449 1.1375256101\n",
      "-0.163136827169 0.208774959456 0.825411819344 1.15468653809\n",
      "-0.218951542539 0.198246038325 0.86545646679 1.13294822955\n",
      "-0.216843598195 0.1841587331 0.854876706012 1.14539093729\n",
      "-0.184325132298 0.18782000645 0.877681698758 1.14607753804\n",
      "-0.199951278206 0.197911458732 0.847665010603 1.22978029564\n",
      "-0.206654581907 0.195201978252 0.878407308985 1.15406802818\n",
      "-0.20437491412 0.191322549539 0.858402436328 1.14076194107\n",
      "-0.20558495464 0.185967871578 0.862151182605 1.15155434123\n",
      "-0.186210928388 0.227452088474 0.867872081867 1.13965760059\n",
      "-0.180288625717 0.184311261643 0.87015350107 1.12869123966\n",
      "-0.232211626681 0.19854605133 0.840691609251 1.16148837842\n",
      "-0.218985205811 0.221109398767 0.846810926425 1.16182495504\n",
      "-0.171455130564 0.203968686744 0.876026884182 1.18448132668\n",
      "-0.179136516363 0.25457484844 0.857672883112 1.15725407675\n",
      "-0.168851149152 0.196488814157 0.85853365183 1.15100947035\n",
      "-0.200782681667 0.245520575034 0.815319213692 1.13505933684\n",
      "-0.196076374254 0.216946720482 0.875178421485 1.14586042992\n",
      "-0.256058410821 0.252588332087 0.835856111409 1.20354823254\n",
      "-0.182925761399 0.195474352741 0.842824233653 1.14459453033\n",
      "-0.214055138057 0.168941626199 0.844726735188 1.13298125855\n",
      "-0.195325163158 0.224660829554 0.858530846361 1.14165473406\n",
      "-0.164389995726 0.233696490852 0.824337181215 1.16021691504\n",
      "-0.168926932499 0.203342206165 0.86365739059 1.12675590037\n",
      "-0.162744903582 0.225948776182 0.820860826687 1.1540144229\n",
      "-0.177636087335 0.180090119558 0.868510528365 1.14555843743\n",
      "-0.185366719749 0.175472965264 0.865415335332 1.15668091424\n",
      "-0.172144455901 0.198971184807 0.880743440967 1.17422968644\n",
      "-0.149990556026 0.188368388553 0.868203617649 1.14774137491\n",
      "-0.184332762657 0.249627322089 0.85248926982 1.1458634033\n",
      "-0.18322972541 0.228401584125 0.805995120347 1.16455251229\n",
      "-0.197336426306 0.20840499965 0.883534174274 1.15774321119\n",
      "-0.201133575704 0.217010114929 0.867981945299 1.13153987447\n",
      "-0.178305165752 0.177380291659 0.853555202397 1.17495468495\n",
      "-0.235686944123 0.27563391389 0.872517827639 1.1485206036\n",
      "-0.238611058438 0.225520759152 0.850135813771 1.1452570092\n",
      "-0.259778672002 0.217992017853 0.867295338731 1.16102238296\n",
      "-0.191238396553 0.205245135192 0.841190956359 1.17083074066\n",
      "-0.199091794564 0.214670972154 0.863521349713 1.16552454106\n",
      "-0.171686853218 0.165020872897 0.863416790237 1.13921198411\n",
      "-0.21576394731 0.218156059674 0.85967427098 1.17932862401\n",
      "-0.211034754928 0.181263462459 0.852238841569 1.15577869761\n",
      "-0.171734897301 0.279100453093 0.85344826544 1.16221910543\n",
      "-0.18058173168 0.215356516476 0.812394427257 1.19214139814\n",
      "-0.22968986685 0.245851465335 0.814961108608 1.13296164351\n",
      "-0.219671751894 0.215431727004 0.840563229436 1.14072504988\n",
      "-0.17199595298 0.196332422511 0.820022687474 1.1601672039\n",
      "-0.16791417979 0.194388700378 0.886054819479 1.18283167625\n",
      "-0.244062893364 0.228499409558 0.869744300466 1.18297066345\n",
      "-0.170131325814 0.197957212453 0.871581331834 1.26309722924\n",
      "-0.221526218891 0.168015691401 0.866273474113 1.16292409454\n",
      "-0.202304700265 0.268614830259 0.817890519584 1.13329663254\n",
      "-0.242547048341 0.211104829698 0.836321649289 1.1456543911\n",
      "-0.210388574893 0.238577685511 0.873284257877 1.15776877689\n",
      "-0.177084135402 0.199950552718 0.859912283595 1.15676093956\n",
      "-0.142507001207 0.181483501425 0.850632692514 1.1843000259\n",
      "-0.226206932601 0.240228493093 0.85312992704 1.14203790583\n",
      "-0.244211728367 0.230956994942 0.852536135596 1.16515601915\n",
      "-0.196783355423 0.217203779164 0.835052970249 1.14935186648\n",
      "-0.173635608596 0.236066523527 0.835443241942 1.17755345428\n",
      "-0.197964129533 0.173897314788 0.873821626864 1.11564752531\n",
      "-0.171468212441 0.19839202565 0.840693423865 1.16581235027\n",
      "-0.164416872794 0.191299332174 0.835008446054 1.15876284314\n",
      "-0.197619636712 0.231716212086 0.859859459155 1.14281457889\n",
      "-0.160798227911 0.192000655987 0.851376909838 1.2099392157\n",
      "-0.195438231889 0.207520012432 0.834400922874 1.22496576616\n",
      "-0.185863514621 0.23616269701 0.875662296086 1.17353868554\n",
      "-0.185753292829 0.210278825767 0.882668921186 1.13729341552\n",
      "-0.172520768407 0.218306559551 0.841147936969 1.16908595573\n",
      "-0.20651110986 0.163589102113 0.852581600469 1.13359533899\n",
      "-0.207419326572 0.222878345199 0.868491130856 1.14052416793\n",
      "-0.173084874526 0.198539726272 0.842677586379 1.14847577783\n",
      "-0.218996348546 0.17531075816 0.867900683075 1.15273858678\n",
      "-0.175761437348 0.182870341533 0.849333139767 1.13308471141\n",
      "-0.150969757134 0.165189388758 0.866035404474 1.1731937917\n",
      "-0.268348360458 0.230749083185 0.866782049306 1.1533809317\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "    x = dropout_selu(X=x, p_dropout=0.10)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
