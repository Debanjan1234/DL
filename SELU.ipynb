{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of SELUs & Dropout-SELUs in NumPy\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An extra explaination from Reddit\n",
    "# # Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "# du = 0.001\n",
    "# u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "# u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# # print (u_new-u_old) / du\n",
    "# print(u_old, u_new)\n",
    "# # Now I see your problem: \n",
    "# #     You do not consider the effect of the weights. \n",
    "# #     From one layer to the next, we have two influences: \n",
    "# #         (1) multiplication with weights and \n",
    "# #         (2) applying the SELU. \n",
    "# #         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "# #         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "# #         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# # Oh yes, thats true, zero mean weights completely kill the mean. Thanks!\n",
    "\n",
    "# Tensorflow implementation\n",
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.176949977598 0.221005443215 0.835309367275 1.17453616899\n",
      "-0.189307534126 0.186617918943 0.788913119832 1.18362789521\n",
      "-0.196552755095 0.188090127733 0.770470014929 1.1616392021\n",
      "-0.215528683477 0.186350067193 0.820428935118 1.20518722095\n",
      "-0.230688823454 0.206173791012 0.799844892938 1.19280368729\n",
      "-0.163523722388 0.183048120893 0.776775004742 1.24549998477\n",
      "-0.144053215751 0.211896999119 0.797482683642 1.20425914826\n",
      "-0.268366141978 0.332837237159 0.780291030843 1.23653341958\n",
      "-0.225042227486 0.211523443746 0.757500838493 1.18271662009\n",
      "-0.228838917957 0.252977585647 0.767139081137 1.20645786101\n",
      "-0.176143296647 0.196899486613 0.809853497338 1.18721244432\n",
      "-0.207192909889 0.187316043074 0.818630409606 1.2251001154\n",
      "-0.135618258806 0.129711697865 0.76846749851 1.17385787684\n",
      "-0.165881752824 0.245504872115 0.74691869659 1.18283066112\n",
      "-0.260200908429 0.294824287792 0.805082083259 1.1886161453\n",
      "-0.173702066776 0.205132958897 0.819200743815 1.17986138705\n",
      "-0.255709430006 0.240450176469 0.835864530401 1.22545372324\n",
      "-0.189921201788 0.16711296743 0.8371772791 1.21481935409\n",
      "-0.265811365017 0.273241648224 0.845880650841 1.28292732138\n",
      "-0.179955238989 0.240247019812 0.845258284439 1.32316377921\n",
      "-0.159193038483 0.171492334821 0.84673062216 1.304410432\n",
      "-0.181565002367 0.232572550442 0.849443808126 1.23941312625\n",
      "-0.202150435233 0.226724207368 0.821768944458 1.24019691675\n",
      "-0.156651776759 0.222984847521 0.816318139372 1.19261996462\n",
      "-0.175974842586 0.197731320332 0.806073171902 1.21460737308\n",
      "-0.160234581952 0.234645998021 0.798371240733 1.22286819813\n",
      "-0.143701029306 0.155592142209 0.795305263662 1.14988337043\n",
      "-0.253073727246 0.252116353665 0.843566553667 1.14974983674\n",
      "-0.16306634479 0.206728738644 0.820293427607 1.17866960444\n",
      "-0.162140485697 0.137241152393 0.832145140082 1.17461729568\n",
      "-0.297909753798 0.258333723622 0.815703946579 1.22817875767\n",
      "-0.193575827937 0.125976212962 0.802957025297 1.16150069245\n",
      "-0.152596189091 0.213728026916 0.819679139922 1.18129608935\n",
      "-0.148992206902 0.138503208726 0.84357117301 1.18241616985\n",
      "-0.155986192869 0.142206752022 0.816627309819 1.14505340099\n",
      "-0.182267893262 0.134440529704 0.81348520163 1.13799455857\n",
      "-0.172748171873 0.187799557973 0.802241627829 1.24503555692\n",
      "-0.291034274227 0.298418763911 0.829050952119 1.25569463789\n",
      "-0.171042719459 0.123582089172 0.794270606787 1.22232760135\n",
      "-0.155363240323 0.181914306064 0.792474285586 1.14427718464\n",
      "-0.174528962128 0.188357098335 0.796096322763 1.18693824511\n",
      "-0.259462192198 0.233728465827 0.814708638187 1.18528757894\n",
      "-0.157733015677 0.167220323531 0.812722742619 1.16343705262\n",
      "-0.201892579849 0.133588941971 0.848033638505 1.2573466079\n",
      "-0.203001038052 0.230575164588 0.818807349092 1.21139907509\n",
      "-0.192207030263 0.167473424589 0.820561426191 1.24331884893\n",
      "-0.217361441002 0.160856041438 0.841201005338 1.17382566171\n",
      "-0.223802264045 0.203106582108 0.813434217811 1.19236801864\n",
      "-0.194956451668 0.198192385494 0.806402596696 1.17172325653\n",
      "-0.240729461462 0.236435298037 0.749100982967 1.21889655438\n",
      "-0.19489581407 0.169197471577 0.756506037767 1.1857584376\n",
      "-0.152618613364 0.17309734802 0.815008467411 1.13225099551\n",
      "-0.187309548529 0.193697871011 0.818394006145 1.18201103308\n",
      "-0.102887782121 0.170794365596 0.807995935331 1.15741170795\n",
      "-0.188538093645 0.181368933106 0.796685253422 1.16978439164\n",
      "-0.196486442148 0.204422670563 0.81352101537 1.16781381111\n",
      "-0.18366018574 0.174463397903 0.817572792856 1.20347934517\n",
      "-0.243603222409 0.225511527215 0.797434267521 1.19175881382\n",
      "-0.154338021317 0.194194966649 0.830886138266 1.18187016335\n",
      "-0.173207876053 0.206741900655 0.809967619765 1.15445506813\n",
      "-0.274443849436 0.210824663017 0.794497472967 1.18864096077\n",
      "-0.160333765694 0.167243265336 0.793689151716 1.12154166226\n",
      "-0.188728193469 0.166847421732 0.801524003395 1.14834127612\n",
      "-0.183992174182 0.193600299967 0.821851028744 1.11543192544\n",
      "-0.174316354221 0.179105717856 0.837007513508 1.1606140732\n",
      "-0.150915280787 0.154413465026 0.783181553144 1.16573921842\n",
      "-0.21999243918 0.239903606735 0.81119292668 1.1652262018\n",
      "-0.224983638888 0.252925948969 0.845321445861 1.19453881985\n",
      "-0.130336388549 0.16577564687 0.810369636514 1.20627331625\n",
      "-0.180401651378 0.224626168158 0.8494267933 1.20388143446\n",
      "-0.166620510251 0.227036422249 0.853843807991 1.20560419472\n",
      "-0.223351467868 0.207728759689 0.839958919708 1.22516680821\n",
      "-0.232616072522 0.156246219412 0.818979994052 1.21307950951\n",
      "-0.169732984716 0.150985093488 0.801035696784 1.13281114982\n",
      "-0.219512203012 0.209911050787 0.808695337514 1.21578554672\n",
      "-0.204483088398 0.266352886913 0.827041685672 1.20689497338\n",
      "-0.1817591531 0.205977020941 0.805578609372 1.13611384305\n",
      "-0.155510982526 0.164103712564 0.751957441592 1.17181477151\n",
      "-0.231224714131 0.211252749097 0.802181490069 1.17318344073\n",
      "-0.176358799899 0.22391757856 0.8023827194 1.15112057639\n",
      "-0.220104230475 0.189097455553 0.802500442735 1.12601885361\n",
      "-0.146054411285 0.142584625033 0.789554276923 1.21390394023\n",
      "-0.232376921396 0.215348328253 0.770332864549 1.19629326102\n",
      "-0.204737515157 0.172044100034 0.82940813046 1.17269429718\n",
      "-0.260479978395 0.177007936206 0.807924121714 1.16953805824\n",
      "-0.193768354129 0.150019304623 0.823923734973 1.19024121054\n",
      "-0.178113061909 0.149875227689 0.8039685098 1.18965368515\n",
      "-0.208662815232 0.211036649316 0.77397256617 1.17206332555\n",
      "-0.136012951358 0.11646833515 0.788921104097 1.27090918141\n",
      "-0.252624089612 0.169545950912 0.78933726937 1.21457254779\n",
      "-0.154152684576 0.127985135242 0.785024562204 1.22474895471\n",
      "-0.119675765064 0.128317205502 0.803339515434 1.15131889079\n",
      "-0.153250247344 0.2069710844 0.777663216879 1.13773394967\n",
      "-0.222268097361 0.18980861468 0.783131592129 1.2298538972\n",
      "-0.167912797785 0.208818803263 0.764557791769 1.20374078227\n",
      "-0.174179277504 0.160532700013 0.814487326531 1.23843769393\n",
      "-0.230812145911 0.201039686736 0.800362034157 1.16755121444\n",
      "-0.159098945775 0.146704836267 0.811704849456 1.20520346974\n",
      "-0.172913392555 0.13037865638 0.813372900156 1.16889200397\n",
      "-0.185361082032 0.140521761237 0.822915239039 1.21970844576\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My NumPy implemetation of Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.27865085009 0.235032226285 0.915129009868 1.29260130653\n",
      "-0.258492571053 0.255327206083 1.00847446396 1.47345938655\n",
      "-0.19713231233 0.266209352556 1.07729960899 1.57705321955\n",
      "-0.193528130951 0.416940693557 1.08031991368 1.68861633923\n",
      "-0.208472944343 0.40818777893 1.12004958261 1.90190355558\n",
      "-0.164595641533 0.460386628564 1.15461609611 1.93369600081\n",
      "-0.230747215363 0.449316824305 1.20421394382 1.95141843399\n",
      "-0.117658759369 0.422397120352 1.20351492159 2.03517261375\n",
      "-0.200750029963 0.562718572385 1.1518401281 2.05301847911\n",
      "-0.22007430707 0.507823249652 1.26724171273 2.10539419892\n",
      "-0.195732915719 0.445585916195 1.28040711766 2.06638456713\n",
      "-0.130107941004 0.442719333733 1.24650609578 2.19915347757\n",
      "-0.12597937045 0.505710491189 1.29548735155 2.1220740626\n",
      "-0.136519137039 0.468136268834 1.26438235906 2.11297345854\n",
      "-0.305066942372 0.542040320604 1.27697375089 2.13053647135\n",
      "-0.16686018046 0.698204815239 1.231065446 2.40950914819\n",
      "-0.190440300193 0.490100777547 1.26205624514 2.36696269247\n",
      "-0.232908475906 0.545598159576 1.27606628684 2.31009568635\n",
      "-0.142320284016 0.557599435713 1.29116255349 2.39997175845\n",
      "-0.251819660894 0.506222286408 1.23303876152 2.24397471166\n",
      "-0.180959891633 0.497508407468 1.25051960468 2.30254079698\n",
      "-0.166825769591 0.520408131787 1.24895393426 2.09566705414\n",
      "-0.167897578141 0.535798309107 1.32064118659 2.15519617653\n",
      "-0.161087264674 0.602586932897 1.28889977834 2.08420727191\n",
      "-0.199167718812 0.5342749998 1.29037757181 2.2769772054\n",
      "-0.0784566351662 0.549485253449 1.32840103748 2.34460685031\n",
      "-0.170781651114 0.579887800466 1.27598140467 2.35975825228\n",
      "-0.117384321118 0.547096454608 1.23848465772 2.25530167076\n",
      "-0.220299205645 0.540838427023 1.24357944838 2.19205433504\n",
      "-0.190352526723 0.538866196281 1.29035135294 2.1545990214\n",
      "-0.182453381164 0.416247197403 1.27994119425 2.15819567736\n",
      "-0.175253312047 0.617732887449 1.33136515564 2.11575221611\n",
      "-0.160139594294 0.686946331516 1.2921104791 2.17792367881\n",
      "-0.272021101257 0.469444794217 1.21885377808 2.07479985044\n",
      "-0.197795625321 0.529103236427 1.16096609047 2.03060319656\n",
      "-0.181370372618 0.5366907369 1.21032487583 2.09876428422\n",
      "-0.171495502108 0.392484366883 1.14673704283 2.06320100315\n",
      "-0.159967602793 0.606236797315 1.13369032378 2.14769411998\n",
      "-0.153632202157 0.605356358471 1.14424390238 2.14768669843\n",
      "-0.162638003261 0.598853638607 1.2351792995 2.17621157595\n",
      "-0.173099436766 0.439904962649 1.23730641887 2.07599595777\n",
      "-0.143612674837 0.610022271585 1.25342516425 2.24748940374\n",
      "-0.272409822419 0.606778001058 1.19830198237 2.18343564042\n",
      "-0.18711484598 0.544534783555 1.21902396387 2.13800515984\n",
      "-0.114363455089 0.607031879031 1.21381730203 2.0835856168\n",
      "-0.196970980563 0.498598797213 1.20525258813 2.20312877682\n",
      "-0.131945544492 0.584179193779 1.18698517921 2.21783896843\n",
      "-0.171897596156 0.591371271759 1.20104625681 2.22020177202\n",
      "-0.225031358775 0.688048451722 1.29966291899 2.15857760957\n",
      "-0.286593645748 0.495723258505 1.24948039226 2.20819260372\n",
      "-0.188644449471 0.618098342893 1.28486672336 2.18348173148\n",
      "-0.162132820939 0.57215700682 1.31750267937 2.23301885901\n",
      "-0.167984487016 0.685734994413 1.2629206765 2.43982650607\n",
      "-0.153719674735 0.788159565549 1.14365585806 2.21186017721\n",
      "-0.222283106282 0.597503719741 1.1549235585 2.12469409415\n",
      "-0.277247807733 0.512577766946 1.22772068154 2.09112818981\n",
      "-0.177452043287 0.613805603811 1.30403095868 2.19607140403\n",
      "-0.181905548688 0.702126964879 1.21570158687 2.29855305907\n",
      "-0.139265161508 0.574750664668 1.27939113237 2.31060102502\n",
      "-0.134320360517 0.45532039172 1.2837984879 2.25417623362\n",
      "-0.169572942243 0.651091220393 1.20093498261 2.24092241527\n",
      "-0.138697442116 0.50621309488 1.1980020717 2.31143411128\n",
      "-0.0951880514798 0.689428990026 1.30293710615 2.43818297169\n",
      "-0.244856838375 0.528790306247 1.29366831515 2.41037546303\n",
      "-0.0653644462237 0.636270845071 1.27781661706 2.29513474123\n",
      "-0.258073075178 0.589269343583 1.21909374133 2.15329344101\n",
      "-0.254281183913 0.579732129173 1.28342587033 2.19516071856\n",
      "-0.215431410252 0.506440518275 1.16479593642 2.08470760758\n",
      "-0.164221896066 0.449960894465 1.20919746858 2.13600941898\n",
      "-0.156142948071 0.541091326863 1.23977367167 2.14843091023\n",
      "-0.21114508537 0.721246606674 1.21234332356 2.23087569496\n",
      "-0.189084345295 0.616402720152 1.24905652655 2.2705168306\n",
      "-0.190837193283 0.478774311405 1.23254458176 2.19589643951\n",
      "-0.149421561976 0.55725824924 1.2874596184 2.17741948739\n",
      "-0.162616085802 0.518553298277 1.22360939102 2.26174003845\n",
      "-0.118386878519 0.558074659616 1.28099090196 2.30695108781\n",
      "-0.159053615791 0.560562243842 1.24354924575 2.10335967511\n",
      "-0.214257635198 0.515500166858 1.25450958796 2.17715777791\n",
      "-0.153734839128 0.635712081035 1.33148243355 2.14171954906\n",
      "-0.180831374657 0.560248901915 1.27270379594 2.17549610393\n",
      "-0.160607369182 0.534794919293 1.30420153361 2.32470404123\n",
      "-0.249413502117 0.488889918636 1.20815377372 2.14682032816\n",
      "-0.199199104629 0.534712464491 1.2533812153 2.17160331706\n",
      "-0.0861523041619 0.574599458475 1.24050080552 2.27212968325\n",
      "-0.168091284732 0.384807968309 1.22485801442 2.40198421116\n",
      "-0.187199193308 0.484922418835 1.25012703156 2.15890390805\n",
      "-0.160600081217 0.586868141045 1.23401812288 2.10090234046\n",
      "-0.159862424288 0.507689235379 1.27720723574 2.08512634187\n",
      "-0.142668562769 0.527262881959 1.23513693231 2.08634804416\n",
      "-0.168871655864 0.501183493438 1.23234271796 2.10437735123\n",
      "-0.133620164158 0.571817457407 1.31113523523 2.1344002488\n",
      "-0.205094232225 0.541704833187 1.23033223403 2.24488237899\n",
      "-0.177998231541 0.591702569037 1.23565363368 2.12982147026\n",
      "-0.140035580463 0.463842811636 1.25418089707 2.30350298158\n",
      "-0.206329321199 0.552999490491 1.31589576847 2.41715610724\n",
      "-0.167209970028 0.64781085176 1.3098431497 2.50283646394\n",
      "-0.123903894702 0.617378684647 1.24091916778 2.4701589547\n",
      "-0.0891447568435 0.697788791713 1.31410306414 2.61561152898\n",
      "-0.178298210677 0.519059822033 1.28920380718 2.41425613341\n",
      "-0.188623141612 0.719423014452 1.27306419037 2.44785564412\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow implementation on github\n",
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    alpha = 1.0\n",
    "    scale = 1.0\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def elu_bwd(dout, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dout = dout * scale\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0267840597416 0.34870387555 0.688048836569 1.04717887996\n",
      "0.0118506955263 0.304876451832 0.56287559608 0.883068359607\n",
      "-0.00848144962242 0.206902931306 0.461438275839 0.773605957189\n",
      "-0.0135542733853 0.188593823674 0.382063563973 0.688186843237\n",
      "-0.0561000469178 0.122238741339 0.349825172164 0.602874984924\n",
      "-0.0343566384099 0.18028897461 0.31253884688 0.548510582005\n",
      "-0.0429958073188 0.116054084819 0.258804537482 0.468348512243\n",
      "-0.0365803448673 0.0872113156711 0.24577948731 0.438054082984\n",
      "-0.0599072152755 0.107624519345 0.215066658884 0.422357677398\n",
      "-0.0350411695585 0.0776162947341 0.205136251217 0.425153568286\n",
      "-0.0283457218838 0.0652535851403 0.185845104457 0.385896334075\n",
      "-0.0272042989458 0.0766230668132 0.17945437564 0.343158539221\n",
      "-0.0265363719492 0.0529204998045 0.175905477592 0.337747682314\n",
      "-0.0195131277272 0.050982043472 0.157488026364 0.320393452692\n",
      "-0.0279429986826 0.0494597719809 0.148463686617 0.302291903009\n",
      "-0.0163050932564 0.0395091568666 0.134573867209 0.275776177388\n",
      "-0.0350636247315 0.0446047271227 0.130681932638 0.260456518769\n",
      "-0.0280860610856 0.0521212852515 0.127825010424 0.242927405648\n",
      "-0.031611045363 0.0448226140599 0.119061552083 0.247784549674\n",
      "-0.0160116482817 0.0350665171291 0.110955975843 0.230000436935\n",
      "-0.032908915089 0.0455961663955 0.106944810848 0.221535694601\n",
      "-0.0255515030656 0.0533079790241 0.101583851262 0.218571710404\n",
      "-0.0287315200628 0.0505785025543 0.100373477838 0.211213550803\n",
      "-0.0218860512348 0.0395445699361 0.09912119378 0.216332757744\n",
      "-0.0155101553201 0.0300637152255 0.0940974816116 0.217682172512\n",
      "-0.0278378861071 0.0382129355701 0.0871000120494 0.213244116615\n",
      "-0.0166008582174 0.0282495426849 0.0856275861282 0.201773089435\n",
      "-0.0241697863 0.0411436133239 0.0820385581023 0.206187616881\n",
      "-0.0157925180858 0.0296287485956 0.0816271592875 0.202022137338\n",
      "-0.0131223551871 0.0344921217048 0.0810812233719 0.189484668356\n",
      "-0.0178672551798 0.0196992522479 0.0769284293791 0.190846401789\n",
      "-0.015907970919 0.0340555629548 0.0739359328161 0.195642685556\n",
      "-0.0167867545129 0.028162573988 0.0688426757733 0.185973613894\n",
      "-0.0198006002858 0.0291724924825 0.0722614785159 0.170296669289\n",
      "-0.0223565323619 0.0271525026234 0.0676686938757 0.165745072873\n",
      "-0.0129344926851 0.0326925108362 0.0674981892731 0.160179767947\n",
      "-0.020047396402 0.0244167398387 0.0696481896366 0.154583522557\n",
      "-0.0354766110678 0.0286733638038 0.0646655802468 0.151386817687\n",
      "-0.0166541157601 0.019886945775 0.0650172585953 0.15715925585\n",
      "-0.0169269825833 0.0263314959328 0.0612392050587 0.148117722604\n",
      "-0.0208108792175 0.0337166211252 0.0567820375857 0.157970273241\n",
      "-0.0175155554353 0.0194623862136 0.050853477726 0.148709289169\n",
      "-0.0163136072795 0.0194587848715 0.0528851158249 0.154573862383\n",
      "-0.0224563452682 0.0330225583336 0.054727681792 0.155185488264\n",
      "-0.0148554864396 0.0232484025477 0.0526528855982 0.155948090419\n",
      "-0.015407728749 0.0226526584702 0.0503735663922 0.161205399028\n",
      "-0.0198421090144 0.0295908023975 0.0545698799751 0.166946872485\n",
      "-0.0101000184353 0.0164851423265 0.0544275861449 0.17017293538\n",
      "-0.0183716129586 0.0214069727876 0.0521637561451 0.1592011914\n",
      "-0.0164162385149 0.0236446085742 0.0512908036539 0.165370371683\n",
      "-0.0124907074268 0.0173635905341 0.0468553304154 0.155802368363\n",
      "-0.0264065451783 0.0237436173671 0.0495317528297 0.144526511002\n",
      "-0.0170799485706 0.0211716388198 0.0497479568515 0.146070335122\n",
      "-0.0126531275369 0.0214300433465 0.0492852147178 0.144859578639\n",
      "-0.0096736574645 0.017352726812 0.0518772176472 0.134865122738\n",
      "-0.0161867091492 0.0147488788412 0.0506882555431 0.132496135044\n",
      "-0.0214809800781 0.0205734417392 0.0512116767513 0.13908018094\n",
      "-0.016115397592 0.025904050316 0.046993164924 0.129877520715\n",
      "-0.0168817140433 0.0189450076508 0.0447395484776 0.127975415041\n",
      "-0.0126701066329 0.0180751524404 0.0417024671629 0.125152155025\n",
      "-0.0144218596113 0.0178510686842 0.0409738874511 0.129313206402\n",
      "-0.0103570746363 0.0142001515875 0.0416981939267 0.129968687474\n",
      "-0.0123291748387 0.0162648937274 0.0406117482539 0.124972189775\n",
      "-0.015346694443 0.0252470819536 0.0382869204296 0.133190426772\n",
      "-0.0140690491473 0.0143299470095 0.0359749590823 0.12737939207\n",
      "-0.0140148315276 0.0145609375856 0.0360992892364 0.121716602723\n",
      "-0.0198422635265 0.0202262833543 0.0354968258464 0.130395775452\n",
      "-0.00848609147717 0.0157402132932 0.0363170194672 0.127288818346\n",
      "-0.0118144287768 0.0155386830612 0.0386537046769 0.127784844402\n",
      "-0.0125379898886 0.0172792918855 0.0376219436861 0.127249539625\n",
      "-0.0148385557007 0.0163772349032 0.0367424337849 0.129384211941\n",
      "-0.012194784855 0.0190643416517 0.0338767916465 0.1170230661\n",
      "-0.0150463114264 0.0153197907742 0.0352578702687 0.117922457118\n",
      "-0.0200746660961 0.0196019318234 0.0340021113441 0.117798748676\n",
      "-0.00999526481601 0.0168783081618 0.0363771894882 0.114587608924\n",
      "-0.0107125452314 0.0155062187427 0.0375831283136 0.11133651553\n",
      "-0.0149699987639 0.0196617516231 0.0361636506983 0.111645390051\n",
      "-0.0130906496703 0.0206873098424 0.0354054042945 0.113875946051\n",
      "-0.0157400081638 0.0198031247077 0.0343283491918 0.119985952594\n",
      "-0.0165416418283 0.0239340061497 0.0321229110038 0.121945496186\n",
      "-0.0181071892377 0.018167803194 0.0335861073213 0.126017344986\n",
      "-0.013953680237 0.0172885594547 0.0343894257796 0.125344627025\n",
      "-0.0186229709036 0.0207122486406 0.0353495796649 0.12250358027\n",
      "-0.00878233107791 0.0165026205361 0.0380215242646 0.129686818452\n",
      "-0.0134546068636 0.0146227046556 0.0365660470591 0.129589844885\n",
      "-0.0111243095259 0.0140677223193 0.0372159083678 0.129993633691\n",
      "-0.0150002017929 0.0234591629626 0.0388439076028 0.129830324013\n",
      "-0.012063932092 0.0165148835715 0.0383054163978 0.124212667474\n",
      "-0.0104740181877 0.0221637545266 0.036589893577 0.12121861639\n",
      "-0.0117376051638 0.0182647042151 0.0354353912801 0.123536789701\n",
      "-0.0128660240953 0.0164226391203 0.0356750709574 0.13046883613\n",
      "-0.0104825693764 0.0208843272362 0.0349852664605 0.122620847611\n",
      "-0.00977902744701 0.0186812053538 0.0324718208798 0.12684655601\n",
      "-0.0107864513671 0.0151510064738 0.0350752961548 0.12362225557\n",
      "-0.0159308759 0.0168429393574 0.0387256721004 0.125421593372\n",
      "-0.0146330148658 0.0147085934465 0.0390732010775 0.130182556005\n",
      "-0.0245935641368 0.0273270660344 0.0382575769433 0.130610395861\n",
      "-0.0128806106158 0.0190988912682 0.038211902248 0.12852023976\n",
      "-0.0120070545454 0.0149455463847 0.0392919992364 0.135765570327\n",
      "-0.0104428986723 0.0159663312185 0.0406177137163 0.133933685466\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, _ = elu_fwd(X=x)\n",
    "    x, _ = dropout_forward(p_dropout=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def selu_bwd(dout, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dout = dout * scale\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX\n",
    "\n",
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.152871155666 0.173813542308 0.820291785309 1.13981820486\n",
      "-0.18122122386 0.207630199258 0.815210431198 1.18937529405\n",
      "-0.297846578441 0.266203730506 0.817983597323 1.23752921224\n",
      "-0.161757570033 0.235202476314 0.831543157724 1.28683767092\n",
      "-0.142265800851 0.215884424058 0.878437825834 1.26805727295\n",
      "-0.165170971108 0.192902736105 0.830602021809 1.22491925504\n",
      "-0.214282420794 0.267225427297 0.829318420082 1.21366357317\n",
      "-0.185252100148 0.173920268415 0.779902394677 1.15248294176\n",
      "-0.153067144439 0.20891501543 0.826645437398 1.16220404025\n",
      "-0.144313463151 0.140307245998 0.816166417019 1.20294289235\n",
      "-0.162489773485 0.161401275126 0.788057464437 1.19460372201\n",
      "-0.177443613252 0.144282207372 0.811307580107 1.22868265658\n",
      "-0.204457829679 0.170138943013 0.804559821665 1.19601136669\n",
      "-0.245177009897 0.220070455351 0.82173445298 1.17316394068\n",
      "-0.180660981614 0.21015737802 0.798093122281 1.1647082751\n",
      "-0.229195097623 0.234943629284 0.806055624367 1.2206629188\n",
      "-0.142119011139 0.159630286891 0.791431772844 1.19119975047\n",
      "-0.19757546707 0.208469488023 0.804217386634 1.18403396457\n",
      "-0.163455452645 0.211711902797 0.818309232278 1.18931705627\n",
      "-0.213903639152 0.246276177609 0.825535571286 1.20175814289\n",
      "-0.176480904938 0.182446482903 0.853132650146 1.24630261563\n",
      "-0.164462755477 0.17574760763 0.828955264082 1.20248309079\n",
      "-0.16390219101 0.163721467781 0.826704077041 1.18248989539\n",
      "-0.155214836968 0.139723109505 0.82346537953 1.15579785019\n",
      "-0.196884564656 0.165546635514 0.827984697102 1.21961752574\n",
      "-0.219426046946 0.251345764181 0.820688496746 1.21844361069\n",
      "-0.227421782775 0.221934802021 0.839172973201 1.23577386868\n",
      "-0.208342578943 0.268311836444 0.820155251765 1.26280581861\n",
      "-0.166154165004 0.18258431149 0.813098390433 1.25212314306\n",
      "-0.132169213269 0.152159707744 0.825133724426 1.23220625361\n",
      "-0.18548311623 0.267359234381 0.809453404082 1.21157641839\n",
      "-0.172970412466 0.232460764289 0.828784245514 1.25993737558\n",
      "-0.161047349919 0.204639575965 0.822845932751 1.2662153207\n",
      "-0.147562006038 0.21482135628 0.815658255433 1.21905016871\n",
      "-0.185652154669 0.252931188987 0.83075666926 1.36897920114\n",
      "-0.232197091399 0.193914760477 0.818449096518 1.31822247931\n",
      "-0.191538055174 0.216432255747 0.787956317559 1.26385399467\n",
      "-0.259253252149 0.26365365049 0.837076645201 1.22697865668\n",
      "-0.150579155611 0.123156360891 0.815531399754 1.22719895728\n",
      "-0.233916905052 0.22233942359 0.842301455103 1.18043902639\n",
      "-0.154251566399 0.165217348806 0.836264662271 1.20717383885\n",
      "-0.217176778499 0.207616691295 0.838111046525 1.18569103358\n",
      "-0.246930619105 0.181142434555 0.812610954009 1.18309879875\n",
      "-0.193238836628 0.163678127517 0.826805482677 1.19378961642\n",
      "-0.178259621605 0.206496558111 0.786834342143 1.26631697792\n",
      "-0.225105938241 0.239691987725 0.815647624612 1.23607665711\n",
      "-0.238950553236 0.210980916303 0.789425220582 1.30622427899\n",
      "-0.176070194088 0.213162092718 0.780938928997 1.30815244895\n",
      "-0.229426151867 0.258924006611 0.788055360759 1.20417981981\n",
      "-0.140368334164 0.147826216566 0.817555321061 1.15195157874\n",
      "-0.144006351342 0.185664407579 0.760456669047 1.13756231067\n",
      "-0.15327908653 0.155176544415 0.778464084077 1.19266492238\n",
      "-0.176303682706 0.200057446055 0.799260379117 1.24276161372\n",
      "-0.207153034443 0.182553294639 0.778609449908 1.19545551424\n",
      "-0.142819868849 0.156147019611 0.811988400115 1.18983555043\n",
      "-0.206754528968 0.233754355655 0.82795045242 1.23595822397\n",
      "-0.191111122907 0.192575886812 0.877096785636 1.25952683772\n",
      "-0.147085930783 0.183937998458 0.851663102233 1.19644074382\n",
      "-0.146806198384 0.160299170455 0.816382400351 1.16280543847\n",
      "-0.181174056014 0.19290145115 0.823016879525 1.16052962061\n",
      "-0.165025444029 0.18785302917 0.82306867832 1.22370062093\n",
      "-0.135543035787 0.140316409313 0.834883930405 1.20649468532\n",
      "-0.153160602623 0.159154054201 0.777821380323 1.18207088865\n",
      "-0.244513344543 0.225482964453 0.783453413044 1.17103811844\n",
      "-0.215554241197 0.21191198033 0.801117037498 1.13344280484\n",
      "-0.210602120807 0.199550786788 0.835522750709 1.17641436417\n",
      "-0.148168829195 0.14469911076 0.821330102264 1.16728999695\n",
      "-0.15560293964 0.153610305085 0.855498212559 1.16733341053\n",
      "-0.167899103388 0.213821268763 0.844525263548 1.18952226631\n",
      "-0.127815842394 0.118626310259 0.812874832265 1.14829343809\n",
      "-0.155721503958 0.160738035169 0.801816406199 1.18199409403\n",
      "-0.154812410368 0.17347307689 0.798587749405 1.160735082\n",
      "-0.176657380963 0.130403814128 0.741694370366 1.19718258679\n",
      "-0.153693016031 0.165572396365 0.759952442241 1.21816332819\n",
      "-0.159094152083 0.194400732814 0.768571703579 1.21760256429\n",
      "-0.164615762704 0.192439559654 0.793207349157 1.19820483998\n",
      "-0.133779304592 0.15793831627 0.774688571602 1.15499360955\n",
      "-0.187684095992 0.232125876416 0.838284852244 1.16454244621\n",
      "-0.138785618211 0.126003160335 0.813592105786 1.23900503748\n",
      "-0.143765064423 0.143512778127 0.84861484968 1.21641684273\n",
      "-0.146714146098 0.141664482115 0.858766525985 1.18783804609\n",
      "-0.177727797395 0.22025160423 0.855630604211 1.1761378008\n",
      "-0.144117374254 0.155413587324 0.82506149129 1.17989980883\n",
      "-0.142075705286 0.16954271807 0.860086813922 1.15844024275\n",
      "-0.153128280652 0.166904408118 0.81080960545 1.16015188182\n",
      "-0.2144361149 0.353493446982 0.780223407886 1.17140194867\n",
      "-0.165693637064 0.145674920171 0.785189886589 1.20126181395\n",
      "-0.157673790317 0.208894367673 0.828121946919 1.23552298412\n",
      "-0.203732639063 0.217467668045 0.827171201297 1.23647417026\n",
      "-0.232416870648 0.196371171649 0.833378638465 1.25086219522\n",
      "-0.153528420561 0.137304962763 0.845058665134 1.26346157201\n",
      "-0.150626171961 0.235401397209 0.841369551976 1.20635998552\n",
      "-0.158511630954 0.251189811046 0.838140475999 1.27535161058\n",
      "-0.12109067267 0.194899147428 0.84037461251 1.27020528911\n",
      "-0.178729967829 0.285854568696 0.823542493367 1.21545669322\n",
      "-0.203011645617 0.202019024217 0.856648968145 1.26330222748\n",
      "-0.195446151427 0.214215657744 0.862953244466 1.24709470966\n",
      "-0.170160978559 0.236838012279 0.839331639655 1.2496511302\n",
      "-0.241448032419 0.196105771291 0.841456860848 1.23370461581\n",
      "-0.109570737874 0.148421704616 0.825450422203 1.17387408618\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "#     x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
