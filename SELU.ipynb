{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of SELUs & Dropout-SELUs in NumPy\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An extra explaination from Reddit\n",
    "# # Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "# du = 0.001\n",
    "# u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "# u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# # print (u_new-u_old) / du\n",
    "# print(u_old, u_new)\n",
    "# # Now I see your problem: \n",
    "# #     You do not consider the effect of the weights. \n",
    "# #     From one layer to the next, we have two influences: \n",
    "# #         (1) multiplication with weights and \n",
    "# #         (2) applying the SELU. \n",
    "# #         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "# #         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "# #         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# # Oh yes, thats true, zero mean weights completely kill the mean. Thanks!\n",
    "\n",
    "# Tensorflow implementation\n",
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.169643980483 0.19326595588 0.831494203398 1.17907301442\n",
      "-0.195905368533 0.187288445337 0.813347344359 1.16342946743\n",
      "-0.165612815069 0.256356293921 0.814312012191 1.19610052215\n",
      "-0.180493963366 0.21574595991 0.807861023787 1.18077772604\n",
      "-0.1572638478 0.157882898432 0.831537691138 1.19411239372\n",
      "-0.170677280441 0.238829762852 0.819557386879 1.18223208257\n",
      "-0.19756382144 0.223737118683 0.844352863237 1.1677981833\n",
      "-0.152764904203 0.182116135091 0.83884410518 1.20924303205\n",
      "-0.232590338728 0.187804600082 0.809210210667 1.18526226435\n",
      "-0.195643972324 0.171561714244 0.832400619265 1.18972806518\n",
      "-0.211125263462 0.207497349536 0.815235769375 1.14424488443\n",
      "-0.179760875045 0.194749334604 0.802945690182 1.15404773795\n",
      "-0.171757947632 0.20891807862 0.766485697323 1.18413733708\n",
      "-0.180197385493 0.190346664807 0.820735061854 1.18700959668\n",
      "-0.156624103796 0.20947264287 0.838359239266 1.16184159831\n",
      "-0.189810181753 0.170410272796 0.844955944581 1.1752121995\n",
      "-0.170784407303 0.194969660057 0.798726494868 1.21080619407\n",
      "-0.229433273004 0.188311768904 0.812384038662 1.19634879733\n",
      "-0.170143290306 0.183681382738 0.841782567699 1.21246703309\n",
      "-0.185537934285 0.197968690508 0.827643792032 1.16220206916\n",
      "-0.21466068919 0.230471910943 0.80013045306 1.20722652911\n",
      "-0.184583803432 0.209227531097 0.814070921844 1.19842451346\n",
      "-0.176399967427 0.151164916513 0.831679278005 1.20004145441\n",
      "-0.22914626246 0.202926257681 0.8391602867 1.18342510278\n",
      "-0.205732491582 0.153672240864 0.836456075726 1.16891617901\n",
      "-0.139799957714 0.176603973999 0.840069899555 1.13684757672\n",
      "-0.176376445624 0.177089995927 0.820001079792 1.16828928843\n",
      "-0.289666570437 0.195662775689 0.814991813662 1.16922584166\n",
      "-0.228301509497 0.189681205141 0.810783906765 1.14696792293\n",
      "-0.189985668128 0.164789333205 0.832339058952 1.14691044782\n",
      "-0.171062003534 0.139418762139 0.864516782066 1.17908994192\n",
      "-0.250202594047 0.218367140487 0.802051806497 1.18362662942\n",
      "-0.130841081292 0.133030494142 0.827091821323 1.15629290054\n",
      "-0.253319720004 0.205901884455 0.838200936183 1.17847937481\n",
      "-0.181561483518 0.245049321881 0.823173588361 1.16540257281\n",
      "-0.151596835659 0.171746326679 0.831944866672 1.18257583052\n",
      "-0.231588186649 0.189876497836 0.84117193612 1.23442125455\n",
      "-0.178315408993 0.176939730103 0.814420557414 1.19784918288\n",
      "-0.157580343787 0.174880067236 0.836907708261 1.14940464894\n",
      "-0.139989928968 0.14230029318 0.796171576524 1.13781646603\n",
      "-0.170955242448 0.18026752925 0.830217594481 1.20236413002\n",
      "-0.203211936683 0.170781927342 0.80975350691 1.166585371\n",
      "-0.207436970216 0.176050310058 0.835863337213 1.1360852345\n",
      "-0.159951457408 0.157868770188 0.834199377904 1.1609358018\n",
      "-0.167686321437 0.171102887805 0.829128095627 1.12832770178\n",
      "-0.229901559765 0.214609398224 0.819235035967 1.15537355668\n",
      "-0.179908216319 0.215058737174 0.778988336716 1.21102141039\n",
      "-0.182446615248 0.260111488279 0.840206479355 1.29151501154\n",
      "-0.178367000983 0.198953345341 0.844158978482 1.2586940336\n",
      "-0.235360240807 0.18739396758 0.802599239028 1.22303108774\n",
      "-0.209563887553 0.167287440262 0.767528613184 1.17595684802\n",
      "-0.160269649373 0.215672843712 0.766649702162 1.15268863444\n",
      "-0.176930373279 0.122299991709 0.802808991135 1.16519478476\n",
      "-0.144688268972 0.190022491188 0.79996002747 1.13318564847\n",
      "-0.165198115333 0.113703894968 0.811537848389 1.13366010685\n",
      "-0.174381280972 0.196048521643 0.824173269505 1.17956699596\n",
      "-0.208243214564 0.164532339116 0.791581702886 1.23053062358\n",
      "-0.200534830534 0.18068500472 0.823110249133 1.19364550529\n",
      "-0.239045579986 0.269771935307 0.789522964454 1.22721250144\n",
      "-0.169654479948 0.190370930736 0.795372307023 1.22096698109\n",
      "-0.160339378908 0.217528451715 0.800660507254 1.19165642575\n",
      "-0.202750838572 0.252700052718 0.766028595246 1.23370787939\n",
      "-0.157885512163 0.168806268001 0.793122106442 1.20558713043\n",
      "-0.145186352313 0.17687767101 0.798040916628 1.17996400517\n",
      "-0.215452396146 0.156434651182 0.781599834888 1.13353049006\n",
      "-0.175042823256 0.169652500397 0.801380729589 1.13603402005\n",
      "-0.181293086839 0.149740070511 0.809748221254 1.13763892813\n",
      "-0.170912820921 0.156615598944 0.803716603689 1.13253237151\n",
      "-0.220101232869 0.158982475597 0.834504627263 1.18021718834\n",
      "-0.207915873081 0.197855209047 0.81862646569 1.15061340341\n",
      "-0.183771560982 0.238558878819 0.795861663442 1.17988585825\n",
      "-0.138478624925 0.179071287173 0.821057795134 1.19632377526\n",
      "-0.189647914296 0.199408611791 0.865983846131 1.18649405543\n",
      "-0.192713409923 0.183686252863 0.851148526477 1.19033924581\n",
      "-0.181496910469 0.178962825215 0.811347670122 1.14564432255\n",
      "-0.145919357582 0.216413859638 0.853024016945 1.15324812633\n",
      "-0.168603012134 0.27455442477 0.854520982061 1.1910083011\n",
      "-0.202588001628 0.222337453363 0.795801983341 1.18398644304\n",
      "-0.190469192272 0.221405488554 0.80766146926 1.16447408577\n",
      "-0.145111343524 0.228779590789 0.809173410341 1.17437120674\n",
      "-0.207351634349 0.23661179686 0.852305483374 1.17793768554\n",
      "-0.180364519321 0.167945706003 0.818945115405 1.17311924035\n",
      "-0.189185567108 0.168376715316 0.761908969328 1.18385819874\n",
      "-0.173563720328 0.154356471009 0.781590446588 1.15960381767\n",
      "-0.127846938661 0.153889053919 0.791905630669 1.12806133221\n",
      "-0.223505619731 0.204914977749 0.80480960688 1.20248100095\n",
      "-0.168400473779 0.182701188287 0.81577098661 1.14452992945\n",
      "-0.156169875381 0.142733584499 0.813999066053 1.13301549482\n",
      "-0.190078347183 0.200983795264 0.819677331366 1.18589417097\n",
      "-0.16892972034 0.151688417292 0.807868559533 1.21492343458\n",
      "-0.165490291455 0.143289320007 0.815928192802 1.21301553773\n",
      "-0.167111282872 0.196506443238 0.835460479188 1.16960525097\n",
      "-0.192137766428 0.172058114439 0.84054618645 1.18195319666\n",
      "-0.129341188763 0.150748964973 0.83833887122 1.16844600256\n",
      "-0.149104107958 0.153842134112 0.812226484865 1.16450104679\n",
      "-0.190541308546 0.19063203494 0.834921106754 1.17161031655\n",
      "-0.164733067521 0.17419656116 0.810633535931 1.12537667047\n",
      "-0.1616666457 0.241653583929 0.792401403863 1.17675686941\n",
      "-0.195891386675 0.23698085265 0.769250267005 1.15097605321\n",
      "-0.167388274546 0.149083139983 0.798086971363 1.24668781665\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# My NumPy implemetation of Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.251072013541 0.209376587988 0.91172223785 1.33068686397\n",
      "-0.27614508784 0.288469922165 0.968164422361 1.47335482914\n",
      "-0.214378498633 0.302353397343 1.04634005926 1.70487722628\n",
      "-0.215165648961 0.350261002442 1.09541807431 1.69191290108\n",
      "-0.191359108966 0.410521963591 1.14341834698 1.8230372023\n",
      "-0.199306717222 0.528614010651 1.17955843745 1.81181994637\n",
      "-0.162545509646 0.537175309387 1.17090324424 1.91887732184\n",
      "-0.15792265875 0.417386601627 1.22381079532 1.90862400086\n",
      "-0.194858398318 0.471407967953 1.2094845386 1.95818537782\n",
      "-0.18064306425 0.47771441999 1.24270997738 2.03678454329\n",
      "-0.20996212582 0.58320725105 1.20960018578 2.07392270081\n",
      "-0.092738860604 0.646332186735 1.23685342565 2.13697041661\n",
      "-0.18380841256 0.572740835567 1.23920963383 2.15900680026\n",
      "-0.100584538208 0.501067627891 1.25326889614 2.10984398015\n",
      "-0.150609705106 0.482804318562 1.22356591219 2.17064366355\n",
      "-0.139344365796 0.625975347271 1.24359615778 2.03614583347\n",
      "-0.259280647441 0.67430472889 1.19352014915 2.22979972235\n",
      "-0.165625649134 0.559610721945 1.2302583373 2.16518643909\n",
      "-0.205054565148 0.533980396865 1.20568583455 2.15739424665\n",
      "-0.115355865934 0.553543782945 1.2817512174 2.08448728146\n",
      "-0.249237738947 0.494637869918 1.25438203358 2.03315525507\n",
      "-0.167468676138 0.506517633793 1.30892245952 2.08529471788\n",
      "-0.214841764126 0.580474628883 1.29896534896 2.22200668053\n",
      "-0.213719270806 0.638912092385 1.26282943588 2.11736834872\n",
      "-0.163446765684 0.524837486581 1.24854989137 2.13035580741\n",
      "-0.147710179244 0.511637446858 1.27889116368 2.21083252395\n",
      "-0.268869532434 0.461455959787 1.30567968111 2.25824488453\n",
      "-0.151502457185 0.575979921408 1.25573162805 2.21698380781\n",
      "-0.175687404272 0.485582399775 1.27167361366 2.32148194058\n",
      "-0.139768643798 0.560956207127 1.33209857413 2.38317807427\n",
      "-0.178748462133 0.504862424515 1.24942657765 2.28756589041\n",
      "-0.217586607012 0.624641649591 1.24124336587 2.2383442905\n",
      "-0.132324294171 0.552423311244 1.28703336448 2.26965080062\n",
      "-0.21448823113 0.559735217105 1.28772071451 2.45975172481\n",
      "-0.149210001179 0.479090262468 1.29866015031 2.20605827765\n",
      "-0.119276514607 0.480318829582 1.30573049701 2.15190315884\n",
      "-0.184360002918 0.440178456942 1.26094091215 2.09711563432\n",
      "-0.196583995115 0.531590413638 1.28719085925 2.07453564102\n",
      "-0.12951267593 0.604828969949 1.30368049927 2.15504558237\n",
      "-0.162035270297 0.560681647556 1.28072183135 2.14798527159\n",
      "-0.10424789428 0.504780535493 1.31903222221 2.14179519598\n",
      "-0.15736243448 0.529253360739 1.28216185011 2.16880078525\n",
      "-0.17206608444 0.528013380901 1.25665223932 2.1457313039\n",
      "-0.256529018297 0.622138279155 1.20894125984 2.07506435813\n",
      "-0.244864203023 0.46430540497 1.27656111004 2.16786489021\n",
      "-0.208791002002 0.551975229888 1.29436073866 2.22993177142\n",
      "-0.1921069115 0.543345819741 1.31395093596 2.28179116995\n",
      "-0.151934241669 0.494188064602 1.36765671171 2.40051495015\n",
      "-0.133572637463 0.636688713506 1.36301140759 2.19230994613\n",
      "-0.170465271 0.428231261858 1.23304217803 2.15487944253\n",
      "-0.12208192073 0.420461973048 1.27599158012 2.12131903057\n",
      "-0.19782016497 0.513218353592 1.25841828637 2.12193733206\n",
      "-0.203719419163 0.469608783871 1.29875410707 2.13442071965\n",
      "-0.225588403835 0.566200056956 1.30775914656 2.19537010158\n",
      "-0.162865693508 0.553422478114 1.24367898132 2.17984338088\n",
      "-0.195605768757 0.492937045753 1.18566636764 2.07544823617\n",
      "-0.214806656749 0.534681152194 1.26741988182 2.19695710295\n",
      "-0.150640697923 0.398148909518 1.21542393716 2.23980218573\n",
      "-0.146251918751 0.586077314337 1.27810882587 2.13438969764\n",
      "-0.124393320328 0.484418938626 1.25112258412 2.12293919338\n",
      "-0.138702406561 0.711196680897 1.29621718853 2.20649261264\n",
      "-0.162821300416 0.627377054215 1.29652505352 2.23109575923\n",
      "-0.119473380501 0.713456809192 1.2967049966 2.21463945904\n",
      "-0.177008483446 0.58524718979 1.26121270379 2.22107570463\n",
      "-0.141695917916 0.462410149904 1.29473028734 2.127312721\n",
      "-0.221110900921 0.6465212582 1.23526610337 2.22942470921\n",
      "-0.214347133479 0.530498495605 1.28872949078 2.14808567025\n",
      "-0.103317198244 0.57343200662 1.23432565077 2.22089345151\n",
      "-0.228432466822 0.426102375552 1.20770815853 2.11288082891\n",
      "-0.17772322724 0.540549609208 1.23557643922 2.14690877899\n",
      "-0.178221554057 0.448104836637 1.2113444941 2.10992447301\n",
      "-0.174575377772 0.479960227747 1.28058435467 2.29972974718\n",
      "-0.160434686766 0.512312094269 1.23516552638 2.28694174615\n",
      "-0.130194365981 0.53449684854 1.27358857733 2.43094538212\n",
      "-0.142486145419 0.8608632835 1.24556937616 2.50279067901\n",
      "-0.109520114227 0.631787490845 1.22845719348 2.50440369582\n",
      "-0.187107141811 0.612712493703 1.28269179533 2.3690841959\n",
      "-0.15323425929 0.631779737519 1.28012997642 2.27174553233\n",
      "-0.148063837244 0.642472317279 1.24505907633 2.45387264454\n",
      "-0.165207585831 0.704166506693 1.32892120709 2.47062997174\n",
      "-0.159150266876 0.528002367604 1.27949434701 2.27793246164\n",
      "-0.111828506208 0.486990452348 1.2795820008 2.2643629796\n",
      "-0.224088007393 0.500567250664 1.26962488667 2.1654988299\n",
      "-0.142956678688 0.503889521364 1.29148111223 2.171055431\n",
      "-0.178697825384 0.58337581447 1.24027390562 2.20938324833\n",
      "-0.17964596774 0.442819908475 1.30195514075 2.0813229148\n",
      "-0.16585177435 0.487524672213 1.38427310159 2.05322456676\n",
      "-0.20474870287 0.581004440377 1.36883241586 2.14432142662\n",
      "-0.206478007851 0.526745193814 1.28471782925 2.31373889975\n",
      "-0.141848669647 0.603547938704 1.23276292106 2.26150710881\n",
      "-0.149597292312 0.575391131599 1.24432399623 2.18376798203\n",
      "-0.136141953111 0.446158666316 1.20883301291 2.09222727363\n",
      "-0.127012385634 0.62907258469 1.21003955731 2.25852117825\n",
      "-0.205403346469 0.499151238883 1.26764978864 2.12426928492\n",
      "-0.17804033556 0.619508174269 1.27810938942 2.24442584478\n",
      "-0.123110876402 0.78822718275 1.27035676658 2.25718146484\n",
      "-0.173501608412 0.727927900464 1.22424764739 2.23653810555\n",
      "-0.177739660749 0.562946836962 1.22342177554 2.24477599215\n",
      "-0.184697129269 0.570228919648 1.30052474028 2.1832100745\n",
      "-0.111856058559 0.584360849802 1.30512430154 2.19431428521\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow implementation on github\n",
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    alpha = 1.0\n",
    "    scale = 1.0\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def elu_bwd(dout, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dout = dout * scale\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00617050592252 0.337673784589 0.646991767381 0.962698565859\n",
      "-0.0575137185421 0.255565877841 0.542622466441 0.854510964778\n",
      "-0.0230847296611 0.199563161172 0.445756221729 0.727346459383\n",
      "-0.016513908502 0.136585717373 0.388690840169 0.6546448826\n",
      "-0.0326313282382 0.153107850779 0.333549029594 0.572333028999\n",
      "-0.0422632935911 0.137046160828 0.307162464527 0.546718161965\n",
      "-0.0285822914336 0.12916218473 0.273611287149 0.515238746334\n",
      "-0.0478919735396 0.118872121176 0.245816897573 0.502284015533\n",
      "-0.0458842998415 0.092463329908 0.211156449503 0.47229103747\n",
      "-0.00918631706758 0.084913351153 0.199205721295 0.449707867937\n",
      "-0.0228903538636 0.0808967569073 0.186963659871 0.38590401974\n",
      "-0.0302475644433 0.0798865404621 0.178715249854 0.367585993646\n",
      "-0.0306024780895 0.0532624298075 0.162414124151 0.340216091126\n",
      "-0.0212268755842 0.0561615458685 0.159430684408 0.342516259688\n",
      "-0.0222513430424 0.0557010615243 0.147176968751 0.325957270058\n",
      "-0.0309229442399 0.0475138824285 0.145784383225 0.28792104075\n",
      "-0.0262123699349 0.0538427644509 0.136968668339 0.284259227431\n",
      "-0.0244733391531 0.0531369687821 0.131072974955 0.265905332092\n",
      "-0.028812417037 0.0566367709177 0.121408983308 0.273547838562\n",
      "-0.0360643423143 0.0314871206205 0.124830114479 0.247023406499\n",
      "-0.0248700868444 0.0398857966007 0.110534458999 0.246758893258\n",
      "-0.0271520402225 0.0341243627483 0.108852271372 0.231211406454\n",
      "-0.0174423437989 0.0382078987711 0.106593109657 0.226349970559\n",
      "-0.0271958529165 0.044689109455 0.104022107025 0.218689040258\n",
      "-0.0184373963271 0.0408406751117 0.101800708139 0.207727422336\n",
      "-0.0263965299635 0.0236734488633 0.0967895393634 0.193643124957\n",
      "-0.0153363156203 0.036584645295 0.0909428473282 0.195395331643\n",
      "-0.0186412046543 0.0282443197257 0.0891179366005 0.200459607775\n",
      "-0.0185462084779 0.0388848772896 0.0869120755493 0.200157060282\n",
      "-0.0199354992525 0.0292524191055 0.0773070246082 0.190640958001\n",
      "-0.0126308294128 0.0370657715758 0.0749400462166 0.1885902931\n",
      "-0.0161332687709 0.0333396014737 0.0721040308901 0.193651378683\n",
      "-0.0214905666908 0.0283190958382 0.0693105283754 0.179113975867\n",
      "-0.014281316249 0.0251368262697 0.0722952411391 0.178545322235\n",
      "-0.0240806681432 0.0390462169149 0.066561360205 0.181772329208\n",
      "-0.0121716149777 0.0296474415264 0.0703211887701 0.188454498015\n",
      "-0.0172330432774 0.0221160702798 0.0752372804221 0.189535254143\n",
      "-0.0149124874173 0.0206202566018 0.069123962964 0.173165016316\n",
      "-0.0131068637245 0.0286625095771 0.0627941581545 0.171975718196\n",
      "-0.0229007853719 0.0269035893546 0.0618716472152 0.175455130422\n",
      "-0.0308915599428 0.0254260874628 0.0616801172788 0.155719467985\n",
      "-0.0103458220932 0.0253439749042 0.0618487713107 0.151368992141\n",
      "-0.0183694920644 0.0185244701136 0.0629998689624 0.150753360722\n",
      "-0.0232375589368 0.0238517023864 0.0637601893688 0.1533032158\n",
      "-0.0126900207504 0.0239357259882 0.0623520819867 0.158685129882\n",
      "-0.014749979771 0.0229039664082 0.0598063682146 0.148263213085\n",
      "-0.0194399912671 0.0282525172979 0.0541797261623 0.153146694667\n",
      "-0.0147919428899 0.0194806316987 0.0550578463148 0.148261356733\n",
      "-0.0143791490413 0.022625252387 0.0545651552362 0.151169596666\n",
      "-0.0160432824313 0.0164007759261 0.0568124615264 0.145241949492\n",
      "-0.027860688713 0.0287283896644 0.0523913214822 0.147486634347\n",
      "-0.015554788881 0.0187704511463 0.0561528687803 0.144438083311\n",
      "-0.0224436589979 0.0360109141722 0.0531063138123 0.151362549581\n",
      "-0.013211580099 0.01916327409 0.0518881736975 0.139912523632\n",
      "-0.0107857109654 0.023123224181 0.0513227640734 0.138076782221\n",
      "-0.0163831322828 0.0232939645255 0.0471075874216 0.132233135181\n",
      "-0.0166718739455 0.0171547700856 0.0471971530347 0.133294482437\n",
      "-0.0140554738927 0.0202008529131 0.0445391100681 0.135183525366\n",
      "-0.0120934402949 0.0178923199302 0.0406854593222 0.138385544654\n",
      "-0.0203732438295 0.0205848321912 0.0412841167022 0.139898088082\n",
      "-0.0124718414229 0.0163484304298 0.0413490119646 0.136963610754\n",
      "-0.0105049763353 0.0173180141646 0.0369254123855 0.136379166791\n",
      "-0.0199407055027 0.0344793353718 0.0359806839683 0.144334404439\n",
      "-0.0132958358437 0.024659442981 0.0388106500863 0.137372043661\n",
      "-0.0164368264618 0.0180478405562 0.0405041263109 0.140707054809\n",
      "-0.0109120100633 0.0135915156005 0.0363046369201 0.13431323311\n",
      "-0.0100936482034 0.025698910105 0.0361485775715 0.138778479608\n",
      "-0.0150017132557 0.0170062778004 0.0365295952645 0.136474867265\n",
      "-0.0176711622756 0.0203097428633 0.0361799083707 0.134695437075\n",
      "-0.0119417558396 0.0199227609078 0.0374208968028 0.126479939757\n",
      "-0.0173261471129 0.0151252249812 0.0380446418199 0.129281219653\n",
      "-0.0120973891574 0.0185984105024 0.037852123882 0.133606475051\n",
      "-0.0143303069718 0.0178764382712 0.040472854276 0.126116780615\n",
      "-0.0137617342109 0.0130659215971 0.0396416517785 0.12856417635\n",
      "-0.0161311027951 0.0227029187012 0.036440474791 0.120986837803\n",
      "-0.0138344909643 0.0201960274818 0.0340029970849 0.119081020302\n",
      "-0.0109527594712 0.0218813565739 0.0349369574886 0.12122419591\n",
      "-0.0138643687329 0.018684631798 0.0347352637241 0.117912528715\n",
      "-0.01273654259 0.0169610358314 0.0344745357501 0.112698778753\n",
      "-0.0141948802797 0.0158686491763 0.0324930673938 0.112237074796\n",
      "-0.0209727324627 0.0162088153054 0.0316716629352 0.115576682358\n",
      "-0.0138282634444 0.0198755636582 0.0306411500096 0.121581746235\n",
      "-0.0142680794709 0.0163718387597 0.0294994643179 0.118340426758\n",
      "-0.00928649425116 0.011239371007 0.0313944538956 0.111990749348\n",
      "-0.00712630357725 0.0103529046094 0.0299569355145 0.111547804424\n",
      "-0.0109983061406 0.0161970204349 0.0302377349073 0.111722941022\n",
      "-0.0184235001506 0.0148238961093 0.0295954108029 0.113815556189\n",
      "-0.021653688478 0.0286437653168 0.0276226021731 0.118629159178\n",
      "-0.0160391578064 0.014496369328 0.0285837774318 0.112487129402\n",
      "-0.0111563115643 0.0172275826089 0.028790479011 0.11785790482\n",
      "-0.0113923510058 0.016689789706 0.0278352687237 0.115441109318\n",
      "-0.0134201064377 0.0242469004503 0.0300286713479 0.120549020214\n",
      "-0.0106315863005 0.0143642023817 0.031699849785 0.11944696646\n",
      "-0.016497583813 0.0211393747422 0.0305781496301 0.115998257016\n",
      "-0.0118553091828 0.0143627767788 0.0306233044723 0.11487632668\n",
      "-0.0176932425252 0.0214022893948 0.0314711493309 0.121853588979\n",
      "-0.00832904379256 0.0168636044584 0.0307455694735 0.110393201345\n",
      "-0.0130178847663 0.0195190645722 0.0324854384818 0.107940170995\n",
      "-0.0142593100602 0.0142008006658 0.0309258096142 0.11041405199\n",
      "-0.0153943069812 0.0173202061635 0.0290092214708 0.112171677784\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, _ = elu_fwd(X=x)\n",
    "    x, _ = dropout_forward(p_dropout=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def selu_bwd(dout, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dout = dout * scale\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX\n",
    "\n",
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.128076676934 0.345373998275 0.896616335816 1.23852215989\n",
      "-0.131370484455 0.317562586576 0.88544207764 1.36043184665\n",
      "-0.149992044707 0.301410324278 0.93351947205 1.29715189353\n",
      "-0.113140599252 0.336050568982 0.888520950881 1.34689270297\n",
      "-0.121992763956 0.329679584586 0.919620670954 1.36270007557\n",
      "-0.121133497655 0.314879313582 0.964087664337 1.34620887674\n",
      "-0.0847146343084 0.365416689223 0.97900041256 1.37083523972\n",
      "-0.0915272851586 0.300225203273 0.972339324631 1.37597180954\n",
      "-0.104276864299 0.357964795285 0.991599333995 1.3957291022\n",
      "-0.0943609404869 0.310337867312 0.993450035038 1.37346641576\n",
      "-0.0876511594656 0.376730864096 0.985113284654 1.45074599094\n",
      "-0.0853973489834 0.335766057655 1.00027288442 1.4329469673\n",
      "-0.0738033660733 0.373493385626 1.00256920071 1.45065317169\n",
      "-0.0975552843196 0.435007371725 0.970736819116 1.4555096332\n",
      "-0.110054128154 0.404458233626 0.967042990893 1.43269307518\n",
      "-0.112845667359 0.39379534816 0.997164004504 1.47155570554\n",
      "-0.110709560509 0.319015684186 0.937776501339 1.41327284741\n",
      "-0.130205149926 0.417144303975 0.96243052696 1.41475028864\n",
      "-0.121307224009 0.323396205508 0.98143942215 1.44635421693\n",
      "-0.128827534521 0.360053841381 1.03585371195 1.44188514451\n",
      "-0.0720569508813 0.359754517611 0.987931614597 1.38381519806\n",
      "-0.118239635387 0.355419564025 0.983009347232 1.37581588601\n",
      "-0.136826448075 0.3474630085 0.955658159124 1.39717565997\n",
      "-0.157478394591 0.374800767677 0.981724536047 1.41690724499\n",
      "-0.0879908091167 0.353120870849 1.00721447174 1.37091230062\n",
      "-0.064062185865 0.329594547866 0.979708981209 1.43910957766\n",
      "-0.126484419045 0.422966684083 0.969287497311 1.41430407918\n",
      "-0.122235778893 0.325732543217 0.961920349504 1.40226574491\n",
      "-0.13953391686 0.336107620788 0.972783358515 1.38473501953\n",
      "-0.0468811019387 0.355299374627 0.996504268258 1.40103470881\n",
      "-0.049221919626 0.3315014621 0.947224718967 1.40137320431\n",
      "-0.115667921485 0.341850217473 1.02573823419 1.40547052914\n",
      "-0.151864881096 0.40066889982 0.978547704083 1.44949126813\n",
      "-0.0953806391345 0.314779754026 0.960726828822 1.3953749383\n",
      "-0.088484539445 0.394434431373 0.937648666939 1.37846740449\n",
      "-0.0837855610736 0.31294049822 0.982847125534 1.46193718523\n",
      "-0.126542505204 0.319244933231 0.990878141233 1.40570108304\n",
      "-0.0741130521487 0.346449727432 1.01561247546 1.39072232425\n",
      "-0.14579175185 0.389579874933 1.00925447464 1.40211978339\n",
      "-0.122156253228 0.390124002132 1.00981574509 1.39581340026\n",
      "-0.159648692612 0.401076183092 1.01920346114 1.51138616484\n",
      "-0.0453082462621 0.396630991492 0.98948582055 1.42641589241\n",
      "-0.180855776909 0.414242899124 0.992254989842 1.39976836648\n",
      "-0.0962095241396 0.388007473565 0.973728610342 1.46756377935\n",
      "-0.120349518935 0.398618889776 0.980012155067 1.44795988037\n",
      "-0.127058475239 0.39041656072 0.951521600725 1.55316175799\n",
      "-0.0971799533481 0.399208298489 0.991557832916 1.48929780282\n",
      "-0.115725656357 0.291933807335 1.00367775143 1.41826033148\n",
      "-0.0882269265582 0.334054351692 1.01047533389 1.39761101371\n",
      "-0.152884024695 0.373348561821 0.948233597603 1.37601093075\n",
      "-0.137012171479 0.374397247892 0.948313179546 1.44010552688\n",
      "-0.0713512196013 0.382511716854 0.989334147583 1.45671976464\n",
      "-0.115815675179 0.372556317465 0.982326673772 1.49223029019\n",
      "-0.081669886505 0.421860758618 1.01058235903 1.44926521163\n",
      "-0.102964444408 0.318390493685 0.987840272775 1.41744852116\n",
      "-0.104074421092 0.360264655254 0.9804252297 1.42878261078\n",
      "-0.119248577366 0.362866133539 0.954399354886 1.40620723535\n",
      "-0.103259780901 0.387848497413 0.990062931425 1.40437156375\n",
      "-0.0921487194707 0.384693232149 1.01846224609 1.39623822049\n",
      "-0.0919269877777 0.418560702617 1.02267276154 1.50947293992\n",
      "-0.134951586176 0.379542387607 0.95889985029 1.37918162344\n",
      "-0.194183813408 0.510827912841 1.01113482614 1.4334983847\n",
      "-0.12713537431 0.484484991351 0.986883344598 1.4593464482\n",
      "-0.116292137852 0.36286941321 1.0088686859 1.48647127627\n",
      "-0.112873053848 0.483077631692 1.02268340951 1.43626750518\n",
      "-0.0981390908956 0.37429212826 1.01240577523 1.46231659538\n",
      "-0.226524978046 0.340426963779 1.02971273784 1.46548049273\n",
      "-0.0863640254986 0.417214500685 1.00812161689 1.42852905569\n",
      "-0.104286852406 0.353676024072 1.05569126766 1.41971410982\n",
      "-0.143318247608 0.365252934929 0.9925484206 1.52725930554\n",
      "-0.172123095447 0.347090185973 0.99217987709 1.47046267712\n",
      "-0.178770437185 0.343951544772 0.978911337512 1.4000620688\n",
      "-0.0888766414485 0.322890671328 1.00649996962 1.38860866155\n",
      "-0.0802599436538 0.367818088468 0.983009964818 1.40082523817\n",
      "-0.134114825822 0.383937639877 1.00646502678 1.41795080364\n",
      "-0.077960820923 0.339901998496 1.02189027942 1.41024395935\n",
      "-0.087876993129 0.355051772657 1.034312065 1.39072614511\n",
      "-0.164114834752 0.392431963139 0.98529188906 1.46969512091\n",
      "-0.0792525717825 0.368709823698 0.933701764219 1.48499738212\n",
      "-0.158860899229 0.371495986367 0.947548807454 1.48037380277\n",
      "-0.123523774311 0.377659010351 0.950531640384 1.46609186972\n",
      "-0.146763213903 0.300851465225 1.01794745643 1.52787423184\n",
      "-0.18436365452 0.445455556784 0.986451989932 1.49384170497\n",
      "-0.0753379307346 0.324305345227 1.00584364159 1.47183339418\n",
      "-0.139395406042 0.405147299347 0.99437246876 1.43520756515\n",
      "-0.109053784133 0.383247652656 0.98991455118 1.42204440604\n",
      "-0.0954716658951 0.376544372249 0.974734890459 1.45264694884\n",
      "-0.11342714101 0.430896252504 0.974731766171 1.42729567421\n",
      "-0.114672669965 0.400612336485 0.991814240384 1.43308128912\n",
      "-0.134894049675 0.32287107304 1.00413937995 1.39895594664\n",
      "-0.119492875362 0.353440342561 0.989330282009 1.37144843356\n",
      "-0.120688499898 0.352471365525 0.994094119518 1.38008384061\n",
      "-0.0989447258082 0.31661732979 1.00926827013 1.44867934255\n",
      "-0.0791245530407 0.352182295877 1.01386056447 1.40148817136\n",
      "-0.104075145743 0.285606131558 1.02384996953 1.4076975012\n",
      "-0.0882981103301 0.405121782173 0.999099960274 1.43406280666\n",
      "-0.135956643559 0.347717596516 0.980555245613 1.41708681342\n",
      "-0.102749846529 0.409829229553 0.97204750076 1.41836213568\n",
      "-0.0981864453444 0.319408154166 0.972310514043 1.4160438651\n",
      "-0.164483265156 0.398140268552 0.966240555864 1.44006725447\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "    x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
