{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of SELUs\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "#     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)    \n",
    "#     return scale * np.maximum(0.0, alpha*np.exp(x)-alpha)\n",
    "#     return scale * alpha * np.where(x>=0.0, x, np.exp(x)-1)\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.226280618112 0.157101420837 0.841948040203 1.14979915265\n",
      "-0.219531539876 0.179512100296 0.796927989605 1.15261565183\n",
      "-0.177780810798 0.229451335742 0.814764299485 1.16992643409\n",
      "-0.186910837203 0.231515171919 0.828413658951 1.20870017954\n",
      "-0.165802797055 0.171044685324 0.810327272733 1.19773988506\n",
      "-0.188259957078 0.267085156748 0.836992533739 1.18523145426\n",
      "-0.136451796353 0.162276851445 0.850360163947 1.17156291446\n",
      "-0.152443535955 0.136550321296 0.819058614518 1.17877895506\n",
      "-0.181833709974 0.203302979565 0.777200328484 1.16108528077\n",
      "-0.145778655301 0.168559282913 0.797024356625 1.14929334275\n",
      "-0.167205087382 0.215430582709 0.80950368535 1.15830895308\n",
      "-0.222207382542 0.174306363247 0.827623512143 1.22334920558\n",
      "-0.152153865457 0.14733509397 0.819734686091 1.18635229335\n",
      "-0.195824443205 0.211542206568 0.836499113581 1.22836095804\n",
      "-0.159772843349 0.131913469549 0.846632246149 1.19292338207\n",
      "-0.140903776704 0.169753840575 0.795836642973 1.16149099134\n",
      "-0.190229535259 0.233542304869 0.808053136353 1.16879223297\n",
      "-0.231434823757 0.200017045864 0.839768695333 1.17263519548\n",
      "-0.147293041495 0.161485190571 0.794844606171 1.12495377189\n",
      "-0.200098800431 0.185813680636 0.804272020217 1.15084038102\n",
      "-0.197032022599 0.284462362928 0.813317707208 1.18811333085\n",
      "-0.245305644451 0.312477081909 0.773616313889 1.22435571402\n",
      "-0.189682319188 0.16626435155 0.777831532781 1.21018485441\n",
      "-0.142941456045 0.136454852272 0.835246025767 1.24984020527\n",
      "-0.23136984163 0.148387836146 0.815258922431 1.22464732025\n",
      "-0.223405153716 0.233817526877 0.806493224779 1.19392900617\n",
      "-0.157330669272 0.181353998261 0.819911280953 1.22541205648\n",
      "-0.142769587259 0.199809849764 0.817474245379 1.25269159118\n",
      "-0.156072653961 0.145091823195 0.830690765405 1.27806225013\n",
      "-0.257955545228 0.267733858033 0.768488577175 1.13660582911\n",
      "-0.199473475301 0.15888077457 0.838791746403 1.15357178148\n",
      "-0.179753840661 0.195143880732 0.807399170331 1.16272911535\n",
      "-0.22670774659 0.18476881534 0.775854224573 1.13966520498\n",
      "-0.185977446853 0.189745848893 0.812223189086 1.15102912291\n",
      "-0.209158897102 0.229739988987 0.804953308158 1.13522817337\n",
      "-0.193150258451 0.239591232401 0.812040444202 1.17515716534\n",
      "-0.133672057415 0.169498078979 0.83870679537 1.21389278253\n",
      "-0.181502093034 0.208746852428 0.823755079278 1.17514116445\n",
      "-0.138288254828 0.173513958354 0.825276598217 1.14207474367\n",
      "-0.167547822679 0.183187839306 0.831529952334 1.11508247696\n",
      "-0.161848688571 0.160592157822 0.838772002505 1.17352451274\n",
      "-0.145008421642 0.156055054849 0.816150125449 1.20935506181\n",
      "-0.233015994602 0.195688303326 0.786941309299 1.24589328892\n",
      "-0.16700329911 0.15278195692 0.777224695124 1.17030841727\n",
      "-0.148054015614 0.16569673492 0.78772543865 1.1800017294\n",
      "-0.201285231916 0.191151547438 0.721642316101 1.17476677308\n",
      "-0.187522760041 0.239765234803 0.773779211382 1.22362487243\n",
      "-0.169118333793 0.222534931802 0.792867805828 1.18063294922\n",
      "-0.192704989214 0.212594713898 0.811546844907 1.19894673361\n",
      "-0.195048628544 0.136997169839 0.814874829626 1.17614126105\n",
      "-0.176062027656 0.159971858383 0.818490514717 1.14592150024\n",
      "-0.160758326989 0.214344007165 0.811659935465 1.27979297065\n",
      "-0.18331056793 0.167396431172 0.799358905649 1.20700447518\n",
      "-0.167075254417 0.187480702583 0.762977020142 1.25525551357\n",
      "-0.249909643129 0.202253287497 0.747859801418 1.26539317009\n",
      "-0.187090188196 0.216220455444 0.806613651032 1.27154853596\n",
      "-0.154354889221 0.160571286381 0.783364936515 1.23015232948\n",
      "-0.200945233378 0.183951415208 0.81172981919 1.19843302621\n",
      "-0.283928207739 0.285485318323 0.795641264302 1.20391521261\n",
      "-0.166636586066 0.171814043027 0.801459087277 1.14304755156\n",
      "-0.141396714349 0.152037336551 0.808204087991 1.20602869895\n",
      "-0.209957761823 0.280954621443 0.809834088474 1.1937257328\n",
      "-0.236285120707 0.19262132095 0.802230125845 1.21122688553\n",
      "-0.143484383695 0.169543149253 0.828630766581 1.26403729253\n",
      "-0.194514943517 0.217651504016 0.820057640939 1.23183330484\n",
      "-0.13259276223 0.198002628278 0.804670461619 1.28366421218\n",
      "-0.174600083337 0.168192364962 0.814156139533 1.29365900039\n",
      "-0.146427511179 0.19661821072 0.8427226444 1.21461886978\n",
      "-0.211251901474 0.233895519881 0.84414904004 1.1950714773\n",
      "-0.169084900751 0.159374167532 0.838739236038 1.16921329216\n",
      "-0.164115587152 0.189506390457 0.828888407816 1.17466994922\n",
      "-0.163307075924 0.137595571746 0.821425671809 1.20308033279\n",
      "-0.158924061806 0.189708429935 0.82184464687 1.19206192935\n",
      "-0.202624032125 0.145607474603 0.838405224661 1.17119804786\n",
      "-0.186652295433 0.209508077114 0.815269510558 1.17985926099\n",
      "-0.162120026932 0.170851819236 0.784004210386 1.19963784082\n",
      "-0.194001411594 0.16259379924 0.836164604512 1.25576753551\n",
      "-0.174668934357 0.142828901039 0.827893322399 1.25108288409\n",
      "-0.191104373602 0.187190460009 0.823500459302 1.20704390896\n",
      "-0.162772718463 0.156834244159 0.779980479533 1.20052687534\n",
      "-0.133029954975 0.14040557112 0.843823360795 1.19668229769\n",
      "-0.190991795675 0.123939741968 0.800126873369 1.13103920924\n",
      "-0.210745952613 0.221571176361 0.794993425455 1.11126824249\n",
      "-0.213997539727 0.140872696056 0.814438830425 1.15334707466\n",
      "-0.174034038646 0.155062278084 0.846808771947 1.2004985531\n",
      "-0.190643934876 0.156285373422 0.797545534419 1.18333915103\n",
      "-0.134197586265 0.173285795412 0.806602535712 1.18719159997\n",
      "-0.184925843059 0.214590520753 0.828114507216 1.15753360232\n",
      "-0.212112675825 0.238868002083 0.815087084586 1.14454147179\n",
      "-0.146580462746 0.146387079195 0.802624550398 1.16638760707\n",
      "-0.177932877601 0.167488067518 0.826995501684 1.23143202058\n",
      "-0.23691349075 0.212844894817 0.845747844894 1.17378453683\n",
      "-0.248202565605 0.203394358244 0.828408862111 1.20269234135\n",
      "-0.154017706065 0.249933948725 0.860035899465 1.19422877486\n",
      "-0.181036497565 0.193671065589 0.831496698705 1.2012961795\n",
      "-0.175748281294 0.153693446548 0.807309147849 1.19068326833\n",
      "-0.203969604257 0.196318349326 0.81610454117 1.19866524707\n",
      "-0.192743426784 0.187120126936 0.85234539726 1.2264871976\n",
      "-0.139695934992 0.143090472025 0.811603742758 1.19594718011\n",
      "-0.146843225134 0.169133337686 0.856107728043 1.22630990197\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "#     x = selu(np.dot(x, w))\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "#     m = np.mean(x, axis=1)\n",
    "    mean = x.mean(axis=1)\n",
    "#     s = np.std(x, axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "#     print(m.min(), m.max(), s.min(), s.max())\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())\n",
    "# mean.shape, scale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000237026032164 0.000949473650137\n"
     ]
    }
   ],
   "source": [
    "# Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "import numpy as np\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)\n",
    "\n",
    "du = 0.001\n",
    "u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# print (u_new-u_old) / du\n",
    "print(u_old, u_new)\n",
    "# Now I see your problem: \n",
    "#     You do not consider the effect of the weights. \n",
    "#     From one layer to the next, we have two influences: \n",
    "#         (1) multiplication with weights and \n",
    "#         (2) applying the SELU. \n",
    "#         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "#         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "#         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# Oh yes, thats true, zero mean weights completely kill the mean. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.202087716961 0.31848265518 0.921132446515 1.34398562218\n",
      "-0.224589444414 0.249901125861 0.979570395886 1.46352922135\n",
      "-0.231546855393 0.435013162508 1.04231740103 1.69483860123\n",
      "-0.220956906992 0.426276400395 1.10515576116 1.73862261421\n",
      "-0.150214064727 0.32243380693 1.15166428537 1.79592882927\n",
      "-0.196446409815 0.415526248206 1.11515227107 1.93494812263\n",
      "-0.249493613731 0.449090445479 1.15793425926 1.96928876524\n",
      "-0.172417897284 0.449128740856 1.20701340274 2.14773203716\n",
      "-0.207781721184 0.461443690264 1.21460613652 2.10236102509\n",
      "-0.165799063507 0.472724604454 1.25855086499 2.20787750648\n",
      "-0.165732280502 0.521963847454 1.26382439544 2.45992926742\n",
      "-0.134203731115 0.509548076499 1.20292069413 2.27706815833\n",
      "-0.128702694999 0.57964976766 1.15960113226 2.41472515484\n",
      "-0.162889987756 0.522153919153 1.27461349376 2.23938190747\n",
      "-0.184802914837 0.576010399855 1.29320022619 2.37270592145\n",
      "-0.18455431705 0.453381752338 1.2613018499 2.29176466605\n",
      "-0.106126902921 0.531980795461 1.24289005743 2.09062466464\n",
      "-0.264286616537 0.496057869422 1.22698523008 2.05908489374\n",
      "-0.208083516973 0.51493183315 1.25129962271 2.17325989695\n",
      "-0.13341759389 0.620533690751 1.24750431539 2.32942012199\n",
      "-0.146531319682 0.609281517961 1.27166520298 2.09206027916\n",
      "-0.148925626315 0.623139541188 1.31709906548 2.2068607201\n",
      "-0.126926790243 0.472106156321 1.25349948084 2.23452217975\n",
      "-0.17779487429 0.564931830821 1.29186715975 2.2665848422\n",
      "-0.14160169295 0.664268784578 1.34704163512 2.25695311715\n",
      "-0.134789570478 0.453739008873 1.27178093124 2.42325776375\n",
      "-0.192434083183 0.552188397278 1.29132549386 2.25974297062\n",
      "-0.126004469703 0.533358493251 1.30372846026 2.30452119806\n",
      "-0.132519679992 0.464626510775 1.26367111582 2.09659089396\n",
      "-0.251237079667 0.558005460184 1.27828144189 2.17417595572\n",
      "-0.100815409041 0.591246602522 1.31014143719 2.26267508981\n",
      "-0.209636396821 0.625074345101 1.21968598069 2.22395756646\n",
      "-0.159592889981 0.529409323777 1.18025865023 2.30385232927\n",
      "-0.109751358095 0.540030946914 1.30470402761 2.31060964228\n",
      "-0.180504510408 0.640044719054 1.24289542995 2.14731299145\n",
      "-0.186312077664 0.563526826187 1.35982021888 2.22470457977\n",
      "-0.186022319903 0.493461671745 1.27837591646 2.21897926075\n",
      "-0.159345695501 0.64786260874 1.30450497241 2.39870788565\n",
      "-0.212615988353 0.454938446331 1.33400607742 2.30471151099\n",
      "-0.284262023633 0.477650996194 1.31988259445 2.24119069862\n",
      "-0.172008183523 0.420002996404 1.29549701232 2.26090597527\n",
      "-0.156179605244 0.472593833689 1.23062653274 2.20770820642\n",
      "-0.118123670405 0.522550284162 1.3261130062 2.16241219911\n",
      "-0.170198648657 0.518048104076 1.31896027653 2.05267454183\n",
      "-0.169385501696 0.45639489597 1.27628765163 2.18899162086\n",
      "-0.224363689409 0.619934324107 1.33192854438 2.26112900887\n",
      "-0.150037980999 0.507509337618 1.30090884211 2.08491112253\n",
      "-0.35886262087 0.491409287128 1.28930125532 2.20274481549\n",
      "-0.230266937605 0.546032527494 1.27257752593 2.14115615741\n",
      "-0.119452260332 0.508475526487 1.29218100742 2.12995506464\n",
      "-0.150749861667 0.589810108072 1.23773725135 2.15517467717\n",
      "-0.152318138259 0.524551434348 1.33677132736 2.2433096882\n",
      "-0.138418013405 0.476235783346 1.28599574556 2.24930717508\n",
      "-0.119951573597 0.54236896586 1.23061560824 2.19715080828\n",
      "-0.205329761616 0.527155203765 1.2917089241 2.15066639512\n",
      "-0.135601255097 0.508401558012 1.263249614 2.20519991355\n",
      "-0.133584459036 0.591830760024 1.27347377614 2.21624262123\n",
      "-0.172026651033 0.452791255934 1.22682845415 2.35263801944\n",
      "-0.235097106398 0.529842431297 1.25891002734 2.21289194198\n",
      "-0.202335116948 0.676746662254 1.285669692 2.48219058715\n",
      "-0.12743444109 0.665145621226 1.18476284095 2.48059505779\n",
      "-0.134616329281 0.537063120295 1.19945317091 2.30792517453\n",
      "-0.103863342824 0.597642940003 1.31809637269 2.29758961779\n",
      "-0.108087815417 0.546273385567 1.27090934048 2.20857535734\n",
      "-0.171385308864 0.672479331552 1.29651240155 2.2656451715\n",
      "-0.152906903702 0.666770042676 1.27411200286 2.15720849947\n",
      "-0.177770758135 0.526347591557 1.27151656009 2.18083857651\n",
      "-0.2197008198 0.665844075516 1.28526152643 2.12143218083\n",
      "-0.166926430725 0.582165176635 1.2169490601 2.23715101426\n",
      "-0.166316658272 0.483356137905 1.23526725243 2.13149583519\n",
      "-0.194382586586 0.448325208416 1.23163699126 2.10350354221\n",
      "-0.194931292802 0.552092349928 1.19529603579 2.02778287492\n",
      "-0.246625655157 0.454550481618 1.21134883226 2.01926853464\n",
      "-0.180848829188 0.477187185248 1.17109444535 2.08577496404\n",
      "-0.183867189218 0.626451093916 1.21046673062 2.18287657049\n",
      "-0.178128836466 0.6322980769 1.2233719027 2.15610504206\n",
      "-0.157354410009 0.589542210009 1.26490483528 2.11696919545\n",
      "-0.118220507166 0.537701057202 1.31831474788 2.15514929064\n",
      "-0.170836976544 0.530823256261 1.31779078847 2.12957423855\n",
      "-0.115477562552 0.559052820507 1.32613102678 2.14516347612\n",
      "-0.260166645641 0.508798601152 1.22180964416 2.15215443967\n",
      "-0.218479111211 0.646986966338 1.1809898723 2.2671433502\n",
      "-0.230264194112 0.556316650019 1.1546747533 2.27102867772\n",
      "-0.163271357631 0.640173215663 1.2050834537 2.26588809698\n",
      "-0.159725558199 0.517346964989 1.26111588074 2.19382180451\n",
      "-0.109673597563 0.565733413296 1.14965884056 2.24113737001\n",
      "-0.173741056287 0.58086062592 1.21838713763 2.20078220202\n",
      "-0.171344062826 0.490258093511 1.29864623273 2.14189514183\n",
      "-0.123615962661 0.555638288704 1.19288987911 2.18524291749\n",
      "-0.121593010814 0.59965394288 1.27173491585 2.1806336437\n",
      "-0.18516851206 0.608575855318 1.33835596465 2.29351666933\n",
      "-0.119016404039 0.550402047439 1.27513398685 2.22927459803\n",
      "-0.125824968809 0.684861142596 1.22025055193 2.22515932494\n",
      "-0.126560690346 0.517324104002 1.19789634894 2.1541335686\n",
      "-0.156096238299 0.439820731126 1.19919071096 2.13080091041\n",
      "-0.159370991187 0.508225863526 1.22544663277 2.04291555859\n",
      "-0.144811531127 0.512376000738 1.30437967211 2.06672017751\n",
      "-0.195679996938 0.512809736041 1.31777502262 2.25879180919\n",
      "-0.219151027948 0.595576929898 1.28857415613 2.23384003167\n",
      "-0.123186130515 0.497336344658 1.24937979242 2.16145275041\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "    \n",
    "    #     keep_prob = 1.0 - p_dropout # keep_prob==p_dropout, 1-rate for dropout, 80% is keep_prob\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.105856907841 0.279570434562 0.86660190467 1.20251243972\n",
      "-0.155191664383 0.288261515002 0.889275976739 1.25138699752\n",
      "-0.0705334706002 0.344630660594 0.951567828874 1.31551801341\n",
      "-0.132482524009 0.353791403522 0.938117529417 1.37189346045\n",
      "-0.113616815796 0.306603972222 0.956869698278 1.35072670805\n",
      "-0.0918661701641 0.325162981366 0.950788678996 1.42381428375\n",
      "-0.0869468239639 0.324479072308 0.961812747501 1.41067656968\n",
      "-0.0606090730805 0.351917586035 0.972776951234 1.38533958476\n",
      "-0.10872708601 0.383740546293 1.02475160705 1.40877286324\n",
      "-0.183687697566 0.388829816808 0.985993555097 1.45597050914\n",
      "-0.183079470999 0.353045997825 1.04372547392 1.43863367183\n",
      "-0.101662083454 0.424148949162 0.99622128851 1.40205418122\n",
      "-0.0976699484971 0.387634760108 1.04357712112 1.40889933699\n",
      "-0.173219181539 0.327215656716 0.983300254122 1.40398571873\n",
      "-0.10727509687 0.343160040971 0.976136432118 1.39255829901\n",
      "-0.143136983458 0.379707702698 0.983961253262 1.42320519678\n",
      "-0.0508431911137 0.367824686636 1.01587006938 1.37922871719\n",
      "-0.14423430431 0.388801946984 1.01999378999 1.42200632674\n",
      "-0.0907774778577 0.350282938544 1.0272181687 1.40841459161\n",
      "-0.099780321049 0.332181176705 0.995668407528 1.39780248066\n",
      "-0.0936452039116 0.32894033908 1.01242094086 1.3856004105\n",
      "-0.12873050587 0.374621821742 0.983475315891 1.40277822077\n",
      "-0.191174660794 0.363091845041 0.981914945408 1.379078248\n",
      "-0.0886902784812 0.378142399963 0.993598177827 1.48819147555\n",
      "-0.0832197066663 0.421642898601 1.01124529103 1.44319012462\n",
      "-0.0757957105157 0.368033507942 0.980818252779 1.43007606514\n",
      "-0.155626439054 0.456626029886 1.02057722158 1.47238522294\n",
      "-0.13907825406 0.43443071019 1.03309231025 1.43898395803\n",
      "-0.101614273149 0.289890367685 1.03741811533 1.44021545664\n",
      "-0.158100893274 0.34407726894 0.985793012113 1.40981143567\n",
      "-0.0701975862664 0.332641543615 1.02068130247 1.40924755362\n",
      "-0.180669480582 0.324776043596 0.98858508761 1.41116676755\n",
      "-0.144297664699 0.323693510728 1.01214463998 1.42303285488\n",
      "-0.129679786563 0.398597868849 1.0052938952 1.49103808982\n",
      "-0.0873110418352 0.394553398371 0.954948765636 1.41439308359\n",
      "-0.0833427391268 0.368808169537 0.967462359055 1.49489344261\n",
      "-0.183694143344 0.423474347522 1.00367629477 1.43491857586\n",
      "-0.15875615821 0.348240621974 0.991233086461 1.42182484976\n",
      "-0.172169123323 0.411562552902 1.03271083239 1.52669679608\n",
      "-0.131316568816 0.382693762298 0.982302279505 1.4501830085\n",
      "-0.075134209026 0.387407500492 1.01872296119 1.48454766722\n",
      "-0.112615839756 0.373605006582 0.995803812218 1.37288995574\n",
      "-0.131826778899 0.394609610503 1.00460209961 1.40744238525\n",
      "-0.169122951325 0.4258099927 0.96974594922 1.41247418143\n",
      "-0.0825781648669 0.509766812369 0.964437678741 1.41697197251\n",
      "-0.135105023614 0.412983432631 1.01915130921 1.4259438232\n",
      "-0.103560688513 0.397883045316 1.01517424784 1.45959356754\n",
      "-0.125616463892 0.394685981194 0.992629653133 1.44586571341\n",
      "-0.0744499856107 0.330701551279 1.0073968316 1.42884043904\n",
      "-0.112086566539 0.461213821608 1.02680823013 1.41036959406\n",
      "-0.0683232029272 0.415318961781 0.98921632883 1.43123472777\n",
      "-0.0974113546815 0.35480116942 0.927949237315 1.42034489314\n",
      "-0.10938341159 0.348173614166 1.03166227013 1.39812704891\n",
      "-0.0913947266023 0.424016562266 1.00640765121 1.4456213709\n",
      "-0.151547167669 0.375111688761 0.973024250394 1.34886171405\n",
      "-0.0777966301288 0.41879048643 0.999395129882 1.50318125896\n",
      "-0.25348942241 0.369687255433 0.991157084801 1.49277216003\n",
      "-0.0683190840521 0.389622871533 0.98639904515 1.47990848486\n",
      "-0.136972851831 0.383826766531 0.981227940035 1.44657564592\n",
      "-0.0641828984764 0.484222943788 1.01510496922 1.45478465783\n",
      "-0.0844447966704 0.35889414275 1.00462362477 1.4485787834\n",
      "-0.0730183404051 0.34543067689 0.997364040923 1.48926101447\n",
      "-0.107004172468 0.413684057673 0.965351045204 1.37999125561\n",
      "-0.129072058851 0.304427542226 0.957689381905 1.43603198146\n",
      "-0.138968972847 0.306324394695 0.96119888205 1.39978112736\n",
      "-0.0996327059157 0.351304758145 0.991324519161 1.41695139121\n",
      "-0.140221956903 0.495096440958 0.982916618683 1.38062793333\n",
      "-0.119523528985 0.348477715503 0.927304418921 1.43216760961\n",
      "-0.11022064975 0.333872043417 0.990328675119 1.40044565624\n",
      "-0.15644121314 0.492777488171 0.958113971841 1.38370938876\n",
      "-0.189592801559 0.351934555252 0.957329537299 1.44285121305\n",
      "-0.0821794319732 0.392936284259 0.982373472803 1.41054397771\n",
      "-0.0855939517805 0.326108248831 0.972171783566 1.45399513407\n",
      "-0.165575320128 0.341333445593 0.984403137319 1.43103780888\n",
      "-0.0424847030958 0.357231923458 1.00037256186 1.45877302312\n",
      "-0.116997066308 0.384462729279 0.993518052302 1.41484951258\n",
      "-0.0898684058926 0.371627036776 0.992045561631 1.43922008319\n",
      "-0.203130313364 0.452882528568 0.992809247348 1.49552515749\n",
      "-0.0985609541459 0.397392086513 1.03009933392 1.51128938445\n",
      "-0.113378401239 0.320506315108 0.971137733725 1.47117932868\n",
      "-0.127603695299 0.304810997216 0.926319655819 1.4104805476\n",
      "-0.143188051138 0.386893615517 0.926586150839 1.39911462329\n",
      "-0.0852358212442 0.332782040463 1.00009370527 1.4388211355\n",
      "-0.119066467986 0.254370442886 0.97063037884 1.41456426151\n",
      "-0.0887582625221 0.327823585075 0.986784534029 1.4594690694\n",
      "-0.0883587589969 0.378394729736 0.966069865407 1.45146185658\n",
      "-0.132589935673 0.372382619837 0.999832216471 1.43604512686\n",
      "-0.137410934219 0.350580785545 1.00009007944 1.41428223337\n",
      "-0.102292269693 0.392804278498 0.96901478357 1.44370776101\n",
      "-0.121985176391 0.38473378992 0.980908624338 1.40991999336\n",
      "-0.0790669259954 0.412573225024 0.992611651881 1.42024431168\n",
      "-0.183119087131 0.370691598845 0.980644397384 1.42124218189\n",
      "-0.109457308819 0.315651641014 1.01042559043 1.39320295873\n",
      "-0.13924534772 0.346502412743 1.00231558981 1.38659531556\n",
      "-0.185705071067 0.383301792267 1.022817748 1.42403681746\n",
      "-0.127505355351 0.359614878189 0.991047622109 1.43392534727\n",
      "-0.104511415126 0.446705639569 0.997236512961 1.41871757088\n",
      "-0.0744350016982 0.289508812316 0.99331770957 1.40164892106\n",
      "-0.125138387328 0.34950112854 1.00604459427 1.39352372356\n",
      "-0.0520981511208 0.376654334717 0.988266724716 1.40372206665\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = m * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return X_pos + X_neg_exp\n",
    "\n",
    "def elu_bwd(X, dX):\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    m_neg_exp = m * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return dX * m_neg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = m * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return X_pos + X_neg_exp\n",
    "\n",
    "def selu_bwd(X, dX):\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    m_neg_exp = m * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return dX * m_neg_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
