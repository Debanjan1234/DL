{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of SELUs\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "#     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)    \n",
    "#     return scale * np.maximum(0.0, alpha*np.exp(x)-alpha)\n",
    "#     return scale * alpha * np.where(x>=0.0, x, np.exp(x)-1)\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.196616111925 0.224280364437 0.818826323087 1.20876426248\n",
      "-0.191604771465 0.188587363549 0.822755874019 1.24344800241\n",
      "-0.157282508598 0.174637868892 0.791649928783 1.20126673366\n",
      "-0.167062400419 0.22278489766 0.773504788939 1.16375238425\n",
      "-0.167852316628 0.200074674407 0.811787042106 1.16186432918\n",
      "-0.187030267758 0.196644459961 0.800800121371 1.17807256893\n",
      "-0.227842424975 0.206835105566 0.821447906304 1.18298208072\n",
      "-0.165911049646 0.223727085267 0.837952640153 1.20291223138\n",
      "-0.187175510158 0.288024185182 0.8039830216 1.20672812329\n",
      "-0.163073198289 0.197713920598 0.832939870669 1.14407170008\n",
      "-0.187042891809 0.133347033808 0.815552199062 1.19333569547\n",
      "-0.188739606349 0.203325048064 0.755867955484 1.21199247572\n",
      "-0.233773019439 0.248088338015 0.715002047001 1.23617554245\n",
      "-0.16348247404 0.18039628769 0.797514923525 1.23010780634\n",
      "-0.226971844705 0.29050489033 0.818160110464 1.21763362071\n",
      "-0.277775262046 0.237132822377 0.831482243349 1.21348822998\n",
      "-0.149755553551 0.194284701388 0.803702086571 1.20119470292\n",
      "-0.201518501026 0.162210730687 0.796598728062 1.21693694789\n",
      "-0.196197670286 0.181350260232 0.842906617991 1.17441822506\n",
      "-0.18692005334 0.181794202622 0.779888578152 1.16385299252\n",
      "-0.180456898403 0.172970180525 0.805902113426 1.14161222925\n",
      "-0.116321369052 0.124750941201 0.797989121075 1.20242999298\n",
      "-0.139318945697 0.188116328349 0.836803216477 1.18035077919\n",
      "-0.173102591959 0.189168238868 0.821145213887 1.22225518512\n",
      "-0.208971043491 0.196647496007 0.790439941333 1.22514531724\n",
      "-0.154669345462 0.217667097272 0.811275959121 1.20192658966\n",
      "-0.324027689753 0.204637587327 0.788903557139 1.1997047533\n",
      "-0.146248000963 0.147900084355 0.806010895671 1.24226124984\n",
      "-0.223555838231 0.154666292926 0.825895089917 1.24302224872\n",
      "-0.204034240317 0.275579351526 0.826411568589 1.28585043105\n",
      "-0.179568215271 0.179492755806 0.834403974947 1.28057849618\n",
      "-0.141829669341 0.14752123485 0.824090965825 1.31512662662\n",
      "-0.175306663074 0.169367197628 0.811592271924 1.25641534256\n",
      "-0.159759127914 0.194261853795 0.789036646333 1.18892201407\n",
      "-0.212234019736 0.227469714998 0.791986204494 1.15119324984\n",
      "-0.178092174029 0.153031506398 0.802554074041 1.18734702617\n",
      "-0.194473726402 0.192746959794 0.79627783797 1.19081666878\n",
      "-0.125386318033 0.124145413396 0.821782296251 1.17403707888\n",
      "-0.102742980299 0.114701437334 0.811942114877 1.21501440465\n",
      "-0.168122489052 0.132056276183 0.796256581899 1.19774585919\n",
      "-0.28172923972 0.25328261384 0.782903679542 1.15236054005\n",
      "-0.163821384997 0.188462944034 0.81172744623 1.19555089089\n",
      "-0.191276268589 0.174684524962 0.842992593493 1.23828214201\n",
      "-0.137080040451 0.172160845946 0.848753092574 1.25682936352\n",
      "-0.216974324949 0.221635598817 0.859639914048 1.27352421716\n",
      "-0.233289552566 0.217009300795 0.852871893431 1.18807908949\n",
      "-0.214606691932 0.212525321149 0.853238865551 1.15019769384\n",
      "-0.13737901228 0.136986424603 0.846526084975 1.18365213026\n",
      "-0.15681842955 0.217581558404 0.829406029794 1.23001572871\n",
      "-0.230230745848 0.171989687149 0.790615964361 1.1755441062\n",
      "-0.176738795531 0.273761785058 0.804817421386 1.18166606303\n",
      "-0.17391122995 0.207746198757 0.855095926534 1.25006386549\n",
      "-0.178091738669 0.167408675591 0.812479189717 1.18053573531\n",
      "-0.231579742108 0.217932790087 0.816263068536 1.16007691497\n",
      "-0.151289950174 0.14296218039 0.802422888197 1.16060893561\n",
      "-0.132729449707 0.16634308415 0.840538060024 1.17847352932\n",
      "-0.222994073093 0.225146710536 0.846647816463 1.19725517144\n",
      "-0.186193400768 0.168615323845 0.779377947584 1.17995624239\n",
      "-0.209449010711 0.169394271415 0.787793889372 1.18288348346\n",
      "-0.164358327163 0.20770676411 0.787052703669 1.16934841669\n",
      "-0.190613613936 0.241535079737 0.782688013374 1.16700174852\n",
      "-0.144938949823 0.141342132876 0.81526357363 1.17574671463\n",
      "-0.148746578993 0.191963120601 0.856361222183 1.16824500396\n",
      "-0.196300856186 0.113494384742 0.839930072475 1.16562357828\n",
      "-0.170575540984 0.169949794326 0.834569829187 1.16493805344\n",
      "-0.156498926687 0.154624311005 0.830542867909 1.18795124812\n",
      "-0.183303216527 0.237840461045 0.803787647722 1.19478949648\n",
      "-0.22365180148 0.236235950712 0.824494111215 1.19557789228\n",
      "-0.113749457964 0.131968210021 0.844614443621 1.16924287138\n",
      "-0.161889243131 0.175220340027 0.807725108164 1.16715887599\n",
      "-0.170882686245 0.132716683809 0.783980011566 1.17643753716\n",
      "-0.152901739209 0.234856606553 0.809531467671 1.21773933002\n",
      "-0.18010841511 0.191312820862 0.786360616712 1.21581168205\n",
      "-0.206380169421 0.19589048392 0.769637300956 1.21263293139\n",
      "-0.106977357023 0.102291793296 0.801375754899 1.21450107041\n",
      "-0.114791986484 0.173574182222 0.860849847834 1.23772075263\n",
      "-0.226409123994 0.175760036656 0.862945144755 1.21909926804\n",
      "-0.15449787473 0.197177988616 0.8426649094 1.17996555764\n",
      "-0.161816940057 0.171181922196 0.83503249839 1.18004802514\n",
      "-0.166410808893 0.176616054966 0.835321581783 1.1789164489\n",
      "-0.224351325315 0.279420192824 0.850351491226 1.29316453494\n",
      "-0.18790580873 0.2024907975 0.85371951796 1.32168624009\n",
      "-0.184424389297 0.28762934084 0.857891830283 1.19224921491\n",
      "-0.173993784777 0.189447545778 0.845541112405 1.18224569145\n",
      "-0.197362294955 0.196047090478 0.871213796514 1.17278714614\n",
      "-0.141612178584 0.155572487251 0.82574137642 1.1774857753\n",
      "-0.182895789628 0.240247693715 0.837837934095 1.18349335165\n",
      "-0.151272179656 0.174279025489 0.83525380912 1.21506724521\n",
      "-0.193135175031 0.183306286123 0.80818228685 1.26499100036\n",
      "-0.13581371729 0.172958142843 0.830557128473 1.21984205809\n",
      "-0.128360459333 0.229342772204 0.832384556577 1.16285033075\n",
      "-0.159081511668 0.194402120195 0.830386031536 1.17169908504\n",
      "-0.151722488764 0.132346420649 0.826412866608 1.17720178762\n",
      "-0.15805231077 0.204682466018 0.820974313069 1.14770112638\n",
      "-0.17635255231 0.249724730988 0.818253941176 1.13298556828\n",
      "-0.203584409458 0.163518915994 0.801144913483 1.13161414016\n",
      "-0.166457969769 0.18388524512 0.807061940171 1.1738830705\n",
      "-0.167514793531 0.226263665329 0.79440762701 1.16726626935\n",
      "-0.200173995349 0.160708958484 0.843306797636 1.20726753983\n",
      "-0.171828987719 0.162995391834 0.838145515424 1.16973484646\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-5.96593216528e-05 0.000961902494112\n"
     ]
    }
   ],
   "source": [
    "# Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "import numpy as np\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)\n",
    "\n",
    "du = 0.001\n",
    "u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# print (u_new-u_old) / du\n",
    "print(u_old, u_new)\n",
    "# Now I see your problem: \n",
    "#     You do not consider the effect of the weights. \n",
    "#     From one layer to the next, we have two influences: \n",
    "#         (1) multiplication with weights and \n",
    "#         (2) applying the SELU. \n",
    "#         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "#         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "#         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# Oh yes, thats true, zero mean weights completely kill the mean. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.256957516928 0.307727747979 0.918308217507 1.2931015458\n",
      "-0.182192403207 0.318422064717 0.959007439863 1.49359179336\n",
      "-0.230316499051 0.314326798104 1.0321997765 1.63824815892\n",
      "-0.23649657688 0.348654017622 1.05635651738 1.74116137036\n",
      "-0.24933939289 0.392245158414 1.14475114238 1.76191160941\n",
      "-0.163087489105 0.421406263884 1.17250788374 1.94479958698\n",
      "-0.267504611261 0.436318338005 1.13018738786 1.86434460042\n",
      "-0.208328031694 0.469791982325 1.18507012108 1.90226164169\n",
      "-0.140344345836 0.435378766854 1.19170773151 2.06887862909\n",
      "-0.168632731811 0.493313071662 1.09673156178 2.11524893245\n",
      "-0.119108868741 0.52054718538 1.10884678308 2.06723729255\n",
      "-0.131501857725 0.563248660121 1.16180977707 2.14694152282\n",
      "-0.153622970919 0.537861007285 1.22662114284 2.26374826768\n",
      "-0.243633404724 0.640730433516 1.16749581758 2.24247800773\n",
      "-0.126240620186 0.536816259405 1.25405538517 2.1429614854\n",
      "-0.206910851628 0.570610760406 1.25953298042 2.27830865496\n",
      "-0.222444622614 0.493762157422 1.21738094279 2.12734238594\n",
      "-0.197535127605 0.533158859215 1.23949578857 2.13746886349\n",
      "-0.216761309777 0.524872537247 1.18972324316 2.17504717964\n",
      "-0.220708194146 0.555987911858 1.24750074944 2.18499134775\n",
      "-0.141573572209 0.534611530915 1.30166699551 2.26751898085\n",
      "-0.214349384958 0.567201035259 1.28429820862 2.16664952438\n",
      "-0.326252695021 0.494425219276 1.28746005837 2.18307414581\n",
      "-0.178587233008 0.477481190377 1.2729904534 2.196355872\n",
      "-0.16955130309 0.488363039056 1.28824232763 2.29094461174\n",
      "-0.288509096336 0.522628094863 1.24789600698 2.31132336924\n",
      "-0.180725751641 0.546589087627 1.15762660079 2.21905371978\n",
      "-0.181093468279 0.569384281326 1.22830638988 2.28927781688\n",
      "-0.13612497145 0.504898079244 1.20917763486 2.22528490313\n",
      "-0.222435972178 0.694627079022 1.28811759637 2.15457603887\n",
      "-0.173964637195 0.56565255066 1.27393252943 2.18194245408\n",
      "-0.127771988888 0.445402290263 1.17294115504 2.20499335165\n",
      "-0.131890841838 0.635520686291 1.21833312477 2.31401652444\n",
      "-0.214065673681 0.497145275551 1.26888763934 2.16455517704\n",
      "-0.158079748928 0.496056479091 1.28930784428 2.09791418667\n",
      "-0.126245950053 0.620457186628 1.30978867073 2.18430563318\n",
      "-0.209160443624 0.559280133127 1.27997637314 2.2352282363\n",
      "-0.165055081505 0.67949962374 1.21035097712 2.25085316918\n",
      "-0.190886301253 0.485620738525 1.2971287156 2.17126795245\n",
      "-0.227373148383 0.480344126098 1.27729625962 2.15123183878\n",
      "-0.171150034821 0.548913222023 1.23360935344 2.17280238101\n",
      "-0.174159929561 0.573655477221 1.3080575235 2.22959703426\n",
      "-0.1352673301 0.584694600911 1.32946088737 2.12096951741\n",
      "-0.195791866657 0.529972812044 1.2056448384 2.17768864626\n",
      "-0.187381702228 0.501667239203 1.19248160312 2.23343045587\n",
      "-0.27995225401 0.662404460528 1.30046890577 2.22821765359\n",
      "-0.147422175922 0.572018215579 1.2254655441 2.19533917227\n",
      "-0.140215159387 0.642750710328 1.30497213027 2.39897580394\n",
      "-0.152905977297 0.705374536789 1.297709595 2.29532720714\n",
      "-0.233183403491 0.556368839908 1.28809797231 2.26913305939\n",
      "-0.206735746111 0.450235971384 1.1692675432 2.22873744556\n",
      "-0.144751119924 0.569520704587 1.218217455 2.28878759768\n",
      "-0.222600725454 0.46124095281 1.2680641686 2.21226126909\n",
      "-0.110967795464 0.514331123497 1.32088635742 2.19216117348\n",
      "-0.174540305343 0.623296046082 1.29908581802 2.18493977725\n",
      "-0.147385746152 0.514873026978 1.20937480219 2.24482942006\n",
      "-0.132171133426 0.570257642601 1.27720366437 2.25400490063\n",
      "-0.102907330142 0.531254696287 1.32896869609 2.3068804124\n",
      "-0.154757672255 0.574791875172 1.33432357157 2.26065964639\n",
      "-0.154291598454 0.586265288287 1.31912413384 2.24347231592\n",
      "-0.163185794546 0.574092558325 1.31932331892 2.42717770402\n",
      "-0.215783294212 0.472684663726 1.28455019212 2.30200851179\n",
      "-0.139585308942 0.511521090517 1.29461566778 2.19585879146\n",
      "-0.168678058041 0.631430400839 1.32371967892 2.27195776683\n",
      "-0.188060972216 0.582307245472 1.31243814847 2.28429604726\n",
      "-0.134676389209 0.571723294054 1.32128876664 2.28958137876\n",
      "-0.171552224824 0.522164809076 1.28763704105 2.10530859345\n",
      "-0.123651169164 0.51092175143 1.22851296715 2.10154221181\n",
      "-0.156243255234 0.645786107607 1.27068427659 2.14407357046\n",
      "-0.186355263369 0.503312773801 1.30258071526 2.2803714527\n",
      "-0.148292636151 0.703271603665 1.27355019087 2.21009526416\n",
      "-0.16149384756 0.46146032988 1.31277919128 2.31978356201\n",
      "-0.1364667951 0.543212587293 1.27411607173 2.09862991441\n",
      "-0.205507871241 0.470251963384 1.334204059 2.09864857899\n",
      "-0.146647523149 0.558912286298 1.29600565573 2.08263730847\n",
      "-0.174919449671 0.470642923983 1.22481604924 2.08380221742\n",
      "-0.106339452882 0.570900460154 1.25994591919 2.12019319706\n",
      "-0.271449920878 0.615083599475 1.2746968469 2.14675605012\n",
      "-0.2264035987 0.564943642975 1.30273189178 2.14184866327\n",
      "-0.274133787169 0.529160185582 1.28156595226 2.2699254513\n",
      "-0.186967494455 0.513494534598 1.24933246175 2.16235783823\n",
      "-0.194486527995 0.599830593228 1.24372307438 2.25506286514\n",
      "-0.272220761247 0.495484450291 1.25761049472 2.18689739141\n",
      "-0.0897460476127 0.556224956034 1.177948975 2.17112552646\n",
      "-0.203813327168 0.591471995635 1.22939904378 2.2244225465\n",
      "-0.195780643201 0.480930898959 1.23869983447 2.11295698937\n",
      "-0.130488293161 0.532809644699 1.19395729502 2.08663414325\n",
      "-0.142591466601 0.586618806371 1.24880539738 2.13688365273\n",
      "-0.171761898699 0.709318058038 1.20209690084 2.35439287137\n",
      "-0.137161203259 0.535954714052 1.29969731741 2.25469987169\n",
      "-0.203557444016 0.547264672207 1.29720157982 2.13920764314\n",
      "-0.186544513054 0.555111190842 1.32551923093 2.10825414137\n",
      "-0.243299852055 0.567137106032 1.24017445168 2.22218261716\n",
      "-0.147274436105 0.630623862779 1.20814238623 2.23508277886\n",
      "-0.168869004489 0.662266933264 1.23888323917 2.28784954265\n",
      "-0.159978143514 0.432925622321 1.28092053686 2.20848884542\n",
      "-0.189111982656 0.474292776433 1.29041115578 2.12033654926\n",
      "-0.120888239651 0.433950914924 1.3019053908 2.1074216272\n",
      "-0.148725077243 0.450127565971 1.2226818502 2.19054074306\n",
      "-0.159877902641 0.584018783049 1.18748380802 2.20683057364\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "    \n",
    "    #     keep_prob = 1.0 - p_dropout # keep_prob==p_dropout, 1-rate for dropout, 80% is keep_prob\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.105856907841 0.279570434562 0.86660190467 1.20251243972\n",
      "-0.155191664383 0.288261515002 0.889275976739 1.25138699752\n",
      "-0.0705334706002 0.344630660594 0.951567828874 1.31551801341\n",
      "-0.132482524009 0.353791403522 0.938117529417 1.37189346045\n",
      "-0.113616815796 0.306603972222 0.956869698278 1.35072670805\n",
      "-0.0918661701641 0.325162981366 0.950788678996 1.42381428375\n",
      "-0.0869468239639 0.324479072308 0.961812747501 1.41067656968\n",
      "-0.0606090730805 0.351917586035 0.972776951234 1.38533958476\n",
      "-0.10872708601 0.383740546293 1.02475160705 1.40877286324\n",
      "-0.183687697566 0.388829816808 0.985993555097 1.45597050914\n",
      "-0.183079470999 0.353045997825 1.04372547392 1.43863367183\n",
      "-0.101662083454 0.424148949162 0.99622128851 1.40205418122\n",
      "-0.0976699484971 0.387634760108 1.04357712112 1.40889933699\n",
      "-0.173219181539 0.327215656716 0.983300254122 1.40398571873\n",
      "-0.10727509687 0.343160040971 0.976136432118 1.39255829901\n",
      "-0.143136983458 0.379707702698 0.983961253262 1.42320519678\n",
      "-0.0508431911137 0.367824686636 1.01587006938 1.37922871719\n",
      "-0.14423430431 0.388801946984 1.01999378999 1.42200632674\n",
      "-0.0907774778577 0.350282938544 1.0272181687 1.40841459161\n",
      "-0.099780321049 0.332181176705 0.995668407528 1.39780248066\n",
      "-0.0936452039116 0.32894033908 1.01242094086 1.3856004105\n",
      "-0.12873050587 0.374621821742 0.983475315891 1.40277822077\n",
      "-0.191174660794 0.363091845041 0.981914945408 1.379078248\n",
      "-0.0886902784812 0.378142399963 0.993598177827 1.48819147555\n",
      "-0.0832197066663 0.421642898601 1.01124529103 1.44319012462\n",
      "-0.0757957105157 0.368033507942 0.980818252779 1.43007606514\n",
      "-0.155626439054 0.456626029886 1.02057722158 1.47238522294\n",
      "-0.13907825406 0.43443071019 1.03309231025 1.43898395803\n",
      "-0.101614273149 0.289890367685 1.03741811533 1.44021545664\n",
      "-0.158100893274 0.34407726894 0.985793012113 1.40981143567\n",
      "-0.0701975862664 0.332641543615 1.02068130247 1.40924755362\n",
      "-0.180669480582 0.324776043596 0.98858508761 1.41116676755\n",
      "-0.144297664699 0.323693510728 1.01214463998 1.42303285488\n",
      "-0.129679786563 0.398597868849 1.0052938952 1.49103808982\n",
      "-0.0873110418352 0.394553398371 0.954948765636 1.41439308359\n",
      "-0.0833427391268 0.368808169537 0.967462359055 1.49489344261\n",
      "-0.183694143344 0.423474347522 1.00367629477 1.43491857586\n",
      "-0.15875615821 0.348240621974 0.991233086461 1.42182484976\n",
      "-0.172169123323 0.411562552902 1.03271083239 1.52669679608\n",
      "-0.131316568816 0.382693762298 0.982302279505 1.4501830085\n",
      "-0.075134209026 0.387407500492 1.01872296119 1.48454766722\n",
      "-0.112615839756 0.373605006582 0.995803812218 1.37288995574\n",
      "-0.131826778899 0.394609610503 1.00460209961 1.40744238525\n",
      "-0.169122951325 0.4258099927 0.96974594922 1.41247418143\n",
      "-0.0825781648669 0.509766812369 0.964437678741 1.41697197251\n",
      "-0.135105023614 0.412983432631 1.01915130921 1.4259438232\n",
      "-0.103560688513 0.397883045316 1.01517424784 1.45959356754\n",
      "-0.125616463892 0.394685981194 0.992629653133 1.44586571341\n",
      "-0.0744499856107 0.330701551279 1.0073968316 1.42884043904\n",
      "-0.112086566539 0.461213821608 1.02680823013 1.41036959406\n",
      "-0.0683232029272 0.415318961781 0.98921632883 1.43123472777\n",
      "-0.0974113546815 0.35480116942 0.927949237315 1.42034489314\n",
      "-0.10938341159 0.348173614166 1.03166227013 1.39812704891\n",
      "-0.0913947266023 0.424016562266 1.00640765121 1.4456213709\n",
      "-0.151547167669 0.375111688761 0.973024250394 1.34886171405\n",
      "-0.0777966301288 0.41879048643 0.999395129882 1.50318125896\n",
      "-0.25348942241 0.369687255433 0.991157084801 1.49277216003\n",
      "-0.0683190840521 0.389622871533 0.98639904515 1.47990848486\n",
      "-0.136972851831 0.383826766531 0.981227940035 1.44657564592\n",
      "-0.0641828984764 0.484222943788 1.01510496922 1.45478465783\n",
      "-0.0844447966704 0.35889414275 1.00462362477 1.4485787834\n",
      "-0.0730183404051 0.34543067689 0.997364040923 1.48926101447\n",
      "-0.107004172468 0.413684057673 0.965351045204 1.37999125561\n",
      "-0.129072058851 0.304427542226 0.957689381905 1.43603198146\n",
      "-0.138968972847 0.306324394695 0.96119888205 1.39978112736\n",
      "-0.0996327059157 0.351304758145 0.991324519161 1.41695139121\n",
      "-0.140221956903 0.495096440958 0.982916618683 1.38062793333\n",
      "-0.119523528985 0.348477715503 0.927304418921 1.43216760961\n",
      "-0.11022064975 0.333872043417 0.990328675119 1.40044565624\n",
      "-0.15644121314 0.492777488171 0.958113971841 1.38370938876\n",
      "-0.189592801559 0.351934555252 0.957329537299 1.44285121305\n",
      "-0.0821794319732 0.392936284259 0.982373472803 1.41054397771\n",
      "-0.0855939517805 0.326108248831 0.972171783566 1.45399513407\n",
      "-0.165575320128 0.341333445593 0.984403137319 1.43103780888\n",
      "-0.0424847030958 0.357231923458 1.00037256186 1.45877302312\n",
      "-0.116997066308 0.384462729279 0.993518052302 1.41484951258\n",
      "-0.0898684058926 0.371627036776 0.992045561631 1.43922008319\n",
      "-0.203130313364 0.452882528568 0.992809247348 1.49552515749\n",
      "-0.0985609541459 0.397392086513 1.03009933392 1.51128938445\n",
      "-0.113378401239 0.320506315108 0.971137733725 1.47117932868\n",
      "-0.127603695299 0.304810997216 0.926319655819 1.4104805476\n",
      "-0.143188051138 0.386893615517 0.926586150839 1.39911462329\n",
      "-0.0852358212442 0.332782040463 1.00009370527 1.4388211355\n",
      "-0.119066467986 0.254370442886 0.97063037884 1.41456426151\n",
      "-0.0887582625221 0.327823585075 0.986784534029 1.4594690694\n",
      "-0.0883587589969 0.378394729736 0.966069865407 1.45146185658\n",
      "-0.132589935673 0.372382619837 0.999832216471 1.43604512686\n",
      "-0.137410934219 0.350580785545 1.00009007944 1.41428223337\n",
      "-0.102292269693 0.392804278498 0.96901478357 1.44370776101\n",
      "-0.121985176391 0.38473378992 0.980908624338 1.40991999336\n",
      "-0.0790669259954 0.412573225024 0.992611651881 1.42024431168\n",
      "-0.183119087131 0.370691598845 0.980644397384 1.42124218189\n",
      "-0.109457308819 0.315651641014 1.01042559043 1.39320295873\n",
      "-0.13924534772 0.346502412743 1.00231558981 1.38659531556\n",
      "-0.185705071067 0.383301792267 1.022817748 1.42403681746\n",
      "-0.127505355351 0.359614878189 0.991047622109 1.43392534727\n",
      "-0.104511415126 0.446705639569 0.997236512961 1.41871757088\n",
      "-0.0744350016982 0.289508812316 0.99331770957 1.40164892106\n",
      "-0.125138387328 0.34950112854 1.00604459427 1.39352372356\n",
      "-0.0520981511208 0.376654334717 0.988266724716 1.40372206665\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = m * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return X_pos + X_neg_exp\n",
    "\n",
    "def elu_bwd(X, dX):\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    m_neg_exp = m * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return dX * m_neg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def selu_bwd(dX, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dX = dX * scale\n",
    "    dX_neg = dX.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dX.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.192573130677 0.225464851152 0.8517251468 1.17343643636\n",
      "-0.181990439693 0.19236772977 0.830614924998 1.16460880061\n",
      "-0.156997657224 0.180271015029 0.828943363818 1.16442619197\n",
      "-0.199104746591 0.22398155216 0.796812820533 1.2104985597\n",
      "-0.173998806034 0.170736138442 0.779529801004 1.19198148117\n",
      "-0.219109562845 0.156020176235 0.770437549522 1.212449289\n",
      "-0.18967747706 0.221089878525 0.758048489469 1.23475467222\n",
      "-0.223147408747 0.194725050129 0.820239419803 1.21362171248\n",
      "-0.17199878824 0.166789833841 0.837366033913 1.21879645082\n",
      "-0.212695078258 0.297249892012 0.824467642147 1.21928508349\n",
      "-0.206781718701 0.183024553619 0.850118701275 1.22015661902\n",
      "-0.193197569119 0.230625786613 0.82600270733 1.18500910834\n",
      "-0.163987888743 0.171045279072 0.807928639787 1.17680909555\n",
      "-0.20329546972 0.243542309672 0.843773195816 1.16241449782\n",
      "-0.222863285566 0.209524992637 0.832198097308 1.14877634514\n",
      "-0.213679413185 0.196324396734 0.820204317897 1.18841301673\n",
      "-0.141203904392 0.215977872027 0.823700795761 1.22869621263\n",
      "-0.17519784277 0.158848571283 0.814101882395 1.18890987759\n",
      "-0.184178703777 0.202934871312 0.845279084522 1.20320179179\n",
      "-0.17443215272 0.216330907769 0.808508473833 1.19795740906\n",
      "-0.151028771474 0.21761994517 0.832283283778 1.19595374281\n",
      "-0.173321858802 0.196361696544 0.847027911461 1.21708862855\n",
      "-0.147361972794 0.178461972109 0.839563272027 1.18628878579\n",
      "-0.17315748284 0.204504844374 0.825049432785 1.19807374423\n",
      "-0.163660193111 0.201485988201 0.85173375359 1.20129316235\n",
      "-0.134132096331 0.160266996746 0.811296396795 1.15893493704\n",
      "-0.178347073695 0.162555890524 0.822835164355 1.18734141342\n",
      "-0.2075659661 0.211796165013 0.811277142954 1.18340023456\n",
      "-0.214607453273 0.190838365409 0.813193699212 1.18882909064\n",
      "-0.187860127582 0.216788538445 0.822626622502 1.16554074629\n",
      "-0.158013961805 0.159457665197 0.813178306997 1.1889781034\n",
      "-0.17168872351 0.14096583262 0.828379697472 1.23340329884\n",
      "-0.193202650213 0.176233259162 0.832912634018 1.20112744909\n",
      "-0.224188397425 0.223296955946 0.810882578932 1.2349523553\n",
      "-0.219401351688 0.207237738028 0.815647455273 1.19485163246\n",
      "-0.143033719043 0.168380455037 0.82336194186 1.20493342817\n",
      "-0.153391850015 0.10411474563 0.803693167988 1.15118376231\n",
      "-0.104541984863 0.116179014503 0.82954614583 1.15992009024\n",
      "-0.189063785321 0.133048318582 0.844311825279 1.18492036716\n",
      "-0.128988166085 0.139889770503 0.830169519913 1.19938855242\n",
      "-0.214566475296 0.189757971491 0.802237023821 1.20795078386\n",
      "-0.143037589227 0.135077199174 0.781558982451 1.1540849175\n",
      "-0.19478943273 0.260720598042 0.827015085274 1.15142064625\n",
      "-0.161399538685 0.15365516658 0.766089402059 1.20414367495\n",
      "-0.212756949079 0.18696545128 0.727323934638 1.17279179581\n",
      "-0.210114201626 0.215006868952 0.765061887944 1.18409963866\n",
      "-0.112734227528 0.115407958946 0.851174603501 1.20691699789\n",
      "-0.256938217342 0.263200507384 0.795772041404 1.23985304852\n",
      "-0.200884204729 0.202748300765 0.815976826207 1.30726489113\n",
      "-0.193561073042 0.202564890962 0.858167040089 1.28504425136\n",
      "-0.187491832486 0.253281349227 0.806541097687 1.35409646864\n",
      "-0.173432071844 0.179726514543 0.822432535738 1.21063726951\n",
      "-0.159161734079 0.162723591476 0.837557101438 1.20532265195\n",
      "-0.179594235726 0.292054222336 0.788048835395 1.18995867765\n",
      "-0.169968907855 0.190945500111 0.865380452272 1.18887697679\n",
      "-0.151438019338 0.210544577728 0.832513704364 1.20515477636\n",
      "-0.157184016022 0.154802196408 0.816467240017 1.20537250909\n",
      "-0.16887418506 0.16733237814 0.836465371176 1.14835495281\n",
      "-0.182010336786 0.174594961912 0.794838248412 1.20492937472\n",
      "-0.172624990499 0.249964799628 0.783454667798 1.21901842135\n",
      "-0.212551613852 0.188440688126 0.779835791714 1.21400819228\n",
      "-0.164845970196 0.281912310376 0.791614281805 1.18999302972\n",
      "-0.13916132736 0.142270697548 0.813898236058 1.15865283584\n",
      "-0.189849273688 0.232860104041 0.842109926097 1.12261968686\n",
      "-0.145285323792 0.211184037421 0.783695609601 1.15036187936\n",
      "-0.115695204026 0.175854738737 0.774579294736 1.1592509854\n",
      "-0.142791647492 0.162176891228 0.792264290237 1.19568133032\n",
      "-0.215990875502 0.216096198167 0.809231830407 1.17284275637\n",
      "-0.142843678937 0.121106808242 0.755349775729 1.18896984718\n",
      "-0.156363669101 0.184421864773 0.793909027612 1.20320343454\n",
      "-0.219301655729 0.248401927574 0.809234801712 1.23441584595\n",
      "-0.145008656405 0.224794801883 0.781205837088 1.24093652009\n",
      "-0.222019954711 0.230018238374 0.719891775034 1.21300662284\n",
      "-0.133269349401 0.12287208421 0.775259937948 1.20665962476\n",
      "-0.163166891915 0.154594812755 0.824715730706 1.26935254162\n",
      "-0.178973051973 0.138548652523 0.753574657086 1.16056211923\n",
      "-0.279017051688 0.210223655918 0.774293170084 1.14188954169\n",
      "-0.163234613288 0.162473983186 0.839588022155 1.16322664578\n",
      "-0.236488116978 0.284203185451 0.849231362667 1.19437537537\n",
      "-0.311784428538 0.209947711856 0.822299327216 1.16678860229\n",
      "-0.169859379449 0.191167808504 0.844718649672 1.20022245392\n",
      "-0.208800563549 0.21828843524 0.825719107431 1.2430666696\n",
      "-0.133480338644 0.193123155785 0.83861737149 1.22841705487\n",
      "-0.187932891855 0.243658219156 0.827356816012 1.19046196275\n",
      "-0.185737865954 0.194087718723 0.804246720761 1.15841343134\n",
      "-0.173063785851 0.177944186695 0.835172937955 1.20051892974\n",
      "-0.27912689108 0.239750921854 0.826517829544 1.16464462598\n",
      "-0.201078477977 0.180501142281 0.815432887091 1.17304875776\n",
      "-0.159107742191 0.172435874131 0.824536583996 1.13157286715\n",
      "-0.166422307121 0.187829016879 0.786372575077 1.17795798142\n",
      "-0.147527578546 0.186678346507 0.779563100203 1.1736355097\n",
      "-0.255222588451 0.266211440415 0.814796888992 1.20089871181\n",
      "-0.202456396353 0.218616088757 0.837447176522 1.22596604483\n",
      "-0.269257100985 0.192130997373 0.800922361518 1.17412431159\n",
      "-0.106035367793 0.132268195289 0.823229981929 1.18469795718\n",
      "-0.21608671482 0.141019422021 0.776661914802 1.15852060849\n",
      "-0.166355481031 0.122716665909 0.787468337112 1.15454255487\n",
      "-0.209712149234 0.227753823594 0.756480907334 1.17472071903\n",
      "-0.189498641007 0.193560067706 0.762585606527 1.19198336247\n",
      "-0.198339288141 0.203299536863 0.782620296129 1.16358963307\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
