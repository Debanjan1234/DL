{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of SELUs\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "#     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)    \n",
    "#     return scale * np.maximum(0.0, alpha*np.exp(x)-alpha)\n",
    "#     return scale * alpha * np.where(x>=0.0, x, np.exp(x)-1)\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.211805449621 0.214537908859 0.842938370494 1.14091006749\n",
      "-0.200125012491 0.158273826137 0.858444669156 1.17140790041\n",
      "-0.191315253621 0.176234937548 0.836895311786 1.18280097899\n",
      "-0.170932342201 0.211040584341 0.794149361877 1.17795657195\n",
      "-0.175543325543 0.161170174093 0.80864912391 1.20835847551\n",
      "-0.213287568908 0.18399906103 0.799333237886 1.24433407569\n",
      "-0.193050470843 0.245472522944 0.825181182401 1.16985852225\n",
      "-0.177510029272 0.166748437703 0.811895266636 1.19495467176\n",
      "-0.203008231699 0.194264608482 0.821133233382 1.20499985093\n",
      "-0.17970938012 0.164429977096 0.826295744944 1.19749631028\n",
      "-0.220696751639 0.156125585793 0.821054090977 1.20889120951\n",
      "-0.17119528775 0.155144760343 0.768384749047 1.24650103596\n",
      "-0.166648108084 0.183646142281 0.790525932798 1.15251404043\n",
      "-0.157703686021 0.17077656876 0.773552250175 1.15905895152\n",
      "-0.207592092765 0.224240693153 0.790955315252 1.23163042894\n",
      "-0.222670288147 0.180404669155 0.815902084366 1.18769148845\n",
      "-0.197734000218 0.244389555417 0.788361909222 1.20590307269\n",
      "-0.207209238533 0.223327778295 0.78944208115 1.25530258618\n",
      "-0.145151468541 0.210788203567 0.790643875893 1.18362468001\n",
      "-0.203124352247 0.200943960309 0.81212720527 1.14273682256\n",
      "-0.23118755828 0.180245555668 0.807838208736 1.17533081782\n",
      "-0.182460386534 0.175135521059 0.761860464398 1.15430490717\n",
      "-0.18785776323 0.20554573358 0.795728469636 1.16571770129\n",
      "-0.188654192196 0.231561032674 0.742915617436 1.19825333609\n",
      "-0.195854840117 0.204784794965 0.81104297872 1.21160165405\n",
      "-0.19018384174 0.166892561167 0.810895988817 1.19095612302\n",
      "-0.207156177523 0.217618789392 0.813563657071 1.20420545993\n",
      "-0.237817868765 0.30295007537 0.797762744621 1.32696158919\n",
      "-0.168777898466 0.192977439791 0.789498702276 1.28222076436\n",
      "-0.135242991724 0.165882755086 0.79292946708 1.23631977368\n",
      "-0.188715898018 0.205945817774 0.833391896194 1.25576610863\n",
      "-0.153030004216 0.195233621766 0.824828707721 1.22394574799\n",
      "-0.220294363304 0.218676220701 0.785768389436 1.20397499223\n",
      "-0.181346097108 0.166708292361 0.81712162341 1.20011302447\n",
      "-0.261313775754 0.260458806194 0.814419699602 1.19504915114\n",
      "-0.180697536647 0.187236738963 0.804895823891 1.20003241277\n",
      "-0.191705637011 0.181828615742 0.789623449802 1.20895095094\n",
      "-0.211920658459 0.221181357197 0.78156528131 1.19804755671\n",
      "-0.216672433032 0.203946122508 0.827759187001 1.2372447333\n",
      "-0.240115729485 0.185614590617 0.828698746976 1.18733813409\n",
      "-0.161116817689 0.18826659931 0.79859416183 1.16728378783\n",
      "-0.165206265006 0.147652455508 0.819769394289 1.16294036181\n",
      "-0.178947281177 0.105630009853 0.833167730721 1.1432828661\n",
      "-0.170858998433 0.190796787243 0.812886660313 1.14762688423\n",
      "-0.135696731428 0.183643416498 0.80839302913 1.14353220732\n",
      "-0.182882509747 0.207357698908 0.827859225013 1.17438730841\n",
      "-0.19257252265 0.200662247195 0.794218376339 1.18426519718\n",
      "-0.156227459584 0.193800242828 0.795639946287 1.18571372216\n",
      "-0.12683681295 0.12564504179 0.823253806362 1.15080602677\n",
      "-0.157819042315 0.161871830768 0.832912902773 1.14417604834\n",
      "-0.177616117749 0.213129262412 0.856577961235 1.17295950998\n",
      "-0.143289260228 0.164036832047 0.858143825475 1.14333549221\n",
      "-0.168341574414 0.164142360791 0.844052172108 1.15170380777\n",
      "-0.226111997336 0.261102609526 0.842844862875 1.15843456881\n",
      "-0.158108961741 0.175260988304 0.823291984202 1.10681184126\n",
      "-0.183293286167 0.218488452056 0.796882285868 1.17034134096\n",
      "-0.193391703275 0.19851660939 0.835960219669 1.24395019899\n",
      "-0.198425653579 0.198060407584 0.827544183312 1.19323721047\n",
      "-0.206312264687 0.119165499092 0.83724641483 1.2238889229\n",
      "-0.200056538048 0.182870173527 0.7492683483 1.17654454302\n",
      "-0.184769565932 0.197853159454 0.784489770247 1.14489448171\n",
      "-0.115881279829 0.132506831383 0.842572528651 1.1716451682\n",
      "-0.161969665038 0.184588064586 0.812995026271 1.17522728024\n",
      "-0.193202473903 0.180535442934 0.771145817011 1.19923306568\n",
      "-0.134248522304 0.183074900463 0.81460703287 1.1880752381\n",
      "-0.189277312079 0.189652221243 0.812808082495 1.24059893339\n",
      "-0.167952224712 0.196646897975 0.83550874123 1.30697710125\n",
      "-0.191129229188 0.171686869074 0.829240791258 1.18465786165\n",
      "-0.140515003467 0.141919103325 0.830869725359 1.30030099863\n",
      "-0.163653188589 0.226462739534 0.835532520083 1.21175594828\n",
      "-0.172813192899 0.187271238064 0.860274455916 1.18638756055\n",
      "-0.237829510753 0.188091032056 0.828102511962 1.1893904642\n",
      "-0.165814699338 0.198282108058 0.833766756319 1.2825453728\n",
      "-0.205944397634 0.240218328983 0.832485395395 1.22420937184\n",
      "-0.185782097212 0.242002463662 0.824195511801 1.26149391204\n",
      "-0.25834466104 0.231498067757 0.849170319281 1.22926669734\n",
      "-0.213261263299 0.232647662979 0.854310369512 1.19114619062\n",
      "-0.110830801249 0.1317394107 0.829932200276 1.2278985522\n",
      "-0.212001708906 0.179470335404 0.853616621941 1.22994657992\n",
      "-0.269948895595 0.246402706515 0.811299714027 1.17943898134\n",
      "-0.178956152378 0.141332421461 0.788187462655 1.13413679366\n",
      "-0.186458891977 0.143558514261 0.799003914294 1.1384799274\n",
      "-0.20622537266 0.230617092636 0.797888246802 1.15711204413\n",
      "-0.179476300535 0.117696783801 0.790616983028 1.14275856508\n",
      "-0.186027187565 0.149407940085 0.847960474191 1.1704419141\n",
      "-0.24226857263 0.19401689719 0.784567565126 1.13720822423\n",
      "-0.155751728869 0.188530982186 0.785673218603 1.1421929614\n",
      "-0.24476694041 0.330299714402 0.821297186091 1.18396477603\n",
      "-0.171225385563 0.293010871611 0.81772555575 1.20973011732\n",
      "-0.229780552917 0.184084263068 0.803232840143 1.16548926805\n",
      "-0.184725168002 0.196481581876 0.821386704773 1.23440512241\n",
      "-0.128744403951 0.115919270712 0.810278783614 1.24641028087\n",
      "-0.13778563893 0.150766316494 0.803652924887 1.24806239537\n",
      "-0.140067182016 0.196062374063 0.82938330926 1.27366019108\n",
      "-0.207108749161 0.214220109185 0.841766545971 1.242239024\n",
      "-0.178276489421 0.214451492006 0.859182247566 1.20441591748\n",
      "-0.189805517208 0.25601544935 0.858034356437 1.21421467562\n",
      "-0.114761010749 0.201927161844 0.855888332673 1.19294557433\n",
      "-0.179018240797 0.219037607506 0.828070760784 1.17940542421\n",
      "-0.134623933876 0.154206281108 0.843383395399 1.16963712212\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000237026032164 0.000949473650137\n"
     ]
    }
   ],
   "source": [
    "# Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "import numpy as np\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)\n",
    "\n",
    "du = 0.001\n",
    "u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# print (u_new-u_old) / du\n",
    "print(u_old, u_new)\n",
    "# Now I see your problem: \n",
    "#     You do not consider the effect of the weights. \n",
    "#     From one layer to the next, we have two influences: \n",
    "#         (1) multiplication with weights and \n",
    "#         (2) applying the SELU. \n",
    "#         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "#         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "#         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# Oh yes, thats true, zero mean weights completely kill the mean. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.202087716961 0.31848265518 0.921132446515 1.34398562218\n",
      "-0.224589444414 0.249901125861 0.979570395886 1.46352922135\n",
      "-0.231546855393 0.435013162508 1.04231740103 1.69483860123\n",
      "-0.220956906992 0.426276400395 1.10515576116 1.73862261421\n",
      "-0.150214064727 0.32243380693 1.15166428537 1.79592882927\n",
      "-0.196446409815 0.415526248206 1.11515227107 1.93494812263\n",
      "-0.249493613731 0.449090445479 1.15793425926 1.96928876524\n",
      "-0.172417897284 0.449128740856 1.20701340274 2.14773203716\n",
      "-0.207781721184 0.461443690264 1.21460613652 2.10236102509\n",
      "-0.165799063507 0.472724604454 1.25855086499 2.20787750648\n",
      "-0.165732280502 0.521963847454 1.26382439544 2.45992926742\n",
      "-0.134203731115 0.509548076499 1.20292069413 2.27706815833\n",
      "-0.128702694999 0.57964976766 1.15960113226 2.41472515484\n",
      "-0.162889987756 0.522153919153 1.27461349376 2.23938190747\n",
      "-0.184802914837 0.576010399855 1.29320022619 2.37270592145\n",
      "-0.18455431705 0.453381752338 1.2613018499 2.29176466605\n",
      "-0.106126902921 0.531980795461 1.24289005743 2.09062466464\n",
      "-0.264286616537 0.496057869422 1.22698523008 2.05908489374\n",
      "-0.208083516973 0.51493183315 1.25129962271 2.17325989695\n",
      "-0.13341759389 0.620533690751 1.24750431539 2.32942012199\n",
      "-0.146531319682 0.609281517961 1.27166520298 2.09206027916\n",
      "-0.148925626315 0.623139541188 1.31709906548 2.2068607201\n",
      "-0.126926790243 0.472106156321 1.25349948084 2.23452217975\n",
      "-0.17779487429 0.564931830821 1.29186715975 2.2665848422\n",
      "-0.14160169295 0.664268784578 1.34704163512 2.25695311715\n",
      "-0.134789570478 0.453739008873 1.27178093124 2.42325776375\n",
      "-0.192434083183 0.552188397278 1.29132549386 2.25974297062\n",
      "-0.126004469703 0.533358493251 1.30372846026 2.30452119806\n",
      "-0.132519679992 0.464626510775 1.26367111582 2.09659089396\n",
      "-0.251237079667 0.558005460184 1.27828144189 2.17417595572\n",
      "-0.100815409041 0.591246602522 1.31014143719 2.26267508981\n",
      "-0.209636396821 0.625074345101 1.21968598069 2.22395756646\n",
      "-0.159592889981 0.529409323777 1.18025865023 2.30385232927\n",
      "-0.109751358095 0.540030946914 1.30470402761 2.31060964228\n",
      "-0.180504510408 0.640044719054 1.24289542995 2.14731299145\n",
      "-0.186312077664 0.563526826187 1.35982021888 2.22470457977\n",
      "-0.186022319903 0.493461671745 1.27837591646 2.21897926075\n",
      "-0.159345695501 0.64786260874 1.30450497241 2.39870788565\n",
      "-0.212615988353 0.454938446331 1.33400607742 2.30471151099\n",
      "-0.284262023633 0.477650996194 1.31988259445 2.24119069862\n",
      "-0.172008183523 0.420002996404 1.29549701232 2.26090597527\n",
      "-0.156179605244 0.472593833689 1.23062653274 2.20770820642\n",
      "-0.118123670405 0.522550284162 1.3261130062 2.16241219911\n",
      "-0.170198648657 0.518048104076 1.31896027653 2.05267454183\n",
      "-0.169385501696 0.45639489597 1.27628765163 2.18899162086\n",
      "-0.224363689409 0.619934324107 1.33192854438 2.26112900887\n",
      "-0.150037980999 0.507509337618 1.30090884211 2.08491112253\n",
      "-0.35886262087 0.491409287128 1.28930125532 2.20274481549\n",
      "-0.230266937605 0.546032527494 1.27257752593 2.14115615741\n",
      "-0.119452260332 0.508475526487 1.29218100742 2.12995506464\n",
      "-0.150749861667 0.589810108072 1.23773725135 2.15517467717\n",
      "-0.152318138259 0.524551434348 1.33677132736 2.2433096882\n",
      "-0.138418013405 0.476235783346 1.28599574556 2.24930717508\n",
      "-0.119951573597 0.54236896586 1.23061560824 2.19715080828\n",
      "-0.205329761616 0.527155203765 1.2917089241 2.15066639512\n",
      "-0.135601255097 0.508401558012 1.263249614 2.20519991355\n",
      "-0.133584459036 0.591830760024 1.27347377614 2.21624262123\n",
      "-0.172026651033 0.452791255934 1.22682845415 2.35263801944\n",
      "-0.235097106398 0.529842431297 1.25891002734 2.21289194198\n",
      "-0.202335116948 0.676746662254 1.285669692 2.48219058715\n",
      "-0.12743444109 0.665145621226 1.18476284095 2.48059505779\n",
      "-0.134616329281 0.537063120295 1.19945317091 2.30792517453\n",
      "-0.103863342824 0.597642940003 1.31809637269 2.29758961779\n",
      "-0.108087815417 0.546273385567 1.27090934048 2.20857535734\n",
      "-0.171385308864 0.672479331552 1.29651240155 2.2656451715\n",
      "-0.152906903702 0.666770042676 1.27411200286 2.15720849947\n",
      "-0.177770758135 0.526347591557 1.27151656009 2.18083857651\n",
      "-0.2197008198 0.665844075516 1.28526152643 2.12143218083\n",
      "-0.166926430725 0.582165176635 1.2169490601 2.23715101426\n",
      "-0.166316658272 0.483356137905 1.23526725243 2.13149583519\n",
      "-0.194382586586 0.448325208416 1.23163699126 2.10350354221\n",
      "-0.194931292802 0.552092349928 1.19529603579 2.02778287492\n",
      "-0.246625655157 0.454550481618 1.21134883226 2.01926853464\n",
      "-0.180848829188 0.477187185248 1.17109444535 2.08577496404\n",
      "-0.183867189218 0.626451093916 1.21046673062 2.18287657049\n",
      "-0.178128836466 0.6322980769 1.2233719027 2.15610504206\n",
      "-0.157354410009 0.589542210009 1.26490483528 2.11696919545\n",
      "-0.118220507166 0.537701057202 1.31831474788 2.15514929064\n",
      "-0.170836976544 0.530823256261 1.31779078847 2.12957423855\n",
      "-0.115477562552 0.559052820507 1.32613102678 2.14516347612\n",
      "-0.260166645641 0.508798601152 1.22180964416 2.15215443967\n",
      "-0.218479111211 0.646986966338 1.1809898723 2.2671433502\n",
      "-0.230264194112 0.556316650019 1.1546747533 2.27102867772\n",
      "-0.163271357631 0.640173215663 1.2050834537 2.26588809698\n",
      "-0.159725558199 0.517346964989 1.26111588074 2.19382180451\n",
      "-0.109673597563 0.565733413296 1.14965884056 2.24113737001\n",
      "-0.173741056287 0.58086062592 1.21838713763 2.20078220202\n",
      "-0.171344062826 0.490258093511 1.29864623273 2.14189514183\n",
      "-0.123615962661 0.555638288704 1.19288987911 2.18524291749\n",
      "-0.121593010814 0.59965394288 1.27173491585 2.1806336437\n",
      "-0.18516851206 0.608575855318 1.33835596465 2.29351666933\n",
      "-0.119016404039 0.550402047439 1.27513398685 2.22927459803\n",
      "-0.125824968809 0.684861142596 1.22025055193 2.22515932494\n",
      "-0.126560690346 0.517324104002 1.19789634894 2.1541335686\n",
      "-0.156096238299 0.439820731126 1.19919071096 2.13080091041\n",
      "-0.159370991187 0.508225863526 1.22544663277 2.04291555859\n",
      "-0.144811531127 0.512376000738 1.30437967211 2.06672017751\n",
      "-0.195679996938 0.512809736041 1.31777502262 2.25879180919\n",
      "-0.219151027948 0.595576929898 1.28857415613 2.23384003167\n",
      "-0.123186130515 0.497336344658 1.24937979242 2.16145275041\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "    \n",
    "    #     keep_prob = 1.0 - p_dropout # keep_prob==p_dropout, 1-rate for dropout, 80% is keep_prob\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.105856907841 0.279570434562 0.86660190467 1.20251243972\n",
      "-0.155191664383 0.288261515002 0.889275976739 1.25138699752\n",
      "-0.0705334706002 0.344630660594 0.951567828874 1.31551801341\n",
      "-0.132482524009 0.353791403522 0.938117529417 1.37189346045\n",
      "-0.113616815796 0.306603972222 0.956869698278 1.35072670805\n",
      "-0.0918661701641 0.325162981366 0.950788678996 1.42381428375\n",
      "-0.0869468239639 0.324479072308 0.961812747501 1.41067656968\n",
      "-0.0606090730805 0.351917586035 0.972776951234 1.38533958476\n",
      "-0.10872708601 0.383740546293 1.02475160705 1.40877286324\n",
      "-0.183687697566 0.388829816808 0.985993555097 1.45597050914\n",
      "-0.183079470999 0.353045997825 1.04372547392 1.43863367183\n",
      "-0.101662083454 0.424148949162 0.99622128851 1.40205418122\n",
      "-0.0976699484971 0.387634760108 1.04357712112 1.40889933699\n",
      "-0.173219181539 0.327215656716 0.983300254122 1.40398571873\n",
      "-0.10727509687 0.343160040971 0.976136432118 1.39255829901\n",
      "-0.143136983458 0.379707702698 0.983961253262 1.42320519678\n",
      "-0.0508431911137 0.367824686636 1.01587006938 1.37922871719\n",
      "-0.14423430431 0.388801946984 1.01999378999 1.42200632674\n",
      "-0.0907774778577 0.350282938544 1.0272181687 1.40841459161\n",
      "-0.099780321049 0.332181176705 0.995668407528 1.39780248066\n",
      "-0.0936452039116 0.32894033908 1.01242094086 1.3856004105\n",
      "-0.12873050587 0.374621821742 0.983475315891 1.40277822077\n",
      "-0.191174660794 0.363091845041 0.981914945408 1.379078248\n",
      "-0.0886902784812 0.378142399963 0.993598177827 1.48819147555\n",
      "-0.0832197066663 0.421642898601 1.01124529103 1.44319012462\n",
      "-0.0757957105157 0.368033507942 0.980818252779 1.43007606514\n",
      "-0.155626439054 0.456626029886 1.02057722158 1.47238522294\n",
      "-0.13907825406 0.43443071019 1.03309231025 1.43898395803\n",
      "-0.101614273149 0.289890367685 1.03741811533 1.44021545664\n",
      "-0.158100893274 0.34407726894 0.985793012113 1.40981143567\n",
      "-0.0701975862664 0.332641543615 1.02068130247 1.40924755362\n",
      "-0.180669480582 0.324776043596 0.98858508761 1.41116676755\n",
      "-0.144297664699 0.323693510728 1.01214463998 1.42303285488\n",
      "-0.129679786563 0.398597868849 1.0052938952 1.49103808982\n",
      "-0.0873110418352 0.394553398371 0.954948765636 1.41439308359\n",
      "-0.0833427391268 0.368808169537 0.967462359055 1.49489344261\n",
      "-0.183694143344 0.423474347522 1.00367629477 1.43491857586\n",
      "-0.15875615821 0.348240621974 0.991233086461 1.42182484976\n",
      "-0.172169123323 0.411562552902 1.03271083239 1.52669679608\n",
      "-0.131316568816 0.382693762298 0.982302279505 1.4501830085\n",
      "-0.075134209026 0.387407500492 1.01872296119 1.48454766722\n",
      "-0.112615839756 0.373605006582 0.995803812218 1.37288995574\n",
      "-0.131826778899 0.394609610503 1.00460209961 1.40744238525\n",
      "-0.169122951325 0.4258099927 0.96974594922 1.41247418143\n",
      "-0.0825781648669 0.509766812369 0.964437678741 1.41697197251\n",
      "-0.135105023614 0.412983432631 1.01915130921 1.4259438232\n",
      "-0.103560688513 0.397883045316 1.01517424784 1.45959356754\n",
      "-0.125616463892 0.394685981194 0.992629653133 1.44586571341\n",
      "-0.0744499856107 0.330701551279 1.0073968316 1.42884043904\n",
      "-0.112086566539 0.461213821608 1.02680823013 1.41036959406\n",
      "-0.0683232029272 0.415318961781 0.98921632883 1.43123472777\n",
      "-0.0974113546815 0.35480116942 0.927949237315 1.42034489314\n",
      "-0.10938341159 0.348173614166 1.03166227013 1.39812704891\n",
      "-0.0913947266023 0.424016562266 1.00640765121 1.4456213709\n",
      "-0.151547167669 0.375111688761 0.973024250394 1.34886171405\n",
      "-0.0777966301288 0.41879048643 0.999395129882 1.50318125896\n",
      "-0.25348942241 0.369687255433 0.991157084801 1.49277216003\n",
      "-0.0683190840521 0.389622871533 0.98639904515 1.47990848486\n",
      "-0.136972851831 0.383826766531 0.981227940035 1.44657564592\n",
      "-0.0641828984764 0.484222943788 1.01510496922 1.45478465783\n",
      "-0.0844447966704 0.35889414275 1.00462362477 1.4485787834\n",
      "-0.0730183404051 0.34543067689 0.997364040923 1.48926101447\n",
      "-0.107004172468 0.413684057673 0.965351045204 1.37999125561\n",
      "-0.129072058851 0.304427542226 0.957689381905 1.43603198146\n",
      "-0.138968972847 0.306324394695 0.96119888205 1.39978112736\n",
      "-0.0996327059157 0.351304758145 0.991324519161 1.41695139121\n",
      "-0.140221956903 0.495096440958 0.982916618683 1.38062793333\n",
      "-0.119523528985 0.348477715503 0.927304418921 1.43216760961\n",
      "-0.11022064975 0.333872043417 0.990328675119 1.40044565624\n",
      "-0.15644121314 0.492777488171 0.958113971841 1.38370938876\n",
      "-0.189592801559 0.351934555252 0.957329537299 1.44285121305\n",
      "-0.0821794319732 0.392936284259 0.982373472803 1.41054397771\n",
      "-0.0855939517805 0.326108248831 0.972171783566 1.45399513407\n",
      "-0.165575320128 0.341333445593 0.984403137319 1.43103780888\n",
      "-0.0424847030958 0.357231923458 1.00037256186 1.45877302312\n",
      "-0.116997066308 0.384462729279 0.993518052302 1.41484951258\n",
      "-0.0898684058926 0.371627036776 0.992045561631 1.43922008319\n",
      "-0.203130313364 0.452882528568 0.992809247348 1.49552515749\n",
      "-0.0985609541459 0.397392086513 1.03009933392 1.51128938445\n",
      "-0.113378401239 0.320506315108 0.971137733725 1.47117932868\n",
      "-0.127603695299 0.304810997216 0.926319655819 1.4104805476\n",
      "-0.143188051138 0.386893615517 0.926586150839 1.39911462329\n",
      "-0.0852358212442 0.332782040463 1.00009370527 1.4388211355\n",
      "-0.119066467986 0.254370442886 0.97063037884 1.41456426151\n",
      "-0.0887582625221 0.327823585075 0.986784534029 1.4594690694\n",
      "-0.0883587589969 0.378394729736 0.966069865407 1.45146185658\n",
      "-0.132589935673 0.372382619837 0.999832216471 1.43604512686\n",
      "-0.137410934219 0.350580785545 1.00009007944 1.41428223337\n",
      "-0.102292269693 0.392804278498 0.96901478357 1.44370776101\n",
      "-0.121985176391 0.38473378992 0.980908624338 1.40991999336\n",
      "-0.0790669259954 0.412573225024 0.992611651881 1.42024431168\n",
      "-0.183119087131 0.370691598845 0.980644397384 1.42124218189\n",
      "-0.109457308819 0.315651641014 1.01042559043 1.39320295873\n",
      "-0.13924534772 0.346502412743 1.00231558981 1.38659531556\n",
      "-0.185705071067 0.383301792267 1.022817748 1.42403681746\n",
      "-0.127505355351 0.359614878189 0.991047622109 1.43392534727\n",
      "-0.104511415126 0.446705639569 0.997236512961 1.41871757088\n",
      "-0.0744350016982 0.289508812316 0.99331770957 1.40164892106\n",
      "-0.125138387328 0.34950112854 1.00604459427 1.39352372356\n",
      "-0.0520981511208 0.376654334717 0.988266724716 1.40372206665\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = m * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return X_pos + X_neg_exp\n",
    "\n",
    "def elu_bwd(X, dX):\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    m_neg_exp = m * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return dX * m_neg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def selu_bwd(dX, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dX = dX * scale\n",
    "    dX_neg = dX.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dX.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.192573130677 0.225464851152 0.8517251468 1.17343643636\n",
      "-0.181990439693 0.19236772977 0.830614924998 1.16460880061\n",
      "-0.156997657224 0.180271015029 0.828943363818 1.16442619197\n",
      "-0.199104746591 0.22398155216 0.796812820533 1.2104985597\n",
      "-0.173998806034 0.170736138442 0.779529801004 1.19198148117\n",
      "-0.219109562845 0.156020176235 0.770437549522 1.212449289\n",
      "-0.18967747706 0.221089878525 0.758048489469 1.23475467222\n",
      "-0.223147408747 0.194725050129 0.820239419803 1.21362171248\n",
      "-0.17199878824 0.166789833841 0.837366033913 1.21879645082\n",
      "-0.212695078258 0.297249892012 0.824467642147 1.21928508349\n",
      "-0.206781718701 0.183024553619 0.850118701275 1.22015661902\n",
      "-0.193197569119 0.230625786613 0.82600270733 1.18500910834\n",
      "-0.163987888743 0.171045279072 0.807928639787 1.17680909555\n",
      "-0.20329546972 0.243542309672 0.843773195816 1.16241449782\n",
      "-0.222863285566 0.209524992637 0.832198097308 1.14877634514\n",
      "-0.213679413185 0.196324396734 0.820204317897 1.18841301673\n",
      "-0.141203904392 0.215977872027 0.823700795761 1.22869621263\n",
      "-0.17519784277 0.158848571283 0.814101882395 1.18890987759\n",
      "-0.184178703777 0.202934871312 0.845279084522 1.20320179179\n",
      "-0.17443215272 0.216330907769 0.808508473833 1.19795740906\n",
      "-0.151028771474 0.21761994517 0.832283283778 1.19595374281\n",
      "-0.173321858802 0.196361696544 0.847027911461 1.21708862855\n",
      "-0.147361972794 0.178461972109 0.839563272027 1.18628878579\n",
      "-0.17315748284 0.204504844374 0.825049432785 1.19807374423\n",
      "-0.163660193111 0.201485988201 0.85173375359 1.20129316235\n",
      "-0.134132096331 0.160266996746 0.811296396795 1.15893493704\n",
      "-0.178347073695 0.162555890524 0.822835164355 1.18734141342\n",
      "-0.2075659661 0.211796165013 0.811277142954 1.18340023456\n",
      "-0.214607453273 0.190838365409 0.813193699212 1.18882909064\n",
      "-0.187860127582 0.216788538445 0.822626622502 1.16554074629\n",
      "-0.158013961805 0.159457665197 0.813178306997 1.1889781034\n",
      "-0.17168872351 0.14096583262 0.828379697472 1.23340329884\n",
      "-0.193202650213 0.176233259162 0.832912634018 1.20112744909\n",
      "-0.224188397425 0.223296955946 0.810882578932 1.2349523553\n",
      "-0.219401351688 0.207237738028 0.815647455273 1.19485163246\n",
      "-0.143033719043 0.168380455037 0.82336194186 1.20493342817\n",
      "-0.153391850015 0.10411474563 0.803693167988 1.15118376231\n",
      "-0.104541984863 0.116179014503 0.82954614583 1.15992009024\n",
      "-0.189063785321 0.133048318582 0.844311825279 1.18492036716\n",
      "-0.128988166085 0.139889770503 0.830169519913 1.19938855242\n",
      "-0.214566475296 0.189757971491 0.802237023821 1.20795078386\n",
      "-0.143037589227 0.135077199174 0.781558982451 1.1540849175\n",
      "-0.19478943273 0.260720598042 0.827015085274 1.15142064625\n",
      "-0.161399538685 0.15365516658 0.766089402059 1.20414367495\n",
      "-0.212756949079 0.18696545128 0.727323934638 1.17279179581\n",
      "-0.210114201626 0.215006868952 0.765061887944 1.18409963866\n",
      "-0.112734227528 0.115407958946 0.851174603501 1.20691699789\n",
      "-0.256938217342 0.263200507384 0.795772041404 1.23985304852\n",
      "-0.200884204729 0.202748300765 0.815976826207 1.30726489113\n",
      "-0.193561073042 0.202564890962 0.858167040089 1.28504425136\n",
      "-0.187491832486 0.253281349227 0.806541097687 1.35409646864\n",
      "-0.173432071844 0.179726514543 0.822432535738 1.21063726951\n",
      "-0.159161734079 0.162723591476 0.837557101438 1.20532265195\n",
      "-0.179594235726 0.292054222336 0.788048835395 1.18995867765\n",
      "-0.169968907855 0.190945500111 0.865380452272 1.18887697679\n",
      "-0.151438019338 0.210544577728 0.832513704364 1.20515477636\n",
      "-0.157184016022 0.154802196408 0.816467240017 1.20537250909\n",
      "-0.16887418506 0.16733237814 0.836465371176 1.14835495281\n",
      "-0.182010336786 0.174594961912 0.794838248412 1.20492937472\n",
      "-0.172624990499 0.249964799628 0.783454667798 1.21901842135\n",
      "-0.212551613852 0.188440688126 0.779835791714 1.21400819228\n",
      "-0.164845970196 0.281912310376 0.791614281805 1.18999302972\n",
      "-0.13916132736 0.142270697548 0.813898236058 1.15865283584\n",
      "-0.189849273688 0.232860104041 0.842109926097 1.12261968686\n",
      "-0.145285323792 0.211184037421 0.783695609601 1.15036187936\n",
      "-0.115695204026 0.175854738737 0.774579294736 1.1592509854\n",
      "-0.142791647492 0.162176891228 0.792264290237 1.19568133032\n",
      "-0.215990875502 0.216096198167 0.809231830407 1.17284275637\n",
      "-0.142843678937 0.121106808242 0.755349775729 1.18896984718\n",
      "-0.156363669101 0.184421864773 0.793909027612 1.20320343454\n",
      "-0.219301655729 0.248401927574 0.809234801712 1.23441584595\n",
      "-0.145008656405 0.224794801883 0.781205837088 1.24093652009\n",
      "-0.222019954711 0.230018238374 0.719891775034 1.21300662284\n",
      "-0.133269349401 0.12287208421 0.775259937948 1.20665962476\n",
      "-0.163166891915 0.154594812755 0.824715730706 1.26935254162\n",
      "-0.178973051973 0.138548652523 0.753574657086 1.16056211923\n",
      "-0.279017051688 0.210223655918 0.774293170084 1.14188954169\n",
      "-0.163234613288 0.162473983186 0.839588022155 1.16322664578\n",
      "-0.236488116978 0.284203185451 0.849231362667 1.19437537537\n",
      "-0.311784428538 0.209947711856 0.822299327216 1.16678860229\n",
      "-0.169859379449 0.191167808504 0.844718649672 1.20022245392\n",
      "-0.208800563549 0.21828843524 0.825719107431 1.2430666696\n",
      "-0.133480338644 0.193123155785 0.83861737149 1.22841705487\n",
      "-0.187932891855 0.243658219156 0.827356816012 1.19046196275\n",
      "-0.185737865954 0.194087718723 0.804246720761 1.15841343134\n",
      "-0.173063785851 0.177944186695 0.835172937955 1.20051892974\n",
      "-0.27912689108 0.239750921854 0.826517829544 1.16464462598\n",
      "-0.201078477977 0.180501142281 0.815432887091 1.17304875776\n",
      "-0.159107742191 0.172435874131 0.824536583996 1.13157286715\n",
      "-0.166422307121 0.187829016879 0.786372575077 1.17795798142\n",
      "-0.147527578546 0.186678346507 0.779563100203 1.1736355097\n",
      "-0.255222588451 0.266211440415 0.814796888992 1.20089871181\n",
      "-0.202456396353 0.218616088757 0.837447176522 1.22596604483\n",
      "-0.269257100985 0.192130997373 0.800922361518 1.17412431159\n",
      "-0.106035367793 0.132268195289 0.823229981929 1.18469795718\n",
      "-0.21608671482 0.141019422021 0.776661914802 1.15852060849\n",
      "-0.166355481031 0.122716665909 0.787468337112 1.15454255487\n",
      "-0.209712149234 0.227753823594 0.756480907334 1.17472071903\n",
      "-0.189498641007 0.193560067706 0.762585606527 1.19198336247\n",
      "-0.198339288141 0.203299536863 0.782620296129 1.16358963307\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
