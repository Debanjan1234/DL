{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of SELUs\n",
    "This looks pretty neat. \n",
    "They can prove that when you slightly modify the ELU activation,\n",
    "your average unit activation goes towards zero mean/unit variance (if the network is deep enough). \n",
    "If they're right, this might make batch norm obsolete, which would be a huge bon to training speeds! \n",
    "\n",
    "The experiments look convincing, so apparently it even beats BN+ReLU in accuracy... though \n",
    "\n",
    "I wish they would've shown the resulting distributions of activations after training. \n",
    "\n",
    "But assuming their fixed point proof is true, it will. \n",
    "\n",
    "Still, still would've been nice if they'd shown it -- maybe they ran out of space in their appendix ;)\n",
    "\n",
    "Weirdly, the exact ELU modification they proposed isn't stated explicitly in the paper! \n",
    "\n",
    "For those wondering, it can be found in the available sourcecode, and looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "#     return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)    \n",
    "#     return scale * np.maximum(0.0, alpha*np.exp(x)-alpha)\n",
    "#     return scale * alpha * np.where(x>=0.0, x, np.exp(x)-1)\n",
    "    return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.200722364762 0.235950132364 0.818004519901 1.16539839964\n",
      "-0.210238205243 0.187069807831 0.784208044793 1.22777505589\n",
      "-0.213019950326 0.218840106053 0.789525660229 1.20909991307\n",
      "-0.184370124578 0.21100906431 0.733278396959 1.18539363081\n",
      "-0.16420170663 0.165623749388 0.805945248365 1.16962160607\n",
      "-0.180324213651 0.201006020128 0.749119701788 1.16372995476\n",
      "-0.195055732409 0.22069681423 0.808945533697 1.2093679595\n",
      "-0.164405351753 0.172975739953 0.817439295592 1.22114119378\n",
      "-0.165494331352 0.143841341237 0.754631370389 1.1943995735\n",
      "-0.182485648488 0.164054628488 0.804435157556 1.24141562022\n",
      "-0.182433413818 0.277047163508 0.815243635519 1.17651074657\n",
      "-0.175020743306 0.213344661547 0.767409251202 1.17094594923\n",
      "-0.171120811426 0.230726156082 0.777606430223 1.21669188717\n",
      "-0.171321758567 0.180036006853 0.816664302775 1.18855729342\n",
      "-0.218515470657 0.223162611775 0.819726574405 1.24600830499\n",
      "-0.222653085437 0.180430717623 0.834509675591 1.25733031225\n",
      "-0.1753920658 0.206527941233 0.816119854501 1.2450395178\n",
      "-0.165408433732 0.216743688937 0.787074116119 1.24915071296\n",
      "-0.138365519607 0.179148639239 0.798735200974 1.1803835281\n",
      "-0.135906920496 0.171983887539 0.768371621806 1.2083556801\n",
      "-0.25997394149 0.359988748794 0.753729744875 1.24506546226\n",
      "-0.20478268684 0.257501023989 0.820106774384 1.24799106766\n",
      "-0.224887512817 0.21073155023 0.786609048295 1.27292272409\n",
      "-0.232038920328 0.325230668139 0.789216954257 1.16722374166\n",
      "-0.143904198425 0.124378137113 0.792631073032 1.1786216017\n",
      "-0.229491875018 0.221877696525 0.809531914172 1.18172086513\n",
      "-0.243087330034 0.17194462111 0.812504788772 1.15459797893\n",
      "-0.177613481605 0.213545732021 0.823001615441 1.17239500172\n",
      "-0.159012773137 0.1568048133 0.796863801273 1.21711551\n",
      "-0.15742938913 0.198747196725 0.812428795446 1.20657075408\n",
      "-0.137396680038 0.171362887912 0.779754630935 1.2435077461\n",
      "-0.173167144827 0.201004662593 0.822251753743 1.19229606804\n",
      "-0.208686148372 0.194648320541 0.841600382072 1.20308002608\n",
      "-0.225825177567 0.213112030842 0.822544359515 1.23281031093\n",
      "-0.229114567458 0.281172872508 0.824381254504 1.19660049463\n",
      "-0.163964611807 0.143479930162 0.817631607427 1.18672890747\n",
      "-0.253932798316 0.206712737797 0.812658315476 1.18921834889\n",
      "-0.199076192576 0.136034788182 0.793369526625 1.14963902423\n",
      "-0.154697144788 0.148371256783 0.802666616789 1.1713311834\n",
      "-0.16960514046 0.180836787898 0.821309538677 1.16531926402\n",
      "-0.194439003616 0.153432114303 0.80586758344 1.16488315641\n",
      "-0.183840646183 0.171928540303 0.801992567684 1.1422670431\n",
      "-0.161789508629 0.134132401644 0.804618623667 1.20122483105\n",
      "-0.123385057517 0.114675409127 0.805065587982 1.20398354765\n",
      "-0.19627861423 0.203075214339 0.779065610691 1.09696843489\n",
      "-0.184272855768 0.196298404632 0.788312622288 1.10923104175\n",
      "-0.150385285962 0.186060794466 0.737864048319 1.13765415103\n",
      "-0.183954519436 0.190513527781 0.81352696358 1.1726355972\n",
      "-0.143364701939 0.194623890374 0.755262517806 1.11165372827\n",
      "-0.176876861039 0.136048776586 0.787403391425 1.11934737144\n",
      "-0.167269402073 0.182039482153 0.814804762695 1.13582167883\n",
      "-0.225037891987 0.319279092698 0.795941469342 1.13742963916\n",
      "-0.184146381717 0.187930477272 0.819158405902 1.2175531982\n",
      "-0.152362687228 0.202577491609 0.840507944137 1.21129805297\n",
      "-0.177639705157 0.183540556306 0.818283183289 1.20772031777\n",
      "-0.188742488021 0.182603061181 0.801963220443 1.13374944245\n",
      "-0.18077049685 0.216686598755 0.751813153447 1.12898742767\n",
      "-0.216986093758 0.192232135954 0.773448758268 1.12099504968\n",
      "-0.172286490325 0.200133006783 0.794325593645 1.12380818889\n",
      "-0.142927591767 0.140560731771 0.819906018606 1.1543210377\n",
      "-0.213472940834 0.230073924964 0.803428547107 1.1441917042\n",
      "-0.133937310844 0.192053017306 0.824515685651 1.18288650441\n",
      "-0.160983588305 0.179083497172 0.81257497673 1.12709623642\n",
      "-0.174285916833 0.169135415974 0.812520149019 1.14536728463\n",
      "-0.183885949926 0.175968583494 0.823076495536 1.16304947714\n",
      "-0.110932048542 0.0967228074294 0.773116197856 1.17018390075\n",
      "-0.208641449058 0.265055781719 0.80778552098 1.16474970744\n",
      "-0.139152136553 0.147944026527 0.77873760469 1.23129229964\n",
      "-0.21291785676 0.207454135101 0.76047857544 1.15851354185\n",
      "-0.152116288363 0.182452683265 0.799902671724 1.15863417601\n",
      "-0.221613409063 0.155864208359 0.823841767521 1.18897617584\n",
      "-0.145916061909 0.143636867974 0.82680956903 1.24166746982\n",
      "-0.151429185158 0.249768828995 0.77593093632 1.19943110971\n",
      "-0.197245340513 0.202666677843 0.793205282145 1.18845905205\n",
      "-0.155398515473 0.195498744628 0.791992431111 1.18849123865\n",
      "-0.216441767394 0.246811527753 0.788874625003 1.1504692099\n",
      "-0.203491410565 0.188312958473 0.84471221189 1.12352002704\n",
      "-0.182862605965 0.133236445478 0.82896112551 1.15271542858\n",
      "-0.17360508905 0.173030312017 0.82896375025 1.15995719776\n",
      "-0.14642135778 0.138108738391 0.839499040685 1.17725528499\n",
      "-0.181172887965 0.243089978909 0.811779829797 1.14817297906\n",
      "-0.22691822791 0.207887634382 0.820730067101 1.17448218429\n",
      "-0.184671946276 0.208146538685 0.844072832995 1.19724624006\n",
      "-0.196132095235 0.168947414699 0.78217398492 1.1969937563\n",
      "-0.192186122967 0.189855950417 0.816429420939 1.17398553044\n",
      "-0.15114034663 0.192826263455 0.829881054468 1.15361465452\n",
      "-0.193543562705 0.238010307441 0.802927847031 1.19775838105\n",
      "-0.192540316984 0.23045219077 0.784537560366 1.23576821988\n",
      "-0.160507866839 0.191988567267 0.79284403697 1.17397172626\n",
      "-0.17328323275 0.187645724901 0.774099865337 1.16957639014\n",
      "-0.342557753595 0.258110642979 0.797847341314 1.186790765\n",
      "-0.231770100544 0.239285412536 0.819448364547 1.15798037649\n",
      "-0.162617110381 0.168514957312 0.832046890493 1.13150486346\n",
      "-0.190468966821 0.189514991013 0.834335347035 1.15071845426\n",
      "-0.21818309838 0.156470653584 0.822570262607 1.13644368236\n",
      "-0.16869000135 0.243926899027 0.814784267741 1.19027557153\n",
      "-0.166400467663 0.131150230891 0.795761458475 1.19628973512\n",
      "-0.179024244244 0.210236146926 0.796021707196 1.22822307343\n",
      "-0.23368127105 0.28596474277 0.82790802078 1.2321760098\n",
      "-0.173999983403 0.143717728582 0.784068077347 1.20561158175\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56013655077e-05 0.00100655640791\n"
     ]
    }
   ],
   "source": [
    "# Thanks, I will double check the analytical solution. For the numerical one, could you please explain why running the following code results in a value close to 1 rather than 0?\n",
    "import numpy as np\n",
    "def selu(x):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    return scale*np.where(x>=0.0, x, alpha*np.exp(x)-alpha)\n",
    "\n",
    "du = 0.001\n",
    "u_old = np.mean(selu(np.random.normal(0,    1, 100000000)))\n",
    "u_new = np.mean(selu(np.random.normal(0+du, 1, 100000000)))\n",
    "# print (u_new-u_old) / du\n",
    "print(u_old, u_new)\n",
    "# Now I see your problem: \n",
    "#     You do not consider the effect of the weights. \n",
    "#     From one layer to the next, we have two influences: \n",
    "#         (1) multiplication with weights and \n",
    "#         (2) applying the SELU. \n",
    "#         (1) has a centering and symmetrising effect (draws mean towards zero) and \n",
    "#         (2) has a variance stabilizing effect (draws variance towards 1). \n",
    "\n",
    "#         That is why we use the variables \\mu&\\omega and \\nu&\\tau to analyze the both effects.\n",
    "# Oh yes, thats true, zero mean weights completely kill the mean. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normal dropout for ReLU\n",
    "def dropout_forward(X, p_dropout):\n",
    "    u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "    out = X * u\n",
    "    cache = u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_backward(dout, cache):\n",
    "    dX = dout * cache\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.293067324719 0.264310233667 0.935668618548 1.31694509107\n",
      "-0.221194391869 0.262546631219 1.01401498655 1.50748084839\n",
      "-0.279087704151 0.308256161849 1.03434864291 1.68582200979\n",
      "-0.170384134029 0.351805467366 1.07493858447 1.66927554414\n",
      "-0.231188362698 0.41709220799 1.10811786703 1.8168548518\n",
      "-0.161114289725 0.414170821621 1.19664855198 1.86511997249\n",
      "-0.187824668416 0.497071976062 1.23137862939 2.02396804287\n",
      "-0.290698649207 0.567651242281 1.18406577403 2.1429348015\n",
      "-0.173369902764 0.768781534065 1.16322081025 2.05823971535\n",
      "-0.17157800897 0.511971173042 1.20160219421 2.0415383258\n",
      "-0.19380373084 0.498882679674 1.25130404218 2.16846882132\n",
      "-0.189364422826 0.584815754069 1.24614115124 2.28789729692\n",
      "-0.157033067074 0.477995593401 1.26047156358 2.11669381717\n",
      "-0.124557524294 0.528771320593 1.25599712751 2.14135708912\n",
      "-0.168523091469 0.460096968838 1.19650395694 2.22530337231\n",
      "-0.215729144557 0.531666959387 1.21453181741 2.4103466735\n",
      "-0.218949084068 0.582117513711 1.29704380387 2.17159564564\n",
      "-0.163466224359 0.570431951743 1.2642754048 2.16463961221\n",
      "-0.22971803285 0.537288331128 1.19762410567 2.09587094312\n",
      "-0.163064348799 0.669183174886 1.23315813514 2.09280851272\n",
      "-0.250785010632 0.535255210773 1.25182589281 2.21080863088\n",
      "-0.201711213743 0.518984765449 1.28628916822 2.17475430857\n",
      "-0.217383175145 0.606099864134 1.2874522756 2.19275247057\n",
      "-0.19503087644 0.811784431844 1.31385323972 2.31814932209\n",
      "-0.183238926965 0.597410260059 1.18435524642 2.42561767776\n",
      "-0.193817851255 0.528248533782 1.24445643347 2.29902084354\n",
      "-0.140381175205 0.518497626232 1.18130974264 2.290577804\n",
      "-0.17361875998 0.68597414662 1.26755988883 2.13896784894\n",
      "-0.254419425012 0.580352508015 1.33737087444 2.38207582265\n",
      "-0.158828536786 0.629843947147 1.29476951521 2.33746330256\n",
      "-0.212213883627 0.598368836115 1.24678799767 2.32353438935\n",
      "-0.143680957674 0.603761794468 1.31910251914 2.32370067754\n",
      "-0.141212862971 0.650422037605 1.23333973854 2.31265574737\n",
      "-0.0935038956572 0.587949981542 1.23126654149 2.20057980989\n",
      "-0.325680462052 0.585629423228 1.31246423019 2.34256589989\n",
      "-0.169019027728 0.693073410615 1.24396226063 2.23096185536\n",
      "-0.0966727703265 0.514474891282 1.32773224603 2.16254687924\n",
      "-0.169423129601 0.491957769756 1.21207051035 2.18778140978\n",
      "-0.234497769323 0.531888737628 1.23462417646 2.17060616366\n",
      "-0.165229941234 0.485338723627 1.26933926772 2.17553596594\n",
      "-0.0708557596434 0.531462562484 1.33157088814 2.29084510102\n",
      "-0.152906617605 0.611530501998 1.30898643179 2.26704471754\n",
      "-0.154370690027 0.539519275692 1.31349260658 2.24060757004\n",
      "-0.224470646483 0.506115337772 1.23917170873 2.21262892616\n",
      "-0.203528081364 0.609412782298 1.2882796698 2.13077831198\n",
      "-0.127762077292 0.489467555946 1.29229313448 2.19535429275\n",
      "-0.201738921449 0.456399040676 1.26916433513 2.01900940276\n",
      "-0.120709049711 0.588016084864 1.32381713011 2.23616204936\n",
      "-0.224203037614 0.453554989225 1.23808339506 2.01008018436\n",
      "-0.187004225311 0.450807258033 1.36114768196 2.10663261824\n",
      "-0.111502508947 0.512664878134 1.28852328325 2.2416816084\n",
      "-0.13012911755 0.575713738028 1.27231361614 2.37466598749\n",
      "-0.133960600676 0.660658038735 1.2517219863 2.23167701327\n",
      "-0.164169412404 0.518214907631 1.28573089807 2.25289064859\n",
      "-0.182093279969 0.582184772137 1.20560302911 2.17110751572\n",
      "-0.205260140596 0.544518466397 1.26300649338 2.19860784703\n",
      "-0.184018085116 0.582623045491 1.26634806785 2.22913207314\n",
      "-0.183042045061 0.578907918451 1.21050377648 2.29400507898\n",
      "-0.165788721759 0.613106266409 1.14329984134 2.22722390445\n",
      "-0.130739666588 0.542039664011 1.22249725124 2.24409758351\n",
      "-0.0790487883845 0.597941005959 1.28932413343 2.21420941431\n",
      "-0.185623312404 0.539216430263 1.31221944353 2.21116775818\n",
      "-0.198615879469 0.548294461007 1.29110529007 2.13470689154\n",
      "-0.133808893753 0.514949521465 1.30761509347 2.08943939174\n",
      "-0.2445704675 0.423681485484 1.23325964463 2.18385846334\n",
      "-0.196146380644 0.617699921824 1.29503071273 2.2258144589\n",
      "-0.206671537547 0.559667376227 1.23242303005 2.20832297689\n",
      "-0.219720273586 0.561573382776 1.26453406254 2.2820844641\n",
      "-0.180102054605 0.572754118649 1.28574964892 2.1532352952\n",
      "-0.141550453311 0.447241443509 1.29279612251 2.16564632716\n",
      "-0.129227598609 0.519474227691 1.18741362208 2.11464846081\n",
      "-0.168271354467 0.790397072616 1.27138883998 2.21661244978\n",
      "-0.155082820695 0.552139674616 1.33966451089 2.30094991082\n",
      "-0.121850424796 0.512300117002 1.31144174259 2.20323033832\n",
      "-0.179124424516 0.537534076605 1.29761221773 2.25119160842\n",
      "-0.0832932175242 0.63007485421 1.31908699615 2.32995750138\n",
      "-0.217846544671 0.582527713529 1.2575406967 2.28243953842\n",
      "-0.134529345076 0.579528800349 1.27684549614 2.19849627667\n",
      "-0.179038195887 0.51474953486 1.31096024345 2.20630741656\n",
      "-0.118468982854 0.616724851636 1.36986970213 2.15643592345\n",
      "-0.181478521715 0.564339468586 1.36756294225 2.36202021359\n",
      "-0.181816600301 0.537079400641 1.29591854968 2.18868519146\n",
      "-0.218352675269 0.553678227948 1.26429140754 2.15465102105\n",
      "-0.150489664152 0.521161324889 1.23892352687 2.22489719825\n",
      "-0.137418447652 0.729619753518 1.19121418784 2.33209183829\n",
      "-0.168598995118 0.439699740066 1.26734127802 2.20359686433\n",
      "-0.270043382463 0.562427806445 1.22465630248 2.25779161692\n",
      "-0.173656210036 0.523316875407 1.23273877071 2.19628628888\n",
      "-0.114916814061 0.506262417367 1.21781250627 2.13452817917\n",
      "-0.267676960569 0.436586705558 1.24776526837 2.05171230652\n",
      "-0.137542823923 0.543435849401 1.1659716356 2.19167790188\n",
      "-0.158022349698 0.432632919482 1.25986110943 2.06283028721\n",
      "-0.198094380848 0.719811678381 1.23183146848 2.095268355\n",
      "-0.19551528932 0.603244435347 1.27234882407 2.14026940678\n",
      "-0.188674433031 0.5305477661 1.23069158443 2.29008757481\n",
      "-0.192477938372 0.529292954365 1.2393529471 2.24162733293\n",
      "-0.17457854986 0.49969542237 1.25590317612 2.22209352352\n",
      "-0.203535594158 0.514134879648 1.22592448964 2.17600175229\n",
      "-0.241563351491 0.505994661312 1.18126002934 2.20740355213\n",
      "-0.209395760238 0.44673236432 1.20108310269 2.10930058181\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_forward(p_dropout=0.8, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0, \n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "            lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "            lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def dropout_selu_forward(X, p_dropout):\n",
    "def dropout_selu_forward(X, keep_prob):\n",
    "    alpha= -1.7580993408473766\n",
    "    fixedPointMean=0.0\n",
    "    fixedPointVar=1.0\n",
    "\n",
    "    u = np.random.binomial(1, keep_prob, size=X.shape) / keep_prob\n",
    "    out = X * u + alpha * (1-u)\n",
    "    \n",
    "    #     keep_prob = 1.0 - p_dropout # keep_prob==p_dropout, 1-rate for dropout, 80% is keep_prob\n",
    "    #     a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * (alpha-fixedPointMean)**2 + fixedPointVar)))\n",
    "    b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    out = a * out + b\n",
    "    cache = a, u\n",
    "    return out, cache\n",
    "\n",
    "def dropout_selu_backward(dout, cache):\n",
    "    a, u = cache\n",
    "    dout = dout * a\n",
    "    dX = dout * u\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.105856907841 0.279570434562 0.86660190467 1.20251243972\n",
      "-0.155191664383 0.288261515002 0.889275976739 1.25138699752\n",
      "-0.0705334706002 0.344630660594 0.951567828874 1.31551801341\n",
      "-0.132482524009 0.353791403522 0.938117529417 1.37189346045\n",
      "-0.113616815796 0.306603972222 0.956869698278 1.35072670805\n",
      "-0.0918661701641 0.325162981366 0.950788678996 1.42381428375\n",
      "-0.0869468239639 0.324479072308 0.961812747501 1.41067656968\n",
      "-0.0606090730805 0.351917586035 0.972776951234 1.38533958476\n",
      "-0.10872708601 0.383740546293 1.02475160705 1.40877286324\n",
      "-0.183687697566 0.388829816808 0.985993555097 1.45597050914\n",
      "-0.183079470999 0.353045997825 1.04372547392 1.43863367183\n",
      "-0.101662083454 0.424148949162 0.99622128851 1.40205418122\n",
      "-0.0976699484971 0.387634760108 1.04357712112 1.40889933699\n",
      "-0.173219181539 0.327215656716 0.983300254122 1.40398571873\n",
      "-0.10727509687 0.343160040971 0.976136432118 1.39255829901\n",
      "-0.143136983458 0.379707702698 0.983961253262 1.42320519678\n",
      "-0.0508431911137 0.367824686636 1.01587006938 1.37922871719\n",
      "-0.14423430431 0.388801946984 1.01999378999 1.42200632674\n",
      "-0.0907774778577 0.350282938544 1.0272181687 1.40841459161\n",
      "-0.099780321049 0.332181176705 0.995668407528 1.39780248066\n",
      "-0.0936452039116 0.32894033908 1.01242094086 1.3856004105\n",
      "-0.12873050587 0.374621821742 0.983475315891 1.40277822077\n",
      "-0.191174660794 0.363091845041 0.981914945408 1.379078248\n",
      "-0.0886902784812 0.378142399963 0.993598177827 1.48819147555\n",
      "-0.0832197066663 0.421642898601 1.01124529103 1.44319012462\n",
      "-0.0757957105157 0.368033507942 0.980818252779 1.43007606514\n",
      "-0.155626439054 0.456626029886 1.02057722158 1.47238522294\n",
      "-0.13907825406 0.43443071019 1.03309231025 1.43898395803\n",
      "-0.101614273149 0.289890367685 1.03741811533 1.44021545664\n",
      "-0.158100893274 0.34407726894 0.985793012113 1.40981143567\n",
      "-0.0701975862664 0.332641543615 1.02068130247 1.40924755362\n",
      "-0.180669480582 0.324776043596 0.98858508761 1.41116676755\n",
      "-0.144297664699 0.323693510728 1.01214463998 1.42303285488\n",
      "-0.129679786563 0.398597868849 1.0052938952 1.49103808982\n",
      "-0.0873110418352 0.394553398371 0.954948765636 1.41439308359\n",
      "-0.0833427391268 0.368808169537 0.967462359055 1.49489344261\n",
      "-0.183694143344 0.423474347522 1.00367629477 1.43491857586\n",
      "-0.15875615821 0.348240621974 0.991233086461 1.42182484976\n",
      "-0.172169123323 0.411562552902 1.03271083239 1.52669679608\n",
      "-0.131316568816 0.382693762298 0.982302279505 1.4501830085\n",
      "-0.075134209026 0.387407500492 1.01872296119 1.48454766722\n",
      "-0.112615839756 0.373605006582 0.995803812218 1.37288995574\n",
      "-0.131826778899 0.394609610503 1.00460209961 1.40744238525\n",
      "-0.169122951325 0.4258099927 0.96974594922 1.41247418143\n",
      "-0.0825781648669 0.509766812369 0.964437678741 1.41697197251\n",
      "-0.135105023614 0.412983432631 1.01915130921 1.4259438232\n",
      "-0.103560688513 0.397883045316 1.01517424784 1.45959356754\n",
      "-0.125616463892 0.394685981194 0.992629653133 1.44586571341\n",
      "-0.0744499856107 0.330701551279 1.0073968316 1.42884043904\n",
      "-0.112086566539 0.461213821608 1.02680823013 1.41036959406\n",
      "-0.0683232029272 0.415318961781 0.98921632883 1.43123472777\n",
      "-0.0974113546815 0.35480116942 0.927949237315 1.42034489314\n",
      "-0.10938341159 0.348173614166 1.03166227013 1.39812704891\n",
      "-0.0913947266023 0.424016562266 1.00640765121 1.4456213709\n",
      "-0.151547167669 0.375111688761 0.973024250394 1.34886171405\n",
      "-0.0777966301288 0.41879048643 0.999395129882 1.50318125896\n",
      "-0.25348942241 0.369687255433 0.991157084801 1.49277216003\n",
      "-0.0683190840521 0.389622871533 0.98639904515 1.47990848486\n",
      "-0.136972851831 0.383826766531 0.981227940035 1.44657564592\n",
      "-0.0641828984764 0.484222943788 1.01510496922 1.45478465783\n",
      "-0.0844447966704 0.35889414275 1.00462362477 1.4485787834\n",
      "-0.0730183404051 0.34543067689 0.997364040923 1.48926101447\n",
      "-0.107004172468 0.413684057673 0.965351045204 1.37999125561\n",
      "-0.129072058851 0.304427542226 0.957689381905 1.43603198146\n",
      "-0.138968972847 0.306324394695 0.96119888205 1.39978112736\n",
      "-0.0996327059157 0.351304758145 0.991324519161 1.41695139121\n",
      "-0.140221956903 0.495096440958 0.982916618683 1.38062793333\n",
      "-0.119523528985 0.348477715503 0.927304418921 1.43216760961\n",
      "-0.11022064975 0.333872043417 0.990328675119 1.40044565624\n",
      "-0.15644121314 0.492777488171 0.958113971841 1.38370938876\n",
      "-0.189592801559 0.351934555252 0.957329537299 1.44285121305\n",
      "-0.0821794319732 0.392936284259 0.982373472803 1.41054397771\n",
      "-0.0855939517805 0.326108248831 0.972171783566 1.45399513407\n",
      "-0.165575320128 0.341333445593 0.984403137319 1.43103780888\n",
      "-0.0424847030958 0.357231923458 1.00037256186 1.45877302312\n",
      "-0.116997066308 0.384462729279 0.993518052302 1.41484951258\n",
      "-0.0898684058926 0.371627036776 0.992045561631 1.43922008319\n",
      "-0.203130313364 0.452882528568 0.992809247348 1.49552515749\n",
      "-0.0985609541459 0.397392086513 1.03009933392 1.51128938445\n",
      "-0.113378401239 0.320506315108 0.971137733725 1.47117932868\n",
      "-0.127603695299 0.304810997216 0.926319655819 1.4104805476\n",
      "-0.143188051138 0.386893615517 0.926586150839 1.39911462329\n",
      "-0.0852358212442 0.332782040463 1.00009370527 1.4388211355\n",
      "-0.119066467986 0.254370442886 0.97063037884 1.41456426151\n",
      "-0.0887582625221 0.327823585075 0.986784534029 1.4594690694\n",
      "-0.0883587589969 0.378394729736 0.966069865407 1.45146185658\n",
      "-0.132589935673 0.372382619837 0.999832216471 1.43604512686\n",
      "-0.137410934219 0.350580785545 1.00009007944 1.41428223337\n",
      "-0.102292269693 0.392804278498 0.96901478357 1.44370776101\n",
      "-0.121985176391 0.38473378992 0.980908624338 1.40991999336\n",
      "-0.0790669259954 0.412573225024 0.992611651881 1.42024431168\n",
      "-0.183119087131 0.370691598845 0.980644397384 1.42124218189\n",
      "-0.109457308819 0.315651641014 1.01042559043 1.39320295873\n",
      "-0.13924534772 0.346502412743 1.00231558981 1.38659531556\n",
      "-0.185705071067 0.383301792267 1.022817748 1.42403681746\n",
      "-0.127505355351 0.359614878189 0.991047622109 1.43392534727\n",
      "-0.104511415126 0.446705639569 0.997236512961 1.41871757088\n",
      "-0.0744350016982 0.289508812316 0.99331770957 1.40164892106\n",
      "-0.125138387328 0.34950112854 1.00604459427 1.39352372356\n",
      "-0.0520981511208 0.376654334717 0.988266724716 1.40372206665\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x = selu(x)\n",
    "    x, _ = dropout_selu_forward(keep_prob=0.95, X=x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elu_fwd(X):\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = m * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return X_pos + X_neg_exp\n",
    "\n",
    "def elu_bwd(X, dX):\n",
    "    m = 1.0 # 1e-3==0.001, a==m, 0.0 <= a <= 1.0, active/passive, on/off\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    m_neg_exp = m * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    return dX * m_neg_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_fwd(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    #     return scale * np.where(x>=0.0, x, alpha * (np.exp(x)-1))\n",
    "    X_pos = np.maximum(0.0, X) # ReLU\n",
    "    X_neg = np.minimum(X, 0.0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    X_neg_exp = alpha * (np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    out = scale * (X_pos + X_neg_exp)\n",
    "    cache = (scale, alpha, X) # mean=0, std=1\n",
    "    return out, cache\n",
    "\n",
    "def selu_bwd(dX, cache):\n",
    "    scale, alpha, X = cache # mean=0, std=1\n",
    "    dX = dX * scale\n",
    "    dX_neg = dX.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    X_neg = np.minimum(X, 0) # otherwise: if X<=0, Exp Leaky ReLU\n",
    "    dX_neg = dX_neg * alpha * np.exp(X_neg) # derivative of abs(np.exp(X_neg)-1) # a: slope, a>=0\n",
    "    dX_pos = dX.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_pos = dX_pos * 1\n",
    "    dX = dX_neg + dX_pos\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.192573130677 0.225464851152 0.8517251468 1.17343643636\n",
      "-0.181990439693 0.19236772977 0.830614924998 1.16460880061\n",
      "-0.156997657224 0.180271015029 0.828943363818 1.16442619197\n",
      "-0.199104746591 0.22398155216 0.796812820533 1.2104985597\n",
      "-0.173998806034 0.170736138442 0.779529801004 1.19198148117\n",
      "-0.219109562845 0.156020176235 0.770437549522 1.212449289\n",
      "-0.18967747706 0.221089878525 0.758048489469 1.23475467222\n",
      "-0.223147408747 0.194725050129 0.820239419803 1.21362171248\n",
      "-0.17199878824 0.166789833841 0.837366033913 1.21879645082\n",
      "-0.212695078258 0.297249892012 0.824467642147 1.21928508349\n",
      "-0.206781718701 0.183024553619 0.850118701275 1.22015661902\n",
      "-0.193197569119 0.230625786613 0.82600270733 1.18500910834\n",
      "-0.163987888743 0.171045279072 0.807928639787 1.17680909555\n",
      "-0.20329546972 0.243542309672 0.843773195816 1.16241449782\n",
      "-0.222863285566 0.209524992637 0.832198097308 1.14877634514\n",
      "-0.213679413185 0.196324396734 0.820204317897 1.18841301673\n",
      "-0.141203904392 0.215977872027 0.823700795761 1.22869621263\n",
      "-0.17519784277 0.158848571283 0.814101882395 1.18890987759\n",
      "-0.184178703777 0.202934871312 0.845279084522 1.20320179179\n",
      "-0.17443215272 0.216330907769 0.808508473833 1.19795740906\n",
      "-0.151028771474 0.21761994517 0.832283283778 1.19595374281\n",
      "-0.173321858802 0.196361696544 0.847027911461 1.21708862855\n",
      "-0.147361972794 0.178461972109 0.839563272027 1.18628878579\n",
      "-0.17315748284 0.204504844374 0.825049432785 1.19807374423\n",
      "-0.163660193111 0.201485988201 0.85173375359 1.20129316235\n",
      "-0.134132096331 0.160266996746 0.811296396795 1.15893493704\n",
      "-0.178347073695 0.162555890524 0.822835164355 1.18734141342\n",
      "-0.2075659661 0.211796165013 0.811277142954 1.18340023456\n",
      "-0.214607453273 0.190838365409 0.813193699212 1.18882909064\n",
      "-0.187860127582 0.216788538445 0.822626622502 1.16554074629\n",
      "-0.158013961805 0.159457665197 0.813178306997 1.1889781034\n",
      "-0.17168872351 0.14096583262 0.828379697472 1.23340329884\n",
      "-0.193202650213 0.176233259162 0.832912634018 1.20112744909\n",
      "-0.224188397425 0.223296955946 0.810882578932 1.2349523553\n",
      "-0.219401351688 0.207237738028 0.815647455273 1.19485163246\n",
      "-0.143033719043 0.168380455037 0.82336194186 1.20493342817\n",
      "-0.153391850015 0.10411474563 0.803693167988 1.15118376231\n",
      "-0.104541984863 0.116179014503 0.82954614583 1.15992009024\n",
      "-0.189063785321 0.133048318582 0.844311825279 1.18492036716\n",
      "-0.128988166085 0.139889770503 0.830169519913 1.19938855242\n",
      "-0.214566475296 0.189757971491 0.802237023821 1.20795078386\n",
      "-0.143037589227 0.135077199174 0.781558982451 1.1540849175\n",
      "-0.19478943273 0.260720598042 0.827015085274 1.15142064625\n",
      "-0.161399538685 0.15365516658 0.766089402059 1.20414367495\n",
      "-0.212756949079 0.18696545128 0.727323934638 1.17279179581\n",
      "-0.210114201626 0.215006868952 0.765061887944 1.18409963866\n",
      "-0.112734227528 0.115407958946 0.851174603501 1.20691699789\n",
      "-0.256938217342 0.263200507384 0.795772041404 1.23985304852\n",
      "-0.200884204729 0.202748300765 0.815976826207 1.30726489113\n",
      "-0.193561073042 0.202564890962 0.858167040089 1.28504425136\n",
      "-0.187491832486 0.253281349227 0.806541097687 1.35409646864\n",
      "-0.173432071844 0.179726514543 0.822432535738 1.21063726951\n",
      "-0.159161734079 0.162723591476 0.837557101438 1.20532265195\n",
      "-0.179594235726 0.292054222336 0.788048835395 1.18995867765\n",
      "-0.169968907855 0.190945500111 0.865380452272 1.18887697679\n",
      "-0.151438019338 0.210544577728 0.832513704364 1.20515477636\n",
      "-0.157184016022 0.154802196408 0.816467240017 1.20537250909\n",
      "-0.16887418506 0.16733237814 0.836465371176 1.14835495281\n",
      "-0.182010336786 0.174594961912 0.794838248412 1.20492937472\n",
      "-0.172624990499 0.249964799628 0.783454667798 1.21901842135\n",
      "-0.212551613852 0.188440688126 0.779835791714 1.21400819228\n",
      "-0.164845970196 0.281912310376 0.791614281805 1.18999302972\n",
      "-0.13916132736 0.142270697548 0.813898236058 1.15865283584\n",
      "-0.189849273688 0.232860104041 0.842109926097 1.12261968686\n",
      "-0.145285323792 0.211184037421 0.783695609601 1.15036187936\n",
      "-0.115695204026 0.175854738737 0.774579294736 1.1592509854\n",
      "-0.142791647492 0.162176891228 0.792264290237 1.19568133032\n",
      "-0.215990875502 0.216096198167 0.809231830407 1.17284275637\n",
      "-0.142843678937 0.121106808242 0.755349775729 1.18896984718\n",
      "-0.156363669101 0.184421864773 0.793909027612 1.20320343454\n",
      "-0.219301655729 0.248401927574 0.809234801712 1.23441584595\n",
      "-0.145008656405 0.224794801883 0.781205837088 1.24093652009\n",
      "-0.222019954711 0.230018238374 0.719891775034 1.21300662284\n",
      "-0.133269349401 0.12287208421 0.775259937948 1.20665962476\n",
      "-0.163166891915 0.154594812755 0.824715730706 1.26935254162\n",
      "-0.178973051973 0.138548652523 0.753574657086 1.16056211923\n",
      "-0.279017051688 0.210223655918 0.774293170084 1.14188954169\n",
      "-0.163234613288 0.162473983186 0.839588022155 1.16322664578\n",
      "-0.236488116978 0.284203185451 0.849231362667 1.19437537537\n",
      "-0.311784428538 0.209947711856 0.822299327216 1.16678860229\n",
      "-0.169859379449 0.191167808504 0.844718649672 1.20022245392\n",
      "-0.208800563549 0.21828843524 0.825719107431 1.2430666696\n",
      "-0.133480338644 0.193123155785 0.83861737149 1.22841705487\n",
      "-0.187932891855 0.243658219156 0.827356816012 1.19046196275\n",
      "-0.185737865954 0.194087718723 0.804246720761 1.15841343134\n",
      "-0.173063785851 0.177944186695 0.835172937955 1.20051892974\n",
      "-0.27912689108 0.239750921854 0.826517829544 1.16464462598\n",
      "-0.201078477977 0.180501142281 0.815432887091 1.17304875776\n",
      "-0.159107742191 0.172435874131 0.824536583996 1.13157286715\n",
      "-0.166422307121 0.187829016879 0.786372575077 1.17795798142\n",
      "-0.147527578546 0.186678346507 0.779563100203 1.1736355097\n",
      "-0.255222588451 0.266211440415 0.814796888992 1.20089871181\n",
      "-0.202456396353 0.218616088757 0.837447176522 1.22596604483\n",
      "-0.269257100985 0.192130997373 0.800922361518 1.17412431159\n",
      "-0.106035367793 0.132268195289 0.823229981929 1.18469795718\n",
      "-0.21608671482 0.141019422021 0.776661914802 1.15852060849\n",
      "-0.166355481031 0.122716665909 0.787468337112 1.15454255487\n",
      "-0.209712149234 0.227753823594 0.756480907334 1.17472071903\n",
      "-0.189498641007 0.193560067706 0.762585606527 1.19198336247\n",
      "-0.198339288141 0.203299536863 0.782620296129 1.16358963307\n"
     ]
    }
   ],
   "source": [
    "# EDIT: For the fun of it, I ran a quick experiment to see if activations would really stay close to 0/1:\n",
    "x = np.random.normal(size=(300, 200))\n",
    "for _ in range(100):\n",
    "    w = np.random.normal(size=(200, 200), scale=np.sqrt(1/200))  # their initialization scheme\n",
    "    x = x @ w\n",
    "    x, cache = selu_fwd(x)\n",
    "    mean = x.mean(axis=1)\n",
    "    scale = x.std(axis=1) # standard deviation=square-root(variance)\n",
    "    print(mean.min(), mean.max(), scale.min(), scale.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion & wrapup\n",
    "According to this, even after a 100 layers, mean neuron activations stay fairly close to mean 0 / variance 1 \n",
    "(even the most extreme means/variances are only off by 0.2).\n",
    "\n",
    "Sepp Hochreiter is amazing: LSTM, meta-learning, SNNN. \n",
    "\n",
    "I think he has already done a much larger contribution to science than some self-proclaimed pioneers of DL \n",
    "who spend more time on social networks than actually doing any good research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
