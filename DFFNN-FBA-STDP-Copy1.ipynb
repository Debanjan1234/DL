{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y, nl_cache = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "        y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y, nl_cache = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "            y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.4474 valid loss: 2.4086, valid accuracy: 0.1070\n",
      "Iter-20 train loss: 2.4225 valid loss: 2.4064, valid accuracy: 0.1070\n",
      "Iter-30 train loss: 2.3792 valid loss: 2.4042, valid accuracy: 0.1070\n",
      "Iter-40 train loss: 2.4903 valid loss: 2.4024, valid accuracy: 0.1070\n",
      "Iter-50 train loss: 2.4171 valid loss: 2.4004, valid accuracy: 0.1070\n",
      "Iter-60 train loss: 2.4229 valid loss: 2.3986, valid accuracy: 0.1070\n",
      "Iter-70 train loss: 2.4264 valid loss: 2.3968, valid accuracy: 0.1070\n",
      "Iter-80 train loss: 2.4602 valid loss: 2.3949, valid accuracy: 0.1070\n",
      "Iter-90 train loss: 2.3155 valid loss: 2.3934, valid accuracy: 0.1070\n",
      "Iter-100 train loss: 2.4829 valid loss: 2.3914, valid accuracy: 0.1070\n",
      "Iter-110 train loss: 2.4056 valid loss: 2.3899, valid accuracy: 0.1070\n",
      "Iter-120 train loss: 2.3673 valid loss: 2.3883, valid accuracy: 0.1070\n",
      "Iter-130 train loss: 2.3941 valid loss: 2.3866, valid accuracy: 0.1070\n",
      "Iter-140 train loss: 2.3537 valid loss: 2.3848, valid accuracy: 0.1070\n",
      "Iter-150 train loss: 2.2976 valid loss: 2.3833, valid accuracy: 0.1070\n",
      "Iter-160 train loss: 2.3913 valid loss: 2.3819, valid accuracy: 0.1070\n",
      "Iter-170 train loss: 2.4101 valid loss: 2.3803, valid accuracy: 0.1070\n",
      "Iter-180 train loss: 2.3558 valid loss: 2.3789, valid accuracy: 0.1070\n",
      "Iter-190 train loss: 2.4201 valid loss: 2.3774, valid accuracy: 0.1070\n",
      "Iter-200 train loss: 2.3257 valid loss: 2.3761, valid accuracy: 0.1070\n",
      "Iter-210 train loss: 2.3452 valid loss: 2.3748, valid accuracy: 0.1070\n",
      "Iter-220 train loss: 2.3461 valid loss: 2.3734, valid accuracy: 0.1070\n",
      "Iter-230 train loss: 2.4179 valid loss: 2.3720, valid accuracy: 0.1070\n",
      "Iter-240 train loss: 2.3159 valid loss: 2.3709, valid accuracy: 0.1070\n",
      "Iter-250 train loss: 2.4357 valid loss: 2.3695, valid accuracy: 0.1070\n",
      "Iter-260 train loss: 2.3969 valid loss: 2.3681, valid accuracy: 0.1070\n",
      "Iter-270 train loss: 2.4793 valid loss: 2.3667, valid accuracy: 0.1070\n",
      "Iter-280 train loss: 2.3625 valid loss: 2.3654, valid accuracy: 0.1070\n",
      "Iter-290 train loss: 2.3204 valid loss: 2.3642, valid accuracy: 0.1070\n",
      "Iter-300 train loss: 2.3419 valid loss: 2.3629, valid accuracy: 0.1070\n",
      "Iter-310 train loss: 2.3309 valid loss: 2.3617, valid accuracy: 0.1070\n",
      "Iter-320 train loss: 2.3144 valid loss: 2.3607, valid accuracy: 0.1070\n",
      "Iter-330 train loss: 2.3684 valid loss: 2.3593, valid accuracy: 0.1070\n",
      "Iter-340 train loss: 2.4132 valid loss: 2.3581, valid accuracy: 0.1070\n",
      "Iter-350 train loss: 2.3931 valid loss: 2.3570, valid accuracy: 0.1070\n",
      "Iter-360 train loss: 2.3727 valid loss: 2.3559, valid accuracy: 0.1070\n",
      "Iter-370 train loss: 2.3780 valid loss: 2.3549, valid accuracy: 0.1070\n",
      "Iter-380 train loss: 2.3700 valid loss: 2.3537, valid accuracy: 0.1070\n",
      "Iter-390 train loss: 2.3994 valid loss: 2.3526, valid accuracy: 0.1070\n",
      "Iter-400 train loss: 2.3901 valid loss: 2.3517, valid accuracy: 0.1070\n",
      "Iter-410 train loss: 2.3351 valid loss: 2.3508, valid accuracy: 0.1070\n",
      "Iter-420 train loss: 2.3594 valid loss: 2.3499, valid accuracy: 0.1070\n",
      "Iter-430 train loss: 2.4109 valid loss: 2.3489, valid accuracy: 0.1070\n",
      "Iter-440 train loss: 2.3367 valid loss: 2.3480, valid accuracy: 0.1070\n",
      "Iter-450 train loss: 2.3956 valid loss: 2.3472, valid accuracy: 0.1070\n",
      "Iter-460 train loss: 2.3800 valid loss: 2.3464, valid accuracy: 0.1070\n",
      "Iter-470 train loss: 2.3355 valid loss: 2.3454, valid accuracy: 0.1070\n",
      "Iter-480 train loss: 2.3189 valid loss: 2.3445, valid accuracy: 0.1070\n",
      "Iter-490 train loss: 2.2585 valid loss: 2.3437, valid accuracy: 0.1070\n",
      "Iter-500 train loss: 2.3686 valid loss: 2.3429, valid accuracy: 0.1070\n",
      "Iter-510 train loss: 2.2217 valid loss: 2.3422, valid accuracy: 0.1070\n",
      "Iter-520 train loss: 2.3436 valid loss: 2.3413, valid accuracy: 0.1070\n",
      "Iter-530 train loss: 2.3533 valid loss: 2.3405, valid accuracy: 0.1070\n",
      "Iter-540 train loss: 2.3105 valid loss: 2.3398, valid accuracy: 0.1070\n",
      "Iter-550 train loss: 2.3268 valid loss: 2.3390, valid accuracy: 0.1070\n",
      "Iter-560 train loss: 2.3275 valid loss: 2.3382, valid accuracy: 0.1070\n",
      "Iter-570 train loss: 2.2672 valid loss: 2.3377, valid accuracy: 0.1070\n",
      "Iter-580 train loss: 2.2642 valid loss: 2.3370, valid accuracy: 0.1070\n",
      "Iter-590 train loss: 2.3164 valid loss: 2.3363, valid accuracy: 0.1070\n",
      "Iter-600 train loss: 2.3280 valid loss: 2.3357, valid accuracy: 0.1070\n",
      "Iter-610 train loss: 2.3594 valid loss: 2.3350, valid accuracy: 0.1070\n",
      "Iter-620 train loss: 2.3302 valid loss: 2.3342, valid accuracy: 0.1070\n",
      "Iter-630 train loss: 2.2601 valid loss: 2.3337, valid accuracy: 0.1070\n",
      "Iter-640 train loss: 2.3227 valid loss: 2.3332, valid accuracy: 0.1070\n",
      "Iter-650 train loss: 2.3471 valid loss: 2.3326, valid accuracy: 0.1070\n",
      "Iter-660 train loss: 2.3239 valid loss: 2.3319, valid accuracy: 0.1070\n",
      "Iter-670 train loss: 2.2975 valid loss: 2.3313, valid accuracy: 0.1070\n",
      "Iter-680 train loss: 2.3039 valid loss: 2.3309, valid accuracy: 0.1070\n",
      "Iter-690 train loss: 2.3510 valid loss: 2.3304, valid accuracy: 0.1070\n",
      "Iter-700 train loss: 2.3235 valid loss: 2.3299, valid accuracy: 0.1070\n",
      "Iter-710 train loss: 2.3647 valid loss: 2.3293, valid accuracy: 0.1070\n",
      "Iter-720 train loss: 2.3895 valid loss: 2.3287, valid accuracy: 0.1070\n",
      "Iter-730 train loss: 2.3543 valid loss: 2.3282, valid accuracy: 0.1070\n",
      "Iter-740 train loss: 2.3186 valid loss: 2.3277, valid accuracy: 0.1070\n",
      "Iter-750 train loss: 2.3475 valid loss: 2.3271, valid accuracy: 0.1070\n",
      "Iter-760 train loss: 2.3354 valid loss: 2.3267, valid accuracy: 0.1070\n",
      "Iter-770 train loss: 2.2897 valid loss: 2.3261, valid accuracy: 0.1070\n",
      "Iter-780 train loss: 2.2855 valid loss: 2.3256, valid accuracy: 0.1070\n",
      "Iter-790 train loss: 2.3843 valid loss: 2.3251, valid accuracy: 0.1070\n",
      "Iter-800 train loss: 2.3195 valid loss: 2.3246, valid accuracy: 0.1070\n",
      "Iter-810 train loss: 2.3346 valid loss: 2.3241, valid accuracy: 0.1070\n",
      "Iter-820 train loss: 2.3554 valid loss: 2.3236, valid accuracy: 0.1070\n",
      "Iter-830 train loss: 2.3763 valid loss: 2.3231, valid accuracy: 0.1070\n",
      "Iter-840 train loss: 2.3402 valid loss: 2.3226, valid accuracy: 0.1070\n",
      "Iter-850 train loss: 2.3585 valid loss: 2.3222, valid accuracy: 0.1070\n",
      "Iter-860 train loss: 2.3164 valid loss: 2.3216, valid accuracy: 0.1070\n",
      "Iter-870 train loss: 2.3552 valid loss: 2.3211, valid accuracy: 0.1070\n",
      "Iter-880 train loss: 2.3732 valid loss: 2.3207, valid accuracy: 0.1070\n",
      "Iter-890 train loss: 2.3666 valid loss: 2.3203, valid accuracy: 0.1070\n",
      "Iter-900 train loss: 2.3449 valid loss: 2.3198, valid accuracy: 0.1070\n",
      "Iter-910 train loss: 2.3489 valid loss: 2.3194, valid accuracy: 0.1070\n",
      "Iter-920 train loss: 2.2914 valid loss: 2.3192, valid accuracy: 0.1070\n",
      "Iter-930 train loss: 2.3058 valid loss: 2.3188, valid accuracy: 0.1070\n",
      "Iter-940 train loss: 2.3419 valid loss: 2.3185, valid accuracy: 0.1070\n",
      "Iter-950 train loss: 2.2980 valid loss: 2.3181, valid accuracy: 0.1070\n",
      "Iter-960 train loss: 2.3168 valid loss: 2.3178, valid accuracy: 0.1070\n",
      "Iter-970 train loss: 2.3105 valid loss: 2.3176, valid accuracy: 0.1070\n",
      "Iter-980 train loss: 2.3464 valid loss: 2.3172, valid accuracy: 0.0986\n",
      "Iter-990 train loss: 2.3121 valid loss: 2.3166, valid accuracy: 0.0986\n",
      "Iter-1000 train loss: 2.3499 valid loss: 2.3163, valid accuracy: 0.1070\n",
      "Iter-1010 train loss: 2.3263 valid loss: 2.3160, valid accuracy: 0.0986\n",
      "Iter-1020 train loss: 2.3039 valid loss: 2.3158, valid accuracy: 0.0986\n",
      "Iter-1030 train loss: 2.2931 valid loss: 2.3155, valid accuracy: 0.0986\n",
      "Iter-1040 train loss: 2.3454 valid loss: 2.3152, valid accuracy: 0.0986\n",
      "Iter-1050 train loss: 2.2599 valid loss: 2.3150, valid accuracy: 0.0986\n",
      "Iter-1060 train loss: 2.3190 valid loss: 2.3146, valid accuracy: 0.0986\n",
      "Iter-1070 train loss: 2.3384 valid loss: 2.3143, valid accuracy: 0.0986\n",
      "Iter-1080 train loss: 2.2896 valid loss: 2.3141, valid accuracy: 0.0986\n",
      "Iter-1090 train loss: 2.3231 valid loss: 2.3137, valid accuracy: 0.0986\n",
      "Iter-1100 train loss: 2.3408 valid loss: 2.3134, valid accuracy: 0.0986\n",
      "Iter-1110 train loss: 2.2994 valid loss: 2.3132, valid accuracy: 0.0986\n",
      "Iter-1120 train loss: 2.2789 valid loss: 2.3129, valid accuracy: 0.0986\n",
      "Iter-1130 train loss: 2.3272 valid loss: 2.3126, valid accuracy: 0.0986\n",
      "Iter-1140 train loss: 2.2977 valid loss: 2.3123, valid accuracy: 0.0986\n",
      "Iter-1150 train loss: 2.3033 valid loss: 2.3121, valid accuracy: 0.0986\n",
      "Iter-1160 train loss: 2.3188 valid loss: 2.3119, valid accuracy: 0.0986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3185 valid loss: 2.3116, valid accuracy: 0.0986\n",
      "Iter-1180 train loss: 2.2991 valid loss: 2.3115, valid accuracy: 0.0986\n",
      "Iter-1190 train loss: 2.3020 valid loss: 2.3113, valid accuracy: 0.0986\n",
      "Iter-1200 train loss: 2.3410 valid loss: 2.3110, valid accuracy: 0.0986\n",
      "Iter-1210 train loss: 2.3092 valid loss: 2.3108, valid accuracy: 0.0986\n",
      "Iter-1220 train loss: 2.2914 valid loss: 2.3105, valid accuracy: 0.0986\n",
      "Iter-1230 train loss: 2.2815 valid loss: 2.3103, valid accuracy: 0.0986\n",
      "Iter-1240 train loss: 2.3082 valid loss: 2.3102, valid accuracy: 0.0986\n",
      "Iter-1250 train loss: 2.3233 valid loss: 2.3099, valid accuracy: 0.0986\n",
      "Iter-1260 train loss: 2.3087 valid loss: 2.3098, valid accuracy: 0.0986\n",
      "Iter-1270 train loss: 2.3234 valid loss: 2.3095, valid accuracy: 0.0986\n",
      "Iter-1280 train loss: 2.3258 valid loss: 2.3093, valid accuracy: 0.0986\n",
      "Iter-1290 train loss: 2.3115 valid loss: 2.3091, valid accuracy: 0.0986\n",
      "Iter-1300 train loss: 2.2788 valid loss: 2.3090, valid accuracy: 0.0986\n",
      "Iter-1310 train loss: 2.3093 valid loss: 2.3089, valid accuracy: 0.0986\n",
      "Iter-1320 train loss: 2.2979 valid loss: 2.3088, valid accuracy: 0.0986\n",
      "Iter-1330 train loss: 2.2653 valid loss: 2.3086, valid accuracy: 0.0986\n",
      "Iter-1340 train loss: 2.3050 valid loss: 2.3085, valid accuracy: 0.0986\n",
      "Iter-1350 train loss: 2.3189 valid loss: 2.3084, valid accuracy: 0.0986\n",
      "Iter-1360 train loss: 2.3297 valid loss: 2.3083, valid accuracy: 0.0986\n",
      "Iter-1370 train loss: 2.3166 valid loss: 2.3083, valid accuracy: 0.0986\n",
      "Iter-1380 train loss: 2.3093 valid loss: 2.3081, valid accuracy: 0.0986\n",
      "Iter-1390 train loss: 2.3108 valid loss: 2.3079, valid accuracy: 0.0986\n",
      "Iter-1400 train loss: 2.3063 valid loss: 2.3077, valid accuracy: 0.0986\n",
      "Iter-1410 train loss: 2.2965 valid loss: 2.3076, valid accuracy: 0.0986\n",
      "Iter-1420 train loss: 2.2981 valid loss: 2.3075, valid accuracy: 0.0986\n",
      "Iter-1430 train loss: 2.2889 valid loss: 2.3074, valid accuracy: 0.0986\n",
      "Iter-1440 train loss: 2.3384 valid loss: 2.3073, valid accuracy: 0.0986\n",
      "Iter-1450 train loss: 2.3209 valid loss: 2.3072, valid accuracy: 0.0986\n",
      "Iter-1460 train loss: 2.3082 valid loss: 2.3071, valid accuracy: 0.1126\n",
      "Iter-1470 train loss: 2.3235 valid loss: 2.3070, valid accuracy: 0.1126\n",
      "Iter-1480 train loss: 2.3200 valid loss: 2.3068, valid accuracy: 0.1126\n",
      "Iter-1490 train loss: 2.3074 valid loss: 2.3067, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.3024 valid loss: 2.3066, valid accuracy: 0.1126\n",
      "Iter-1510 train loss: 2.3094 valid loss: 2.3065, valid accuracy: 0.0986\n",
      "Iter-1520 train loss: 2.2916 valid loss: 2.3064, valid accuracy: 0.1126\n",
      "Iter-1530 train loss: 2.3207 valid loss: 2.3063, valid accuracy: 0.1126\n",
      "Iter-1540 train loss: 2.2999 valid loss: 2.3062, valid accuracy: 0.1126\n",
      "Iter-1550 train loss: 2.2908 valid loss: 2.3061, valid accuracy: 0.1126\n",
      "Iter-1560 train loss: 2.3146 valid loss: 2.3061, valid accuracy: 0.1126\n",
      "Iter-1570 train loss: 2.2996 valid loss: 2.3059, valid accuracy: 0.1126\n",
      "Iter-1580 train loss: 2.3160 valid loss: 2.3058, valid accuracy: 0.1126\n",
      "Iter-1590 train loss: 2.3065 valid loss: 2.3057, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3412 valid loss: 2.3056, valid accuracy: 0.1126\n",
      "Iter-1610 train loss: 2.3176 valid loss: 2.3055, valid accuracy: 0.1126\n",
      "Iter-1620 train loss: 2.3114 valid loss: 2.3053, valid accuracy: 0.1126\n",
      "Iter-1630 train loss: 2.3076 valid loss: 2.3053, valid accuracy: 0.1126\n",
      "Iter-1640 train loss: 2.3151 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-1650 train loss: 2.3345 valid loss: 2.3051, valid accuracy: 0.1126\n",
      "Iter-1660 train loss: 2.3108 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-1670 train loss: 2.3146 valid loss: 2.3050, valid accuracy: 0.1126\n",
      "Iter-1680 train loss: 2.3497 valid loss: 2.3049, valid accuracy: 0.1126\n",
      "Iter-1690 train loss: 2.2956 valid loss: 2.3048, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3382 valid loss: 2.3048, valid accuracy: 0.1126\n",
      "Iter-1710 train loss: 2.3238 valid loss: 2.3047, valid accuracy: 0.1126\n",
      "Iter-1720 train loss: 2.3044 valid loss: 2.3046, valid accuracy: 0.1126\n",
      "Iter-1730 train loss: 2.3157 valid loss: 2.3046, valid accuracy: 0.1126\n",
      "Iter-1740 train loss: 2.2843 valid loss: 2.3045, valid accuracy: 0.1126\n",
      "Iter-1750 train loss: 2.2862 valid loss: 2.3045, valid accuracy: 0.1126\n",
      "Iter-1760 train loss: 2.3264 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-1770 train loss: 2.3147 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-1780 train loss: 2.3204 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-1790 train loss: 2.3049 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3104 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-1810 train loss: 2.3186 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-1820 train loss: 2.3333 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-1830 train loss: 2.3002 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-1840 train loss: 2.2983 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-1850 train loss: 2.3280 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-1860 train loss: 2.3222 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-1870 train loss: 2.3001 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-1880 train loss: 2.2972 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-1890 train loss: 2.3220 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3029 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-1910 train loss: 2.3279 valid loss: 2.3034, valid accuracy: 0.1126\n",
      "Iter-1920 train loss: 2.3132 valid loss: 2.3034, valid accuracy: 0.1126\n",
      "Iter-1930 train loss: 2.2956 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-1940 train loss: 2.2985 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-1950 train loss: 2.2971 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-1960 train loss: 2.3013 valid loss: 2.3032, valid accuracy: 0.1126\n",
      "Iter-1970 train loss: 2.2940 valid loss: 2.3031, valid accuracy: 0.1126\n",
      "Iter-1980 train loss: 2.2983 valid loss: 2.3031, valid accuracy: 0.1126\n",
      "Iter-1990 train loss: 2.3029 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3053 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-2010 train loss: 2.3133 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-2020 train loss: 2.2884 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-2030 train loss: 2.3183 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-2040 train loss: 2.3041 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-2050 train loss: 2.2988 valid loss: 2.3028, valid accuracy: 0.1126\n",
      "Iter-2060 train loss: 2.2961 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-2070 train loss: 2.2982 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-2080 train loss: 2.3025 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-2090 train loss: 2.3035 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3044 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2110 train loss: 2.2926 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2120 train loss: 2.3009 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2130 train loss: 2.3029 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2140 train loss: 2.3057 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2150 train loss: 2.3149 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2160 train loss: 2.3121 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2170 train loss: 2.2974 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2180 train loss: 2.2964 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2190 train loss: 2.3203 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2210 train loss: 2.2976 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2220 train loss: 2.3189 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2230 train loss: 2.3305 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2240 train loss: 2.3085 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2250 train loss: 2.2995 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2260 train loss: 2.3144 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2270 train loss: 2.3143 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2280 train loss: 2.3047 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2290 train loss: 2.3003 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3295 valid loss: 2.3018, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3032 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-2320 train loss: 2.3084 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-2330 train loss: 2.2960 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-2340 train loss: 2.3111 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-2350 train loss: 2.3129 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-2360 train loss: 2.2981 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-2370 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-2380 train loss: 2.3155 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-2390 train loss: 2.2854 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.3107 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2410 train loss: 2.2881 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2420 train loss: 2.3004 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2430 train loss: 2.3128 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2440 train loss: 2.2849 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2450 train loss: 2.3080 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2460 train loss: 2.2996 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2470 train loss: 2.3145 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2480 train loss: 2.3132 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2490 train loss: 2.2990 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3014 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2510 train loss: 2.2786 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2520 train loss: 2.2949 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-2530 train loss: 2.3097 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-2540 train loss: 2.3017 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-2550 train loss: 2.3050 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-2560 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-2570 train loss: 2.2921 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-2580 train loss: 2.3138 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-2590 train loss: 2.3031 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.3060 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-2610 train loss: 2.2901 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-2620 train loss: 2.3074 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-2630 train loss: 2.3043 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2640 train loss: 2.3039 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2650 train loss: 2.2983 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2660 train loss: 2.3164 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2670 train loss: 2.2904 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2680 train loss: 2.3069 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2690 train loss: 2.3042 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.2886 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2710 train loss: 2.2919 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2720 train loss: 2.3175 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2730 train loss: 2.2861 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2740 train loss: 2.3075 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2750 train loss: 2.3162 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2760 train loss: 2.3104 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2770 train loss: 2.3058 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-2780 train loss: 2.3131 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2790 train loss: 2.2955 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.3092 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2810 train loss: 2.3141 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2820 train loss: 2.3055 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2830 train loss: 2.2939 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2840 train loss: 2.2986 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2850 train loss: 2.2965 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2860 train loss: 2.2936 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2870 train loss: 2.2900 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2880 train loss: 2.2950 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2890 train loss: 2.2984 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.2916 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2910 train loss: 2.2983 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2920 train loss: 2.3051 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2930 train loss: 2.2931 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2940 train loss: 2.3022 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2950 train loss: 2.2997 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2960 train loss: 2.2998 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2970 train loss: 2.3127 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2980 train loss: 2.3065 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2990 train loss: 2.3002 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.2873 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3010 train loss: 2.2938 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3020 train loss: 2.2918 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3030 train loss: 2.2924 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3040 train loss: 2.2973 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3050 train loss: 2.2974 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3060 train loss: 2.3045 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3070 train loss: 2.2915 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3080 train loss: 2.2752 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3090 train loss: 2.3022 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3006 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3110 train loss: 2.3050 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3120 train loss: 2.3030 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3130 train loss: 2.3084 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3140 train loss: 2.2951 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3150 train loss: 2.3037 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3160 train loss: 2.3076 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3170 train loss: 2.2928 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3180 train loss: 2.3011 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3190 train loss: 2.2905 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3013 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3210 train loss: 2.3175 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3220 train loss: 2.3043 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3230 train loss: 2.2918 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3240 train loss: 2.3098 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3250 train loss: 2.2992 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3260 train loss: 2.2948 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3270 train loss: 2.2918 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3280 train loss: 2.2943 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3290 train loss: 2.3188 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.3040 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3310 train loss: 2.3010 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3320 train loss: 2.2942 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3330 train loss: 2.2982 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3340 train loss: 2.2939 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3350 train loss: 2.2905 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3360 train loss: 2.3087 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3370 train loss: 2.2980 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3380 train loss: 2.2985 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3390 train loss: 2.3098 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.3076 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3410 train loss: 2.3027 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3420 train loss: 2.2887 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3430 train loss: 2.2863 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3440 train loss: 2.3102 valid loss: 2.3009, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3051 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3460 train loss: 2.3047 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3470 train loss: 2.3143 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3480 train loss: 2.3083 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3490 train loss: 2.2936 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.2729 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3510 train loss: 2.2914 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3520 train loss: 2.3090 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3530 train loss: 2.2893 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3540 train loss: 2.3091 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3550 train loss: 2.2945 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3560 train loss: 2.3068 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3570 train loss: 2.3077 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3580 train loss: 2.3045 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-3590 train loss: 2.3014 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.2986 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-3610 train loss: 2.3143 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3620 train loss: 2.2959 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3630 train loss: 2.3018 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3640 train loss: 2.2874 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3650 train loss: 2.3017 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3660 train loss: 2.3033 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3670 train loss: 2.2903 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3680 train loss: 2.3112 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3690 train loss: 2.2945 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.2938 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3710 train loss: 2.2923 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3720 train loss: 2.3097 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3730 train loss: 2.3017 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3740 train loss: 2.2887 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3750 train loss: 2.2946 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3760 train loss: 2.2936 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3770 train loss: 2.3010 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3780 train loss: 2.3037 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3790 train loss: 2.2993 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.2929 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3810 train loss: 2.3150 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3820 train loss: 2.3114 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3830 train loss: 2.3084 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3840 train loss: 2.3034 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3850 train loss: 2.3097 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3860 train loss: 2.2969 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3870 train loss: 2.3085 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3880 train loss: 2.3017 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3890 train loss: 2.3023 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3117 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3910 train loss: 2.3114 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3920 train loss: 2.2937 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3930 train loss: 2.2997 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3940 train loss: 2.2961 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3950 train loss: 2.3132 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3960 train loss: 2.3003 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3970 train loss: 2.3046 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3980 train loss: 2.2980 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3990 train loss: 2.2897 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.2949 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4010 train loss: 2.3012 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4020 train loss: 2.3143 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4030 train loss: 2.3042 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4040 train loss: 2.3077 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4050 train loss: 2.3065 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4060 train loss: 2.3071 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4070 train loss: 2.3072 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4080 train loss: 2.2984 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4090 train loss: 2.3092 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3023 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4110 train loss: 2.3032 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4120 train loss: 2.3017 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4130 train loss: 2.2973 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4140 train loss: 2.2920 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4150 train loss: 2.3043 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4160 train loss: 2.3085 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4170 train loss: 2.2992 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4180 train loss: 2.2873 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4190 train loss: 2.3103 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3015 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4210 train loss: 2.3049 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4220 train loss: 2.2936 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4230 train loss: 2.2967 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4240 train loss: 2.2892 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4250 train loss: 2.3212 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4260 train loss: 2.3120 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4270 train loss: 2.2938 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4280 train loss: 2.3086 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4290 train loss: 2.3028 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.2962 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4310 train loss: 2.2928 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4320 train loss: 2.3034 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4330 train loss: 2.3048 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4340 train loss: 2.2925 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4350 train loss: 2.2933 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4360 train loss: 2.3112 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4370 train loss: 2.2976 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4380 train loss: 2.2903 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4390 train loss: 2.3042 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.2973 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4410 train loss: 2.3091 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4420 train loss: 2.3056 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4430 train loss: 2.3026 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4440 train loss: 2.3017 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4450 train loss: 2.2911 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4460 train loss: 2.3077 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4470 train loss: 2.3015 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-4480 train loss: 2.3125 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4490 train loss: 2.3059 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.3025 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4510 train loss: 2.2995 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4520 train loss: 2.2918 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4530 train loss: 2.3037 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4540 train loss: 2.2999 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4550 train loss: 2.2888 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4560 train loss: 2.3202 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4570 train loss: 2.3008 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4580 train loss: 2.3047 valid loss: 2.3008, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3033 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.2943 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4610 train loss: 2.2984 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4620 train loss: 2.3020 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4630 train loss: 2.3003 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4640 train loss: 2.3028 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4650 train loss: 2.2943 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4660 train loss: 2.3011 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4670 train loss: 2.2896 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4680 train loss: 2.2987 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4690 train loss: 2.3073 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.2887 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4710 train loss: 2.2985 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4720 train loss: 2.2867 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4730 train loss: 2.3082 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4740 train loss: 2.3069 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-4750 train loss: 2.2958 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4760 train loss: 2.3071 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4770 train loss: 2.3004 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4780 train loss: 2.2935 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4790 train loss: 2.2867 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.3038 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4810 train loss: 2.3164 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4820 train loss: 2.2914 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4830 train loss: 2.2916 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4840 train loss: 2.2996 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4850 train loss: 2.2857 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4860 train loss: 2.2982 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4870 train loss: 2.3081 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4880 train loss: 2.3025 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4890 train loss: 2.2916 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.3028 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4910 train loss: 2.3046 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4920 train loss: 2.2906 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4930 train loss: 2.3058 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4940 train loss: 2.3089 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4950 train loss: 2.2999 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4960 train loss: 2.2905 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4970 train loss: 2.3024 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4980 train loss: 2.2934 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4990 train loss: 2.3053 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.2867 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5010 train loss: 2.2955 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5020 train loss: 2.3010 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5030 train loss: 2.3073 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5040 train loss: 2.3134 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5050 train loss: 2.3050 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5060 train loss: 2.2968 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5070 train loss: 2.2915 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5080 train loss: 2.3102 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5090 train loss: 2.2941 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3018 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5110 train loss: 2.3150 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5120 train loss: 2.3040 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5130 train loss: 2.3009 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5140 train loss: 2.2867 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5150 train loss: 2.2934 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5160 train loss: 2.3037 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5170 train loss: 2.2967 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5180 train loss: 2.3087 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5190 train loss: 2.2925 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.2857 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5210 train loss: 2.2923 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5220 train loss: 2.2971 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5230 train loss: 2.2979 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5240 train loss: 2.3054 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5250 train loss: 2.3089 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5260 train loss: 2.3051 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5270 train loss: 2.3053 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5280 train loss: 2.2969 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5290 train loss: 2.3035 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.3000 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5310 train loss: 2.3107 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5320 train loss: 2.2991 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5330 train loss: 2.2953 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5340 train loss: 2.3057 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5350 train loss: 2.3086 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5360 train loss: 2.2988 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5370 train loss: 2.3045 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5380 train loss: 2.3099 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5390 train loss: 2.2981 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3110 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5410 train loss: 2.3009 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5420 train loss: 2.3120 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5430 train loss: 2.2935 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5440 train loss: 2.2876 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5450 train loss: 2.3117 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5460 train loss: 2.3021 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5470 train loss: 2.3139 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5480 train loss: 2.2907 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5490 train loss: 2.3156 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3027 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5510 train loss: 2.2969 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5520 train loss: 2.3086 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5530 train loss: 2.3017 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5540 train loss: 2.3136 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5550 train loss: 2.2899 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5560 train loss: 2.3072 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5570 train loss: 2.3080 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5580 train loss: 2.2994 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5590 train loss: 2.3141 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.2923 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5610 train loss: 2.3017 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5620 train loss: 2.2976 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5630 train loss: 2.3011 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5640 train loss: 2.3078 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5650 train loss: 2.3190 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5660 train loss: 2.3008 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5670 train loss: 2.2958 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5680 train loss: 2.2869 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5690 train loss: 2.2917 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.2877 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5710 train loss: 2.3024 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5720 train loss: 2.2922 valid loss: 2.3010, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.3152 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5740 train loss: 2.2936 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5750 train loss: 2.3074 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5760 train loss: 2.2946 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5770 train loss: 2.2992 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-5780 train loss: 2.3056 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5790 train loss: 2.3036 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3052 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5810 train loss: 2.2864 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5820 train loss: 2.3018 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5830 train loss: 2.2990 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5840 train loss: 2.3009 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5850 train loss: 2.2976 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5860 train loss: 2.2921 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5870 train loss: 2.2934 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5880 train loss: 2.2936 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5890 train loss: 2.3054 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.2859 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5910 train loss: 2.3006 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5920 train loss: 2.2960 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5930 train loss: 2.2914 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5940 train loss: 2.2900 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5950 train loss: 2.3069 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5960 train loss: 2.3079 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5970 train loss: 2.2911 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5980 train loss: 2.3122 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5990 train loss: 2.3079 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3094 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6010 train loss: 2.2978 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6020 train loss: 2.3172 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6030 train loss: 2.3133 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6040 train loss: 2.3052 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6050 train loss: 2.3144 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6060 train loss: 2.2924 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6070 train loss: 2.3085 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6080 train loss: 2.3105 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6090 train loss: 2.3069 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.2967 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6110 train loss: 2.3079 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6120 train loss: 2.2988 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6130 train loss: 2.2952 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6140 train loss: 2.3160 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6150 train loss: 2.2983 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6160 train loss: 2.3025 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6170 train loss: 2.3162 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6180 train loss: 2.2947 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6190 train loss: 2.3065 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3059 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6210 train loss: 2.2991 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6220 train loss: 2.3070 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6230 train loss: 2.3098 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6240 train loss: 2.3087 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6250 train loss: 2.3060 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6260 train loss: 2.3067 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6270 train loss: 2.2999 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6280 train loss: 2.2970 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6290 train loss: 2.3077 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3101 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6310 train loss: 2.2940 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6320 train loss: 2.2993 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6330 train loss: 2.3054 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6340 train loss: 2.3009 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6350 train loss: 2.2916 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6360 train loss: 2.3080 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6370 train loss: 2.2961 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6380 train loss: 2.2748 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6390 train loss: 2.2964 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.2932 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6410 train loss: 2.2946 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6420 train loss: 2.3007 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6430 train loss: 2.2962 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6440 train loss: 2.2923 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6450 train loss: 2.2981 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6460 train loss: 2.3106 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6470 train loss: 2.2972 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6480 train loss: 2.3038 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6490 train loss: 2.2892 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.3089 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6510 train loss: 2.3014 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6520 train loss: 2.3043 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6530 train loss: 2.3135 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6540 train loss: 2.3022 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6550 train loss: 2.3078 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6560 train loss: 2.3035 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6570 train loss: 2.2977 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6580 train loss: 2.2900 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6590 train loss: 2.3114 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.2969 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6610 train loss: 2.3097 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6620 train loss: 2.2906 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6630 train loss: 2.3146 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-6640 train loss: 2.3003 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6650 train loss: 2.2994 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6660 train loss: 2.2878 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6670 train loss: 2.3161 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6680 train loss: 2.3014 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6690 train loss: 2.2940 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.3122 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6710 train loss: 2.2966 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6720 train loss: 2.2964 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6730 train loss: 2.3252 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6740 train loss: 2.3139 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6750 train loss: 2.3071 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6760 train loss: 2.3257 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6770 train loss: 2.2910 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6780 train loss: 2.3013 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6790 train loss: 2.2996 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.2869 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6810 train loss: 2.3147 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6820 train loss: 2.3125 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6830 train loss: 2.3070 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6840 train loss: 2.3026 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6850 train loss: 2.2938 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6860 train loss: 2.3083 valid loss: 2.3010, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.2936 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6880 train loss: 2.3065 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6890 train loss: 2.3043 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.3144 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6910 train loss: 2.3080 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6920 train loss: 2.2970 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6930 train loss: 2.2938 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6940 train loss: 2.3022 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6950 train loss: 2.2856 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6960 train loss: 2.3152 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6970 train loss: 2.2933 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6980 train loss: 2.3096 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6990 train loss: 2.3059 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.3051 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7010 train loss: 2.3031 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7020 train loss: 2.3056 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7030 train loss: 2.2961 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7040 train loss: 2.3033 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7050 train loss: 2.2958 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7060 train loss: 2.3087 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7070 train loss: 2.2902 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7080 train loss: 2.3019 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7090 train loss: 2.3055 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3113 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7110 train loss: 2.3037 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7120 train loss: 2.3064 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7130 train loss: 2.2935 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7140 train loss: 2.3034 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7150 train loss: 2.2884 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7160 train loss: 2.3040 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7170 train loss: 2.3008 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7180 train loss: 2.2848 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7190 train loss: 2.3009 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.3018 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7210 train loss: 2.3117 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7220 train loss: 2.2922 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7230 train loss: 2.2987 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7240 train loss: 2.2843 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7250 train loss: 2.3043 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7260 train loss: 2.3052 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7270 train loss: 2.2981 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7280 train loss: 2.3059 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7290 train loss: 2.2853 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.2981 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7310 train loss: 2.3097 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7320 train loss: 2.3048 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7330 train loss: 2.3010 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7340 train loss: 2.3105 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7350 train loss: 2.3014 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7360 train loss: 2.2970 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7370 train loss: 2.2943 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7380 train loss: 2.2861 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7390 train loss: 2.3067 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.3147 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7410 train loss: 2.3096 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7420 train loss: 2.2943 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7430 train loss: 2.2977 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7440 train loss: 2.3120 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7450 train loss: 2.3044 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7460 train loss: 2.2955 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7470 train loss: 2.3035 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7480 train loss: 2.2904 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7490 train loss: 2.3066 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.3132 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7510 train loss: 2.3107 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7520 train loss: 2.2989 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7530 train loss: 2.3050 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7540 train loss: 2.3149 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7550 train loss: 2.3021 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7560 train loss: 2.2989 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7570 train loss: 2.3185 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7580 train loss: 2.3108 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7590 train loss: 2.2845 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.3150 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7610 train loss: 2.3041 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-7620 train loss: 2.2979 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7630 train loss: 2.3070 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7640 train loss: 2.2918 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7650 train loss: 2.3042 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7660 train loss: 2.3069 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7670 train loss: 2.2995 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7680 train loss: 2.2961 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7690 train loss: 2.2835 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.3150 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7710 train loss: 2.3022 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7720 train loss: 2.3012 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7730 train loss: 2.3072 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-7740 train loss: 2.3049 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-7750 train loss: 2.2961 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-7760 train loss: 2.3098 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7770 train loss: 2.3072 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7780 train loss: 2.2982 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7790 train loss: 2.3138 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3053 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7810 train loss: 2.3048 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7820 train loss: 2.2897 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7830 train loss: 2.3022 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7840 train loss: 2.2846 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7850 train loss: 2.3058 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7860 train loss: 2.3098 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7870 train loss: 2.3070 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7880 train loss: 2.2945 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7890 train loss: 2.3012 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.3036 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7910 train loss: 2.3046 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7920 train loss: 2.2830 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7930 train loss: 2.3004 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7940 train loss: 2.2996 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7950 train loss: 2.2999 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7960 train loss: 2.3081 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7970 train loss: 2.3136 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7980 train loss: 2.3039 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7990 train loss: 2.3131 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.2985 valid loss: 2.3010, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.3022 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8020 train loss: 2.2934 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8030 train loss: 2.3103 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8040 train loss: 2.3021 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-8050 train loss: 2.3001 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-8060 train loss: 2.3132 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-8070 train loss: 2.3073 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-8080 train loss: 2.3005 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8090 train loss: 2.2890 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.2930 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8110 train loss: 2.3017 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8120 train loss: 2.2955 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8130 train loss: 2.3023 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8140 train loss: 2.3120 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8150 train loss: 2.3156 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8160 train loss: 2.2947 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8170 train loss: 2.2939 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8180 train loss: 2.2946 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8190 train loss: 2.3019 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.3059 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8210 train loss: 2.2979 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8220 train loss: 2.3019 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8230 train loss: 2.2930 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8240 train loss: 2.3000 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8250 train loss: 2.3140 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8260 train loss: 2.2872 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8270 train loss: 2.3025 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8280 train loss: 2.3020 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8290 train loss: 2.3023 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.3046 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8310 train loss: 2.3185 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8320 train loss: 2.2998 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8330 train loss: 2.2958 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8340 train loss: 2.3107 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8350 train loss: 2.2992 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8360 train loss: 2.2949 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8370 train loss: 2.2876 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8380 train loss: 2.3028 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8390 train loss: 2.3070 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.3013 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8410 train loss: 2.3035 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8420 train loss: 2.3044 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8430 train loss: 2.3053 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8440 train loss: 2.2953 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8450 train loss: 2.3010 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8460 train loss: 2.2964 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8470 train loss: 2.2973 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8480 train loss: 2.3041 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8490 train loss: 2.3150 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.2881 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8510 train loss: 2.3074 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8520 train loss: 2.3094 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8530 train loss: 2.2999 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8540 train loss: 2.3042 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8550 train loss: 2.3042 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8560 train loss: 2.3063 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8570 train loss: 2.3004 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8580 train loss: 2.2994 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8590 train loss: 2.3055 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.3043 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8610 train loss: 2.2924 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8620 train loss: 2.3068 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8630 train loss: 2.3163 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8640 train loss: 2.3135 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8650 train loss: 2.3000 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8660 train loss: 2.3071 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8670 train loss: 2.2941 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8680 train loss: 2.3121 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8690 train loss: 2.3193 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.2988 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8710 train loss: 2.2946 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8720 train loss: 2.3065 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8730 train loss: 2.2983 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8740 train loss: 2.3081 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8750 train loss: 2.2874 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8760 train loss: 2.2960 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8770 train loss: 2.2936 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8780 train loss: 2.3048 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8790 train loss: 2.2929 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.3012 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8810 train loss: 2.3058 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8820 train loss: 2.2912 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8830 train loss: 2.3069 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8840 train loss: 2.3079 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8850 train loss: 2.2974 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8860 train loss: 2.3080 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-8870 train loss: 2.2957 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8880 train loss: 2.3035 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8890 train loss: 2.3059 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.2977 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8910 train loss: 2.3012 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8920 train loss: 2.3033 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8930 train loss: 2.2973 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8940 train loss: 2.3116 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8950 train loss: 2.3044 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8960 train loss: 2.3041 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8970 train loss: 2.2928 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8980 train loss: 2.3126 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8990 train loss: 2.3101 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.3057 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9010 train loss: 2.3000 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9020 train loss: 2.3052 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9030 train loss: 2.2904 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9040 train loss: 2.2988 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9050 train loss: 2.2922 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9060 train loss: 2.3083 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9070 train loss: 2.3024 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9080 train loss: 2.3068 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9090 train loss: 2.3014 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.2867 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9110 train loss: 2.3012 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9120 train loss: 2.2976 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9130 train loss: 2.2956 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9140 train loss: 2.2938 valid loss: 2.3008, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.3048 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9160 train loss: 2.2981 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9170 train loss: 2.2970 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9180 train loss: 2.3034 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9190 train loss: 2.3029 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.3113 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9210 train loss: 2.2999 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9220 train loss: 2.2812 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9230 train loss: 2.3091 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9240 train loss: 2.3036 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9250 train loss: 2.3085 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9260 train loss: 2.2982 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9270 train loss: 2.3045 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9280 train loss: 2.2928 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9290 train loss: 2.3036 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.3023 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9310 train loss: 2.3004 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9320 train loss: 2.2953 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9330 train loss: 2.3061 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9340 train loss: 2.3122 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9350 train loss: 2.3103 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9360 train loss: 2.3077 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9370 train loss: 2.3178 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9380 train loss: 2.3063 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9390 train loss: 2.3135 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2971 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9410 train loss: 2.3000 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9420 train loss: 2.3093 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9430 train loss: 2.2983 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9440 train loss: 2.3083 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9450 train loss: 2.2956 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9460 train loss: 2.2957 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9470 train loss: 2.2943 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9480 train loss: 2.3048 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9490 train loss: 2.2977 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.3026 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9510 train loss: 2.3090 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9520 train loss: 2.3034 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9530 train loss: 2.2959 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9540 train loss: 2.3173 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9550 train loss: 2.2931 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9560 train loss: 2.3076 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9570 train loss: 2.3115 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9580 train loss: 2.3073 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9590 train loss: 2.2953 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.2998 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9610 train loss: 2.3061 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9620 train loss: 2.2962 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9630 train loss: 2.3089 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9640 train loss: 2.3010 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9650 train loss: 2.2997 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9660 train loss: 2.3077 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9670 train loss: 2.2881 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9680 train loss: 2.2979 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9690 train loss: 2.3008 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.3025 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9710 train loss: 2.3001 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9720 train loss: 2.2941 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9730 train loss: 2.2924 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9740 train loss: 2.3073 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9750 train loss: 2.3043 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9760 train loss: 2.2989 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9770 train loss: 2.3066 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9780 train loss: 2.3078 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9790 train loss: 2.3041 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.3011 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9810 train loss: 2.2943 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9820 train loss: 2.3029 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9830 train loss: 2.3033 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9840 train loss: 2.3099 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9850 train loss: 2.3021 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9860 train loss: 2.3044 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9870 train loss: 2.2976 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9880 train loss: 2.3004 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9890 train loss: 2.3071 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3060 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9910 train loss: 2.2943 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9920 train loss: 2.2999 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9930 train loss: 2.3059 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9940 train loss: 2.2894 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9950 train loss: 2.3152 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9960 train loss: 2.3072 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9970 train loss: 2.2997 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9980 train loss: 2.2959 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-9990 train loss: 2.3081 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.2919 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3011\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ee3ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkv1/WQBzAx9nUuEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121227358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
