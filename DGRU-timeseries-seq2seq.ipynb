{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6,), (6,), array([  1.69625365e+04,   9.30458454e+03,   2.63488844e+04,\n",
       "          3.35023841e-01,   5.98810203e-01,   5.40343592e-01]), array([  1.69625365e+04,   9.30458454e+03,   2.63488844e+04,\n",
       "          3.35023841e-01,   5.98810203e-01,   5.40343592e-01]), array([ 0.,  0.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data: time-serie smartwatch or wristband or smartband data\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio\n",
    "\n",
    "# Data reading\n",
    "# # Linux-Ubuntu\n",
    "# data_path = '/home/arasdar/data/Training_data/DATA_01_TYPE01.mat'\n",
    "# Macbook\n",
    "data_path = '/Users/arasdar/data/Training_data/DATA_01_TYPE01.mat'\n",
    "watch = spio.loadmat(data_path)\n",
    "data = watch['sig']\n",
    "data = np.array(data)\n",
    "data.shape, watch['sig'].size/6\n",
    "\n",
    "# Normalizing each batch of the data, each batch = each file\n",
    "# Can we normalize them all at the same time?\n",
    "mean = np.mean(data, axis=1)\n",
    "std = np.std(data, axis=1)\n",
    "var = np.var(data, axis=1)\n",
    "std2 = np.sqrt(var)\n",
    "mean.shape, var.shape, std.shape, std2.shape, std, std2, std-std2\n",
    "\n",
    "mean = mean.reshape(-1, 1)\n",
    "std = std.reshape(-1, 1)\n",
    "mean.shape, std.shape\n",
    "data_norm = (data - mean)/std\n",
    "data.shape, data_norm.shape\n",
    "var2 = np.square(data - mean)\n",
    "var2 = np.mean(var2, axis=1)\n",
    "var.shape, var2.shape, var, var2, var-var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The features/ channels of the data = ['HR-ECG', 'HR-PPG', 'ACC-X', 'ACC-Y', 'ACC-Z']\n",
    "plt.plot(data[0, :1000], label='ECG')\n",
    "plt.plot(data[1, :1000], label='PPG-1')\n",
    "plt.plot(data[2, :1000], label='PPG-2')\n",
    "# plt.plot(x_sig[3, :1000], label='ACC-X')\n",
    "# plt.plot(x_sig[4, :1000], label='SCC-Y')\n",
    "# plt.plot(x_sig[5, :1000], label='ACC-Z')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The features/ channels of the data = ['HR-ECG', 'HR-PPG', 'ACC-X', 'ACC-Y', 'ACC-Z']\n",
    "plt.plot(data_norm[0, :1000], label='ECG')\n",
    "plt.plot(data_norm[1, :1000], label='PPG-1')\n",
    "plt.plot(data_norm[2, :1000], label='PPG-2')\n",
    "# plt.plot(x_sig[3, :1000], label='ACC-X')\n",
    "# plt.plot(x_sig[4, :1000], label='SCC-Y')\n",
    "# plt.plot(x_sig[5, :1000], label='ACC-Z')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ECG\n",
    "Y = data_norm[0]\n",
    "X = data_norm[1:]\n",
    "Y.shape, X.shape\n",
    "Y = Y.reshape(1, -1)\n",
    "Y.shape, X.shape\n",
    "plt.plot(Y[0, :1000], label='HR-ECG-1: Output signal')\n",
    "plt.plot(X[0, :1000], label='HR-PPG-1: Input signal')\n",
    "plt.plot(X[1, :1000], label='HR-PPG-2: Input signal')\n",
    "plt.plot(X[2, :1000], label='ACC-X-3: Input signal')\n",
    "plt.plot(X[3, :1000], label='ACC-Y-4: Input signal')\n",
    "plt.plot(X[4, :1000], label='ACC-Z-5: Input signal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "X_train = X.T\n",
    "Y_train = Y.T\n",
    "X_train.shape, Y_train.shape\n",
    "\n",
    "# Preparing the training data for seq2seq learning\n",
    "Y_train_in = Y_train[:-2]\n",
    "Y_train_out = Y_train[:-1]\n",
    "#     Y_train_in.shape, Y_train.shape\n",
    "firstrow = np.zeros([1, 1])\n",
    "#     firstrow.shape, Y_train_in.shape\n",
    "Y_train_in = np.row_stack((firstrow, Y_train_in))\n",
    "#     Y_train_in.shape, Y_train_in[:5], Y_train[:5]\n",
    "#     Y_train_out = Y_train.copy()\n",
    "X_train.shape, Y_train_in.shape, Y_train_out.shape\n",
    "XY_train = (X_train, Y_train)\n",
    "\n",
    "# Read and normalize one batch/file of data\n",
    "def read_data(data_path):\n",
    "    band = spio.loadmat(data_path)\n",
    "    data = band['sig']\n",
    "    data = np.array(data)\n",
    "    #     data.shape, band['sig'].size/6\n",
    "\n",
    "    # Normalizing each batch of the data, each batch = each file\n",
    "    # Can we normalize them all at the same time?\n",
    "    mean = np.mean(data, axis=1)\n",
    "    var = np.var(data, axis=1)\n",
    "    std = np.sqrt(var)\n",
    "    mean.shape, var.shape, std.shape\n",
    "    mean = mean.reshape(-1, 1)\n",
    "    std = std.reshape(-1, 1)\n",
    "    mean.shape, std.shape\n",
    "    data_norm = (data - mean)/std\n",
    "    #     print(data.shape, data_norm.shape)\n",
    "    \n",
    "    # ECG\n",
    "    Y = data_norm[0]\n",
    "    # PPG+ACC\n",
    "    X = data_norm[1:]\n",
    "    # Y.shape, X.shape\n",
    "    Y = Y.reshape(1, -1)\n",
    "    # Y.shape, X.shape\n",
    "    X_train = X.T\n",
    "    Y_train = Y.T\n",
    "    #     print(X_train.shape, Y_train.shape)\n",
    "    \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.D = D # number of input dimensions\n",
    "        self.H = H # number of hidden units\n",
    "        self.L = L # number of hidden layers\n",
    "        self.C = C # number of output classes/dimensions\n",
    "        self.losses = {'train':[], 'train2':[], 'train3':[]} #, 'valid':[], 'test':[]\n",
    "        \n",
    "        # Input sequence model parameters\n",
    "        Z = H + D\n",
    "        params_in_seq = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(D / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        # This is the last layer in the input mode\n",
    "        params_in_seq_ = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H))\n",
    "        )\n",
    "        \n",
    "        # Output sequence model parameters\n",
    "        Z = H + C\n",
    "        params_out_seq = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, C))\n",
    "        )\n",
    "\n",
    "        # Model parameters\n",
    "        self.model = []\n",
    "        num_modes = 2 # num of modality as the source of sequences\n",
    "        for _ in range(num_modes):\n",
    "            self.model.append([])\n",
    "        \n",
    "        for _ in range(self.L-1):\n",
    "            self.model[0].append(params_in_seq)\n",
    "\n",
    "        # The last layer: self.L-1\n",
    "        self.model[0].append(params_in_seq_)\n",
    "        \n",
    "        # Number of layers for each mode\n",
    "        for _ in range(self.L):\n",
    "            self.model[1].append(params_out_seq)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # The final output\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache,\n",
    "                 hh, hh_cache, hh_tanh_cache, h, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, h, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        # The final output\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def forward_(self, X, h, m):\n",
    "        Wz, Wr, Wh = m['Wz'], m['Wr'], m['Wh']\n",
    "        bz, br, bh = m['bz'], m['br'], m['bh']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # The output\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache,\n",
    "                 hh, hh_cache, hh_tanh_cache)\n",
    "\n",
    "        return h, cache\n",
    "\n",
    "    def backward_(self, dh, cache):\n",
    "        X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache = cache\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        # The final output\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, bz=dbz, br=dbr, bh=dbh)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, XY_train, h):\n",
    "        # Adding the output layer cache\n",
    "        caches = []\n",
    "        num_modes = 2\n",
    "        for _ in range(num_modes):\n",
    "            caches.append([])\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches[0].append([])\n",
    "            caches[1].append([])\n",
    "            \n",
    "        ys = []\n",
    "        X, Y = XY_train\n",
    "        \n",
    "        # Input sequence\n",
    "        for x in X:\n",
    "            #             print(x.shape)\n",
    "            x= x.reshape(1, -1) # mat_1xn\n",
    "            #             print(x.shape)\n",
    "            \n",
    "            for layer in range(self.L-1):\n",
    "                x, h[layer], cache = self.forward(x, h[layer], self.model[0][layer])\n",
    "                caches[0][layer].append(cache)\n",
    "            \n",
    "            # The last layer without output\n",
    "            layer = self.L - 1\n",
    "            h[layer], cache = self.forward_(x, h[layer], self.model[0][layer])\n",
    "            caches[0][layer].append(cache)\n",
    "            \n",
    "        # Output sequence\n",
    "        for y in Y:\n",
    "            #             print(y.shape)\n",
    "            y= y.reshape(1, -1) # mat_1xn\n",
    "            #             print(y.shape)\n",
    "            \n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[1][layer])\n",
    "                caches[1][layer].append(cache)\n",
    "\n",
    "            # Output list\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def l2_regression(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        #         print('y_pred.shape[0]', m)\n",
    "\n",
    "        # (F(x)-y)^2: convex as X^2 or (aX-b)^2\n",
    "        data_loss = 0.5 * np.sum((y_pred - y_train)**2) / m # number of dimensions\n",
    "    \n",
    "        return data_loss\n",
    "\n",
    "    def dl2_regression(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        #         print('y_pred.shape[0]', m)\n",
    "\n",
    "        # (F(x)-y)^2: convex as X^2 or (aX-b)^2\n",
    "        dy = (y_pred - y_train)/ m # number of dimensions\n",
    "\n",
    "        return dy\n",
    "\n",
    "    def loss_function(self, y_pred, y_train):\n",
    "        loss, dys = 0.0, []\n",
    "        #         m = y_train.shape[0]\n",
    "        #         print('y_train.shape[0]', m)\n",
    "        #         print('len(y_pred):', len(y_pred))\n",
    "        #         print('np.array(y_pred).shape:', np.array(y_pred).shape)\n",
    "        #         print('y_train.shape:', y_train.shape)\n",
    "\n",
    "        for y, Y in zip(y_pred, y_train):\n",
    "            loss += self.l2_regression(y_pred=y, y_train=Y) #/ m # t or number of samples for taking the average\n",
    "            dy = self.dl2_regression(y_pred=y, y_train=Y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        num_modes = 2\n",
    "        for _ in range(num_modes):\n",
    "            grad.append([])\n",
    "            grads.append([])\n",
    "        \n",
    "        for _ in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            \n",
    "        for mode in range(num_modes):\n",
    "            for layer in range(self.L):\n",
    "                grad[mode].append({key: np.zeros_like(val) for key, val in self.model[mode][layer].items()})\n",
    "                grads[mode].append({key: np.zeros_like(val) for key, val in self.model[mode][layer].items()})\n",
    "\n",
    "        # Output sequence\n",
    "        mode = 1\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t].copy()\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[mode][layer] = self.backward(dX, dh[layer], caches[mode][layer][t])\n",
    "                for key in grad[mode][layer].keys():\n",
    "                    grads[mode][layer][key] += grad[mode][layer][key]\n",
    "\n",
    "        # Input sequence\n",
    "        mode = 0\n",
    "        for t in reversed(range(len(dys))):\n",
    "            # Output layer or last layer\n",
    "            layer = self.L-1\n",
    "            dX, dh[layer], grad[mode][layer] = self.backward_(dh[layer], caches[mode][layer][t])\n",
    "            for key in grad[mode][layer].keys():\n",
    "                grads[mode][layer][key] += grad[mode][layer][key]\n",
    "\n",
    "            # The depth and number of layers for RNN\n",
    "            for layer in reversed(range(self.L-1)):\n",
    "                dX, dh[layer], grad[mode][layer] = self.backward(dX, dh[layer], caches[mode][layer][t])\n",
    "                for k in grad[mode][layer].keys():\n",
    "                    grads[mode][layer][k] += grad[mode][layer][k]\n",
    "\n",
    "        return dX, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, Y, minibatch_size): # shuffle: this is for static data not dynamic/sequential data\n",
    "    minibatches = []\n",
    "    \n",
    "    # for i in range(start=0, stop=X.shape[0], step=minibatch_size):\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        Y_mini = Y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, Y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, alpha, mb_size, n_iter, print_after, n_files):\n",
    "    M, R = [], []\n",
    "    num_modes = 2\n",
    "    for _ in range(num_modes):\n",
    "        M.append([])\n",
    "        R.append([])\n",
    "\n",
    "    for mode in range(num_modes):\n",
    "        for layer in range(nn.L):\n",
    "            M[mode].append({k: np.zeros_like(v) for k, v in nn.model[mode][layer].items()})\n",
    "            R[mode].append({k: np.zeros_like(v) for k, v in nn.model[mode][layer].items()})\n",
    "\n",
    "    beta1 = .9\n",
    "    beta2 = .99\n",
    "    state = nn.initial_state()\n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    \n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        \n",
    "        # Read the new file, normalize it, and separate it into input and target\n",
    "        for file_num in range(1, n_files + 1):\n",
    "            \n",
    "#             if file_num == 1: data_path = '/Users/arasdar/data/Training_data/DATA_{:02}_TYPE01.mat'.format(file_num)\n",
    "#             else: data_path = '/Users/arasdar/data/Training_data/DATA_{:02}_TYPE02.mat'.format(file_num)\n",
    "            if file_num == 1: data_path = '/home/arasdar/data/Training_data/DATA_{:02}_TYPE01.mat'.format(file_num)\n",
    "            else: data_path = '/home/arasdar/data/Training_data/DATA_{:02}_TYPE02.mat'.format(file_num)\n",
    "            print(data_path)\n",
    "\n",
    "            # Read the mat files, normalize each batch of them, and seperate the output and input\n",
    "            X_train, Y_train = read_data(data_path=data_path)\n",
    "            #             X_train, Y_train_in = XY_train\n",
    "            #             print(X_train.shape, Y_train_in.shape, Y_train_out.shape)\n",
    "\n",
    "            # Minibatches/ Stochasticity\n",
    "            #             minibatches = get_minibatch(X_train, Y_train, mb_size, shuffle=False)\n",
    "            minibatches = get_minibatch(X=X_train, Y=Y_train, minibatch_size=mb_size) #, shuffle=False\n",
    "            #             print('The number of minibatches in a sequence for each epoch iteration: {}'.format (len(minibatches)))\n",
    "\n",
    "            # Minibatches/ Stochasticity\n",
    "            #             print('The number of minibatches in a sequence for each epoch iteration: {}'.format (len(minibatches)))\n",
    "            for idx in range(len(minibatches)):\n",
    "                \n",
    "                # Gradients\n",
    "                X_mini, Y_mini = minibatches[idx]\n",
    "                Y_mini_out = Y_mini[:-1] # mat[t, n]== mat_txn\n",
    "                Y_mini_in = np.row_stack((np.array([0.0]), Y_mini[:-2]))\n",
    "                #                 print(X_mini.shape, Y_mini.shape, Y_mini_out.shape)\n",
    "                print(X_mini[:4], Y_mini_in[:4], Y_mini_out[:4])\n",
    "                #                 print(Y_mini[:4], Y_mini_out[:4])\n",
    "                #             print(nn.model[0][0]['Wz'].shape, nn.model[1][0]['Wz'].shape, nn.C, nn.D)\n",
    "                #             print(nn.model[0][0]['Wr'].shape, nn.model[1][0]['Wh'].shape, nn.C, nn.D)\n",
    "\n",
    "                XY_mini = (X_mini, Y_mini_in)\n",
    "                ys, caches = nn.train_forward(XY_mini, state)\n",
    "                loss, dys = nn.loss_function(y_train=Y_mini_out, y_pred=ys)\n",
    "                _, grads = nn.train_backward(dys, caches)\n",
    "\n",
    "                # Descend\n",
    "                for mode in range(num_modes):\n",
    "                    for layer in range(nn.L):\n",
    "                        for key in grads[mode][layer].keys(): #key, value: items\n",
    "                            M[mode][layer][key] = l.exp_running_avg(M[mode][layer][key], grads[mode][layer][key], beta1)\n",
    "                            R[mode][layer][key] = l.exp_running_avg(R[mode][layer][key], grads[mode][layer][key]**2, beta2)\n",
    "\n",
    "                            m_k_hat = M[mode][layer][key] / (1. - (beta1**(iter)))\n",
    "                            r_k_hat = R[mode][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                            nn.model[mode][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                            \n",
    "                # MiniBatches: Drawing the learning curve for loss (Error/loss curve)\n",
    "                nn.losses['train'].append(loss)\n",
    "                \n",
    "            # FullBatches: Drawing the learning curve for loss (Error/loss curve)\n",
    "            # (based on batch normalization/bn at the input layer)\n",
    "            nn.losses['train2'].append(loss)\n",
    "            \n",
    "        # Epochs: Drawing the learning curve for loss (Error/loss curve)\n",
    "        nn.losses['train3'].append(loss)\n",
    "            \n",
    "        # Print training loss\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "                \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "n_iter = 1 # epochs: processing speed and how much time it takes.\n",
    "n_files = 1 # dataset size: maximum number of files in dataset based on 12 subjects\n",
    "print_after = n_iter//10 # print loss of train, valid, and test\n",
    "time_step = 200 # width of the model or minibatch size\n",
    "alpha = 1/time_step # learning_rate: 1e-3=0.001 - This is set to 1/miniatch_size (mb_size) or num_time_steps\n",
    "num_hidden_units = 64 # width of the hidden layers or number of hidden units in hidden layer\n",
    "num_hidden_layers = 1 # depth or number of hidden layer\n",
    "num_input_units = X_train.shape[1] # number of input features/dimensions\n",
    "num_output_units = Y_train.shape[1]\n",
    "# X_train.shape, Y_train.shape\n",
    "\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_hidden_layers, C=num_output_units)\n",
    "\n",
    "adam_rnn(nn=net, alpha=alpha, mb_size=time_step, n_iter=n_iter, n_files=n_files, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0d00cdc5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# % matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss2')\n",
    "# plt.plot(net.losses['train3'], label='Train loss3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
