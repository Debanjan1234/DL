{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "#             Wxh_res=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "#             Whh_res=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "#             bh_res=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "#         Wxh, Whh, Wxh_res, Whh_res, Why = m['Wxh'], m['Whh'], m['Wxh_res'], m['Whh_res'], m['Why']\n",
    "#         bh, bh_res, by = m['bh'], m['bh_res'], m['by']\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "\n",
    "#         # h_res for residual connection or skip connection for gradients\n",
    "#         # Residual connection to avoid vanishing gradients\n",
    "#         # SELU act_function to avoid exploding gradients\n",
    "#         # x+ f(x)\n",
    "#         h_res = (X @ Wxh_res) + (hprev @ Whh_res) + bh_res\n",
    "#         h += h_res\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "\n",
    "#         cache = (X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, do_cache)\n",
    "        cache = (X, hprev, Wxh, Whh, h_cache, y_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "#         X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, do_cache = cache\n",
    "        X, hprev, Wxh, Whh, h_cache, y_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "#         dh_res = dh.copy()\n",
    "#         dbh_res = dh_res * 1.0\n",
    "#         dWhh_res = hprev.T @ dh_res\n",
    "#         dWxh_res = X.T @ dh_res\n",
    "#         dX_res = dh_res @ Wxh_res.T\n",
    "#         dh_res = dh_res @ Whh_res.T\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh * 1.0\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "#         dX += dX_res\n",
    "#         dh += dh_res\n",
    "\n",
    "#         grad = dict(Wxh=dWxh, Whh=dWhh, Wxh_res=dWxh_res, Whh_res=dWhh_res, Why=dWhy, bh=dbh, bh_res=dbh_res, \n",
    "#                     by=dby)\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - beta1**(iter))\n",
    "                    r_k_hat = R[layer][key] / (1. - beta2**(iter))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 training loss: 3.0368\n",
      "e cs iver an Eh macePe G ontdlac  Fechaer.e res , ccon ank belode r okhel, coneodyr h6or fy kh lon ro\n",
      "Iter-260 training loss: 2.9243\n",
      "e nry Ged wan. onte is.Ne otittady本 sererth ocint Wou1. und woro r'n isuI9esore–s)le citelsk. anxled \n",
      "Iter-390 training loss: 2.9145\n",
      "ed ponisiuleteled one87 as Neve sard In arrJauaty, aprledlo, hed Jo187 ifo ms Jhe pe tord h hi idg iG\n",
      "Iter-520 training loss: 2.8611\n",
      "er sAraBdnito ltaUFtevarse ghe Nadopchhercd 6 morylitotomon n,rg-padegebe 1d Wacs gLhe thgemeture akh\n",
      "Iter-650 training loss: 3.0806\n",
      "ed rasasoBatea oma, imokh Jhif Elcane hh. mh Kritectee5de looLas on't tfans Ro ph an S wEmipace edice\n",
      "Iter-780 training loss: 3.0926\n",
      "er thitF dP rufsy rthe wor rand-f97 wht fsy an osigede th atOoriplitipa2ifasaladead Serad ensice thir\n",
      "Iter-910 training loss: 2.9310\n",
      "ernoranibzesisk cas if WomacriDe iOhe aoces paoIr 日8 atP mone, th id mteTaco pNonded lpginsre ifl. 1d\n",
      "Iter-1040 training loss: 3.0142\n",
      "e whh'itt fand ceFlt 1kh roriderar arir. cem oltunst a, Emvv'fl ipdibalado jl'cekin pheupol any rth I\n",
      "Iter-1170 training loss: 2.7526\n",
      "einris. Asty mhe irWa:7g y2 an cArthe.8r\"e Aest. 1m ylepesa0r'saky is. Th anrse, chosd Fun efe Sbitan\n",
      "Iter-1300 training loss: 2.6808\n",
      "erudaced hajdecaped any opy op–nde casetard onlosg'f Gntelasy ras wolax thopre G Jar\"ane, on thy Aesd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7fec291d2eb8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 1300 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.95 # keep_prob\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRORj/x+Tl02ehpL05QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7febfd8fc4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
