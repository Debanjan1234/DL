{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "#         y, _ = l.relu_forward(X=y)\n",
    "#         y, _ = l.lrelu_forward(X=y)\n",
    "        y = l.elu_fwd(X=y)\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "#             y, _ = l.relu_forward(X=y)\n",
    "#             y, _ = l.lrelu_forward(X=y)\n",
    "            y = l.elu_fwd(X=y)\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3031 valid loss: 2.3038, valid accuracy: 0.0882\n",
      "Iter-20 train loss: 2.3029 valid loss: 2.3038, valid accuracy: 0.0880\n",
      "Iter-30 train loss: 2.3032 valid loss: 2.3038, valid accuracy: 0.0882\n",
      "Iter-40 train loss: 2.3002 valid loss: 2.3038, valid accuracy: 0.0868\n",
      "Iter-50 train loss: 2.3047 valid loss: 2.3038, valid accuracy: 0.0870\n",
      "Iter-60 train loss: 2.3018 valid loss: 2.3037, valid accuracy: 0.0870\n",
      "Iter-70 train loss: 2.3036 valid loss: 2.3037, valid accuracy: 0.0858\n",
      "Iter-80 train loss: 2.3016 valid loss: 2.3037, valid accuracy: 0.0860\n",
      "Iter-90 train loss: 2.3005 valid loss: 2.3037, valid accuracy: 0.0860\n",
      "Iter-100 train loss: 2.3008 valid loss: 2.3037, valid accuracy: 0.0862\n",
      "Iter-110 train loss: 2.3052 valid loss: 2.3037, valid accuracy: 0.0858\n",
      "Iter-120 train loss: 2.3046 valid loss: 2.3037, valid accuracy: 0.0862\n",
      "Iter-130 train loss: 2.3051 valid loss: 2.3037, valid accuracy: 0.0864\n",
      "Iter-140 train loss: 2.3033 valid loss: 2.3037, valid accuracy: 0.0852\n",
      "Iter-150 train loss: 2.3007 valid loss: 2.3037, valid accuracy: 0.0866\n",
      "Iter-160 train loss: 2.3011 valid loss: 2.3037, valid accuracy: 0.0866\n",
      "Iter-170 train loss: 2.3031 valid loss: 2.3037, valid accuracy: 0.0858\n",
      "Iter-180 train loss: 2.3034 valid loss: 2.3037, valid accuracy: 0.0860\n",
      "Iter-190 train loss: 2.3034 valid loss: 2.3037, valid accuracy: 0.0864\n",
      "Iter-200 train loss: 2.3038 valid loss: 2.3037, valid accuracy: 0.0868\n",
      "Iter-210 train loss: 2.2993 valid loss: 2.3037, valid accuracy: 0.0862\n",
      "Iter-220 train loss: 2.3041 valid loss: 2.3036, valid accuracy: 0.0864\n",
      "Iter-230 train loss: 2.3035 valid loss: 2.3036, valid accuracy: 0.0856\n",
      "Iter-240 train loss: 2.3038 valid loss: 2.3036, valid accuracy: 0.0864\n",
      "Iter-250 train loss: 2.2999 valid loss: 2.3036, valid accuracy: 0.0860\n",
      "Iter-260 train loss: 2.3037 valid loss: 2.3036, valid accuracy: 0.0856\n",
      "Iter-270 train loss: 2.3055 valid loss: 2.3036, valid accuracy: 0.0854\n",
      "Iter-280 train loss: 2.3045 valid loss: 2.3036, valid accuracy: 0.0850\n",
      "Iter-290 train loss: 2.3023 valid loss: 2.3036, valid accuracy: 0.0862\n",
      "Iter-300 train loss: 2.3063 valid loss: 2.3036, valid accuracy: 0.0854\n",
      "Iter-310 train loss: 2.3039 valid loss: 2.3036, valid accuracy: 0.0848\n",
      "Iter-320 train loss: 2.3017 valid loss: 2.3036, valid accuracy: 0.0854\n",
      "Iter-330 train loss: 2.3069 valid loss: 2.3036, valid accuracy: 0.0856\n",
      "Iter-340 train loss: 2.3012 valid loss: 2.3036, valid accuracy: 0.0850\n",
      "Iter-350 train loss: 2.3044 valid loss: 2.3036, valid accuracy: 0.0850\n",
      "Iter-360 train loss: 2.3048 valid loss: 2.3036, valid accuracy: 0.0848\n",
      "Iter-370 train loss: 2.3013 valid loss: 2.3036, valid accuracy: 0.0858\n",
      "Iter-380 train loss: 2.3009 valid loss: 2.3035, valid accuracy: 0.0860\n",
      "Iter-390 train loss: 2.3030 valid loss: 2.3035, valid accuracy: 0.0850\n",
      "Iter-400 train loss: 2.3064 valid loss: 2.3035, valid accuracy: 0.0844\n",
      "Iter-410 train loss: 2.3039 valid loss: 2.3035, valid accuracy: 0.0842\n",
      "Iter-420 train loss: 2.3023 valid loss: 2.3035, valid accuracy: 0.0840\n",
      "Iter-430 train loss: 2.3049 valid loss: 2.3035, valid accuracy: 0.0838\n",
      "Iter-440 train loss: 2.3038 valid loss: 2.3035, valid accuracy: 0.0838\n",
      "Iter-450 train loss: 2.3040 valid loss: 2.3035, valid accuracy: 0.0850\n",
      "Iter-460 train loss: 2.3012 valid loss: 2.3035, valid accuracy: 0.0862\n",
      "Iter-470 train loss: 2.3019 valid loss: 2.3035, valid accuracy: 0.0862\n",
      "Iter-480 train loss: 2.3063 valid loss: 2.3035, valid accuracy: 0.0860\n",
      "Iter-490 train loss: 2.3036 valid loss: 2.3035, valid accuracy: 0.0860\n",
      "Iter-500 train loss: 2.3013 valid loss: 2.3035, valid accuracy: 0.0858\n",
      "Iter-510 train loss: 2.3018 valid loss: 2.3035, valid accuracy: 0.0848\n",
      "Iter-520 train loss: 2.3035 valid loss: 2.3035, valid accuracy: 0.0846\n",
      "Iter-530 train loss: 2.3014 valid loss: 2.3035, valid accuracy: 0.0844\n",
      "Iter-540 train loss: 2.3081 valid loss: 2.3035, valid accuracy: 0.0840\n",
      "Iter-550 train loss: 2.3050 valid loss: 2.3035, valid accuracy: 0.0844\n",
      "Iter-560 train loss: 2.3032 valid loss: 2.3035, valid accuracy: 0.0860\n",
      "Iter-570 train loss: 2.3030 valid loss: 2.3035, valid accuracy: 0.0860\n",
      "Iter-580 train loss: 2.3032 valid loss: 2.3035, valid accuracy: 0.0868\n",
      "Iter-590 train loss: 2.3032 valid loss: 2.3034, valid accuracy: 0.0868\n",
      "Iter-600 train loss: 2.3037 valid loss: 2.3034, valid accuracy: 0.0862\n",
      "Iter-610 train loss: 2.3007 valid loss: 2.3034, valid accuracy: 0.0854\n",
      "Iter-620 train loss: 2.3033 valid loss: 2.3034, valid accuracy: 0.0850\n",
      "Iter-630 train loss: 2.3029 valid loss: 2.3034, valid accuracy: 0.0842\n",
      "Iter-640 train loss: 2.3044 valid loss: 2.3034, valid accuracy: 0.0854\n",
      "Iter-650 train loss: 2.3025 valid loss: 2.3034, valid accuracy: 0.0848\n",
      "Iter-660 train loss: 2.3021 valid loss: 2.3034, valid accuracy: 0.0866\n",
      "Iter-670 train loss: 2.3023 valid loss: 2.3034, valid accuracy: 0.0854\n",
      "Iter-680 train loss: 2.3034 valid loss: 2.3034, valid accuracy: 0.0856\n",
      "Iter-690 train loss: 2.3009 valid loss: 2.3034, valid accuracy: 0.0852\n",
      "Iter-700 train loss: 2.3009 valid loss: 2.3034, valid accuracy: 0.0854\n",
      "Iter-710 train loss: 2.3062 valid loss: 2.3034, valid accuracy: 0.0868\n",
      "Iter-720 train loss: 2.3045 valid loss: 2.3034, valid accuracy: 0.0862\n",
      "Iter-730 train loss: 2.2993 valid loss: 2.3034, valid accuracy: 0.0870\n",
      "Iter-740 train loss: 2.3017 valid loss: 2.3034, valid accuracy: 0.0870\n",
      "Iter-750 train loss: 2.3044 valid loss: 2.3034, valid accuracy: 0.0858\n",
      "Iter-760 train loss: 2.3014 valid loss: 2.3034, valid accuracy: 0.0860\n",
      "Iter-770 train loss: 2.3029 valid loss: 2.3034, valid accuracy: 0.0856\n",
      "Iter-780 train loss: 2.3032 valid loss: 2.3034, valid accuracy: 0.0858\n",
      "Iter-790 train loss: 2.3052 valid loss: 2.3034, valid accuracy: 0.0864\n",
      "Iter-800 train loss: 2.3066 valid loss: 2.3034, valid accuracy: 0.0860\n",
      "Iter-810 train loss: 2.3059 valid loss: 2.3034, valid accuracy: 0.0858\n",
      "Iter-820 train loss: 2.3014 valid loss: 2.3034, valid accuracy: 0.0862\n",
      "Iter-830 train loss: 2.3001 valid loss: 2.3034, valid accuracy: 0.0872\n",
      "Iter-840 train loss: 2.3020 valid loss: 2.3034, valid accuracy: 0.0872\n",
      "Iter-850 train loss: 2.3056 valid loss: 2.3033, valid accuracy: 0.0862\n",
      "Iter-860 train loss: 2.3066 valid loss: 2.3033, valid accuracy: 0.0868\n",
      "Iter-870 train loss: 2.3036 valid loss: 2.3033, valid accuracy: 0.0882\n",
      "Iter-880 train loss: 2.3041 valid loss: 2.3033, valid accuracy: 0.0880\n",
      "Iter-890 train loss: 2.3000 valid loss: 2.3033, valid accuracy: 0.0880\n",
      "Iter-900 train loss: 2.3047 valid loss: 2.3033, valid accuracy: 0.0892\n",
      "Iter-910 train loss: 2.3041 valid loss: 2.3033, valid accuracy: 0.0892\n",
      "Iter-920 train loss: 2.3029 valid loss: 2.3033, valid accuracy: 0.0896\n",
      "Iter-930 train loss: 2.2995 valid loss: 2.3033, valid accuracy: 0.0894\n",
      "Iter-940 train loss: 2.3019 valid loss: 2.3033, valid accuracy: 0.0894\n",
      "Iter-950 train loss: 2.3028 valid loss: 2.3033, valid accuracy: 0.0896\n",
      "Iter-960 train loss: 2.3030 valid loss: 2.3033, valid accuracy: 0.0884\n",
      "Iter-970 train loss: 2.2998 valid loss: 2.3033, valid accuracy: 0.0888\n",
      "Iter-980 train loss: 2.3044 valid loss: 2.3033, valid accuracy: 0.0894\n",
      "Iter-990 train loss: 2.3029 valid loss: 2.3033, valid accuracy: 0.0892\n",
      "Iter-1000 train loss: 2.3033 valid loss: 2.3033, valid accuracy: 0.0896\n",
      "Iter-1010 train loss: 2.3043 valid loss: 2.3033, valid accuracy: 0.0900\n",
      "Iter-1020 train loss: 2.3015 valid loss: 2.3033, valid accuracy: 0.0898\n",
      "Iter-1030 train loss: 2.3046 valid loss: 2.3033, valid accuracy: 0.0884\n",
      "Iter-1040 train loss: 2.3031 valid loss: 2.3032, valid accuracy: 0.0894\n",
      "Iter-1050 train loss: 2.3029 valid loss: 2.3032, valid accuracy: 0.0902\n",
      "Iter-1060 train loss: 2.3026 valid loss: 2.3032, valid accuracy: 0.0900\n",
      "Iter-1070 train loss: 2.3029 valid loss: 2.3032, valid accuracy: 0.0896\n",
      "Iter-1080 train loss: 2.3037 valid loss: 2.3032, valid accuracy: 0.0888\n",
      "Iter-1090 train loss: 2.3049 valid loss: 2.3032, valid accuracy: 0.0886\n",
      "Iter-1100 train loss: 2.3042 valid loss: 2.3032, valid accuracy: 0.0878\n",
      "Iter-1110 train loss: 2.3031 valid loss: 2.3032, valid accuracy: 0.0888\n",
      "Iter-1120 train loss: 2.3062 valid loss: 2.3032, valid accuracy: 0.0888\n",
      "Iter-1130 train loss: 2.3013 valid loss: 2.3032, valid accuracy: 0.0884\n",
      "Iter-1140 train loss: 2.2992 valid loss: 2.3032, valid accuracy: 0.0878\n",
      "Iter-1150 train loss: 2.3028 valid loss: 2.3032, valid accuracy: 0.0894\n",
      "Iter-1160 train loss: 2.3043 valid loss: 2.3032, valid accuracy: 0.0888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3060 valid loss: 2.3032, valid accuracy: 0.0882\n",
      "Iter-1180 train loss: 2.3074 valid loss: 2.3032, valid accuracy: 0.0886\n",
      "Iter-1190 train loss: 2.3046 valid loss: 2.3032, valid accuracy: 0.0882\n",
      "Iter-1200 train loss: 2.3056 valid loss: 2.3032, valid accuracy: 0.0880\n",
      "Iter-1210 train loss: 2.3030 valid loss: 2.3032, valid accuracy: 0.0876\n",
      "Iter-1220 train loss: 2.3018 valid loss: 2.3032, valid accuracy: 0.0876\n",
      "Iter-1230 train loss: 2.3008 valid loss: 2.3032, valid accuracy: 0.0880\n",
      "Iter-1240 train loss: 2.3057 valid loss: 2.3032, valid accuracy: 0.0888\n",
      "Iter-1250 train loss: 2.3038 valid loss: 2.3032, valid accuracy: 0.0884\n",
      "Iter-1260 train loss: 2.3031 valid loss: 2.3032, valid accuracy: 0.0882\n",
      "Iter-1270 train loss: 2.3025 valid loss: 2.3032, valid accuracy: 0.0890\n",
      "Iter-1280 train loss: 2.3028 valid loss: 2.3031, valid accuracy: 0.0890\n",
      "Iter-1290 train loss: 2.3012 valid loss: 2.3031, valid accuracy: 0.0900\n",
      "Iter-1300 train loss: 2.3035 valid loss: 2.3031, valid accuracy: 0.0900\n",
      "Iter-1310 train loss: 2.3029 valid loss: 2.3031, valid accuracy: 0.0904\n",
      "Iter-1320 train loss: 2.3028 valid loss: 2.3031, valid accuracy: 0.0896\n",
      "Iter-1330 train loss: 2.3023 valid loss: 2.3031, valid accuracy: 0.0914\n",
      "Iter-1340 train loss: 2.3053 valid loss: 2.3031, valid accuracy: 0.0918\n",
      "Iter-1350 train loss: 2.3012 valid loss: 2.3031, valid accuracy: 0.0904\n",
      "Iter-1360 train loss: 2.3039 valid loss: 2.3031, valid accuracy: 0.0898\n",
      "Iter-1370 train loss: 2.3026 valid loss: 2.3031, valid accuracy: 0.0908\n",
      "Iter-1380 train loss: 2.3057 valid loss: 2.3031, valid accuracy: 0.0906\n",
      "Iter-1390 train loss: 2.3029 valid loss: 2.3031, valid accuracy: 0.0898\n",
      "Iter-1400 train loss: 2.3013 valid loss: 2.3031, valid accuracy: 0.0890\n",
      "Iter-1410 train loss: 2.3009 valid loss: 2.3031, valid accuracy: 0.0888\n",
      "Iter-1420 train loss: 2.3052 valid loss: 2.3031, valid accuracy: 0.0892\n",
      "Iter-1430 train loss: 2.3063 valid loss: 2.3031, valid accuracy: 0.0894\n",
      "Iter-1440 train loss: 2.3032 valid loss: 2.3031, valid accuracy: 0.0892\n",
      "Iter-1450 train loss: 2.3014 valid loss: 2.3031, valid accuracy: 0.0898\n",
      "Iter-1460 train loss: 2.3047 valid loss: 2.3031, valid accuracy: 0.0900\n",
      "Iter-1470 train loss: 2.3035 valid loss: 2.3031, valid accuracy: 0.0900\n",
      "Iter-1480 train loss: 2.3016 valid loss: 2.3031, valid accuracy: 0.0900\n",
      "Iter-1490 train loss: 2.3023 valid loss: 2.3030, valid accuracy: 0.0896\n",
      "Iter-1500 train loss: 2.3029 valid loss: 2.3030, valid accuracy: 0.0902\n",
      "Iter-1510 train loss: 2.3049 valid loss: 2.3030, valid accuracy: 0.0900\n",
      "Iter-1520 train loss: 2.2994 valid loss: 2.3030, valid accuracy: 0.0906\n",
      "Iter-1530 train loss: 2.3038 valid loss: 2.3030, valid accuracy: 0.0900\n",
      "Iter-1540 train loss: 2.3031 valid loss: 2.3030, valid accuracy: 0.0892\n",
      "Iter-1550 train loss: 2.3027 valid loss: 2.3030, valid accuracy: 0.0894\n",
      "Iter-1560 train loss: 2.3035 valid loss: 2.3030, valid accuracy: 0.0898\n",
      "Iter-1570 train loss: 2.2996 valid loss: 2.3030, valid accuracy: 0.0900\n",
      "Iter-1580 train loss: 2.3038 valid loss: 2.3030, valid accuracy: 0.0900\n",
      "Iter-1590 train loss: 2.3027 valid loss: 2.3030, valid accuracy: 0.0902\n",
      "Iter-1600 train loss: 2.3053 valid loss: 2.3030, valid accuracy: 0.0896\n",
      "Iter-1610 train loss: 2.3052 valid loss: 2.3030, valid accuracy: 0.0910\n",
      "Iter-1620 train loss: 2.3012 valid loss: 2.3030, valid accuracy: 0.0912\n",
      "Iter-1630 train loss: 2.3062 valid loss: 2.3030, valid accuracy: 0.0930\n",
      "Iter-1640 train loss: 2.3053 valid loss: 2.3030, valid accuracy: 0.0926\n",
      "Iter-1650 train loss: 2.3029 valid loss: 2.3030, valid accuracy: 0.0936\n",
      "Iter-1660 train loss: 2.2992 valid loss: 2.3030, valid accuracy: 0.0942\n",
      "Iter-1670 train loss: 2.3025 valid loss: 2.3030, valid accuracy: 0.0942\n",
      "Iter-1680 train loss: 2.3068 valid loss: 2.3030, valid accuracy: 0.0944\n",
      "Iter-1690 train loss: 2.3025 valid loss: 2.3030, valid accuracy: 0.0938\n",
      "Iter-1700 train loss: 2.3047 valid loss: 2.3030, valid accuracy: 0.0936\n",
      "Iter-1710 train loss: 2.3037 valid loss: 2.3030, valid accuracy: 0.0932\n",
      "Iter-1720 train loss: 2.3037 valid loss: 2.3030, valid accuracy: 0.0932\n",
      "Iter-1730 train loss: 2.3028 valid loss: 2.3029, valid accuracy: 0.0938\n",
      "Iter-1740 train loss: 2.3034 valid loss: 2.3029, valid accuracy: 0.0954\n",
      "Iter-1750 train loss: 2.3001 valid loss: 2.3029, valid accuracy: 0.0952\n",
      "Iter-1760 train loss: 2.3025 valid loss: 2.3029, valid accuracy: 0.0950\n",
      "Iter-1770 train loss: 2.3025 valid loss: 2.3029, valid accuracy: 0.0944\n",
      "Iter-1780 train loss: 2.3014 valid loss: 2.3029, valid accuracy: 0.0960\n",
      "Iter-1790 train loss: 2.3027 valid loss: 2.3029, valid accuracy: 0.0950\n",
      "Iter-1800 train loss: 2.3068 valid loss: 2.3029, valid accuracy: 0.0952\n",
      "Iter-1810 train loss: 2.3011 valid loss: 2.3029, valid accuracy: 0.0962\n",
      "Iter-1820 train loss: 2.3003 valid loss: 2.3029, valid accuracy: 0.0950\n",
      "Iter-1830 train loss: 2.3047 valid loss: 2.3029, valid accuracy: 0.0938\n",
      "Iter-1840 train loss: 2.3040 valid loss: 2.3029, valid accuracy: 0.0960\n",
      "Iter-1850 train loss: 2.3058 valid loss: 2.3029, valid accuracy: 0.0946\n",
      "Iter-1860 train loss: 2.3050 valid loss: 2.3029, valid accuracy: 0.0950\n",
      "Iter-1870 train loss: 2.3031 valid loss: 2.3029, valid accuracy: 0.0954\n",
      "Iter-1880 train loss: 2.3042 valid loss: 2.3029, valid accuracy: 0.0964\n",
      "Iter-1890 train loss: 2.3036 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-1900 train loss: 2.3015 valid loss: 2.3028, valid accuracy: 0.0966\n",
      "Iter-1910 train loss: 2.3031 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1920 train loss: 2.3019 valid loss: 2.3028, valid accuracy: 0.0974\n",
      "Iter-1930 train loss: 2.3033 valid loss: 2.3028, valid accuracy: 0.0958\n",
      "Iter-1940 train loss: 2.3037 valid loss: 2.3028, valid accuracy: 0.0954\n",
      "Iter-1950 train loss: 2.3028 valid loss: 2.3028, valid accuracy: 0.0950\n",
      "Iter-1960 train loss: 2.3025 valid loss: 2.3028, valid accuracy: 0.0954\n",
      "Iter-1970 train loss: 2.3047 valid loss: 2.3028, valid accuracy: 0.0958\n",
      "Iter-1980 train loss: 2.3043 valid loss: 2.3028, valid accuracy: 0.0960\n",
      "Iter-1990 train loss: 2.3035 valid loss: 2.3028, valid accuracy: 0.0966\n",
      "Iter-2000 train loss: 2.3027 valid loss: 2.3028, valid accuracy: 0.0966\n",
      "Iter-2010 train loss: 2.3016 valid loss: 2.3028, valid accuracy: 0.0964\n",
      "Iter-2020 train loss: 2.3029 valid loss: 2.3028, valid accuracy: 0.0968\n",
      "Iter-2030 train loss: 2.3047 valid loss: 2.3028, valid accuracy: 0.0962\n",
      "Iter-2040 train loss: 2.3043 valid loss: 2.3028, valid accuracy: 0.0964\n",
      "Iter-2050 train loss: 2.2995 valid loss: 2.3028, valid accuracy: 0.0962\n",
      "Iter-2060 train loss: 2.3015 valid loss: 2.3028, valid accuracy: 0.0952\n",
      "Iter-2070 train loss: 2.3009 valid loss: 2.3028, valid accuracy: 0.0956\n",
      "Iter-2080 train loss: 2.3044 valid loss: 2.3028, valid accuracy: 0.0960\n",
      "Iter-2090 train loss: 2.3011 valid loss: 2.3027, valid accuracy: 0.0966\n",
      "Iter-2100 train loss: 2.3010 valid loss: 2.3027, valid accuracy: 0.0962\n",
      "Iter-2110 train loss: 2.3025 valid loss: 2.3027, valid accuracy: 0.0962\n",
      "Iter-2120 train loss: 2.3038 valid loss: 2.3027, valid accuracy: 0.0964\n",
      "Iter-2130 train loss: 2.3016 valid loss: 2.3027, valid accuracy: 0.0968\n",
      "Iter-2140 train loss: 2.3018 valid loss: 2.3027, valid accuracy: 0.0970\n",
      "Iter-2150 train loss: 2.3016 valid loss: 2.3027, valid accuracy: 0.0964\n",
      "Iter-2160 train loss: 2.3020 valid loss: 2.3027, valid accuracy: 0.0958\n",
      "Iter-2170 train loss: 2.3015 valid loss: 2.3027, valid accuracy: 0.0954\n",
      "Iter-2180 train loss: 2.3030 valid loss: 2.3027, valid accuracy: 0.0958\n",
      "Iter-2190 train loss: 2.3017 valid loss: 2.3027, valid accuracy: 0.0966\n",
      "Iter-2200 train loss: 2.3002 valid loss: 2.3027, valid accuracy: 0.0960\n",
      "Iter-2210 train loss: 2.3000 valid loss: 2.3027, valid accuracy: 0.0968\n",
      "Iter-2220 train loss: 2.3000 valid loss: 2.3027, valid accuracy: 0.0960\n",
      "Iter-2230 train loss: 2.3037 valid loss: 2.3027, valid accuracy: 0.0964\n",
      "Iter-2240 train loss: 2.2987 valid loss: 2.3027, valid accuracy: 0.0966\n",
      "Iter-2250 train loss: 2.3021 valid loss: 2.3027, valid accuracy: 0.0964\n",
      "Iter-2260 train loss: 2.3020 valid loss: 2.3027, valid accuracy: 0.0970\n",
      "Iter-2270 train loss: 2.3034 valid loss: 2.3027, valid accuracy: 0.0972\n",
      "Iter-2280 train loss: 2.3015 valid loss: 2.3027, valid accuracy: 0.0968\n",
      "Iter-2290 train loss: 2.3064 valid loss: 2.3027, valid accuracy: 0.0964\n",
      "Iter-2300 train loss: 2.2993 valid loss: 2.3026, valid accuracy: 0.0960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3046 valid loss: 2.3026, valid accuracy: 0.0958\n",
      "Iter-2320 train loss: 2.3002 valid loss: 2.3026, valid accuracy: 0.0962\n",
      "Iter-2330 train loss: 2.3011 valid loss: 2.3026, valid accuracy: 0.0962\n",
      "Iter-2340 train loss: 2.2981 valid loss: 2.3026, valid accuracy: 0.0962\n",
      "Iter-2350 train loss: 2.3040 valid loss: 2.3026, valid accuracy: 0.0960\n",
      "Iter-2360 train loss: 2.3028 valid loss: 2.3026, valid accuracy: 0.0964\n",
      "Iter-2370 train loss: 2.3038 valid loss: 2.3026, valid accuracy: 0.0958\n",
      "Iter-2380 train loss: 2.3015 valid loss: 2.3026, valid accuracy: 0.0958\n",
      "Iter-2390 train loss: 2.3047 valid loss: 2.3026, valid accuracy: 0.0956\n",
      "Iter-2400 train loss: 2.3049 valid loss: 2.3026, valid accuracy: 0.0960\n",
      "Iter-2410 train loss: 2.3034 valid loss: 2.3026, valid accuracy: 0.0956\n",
      "Iter-2420 train loss: 2.3022 valid loss: 2.3026, valid accuracy: 0.0958\n",
      "Iter-2430 train loss: 2.3024 valid loss: 2.3026, valid accuracy: 0.0960\n",
      "Iter-2440 train loss: 2.3031 valid loss: 2.3026, valid accuracy: 0.0958\n",
      "Iter-2450 train loss: 2.3043 valid loss: 2.3026, valid accuracy: 0.0958\n",
      "Iter-2460 train loss: 2.3034 valid loss: 2.3026, valid accuracy: 0.0960\n",
      "Iter-2470 train loss: 2.3002 valid loss: 2.3026, valid accuracy: 0.0968\n",
      "Iter-2480 train loss: 2.3018 valid loss: 2.3026, valid accuracy: 0.0980\n",
      "Iter-2490 train loss: 2.3030 valid loss: 2.3026, valid accuracy: 0.0974\n",
      "Iter-2500 train loss: 2.2994 valid loss: 2.3026, valid accuracy: 0.0974\n",
      "Iter-2510 train loss: 2.3035 valid loss: 2.3026, valid accuracy: 0.0974\n",
      "Iter-2520 train loss: 2.3041 valid loss: 2.3026, valid accuracy: 0.0978\n",
      "Iter-2530 train loss: 2.3061 valid loss: 2.3026, valid accuracy: 0.0974\n",
      "Iter-2540 train loss: 2.3010 valid loss: 2.3026, valid accuracy: 0.0980\n",
      "Iter-2550 train loss: 2.2996 valid loss: 2.3026, valid accuracy: 0.0978\n",
      "Iter-2560 train loss: 2.3043 valid loss: 2.3026, valid accuracy: 0.0978\n",
      "Iter-2570 train loss: 2.2993 valid loss: 2.3025, valid accuracy: 0.0970\n",
      "Iter-2580 train loss: 2.3009 valid loss: 2.3025, valid accuracy: 0.0972\n",
      "Iter-2590 train loss: 2.3048 valid loss: 2.3025, valid accuracy: 0.0976\n",
      "Iter-2600 train loss: 2.3009 valid loss: 2.3025, valid accuracy: 0.0976\n",
      "Iter-2610 train loss: 2.3030 valid loss: 2.3025, valid accuracy: 0.0974\n",
      "Iter-2620 train loss: 2.3009 valid loss: 2.3025, valid accuracy: 0.0976\n",
      "Iter-2630 train loss: 2.3042 valid loss: 2.3025, valid accuracy: 0.0972\n",
      "Iter-2640 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.0974\n",
      "Iter-2650 train loss: 2.3041 valid loss: 2.3025, valid accuracy: 0.0970\n",
      "Iter-2660 train loss: 2.3040 valid loss: 2.3025, valid accuracy: 0.0974\n",
      "Iter-2670 train loss: 2.3017 valid loss: 2.3025, valid accuracy: 0.0968\n",
      "Iter-2680 train loss: 2.2991 valid loss: 2.3025, valid accuracy: 0.0968\n",
      "Iter-2690 train loss: 2.3010 valid loss: 2.3025, valid accuracy: 0.0974\n",
      "Iter-2700 train loss: 2.3035 valid loss: 2.3025, valid accuracy: 0.0972\n",
      "Iter-2710 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.0984\n",
      "Iter-2720 train loss: 2.2963 valid loss: 2.3025, valid accuracy: 0.0986\n",
      "Iter-2730 train loss: 2.3015 valid loss: 2.3025, valid accuracy: 0.0992\n",
      "Iter-2740 train loss: 2.3037 valid loss: 2.3025, valid accuracy: 0.0988\n",
      "Iter-2750 train loss: 2.3043 valid loss: 2.3025, valid accuracy: 0.0996\n",
      "Iter-2760 train loss: 2.3010 valid loss: 2.3025, valid accuracy: 0.1000\n",
      "Iter-2770 train loss: 2.3054 valid loss: 2.3025, valid accuracy: 0.1008\n",
      "Iter-2780 train loss: 2.2983 valid loss: 2.3025, valid accuracy: 0.1014\n",
      "Iter-2790 train loss: 2.3013 valid loss: 2.3025, valid accuracy: 0.1010\n",
      "Iter-2800 train loss: 2.2987 valid loss: 2.3025, valid accuracy: 0.1022\n",
      "Iter-2810 train loss: 2.2974 valid loss: 2.3024, valid accuracy: 0.1032\n",
      "Iter-2820 train loss: 2.2993 valid loss: 2.3024, valid accuracy: 0.1038\n",
      "Iter-2830 train loss: 2.3002 valid loss: 2.3024, valid accuracy: 0.1036\n",
      "Iter-2840 train loss: 2.3056 valid loss: 2.3024, valid accuracy: 0.1044\n",
      "Iter-2850 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1050\n",
      "Iter-2860 train loss: 2.3038 valid loss: 2.3024, valid accuracy: 0.1058\n",
      "Iter-2870 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1060\n",
      "Iter-2880 train loss: 2.2989 valid loss: 2.3024, valid accuracy: 0.1054\n",
      "Iter-2890 train loss: 2.3058 valid loss: 2.3024, valid accuracy: 0.1054\n",
      "Iter-2900 train loss: 2.3015 valid loss: 2.3024, valid accuracy: 0.1062\n",
      "Iter-2910 train loss: 2.3015 valid loss: 2.3024, valid accuracy: 0.1064\n",
      "Iter-2920 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1056\n",
      "Iter-2930 train loss: 2.3013 valid loss: 2.3024, valid accuracy: 0.1074\n",
      "Iter-2940 train loss: 2.3012 valid loss: 2.3024, valid accuracy: 0.1064\n",
      "Iter-2950 train loss: 2.3006 valid loss: 2.3024, valid accuracy: 0.1062\n",
      "Iter-2960 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1066\n",
      "Iter-2970 train loss: 2.3064 valid loss: 2.3024, valid accuracy: 0.1064\n",
      "Iter-2980 train loss: 2.2998 valid loss: 2.3024, valid accuracy: 0.1076\n",
      "Iter-2990 train loss: 2.3044 valid loss: 2.3024, valid accuracy: 0.1074\n",
      "Iter-3000 train loss: 2.3020 valid loss: 2.3024, valid accuracy: 0.1068\n",
      "Iter-3010 train loss: 2.3041 valid loss: 2.3023, valid accuracy: 0.1070\n",
      "Iter-3020 train loss: 2.3039 valid loss: 2.3023, valid accuracy: 0.1068\n",
      "Iter-3030 train loss: 2.3013 valid loss: 2.3023, valid accuracy: 0.1074\n",
      "Iter-3040 train loss: 2.2986 valid loss: 2.3023, valid accuracy: 0.1068\n",
      "Iter-3050 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1066\n",
      "Iter-3060 train loss: 2.2987 valid loss: 2.3023, valid accuracy: 0.1070\n",
      "Iter-3070 train loss: 2.3015 valid loss: 2.3023, valid accuracy: 0.1076\n",
      "Iter-3080 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1074\n",
      "Iter-3090 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1074\n",
      "Iter-3100 train loss: 2.3007 valid loss: 2.3023, valid accuracy: 0.1074\n",
      "Iter-3110 train loss: 2.2963 valid loss: 2.3023, valid accuracy: 0.1072\n",
      "Iter-3120 train loss: 2.3014 valid loss: 2.3023, valid accuracy: 0.1072\n",
      "Iter-3130 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1072\n",
      "Iter-3140 train loss: 2.3000 valid loss: 2.3023, valid accuracy: 0.1076\n",
      "Iter-3150 train loss: 2.3047 valid loss: 2.3023, valid accuracy: 0.1084\n",
      "Iter-3160 train loss: 2.2967 valid loss: 2.3023, valid accuracy: 0.1076\n",
      "Iter-3170 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1082\n",
      "Iter-3180 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1084\n",
      "Iter-3190 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1084\n",
      "Iter-3200 train loss: 2.3038 valid loss: 2.3022, valid accuracy: 0.1076\n",
      "Iter-3210 train loss: 2.3039 valid loss: 2.3022, valid accuracy: 0.1072\n",
      "Iter-3220 train loss: 2.2992 valid loss: 2.3022, valid accuracy: 0.1068\n",
      "Iter-3230 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1068\n",
      "Iter-3240 train loss: 2.3023 valid loss: 2.3022, valid accuracy: 0.1070\n",
      "Iter-3250 train loss: 2.3030 valid loss: 2.3022, valid accuracy: 0.1062\n",
      "Iter-3260 train loss: 2.3016 valid loss: 2.3022, valid accuracy: 0.1066\n",
      "Iter-3270 train loss: 2.3054 valid loss: 2.3022, valid accuracy: 0.1070\n",
      "Iter-3280 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1068\n",
      "Iter-3290 train loss: 2.2990 valid loss: 2.3022, valid accuracy: 0.1066\n",
      "Iter-3300 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1066\n",
      "Iter-3310 train loss: 2.3017 valid loss: 2.3022, valid accuracy: 0.1068\n",
      "Iter-3320 train loss: 2.3008 valid loss: 2.3022, valid accuracy: 0.1072\n",
      "Iter-3330 train loss: 2.2996 valid loss: 2.3022, valid accuracy: 0.1072\n",
      "Iter-3340 train loss: 2.3033 valid loss: 2.3022, valid accuracy: 0.1072\n",
      "Iter-3350 train loss: 2.2961 valid loss: 2.3022, valid accuracy: 0.1072\n",
      "Iter-3360 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1070\n",
      "Iter-3370 train loss: 2.3027 valid loss: 2.3022, valid accuracy: 0.1070\n",
      "Iter-3380 train loss: 2.3060 valid loss: 2.3022, valid accuracy: 0.1070\n",
      "Iter-3390 train loss: 2.3042 valid loss: 2.3022, valid accuracy: 0.1068\n",
      "Iter-3400 train loss: 2.3008 valid loss: 2.3022, valid accuracy: 0.1066\n",
      "Iter-3410 train loss: 2.3059 valid loss: 2.3022, valid accuracy: 0.1070\n",
      "Iter-3420 train loss: 2.3028 valid loss: 2.3021, valid accuracy: 0.1068\n",
      "Iter-3430 train loss: 2.3063 valid loss: 2.3021, valid accuracy: 0.1062\n",
      "Iter-3440 train loss: 2.2987 valid loss: 2.3021, valid accuracy: 0.1056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3040 valid loss: 2.3021, valid accuracy: 0.1056\n",
      "Iter-3460 train loss: 2.3032 valid loss: 2.3021, valid accuracy: 0.1054\n",
      "Iter-3470 train loss: 2.2991 valid loss: 2.3021, valid accuracy: 0.1060\n",
      "Iter-3480 train loss: 2.3039 valid loss: 2.3021, valid accuracy: 0.1062\n",
      "Iter-3490 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1064\n",
      "Iter-3500 train loss: 2.2991 valid loss: 2.3021, valid accuracy: 0.1066\n",
      "Iter-3510 train loss: 2.3009 valid loss: 2.3021, valid accuracy: 0.1064\n",
      "Iter-3520 train loss: 2.3025 valid loss: 2.3021, valid accuracy: 0.1060\n",
      "Iter-3530 train loss: 2.3040 valid loss: 2.3021, valid accuracy: 0.1068\n",
      "Iter-3540 train loss: 2.2982 valid loss: 2.3021, valid accuracy: 0.1064\n",
      "Iter-3550 train loss: 2.3020 valid loss: 2.3021, valid accuracy: 0.1072\n",
      "Iter-3560 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1060\n",
      "Iter-3570 train loss: 2.3029 valid loss: 2.3021, valid accuracy: 0.1060\n",
      "Iter-3580 train loss: 2.2986 valid loss: 2.3021, valid accuracy: 0.1060\n",
      "Iter-3590 train loss: 2.3010 valid loss: 2.3021, valid accuracy: 0.1058\n",
      "Iter-3600 train loss: 2.2969 valid loss: 2.3021, valid accuracy: 0.1054\n",
      "Iter-3610 train loss: 2.3052 valid loss: 2.3021, valid accuracy: 0.1054\n",
      "Iter-3620 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1066\n",
      "Iter-3630 train loss: 2.3025 valid loss: 2.3021, valid accuracy: 0.1066\n",
      "Iter-3640 train loss: 2.3060 valid loss: 2.3021, valid accuracy: 0.1068\n",
      "Iter-3650 train loss: 2.3038 valid loss: 2.3020, valid accuracy: 0.1062\n",
      "Iter-3660 train loss: 2.3027 valid loss: 2.3020, valid accuracy: 0.1068\n",
      "Iter-3670 train loss: 2.2975 valid loss: 2.3020, valid accuracy: 0.1072\n",
      "Iter-3680 train loss: 2.2995 valid loss: 2.3020, valid accuracy: 0.1088\n",
      "Iter-3690 train loss: 2.2992 valid loss: 2.3020, valid accuracy: 0.1088\n",
      "Iter-3700 train loss: 2.3005 valid loss: 2.3020, valid accuracy: 0.1088\n",
      "Iter-3710 train loss: 2.3036 valid loss: 2.3020, valid accuracy: 0.1088\n",
      "Iter-3720 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.1098\n",
      "Iter-3730 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1098\n",
      "Iter-3740 train loss: 2.3020 valid loss: 2.3020, valid accuracy: 0.1096\n",
      "Iter-3750 train loss: 2.3006 valid loss: 2.3020, valid accuracy: 0.1098\n",
      "Iter-3760 train loss: 2.3047 valid loss: 2.3020, valid accuracy: 0.1096\n",
      "Iter-3770 train loss: 2.3029 valid loss: 2.3020, valid accuracy: 0.1100\n",
      "Iter-3780 train loss: 2.3040 valid loss: 2.3020, valid accuracy: 0.1098\n",
      "Iter-3790 train loss: 2.3058 valid loss: 2.3020, valid accuracy: 0.1104\n",
      "Iter-3800 train loss: 2.3048 valid loss: 2.3020, valid accuracy: 0.1110\n",
      "Iter-3810 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.1116\n",
      "Iter-3820 train loss: 2.3050 valid loss: 2.3020, valid accuracy: 0.1116\n",
      "Iter-3830 train loss: 2.3034 valid loss: 2.3020, valid accuracy: 0.1122\n",
      "Iter-3840 train loss: 2.2950 valid loss: 2.3020, valid accuracy: 0.1120\n",
      "Iter-3850 train loss: 2.3039 valid loss: 2.3020, valid accuracy: 0.1124\n",
      "Iter-3860 train loss: 2.3023 valid loss: 2.3020, valid accuracy: 0.1120\n",
      "Iter-3870 train loss: 2.2979 valid loss: 2.3020, valid accuracy: 0.1120\n",
      "Iter-3880 train loss: 2.3041 valid loss: 2.3020, valid accuracy: 0.1122\n",
      "Iter-3890 train loss: 2.2987 valid loss: 2.3020, valid accuracy: 0.1114\n",
      "Iter-3900 train loss: 2.3025 valid loss: 2.3019, valid accuracy: 0.1116\n",
      "Iter-3910 train loss: 2.3003 valid loss: 2.3019, valid accuracy: 0.1114\n",
      "Iter-3920 train loss: 2.3034 valid loss: 2.3019, valid accuracy: 0.1114\n",
      "Iter-3930 train loss: 2.3062 valid loss: 2.3019, valid accuracy: 0.1118\n",
      "Iter-3940 train loss: 2.3009 valid loss: 2.3019, valid accuracy: 0.1114\n",
      "Iter-3950 train loss: 2.3062 valid loss: 2.3019, valid accuracy: 0.1112\n",
      "Iter-3960 train loss: 2.3018 valid loss: 2.3019, valid accuracy: 0.1114\n",
      "Iter-3970 train loss: 2.3008 valid loss: 2.3019, valid accuracy: 0.1112\n",
      "Iter-3980 train loss: 2.3038 valid loss: 2.3019, valid accuracy: 0.1112\n",
      "Iter-3990 train loss: 2.3019 valid loss: 2.3019, valid accuracy: 0.1106\n",
      "Iter-4000 train loss: 2.3090 valid loss: 2.3019, valid accuracy: 0.1106\n",
      "Iter-4010 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.1112\n",
      "Iter-4020 train loss: 2.2995 valid loss: 2.3019, valid accuracy: 0.1114\n",
      "Iter-4030 train loss: 2.3050 valid loss: 2.3019, valid accuracy: 0.1118\n",
      "Iter-4040 train loss: 2.3006 valid loss: 2.3019, valid accuracy: 0.1122\n",
      "Iter-4050 train loss: 2.2992 valid loss: 2.3019, valid accuracy: 0.1120\n",
      "Iter-4060 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1118\n",
      "Iter-4070 train loss: 2.3049 valid loss: 2.3019, valid accuracy: 0.1116\n",
      "Iter-4080 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1120\n",
      "Iter-4090 train loss: 2.3028 valid loss: 2.3019, valid accuracy: 0.1124\n",
      "Iter-4100 train loss: 2.2999 valid loss: 2.3018, valid accuracy: 0.1122\n",
      "Iter-4110 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1116\n",
      "Iter-4120 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1116\n",
      "Iter-4130 train loss: 2.3083 valid loss: 2.3018, valid accuracy: 0.1118\n",
      "Iter-4140 train loss: 2.2981 valid loss: 2.3018, valid accuracy: 0.1118\n",
      "Iter-4150 train loss: 2.2986 valid loss: 2.3018, valid accuracy: 0.1116\n",
      "Iter-4160 train loss: 2.3033 valid loss: 2.3018, valid accuracy: 0.1118\n",
      "Iter-4170 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1124\n",
      "Iter-4180 train loss: 2.2969 valid loss: 2.3018, valid accuracy: 0.1122\n",
      "Iter-4190 train loss: 2.3019 valid loss: 2.3018, valid accuracy: 0.1120\n",
      "Iter-4200 train loss: 2.2982 valid loss: 2.3018, valid accuracy: 0.1134\n",
      "Iter-4210 train loss: 2.3066 valid loss: 2.3018, valid accuracy: 0.1134\n",
      "Iter-4220 train loss: 2.3029 valid loss: 2.3018, valid accuracy: 0.1120\n",
      "Iter-4230 train loss: 2.3047 valid loss: 2.3018, valid accuracy: 0.1114\n",
      "Iter-4240 train loss: 2.3040 valid loss: 2.3018, valid accuracy: 0.1112\n",
      "Iter-4250 train loss: 2.2981 valid loss: 2.3018, valid accuracy: 0.1110\n",
      "Iter-4260 train loss: 2.3036 valid loss: 2.3018, valid accuracy: 0.1114\n",
      "Iter-4270 train loss: 2.3020 valid loss: 2.3018, valid accuracy: 0.1116\n",
      "Iter-4280 train loss: 2.3070 valid loss: 2.3018, valid accuracy: 0.1114\n",
      "Iter-4290 train loss: 2.3021 valid loss: 2.3018, valid accuracy: 0.1112\n",
      "Iter-4300 train loss: 2.3030 valid loss: 2.3018, valid accuracy: 0.1114\n",
      "Iter-4310 train loss: 2.3022 valid loss: 2.3018, valid accuracy: 0.1116\n",
      "Iter-4320 train loss: 2.3080 valid loss: 2.3017, valid accuracy: 0.1120\n",
      "Iter-4330 train loss: 2.3004 valid loss: 2.3017, valid accuracy: 0.1122\n",
      "Iter-4340 train loss: 2.2936 valid loss: 2.3017, valid accuracy: 0.1118\n",
      "Iter-4350 train loss: 2.2964 valid loss: 2.3017, valid accuracy: 0.1114\n",
      "Iter-4360 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1118\n",
      "Iter-4370 train loss: 2.3011 valid loss: 2.3017, valid accuracy: 0.1112\n",
      "Iter-4380 train loss: 2.3089 valid loss: 2.3017, valid accuracy: 0.1110\n",
      "Iter-4390 train loss: 2.3036 valid loss: 2.3017, valid accuracy: 0.1112\n",
      "Iter-4400 train loss: 2.3027 valid loss: 2.3017, valid accuracy: 0.1106\n",
      "Iter-4410 train loss: 2.3053 valid loss: 2.3017, valid accuracy: 0.1106\n",
      "Iter-4420 train loss: 2.3048 valid loss: 2.3017, valid accuracy: 0.1110\n",
      "Iter-4430 train loss: 2.3027 valid loss: 2.3017, valid accuracy: 0.1110\n",
      "Iter-4440 train loss: 2.2996 valid loss: 2.3017, valid accuracy: 0.1112\n",
      "Iter-4450 train loss: 2.3072 valid loss: 2.3017, valid accuracy: 0.1114\n",
      "Iter-4460 train loss: 2.3032 valid loss: 2.3017, valid accuracy: 0.1108\n",
      "Iter-4470 train loss: 2.3043 valid loss: 2.3017, valid accuracy: 0.1116\n",
      "Iter-4480 train loss: 2.3054 valid loss: 2.3017, valid accuracy: 0.1118\n",
      "Iter-4490 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1114\n",
      "Iter-4500 train loss: 2.3022 valid loss: 2.3017, valid accuracy: 0.1128\n",
      "Iter-4510 train loss: 2.3020 valid loss: 2.3017, valid accuracy: 0.1128\n",
      "Iter-4520 train loss: 2.3072 valid loss: 2.3017, valid accuracy: 0.1140\n",
      "Iter-4530 train loss: 2.2987 valid loss: 2.3017, valid accuracy: 0.1142\n",
      "Iter-4540 train loss: 2.3058 valid loss: 2.3017, valid accuracy: 0.1140\n",
      "Iter-4550 train loss: 2.3052 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4560 train loss: 2.2985 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4570 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1132\n",
      "Iter-4580 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.2990 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4600 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4610 train loss: 2.3019 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4620 train loss: 2.3008 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4630 train loss: 2.3055 valid loss: 2.3016, valid accuracy: 0.1144\n",
      "Iter-4640 train loss: 2.2993 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4650 train loss: 2.3017 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4660 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4670 train loss: 2.2958 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4680 train loss: 2.3058 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4690 train loss: 2.3049 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4700 train loss: 2.3019 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4710 train loss: 2.2972 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4720 train loss: 2.2984 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4730 train loss: 2.2978 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4740 train loss: 2.3054 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4750 train loss: 2.3011 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4760 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4770 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4780 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4790 train loss: 2.2996 valid loss: 2.3016, valid accuracy: 0.1144\n",
      "Iter-4800 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1138\n",
      "Iter-4810 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1142\n",
      "Iter-4820 train loss: 2.3014 valid loss: 2.3016, valid accuracy: 0.1140\n",
      "Iter-4830 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1142\n",
      "Iter-4840 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1148\n",
      "Iter-4850 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1146\n",
      "Iter-4860 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1142\n",
      "Iter-4870 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1154\n",
      "Iter-4880 train loss: 2.2973 valid loss: 2.3015, valid accuracy: 0.1162\n",
      "Iter-4890 train loss: 2.3015 valid loss: 2.3015, valid accuracy: 0.1158\n",
      "Iter-4900 train loss: 2.3037 valid loss: 2.3015, valid accuracy: 0.1158\n",
      "Iter-4910 train loss: 2.2960 valid loss: 2.3015, valid accuracy: 0.1166\n",
      "Iter-4920 train loss: 2.3034 valid loss: 2.3015, valid accuracy: 0.1166\n",
      "Iter-4930 train loss: 2.2984 valid loss: 2.3015, valid accuracy: 0.1168\n",
      "Iter-4940 train loss: 2.3005 valid loss: 2.3015, valid accuracy: 0.1170\n",
      "Iter-4950 train loss: 2.2995 valid loss: 2.3015, valid accuracy: 0.1164\n",
      "Iter-4960 train loss: 2.3035 valid loss: 2.3015, valid accuracy: 0.1164\n",
      "Iter-4970 train loss: 2.2990 valid loss: 2.3015, valid accuracy: 0.1166\n",
      "Iter-4980 train loss: 2.2990 valid loss: 2.3015, valid accuracy: 0.1170\n",
      "Iter-4990 train loss: 2.2973 valid loss: 2.3015, valid accuracy: 0.1170\n",
      "Iter-5000 train loss: 2.3078 valid loss: 2.3015, valid accuracy: 0.1170\n",
      "Iter-5010 train loss: 2.3004 valid loss: 2.3015, valid accuracy: 0.1174\n",
      "Iter-5020 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1176\n",
      "Iter-5030 train loss: 2.3042 valid loss: 2.3015, valid accuracy: 0.1174\n",
      "Iter-5040 train loss: 2.3020 valid loss: 2.3015, valid accuracy: 0.1174\n",
      "Iter-5050 train loss: 2.3006 valid loss: 2.3015, valid accuracy: 0.1174\n",
      "Iter-5060 train loss: 2.3051 valid loss: 2.3015, valid accuracy: 0.1174\n",
      "Iter-5070 train loss: 2.2995 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5080 train loss: 2.3058 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5090 train loss: 2.2949 valid loss: 2.3014, valid accuracy: 0.1172\n",
      "Iter-5100 train loss: 2.3008 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5110 train loss: 2.3005 valid loss: 2.3014, valid accuracy: 0.1172\n",
      "Iter-5120 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5130 train loss: 2.3005 valid loss: 2.3014, valid accuracy: 0.1172\n",
      "Iter-5140 train loss: 2.3050 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5150 train loss: 2.3052 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5160 train loss: 2.3087 valid loss: 2.3014, valid accuracy: 0.1172\n",
      "Iter-5170 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1172\n",
      "Iter-5180 train loss: 2.2948 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5190 train loss: 2.2968 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5200 train loss: 2.2931 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5210 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1174\n",
      "Iter-5220 train loss: 2.2938 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5230 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5240 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5250 train loss: 2.3017 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5260 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5270 train loss: 2.3025 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5280 train loss: 2.3047 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5290 train loss: 2.3064 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5300 train loss: 2.3022 valid loss: 2.3014, valid accuracy: 0.1176\n",
      "Iter-5310 train loss: 2.3003 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5320 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5330 train loss: 2.3044 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5340 train loss: 2.2991 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5350 train loss: 2.3074 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5360 train loss: 2.2976 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5370 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5380 train loss: 2.3024 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5390 train loss: 2.3056 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5400 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5410 train loss: 2.2961 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5420 train loss: 2.3012 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5430 train loss: 2.3027 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5440 train loss: 2.2992 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5450 train loss: 2.2941 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5460 train loss: 2.2994 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5470 train loss: 2.3001 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5480 train loss: 2.3055 valid loss: 2.3013, valid accuracy: 0.1178\n",
      "Iter-5490 train loss: 2.3065 valid loss: 2.3013, valid accuracy: 0.1178\n",
      "Iter-5500 train loss: 2.3018 valid loss: 2.3013, valid accuracy: 0.1178\n",
      "Iter-5510 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5520 train loss: 2.2997 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5530 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5540 train loss: 2.2981 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5550 train loss: 2.2981 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5560 train loss: 2.3003 valid loss: 2.3013, valid accuracy: 0.1176\n",
      "Iter-5570 train loss: 2.3033 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5580 train loss: 2.3002 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5590 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1170\n",
      "Iter-5600 train loss: 2.3060 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5610 train loss: 2.3032 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5620 train loss: 2.3017 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5630 train loss: 2.2956 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5640 train loss: 2.2980 valid loss: 2.3012, valid accuracy: 0.1176\n",
      "Iter-5650 train loss: 2.3068 valid loss: 2.3012, valid accuracy: 0.1172\n",
      "Iter-5660 train loss: 2.3041 valid loss: 2.3012, valid accuracy: 0.1170\n",
      "Iter-5670 train loss: 2.2963 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5680 train loss: 2.3042 valid loss: 2.3012, valid accuracy: 0.1168\n",
      "Iter-5690 train loss: 2.2956 valid loss: 2.3012, valid accuracy: 0.1168\n",
      "Iter-5700 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1170\n",
      "Iter-5710 train loss: 2.3026 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5720 train loss: 2.3052 valid loss: 2.3012, valid accuracy: 0.1166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.2940 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5740 train loss: 2.2999 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5750 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5760 train loss: 2.2935 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5770 train loss: 2.3027 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5780 train loss: 2.3039 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5790 train loss: 2.3013 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5800 train loss: 2.3045 valid loss: 2.3012, valid accuracy: 0.1166\n",
      "Iter-5810 train loss: 2.3084 valid loss: 2.3011, valid accuracy: 0.1168\n",
      "Iter-5820 train loss: 2.3020 valid loss: 2.3011, valid accuracy: 0.1170\n",
      "Iter-5830 train loss: 2.3034 valid loss: 2.3011, valid accuracy: 0.1170\n",
      "Iter-5840 train loss: 2.2950 valid loss: 2.3011, valid accuracy: 0.1166\n",
      "Iter-5850 train loss: 2.3020 valid loss: 2.3011, valid accuracy: 0.1168\n",
      "Iter-5860 train loss: 2.2993 valid loss: 2.3011, valid accuracy: 0.1170\n",
      "Iter-5870 train loss: 2.3002 valid loss: 2.3011, valid accuracy: 0.1170\n",
      "Iter-5880 train loss: 2.2892 valid loss: 2.3011, valid accuracy: 0.1170\n",
      "Iter-5890 train loss: 2.3004 valid loss: 2.3011, valid accuracy: 0.1168\n",
      "Iter-5900 train loss: 2.2960 valid loss: 2.3011, valid accuracy: 0.1170\n",
      "Iter-5910 train loss: 2.3009 valid loss: 2.3011, valid accuracy: 0.1168\n",
      "Iter-5920 train loss: 2.3077 valid loss: 2.3011, valid accuracy: 0.1166\n",
      "Iter-5930 train loss: 2.3028 valid loss: 2.3011, valid accuracy: 0.1168\n",
      "Iter-5940 train loss: 2.3010 valid loss: 2.3011, valid accuracy: 0.1168\n",
      "Iter-5950 train loss: 2.3014 valid loss: 2.3011, valid accuracy: 0.1166\n",
      "Iter-5960 train loss: 2.3036 valid loss: 2.3011, valid accuracy: 0.1166\n",
      "Iter-5970 train loss: 2.3006 valid loss: 2.3011, valid accuracy: 0.1166\n",
      "Iter-5980 train loss: 2.3037 valid loss: 2.3011, valid accuracy: 0.1166\n",
      "Iter-5990 train loss: 2.3018 valid loss: 2.3011, valid accuracy: 0.1164\n",
      "Iter-6000 train loss: 2.2991 valid loss: 2.3011, valid accuracy: 0.1162\n",
      "Iter-6010 train loss: 2.3037 valid loss: 2.3011, valid accuracy: 0.1160\n",
      "Iter-6020 train loss: 2.3009 valid loss: 2.3011, valid accuracy: 0.1162\n",
      "Iter-6030 train loss: 2.3057 valid loss: 2.3011, valid accuracy: 0.1162\n",
      "Iter-6040 train loss: 2.3033 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6050 train loss: 2.2921 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6060 train loss: 2.3005 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6070 train loss: 2.3105 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6080 train loss: 2.2992 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6090 train loss: 2.3008 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6100 train loss: 2.3028 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6110 train loss: 2.2997 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6120 train loss: 2.3007 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6130 train loss: 2.3032 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6140 train loss: 2.2984 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6150 train loss: 2.2997 valid loss: 2.3010, valid accuracy: 0.1162\n",
      "Iter-6160 train loss: 2.3017 valid loss: 2.3010, valid accuracy: 0.1162\n",
      "Iter-6170 train loss: 2.3037 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6180 train loss: 2.2964 valid loss: 2.3010, valid accuracy: 0.1162\n",
      "Iter-6190 train loss: 2.3023 valid loss: 2.3010, valid accuracy: 0.1164\n",
      "Iter-6200 train loss: 2.3011 valid loss: 2.3010, valid accuracy: 0.1164\n",
      "Iter-6210 train loss: 2.3027 valid loss: 2.3010, valid accuracy: 0.1164\n",
      "Iter-6220 train loss: 2.3015 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6230 train loss: 2.2967 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6240 train loss: 2.2944 valid loss: 2.3010, valid accuracy: 0.1160\n",
      "Iter-6250 train loss: 2.3048 valid loss: 2.3010, valid accuracy: 0.1156\n",
      "Iter-6260 train loss: 2.3010 valid loss: 2.3010, valid accuracy: 0.1156\n",
      "Iter-6270 train loss: 2.2966 valid loss: 2.3010, valid accuracy: 0.1156\n",
      "Iter-6280 train loss: 2.3057 valid loss: 2.3010, valid accuracy: 0.1156\n",
      "Iter-6290 train loss: 2.3015 valid loss: 2.3010, valid accuracy: 0.1156\n",
      "Iter-6300 train loss: 2.2919 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6310 train loss: 2.2998 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6320 train loss: 2.2978 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6330 train loss: 2.2982 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6340 train loss: 2.3013 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6350 train loss: 2.2985 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6360 train loss: 2.3076 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6370 train loss: 2.2966 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6380 train loss: 2.2989 valid loss: 2.3009, valid accuracy: 0.1154\n",
      "Iter-6390 train loss: 2.2983 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6400 train loss: 2.3001 valid loss: 2.3009, valid accuracy: 0.1154\n",
      "Iter-6410 train loss: 2.3022 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6420 train loss: 2.3019 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6430 train loss: 2.3003 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6440 train loss: 2.3067 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6450 train loss: 2.2961 valid loss: 2.3009, valid accuracy: 0.1154\n",
      "Iter-6460 train loss: 2.3030 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6470 train loss: 2.2921 valid loss: 2.3009, valid accuracy: 0.1156\n",
      "Iter-6480 train loss: 2.3066 valid loss: 2.3009, valid accuracy: 0.1154\n",
      "Iter-6490 train loss: 2.3015 valid loss: 2.3009, valid accuracy: 0.1154\n",
      "Iter-6500 train loss: 2.2969 valid loss: 2.3009, valid accuracy: 0.1152\n",
      "Iter-6510 train loss: 2.2994 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6520 train loss: 2.3072 valid loss: 2.3009, valid accuracy: 0.1154\n",
      "Iter-6530 train loss: 2.2946 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6540 train loss: 2.3043 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6550 train loss: 2.3018 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6560 train loss: 2.2993 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6570 train loss: 2.3030 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6580 train loss: 2.2954 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6590 train loss: 2.3005 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6600 train loss: 2.3064 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6610 train loss: 2.3017 valid loss: 2.3009, valid accuracy: 0.1150\n",
      "Iter-6620 train loss: 2.2976 valid loss: 2.3008, valid accuracy: 0.1150\n",
      "Iter-6630 train loss: 2.2943 valid loss: 2.3008, valid accuracy: 0.1150\n",
      "Iter-6640 train loss: 2.2999 valid loss: 2.3008, valid accuracy: 0.1150\n",
      "Iter-6650 train loss: 2.3017 valid loss: 2.3008, valid accuracy: 0.1150\n",
      "Iter-6660 train loss: 2.3007 valid loss: 2.3008, valid accuracy: 0.1146\n",
      "Iter-6670 train loss: 2.2943 valid loss: 2.3008, valid accuracy: 0.1146\n",
      "Iter-6680 train loss: 2.2995 valid loss: 2.3008, valid accuracy: 0.1146\n",
      "Iter-6690 train loss: 2.3075 valid loss: 2.3008, valid accuracy: 0.1146\n",
      "Iter-6700 train loss: 2.3063 valid loss: 2.3008, valid accuracy: 0.1146\n",
      "Iter-6710 train loss: 2.3018 valid loss: 2.3008, valid accuracy: 0.1144\n",
      "Iter-6720 train loss: 2.3006 valid loss: 2.3008, valid accuracy: 0.1144\n",
      "Iter-6730 train loss: 2.3079 valid loss: 2.3008, valid accuracy: 0.1144\n",
      "Iter-6740 train loss: 2.3003 valid loss: 2.3008, valid accuracy: 0.1144\n",
      "Iter-6750 train loss: 2.3020 valid loss: 2.3008, valid accuracy: 0.1144\n",
      "Iter-6760 train loss: 2.2984 valid loss: 2.3008, valid accuracy: 0.1144\n",
      "Iter-6770 train loss: 2.2964 valid loss: 2.3008, valid accuracy: 0.1142\n",
      "Iter-6780 train loss: 2.2954 valid loss: 2.3008, valid accuracy: 0.1142\n",
      "Iter-6790 train loss: 2.2886 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6800 train loss: 2.2991 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6810 train loss: 2.3057 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6820 train loss: 2.3057 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6830 train loss: 2.2992 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6840 train loss: 2.3062 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6850 train loss: 2.2999 valid loss: 2.3008, valid accuracy: 0.1140\n",
      "Iter-6860 train loss: 2.3038 valid loss: 2.3008, valid accuracy: 0.1140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.3030 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6880 train loss: 2.3051 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6890 train loss: 2.2987 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6900 train loss: 2.3077 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6910 train loss: 2.3021 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6920 train loss: 2.3009 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6930 train loss: 2.2994 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-6940 train loss: 2.3057 valid loss: 2.3007, valid accuracy: 0.1142\n",
      "Iter-6950 train loss: 2.3028 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-6960 train loss: 2.3059 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-6970 train loss: 2.2962 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-6980 train loss: 2.3023 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-6990 train loss: 2.2985 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-7000 train loss: 2.2957 valid loss: 2.3007, valid accuracy: 0.1142\n",
      "Iter-7010 train loss: 2.2984 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-7020 train loss: 2.3006 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-7030 train loss: 2.3080 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-7040 train loss: 2.2932 valid loss: 2.3007, valid accuracy: 0.1146\n",
      "Iter-7050 train loss: 2.3036 valid loss: 2.3007, valid accuracy: 0.1146\n",
      "Iter-7060 train loss: 2.2954 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-7070 train loss: 2.2994 valid loss: 2.3007, valid accuracy: 0.1144\n",
      "Iter-7080 train loss: 2.2934 valid loss: 2.3007, valid accuracy: 0.1142\n",
      "Iter-7090 train loss: 2.2986 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7100 train loss: 2.3051 valid loss: 2.3007, valid accuracy: 0.1142\n",
      "Iter-7110 train loss: 2.3040 valid loss: 2.3007, valid accuracy: 0.1142\n",
      "Iter-7120 train loss: 2.3030 valid loss: 2.3007, valid accuracy: 0.1142\n",
      "Iter-7130 train loss: 2.2988 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7140 train loss: 2.2991 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7150 train loss: 2.3032 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7160 train loss: 2.2977 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7170 train loss: 2.3050 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7180 train loss: 2.3062 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7190 train loss: 2.3017 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7200 train loss: 2.2992 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7210 train loss: 2.3023 valid loss: 2.3007, valid accuracy: 0.1140\n",
      "Iter-7220 train loss: 2.2997 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7230 train loss: 2.2988 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7240 train loss: 2.3009 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7250 train loss: 2.3104 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7260 train loss: 2.2999 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7270 train loss: 2.2974 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7280 train loss: 2.2936 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7290 train loss: 2.3048 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7300 train loss: 2.2993 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7310 train loss: 2.3011 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7320 train loss: 2.3126 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7330 train loss: 2.3051 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7340 train loss: 2.3016 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7350 train loss: 2.2952 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7360 train loss: 2.3065 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7370 train loss: 2.2997 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7380 train loss: 2.3017 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7390 train loss: 2.2920 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7400 train loss: 2.2981 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7410 train loss: 2.2953 valid loss: 2.3006, valid accuracy: 0.1140\n",
      "Iter-7420 train loss: 2.2999 valid loss: 2.3006, valid accuracy: 0.1138\n",
      "Iter-7430 train loss: 2.2946 valid loss: 2.3006, valid accuracy: 0.1138\n",
      "Iter-7440 train loss: 2.3018 valid loss: 2.3006, valid accuracy: 0.1138\n",
      "Iter-7450 train loss: 2.2996 valid loss: 2.3006, valid accuracy: 0.1138\n",
      "Iter-7460 train loss: 2.3066 valid loss: 2.3006, valid accuracy: 0.1138\n",
      "Iter-7470 train loss: 2.3063 valid loss: 2.3006, valid accuracy: 0.1138\n",
      "Iter-7480 train loss: 2.2935 valid loss: 2.3005, valid accuracy: 0.1138\n",
      "Iter-7490 train loss: 2.2996 valid loss: 2.3005, valid accuracy: 0.1140\n",
      "Iter-7500 train loss: 2.2956 valid loss: 2.3005, valid accuracy: 0.1136\n",
      "Iter-7510 train loss: 2.3001 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7520 train loss: 2.2990 valid loss: 2.3005, valid accuracy: 0.1136\n",
      "Iter-7530 train loss: 2.3021 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7540 train loss: 2.2968 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7550 train loss: 2.2983 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7560 train loss: 2.2992 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7570 train loss: 2.2990 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7580 train loss: 2.2989 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7590 train loss: 2.3066 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7600 train loss: 2.2935 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7610 train loss: 2.2961 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7620 train loss: 2.2991 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7630 train loss: 2.3100 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7640 train loss: 2.3002 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7650 train loss: 2.2934 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7660 train loss: 2.3020 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7670 train loss: 2.3021 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7680 train loss: 2.3019 valid loss: 2.3005, valid accuracy: 0.1134\n",
      "Iter-7690 train loss: 2.3000 valid loss: 2.3005, valid accuracy: 0.1132\n",
      "Iter-7700 train loss: 2.2990 valid loss: 2.3005, valid accuracy: 0.1132\n",
      "Iter-7710 train loss: 2.2923 valid loss: 2.3005, valid accuracy: 0.1130\n",
      "Iter-7720 train loss: 2.3082 valid loss: 2.3005, valid accuracy: 0.1130\n",
      "Iter-7730 train loss: 2.2990 valid loss: 2.3005, valid accuracy: 0.1130\n",
      "Iter-7740 train loss: 2.3039 valid loss: 2.3005, valid accuracy: 0.1130\n",
      "Iter-7750 train loss: 2.2960 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7760 train loss: 2.3038 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7770 train loss: 2.3028 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7780 train loss: 2.3063 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7790 train loss: 2.2945 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7800 train loss: 2.3142 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7810 train loss: 2.3008 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7820 train loss: 2.3000 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7830 train loss: 2.3014 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7840 train loss: 2.2985 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7850 train loss: 2.2968 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7860 train loss: 2.2996 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7870 train loss: 2.3028 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7880 train loss: 2.2996 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7890 train loss: 2.3052 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7900 train loss: 2.3046 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7910 train loss: 2.2969 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7920 train loss: 2.3084 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7930 train loss: 2.2896 valid loss: 2.3004, valid accuracy: 0.1130\n",
      "Iter-7940 train loss: 2.3050 valid loss: 2.3004, valid accuracy: 0.1128\n",
      "Iter-7950 train loss: 2.2914 valid loss: 2.3004, valid accuracy: 0.1128\n",
      "Iter-7960 train loss: 2.2964 valid loss: 2.3004, valid accuracy: 0.1128\n",
      "Iter-7970 train loss: 2.2986 valid loss: 2.3004, valid accuracy: 0.1128\n",
      "Iter-7980 train loss: 2.3049 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-7990 train loss: 2.2979 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8000 train loss: 2.3053 valid loss: 2.3003, valid accuracy: 0.1130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.2984 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8020 train loss: 2.2947 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8030 train loss: 2.3030 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8040 train loss: 2.2996 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8050 train loss: 2.2958 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8060 train loss: 2.2974 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8070 train loss: 2.3110 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8080 train loss: 2.3033 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8090 train loss: 2.3025 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8100 train loss: 2.2984 valid loss: 2.3003, valid accuracy: 0.1130\n",
      "Iter-8110 train loss: 2.3030 valid loss: 2.3003, valid accuracy: 0.1130\n",
      "Iter-8120 train loss: 2.3030 valid loss: 2.3003, valid accuracy: 0.1130\n",
      "Iter-8130 train loss: 2.2937 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8140 train loss: 2.2951 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8150 train loss: 2.2990 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8160 train loss: 2.3032 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8170 train loss: 2.2989 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8180 train loss: 2.2966 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8190 train loss: 2.2949 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8200 train loss: 2.3034 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8210 train loss: 2.2988 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8220 train loss: 2.3055 valid loss: 2.3003, valid accuracy: 0.1128\n",
      "Iter-8230 train loss: 2.3044 valid loss: 2.3003, valid accuracy: 0.1126\n",
      "Iter-8240 train loss: 2.2989 valid loss: 2.3003, valid accuracy: 0.1126\n",
      "Iter-8250 train loss: 2.3026 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8260 train loss: 2.3013 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8270 train loss: 2.2940 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8280 train loss: 2.2976 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8290 train loss: 2.2867 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.3020 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8310 train loss: 2.3015 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8320 train loss: 2.2979 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8330 train loss: 2.3018 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8340 train loss: 2.2965 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8350 train loss: 2.3067 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8360 train loss: 2.3002 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8370 train loss: 2.2965 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8380 train loss: 2.3051 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8390 train loss: 2.2941 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.2948 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8410 train loss: 2.2982 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8420 train loss: 2.2999 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8430 train loss: 2.2953 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8440 train loss: 2.3002 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8450 train loss: 2.3090 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8460 train loss: 2.3036 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8470 train loss: 2.2943 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8480 train loss: 2.2992 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8490 train loss: 2.3117 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.3036 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8510 train loss: 2.2941 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8520 train loss: 2.3039 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8530 train loss: 2.2984 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8540 train loss: 2.3053 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8550 train loss: 2.3018 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8560 train loss: 2.3017 valid loss: 2.3002, valid accuracy: 0.1126\n",
      "Iter-8570 train loss: 2.2959 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8580 train loss: 2.2982 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8590 train loss: 2.2954 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.3003 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8610 train loss: 2.2947 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8620 train loss: 2.2983 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8630 train loss: 2.2965 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8640 train loss: 2.2968 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8650 train loss: 2.2955 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8660 train loss: 2.2868 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8670 train loss: 2.2902 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8680 train loss: 2.2943 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8690 train loss: 2.2872 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.2919 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8710 train loss: 2.3038 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8720 train loss: 2.3053 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8730 train loss: 2.2980 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8740 train loss: 2.3034 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8750 train loss: 2.3058 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8760 train loss: 2.3001 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8770 train loss: 2.3029 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8780 train loss: 2.3041 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8790 train loss: 2.2985 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.3005 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8810 train loss: 2.2945 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8820 train loss: 2.3032 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8830 train loss: 2.3025 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8840 train loss: 2.3099 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8850 train loss: 2.3069 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8860 train loss: 2.2971 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8870 train loss: 2.2963 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8880 train loss: 2.2974 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8890 train loss: 2.3054 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.3011 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8910 train loss: 2.2986 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8920 train loss: 2.3022 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8930 train loss: 2.2970 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-8940 train loss: 2.3076 valid loss: 2.3001, valid accuracy: 0.1126\n",
      "Iter-8950 train loss: 2.3044 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-8960 train loss: 2.3071 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-8970 train loss: 2.2988 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-8980 train loss: 2.2878 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-8990 train loss: 2.2913 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.3026 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9010 train loss: 2.3047 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9020 train loss: 2.2996 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9030 train loss: 2.2936 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9040 train loss: 2.3029 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9050 train loss: 2.2977 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9060 train loss: 2.3024 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9070 train loss: 2.3067 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9080 train loss: 2.2848 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9090 train loss: 2.2948 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.2983 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9110 train loss: 2.2977 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9120 train loss: 2.3044 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9130 train loss: 2.2975 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9140 train loss: 2.3018 valid loss: 2.3000, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.3019 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9160 train loss: 2.3041 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9170 train loss: 2.3036 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9180 train loss: 2.2974 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9190 train loss: 2.3056 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.3027 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9210 train loss: 2.3024 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9220 train loss: 2.2995 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9230 train loss: 2.3063 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9240 train loss: 2.3079 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9250 train loss: 2.2985 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9260 train loss: 2.3065 valid loss: 2.3000, valid accuracy: 0.1126\n",
      "Iter-9270 train loss: 2.2959 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9280 train loss: 2.2999 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9290 train loss: 2.3021 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.3044 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9310 train loss: 2.3012 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9320 train loss: 2.2895 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9330 train loss: 2.3033 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9340 train loss: 2.2955 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9350 train loss: 2.3040 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9360 train loss: 2.2944 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9370 train loss: 2.2984 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9380 train loss: 2.3017 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9390 train loss: 2.2940 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2971 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9410 train loss: 2.3029 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9420 train loss: 2.2986 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9430 train loss: 2.3045 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9440 train loss: 2.3076 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9450 train loss: 2.2940 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9460 train loss: 2.3060 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9470 train loss: 2.3041 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9480 train loss: 2.3014 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9490 train loss: 2.3009 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.3002 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9510 train loss: 2.2899 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9520 train loss: 2.2987 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9530 train loss: 2.2949 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9540 train loss: 2.2978 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9550 train loss: 2.2959 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9560 train loss: 2.3011 valid loss: 2.2999, valid accuracy: 0.1126\n",
      "Iter-9570 train loss: 2.3009 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9580 train loss: 2.2942 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9590 train loss: 2.3070 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.2931 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9610 train loss: 2.3015 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9620 train loss: 2.2975 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9630 train loss: 2.3100 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9640 train loss: 2.3030 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9650 train loss: 2.3013 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9660 train loss: 2.2999 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9670 train loss: 2.3048 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9680 train loss: 2.2989 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9690 train loss: 2.2950 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.2986 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9710 train loss: 2.3028 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9720 train loss: 2.3045 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9730 train loss: 2.2986 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9740 train loss: 2.2985 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9750 train loss: 2.3037 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9760 train loss: 2.3084 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9770 train loss: 2.2936 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9780 train loss: 2.3000 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9790 train loss: 2.3059 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.2995 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9810 train loss: 2.2974 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9820 train loss: 2.2935 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9830 train loss: 2.2979 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9840 train loss: 2.3071 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9850 train loss: 2.3060 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9860 train loss: 2.3062 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9870 train loss: 2.3054 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9880 train loss: 2.2989 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-9890 train loss: 2.2994 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3009 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9910 train loss: 2.3047 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9920 train loss: 2.3046 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9930 train loss: 2.3015 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9940 train loss: 2.3099 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9950 train loss: 2.2950 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9960 train loss: 2.3002 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9970 train loss: 2.2960 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9980 train loss: 2.3010 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-9990 train loss: 2.3054 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.3013 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1181f3f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1223d3f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
