{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "#         y, _ = l.relu_forward(X=y)\n",
    "#         y, _ = l.lrelu_forward(X=y)\n",
    "#         y = l.elu_fwd(X=y)\n",
    "#         y, _ = l.softplus_forward(X=y)\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "        y = np.exp(y)/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "#             y, _ = l.relu_forward(X=y)\n",
    "#             y, _ = l.lrelu_forward(X=y)\n",
    "#             y = l.elu_fwd(X=y)\n",
    "#             y, _ = l.softplus_forward(X=y)\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "            y = np.exp(y)/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3063 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-20 train loss: 2.3085 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-30 train loss: 2.3050 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-40 train loss: 2.3030 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-50 train loss: 2.3061 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-60 train loss: 2.2987 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-70 train loss: 2.3051 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-80 train loss: 2.3036 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-90 train loss: 2.3063 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-100 train loss: 2.3035 valid loss: 2.3032, valid accuracy: 0.0976\n",
      "Iter-110 train loss: 2.3078 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-120 train loss: 2.3014 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-130 train loss: 2.3033 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-140 train loss: 2.3065 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-150 train loss: 2.3021 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-160 train loss: 2.3007 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-170 train loss: 2.3017 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-180 train loss: 2.3057 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-190 train loss: 2.3040 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-200 train loss: 2.3066 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-210 train loss: 2.3046 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-220 train loss: 2.3010 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-230 train loss: 2.2994 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-240 train loss: 2.3095 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-250 train loss: 2.3058 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-260 train loss: 2.3047 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-270 train loss: 2.3012 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-280 train loss: 2.3027 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-290 train loss: 2.3033 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-300 train loss: 2.3019 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-310 train loss: 2.3048 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-320 train loss: 2.3069 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-330 train loss: 2.2999 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-340 train loss: 2.3071 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-350 train loss: 2.3051 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-360 train loss: 2.3004 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-370 train loss: 2.3020 valid loss: 2.3031, valid accuracy: 0.0976\n",
      "Iter-380 train loss: 2.3099 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-390 train loss: 2.3024 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-400 train loss: 2.3105 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-410 train loss: 2.3041 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-420 train loss: 2.3022 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-430 train loss: 2.3033 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-440 train loss: 2.3031 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-450 train loss: 2.3016 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-460 train loss: 2.3021 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-470 train loss: 2.3021 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-480 train loss: 2.3001 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-490 train loss: 2.2996 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-500 train loss: 2.3041 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-510 train loss: 2.3034 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-520 train loss: 2.3020 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-530 train loss: 2.3016 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-540 train loss: 2.3010 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-550 train loss: 2.3046 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-560 train loss: 2.3010 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-570 train loss: 2.3060 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-580 train loss: 2.3040 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-590 train loss: 2.3052 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-600 train loss: 2.3019 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-610 train loss: 2.3010 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-620 train loss: 2.2994 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-630 train loss: 2.2996 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-640 train loss: 2.3047 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-650 train loss: 2.3022 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-660 train loss: 2.3028 valid loss: 2.3030, valid accuracy: 0.0976\n",
      "Iter-670 train loss: 2.3037 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-680 train loss: 2.3029 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-690 train loss: 2.3031 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-700 train loss: 2.3060 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-710 train loss: 2.3057 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-720 train loss: 2.3002 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-730 train loss: 2.3049 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-740 train loss: 2.3029 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-750 train loss: 2.3014 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-760 train loss: 2.3058 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-770 train loss: 2.2995 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-780 train loss: 2.3045 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-790 train loss: 2.3048 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-800 train loss: 2.3024 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-810 train loss: 2.3027 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-820 train loss: 2.3006 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-830 train loss: 2.3035 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-840 train loss: 2.3040 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-850 train loss: 2.3057 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-860 train loss: 2.2999 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-870 train loss: 2.3033 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-880 train loss: 2.3035 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-890 train loss: 2.3050 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-900 train loss: 2.3027 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-910 train loss: 2.3059 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-920 train loss: 2.3037 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-930 train loss: 2.3030 valid loss: 2.3029, valid accuracy: 0.0976\n",
      "Iter-940 train loss: 2.3018 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-950 train loss: 2.3019 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-960 train loss: 2.2944 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-970 train loss: 2.3023 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-980 train loss: 2.2989 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-990 train loss: 2.3031 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1000 train loss: 2.3022 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1010 train loss: 2.3074 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1020 train loss: 2.3020 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1030 train loss: 2.3045 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1040 train loss: 2.3010 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1050 train loss: 2.3050 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1060 train loss: 2.3046 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1070 train loss: 2.3014 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1080 train loss: 2.2993 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1090 train loss: 2.3023 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1100 train loss: 2.3024 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1110 train loss: 2.3035 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1120 train loss: 2.3011 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1130 train loss: 2.3057 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1140 train loss: 2.3014 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1150 train loss: 2.2999 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1160 train loss: 2.3063 valid loss: 2.3028, valid accuracy: 0.0976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3033 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1180 train loss: 2.3015 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1190 train loss: 2.3026 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1200 train loss: 2.3019 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1210 train loss: 2.3012 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1220 train loss: 2.3040 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1230 train loss: 2.2991 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1240 train loss: 2.3047 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1250 train loss: 2.3045 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1260 train loss: 2.3020 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1270 train loss: 2.3023 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1280 train loss: 2.3014 valid loss: 2.3028, valid accuracy: 0.0976\n",
      "Iter-1290 train loss: 2.3024 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1300 train loss: 2.3038 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1310 train loss: 2.3032 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1320 train loss: 2.2984 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1330 train loss: 2.3011 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1340 train loss: 2.3006 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1350 train loss: 2.3033 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1360 train loss: 2.3022 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1370 train loss: 2.3016 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1380 train loss: 2.2999 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1390 train loss: 2.2986 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1400 train loss: 2.3017 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1410 train loss: 2.2995 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1420 train loss: 2.3006 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1430 train loss: 2.3017 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1440 train loss: 2.3044 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1450 train loss: 2.3042 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1460 train loss: 2.3004 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1470 train loss: 2.3033 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1480 train loss: 2.3055 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1490 train loss: 2.2979 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1500 train loss: 2.2981 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1510 train loss: 2.3034 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1520 train loss: 2.3016 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1530 train loss: 2.3038 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1540 train loss: 2.3021 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1550 train loss: 2.3045 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1560 train loss: 2.3026 valid loss: 2.3027, valid accuracy: 0.0976\n",
      "Iter-1570 train loss: 2.3070 valid loss: 2.3026, valid accuracy: 0.0976\n",
      "Iter-1580 train loss: 2.3009 valid loss: 2.3026, valid accuracy: 0.0976\n",
      "Iter-1590 train loss: 2.3052 valid loss: 2.3026, valid accuracy: 0.0976\n",
      "Iter-1600 train loss: 2.3053 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1610 train loss: 2.2989 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1620 train loss: 2.2982 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1630 train loss: 2.3016 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1640 train loss: 2.3009 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1650 train loss: 2.2997 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1660 train loss: 2.3049 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1670 train loss: 2.3010 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1680 train loss: 2.3037 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1690 train loss: 2.3029 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1710 train loss: 2.3044 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1720 train loss: 2.3056 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1730 train loss: 2.3017 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1740 train loss: 2.3032 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1750 train loss: 2.3052 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1760 train loss: 2.3031 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1770 train loss: 2.2995 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1780 train loss: 2.3000 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1790 train loss: 2.2996 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3057 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1810 train loss: 2.2991 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1820 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1830 train loss: 2.3028 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1840 train loss: 2.3007 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1850 train loss: 2.3030 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1860 train loss: 2.2988 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1870 train loss: 2.3000 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-1880 train loss: 2.3106 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1890 train loss: 2.3059 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1910 train loss: 2.3001 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1920 train loss: 2.2998 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1930 train loss: 2.3029 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1940 train loss: 2.3042 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1950 train loss: 2.3050 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1960 train loss: 2.3018 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1970 train loss: 2.3030 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1980 train loss: 2.3008 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-1990 train loss: 2.3065 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3038 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2010 train loss: 2.3030 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2020 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2030 train loss: 2.2988 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2040 train loss: 2.3032 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2050 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2060 train loss: 2.2986 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2070 train loss: 2.3037 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2080 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2090 train loss: 2.3035 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3006 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2110 train loss: 2.3061 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2120 train loss: 2.3020 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2130 train loss: 2.3014 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2140 train loss: 2.2986 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2150 train loss: 2.3062 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2160 train loss: 2.3028 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2170 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2180 train loss: 2.3061 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2190 train loss: 2.3040 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3009 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2210 train loss: 2.2979 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2220 train loss: 2.3015 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2230 train loss: 2.3045 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2240 train loss: 2.3013 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2250 train loss: 2.3058 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2260 train loss: 2.3010 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2270 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2280 train loss: 2.3017 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2290 train loss: 2.3013 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3005 valid loss: 2.3024, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3004 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2320 train loss: 2.3013 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2330 train loss: 2.3033 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2340 train loss: 2.3039 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2350 train loss: 2.3080 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2360 train loss: 2.3061 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2370 train loss: 2.3036 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2380 train loss: 2.3058 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2390 train loss: 2.3046 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.3015 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2410 train loss: 2.3040 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2420 train loss: 2.2988 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2430 train loss: 2.3042 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2440 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2450 train loss: 2.3007 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2460 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2470 train loss: 2.3044 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2480 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2490 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2510 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2520 train loss: 2.2995 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2530 train loss: 2.3033 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2540 train loss: 2.3017 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2550 train loss: 2.3070 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2560 train loss: 2.3052 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2570 train loss: 2.2992 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2580 train loss: 2.2999 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2590 train loss: 2.3063 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.3010 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2610 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2620 train loss: 2.2978 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2630 train loss: 2.3043 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2640 train loss: 2.3000 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2650 train loss: 2.3016 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2660 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-2670 train loss: 2.3033 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2680 train loss: 2.3005 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2690 train loss: 2.3010 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.3084 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2710 train loss: 2.2997 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2720 train loss: 2.2984 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2730 train loss: 2.3033 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2740 train loss: 2.2970 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2750 train loss: 2.3013 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2760 train loss: 2.3010 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2770 train loss: 2.2992 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2780 train loss: 2.3056 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2790 train loss: 2.2997 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.2980 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2810 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2820 train loss: 2.3054 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2830 train loss: 2.3005 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2840 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2850 train loss: 2.3000 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2860 train loss: 2.3030 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2870 train loss: 2.2988 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2880 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2890 train loss: 2.3006 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.3017 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2910 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2920 train loss: 2.3041 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2930 train loss: 2.3017 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2940 train loss: 2.3035 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2950 train loss: 2.2997 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2960 train loss: 2.3058 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2970 train loss: 2.3001 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2980 train loss: 2.3039 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2990 train loss: 2.3039 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.3028 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3010 train loss: 2.3040 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3020 train loss: 2.3011 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3030 train loss: 2.3038 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3040 train loss: 2.3042 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3050 train loss: 2.3017 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3060 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3070 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3080 train loss: 2.3011 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3090 train loss: 2.3004 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.2994 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3110 train loss: 2.3083 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3120 train loss: 2.2924 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3130 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3140 train loss: 2.2995 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3150 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3160 train loss: 2.2995 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3170 train loss: 2.2958 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3180 train loss: 2.3033 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3190 train loss: 2.3042 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3011 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3210 train loss: 2.3017 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3220 train loss: 2.3012 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3230 train loss: 2.3008 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3240 train loss: 2.3016 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3250 train loss: 2.2954 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3260 train loss: 2.3004 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3270 train loss: 2.3002 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3280 train loss: 2.3058 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3290 train loss: 2.2984 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3310 train loss: 2.3048 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3320 train loss: 2.3056 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3330 train loss: 2.3074 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3340 train loss: 2.3053 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3350 train loss: 2.2995 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3360 train loss: 2.2987 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3370 train loss: 2.2968 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3380 train loss: 2.3004 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3390 train loss: 2.3020 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.3033 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3410 train loss: 2.2979 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3420 train loss: 2.2993 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3430 train loss: 2.2946 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3440 train loss: 2.3026 valid loss: 2.3021, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3037 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3460 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3470 train loss: 2.3027 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3480 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3490 train loss: 2.3024 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.2975 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3510 train loss: 2.2973 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3520 train loss: 2.3068 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3530 train loss: 2.3047 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3540 train loss: 2.2978 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3550 train loss: 2.2987 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3560 train loss: 2.3039 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3570 train loss: 2.2941 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3580 train loss: 2.2990 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3590 train loss: 2.3039 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.2984 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3610 train loss: 2.3001 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3620 train loss: 2.3050 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3630 train loss: 2.3028 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3640 train loss: 2.2998 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3650 train loss: 2.3059 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3660 train loss: 2.2994 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3670 train loss: 2.3043 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3680 train loss: 2.3037 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3690 train loss: 2.3045 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3008 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3710 train loss: 2.3001 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3720 train loss: 2.3045 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3730 train loss: 2.2992 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3740 train loss: 2.3074 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3750 train loss: 2.2997 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3760 train loss: 2.3017 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3770 train loss: 2.3009 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3780 train loss: 2.2997 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3790 train loss: 2.2971 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3042 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3810 train loss: 2.3020 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3820 train loss: 2.2977 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3830 train loss: 2.3020 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3840 train loss: 2.2985 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3850 train loss: 2.3038 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3860 train loss: 2.3003 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3870 train loss: 2.3056 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3880 train loss: 2.3061 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3890 train loss: 2.2997 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3033 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3910 train loss: 2.3006 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3920 train loss: 2.3011 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3930 train loss: 2.2992 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3940 train loss: 2.3026 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3950 train loss: 2.2991 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3960 train loss: 2.2999 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3970 train loss: 2.3037 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3980 train loss: 2.3008 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3990 train loss: 2.3005 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3068 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4010 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4020 train loss: 2.2969 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4030 train loss: 2.2970 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4040 train loss: 2.2935 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4050 train loss: 2.3092 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4060 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4070 train loss: 2.3039 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4080 train loss: 2.3052 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4090 train loss: 2.3039 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4110 train loss: 2.3031 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4120 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4130 train loss: 2.3077 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4140 train loss: 2.2968 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4150 train loss: 2.3006 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4160 train loss: 2.3022 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4170 train loss: 2.3056 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4180 train loss: 2.3027 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4190 train loss: 2.3030 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3008 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4210 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4220 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4230 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4240 train loss: 2.2982 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4250 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4260 train loss: 2.3069 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4270 train loss: 2.3039 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4280 train loss: 2.2999 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-4290 train loss: 2.2983 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.2996 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4310 train loss: 2.2995 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4320 train loss: 2.2953 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4330 train loss: 2.3030 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4340 train loss: 2.2993 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4350 train loss: 2.3058 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4360 train loss: 2.3054 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4370 train loss: 2.2974 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4380 train loss: 2.3008 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4390 train loss: 2.3002 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.3029 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4410 train loss: 2.2986 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4420 train loss: 2.2998 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4430 train loss: 2.3039 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4440 train loss: 2.3033 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4450 train loss: 2.3047 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4460 train loss: 2.3002 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4470 train loss: 2.3013 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4480 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4490 train loss: 2.3039 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.2986 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4510 train loss: 2.3023 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4520 train loss: 2.3006 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4530 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4540 train loss: 2.3010 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4550 train loss: 2.2990 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4560 train loss: 2.2955 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4570 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4580 train loss: 2.2991 valid loss: 2.3019, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3041 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.3050 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4610 train loss: 2.2999 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4620 train loss: 2.2969 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4630 train loss: 2.3048 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4640 train loss: 2.3058 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4650 train loss: 2.3041 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4660 train loss: 2.3068 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4670 train loss: 2.3042 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4680 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4690 train loss: 2.3068 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.2945 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4710 train loss: 2.3025 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4720 train loss: 2.3006 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4730 train loss: 2.3017 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4740 train loss: 2.2941 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4750 train loss: 2.3099 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4760 train loss: 2.3060 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4770 train loss: 2.3025 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4780 train loss: 2.2953 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4790 train loss: 2.3057 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.2977 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4810 train loss: 2.2999 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4820 train loss: 2.3024 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4830 train loss: 2.3033 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4840 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4850 train loss: 2.3027 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4860 train loss: 2.3046 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4870 train loss: 2.3029 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4880 train loss: 2.2994 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4890 train loss: 2.2966 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.3034 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4910 train loss: 2.2990 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4920 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4930 train loss: 2.2988 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4940 train loss: 2.3075 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4950 train loss: 2.3052 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4960 train loss: 2.3040 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4970 train loss: 2.3086 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4980 train loss: 2.3013 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-4990 train loss: 2.3107 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3076 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5010 train loss: 2.3017 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5020 train loss: 2.3024 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5030 train loss: 2.3033 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5040 train loss: 2.2980 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5050 train loss: 2.3012 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5060 train loss: 2.3013 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5070 train loss: 2.3025 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5080 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5090 train loss: 2.2984 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3015 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5110 train loss: 2.2980 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5120 train loss: 2.2987 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5130 train loss: 2.2956 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5140 train loss: 2.3008 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5150 train loss: 2.3026 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5160 train loss: 2.3021 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-5170 train loss: 2.3004 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5180 train loss: 2.2993 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5190 train loss: 2.3062 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.2979 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5210 train loss: 2.3066 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5220 train loss: 2.3025 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5230 train loss: 2.3032 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5240 train loss: 2.3014 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5250 train loss: 2.2980 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5260 train loss: 2.3029 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5270 train loss: 2.3085 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5280 train loss: 2.3006 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5290 train loss: 2.2998 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.3100 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5310 train loss: 2.3050 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5320 train loss: 2.3054 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5330 train loss: 2.3034 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5340 train loss: 2.3028 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5350 train loss: 2.3096 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5360 train loss: 2.3052 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5370 train loss: 2.3069 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5380 train loss: 2.3030 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5390 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3032 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5410 train loss: 2.3055 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5420 train loss: 2.3011 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5430 train loss: 2.3010 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5440 train loss: 2.2990 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5450 train loss: 2.3002 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5460 train loss: 2.2991 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5470 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5480 train loss: 2.3048 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5490 train loss: 2.3020 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5510 train loss: 2.3039 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5520 train loss: 2.3016 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5530 train loss: 2.3029 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5540 train loss: 2.2996 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5550 train loss: 2.3005 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5560 train loss: 2.3024 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5570 train loss: 2.3013 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5580 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5590 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.3017 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5610 train loss: 2.2981 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5620 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5630 train loss: 2.3023 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5640 train loss: 2.2980 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5650 train loss: 2.3060 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5660 train loss: 2.2955 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5670 train loss: 2.2983 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5680 train loss: 2.3024 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5690 train loss: 2.3001 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.2889 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5710 train loss: 2.3019 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5720 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.3013 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5740 train loss: 2.3039 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5750 train loss: 2.3017 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5760 train loss: 2.2993 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5770 train loss: 2.3041 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5780 train loss: 2.3070 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5790 train loss: 2.2971 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.2974 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5810 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5820 train loss: 2.3027 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5830 train loss: 2.3011 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5840 train loss: 2.3029 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5850 train loss: 2.3065 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5860 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5870 train loss: 2.3073 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5880 train loss: 2.2997 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5890 train loss: 2.2983 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.2951 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5910 train loss: 2.3048 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5920 train loss: 2.3063 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5930 train loss: 2.3072 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5940 train loss: 2.3018 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5950 train loss: 2.3007 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5960 train loss: 2.3004 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5970 train loss: 2.3044 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5980 train loss: 2.2991 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-5990 train loss: 2.3056 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3023 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6010 train loss: 2.3040 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6020 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6030 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6040 train loss: 2.2961 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6050 train loss: 2.3002 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6060 train loss: 2.2997 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6070 train loss: 2.3072 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6080 train loss: 2.2996 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6090 train loss: 2.2983 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3003 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6110 train loss: 2.3070 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6120 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6130 train loss: 2.3055 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6140 train loss: 2.2914 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6150 train loss: 2.3009 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6160 train loss: 2.2993 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6170 train loss: 2.3027 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6180 train loss: 2.3055 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6190 train loss: 2.3057 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3046 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6210 train loss: 2.2933 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6220 train loss: 2.2983 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6230 train loss: 2.3018 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6240 train loss: 2.3028 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6250 train loss: 2.2993 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6260 train loss: 2.2977 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6270 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6280 train loss: 2.3014 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6290 train loss: 2.3063 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3027 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6310 train loss: 2.3033 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6320 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6330 train loss: 2.3051 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6340 train loss: 2.3043 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6350 train loss: 2.2965 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6360 train loss: 2.2993 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6370 train loss: 2.3065 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6380 train loss: 2.3057 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6390 train loss: 2.2952 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.3044 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6410 train loss: 2.2997 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6420 train loss: 2.3088 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6430 train loss: 2.2956 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6440 train loss: 2.2999 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6450 train loss: 2.3074 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6460 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6470 train loss: 2.2927 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6480 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6490 train loss: 2.3057 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.3041 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6510 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6520 train loss: 2.3005 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6530 train loss: 2.2978 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6540 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6550 train loss: 2.2981 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6560 train loss: 2.2995 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6570 train loss: 2.3034 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6580 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6590 train loss: 2.3030 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6610 train loss: 2.3005 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6620 train loss: 2.3061 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6630 train loss: 2.3040 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6640 train loss: 2.3015 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6650 train loss: 2.2917 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6660 train loss: 2.2993 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6670 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6680 train loss: 2.2958 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6690 train loss: 2.3033 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.3034 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6710 train loss: 2.2937 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6720 train loss: 2.3092 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6730 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6740 train loss: 2.3021 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6750 train loss: 2.3021 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6760 train loss: 2.3027 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6770 train loss: 2.3008 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6780 train loss: 2.2960 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6790 train loss: 2.3019 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.3056 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6810 train loss: 2.3049 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6820 train loss: 2.3139 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6830 train loss: 2.3042 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6840 train loss: 2.2990 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6850 train loss: 2.3002 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6860 train loss: 2.3097 valid loss: 2.3016, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.3094 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6880 train loss: 2.2992 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6890 train loss: 2.3016 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.3080 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6910 train loss: 2.3031 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6920 train loss: 2.3029 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6930 train loss: 2.3028 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6940 train loss: 2.2998 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6950 train loss: 2.3047 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6960 train loss: 2.2956 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6970 train loss: 2.3020 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6980 train loss: 2.2935 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-6990 train loss: 2.2954 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.2985 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7010 train loss: 2.3036 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7020 train loss: 2.2935 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7030 train loss: 2.3065 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7040 train loss: 2.3061 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7050 train loss: 2.3037 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7060 train loss: 2.3055 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7070 train loss: 2.3067 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7080 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7090 train loss: 2.2999 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3053 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7110 train loss: 2.2986 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7120 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7130 train loss: 2.2968 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7140 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7150 train loss: 2.2993 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7160 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7170 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7180 train loss: 2.2975 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7190 train loss: 2.3049 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.3050 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7210 train loss: 2.2988 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7220 train loss: 2.3084 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7230 train loss: 2.2960 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7240 train loss: 2.3063 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7250 train loss: 2.2961 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7260 train loss: 2.3000 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7270 train loss: 2.3022 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7280 train loss: 2.3062 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7290 train loss: 2.3032 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7310 train loss: 2.2893 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7320 train loss: 2.3048 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7330 train loss: 2.3033 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7340 train loss: 2.2965 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7350 train loss: 2.2942 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7360 train loss: 2.3046 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7370 train loss: 2.2966 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7380 train loss: 2.2908 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7390 train loss: 2.2952 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.3049 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7410 train loss: 2.3104 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7420 train loss: 2.3078 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7430 train loss: 2.3020 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7440 train loss: 2.2977 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7450 train loss: 2.3081 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7460 train loss: 2.2959 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7470 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7480 train loss: 2.2983 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7490 train loss: 2.3071 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7510 train loss: 2.3054 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7520 train loss: 2.3015 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7530 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7540 train loss: 2.3060 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7550 train loss: 2.3040 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7560 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7570 train loss: 2.3053 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7580 train loss: 2.3066 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7590 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.3053 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7610 train loss: 2.2992 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7620 train loss: 2.2998 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7630 train loss: 2.3035 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7640 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7650 train loss: 2.2987 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7660 train loss: 2.3012 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7670 train loss: 2.3033 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7680 train loss: 2.3057 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7690 train loss: 2.3023 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.2983 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7710 train loss: 2.2958 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7720 train loss: 2.2967 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7730 train loss: 2.3077 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7740 train loss: 2.2994 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7750 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7760 train loss: 2.2960 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7770 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7780 train loss: 2.2971 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7790 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3013 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7810 train loss: 2.3065 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7820 train loss: 2.2957 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7830 train loss: 2.3046 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7840 train loss: 2.2987 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7850 train loss: 2.3049 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7860 train loss: 2.3039 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7870 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7880 train loss: 2.2952 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7890 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.3004 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7910 train loss: 2.3068 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7920 train loss: 2.2960 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7930 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-7940 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7950 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7960 train loss: 2.3069 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7970 train loss: 2.2992 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7980 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7990 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.3045 valid loss: 2.3014, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.3038 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8020 train loss: 2.3007 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8030 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8040 train loss: 2.3107 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8050 train loss: 2.2973 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8060 train loss: 2.2975 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8070 train loss: 2.3002 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8080 train loss: 2.3050 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8090 train loss: 2.3017 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3016 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8110 train loss: 2.3047 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8120 train loss: 2.3065 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8130 train loss: 2.2984 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8140 train loss: 2.3006 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8150 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8160 train loss: 2.2982 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8170 train loss: 2.3044 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8180 train loss: 2.3048 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8190 train loss: 2.3000 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.3078 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8210 train loss: 2.3025 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8220 train loss: 2.2907 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8230 train loss: 2.2959 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8240 train loss: 2.3029 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8250 train loss: 2.3109 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8260 train loss: 2.2952 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8270 train loss: 2.2985 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8280 train loss: 2.2893 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8290 train loss: 2.3053 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.2973 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8310 train loss: 2.3017 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8320 train loss: 2.3038 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8330 train loss: 2.2895 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8340 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8350 train loss: 2.2988 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8360 train loss: 2.3030 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8370 train loss: 2.2999 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8380 train loss: 2.2978 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8390 train loss: 2.2951 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.2944 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8410 train loss: 2.3053 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8420 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8430 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8440 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8450 train loss: 2.3062 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8460 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8470 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8480 train loss: 2.2973 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8490 train loss: 2.2977 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.2955 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8510 train loss: 2.3068 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8520 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8530 train loss: 2.2992 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8540 train loss: 2.3089 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8550 train loss: 2.2964 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8560 train loss: 2.3051 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8570 train loss: 2.3059 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8580 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8590 train loss: 2.2988 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.2987 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8610 train loss: 2.2997 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8620 train loss: 2.3038 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8630 train loss: 2.3061 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8640 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8650 train loss: 2.3001 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8660 train loss: 2.3076 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8670 train loss: 2.3068 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8680 train loss: 2.3055 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8690 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.3025 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8710 train loss: 2.2935 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8720 train loss: 2.2885 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8730 train loss: 2.3008 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8740 train loss: 2.3033 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8750 train loss: 2.2931 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8760 train loss: 2.3080 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8770 train loss: 2.3064 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8780 train loss: 2.2968 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8790 train loss: 2.2966 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.2966 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8810 train loss: 2.2976 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8820 train loss: 2.2952 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8830 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8840 train loss: 2.2979 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8850 train loss: 2.2970 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8860 train loss: 2.3059 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8870 train loss: 2.3006 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8880 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8890 train loss: 2.2921 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.3006 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8910 train loss: 2.3136 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8920 train loss: 2.2913 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8930 train loss: 2.3006 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8940 train loss: 2.2990 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8950 train loss: 2.2934 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8960 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8970 train loss: 2.3108 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8980 train loss: 2.3070 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8990 train loss: 2.2978 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.2956 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-9010 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9020 train loss: 2.3099 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9030 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9040 train loss: 2.2940 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9050 train loss: 2.3025 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9060 train loss: 2.3090 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9070 train loss: 2.2937 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9080 train loss: 2.2995 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9090 train loss: 2.3013 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.3032 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9110 train loss: 2.3013 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9120 train loss: 2.3073 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9130 train loss: 2.3040 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9140 train loss: 2.3083 valid loss: 2.3013, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.3038 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9160 train loss: 2.3070 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9170 train loss: 2.2925 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9180 train loss: 2.3115 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9190 train loss: 2.3060 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9210 train loss: 2.2960 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9220 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9230 train loss: 2.2935 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9240 train loss: 2.3042 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9250 train loss: 2.3071 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9260 train loss: 2.3000 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9270 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9280 train loss: 2.3050 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9290 train loss: 2.3034 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.2993 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9310 train loss: 2.3068 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9320 train loss: 2.2953 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9330 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9340 train loss: 2.2991 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9350 train loss: 2.2998 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9360 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9370 train loss: 2.3033 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9380 train loss: 2.2980 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9390 train loss: 2.3066 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2975 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9410 train loss: 2.3040 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9420 train loss: 2.3016 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9430 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9440 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9450 train loss: 2.3059 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9460 train loss: 2.3015 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9470 train loss: 2.3117 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9480 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9490 train loss: 2.2945 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.2946 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9510 train loss: 2.3015 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9520 train loss: 2.2933 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9530 train loss: 2.3044 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9540 train loss: 2.2964 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9550 train loss: 2.2976 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9560 train loss: 2.2997 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9570 train loss: 2.3053 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9580 train loss: 2.3040 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9590 train loss: 2.3042 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.2947 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9610 train loss: 2.3070 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9620 train loss: 2.2959 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9630 train loss: 2.2986 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9640 train loss: 2.2887 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9650 train loss: 2.3073 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9660 train loss: 2.2983 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9670 train loss: 2.2929 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9680 train loss: 2.3000 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9690 train loss: 2.3079 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.2940 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9710 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9720 train loss: 2.3011 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9730 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9740 train loss: 2.2939 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9750 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9760 train loss: 2.3059 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9770 train loss: 2.3050 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9780 train loss: 2.2905 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9790 train loss: 2.3083 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.3054 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9810 train loss: 2.2979 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9820 train loss: 2.3064 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9830 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9840 train loss: 2.2995 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9850 train loss: 2.3004 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9860 train loss: 2.2978 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9870 train loss: 2.2946 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9880 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9890 train loss: 2.3106 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9910 train loss: 2.3089 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9920 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9930 train loss: 2.3016 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9940 train loss: 2.2988 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9950 train loss: 2.3042 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9960 train loss: 2.2912 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9970 train loss: 2.3064 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9980 train loss: 2.3058 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9990 train loss: 2.2977 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.2938 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3012\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11365c2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZml5tAwM7PU/j9byiAXOwpltwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1182c3f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
