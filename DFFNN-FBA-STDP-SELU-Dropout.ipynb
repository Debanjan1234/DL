{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.selu_forward(X=y)\n",
    "#         if train:\n",
    "#             y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#             caches.append((fc_cache, do_cache)) # caches[0]\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.selu_forward(X=y)\n",
    "#             if train:\n",
    "#                 y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#                 do_caches.append(do_cache)\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "#         if train:\n",
    "#             caches.append((fc_caches, do_caches)) # caches[1]\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "            caches.append((fc_cache, do_cache)) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache, do_cache = caches[2]\n",
    "        dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "#         fc_caches, do_caches = caches[1]\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.dropout_backward(cache=do_caches[layer], dout=dy)\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "#         fc_cache, do_cache = caches[0]\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3086 valid loss: 2.3898, valid accuracy: 0.0918\n",
      "Iter-20 train loss: 2.3255 valid loss: 2.3897, valid accuracy: 0.0918\n",
      "Iter-30 train loss: 2.2965 valid loss: 2.3896, valid accuracy: 0.0918\n",
      "Iter-40 train loss: 2.3115 valid loss: 2.3895, valid accuracy: 0.0918\n",
      "Iter-50 train loss: 2.3000 valid loss: 2.3893, valid accuracy: 0.0918\n",
      "Iter-60 train loss: 2.3085 valid loss: 2.3892, valid accuracy: 0.0918\n",
      "Iter-70 train loss: 2.2994 valid loss: 2.3892, valid accuracy: 0.0918\n",
      "Iter-80 train loss: 2.3078 valid loss: 2.3890, valid accuracy: 0.0922\n",
      "Iter-90 train loss: 2.3134 valid loss: 2.3889, valid accuracy: 0.0920\n",
      "Iter-100 train loss: 2.2832 valid loss: 2.3888, valid accuracy: 0.0918\n",
      "Iter-110 train loss: 2.3359 valid loss: 2.3886, valid accuracy: 0.0918\n",
      "Iter-120 train loss: 2.3218 valid loss: 2.3885, valid accuracy: 0.0924\n",
      "Iter-130 train loss: 2.3092 valid loss: 2.3884, valid accuracy: 0.0924\n",
      "Iter-140 train loss: 2.3118 valid loss: 2.3883, valid accuracy: 0.0926\n",
      "Iter-150 train loss: 2.3114 valid loss: 2.3882, valid accuracy: 0.0926\n",
      "Iter-160 train loss: 2.3156 valid loss: 2.3881, valid accuracy: 0.0926\n",
      "Iter-170 train loss: 2.3192 valid loss: 2.3880, valid accuracy: 0.0926\n",
      "Iter-180 train loss: 2.2746 valid loss: 2.3879, valid accuracy: 0.0926\n",
      "Iter-190 train loss: 2.3143 valid loss: 2.3879, valid accuracy: 0.0926\n",
      "Iter-200 train loss: 2.2980 valid loss: 2.3878, valid accuracy: 0.0926\n",
      "Iter-210 train loss: 2.3090 valid loss: 2.3877, valid accuracy: 0.0926\n",
      "Iter-220 train loss: 2.2981 valid loss: 2.3876, valid accuracy: 0.0926\n",
      "Iter-230 train loss: 2.3069 valid loss: 2.3874, valid accuracy: 0.0926\n",
      "Iter-240 train loss: 2.3131 valid loss: 2.3874, valid accuracy: 0.0924\n",
      "Iter-250 train loss: 2.2968 valid loss: 2.3873, valid accuracy: 0.0924\n",
      "Iter-260 train loss: 2.3116 valid loss: 2.3871, valid accuracy: 0.0926\n",
      "Iter-270 train loss: 2.3051 valid loss: 2.3871, valid accuracy: 0.0926\n",
      "Iter-280 train loss: 2.3206 valid loss: 2.3870, valid accuracy: 0.0926\n",
      "Iter-290 train loss: 2.2931 valid loss: 2.3869, valid accuracy: 0.0924\n",
      "Iter-300 train loss: 2.3131 valid loss: 2.3868, valid accuracy: 0.0924\n",
      "Iter-310 train loss: 2.3091 valid loss: 2.3867, valid accuracy: 0.0924\n",
      "Iter-320 train loss: 2.3015 valid loss: 2.3866, valid accuracy: 0.0924\n",
      "Iter-330 train loss: 2.3000 valid loss: 2.3865, valid accuracy: 0.0924\n",
      "Iter-340 train loss: 2.3030 valid loss: 2.3864, valid accuracy: 0.0924\n",
      "Iter-350 train loss: 2.3087 valid loss: 2.3863, valid accuracy: 0.0924\n",
      "Iter-360 train loss: 2.3065 valid loss: 2.3862, valid accuracy: 0.0924\n",
      "Iter-370 train loss: 2.2934 valid loss: 2.3861, valid accuracy: 0.0924\n",
      "Iter-380 train loss: 2.3234 valid loss: 2.3861, valid accuracy: 0.0924\n",
      "Iter-390 train loss: 2.3281 valid loss: 2.3859, valid accuracy: 0.0924\n",
      "Iter-400 train loss: 2.3121 valid loss: 2.3858, valid accuracy: 0.0926\n",
      "Iter-410 train loss: 2.3076 valid loss: 2.3858, valid accuracy: 0.0926\n",
      "Iter-420 train loss: 2.3207 valid loss: 2.3857, valid accuracy: 0.0926\n",
      "Iter-430 train loss: 2.3077 valid loss: 2.3855, valid accuracy: 0.0926\n",
      "Iter-440 train loss: 2.2987 valid loss: 2.3855, valid accuracy: 0.0926\n",
      "Iter-450 train loss: 2.3017 valid loss: 2.3854, valid accuracy: 0.0926\n",
      "Iter-460 train loss: 2.2992 valid loss: 2.3853, valid accuracy: 0.0926\n",
      "Iter-470 train loss: 2.3069 valid loss: 2.3853, valid accuracy: 0.0926\n",
      "Iter-480 train loss: 2.3044 valid loss: 2.3852, valid accuracy: 0.0928\n",
      "Iter-490 train loss: 2.3272 valid loss: 2.3851, valid accuracy: 0.0928\n",
      "Iter-500 train loss: 2.2997 valid loss: 2.3850, valid accuracy: 0.0928\n",
      "Iter-510 train loss: 2.3053 valid loss: 2.3848, valid accuracy: 0.0928\n",
      "Iter-520 train loss: 2.3076 valid loss: 2.3847, valid accuracy: 0.0928\n",
      "Iter-530 train loss: 2.3214 valid loss: 2.3847, valid accuracy: 0.0928\n",
      "Iter-540 train loss: 2.3033 valid loss: 2.3846, valid accuracy: 0.0930\n",
      "Iter-550 train loss: 2.3117 valid loss: 2.3845, valid accuracy: 0.0930\n",
      "Iter-560 train loss: 2.3014 valid loss: 2.3845, valid accuracy: 0.0928\n",
      "Iter-570 train loss: 2.3046 valid loss: 2.3844, valid accuracy: 0.0930\n",
      "Iter-580 train loss: 2.3143 valid loss: 2.3843, valid accuracy: 0.0930\n",
      "Iter-590 train loss: 2.2976 valid loss: 2.3842, valid accuracy: 0.0930\n",
      "Iter-600 train loss: 2.3067 valid loss: 2.3841, valid accuracy: 0.0930\n",
      "Iter-610 train loss: 2.2993 valid loss: 2.3840, valid accuracy: 0.0930\n",
      "Iter-620 train loss: 2.2961 valid loss: 2.3839, valid accuracy: 0.0930\n",
      "Iter-630 train loss: 2.3003 valid loss: 2.3838, valid accuracy: 0.0930\n",
      "Iter-640 train loss: 2.3045 valid loss: 2.3837, valid accuracy: 0.0930\n",
      "Iter-650 train loss: 2.2991 valid loss: 2.3836, valid accuracy: 0.0930\n",
      "Iter-660 train loss: 2.3320 valid loss: 2.3835, valid accuracy: 0.0928\n",
      "Iter-670 train loss: 2.3158 valid loss: 2.3834, valid accuracy: 0.0930\n",
      "Iter-680 train loss: 2.2781 valid loss: 2.3833, valid accuracy: 0.0930\n",
      "Iter-690 train loss: 2.3072 valid loss: 2.3831, valid accuracy: 0.0930\n",
      "Iter-700 train loss: 2.2914 valid loss: 2.3830, valid accuracy: 0.0930\n",
      "Iter-710 train loss: 2.2989 valid loss: 2.3830, valid accuracy: 0.0930\n",
      "Iter-720 train loss: 2.3102 valid loss: 2.3828, valid accuracy: 0.0930\n",
      "Iter-730 train loss: 2.3020 valid loss: 2.3828, valid accuracy: 0.0930\n",
      "Iter-740 train loss: 2.3069 valid loss: 2.3827, valid accuracy: 0.0928\n",
      "Iter-750 train loss: 2.3062 valid loss: 2.3826, valid accuracy: 0.0928\n",
      "Iter-760 train loss: 2.2988 valid loss: 2.3825, valid accuracy: 0.0928\n",
      "Iter-770 train loss: 2.3028 valid loss: 2.3824, valid accuracy: 0.0928\n",
      "Iter-780 train loss: 2.3419 valid loss: 2.3823, valid accuracy: 0.0928\n",
      "Iter-790 train loss: 2.3018 valid loss: 2.3822, valid accuracy: 0.0928\n",
      "Iter-800 train loss: 2.3086 valid loss: 2.3821, valid accuracy: 0.0926\n",
      "Iter-810 train loss: 2.3143 valid loss: 2.3820, valid accuracy: 0.0928\n",
      "Iter-820 train loss: 2.3112 valid loss: 2.3819, valid accuracy: 0.0926\n",
      "Iter-830 train loss: 2.3095 valid loss: 2.3818, valid accuracy: 0.0926\n",
      "Iter-840 train loss: 2.3141 valid loss: 2.3817, valid accuracy: 0.0926\n",
      "Iter-850 train loss: 2.3001 valid loss: 2.3816, valid accuracy: 0.0926\n",
      "Iter-860 train loss: 2.3272 valid loss: 2.3815, valid accuracy: 0.0926\n",
      "Iter-870 train loss: 2.3062 valid loss: 2.3814, valid accuracy: 0.0926\n",
      "Iter-880 train loss: 2.3254 valid loss: 2.3814, valid accuracy: 0.0926\n",
      "Iter-890 train loss: 2.3074 valid loss: 2.3813, valid accuracy: 0.0926\n",
      "Iter-900 train loss: 2.2925 valid loss: 2.3812, valid accuracy: 0.0926\n",
      "Iter-910 train loss: 2.3208 valid loss: 2.3811, valid accuracy: 0.0928\n",
      "Iter-920 train loss: 2.3077 valid loss: 2.3810, valid accuracy: 0.0926\n",
      "Iter-930 train loss: 2.3006 valid loss: 2.3809, valid accuracy: 0.0926\n",
      "Iter-940 train loss: 2.3241 valid loss: 2.3808, valid accuracy: 0.0926\n",
      "Iter-950 train loss: 2.3307 valid loss: 2.3806, valid accuracy: 0.0926\n",
      "Iter-960 train loss: 2.3087 valid loss: 2.3805, valid accuracy: 0.0926\n",
      "Iter-970 train loss: 2.3134 valid loss: 2.3804, valid accuracy: 0.0924\n",
      "Iter-980 train loss: 2.3116 valid loss: 2.3803, valid accuracy: 0.0926\n",
      "Iter-990 train loss: 2.3094 valid loss: 2.3802, valid accuracy: 0.0926\n",
      "Iter-1000 train loss: 2.3109 valid loss: 2.3802, valid accuracy: 0.0926\n",
      "Iter-1010 train loss: 2.3122 valid loss: 2.3801, valid accuracy: 0.0926\n",
      "Iter-1020 train loss: 2.3068 valid loss: 2.3800, valid accuracy: 0.0926\n",
      "Iter-1030 train loss: 2.3089 valid loss: 2.3799, valid accuracy: 0.0926\n",
      "Iter-1040 train loss: 2.3201 valid loss: 2.3798, valid accuracy: 0.0924\n",
      "Iter-1050 train loss: 2.2966 valid loss: 2.3797, valid accuracy: 0.0924\n",
      "Iter-1060 train loss: 2.2817 valid loss: 2.3797, valid accuracy: 0.0924\n",
      "Iter-1070 train loss: 2.3000 valid loss: 2.3796, valid accuracy: 0.0922\n",
      "Iter-1080 train loss: 2.3015 valid loss: 2.3795, valid accuracy: 0.0922\n",
      "Iter-1090 train loss: 2.3105 valid loss: 2.3793, valid accuracy: 0.0922\n",
      "Iter-1100 train loss: 2.3188 valid loss: 2.3792, valid accuracy: 0.0922\n",
      "Iter-1110 train loss: 2.3131 valid loss: 2.3792, valid accuracy: 0.0922\n",
      "Iter-1120 train loss: 2.3260 valid loss: 2.3790, valid accuracy: 0.0922\n",
      "Iter-1130 train loss: 2.2977 valid loss: 2.3789, valid accuracy: 0.0922\n",
      "Iter-1140 train loss: 2.2864 valid loss: 2.3788, valid accuracy: 0.0924\n",
      "Iter-1150 train loss: 2.2969 valid loss: 2.3787, valid accuracy: 0.0926\n",
      "Iter-1160 train loss: 2.2979 valid loss: 2.3787, valid accuracy: 0.0924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3107 valid loss: 2.3786, valid accuracy: 0.0924\n",
      "Iter-1180 train loss: 2.3064 valid loss: 2.3785, valid accuracy: 0.0926\n",
      "Iter-1190 train loss: 2.3150 valid loss: 2.3784, valid accuracy: 0.0926\n",
      "Iter-1200 train loss: 2.3001 valid loss: 2.3783, valid accuracy: 0.0930\n",
      "Iter-1210 train loss: 2.3187 valid loss: 2.3782, valid accuracy: 0.0930\n",
      "Iter-1220 train loss: 2.3184 valid loss: 2.3781, valid accuracy: 0.0932\n",
      "Iter-1230 train loss: 2.3066 valid loss: 2.3780, valid accuracy: 0.0932\n",
      "Iter-1240 train loss: 2.3393 valid loss: 2.3779, valid accuracy: 0.0934\n",
      "Iter-1250 train loss: 2.3063 valid loss: 2.3778, valid accuracy: 0.0934\n",
      "Iter-1260 train loss: 2.3053 valid loss: 2.3777, valid accuracy: 0.0934\n",
      "Iter-1270 train loss: 2.3162 valid loss: 2.3776, valid accuracy: 0.0934\n",
      "Iter-1280 train loss: 2.2938 valid loss: 2.3775, valid accuracy: 0.0934\n",
      "Iter-1290 train loss: 2.2758 valid loss: 2.3774, valid accuracy: 0.0934\n",
      "Iter-1300 train loss: 2.3060 valid loss: 2.3774, valid accuracy: 0.0936\n",
      "Iter-1310 train loss: 2.3209 valid loss: 2.3773, valid accuracy: 0.0936\n",
      "Iter-1320 train loss: 2.3152 valid loss: 2.3771, valid accuracy: 0.0936\n",
      "Iter-1330 train loss: 2.2951 valid loss: 2.3770, valid accuracy: 0.0936\n",
      "Iter-1340 train loss: 2.3155 valid loss: 2.3768, valid accuracy: 0.0936\n",
      "Iter-1350 train loss: 2.3032 valid loss: 2.3768, valid accuracy: 0.0936\n",
      "Iter-1360 train loss: 2.2985 valid loss: 2.3767, valid accuracy: 0.0936\n",
      "Iter-1370 train loss: 2.3064 valid loss: 2.3766, valid accuracy: 0.0936\n",
      "Iter-1380 train loss: 2.3304 valid loss: 2.3765, valid accuracy: 0.0936\n",
      "Iter-1390 train loss: 2.3027 valid loss: 2.3765, valid accuracy: 0.0936\n",
      "Iter-1400 train loss: 2.2959 valid loss: 2.3764, valid accuracy: 0.0936\n",
      "Iter-1410 train loss: 2.3099 valid loss: 2.3763, valid accuracy: 0.0936\n",
      "Iter-1420 train loss: 2.2973 valid loss: 2.3762, valid accuracy: 0.0938\n",
      "Iter-1430 train loss: 2.3120 valid loss: 2.3761, valid accuracy: 0.0938\n",
      "Iter-1440 train loss: 2.3100 valid loss: 2.3760, valid accuracy: 0.0938\n",
      "Iter-1450 train loss: 2.3104 valid loss: 2.3760, valid accuracy: 0.0938\n",
      "Iter-1460 train loss: 2.3106 valid loss: 2.3759, valid accuracy: 0.0938\n",
      "Iter-1470 train loss: 2.3153 valid loss: 2.3758, valid accuracy: 0.0936\n",
      "Iter-1480 train loss: 2.3182 valid loss: 2.3757, valid accuracy: 0.0936\n",
      "Iter-1490 train loss: 2.3254 valid loss: 2.3756, valid accuracy: 0.0936\n",
      "Iter-1500 train loss: 2.3035 valid loss: 2.3755, valid accuracy: 0.0936\n",
      "Iter-1510 train loss: 2.3022 valid loss: 2.3754, valid accuracy: 0.0936\n",
      "Iter-1520 train loss: 2.2912 valid loss: 2.3753, valid accuracy: 0.0936\n",
      "Iter-1530 train loss: 2.3005 valid loss: 2.3752, valid accuracy: 0.0936\n",
      "Iter-1540 train loss: 2.3156 valid loss: 2.3751, valid accuracy: 0.0936\n",
      "Iter-1550 train loss: 2.3052 valid loss: 2.3749, valid accuracy: 0.0936\n",
      "Iter-1560 train loss: 2.3117 valid loss: 2.3748, valid accuracy: 0.0938\n",
      "Iter-1570 train loss: 2.3067 valid loss: 2.3747, valid accuracy: 0.0938\n",
      "Iter-1580 train loss: 2.3097 valid loss: 2.3746, valid accuracy: 0.0938\n",
      "Iter-1590 train loss: 2.3160 valid loss: 2.3745, valid accuracy: 0.0938\n",
      "Iter-1600 train loss: 2.3298 valid loss: 2.3744, valid accuracy: 0.0938\n",
      "Iter-1610 train loss: 2.3030 valid loss: 2.3743, valid accuracy: 0.0938\n",
      "Iter-1620 train loss: 2.3032 valid loss: 2.3742, valid accuracy: 0.0938\n",
      "Iter-1630 train loss: 2.2998 valid loss: 2.3741, valid accuracy: 0.0938\n",
      "Iter-1640 train loss: 2.3100 valid loss: 2.3740, valid accuracy: 0.0938\n",
      "Iter-1650 train loss: 2.3096 valid loss: 2.3739, valid accuracy: 0.0938\n",
      "Iter-1660 train loss: 2.3120 valid loss: 2.3737, valid accuracy: 0.0940\n",
      "Iter-1670 train loss: 2.3071 valid loss: 2.3737, valid accuracy: 0.0940\n",
      "Iter-1680 train loss: 2.3120 valid loss: 2.3736, valid accuracy: 0.0940\n",
      "Iter-1690 train loss: 2.3153 valid loss: 2.3735, valid accuracy: 0.0942\n",
      "Iter-1700 train loss: 2.3209 valid loss: 2.3734, valid accuracy: 0.0942\n",
      "Iter-1710 train loss: 2.3068 valid loss: 2.3733, valid accuracy: 0.0942\n",
      "Iter-1720 train loss: 2.3050 valid loss: 2.3732, valid accuracy: 0.0942\n",
      "Iter-1730 train loss: 2.3041 valid loss: 2.3732, valid accuracy: 0.0942\n",
      "Iter-1740 train loss: 2.3062 valid loss: 2.3731, valid accuracy: 0.0942\n",
      "Iter-1750 train loss: 2.3354 valid loss: 2.3730, valid accuracy: 0.0942\n",
      "Iter-1760 train loss: 2.3086 valid loss: 2.3730, valid accuracy: 0.0944\n",
      "Iter-1770 train loss: 2.3060 valid loss: 2.3729, valid accuracy: 0.0944\n",
      "Iter-1780 train loss: 2.3144 valid loss: 2.3728, valid accuracy: 0.0944\n",
      "Iter-1790 train loss: 2.2907 valid loss: 2.3727, valid accuracy: 0.0946\n",
      "Iter-1800 train loss: 2.3093 valid loss: 2.3726, valid accuracy: 0.0946\n",
      "Iter-1810 train loss: 2.3119 valid loss: 2.3725, valid accuracy: 0.0946\n",
      "Iter-1820 train loss: 2.3080 valid loss: 2.3724, valid accuracy: 0.0948\n",
      "Iter-1830 train loss: 2.3018 valid loss: 2.3723, valid accuracy: 0.0948\n",
      "Iter-1840 train loss: 2.3039 valid loss: 2.3723, valid accuracy: 0.0946\n",
      "Iter-1850 train loss: 2.3266 valid loss: 2.3722, valid accuracy: 0.0948\n",
      "Iter-1860 train loss: 2.3056 valid loss: 2.3721, valid accuracy: 0.0948\n",
      "Iter-1870 train loss: 2.3076 valid loss: 2.3721, valid accuracy: 0.0948\n",
      "Iter-1880 train loss: 2.3310 valid loss: 2.3719, valid accuracy: 0.0946\n",
      "Iter-1890 train loss: 2.2886 valid loss: 2.3718, valid accuracy: 0.0946\n",
      "Iter-1900 train loss: 2.2984 valid loss: 2.3717, valid accuracy: 0.0946\n",
      "Iter-1910 train loss: 2.2850 valid loss: 2.3716, valid accuracy: 0.0946\n",
      "Iter-1920 train loss: 2.3171 valid loss: 2.3715, valid accuracy: 0.0946\n",
      "Iter-1930 train loss: 2.3190 valid loss: 2.3714, valid accuracy: 0.0948\n",
      "Iter-1940 train loss: 2.3004 valid loss: 2.3713, valid accuracy: 0.0948\n",
      "Iter-1950 train loss: 2.3203 valid loss: 2.3712, valid accuracy: 0.0948\n",
      "Iter-1960 train loss: 2.3278 valid loss: 2.3712, valid accuracy: 0.0946\n",
      "Iter-1970 train loss: 2.3068 valid loss: 2.3710, valid accuracy: 0.0946\n",
      "Iter-1980 train loss: 2.2927 valid loss: 2.3710, valid accuracy: 0.0948\n",
      "Iter-1990 train loss: 2.2945 valid loss: 2.3709, valid accuracy: 0.0948\n",
      "Iter-2000 train loss: 2.3031 valid loss: 2.3708, valid accuracy: 0.0948\n",
      "Iter-2010 train loss: 2.3221 valid loss: 2.3707, valid accuracy: 0.0948\n",
      "Iter-2020 train loss: 2.3249 valid loss: 2.3706, valid accuracy: 0.0950\n",
      "Iter-2030 train loss: 2.3058 valid loss: 2.3705, valid accuracy: 0.0950\n",
      "Iter-2040 train loss: 2.3026 valid loss: 2.3704, valid accuracy: 0.0950\n",
      "Iter-2050 train loss: 2.3150 valid loss: 2.3703, valid accuracy: 0.0952\n",
      "Iter-2060 train loss: 2.2991 valid loss: 2.3702, valid accuracy: 0.0952\n",
      "Iter-2070 train loss: 2.3175 valid loss: 2.3701, valid accuracy: 0.0952\n",
      "Iter-2080 train loss: 2.3217 valid loss: 2.3700, valid accuracy: 0.0952\n",
      "Iter-2090 train loss: 2.3200 valid loss: 2.3700, valid accuracy: 0.0952\n",
      "Iter-2100 train loss: 2.3012 valid loss: 2.3699, valid accuracy: 0.0952\n",
      "Iter-2110 train loss: 2.3078 valid loss: 2.3699, valid accuracy: 0.0952\n",
      "Iter-2120 train loss: 2.3066 valid loss: 2.3698, valid accuracy: 0.0952\n",
      "Iter-2130 train loss: 2.3064 valid loss: 2.3696, valid accuracy: 0.0952\n",
      "Iter-2140 train loss: 2.2989 valid loss: 2.3695, valid accuracy: 0.0952\n",
      "Iter-2150 train loss: 2.3048 valid loss: 2.3694, valid accuracy: 0.0952\n",
      "Iter-2160 train loss: 2.3113 valid loss: 2.3693, valid accuracy: 0.0952\n",
      "Iter-2170 train loss: 2.3036 valid loss: 2.3692, valid accuracy: 0.0952\n",
      "Iter-2180 train loss: 2.2778 valid loss: 2.3691, valid accuracy: 0.0954\n",
      "Iter-2190 train loss: 2.3051 valid loss: 2.3690, valid accuracy: 0.0954\n",
      "Iter-2200 train loss: 2.3159 valid loss: 2.3690, valid accuracy: 0.0954\n",
      "Iter-2210 train loss: 2.3233 valid loss: 2.3689, valid accuracy: 0.0954\n",
      "Iter-2220 train loss: 2.3253 valid loss: 2.3687, valid accuracy: 0.0954\n",
      "Iter-2230 train loss: 2.3025 valid loss: 2.3686, valid accuracy: 0.0954\n",
      "Iter-2240 train loss: 2.3019 valid loss: 2.3685, valid accuracy: 0.0952\n",
      "Iter-2250 train loss: 2.3373 valid loss: 2.3684, valid accuracy: 0.0952\n",
      "Iter-2260 train loss: 2.3412 valid loss: 2.3683, valid accuracy: 0.0950\n",
      "Iter-2270 train loss: 2.3084 valid loss: 2.3683, valid accuracy: 0.0950\n",
      "Iter-2280 train loss: 2.3068 valid loss: 2.3682, valid accuracy: 0.0950\n",
      "Iter-2290 train loss: 2.3181 valid loss: 2.3681, valid accuracy: 0.0952\n",
      "Iter-2300 train loss: 2.3053 valid loss: 2.3681, valid accuracy: 0.0952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3310 valid loss: 2.3680, valid accuracy: 0.0952\n",
      "Iter-2320 train loss: 2.3100 valid loss: 2.3679, valid accuracy: 0.0952\n",
      "Iter-2330 train loss: 2.3089 valid loss: 2.3678, valid accuracy: 0.0952\n",
      "Iter-2340 train loss: 2.3015 valid loss: 2.3677, valid accuracy: 0.0950\n",
      "Iter-2350 train loss: 2.3093 valid loss: 2.3676, valid accuracy: 0.0950\n",
      "Iter-2360 train loss: 2.2904 valid loss: 2.3675, valid accuracy: 0.0950\n",
      "Iter-2370 train loss: 2.3053 valid loss: 2.3674, valid accuracy: 0.0950\n",
      "Iter-2380 train loss: 2.2923 valid loss: 2.3673, valid accuracy: 0.0950\n",
      "Iter-2390 train loss: 2.3022 valid loss: 2.3672, valid accuracy: 0.0950\n",
      "Iter-2400 train loss: 2.3082 valid loss: 2.3671, valid accuracy: 0.0950\n",
      "Iter-2410 train loss: 2.3064 valid loss: 2.3670, valid accuracy: 0.0950\n",
      "Iter-2420 train loss: 2.2968 valid loss: 2.3670, valid accuracy: 0.0950\n",
      "Iter-2430 train loss: 2.3061 valid loss: 2.3669, valid accuracy: 0.0950\n",
      "Iter-2440 train loss: 2.3200 valid loss: 2.3668, valid accuracy: 0.0950\n",
      "Iter-2450 train loss: 2.3076 valid loss: 2.3667, valid accuracy: 0.0946\n",
      "Iter-2460 train loss: 2.3132 valid loss: 2.3667, valid accuracy: 0.0946\n",
      "Iter-2470 train loss: 2.3145 valid loss: 2.3666, valid accuracy: 0.0946\n",
      "Iter-2480 train loss: 2.3023 valid loss: 2.3665, valid accuracy: 0.0946\n",
      "Iter-2490 train loss: 2.3087 valid loss: 2.3664, valid accuracy: 0.0946\n",
      "Iter-2500 train loss: 2.3096 valid loss: 2.3663, valid accuracy: 0.0946\n",
      "Iter-2510 train loss: 2.3310 valid loss: 2.3662, valid accuracy: 0.0946\n",
      "Iter-2520 train loss: 2.3096 valid loss: 2.3661, valid accuracy: 0.0948\n",
      "Iter-2530 train loss: 2.2834 valid loss: 2.3661, valid accuracy: 0.0948\n",
      "Iter-2540 train loss: 2.3140 valid loss: 2.3660, valid accuracy: 0.0948\n",
      "Iter-2550 train loss: 2.3374 valid loss: 2.3659, valid accuracy: 0.0948\n",
      "Iter-2560 train loss: 2.3004 valid loss: 2.3658, valid accuracy: 0.0948\n",
      "Iter-2570 train loss: 2.3182 valid loss: 2.3657, valid accuracy: 0.0948\n",
      "Iter-2580 train loss: 2.3233 valid loss: 2.3656, valid accuracy: 0.0948\n",
      "Iter-2590 train loss: 2.3218 valid loss: 2.3655, valid accuracy: 0.0950\n",
      "Iter-2600 train loss: 2.3036 valid loss: 2.3655, valid accuracy: 0.0950\n",
      "Iter-2610 train loss: 2.3142 valid loss: 2.3654, valid accuracy: 0.0950\n",
      "Iter-2620 train loss: 2.3021 valid loss: 2.3654, valid accuracy: 0.0950\n",
      "Iter-2630 train loss: 2.3324 valid loss: 2.3653, valid accuracy: 0.0950\n",
      "Iter-2640 train loss: 2.2975 valid loss: 2.3651, valid accuracy: 0.0952\n",
      "Iter-2650 train loss: 2.3116 valid loss: 2.3651, valid accuracy: 0.0952\n",
      "Iter-2660 train loss: 2.3208 valid loss: 2.3649, valid accuracy: 0.0952\n",
      "Iter-2670 train loss: 2.3137 valid loss: 2.3649, valid accuracy: 0.0952\n",
      "Iter-2680 train loss: 2.3022 valid loss: 2.3647, valid accuracy: 0.0956\n",
      "Iter-2690 train loss: 2.3050 valid loss: 2.3647, valid accuracy: 0.0958\n",
      "Iter-2700 train loss: 2.3046 valid loss: 2.3646, valid accuracy: 0.0958\n",
      "Iter-2710 train loss: 2.3102 valid loss: 2.3645, valid accuracy: 0.0962\n",
      "Iter-2720 train loss: 2.3174 valid loss: 2.3644, valid accuracy: 0.0962\n",
      "Iter-2730 train loss: 2.3252 valid loss: 2.3643, valid accuracy: 0.0960\n",
      "Iter-2740 train loss: 2.3146 valid loss: 2.3642, valid accuracy: 0.0962\n",
      "Iter-2750 train loss: 2.3275 valid loss: 2.3641, valid accuracy: 0.0962\n",
      "Iter-2760 train loss: 2.3088 valid loss: 2.3640, valid accuracy: 0.0962\n",
      "Iter-2770 train loss: 2.3001 valid loss: 2.3639, valid accuracy: 0.0962\n",
      "Iter-2780 train loss: 2.3207 valid loss: 2.3638, valid accuracy: 0.0962\n",
      "Iter-2790 train loss: 2.3147 valid loss: 2.3637, valid accuracy: 0.0962\n",
      "Iter-2800 train loss: 2.2954 valid loss: 2.3636, valid accuracy: 0.0962\n",
      "Iter-2810 train loss: 2.2925 valid loss: 2.3635, valid accuracy: 0.0962\n",
      "Iter-2820 train loss: 2.3229 valid loss: 2.3634, valid accuracy: 0.0964\n",
      "Iter-2830 train loss: 2.3023 valid loss: 2.3633, valid accuracy: 0.0964\n",
      "Iter-2840 train loss: 2.2988 valid loss: 2.3632, valid accuracy: 0.0964\n",
      "Iter-2850 train loss: 2.3253 valid loss: 2.3631, valid accuracy: 0.0964\n",
      "Iter-2860 train loss: 2.3236 valid loss: 2.3630, valid accuracy: 0.0966\n",
      "Iter-2870 train loss: 2.3356 valid loss: 2.3628, valid accuracy: 0.0966\n",
      "Iter-2880 train loss: 2.3199 valid loss: 2.3627, valid accuracy: 0.0964\n",
      "Iter-2890 train loss: 2.3154 valid loss: 2.3627, valid accuracy: 0.0964\n",
      "Iter-2900 train loss: 2.2973 valid loss: 2.3626, valid accuracy: 0.0964\n",
      "Iter-2910 train loss: 2.2986 valid loss: 2.3625, valid accuracy: 0.0966\n",
      "Iter-2920 train loss: 2.3008 valid loss: 2.3625, valid accuracy: 0.0966\n",
      "Iter-2930 train loss: 2.2922 valid loss: 2.3624, valid accuracy: 0.0966\n",
      "Iter-2940 train loss: 2.3042 valid loss: 2.3623, valid accuracy: 0.0966\n",
      "Iter-2950 train loss: 2.3166 valid loss: 2.3622, valid accuracy: 0.0966\n",
      "Iter-2960 train loss: 2.3161 valid loss: 2.3621, valid accuracy: 0.0966\n",
      "Iter-2970 train loss: 2.2957 valid loss: 2.3620, valid accuracy: 0.0966\n",
      "Iter-2980 train loss: 2.3059 valid loss: 2.3620, valid accuracy: 0.0966\n",
      "Iter-2990 train loss: 2.3169 valid loss: 2.3619, valid accuracy: 0.0966\n",
      "Iter-3000 train loss: 2.3198 valid loss: 2.3619, valid accuracy: 0.0966\n",
      "Iter-3010 train loss: 2.3081 valid loss: 2.3618, valid accuracy: 0.0966\n",
      "Iter-3020 train loss: 2.3111 valid loss: 2.3616, valid accuracy: 0.0966\n",
      "Iter-3030 train loss: 2.2993 valid loss: 2.3616, valid accuracy: 0.0966\n",
      "Iter-3040 train loss: 2.3139 valid loss: 2.3615, valid accuracy: 0.0966\n",
      "Iter-3050 train loss: 2.3116 valid loss: 2.3613, valid accuracy: 0.0964\n",
      "Iter-3060 train loss: 2.2985 valid loss: 2.3612, valid accuracy: 0.0964\n",
      "Iter-3070 train loss: 2.3056 valid loss: 2.3611, valid accuracy: 0.0966\n",
      "Iter-3080 train loss: 2.3087 valid loss: 2.3610, valid accuracy: 0.0968\n",
      "Iter-3090 train loss: 2.3119 valid loss: 2.3609, valid accuracy: 0.0968\n",
      "Iter-3100 train loss: 2.3023 valid loss: 2.3608, valid accuracy: 0.0968\n",
      "Iter-3110 train loss: 2.3164 valid loss: 2.3607, valid accuracy: 0.0968\n",
      "Iter-3120 train loss: 2.3012 valid loss: 2.3607, valid accuracy: 0.0968\n",
      "Iter-3130 train loss: 2.3055 valid loss: 2.3605, valid accuracy: 0.0968\n",
      "Iter-3140 train loss: 2.3140 valid loss: 2.3605, valid accuracy: 0.0968\n",
      "Iter-3150 train loss: 2.3159 valid loss: 2.3603, valid accuracy: 0.0968\n",
      "Iter-3160 train loss: 2.3003 valid loss: 2.3603, valid accuracy: 0.0966\n",
      "Iter-3170 train loss: 2.3390 valid loss: 2.3602, valid accuracy: 0.0966\n",
      "Iter-3180 train loss: 2.3069 valid loss: 2.3601, valid accuracy: 0.0966\n",
      "Iter-3190 train loss: 2.3075 valid loss: 2.3600, valid accuracy: 0.0966\n",
      "Iter-3200 train loss: 2.3071 valid loss: 2.3599, valid accuracy: 0.0964\n",
      "Iter-3210 train loss: 2.3316 valid loss: 2.3598, valid accuracy: 0.0964\n",
      "Iter-3220 train loss: 2.3252 valid loss: 2.3597, valid accuracy: 0.0964\n",
      "Iter-3230 train loss: 2.2989 valid loss: 2.3596, valid accuracy: 0.0964\n",
      "Iter-3240 train loss: 2.3052 valid loss: 2.3595, valid accuracy: 0.0964\n",
      "Iter-3250 train loss: 2.3003 valid loss: 2.3595, valid accuracy: 0.0966\n",
      "Iter-3260 train loss: 2.3181 valid loss: 2.3594, valid accuracy: 0.0966\n",
      "Iter-3270 train loss: 2.3034 valid loss: 2.3593, valid accuracy: 0.0966\n",
      "Iter-3280 train loss: 2.3064 valid loss: 2.3592, valid accuracy: 0.0966\n",
      "Iter-3290 train loss: 2.3082 valid loss: 2.3591, valid accuracy: 0.0966\n",
      "Iter-3300 train loss: 2.3046 valid loss: 2.3590, valid accuracy: 0.0968\n",
      "Iter-3310 train loss: 2.2894 valid loss: 2.3590, valid accuracy: 0.0968\n",
      "Iter-3320 train loss: 2.3065 valid loss: 2.3589, valid accuracy: 0.0968\n",
      "Iter-3330 train loss: 2.3239 valid loss: 2.3588, valid accuracy: 0.0968\n",
      "Iter-3340 train loss: 2.2850 valid loss: 2.3587, valid accuracy: 0.0968\n",
      "Iter-3350 train loss: 2.3142 valid loss: 2.3586, valid accuracy: 0.0968\n",
      "Iter-3360 train loss: 2.3146 valid loss: 2.3585, valid accuracy: 0.0970\n",
      "Iter-3370 train loss: 2.2971 valid loss: 2.3584, valid accuracy: 0.0970\n",
      "Iter-3380 train loss: 2.2948 valid loss: 2.3582, valid accuracy: 0.0972\n",
      "Iter-3390 train loss: 2.3035 valid loss: 2.3582, valid accuracy: 0.0972\n",
      "Iter-3400 train loss: 2.3171 valid loss: 2.3581, valid accuracy: 0.0972\n",
      "Iter-3410 train loss: 2.3066 valid loss: 2.3580, valid accuracy: 0.0972\n",
      "Iter-3420 train loss: 2.2979 valid loss: 2.3578, valid accuracy: 0.0972\n",
      "Iter-3430 train loss: 2.3052 valid loss: 2.3577, valid accuracy: 0.0972\n",
      "Iter-3440 train loss: 2.3357 valid loss: 2.3576, valid accuracy: 0.0972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3229 valid loss: 2.3575, valid accuracy: 0.0972\n",
      "Iter-3460 train loss: 2.3043 valid loss: 2.3575, valid accuracy: 0.0972\n",
      "Iter-3470 train loss: 2.3037 valid loss: 2.3573, valid accuracy: 0.0968\n",
      "Iter-3480 train loss: 2.2810 valid loss: 2.3572, valid accuracy: 0.0970\n",
      "Iter-3490 train loss: 2.3296 valid loss: 2.3571, valid accuracy: 0.0968\n",
      "Iter-3500 train loss: 2.3123 valid loss: 2.3570, valid accuracy: 0.0970\n",
      "Iter-3510 train loss: 2.3143 valid loss: 2.3569, valid accuracy: 0.0970\n",
      "Iter-3520 train loss: 2.3165 valid loss: 2.3568, valid accuracy: 0.0970\n",
      "Iter-3530 train loss: 2.3044 valid loss: 2.3567, valid accuracy: 0.0970\n",
      "Iter-3540 train loss: 2.3108 valid loss: 2.3566, valid accuracy: 0.0970\n",
      "Iter-3550 train loss: 2.3043 valid loss: 2.3566, valid accuracy: 0.0970\n",
      "Iter-3560 train loss: 2.3039 valid loss: 2.3565, valid accuracy: 0.0970\n",
      "Iter-3570 train loss: 2.2949 valid loss: 2.3564, valid accuracy: 0.0972\n",
      "Iter-3580 train loss: 2.3251 valid loss: 2.3563, valid accuracy: 0.0972\n",
      "Iter-3590 train loss: 2.2986 valid loss: 2.3562, valid accuracy: 0.0972\n",
      "Iter-3600 train loss: 2.2908 valid loss: 2.3561, valid accuracy: 0.0974\n",
      "Iter-3610 train loss: 2.3010 valid loss: 2.3561, valid accuracy: 0.0974\n",
      "Iter-3620 train loss: 2.2941 valid loss: 2.3560, valid accuracy: 0.0976\n",
      "Iter-3630 train loss: 2.3027 valid loss: 2.3559, valid accuracy: 0.0976\n",
      "Iter-3640 train loss: 2.3059 valid loss: 2.3558, valid accuracy: 0.0976\n",
      "Iter-3650 train loss: 2.3038 valid loss: 2.3557, valid accuracy: 0.0976\n",
      "Iter-3660 train loss: 2.3083 valid loss: 2.3557, valid accuracy: 0.0976\n",
      "Iter-3670 train loss: 2.3138 valid loss: 2.3556, valid accuracy: 0.0978\n",
      "Iter-3680 train loss: 2.3319 valid loss: 2.3555, valid accuracy: 0.0978\n",
      "Iter-3690 train loss: 2.2967 valid loss: 2.3555, valid accuracy: 0.0978\n",
      "Iter-3700 train loss: 2.3111 valid loss: 2.3554, valid accuracy: 0.0978\n",
      "Iter-3710 train loss: 2.3009 valid loss: 2.3554, valid accuracy: 0.0978\n",
      "Iter-3720 train loss: 2.3081 valid loss: 2.3553, valid accuracy: 0.0978\n",
      "Iter-3730 train loss: 2.3037 valid loss: 2.3552, valid accuracy: 0.0976\n",
      "Iter-3740 train loss: 2.3088 valid loss: 2.3551, valid accuracy: 0.0976\n",
      "Iter-3750 train loss: 2.3054 valid loss: 2.3550, valid accuracy: 0.0976\n",
      "Iter-3760 train loss: 2.3170 valid loss: 2.3549, valid accuracy: 0.0976\n",
      "Iter-3770 train loss: 2.3031 valid loss: 2.3548, valid accuracy: 0.0976\n",
      "Iter-3780 train loss: 2.3124 valid loss: 2.3547, valid accuracy: 0.0976\n",
      "Iter-3790 train loss: 2.2919 valid loss: 2.3546, valid accuracy: 0.0976\n",
      "Iter-3800 train loss: 2.3000 valid loss: 2.3545, valid accuracy: 0.0976\n",
      "Iter-3810 train loss: 2.3143 valid loss: 2.3545, valid accuracy: 0.0976\n",
      "Iter-3820 train loss: 2.2938 valid loss: 2.3544, valid accuracy: 0.0976\n",
      "Iter-3830 train loss: 2.2729 valid loss: 2.3543, valid accuracy: 0.0976\n",
      "Iter-3840 train loss: 2.2734 valid loss: 2.3542, valid accuracy: 0.0976\n",
      "Iter-3850 train loss: 2.3110 valid loss: 2.3541, valid accuracy: 0.0976\n",
      "Iter-3860 train loss: 2.2800 valid loss: 2.3540, valid accuracy: 0.0974\n",
      "Iter-3870 train loss: 2.3125 valid loss: 2.3539, valid accuracy: 0.0976\n",
      "Iter-3880 train loss: 2.3085 valid loss: 2.3537, valid accuracy: 0.0976\n",
      "Iter-3890 train loss: 2.3078 valid loss: 2.3536, valid accuracy: 0.0976\n",
      "Iter-3900 train loss: 2.2977 valid loss: 2.3535, valid accuracy: 0.0976\n",
      "Iter-3910 train loss: 2.3131 valid loss: 2.3534, valid accuracy: 0.0978\n",
      "Iter-3920 train loss: 2.3092 valid loss: 2.3533, valid accuracy: 0.0980\n",
      "Iter-3930 train loss: 2.3116 valid loss: 2.3532, valid accuracy: 0.0980\n",
      "Iter-3940 train loss: 2.3177 valid loss: 2.3531, valid accuracy: 0.0980\n",
      "Iter-3950 train loss: 2.2815 valid loss: 2.3530, valid accuracy: 0.0980\n",
      "Iter-3960 train loss: 2.2907 valid loss: 2.3529, valid accuracy: 0.0980\n",
      "Iter-3970 train loss: 2.2878 valid loss: 2.3528, valid accuracy: 0.0982\n",
      "Iter-3980 train loss: 2.3292 valid loss: 2.3527, valid accuracy: 0.0980\n",
      "Iter-3990 train loss: 2.3205 valid loss: 2.3526, valid accuracy: 0.0980\n",
      "Iter-4000 train loss: 2.3072 valid loss: 2.3526, valid accuracy: 0.0980\n",
      "Iter-4010 train loss: 2.3001 valid loss: 2.3525, valid accuracy: 0.0980\n",
      "Iter-4020 train loss: 2.3032 valid loss: 2.3524, valid accuracy: 0.0980\n",
      "Iter-4030 train loss: 2.3339 valid loss: 2.3523, valid accuracy: 0.0980\n",
      "Iter-4040 train loss: 2.3142 valid loss: 2.3522, valid accuracy: 0.0980\n",
      "Iter-4050 train loss: 2.2989 valid loss: 2.3521, valid accuracy: 0.0980\n",
      "Iter-4060 train loss: 2.2766 valid loss: 2.3521, valid accuracy: 0.0980\n",
      "Iter-4070 train loss: 2.2980 valid loss: 2.3520, valid accuracy: 0.0980\n",
      "Iter-4080 train loss: 2.2750 valid loss: 2.3519, valid accuracy: 0.0980\n",
      "Iter-4090 train loss: 2.2865 valid loss: 2.3518, valid accuracy: 0.0980\n",
      "Iter-4100 train loss: 2.3223 valid loss: 2.3517, valid accuracy: 0.0980\n",
      "Iter-4110 train loss: 2.2938 valid loss: 2.3517, valid accuracy: 0.0980\n",
      "Iter-4120 train loss: 2.2854 valid loss: 2.3516, valid accuracy: 0.0982\n",
      "Iter-4130 train loss: 2.3008 valid loss: 2.3515, valid accuracy: 0.0980\n",
      "Iter-4140 train loss: 2.3002 valid loss: 2.3514, valid accuracy: 0.0982\n",
      "Iter-4150 train loss: 2.3135 valid loss: 2.3513, valid accuracy: 0.0982\n",
      "Iter-4160 train loss: 2.2974 valid loss: 2.3512, valid accuracy: 0.0982\n",
      "Iter-4170 train loss: 2.2971 valid loss: 2.3511, valid accuracy: 0.0982\n",
      "Iter-4180 train loss: 2.2970 valid loss: 2.3510, valid accuracy: 0.0982\n",
      "Iter-4190 train loss: 2.3052 valid loss: 2.3510, valid accuracy: 0.0982\n",
      "Iter-4200 train loss: 2.3144 valid loss: 2.3509, valid accuracy: 0.0982\n",
      "Iter-4210 train loss: 2.3067 valid loss: 2.3508, valid accuracy: 0.0982\n",
      "Iter-4220 train loss: 2.3001 valid loss: 2.3507, valid accuracy: 0.0984\n",
      "Iter-4230 train loss: 2.2947 valid loss: 2.3506, valid accuracy: 0.0984\n",
      "Iter-4240 train loss: 2.3025 valid loss: 2.3505, valid accuracy: 0.0984\n",
      "Iter-4250 train loss: 2.3251 valid loss: 2.3504, valid accuracy: 0.0984\n",
      "Iter-4260 train loss: 2.3040 valid loss: 2.3502, valid accuracy: 0.0984\n",
      "Iter-4270 train loss: 2.2980 valid loss: 2.3502, valid accuracy: 0.0984\n",
      "Iter-4280 train loss: 2.2991 valid loss: 2.3501, valid accuracy: 0.0986\n",
      "Iter-4290 train loss: 2.2945 valid loss: 2.3500, valid accuracy: 0.0986\n",
      "Iter-4300 train loss: 2.3142 valid loss: 2.3499, valid accuracy: 0.0986\n",
      "Iter-4310 train loss: 2.3186 valid loss: 2.3498, valid accuracy: 0.0986\n",
      "Iter-4320 train loss: 2.3026 valid loss: 2.3496, valid accuracy: 0.0986\n",
      "Iter-4330 train loss: 2.3243 valid loss: 2.3495, valid accuracy: 0.0986\n",
      "Iter-4340 train loss: 2.3069 valid loss: 2.3494, valid accuracy: 0.0986\n",
      "Iter-4350 train loss: 2.3142 valid loss: 2.3493, valid accuracy: 0.0986\n",
      "Iter-4360 train loss: 2.3205 valid loss: 2.3492, valid accuracy: 0.0986\n",
      "Iter-4370 train loss: 2.3190 valid loss: 2.3491, valid accuracy: 0.0986\n",
      "Iter-4380 train loss: 2.2888 valid loss: 2.3489, valid accuracy: 0.0986\n",
      "Iter-4390 train loss: 2.3053 valid loss: 2.3488, valid accuracy: 0.0986\n",
      "Iter-4400 train loss: 2.3061 valid loss: 2.3487, valid accuracy: 0.0986\n",
      "Iter-4410 train loss: 2.3147 valid loss: 2.3487, valid accuracy: 0.0986\n",
      "Iter-4420 train loss: 2.3187 valid loss: 2.3485, valid accuracy: 0.0986\n",
      "Iter-4430 train loss: 2.2997 valid loss: 2.3485, valid accuracy: 0.0986\n",
      "Iter-4440 train loss: 2.3082 valid loss: 2.3484, valid accuracy: 0.0986\n",
      "Iter-4450 train loss: 2.3051 valid loss: 2.3483, valid accuracy: 0.0986\n",
      "Iter-4460 train loss: 2.2748 valid loss: 2.3482, valid accuracy: 0.0986\n",
      "Iter-4470 train loss: 2.3130 valid loss: 2.3482, valid accuracy: 0.0986\n",
      "Iter-4480 train loss: 2.3063 valid loss: 2.3481, valid accuracy: 0.0986\n",
      "Iter-4490 train loss: 2.3106 valid loss: 2.3480, valid accuracy: 0.0986\n",
      "Iter-4500 train loss: 2.2884 valid loss: 2.3479, valid accuracy: 0.0986\n",
      "Iter-4510 train loss: 2.3129 valid loss: 2.3479, valid accuracy: 0.0986\n",
      "Iter-4520 train loss: 2.2924 valid loss: 2.3477, valid accuracy: 0.0986\n",
      "Iter-4530 train loss: 2.3029 valid loss: 2.3477, valid accuracy: 0.0986\n",
      "Iter-4540 train loss: 2.3079 valid loss: 2.3475, valid accuracy: 0.0986\n",
      "Iter-4550 train loss: 2.3134 valid loss: 2.3474, valid accuracy: 0.0986\n",
      "Iter-4560 train loss: 2.3100 valid loss: 2.3473, valid accuracy: 0.0986\n",
      "Iter-4570 train loss: 2.3003 valid loss: 2.3472, valid accuracy: 0.0988\n",
      "Iter-4580 train loss: 2.3055 valid loss: 2.3472, valid accuracy: 0.0988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3073 valid loss: 2.3471, valid accuracy: 0.0988\n",
      "Iter-4600 train loss: 2.2906 valid loss: 2.3470, valid accuracy: 0.0988\n",
      "Iter-4610 train loss: 2.3046 valid loss: 2.3470, valid accuracy: 0.0988\n",
      "Iter-4620 train loss: 2.3058 valid loss: 2.3469, valid accuracy: 0.0988\n",
      "Iter-4630 train loss: 2.3054 valid loss: 2.3468, valid accuracy: 0.0990\n",
      "Iter-4640 train loss: 2.2965 valid loss: 2.3466, valid accuracy: 0.0990\n",
      "Iter-4650 train loss: 2.3046 valid loss: 2.3465, valid accuracy: 0.0990\n",
      "Iter-4660 train loss: 2.3118 valid loss: 2.3464, valid accuracy: 0.0992\n",
      "Iter-4670 train loss: 2.2994 valid loss: 2.3463, valid accuracy: 0.0992\n",
      "Iter-4680 train loss: 2.3059 valid loss: 2.3462, valid accuracy: 0.0992\n",
      "Iter-4690 train loss: 2.3096 valid loss: 2.3461, valid accuracy: 0.0992\n",
      "Iter-4700 train loss: 2.2927 valid loss: 2.3460, valid accuracy: 0.0992\n",
      "Iter-4710 train loss: 2.2985 valid loss: 2.3459, valid accuracy: 0.0992\n",
      "Iter-4720 train loss: 2.3171 valid loss: 2.3458, valid accuracy: 0.0992\n",
      "Iter-4730 train loss: 2.3056 valid loss: 2.3457, valid accuracy: 0.0992\n",
      "Iter-4740 train loss: 2.3077 valid loss: 2.3456, valid accuracy: 0.0992\n",
      "Iter-4750 train loss: 2.3075 valid loss: 2.3455, valid accuracy: 0.0992\n",
      "Iter-4760 train loss: 2.3043 valid loss: 2.3454, valid accuracy: 0.0990\n",
      "Iter-4770 train loss: 2.2887 valid loss: 2.3453, valid accuracy: 0.0990\n",
      "Iter-4780 train loss: 2.3032 valid loss: 2.3453, valid accuracy: 0.0992\n",
      "Iter-4790 train loss: 2.3064 valid loss: 2.3452, valid accuracy: 0.0992\n",
      "Iter-4800 train loss: 2.3082 valid loss: 2.3451, valid accuracy: 0.0992\n",
      "Iter-4810 train loss: 2.3059 valid loss: 2.3450, valid accuracy: 0.0992\n",
      "Iter-4820 train loss: 2.3156 valid loss: 2.3450, valid accuracy: 0.0992\n",
      "Iter-4830 train loss: 2.3143 valid loss: 2.3449, valid accuracy: 0.0992\n",
      "Iter-4840 train loss: 2.3215 valid loss: 2.3448, valid accuracy: 0.0992\n",
      "Iter-4850 train loss: 2.3004 valid loss: 2.3448, valid accuracy: 0.0992\n",
      "Iter-4860 train loss: 2.3172 valid loss: 2.3447, valid accuracy: 0.0992\n",
      "Iter-4870 train loss: 2.3076 valid loss: 2.3446, valid accuracy: 0.0994\n",
      "Iter-4880 train loss: 2.2917 valid loss: 2.3445, valid accuracy: 0.0996\n",
      "Iter-4890 train loss: 2.2875 valid loss: 2.3444, valid accuracy: 0.0994\n",
      "Iter-4900 train loss: 2.2887 valid loss: 2.3444, valid accuracy: 0.0996\n",
      "Iter-4910 train loss: 2.3033 valid loss: 2.3443, valid accuracy: 0.0994\n",
      "Iter-4920 train loss: 2.3112 valid loss: 2.3442, valid accuracy: 0.0994\n",
      "Iter-4930 train loss: 2.3077 valid loss: 2.3442, valid accuracy: 0.0994\n",
      "Iter-4940 train loss: 2.2960 valid loss: 2.3441, valid accuracy: 0.0992\n",
      "Iter-4950 train loss: 2.3031 valid loss: 2.3440, valid accuracy: 0.0996\n",
      "Iter-4960 train loss: 2.2827 valid loss: 2.3438, valid accuracy: 0.0998\n",
      "Iter-4970 train loss: 2.2917 valid loss: 2.3438, valid accuracy: 0.0998\n",
      "Iter-4980 train loss: 2.3064 valid loss: 2.3437, valid accuracy: 0.0998\n",
      "Iter-4990 train loss: 2.3030 valid loss: 2.3437, valid accuracy: 0.0998\n",
      "Iter-5000 train loss: 2.3075 valid loss: 2.3436, valid accuracy: 0.1000\n",
      "Iter-5010 train loss: 2.2981 valid loss: 2.3434, valid accuracy: 0.1000\n",
      "Iter-5020 train loss: 2.3028 valid loss: 2.3434, valid accuracy: 0.0998\n",
      "Iter-5030 train loss: 2.2967 valid loss: 2.3433, valid accuracy: 0.1000\n",
      "Iter-5040 train loss: 2.3135 valid loss: 2.3432, valid accuracy: 0.1000\n",
      "Iter-5050 train loss: 2.3042 valid loss: 2.3431, valid accuracy: 0.1000\n",
      "Iter-5060 train loss: 2.2979 valid loss: 2.3431, valid accuracy: 0.1000\n",
      "Iter-5070 train loss: 2.3014 valid loss: 2.3430, valid accuracy: 0.1000\n",
      "Iter-5080 train loss: 2.2717 valid loss: 2.3429, valid accuracy: 0.1000\n",
      "Iter-5090 train loss: 2.3046 valid loss: 2.3428, valid accuracy: 0.1000\n",
      "Iter-5100 train loss: 2.2844 valid loss: 2.3427, valid accuracy: 0.1000\n",
      "Iter-5110 train loss: 2.3043 valid loss: 2.3426, valid accuracy: 0.1000\n",
      "Iter-5120 train loss: 2.2972 valid loss: 2.3426, valid accuracy: 0.1000\n",
      "Iter-5130 train loss: 2.3172 valid loss: 2.3425, valid accuracy: 0.1000\n",
      "Iter-5140 train loss: 2.3136 valid loss: 2.3424, valid accuracy: 0.1000\n",
      "Iter-5150 train loss: 2.2873 valid loss: 2.3423, valid accuracy: 0.1000\n",
      "Iter-5160 train loss: 2.3060 valid loss: 2.3422, valid accuracy: 0.1000\n",
      "Iter-5170 train loss: 2.3184 valid loss: 2.3421, valid accuracy: 0.1000\n",
      "Iter-5180 train loss: 2.3081 valid loss: 2.3420, valid accuracy: 0.1000\n",
      "Iter-5190 train loss: 2.3145 valid loss: 2.3419, valid accuracy: 0.1000\n",
      "Iter-5200 train loss: 2.3215 valid loss: 2.3418, valid accuracy: 0.1000\n",
      "Iter-5210 train loss: 2.3105 valid loss: 2.3418, valid accuracy: 0.1000\n",
      "Iter-5220 train loss: 2.3065 valid loss: 2.3417, valid accuracy: 0.0998\n",
      "Iter-5230 train loss: 2.2991 valid loss: 2.3417, valid accuracy: 0.0998\n",
      "Iter-5240 train loss: 2.3117 valid loss: 2.3416, valid accuracy: 0.0998\n",
      "Iter-5250 train loss: 2.2989 valid loss: 2.3416, valid accuracy: 0.0998\n",
      "Iter-5260 train loss: 2.2989 valid loss: 2.3415, valid accuracy: 0.0998\n",
      "Iter-5270 train loss: 2.3137 valid loss: 2.3414, valid accuracy: 0.0998\n",
      "Iter-5280 train loss: 2.3080 valid loss: 2.3414, valid accuracy: 0.0998\n",
      "Iter-5290 train loss: 2.2986 valid loss: 2.3413, valid accuracy: 0.0998\n",
      "Iter-5300 train loss: 2.3082 valid loss: 2.3412, valid accuracy: 0.0998\n",
      "Iter-5310 train loss: 2.2899 valid loss: 2.3411, valid accuracy: 0.0998\n",
      "Iter-5320 train loss: 2.3065 valid loss: 2.3411, valid accuracy: 0.1000\n",
      "Iter-5330 train loss: 2.2995 valid loss: 2.3410, valid accuracy: 0.1000\n",
      "Iter-5340 train loss: 2.3208 valid loss: 2.3410, valid accuracy: 0.1002\n",
      "Iter-5350 train loss: 2.3031 valid loss: 2.3408, valid accuracy: 0.1004\n",
      "Iter-5360 train loss: 2.3062 valid loss: 2.3408, valid accuracy: 0.1006\n",
      "Iter-5370 train loss: 2.3023 valid loss: 2.3407, valid accuracy: 0.1006\n",
      "Iter-5380 train loss: 2.2965 valid loss: 2.3406, valid accuracy: 0.1004\n",
      "Iter-5390 train loss: 2.2637 valid loss: 2.3405, valid accuracy: 0.1004\n",
      "Iter-5400 train loss: 2.3046 valid loss: 2.3405, valid accuracy: 0.1006\n",
      "Iter-5410 train loss: 2.3035 valid loss: 2.3404, valid accuracy: 0.1006\n",
      "Iter-5420 train loss: 2.2998 valid loss: 2.3404, valid accuracy: 0.1006\n",
      "Iter-5430 train loss: 2.3121 valid loss: 2.3403, valid accuracy: 0.1008\n",
      "Iter-5440 train loss: 2.3177 valid loss: 2.3402, valid accuracy: 0.1008\n",
      "Iter-5450 train loss: 2.3055 valid loss: 2.3401, valid accuracy: 0.1008\n",
      "Iter-5460 train loss: 2.3122 valid loss: 2.3401, valid accuracy: 0.1008\n",
      "Iter-5470 train loss: 2.3178 valid loss: 2.3400, valid accuracy: 0.1008\n",
      "Iter-5480 train loss: 2.2831 valid loss: 2.3400, valid accuracy: 0.1008\n",
      "Iter-5490 train loss: 2.3142 valid loss: 2.3399, valid accuracy: 0.1008\n",
      "Iter-5500 train loss: 2.3051 valid loss: 2.3398, valid accuracy: 0.1008\n",
      "Iter-5510 train loss: 2.2851 valid loss: 2.3397, valid accuracy: 0.1008\n",
      "Iter-5520 train loss: 2.3054 valid loss: 2.3396, valid accuracy: 0.1008\n",
      "Iter-5530 train loss: 2.2950 valid loss: 2.3395, valid accuracy: 0.1010\n",
      "Iter-5540 train loss: 2.2952 valid loss: 2.3394, valid accuracy: 0.1010\n",
      "Iter-5550 train loss: 2.3072 valid loss: 2.3393, valid accuracy: 0.1010\n",
      "Iter-5560 train loss: 2.3070 valid loss: 2.3392, valid accuracy: 0.1010\n",
      "Iter-5570 train loss: 2.3103 valid loss: 2.3391, valid accuracy: 0.1012\n",
      "Iter-5580 train loss: 2.2944 valid loss: 2.3391, valid accuracy: 0.1012\n",
      "Iter-5590 train loss: 2.2989 valid loss: 2.3390, valid accuracy: 0.1012\n",
      "Iter-5600 train loss: 2.3067 valid loss: 2.3389, valid accuracy: 0.1012\n",
      "Iter-5610 train loss: 2.2940 valid loss: 2.3388, valid accuracy: 0.1012\n",
      "Iter-5620 train loss: 2.3127 valid loss: 2.3387, valid accuracy: 0.1012\n",
      "Iter-5630 train loss: 2.2884 valid loss: 2.3386, valid accuracy: 0.1012\n",
      "Iter-5640 train loss: 2.3077 valid loss: 2.3385, valid accuracy: 0.1012\n",
      "Iter-5650 train loss: 2.3053 valid loss: 2.3385, valid accuracy: 0.1012\n",
      "Iter-5660 train loss: 2.3301 valid loss: 2.3384, valid accuracy: 0.1014\n",
      "Iter-5670 train loss: 2.2920 valid loss: 2.3383, valid accuracy: 0.1014\n",
      "Iter-5680 train loss: 2.2981 valid loss: 2.3383, valid accuracy: 0.1016\n",
      "Iter-5690 train loss: 2.3292 valid loss: 2.3382, valid accuracy: 0.1016\n",
      "Iter-5700 train loss: 2.3120 valid loss: 2.3381, valid accuracy: 0.1018\n",
      "Iter-5710 train loss: 2.3173 valid loss: 2.3380, valid accuracy: 0.1018\n",
      "Iter-5720 train loss: 2.2982 valid loss: 2.3379, valid accuracy: 0.1018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.3215 valid loss: 2.3379, valid accuracy: 0.1016\n",
      "Iter-5740 train loss: 2.2977 valid loss: 2.3377, valid accuracy: 0.1018\n",
      "Iter-5750 train loss: 2.3068 valid loss: 2.3376, valid accuracy: 0.1016\n",
      "Iter-5760 train loss: 2.3119 valid loss: 2.3376, valid accuracy: 0.1016\n",
      "Iter-5770 train loss: 2.3008 valid loss: 2.3375, valid accuracy: 0.1016\n",
      "Iter-5780 train loss: 2.3106 valid loss: 2.3375, valid accuracy: 0.1016\n",
      "Iter-5790 train loss: 2.2946 valid loss: 2.3374, valid accuracy: 0.1016\n",
      "Iter-5800 train loss: 2.3132 valid loss: 2.3373, valid accuracy: 0.1016\n",
      "Iter-5810 train loss: 2.3082 valid loss: 2.3373, valid accuracy: 0.1016\n",
      "Iter-5820 train loss: 2.3032 valid loss: 2.3372, valid accuracy: 0.1016\n",
      "Iter-5830 train loss: 2.3213 valid loss: 2.3372, valid accuracy: 0.1018\n",
      "Iter-5840 train loss: 2.3097 valid loss: 2.3371, valid accuracy: 0.1018\n",
      "Iter-5850 train loss: 2.2995 valid loss: 2.3370, valid accuracy: 0.1018\n",
      "Iter-5860 train loss: 2.3113 valid loss: 2.3369, valid accuracy: 0.1016\n",
      "Iter-5870 train loss: 2.3058 valid loss: 2.3368, valid accuracy: 0.1016\n",
      "Iter-5880 train loss: 2.3010 valid loss: 2.3367, valid accuracy: 0.1020\n",
      "Iter-5890 train loss: 2.2920 valid loss: 2.3366, valid accuracy: 0.1020\n",
      "Iter-5900 train loss: 2.3019 valid loss: 2.3366, valid accuracy: 0.1020\n",
      "Iter-5910 train loss: 2.3019 valid loss: 2.3365, valid accuracy: 0.1022\n",
      "Iter-5920 train loss: 2.3001 valid loss: 2.3365, valid accuracy: 0.1022\n",
      "Iter-5930 train loss: 2.3111 valid loss: 2.3364, valid accuracy: 0.1022\n",
      "Iter-5940 train loss: 2.3175 valid loss: 2.3363, valid accuracy: 0.1022\n",
      "Iter-5950 train loss: 2.3087 valid loss: 2.3362, valid accuracy: 0.1022\n",
      "Iter-5960 train loss: 2.3032 valid loss: 2.3361, valid accuracy: 0.1022\n",
      "Iter-5970 train loss: 2.3209 valid loss: 2.3360, valid accuracy: 0.1022\n",
      "Iter-5980 train loss: 2.3007 valid loss: 2.3359, valid accuracy: 0.1022\n",
      "Iter-5990 train loss: 2.3100 valid loss: 2.3359, valid accuracy: 0.1022\n",
      "Iter-6000 train loss: 2.3093 valid loss: 2.3358, valid accuracy: 0.1022\n",
      "Iter-6010 train loss: 2.3108 valid loss: 2.3357, valid accuracy: 0.1022\n",
      "Iter-6020 train loss: 2.3144 valid loss: 2.3356, valid accuracy: 0.1022\n",
      "Iter-6030 train loss: 2.3053 valid loss: 2.3355, valid accuracy: 0.1022\n",
      "Iter-6040 train loss: 2.3034 valid loss: 2.3354, valid accuracy: 0.1022\n",
      "Iter-6050 train loss: 2.3032 valid loss: 2.3354, valid accuracy: 0.1022\n",
      "Iter-6060 train loss: 2.3045 valid loss: 2.3353, valid accuracy: 0.1022\n",
      "Iter-6070 train loss: 2.3110 valid loss: 2.3352, valid accuracy: 0.1022\n",
      "Iter-6080 train loss: 2.3095 valid loss: 2.3352, valid accuracy: 0.1022\n",
      "Iter-6090 train loss: 2.3262 valid loss: 2.3351, valid accuracy: 0.1022\n",
      "Iter-6100 train loss: 2.3108 valid loss: 2.3350, valid accuracy: 0.1020\n",
      "Iter-6110 train loss: 2.3128 valid loss: 2.3349, valid accuracy: 0.1020\n",
      "Iter-6120 train loss: 2.3135 valid loss: 2.3348, valid accuracy: 0.1022\n",
      "Iter-6130 train loss: 2.3086 valid loss: 2.3347, valid accuracy: 0.1020\n",
      "Iter-6140 train loss: 2.3112 valid loss: 2.3347, valid accuracy: 0.1020\n",
      "Iter-6150 train loss: 2.2985 valid loss: 2.3346, valid accuracy: 0.1020\n",
      "Iter-6160 train loss: 2.3038 valid loss: 2.3346, valid accuracy: 0.1022\n",
      "Iter-6170 train loss: 2.3073 valid loss: 2.3345, valid accuracy: 0.1020\n",
      "Iter-6180 train loss: 2.3021 valid loss: 2.3344, valid accuracy: 0.1022\n",
      "Iter-6190 train loss: 2.3048 valid loss: 2.3343, valid accuracy: 0.1022\n",
      "Iter-6200 train loss: 2.3114 valid loss: 2.3342, valid accuracy: 0.1022\n",
      "Iter-6210 train loss: 2.3126 valid loss: 2.3341, valid accuracy: 0.1022\n",
      "Iter-6220 train loss: 2.2930 valid loss: 2.3340, valid accuracy: 0.1022\n",
      "Iter-6230 train loss: 2.3210 valid loss: 2.3340, valid accuracy: 0.1022\n",
      "Iter-6240 train loss: 2.3198 valid loss: 2.3338, valid accuracy: 0.1022\n",
      "Iter-6250 train loss: 2.2955 valid loss: 2.3338, valid accuracy: 0.1022\n",
      "Iter-6260 train loss: 2.3150 valid loss: 2.3336, valid accuracy: 0.1022\n",
      "Iter-6270 train loss: 2.3080 valid loss: 2.3336, valid accuracy: 0.1022\n",
      "Iter-6280 train loss: 2.3047 valid loss: 2.3335, valid accuracy: 0.1022\n",
      "Iter-6290 train loss: 2.3060 valid loss: 2.3334, valid accuracy: 0.1022\n",
      "Iter-6300 train loss: 2.3030 valid loss: 2.3333, valid accuracy: 0.1022\n",
      "Iter-6310 train loss: 2.3057 valid loss: 2.3332, valid accuracy: 0.1024\n",
      "Iter-6320 train loss: 2.2841 valid loss: 2.3331, valid accuracy: 0.1024\n",
      "Iter-6330 train loss: 2.3002 valid loss: 2.3330, valid accuracy: 0.1024\n",
      "Iter-6340 train loss: 2.3106 valid loss: 2.3330, valid accuracy: 0.1024\n",
      "Iter-6350 train loss: 2.3020 valid loss: 2.3329, valid accuracy: 0.1024\n",
      "Iter-6360 train loss: 2.2929 valid loss: 2.3329, valid accuracy: 0.1024\n",
      "Iter-6370 train loss: 2.3114 valid loss: 2.3328, valid accuracy: 0.1024\n",
      "Iter-6380 train loss: 2.3060 valid loss: 2.3327, valid accuracy: 0.1024\n",
      "Iter-6390 train loss: 2.3041 valid loss: 2.3326, valid accuracy: 0.1026\n",
      "Iter-6400 train loss: 2.3143 valid loss: 2.3326, valid accuracy: 0.1024\n",
      "Iter-6410 train loss: 2.2989 valid loss: 2.3325, valid accuracy: 0.1024\n",
      "Iter-6420 train loss: 2.3142 valid loss: 2.3325, valid accuracy: 0.1024\n",
      "Iter-6430 train loss: 2.2833 valid loss: 2.3324, valid accuracy: 0.1024\n",
      "Iter-6440 train loss: 2.3274 valid loss: 2.3323, valid accuracy: 0.1024\n",
      "Iter-6450 train loss: 2.3049 valid loss: 2.3322, valid accuracy: 0.1024\n",
      "Iter-6460 train loss: 2.3011 valid loss: 2.3321, valid accuracy: 0.1024\n",
      "Iter-6470 train loss: 2.3250 valid loss: 2.3320, valid accuracy: 0.1024\n",
      "Iter-6480 train loss: 2.3024 valid loss: 2.3319, valid accuracy: 0.1024\n",
      "Iter-6490 train loss: 2.3070 valid loss: 2.3318, valid accuracy: 0.1024\n",
      "Iter-6500 train loss: 2.2996 valid loss: 2.3317, valid accuracy: 0.1024\n",
      "Iter-6510 train loss: 2.2972 valid loss: 2.3317, valid accuracy: 0.1026\n",
      "Iter-6520 train loss: 2.3122 valid loss: 2.3316, valid accuracy: 0.1026\n",
      "Iter-6530 train loss: 2.3185 valid loss: 2.3315, valid accuracy: 0.1026\n",
      "Iter-6540 train loss: 2.3135 valid loss: 2.3314, valid accuracy: 0.1026\n",
      "Iter-6550 train loss: 2.2953 valid loss: 2.3313, valid accuracy: 0.1026\n",
      "Iter-6560 train loss: 2.3043 valid loss: 2.3312, valid accuracy: 0.1026\n",
      "Iter-6570 train loss: 2.2881 valid loss: 2.3312, valid accuracy: 0.1026\n",
      "Iter-6580 train loss: 2.3166 valid loss: 2.3310, valid accuracy: 0.1026\n",
      "Iter-6590 train loss: 2.3066 valid loss: 2.3309, valid accuracy: 0.1026\n",
      "Iter-6600 train loss: 2.2947 valid loss: 2.3309, valid accuracy: 0.1026\n",
      "Iter-6610 train loss: 2.3042 valid loss: 2.3308, valid accuracy: 0.1026\n",
      "Iter-6620 train loss: 2.3024 valid loss: 2.3307, valid accuracy: 0.1024\n",
      "Iter-6630 train loss: 2.3012 valid loss: 2.3306, valid accuracy: 0.1024\n",
      "Iter-6640 train loss: 2.3045 valid loss: 2.3305, valid accuracy: 0.1024\n",
      "Iter-6650 train loss: 2.2862 valid loss: 2.3305, valid accuracy: 0.1024\n",
      "Iter-6660 train loss: 2.2988 valid loss: 2.3304, valid accuracy: 0.1024\n",
      "Iter-6670 train loss: 2.2908 valid loss: 2.3303, valid accuracy: 0.1028\n",
      "Iter-6680 train loss: 2.3061 valid loss: 2.3303, valid accuracy: 0.1028\n",
      "Iter-6690 train loss: 2.3064 valid loss: 2.3302, valid accuracy: 0.1028\n",
      "Iter-6700 train loss: 2.3182 valid loss: 2.3302, valid accuracy: 0.1028\n",
      "Iter-6710 train loss: 2.3066 valid loss: 2.3301, valid accuracy: 0.1028\n",
      "Iter-6720 train loss: 2.3070 valid loss: 2.3300, valid accuracy: 0.1028\n",
      "Iter-6730 train loss: 2.3104 valid loss: 2.3300, valid accuracy: 0.1028\n",
      "Iter-6740 train loss: 2.2942 valid loss: 2.3299, valid accuracy: 0.1028\n",
      "Iter-6750 train loss: 2.3143 valid loss: 2.3298, valid accuracy: 0.1028\n",
      "Iter-6760 train loss: 2.3110 valid loss: 2.3297, valid accuracy: 0.1028\n",
      "Iter-6770 train loss: 2.3033 valid loss: 2.3297, valid accuracy: 0.1028\n",
      "Iter-6780 train loss: 2.2844 valid loss: 2.3296, valid accuracy: 0.1028\n",
      "Iter-6790 train loss: 2.3014 valid loss: 2.3295, valid accuracy: 0.1028\n",
      "Iter-6800 train loss: 2.3165 valid loss: 2.3294, valid accuracy: 0.1030\n",
      "Iter-6810 train loss: 2.3079 valid loss: 2.3293, valid accuracy: 0.1030\n",
      "Iter-6820 train loss: 2.3150 valid loss: 2.3293, valid accuracy: 0.1030\n",
      "Iter-6830 train loss: 2.2967 valid loss: 2.3293, valid accuracy: 0.1032\n",
      "Iter-6840 train loss: 2.2967 valid loss: 2.3292, valid accuracy: 0.1032\n",
      "Iter-6850 train loss: 2.2997 valid loss: 2.3292, valid accuracy: 0.1032\n",
      "Iter-6860 train loss: 2.3061 valid loss: 2.3291, valid accuracy: 0.1032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.3059 valid loss: 2.3290, valid accuracy: 0.1032\n",
      "Iter-6880 train loss: 2.3106 valid loss: 2.3290, valid accuracy: 0.1032\n",
      "Iter-6890 train loss: 2.2996 valid loss: 2.3289, valid accuracy: 0.1032\n",
      "Iter-6900 train loss: 2.3030 valid loss: 2.3288, valid accuracy: 0.1032\n",
      "Iter-6910 train loss: 2.3125 valid loss: 2.3287, valid accuracy: 0.1034\n",
      "Iter-6920 train loss: 2.2973 valid loss: 2.3286, valid accuracy: 0.1034\n",
      "Iter-6930 train loss: 2.3098 valid loss: 2.3285, valid accuracy: 0.1034\n",
      "Iter-6940 train loss: 2.2918 valid loss: 2.3285, valid accuracy: 0.1036\n",
      "Iter-6950 train loss: 2.3092 valid loss: 2.3284, valid accuracy: 0.1038\n",
      "Iter-6960 train loss: 2.3005 valid loss: 2.3283, valid accuracy: 0.1038\n",
      "Iter-6970 train loss: 2.3099 valid loss: 2.3282, valid accuracy: 0.1036\n",
      "Iter-6980 train loss: 2.2997 valid loss: 2.3282, valid accuracy: 0.1036\n",
      "Iter-6990 train loss: 2.3069 valid loss: 2.3280, valid accuracy: 0.1038\n",
      "Iter-7000 train loss: 2.3192 valid loss: 2.3280, valid accuracy: 0.1036\n",
      "Iter-7010 train loss: 2.3003 valid loss: 2.3279, valid accuracy: 0.1036\n",
      "Iter-7020 train loss: 2.3192 valid loss: 2.3278, valid accuracy: 0.1036\n",
      "Iter-7030 train loss: 2.3084 valid loss: 2.3277, valid accuracy: 0.1036\n",
      "Iter-7040 train loss: 2.3148 valid loss: 2.3276, valid accuracy: 0.1036\n",
      "Iter-7050 train loss: 2.3154 valid loss: 2.3276, valid accuracy: 0.1036\n",
      "Iter-7060 train loss: 2.2931 valid loss: 2.3275, valid accuracy: 0.1038\n",
      "Iter-7070 train loss: 2.3055 valid loss: 2.3274, valid accuracy: 0.1038\n",
      "Iter-7080 train loss: 2.3045 valid loss: 2.3273, valid accuracy: 0.1040\n",
      "Iter-7090 train loss: 2.3029 valid loss: 2.3272, valid accuracy: 0.1038\n",
      "Iter-7100 train loss: 2.3015 valid loss: 2.3271, valid accuracy: 0.1036\n",
      "Iter-7110 train loss: 2.3008 valid loss: 2.3271, valid accuracy: 0.1036\n",
      "Iter-7120 train loss: 2.3011 valid loss: 2.3270, valid accuracy: 0.1036\n",
      "Iter-7130 train loss: 2.2936 valid loss: 2.3269, valid accuracy: 0.1040\n",
      "Iter-7140 train loss: 2.3170 valid loss: 2.3268, valid accuracy: 0.1040\n",
      "Iter-7150 train loss: 2.3124 valid loss: 2.3268, valid accuracy: 0.1040\n",
      "Iter-7160 train loss: 2.3027 valid loss: 2.3267, valid accuracy: 0.1040\n",
      "Iter-7170 train loss: 2.2952 valid loss: 2.3265, valid accuracy: 0.1040\n",
      "Iter-7180 train loss: 2.3155 valid loss: 2.3264, valid accuracy: 0.1040\n",
      "Iter-7190 train loss: 2.2971 valid loss: 2.3264, valid accuracy: 0.1040\n",
      "Iter-7200 train loss: 2.3056 valid loss: 2.3263, valid accuracy: 0.1040\n",
      "Iter-7210 train loss: 2.3127 valid loss: 2.3262, valid accuracy: 0.1040\n",
      "Iter-7220 train loss: 2.3078 valid loss: 2.3261, valid accuracy: 0.1042\n",
      "Iter-7230 train loss: 2.2902 valid loss: 2.3261, valid accuracy: 0.1044\n",
      "Iter-7240 train loss: 2.3112 valid loss: 2.3260, valid accuracy: 0.1046\n",
      "Iter-7250 train loss: 2.3032 valid loss: 2.3259, valid accuracy: 0.1046\n",
      "Iter-7260 train loss: 2.3161 valid loss: 2.3258, valid accuracy: 0.1046\n",
      "Iter-7270 train loss: 2.3257 valid loss: 2.3257, valid accuracy: 0.1046\n",
      "Iter-7280 train loss: 2.3050 valid loss: 2.3257, valid accuracy: 0.1046\n",
      "Iter-7290 train loss: 2.2987 valid loss: 2.3256, valid accuracy: 0.1044\n",
      "Iter-7300 train loss: 2.3133 valid loss: 2.3255, valid accuracy: 0.1046\n",
      "Iter-7310 train loss: 2.3038 valid loss: 2.3254, valid accuracy: 0.1046\n",
      "Iter-7320 train loss: 2.3260 valid loss: 2.3253, valid accuracy: 0.1046\n",
      "Iter-7330 train loss: 2.3052 valid loss: 2.3252, valid accuracy: 0.1046\n",
      "Iter-7340 train loss: 2.3193 valid loss: 2.3251, valid accuracy: 0.1046\n",
      "Iter-7350 train loss: 2.2992 valid loss: 2.3251, valid accuracy: 0.1046\n",
      "Iter-7360 train loss: 2.3055 valid loss: 2.3250, valid accuracy: 0.1046\n",
      "Iter-7370 train loss: 2.2974 valid loss: 2.3249, valid accuracy: 0.1046\n",
      "Iter-7380 train loss: 2.2873 valid loss: 2.3249, valid accuracy: 0.1046\n",
      "Iter-7390 train loss: 2.3056 valid loss: 2.3248, valid accuracy: 0.1046\n",
      "Iter-7400 train loss: 2.3229 valid loss: 2.3248, valid accuracy: 0.1046\n",
      "Iter-7410 train loss: 2.3244 valid loss: 2.3247, valid accuracy: 0.1046\n",
      "Iter-7420 train loss: 2.2980 valid loss: 2.3246, valid accuracy: 0.1046\n",
      "Iter-7430 train loss: 2.3095 valid loss: 2.3245, valid accuracy: 0.1046\n",
      "Iter-7440 train loss: 2.3258 valid loss: 2.3245, valid accuracy: 0.1048\n",
      "Iter-7450 train loss: 2.3123 valid loss: 2.3244, valid accuracy: 0.1050\n",
      "Iter-7460 train loss: 2.2972 valid loss: 2.3243, valid accuracy: 0.1050\n",
      "Iter-7470 train loss: 2.3051 valid loss: 2.3243, valid accuracy: 0.1052\n",
      "Iter-7480 train loss: 2.3042 valid loss: 2.3242, valid accuracy: 0.1052\n",
      "Iter-7490 train loss: 2.2947 valid loss: 2.3241, valid accuracy: 0.1054\n",
      "Iter-7500 train loss: 2.3118 valid loss: 2.3241, valid accuracy: 0.1052\n",
      "Iter-7510 train loss: 2.3223 valid loss: 2.3240, valid accuracy: 0.1052\n",
      "Iter-7520 train loss: 2.2719 valid loss: 2.3239, valid accuracy: 0.1054\n",
      "Iter-7530 train loss: 2.2872 valid loss: 2.3238, valid accuracy: 0.1054\n",
      "Iter-7540 train loss: 2.3067 valid loss: 2.3238, valid accuracy: 0.1054\n",
      "Iter-7550 train loss: 2.3077 valid loss: 2.3236, valid accuracy: 0.1054\n",
      "Iter-7560 train loss: 2.3096 valid loss: 2.3236, valid accuracy: 0.1054\n",
      "Iter-7570 train loss: 2.3169 valid loss: 2.3235, valid accuracy: 0.1054\n",
      "Iter-7580 train loss: 2.2957 valid loss: 2.3234, valid accuracy: 0.1054\n",
      "Iter-7590 train loss: 2.2981 valid loss: 2.3233, valid accuracy: 0.1054\n",
      "Iter-7600 train loss: 2.3139 valid loss: 2.3233, valid accuracy: 0.1054\n",
      "Iter-7610 train loss: 2.2794 valid loss: 2.3233, valid accuracy: 0.1054\n",
      "Iter-7620 train loss: 2.3125 valid loss: 2.3232, valid accuracy: 0.1054\n",
      "Iter-7630 train loss: 2.2912 valid loss: 2.3231, valid accuracy: 0.1054\n",
      "Iter-7640 train loss: 2.3028 valid loss: 2.3230, valid accuracy: 0.1052\n",
      "Iter-7650 train loss: 2.3076 valid loss: 2.3229, valid accuracy: 0.1054\n",
      "Iter-7660 train loss: 2.3057 valid loss: 2.3229, valid accuracy: 0.1054\n",
      "Iter-7670 train loss: 2.3055 valid loss: 2.3228, valid accuracy: 0.1050\n",
      "Iter-7680 train loss: 2.2952 valid loss: 2.3227, valid accuracy: 0.1050\n",
      "Iter-7690 train loss: 2.3103 valid loss: 2.3227, valid accuracy: 0.1050\n",
      "Iter-7700 train loss: 2.2897 valid loss: 2.3226, valid accuracy: 0.1050\n",
      "Iter-7710 train loss: 2.3174 valid loss: 2.3226, valid accuracy: 0.1052\n",
      "Iter-7720 train loss: 2.2859 valid loss: 2.3225, valid accuracy: 0.1052\n",
      "Iter-7730 train loss: 2.3010 valid loss: 2.3225, valid accuracy: 0.1052\n",
      "Iter-7740 train loss: 2.3074 valid loss: 2.3224, valid accuracy: 0.1052\n",
      "Iter-7750 train loss: 2.3015 valid loss: 2.3223, valid accuracy: 0.1052\n",
      "Iter-7760 train loss: 2.2941 valid loss: 2.3223, valid accuracy: 0.1054\n",
      "Iter-7770 train loss: 2.2975 valid loss: 2.3222, valid accuracy: 0.1054\n",
      "Iter-7780 train loss: 2.3005 valid loss: 2.3221, valid accuracy: 0.1056\n",
      "Iter-7790 train loss: 2.2990 valid loss: 2.3220, valid accuracy: 0.1056\n",
      "Iter-7800 train loss: 2.3004 valid loss: 2.3220, valid accuracy: 0.1056\n",
      "Iter-7810 train loss: 2.2990 valid loss: 2.3219, valid accuracy: 0.1056\n",
      "Iter-7820 train loss: 2.2994 valid loss: 2.3219, valid accuracy: 0.1054\n",
      "Iter-7830 train loss: 2.3082 valid loss: 2.3218, valid accuracy: 0.1056\n",
      "Iter-7840 train loss: 2.3000 valid loss: 2.3217, valid accuracy: 0.1056\n",
      "Iter-7850 train loss: 2.3025 valid loss: 2.3215, valid accuracy: 0.1056\n",
      "Iter-7860 train loss: 2.3255 valid loss: 2.3214, valid accuracy: 0.1058\n",
      "Iter-7870 train loss: 2.2843 valid loss: 2.3213, valid accuracy: 0.1056\n",
      "Iter-7880 train loss: 2.3130 valid loss: 2.3213, valid accuracy: 0.1056\n",
      "Iter-7890 train loss: 2.3149 valid loss: 2.3212, valid accuracy: 0.1056\n",
      "Iter-7900 train loss: 2.2919 valid loss: 2.3211, valid accuracy: 0.1058\n",
      "Iter-7910 train loss: 2.3133 valid loss: 2.3210, valid accuracy: 0.1060\n",
      "Iter-7920 train loss: 2.3086 valid loss: 2.3210, valid accuracy: 0.1060\n",
      "Iter-7930 train loss: 2.3255 valid loss: 2.3209, valid accuracy: 0.1064\n",
      "Iter-7940 train loss: 2.2906 valid loss: 2.3208, valid accuracy: 0.1064\n",
      "Iter-7950 train loss: 2.3115 valid loss: 2.3208, valid accuracy: 0.1064\n",
      "Iter-7960 train loss: 2.2953 valid loss: 2.3207, valid accuracy: 0.1064\n",
      "Iter-7970 train loss: 2.3167 valid loss: 2.3207, valid accuracy: 0.1064\n",
      "Iter-7980 train loss: 2.3276 valid loss: 2.3206, valid accuracy: 0.1064\n",
      "Iter-7990 train loss: 2.2938 valid loss: 2.3205, valid accuracy: 0.1062\n",
      "Iter-8000 train loss: 2.3006 valid loss: 2.3204, valid accuracy: 0.1062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.3148 valid loss: 2.3204, valid accuracy: 0.1062\n",
      "Iter-8020 train loss: 2.3087 valid loss: 2.3203, valid accuracy: 0.1064\n",
      "Iter-8030 train loss: 2.3074 valid loss: 2.3202, valid accuracy: 0.1066\n",
      "Iter-8040 train loss: 2.3216 valid loss: 2.3202, valid accuracy: 0.1062\n",
      "Iter-8050 train loss: 2.3132 valid loss: 2.3201, valid accuracy: 0.1062\n",
      "Iter-8060 train loss: 2.3050 valid loss: 2.3200, valid accuracy: 0.1062\n",
      "Iter-8070 train loss: 2.3067 valid loss: 2.3200, valid accuracy: 0.1062\n",
      "Iter-8080 train loss: 2.3062 valid loss: 2.3199, valid accuracy: 0.1062\n",
      "Iter-8090 train loss: 2.3035 valid loss: 2.3198, valid accuracy: 0.1062\n",
      "Iter-8100 train loss: 2.3210 valid loss: 2.3197, valid accuracy: 0.1064\n",
      "Iter-8110 train loss: 2.2950 valid loss: 2.3196, valid accuracy: 0.1064\n",
      "Iter-8120 train loss: 2.2945 valid loss: 2.3196, valid accuracy: 0.1064\n",
      "Iter-8130 train loss: 2.3186 valid loss: 2.3195, valid accuracy: 0.1064\n",
      "Iter-8140 train loss: 2.3079 valid loss: 2.3195, valid accuracy: 0.1064\n",
      "Iter-8150 train loss: 2.3036 valid loss: 2.3195, valid accuracy: 0.1064\n",
      "Iter-8160 train loss: 2.2971 valid loss: 2.3194, valid accuracy: 0.1064\n",
      "Iter-8170 train loss: 2.3145 valid loss: 2.3193, valid accuracy: 0.1066\n",
      "Iter-8180 train loss: 2.3241 valid loss: 2.3192, valid accuracy: 0.1066\n",
      "Iter-8190 train loss: 2.2947 valid loss: 2.3191, valid accuracy: 0.1066\n",
      "Iter-8200 train loss: 2.2918 valid loss: 2.3191, valid accuracy: 0.1068\n",
      "Iter-8210 train loss: 2.2861 valid loss: 2.3191, valid accuracy: 0.1068\n",
      "Iter-8220 train loss: 2.3136 valid loss: 2.3190, valid accuracy: 0.1066\n",
      "Iter-8230 train loss: 2.2874 valid loss: 2.3189, valid accuracy: 0.1068\n",
      "Iter-8240 train loss: 2.2966 valid loss: 2.3188, valid accuracy: 0.1066\n",
      "Iter-8250 train loss: 2.2995 valid loss: 2.3188, valid accuracy: 0.1070\n",
      "Iter-8260 train loss: 2.3166 valid loss: 2.3187, valid accuracy: 0.1072\n",
      "Iter-8270 train loss: 2.2967 valid loss: 2.3186, valid accuracy: 0.1070\n",
      "Iter-8280 train loss: 2.2880 valid loss: 2.3186, valid accuracy: 0.1070\n",
      "Iter-8290 train loss: 2.3008 valid loss: 2.3185, valid accuracy: 0.1070\n",
      "Iter-8300 train loss: 2.3072 valid loss: 2.3185, valid accuracy: 0.1070\n",
      "Iter-8310 train loss: 2.3009 valid loss: 2.3184, valid accuracy: 0.1070\n",
      "Iter-8320 train loss: 2.3067 valid loss: 2.3183, valid accuracy: 0.1070\n",
      "Iter-8330 train loss: 2.2991 valid loss: 2.3182, valid accuracy: 0.1070\n",
      "Iter-8340 train loss: 2.3008 valid loss: 2.3182, valid accuracy: 0.1070\n",
      "Iter-8350 train loss: 2.2933 valid loss: 2.3181, valid accuracy: 0.1072\n",
      "Iter-8360 train loss: 2.3164 valid loss: 2.3181, valid accuracy: 0.1072\n",
      "Iter-8370 train loss: 2.3024 valid loss: 2.3180, valid accuracy: 0.1074\n",
      "Iter-8380 train loss: 2.3086 valid loss: 2.3180, valid accuracy: 0.1070\n",
      "Iter-8390 train loss: 2.3114 valid loss: 2.3179, valid accuracy: 0.1070\n",
      "Iter-8400 train loss: 2.2973 valid loss: 2.3179, valid accuracy: 0.1070\n",
      "Iter-8410 train loss: 2.3129 valid loss: 2.3178, valid accuracy: 0.1072\n",
      "Iter-8420 train loss: 2.3140 valid loss: 2.3177, valid accuracy: 0.1072\n",
      "Iter-8430 train loss: 2.2995 valid loss: 2.3176, valid accuracy: 0.1070\n",
      "Iter-8440 train loss: 2.3129 valid loss: 2.3176, valid accuracy: 0.1072\n",
      "Iter-8450 train loss: 2.2953 valid loss: 2.3175, valid accuracy: 0.1072\n",
      "Iter-8460 train loss: 2.2996 valid loss: 2.3175, valid accuracy: 0.1070\n",
      "Iter-8470 train loss: 2.3050 valid loss: 2.3174, valid accuracy: 0.1070\n",
      "Iter-8480 train loss: 2.3135 valid loss: 2.3173, valid accuracy: 0.1070\n",
      "Iter-8490 train loss: 2.2922 valid loss: 2.3173, valid accuracy: 0.1070\n",
      "Iter-8500 train loss: 2.3004 valid loss: 2.3172, valid accuracy: 0.1070\n",
      "Iter-8510 train loss: 2.3144 valid loss: 2.3171, valid accuracy: 0.1070\n",
      "Iter-8520 train loss: 2.3285 valid loss: 2.3170, valid accuracy: 0.1070\n",
      "Iter-8530 train loss: 2.3059 valid loss: 2.3168, valid accuracy: 0.1070\n",
      "Iter-8540 train loss: 2.3085 valid loss: 2.3168, valid accuracy: 0.1070\n",
      "Iter-8550 train loss: 2.2978 valid loss: 2.3167, valid accuracy: 0.1074\n",
      "Iter-8560 train loss: 2.2930 valid loss: 2.3166, valid accuracy: 0.1072\n",
      "Iter-8570 train loss: 2.3044 valid loss: 2.3166, valid accuracy: 0.1072\n",
      "Iter-8580 train loss: 2.3171 valid loss: 2.3165, valid accuracy: 0.1070\n",
      "Iter-8590 train loss: 2.3128 valid loss: 2.3164, valid accuracy: 0.1074\n",
      "Iter-8600 train loss: 2.3140 valid loss: 2.3163, valid accuracy: 0.1074\n",
      "Iter-8610 train loss: 2.3014 valid loss: 2.3162, valid accuracy: 0.1074\n",
      "Iter-8620 train loss: 2.3115 valid loss: 2.3161, valid accuracy: 0.1074\n",
      "Iter-8630 train loss: 2.3127 valid loss: 2.3161, valid accuracy: 0.1074\n",
      "Iter-8640 train loss: 2.3017 valid loss: 2.3160, valid accuracy: 0.1072\n",
      "Iter-8650 train loss: 2.3158 valid loss: 2.3159, valid accuracy: 0.1072\n",
      "Iter-8660 train loss: 2.3040 valid loss: 2.3158, valid accuracy: 0.1072\n",
      "Iter-8670 train loss: 2.3058 valid loss: 2.3158, valid accuracy: 0.1074\n",
      "Iter-8680 train loss: 2.3076 valid loss: 2.3157, valid accuracy: 0.1074\n",
      "Iter-8690 train loss: 2.3044 valid loss: 2.3156, valid accuracy: 0.1074\n",
      "Iter-8700 train loss: 2.2961 valid loss: 2.3156, valid accuracy: 0.1076\n",
      "Iter-8710 train loss: 2.3112 valid loss: 2.3155, valid accuracy: 0.1076\n",
      "Iter-8720 train loss: 2.2953 valid loss: 2.3155, valid accuracy: 0.1076\n",
      "Iter-8730 train loss: 2.3071 valid loss: 2.3154, valid accuracy: 0.1074\n",
      "Iter-8740 train loss: 2.2927 valid loss: 2.3153, valid accuracy: 0.1074\n",
      "Iter-8750 train loss: 2.3019 valid loss: 2.3152, valid accuracy: 0.1074\n",
      "Iter-8760 train loss: 2.3006 valid loss: 2.3152, valid accuracy: 0.1078\n",
      "Iter-8770 train loss: 2.2991 valid loss: 2.3151, valid accuracy: 0.1076\n",
      "Iter-8780 train loss: 2.2900 valid loss: 2.3150, valid accuracy: 0.1078\n",
      "Iter-8790 train loss: 2.3019 valid loss: 2.3149, valid accuracy: 0.1080\n",
      "Iter-8800 train loss: 2.2989 valid loss: 2.3148, valid accuracy: 0.1082\n",
      "Iter-8810 train loss: 2.3030 valid loss: 2.3148, valid accuracy: 0.1080\n",
      "Iter-8820 train loss: 2.2958 valid loss: 2.3147, valid accuracy: 0.1082\n",
      "Iter-8830 train loss: 2.2845 valid loss: 2.3147, valid accuracy: 0.1082\n",
      "Iter-8840 train loss: 2.2986 valid loss: 2.3146, valid accuracy: 0.1082\n",
      "Iter-8850 train loss: 2.3119 valid loss: 2.3145, valid accuracy: 0.1082\n",
      "Iter-8860 train loss: 2.3018 valid loss: 2.3145, valid accuracy: 0.1084\n",
      "Iter-8870 train loss: 2.2984 valid loss: 2.3144, valid accuracy: 0.1086\n",
      "Iter-8880 train loss: 2.2932 valid loss: 2.3144, valid accuracy: 0.1086\n",
      "Iter-8890 train loss: 2.2970 valid loss: 2.3143, valid accuracy: 0.1086\n",
      "Iter-8900 train loss: 2.3127 valid loss: 2.3143, valid accuracy: 0.1086\n",
      "Iter-8910 train loss: 2.2945 valid loss: 2.3142, valid accuracy: 0.1088\n",
      "Iter-8920 train loss: 2.3163 valid loss: 2.3142, valid accuracy: 0.1090\n",
      "Iter-8930 train loss: 2.3197 valid loss: 2.3141, valid accuracy: 0.1090\n",
      "Iter-8940 train loss: 2.2899 valid loss: 2.3141, valid accuracy: 0.1090\n",
      "Iter-8950 train loss: 2.2941 valid loss: 2.3140, valid accuracy: 0.1088\n",
      "Iter-8960 train loss: 2.2909 valid loss: 2.3139, valid accuracy: 0.1090\n",
      "Iter-8970 train loss: 2.3006 valid loss: 2.3139, valid accuracy: 0.1090\n",
      "Iter-8980 train loss: 2.3003 valid loss: 2.3138, valid accuracy: 0.1090\n",
      "Iter-8990 train loss: 2.2968 valid loss: 2.3137, valid accuracy: 0.1092\n",
      "Iter-9000 train loss: 2.3065 valid loss: 2.3137, valid accuracy: 0.1088\n",
      "Iter-9010 train loss: 2.2960 valid loss: 2.3136, valid accuracy: 0.1092\n",
      "Iter-9020 train loss: 2.2989 valid loss: 2.3136, valid accuracy: 0.1092\n",
      "Iter-9030 train loss: 2.3213 valid loss: 2.3135, valid accuracy: 0.1094\n",
      "Iter-9040 train loss: 2.2976 valid loss: 2.3134, valid accuracy: 0.1092\n",
      "Iter-9050 train loss: 2.2860 valid loss: 2.3134, valid accuracy: 0.1092\n",
      "Iter-9060 train loss: 2.3041 valid loss: 2.3133, valid accuracy: 0.1092\n",
      "Iter-9070 train loss: 2.3034 valid loss: 2.3132, valid accuracy: 0.1092\n",
      "Iter-9080 train loss: 2.2980 valid loss: 2.3132, valid accuracy: 0.1092\n",
      "Iter-9090 train loss: 2.2993 valid loss: 2.3131, valid accuracy: 0.1092\n",
      "Iter-9100 train loss: 2.3138 valid loss: 2.3131, valid accuracy: 0.1092\n",
      "Iter-9110 train loss: 2.3101 valid loss: 2.3130, valid accuracy: 0.1092\n",
      "Iter-9120 train loss: 2.2958 valid loss: 2.3129, valid accuracy: 0.1092\n",
      "Iter-9130 train loss: 2.3076 valid loss: 2.3129, valid accuracy: 0.1092\n",
      "Iter-9140 train loss: 2.2945 valid loss: 2.3128, valid accuracy: 0.1090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.2989 valid loss: 2.3128, valid accuracy: 0.1090\n",
      "Iter-9160 train loss: 2.3124 valid loss: 2.3127, valid accuracy: 0.1090\n",
      "Iter-9170 train loss: 2.3194 valid loss: 2.3126, valid accuracy: 0.1090\n",
      "Iter-9180 train loss: 2.3017 valid loss: 2.3125, valid accuracy: 0.1092\n",
      "Iter-9190 train loss: 2.3075 valid loss: 2.3125, valid accuracy: 0.1090\n",
      "Iter-9200 train loss: 2.2974 valid loss: 2.3124, valid accuracy: 0.1088\n",
      "Iter-9210 train loss: 2.3067 valid loss: 2.3124, valid accuracy: 0.1090\n",
      "Iter-9220 train loss: 2.2956 valid loss: 2.3124, valid accuracy: 0.1092\n",
      "Iter-9230 train loss: 2.2928 valid loss: 2.3123, valid accuracy: 0.1092\n",
      "Iter-9240 train loss: 2.3008 valid loss: 2.3122, valid accuracy: 0.1092\n",
      "Iter-9250 train loss: 2.3053 valid loss: 2.3121, valid accuracy: 0.1094\n",
      "Iter-9260 train loss: 2.3134 valid loss: 2.3121, valid accuracy: 0.1092\n",
      "Iter-9270 train loss: 2.3094 valid loss: 2.3120, valid accuracy: 0.1092\n",
      "Iter-9280 train loss: 2.2769 valid loss: 2.3119, valid accuracy: 0.1092\n",
      "Iter-9290 train loss: 2.3161 valid loss: 2.3119, valid accuracy: 0.1092\n",
      "Iter-9300 train loss: 2.2999 valid loss: 2.3118, valid accuracy: 0.1092\n",
      "Iter-9310 train loss: 2.3138 valid loss: 2.3117, valid accuracy: 0.1092\n",
      "Iter-9320 train loss: 2.2975 valid loss: 2.3116, valid accuracy: 0.1094\n",
      "Iter-9330 train loss: 2.3076 valid loss: 2.3116, valid accuracy: 0.1094\n",
      "Iter-9340 train loss: 2.3076 valid loss: 2.3115, valid accuracy: 0.1092\n",
      "Iter-9350 train loss: 2.3081 valid loss: 2.3115, valid accuracy: 0.1092\n",
      "Iter-9360 train loss: 2.3032 valid loss: 2.3114, valid accuracy: 0.1094\n",
      "Iter-9370 train loss: 2.2850 valid loss: 2.3113, valid accuracy: 0.1094\n",
      "Iter-9380 train loss: 2.3037 valid loss: 2.3112, valid accuracy: 0.1096\n",
      "Iter-9390 train loss: 2.3237 valid loss: 2.3111, valid accuracy: 0.1096\n",
      "Iter-9400 train loss: 2.3066 valid loss: 2.3111, valid accuracy: 0.1096\n",
      "Iter-9410 train loss: 2.3008 valid loss: 2.3110, valid accuracy: 0.1096\n",
      "Iter-9420 train loss: 2.3180 valid loss: 2.3109, valid accuracy: 0.1096\n",
      "Iter-9430 train loss: 2.3031 valid loss: 2.3109, valid accuracy: 0.1098\n",
      "Iter-9440 train loss: 2.2967 valid loss: 2.3108, valid accuracy: 0.1098\n",
      "Iter-9450 train loss: 2.2921 valid loss: 2.3108, valid accuracy: 0.1098\n",
      "Iter-9460 train loss: 2.2918 valid loss: 2.3107, valid accuracy: 0.1098\n",
      "Iter-9470 train loss: 2.2997 valid loss: 2.3107, valid accuracy: 0.1096\n",
      "Iter-9480 train loss: 2.3125 valid loss: 2.3106, valid accuracy: 0.1096\n",
      "Iter-9490 train loss: 2.3113 valid loss: 2.3105, valid accuracy: 0.1098\n",
      "Iter-9500 train loss: 2.3048 valid loss: 2.3104, valid accuracy: 0.1098\n",
      "Iter-9510 train loss: 2.3135 valid loss: 2.3104, valid accuracy: 0.1098\n",
      "Iter-9520 train loss: 2.2813 valid loss: 2.3103, valid accuracy: 0.1100\n",
      "Iter-9530 train loss: 2.3013 valid loss: 2.3102, valid accuracy: 0.1100\n",
      "Iter-9540 train loss: 2.3099 valid loss: 2.3102, valid accuracy: 0.1100\n",
      "Iter-9550 train loss: 2.3076 valid loss: 2.3101, valid accuracy: 0.1100\n",
      "Iter-9560 train loss: 2.3064 valid loss: 2.3100, valid accuracy: 0.1100\n",
      "Iter-9570 train loss: 2.3021 valid loss: 2.3099, valid accuracy: 0.1100\n",
      "Iter-9580 train loss: 2.3132 valid loss: 2.3098, valid accuracy: 0.1102\n",
      "Iter-9590 train loss: 2.3042 valid loss: 2.3098, valid accuracy: 0.1104\n",
      "Iter-9600 train loss: 2.3203 valid loss: 2.3097, valid accuracy: 0.1104\n",
      "Iter-9610 train loss: 2.3055 valid loss: 2.3096, valid accuracy: 0.1104\n",
      "Iter-9620 train loss: 2.2891 valid loss: 2.3096, valid accuracy: 0.1106\n",
      "Iter-9630 train loss: 2.2946 valid loss: 2.3095, valid accuracy: 0.1106\n",
      "Iter-9640 train loss: 2.3190 valid loss: 2.3093, valid accuracy: 0.1108\n",
      "Iter-9650 train loss: 2.3023 valid loss: 2.3093, valid accuracy: 0.1108\n",
      "Iter-9660 train loss: 2.3146 valid loss: 2.3092, valid accuracy: 0.1108\n",
      "Iter-9670 train loss: 2.2968 valid loss: 2.3091, valid accuracy: 0.1108\n",
      "Iter-9680 train loss: 2.3111 valid loss: 2.3090, valid accuracy: 0.1108\n",
      "Iter-9690 train loss: 2.3107 valid loss: 2.3090, valid accuracy: 0.1110\n",
      "Iter-9700 train loss: 2.2991 valid loss: 2.3089, valid accuracy: 0.1108\n",
      "Iter-9710 train loss: 2.3142 valid loss: 2.3088, valid accuracy: 0.1110\n",
      "Iter-9720 train loss: 2.2787 valid loss: 2.3087, valid accuracy: 0.1108\n",
      "Iter-9730 train loss: 2.2752 valid loss: 2.3087, valid accuracy: 0.1108\n",
      "Iter-9740 train loss: 2.3021 valid loss: 2.3086, valid accuracy: 0.1110\n",
      "Iter-9750 train loss: 2.3049 valid loss: 2.3085, valid accuracy: 0.1110\n",
      "Iter-9760 train loss: 2.3101 valid loss: 2.3085, valid accuracy: 0.1110\n",
      "Iter-9770 train loss: 2.3254 valid loss: 2.3084, valid accuracy: 0.1110\n",
      "Iter-9780 train loss: 2.3018 valid loss: 2.3084, valid accuracy: 0.1110\n",
      "Iter-9790 train loss: 2.3108 valid loss: 2.3084, valid accuracy: 0.1110\n",
      "Iter-9800 train loss: 2.3095 valid loss: 2.3083, valid accuracy: 0.1110\n",
      "Iter-9810 train loss: 2.3096 valid loss: 2.3082, valid accuracy: 0.1110\n",
      "Iter-9820 train loss: 2.2929 valid loss: 2.3081, valid accuracy: 0.1112\n",
      "Iter-9830 train loss: 2.3197 valid loss: 2.3081, valid accuracy: 0.1112\n",
      "Iter-9840 train loss: 2.3050 valid loss: 2.3080, valid accuracy: 0.1112\n",
      "Iter-9850 train loss: 2.3087 valid loss: 2.3080, valid accuracy: 0.1112\n",
      "Iter-9860 train loss: 2.3056 valid loss: 2.3079, valid accuracy: 0.1112\n",
      "Iter-9870 train loss: 2.2907 valid loss: 2.3079, valid accuracy: 0.1112\n",
      "Iter-9880 train loss: 2.3027 valid loss: 2.3078, valid accuracy: 0.1114\n",
      "Iter-9890 train loss: 2.3014 valid loss: 2.3077, valid accuracy: 0.1114\n",
      "Iter-9900 train loss: 2.3082 valid loss: 2.3077, valid accuracy: 0.1112\n",
      "Iter-9910 train loss: 2.3006 valid loss: 2.3076, valid accuracy: 0.1112\n",
      "Iter-9920 train loss: 2.3078 valid loss: 2.3075, valid accuracy: 0.1112\n",
      "Iter-9930 train loss: 2.2979 valid loss: 2.3074, valid accuracy: 0.1112\n",
      "Iter-9940 train loss: 2.2997 valid loss: 2.3074, valid accuracy: 0.1112\n",
      "Iter-9950 train loss: 2.3117 valid loss: 2.3073, valid accuracy: 0.1114\n",
      "Iter-9960 train loss: 2.2961 valid loss: 2.3072, valid accuracy: 0.1114\n",
      "Iter-9970 train loss: 2.2989 valid loss: 2.3072, valid accuracy: 0.1114\n",
      "Iter-9980 train loss: 2.3127 valid loss: 2.3071, valid accuracy: 0.1112\n",
      "Iter-9990 train loss: 2.3041 valid loss: 2.3070, valid accuracy: 0.1114\n",
      "Iter-10000 train loss: 2.3030 valid loss: 2.3070, valid accuracy: 0.1114\n",
      "Last iteration - Test accuracy mean: 0.1077, std: 0.0000, loss: 2.3122\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118e8b358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118e8b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
