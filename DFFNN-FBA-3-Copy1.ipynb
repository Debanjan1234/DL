{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad = []\n",
    "        for layer in range(L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        self.grads.append(grad)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = l.sigmoid(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = l.sigmoid(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass to the previous layer\n",
    "        self.grads[2]['W'] = dW\n",
    "        self.grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            self.grads[1][layer]['W'] = dW\n",
    "            self.grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        self.grads[0]['W'] = dW\n",
    "        self.grads[0]['b'] = db\n",
    "\n",
    "        return dX\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _ = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in self.grads[0].keys():\n",
    "                self.model[0][key] -= alpha * self.grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in self.grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * self.grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in self.grads[2].keys():\n",
    "                self.model[2][key] -= alpha * self.grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3193 valid loss: 2.3310, valid accuracy: 0.1126\n",
      "Iter-200 train loss: 2.3464 valid loss: 2.3266, valid accuracy: 0.1126\n",
      "Iter-300 train loss: 2.3006 valid loss: 2.3225, valid accuracy: 0.1126\n",
      "Iter-400 train loss: 2.2766 valid loss: 2.3192, valid accuracy: 0.1126\n",
      "Iter-500 train loss: 2.3220 valid loss: 2.3162, valid accuracy: 0.1126\n",
      "Iter-600 train loss: 2.3347 valid loss: 2.3137, valid accuracy: 0.1126\n",
      "Iter-700 train loss: 2.3074 valid loss: 2.3117, valid accuracy: 0.1126\n",
      "Iter-800 train loss: 2.3155 valid loss: 2.3099, valid accuracy: 0.1126\n",
      "Iter-900 train loss: 2.2991 valid loss: 2.3085, valid accuracy: 0.1126\n",
      "Iter-1000 train loss: 2.2931 valid loss: 2.3074, valid accuracy: 0.1126\n",
      "Iter-1100 train loss: 2.3172 valid loss: 2.3066, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.3048 valid loss: 2.3058, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.3028 valid loss: 2.3052, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3087 valid loss: 2.3046, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.2928 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3057 valid loss: 2.3039, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.2966 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.2913 valid loss: 2.3034, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3085 valid loss: 2.3031, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3047 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3099 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3147 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3084 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.2966 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.2985 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.2984 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.2964 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.2934 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3069 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.2931 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.3093 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3002 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.2926 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.2889 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3182 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3088 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.3055 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.3101 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.3072 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.2993 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.2987 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.3109 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.3061 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.2996 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.2981 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.3033 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.3192 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3070 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3033 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.3017 valid loss: 2.3028, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.2968 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.2948 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.3059 valid loss: 2.3028, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.2970 valid loss: 2.3028, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3052 valid loss: 2.3028, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.2970 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3138 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.3055 valid loss: 2.3029, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.2968 valid loss: 2.3030, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.3010 valid loss: 2.3031, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.2946 valid loss: 2.3032, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.3027 valid loss: 2.3032, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.2940 valid loss: 2.3033, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.3002 valid loss: 2.3034, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3063 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.3073 valid loss: 2.3035, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.2985 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.3106 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.3050 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.3074 valid loss: 2.3036, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.2989 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3131 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.3072 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.2860 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3145 valid loss: 2.3037, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.3124 valid loss: 2.3039, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.2970 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.2948 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.3115 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.2992 valid loss: 2.3039, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.3047 valid loss: 2.3039, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.2959 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.2963 valid loss: 2.3040, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.3056 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.3038 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.3048 valid loss: 2.3041, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.2924 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2876 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.2963 valid loss: 2.3042, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.2933 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.3000 valid loss: 2.3043, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.3025 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3170 valid loss: 2.3044, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.3138 valid loss: 2.3045, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3042\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKaVUFA0OSimlomhwUEopFeX/AVfi+IyKOy0/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1218336a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3Kw4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1137b71d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
