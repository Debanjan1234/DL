{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m, train, with_res):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        # residual connection between the output and the input\n",
    "        # x = x + f(x), f(x) = y, x: input, and y: output\n",
    "        if with_res:\n",
    "            y += X_one_hot # y_1xh and x_1xh\n",
    "        \n",
    "        if train: \n",
    "            #             y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            y, y_nl_cache = self.selu_forward(y)\n",
    "            y, y_do_cache = self.alpha_dropout_fwd(y, q=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, y_nl_cache, y_do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train, with_res):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_nl_cache, y_do_cache = cache\n",
    "            #             dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "            dy = self.alpha_dropout_bwd(dy, cache=y_do_cache)\n",
    "            dy = self.selu_backward(dy, y_nl_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = (hh * dh) - (h_old * dh)\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:] + dX_prime[:, self.H:]\n",
    "\n",
    "        # residual connection between the output and the input\n",
    "        # x = x + f(x), f(x) = y, x: input, and y: output\n",
    "        if with_res:\n",
    "            dX += dy\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        # self.L is the number of layers in total\n",
    "        #         layer = 0 # self.L = 1\n",
    "        for layer in range(self.L):\n",
    "#             if layer == 0 and layer == self.L-1: # Input-Output layers\n",
    "            if layer == 0: # Input-Output layers\n",
    "                for X in X_train:\n",
    "                    X_one_hot = np.zeros(self.D)\n",
    "                    X_one_hot[X] = 1.\n",
    "                    X = X_one_hot.reshape(1, -1)\n",
    "                    y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True, with_res=False)\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "            else:\n",
    "                X_train = ys.copy()\n",
    "                ys = []\n",
    "                for X in X_train:\n",
    "                    y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True, with_res=True)\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            if layer < (self.L - 1): dys = dXs.copy()\n",
    "            dXs = []\n",
    "#             if layer == 0 and layer == self.L-1: # Input-Output layer\n",
    "            if layer == 0: # Input-Output layer\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dX = dys[t]\n",
    "                    dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True, with_res=False)\n",
    "                    for key in grad[layer].keys():\n",
    "                        grads[layer][key] += grad[layer][key]\n",
    "                    dXs.append(dX)\n",
    "            else:  # Hidden layers\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dX = dys[t]\n",
    "                    dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True, with_res=True)\n",
    "                    for key in grad[layer].keys():\n",
    "                        grads[layer][key] += grad[layer][key]\n",
    "                    dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    #     def test(self, X_seed, h, size):\n",
    "    #         chars = [self.idx2char[X_seed]]\n",
    "    #         idx_list = list(range(self.vocab_size))\n",
    "    #         X = X_seed\n",
    "\n",
    "    #         h_init = h.copy()\n",
    "    #         h = []\n",
    "    #         for _ in range(self.L):\n",
    "    #             h.append(h_init.copy())\n",
    "\n",
    "    #         # Test is different than train since y[t+1] is related to y[t] \n",
    "    #         for _ in range(size):\n",
    "    #             X_one_hot = np.zeros(self.D)\n",
    "    #             X_one_hot[X] = 1.\n",
    "    #             X = X_one_hot.reshape(1, -1)\n",
    "    #             for layer in range(self.L): # start, stop, step\n",
    "    #                 y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "    #                 if layer == self.L-1: # this is the last layer\n",
    "    #                     y_logit = y\n",
    "    #                 else: \n",
    "    #                     X = y # y: output for this layer, X: input for this layer\n",
    "    #             y_prob = l.softmax(y_logit)\n",
    "    #             idx = np.random.choice(idx_list, p=y_prob.ravel())\n",
    "    #             chars.append(self.idx2char[idx])\n",
    "    #             X = idx\n",
    "\n",
    "    #         return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # # Backprop\n",
    "        # from sklearn.utils import shuffle as skshuffle\n",
    "        #     if shuffle:\n",
    "        #         X, y = skshuffle(X, y)\n",
    "\n",
    "#         for i in range(0, X.shape[0], minibatch_size):\n",
    "        for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "        for layer in range(nn.L):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # No batches/ full batches/ batch files\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "#                 sample = self.test(X_mini[0], state, size=100)\n",
    "#                 print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 100 # depth\n",
    "n_iter = 13 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//13 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 212.8914\n",
      "Iter-2 loss: 172.9326\n",
      "Iter-3 loss: 189.2016\n",
      "Iter-4 loss: 149.8625\n",
      "Iter-5 loss: 133.4056\n",
      "Iter-6 loss: 122.8095\n",
      "Iter-7 loss: 115.2649\n",
      "Iter-8 loss: 112.8522\n",
      "Iter-9 loss: 115.4768\n",
      "Iter-10 loss: 113.1451\n",
      "Iter-11 loss: 115.5507\n",
      "Iter-12 loss: 112.5494\n",
      "Iter-13 loss: 112.6901\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11303ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 100 # depth\n",
    "n_iter = 13 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//13 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
