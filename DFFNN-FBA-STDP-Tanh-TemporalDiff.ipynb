{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = np.tanh(y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = np.tanh(y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3390 valid loss: 2.3261, valid accuracy: 0.0822\n",
      "Iter-200 train loss: 2.3115 valid loss: 2.3247, valid accuracy: 0.0848\n",
      "Iter-300 train loss: 2.3029 valid loss: 2.3233, valid accuracy: 0.0860\n",
      "Iter-400 train loss: 2.3123 valid loss: 2.3220, valid accuracy: 0.0860\n",
      "Iter-500 train loss: 2.3120 valid loss: 2.3206, valid accuracy: 0.0890\n",
      "Iter-600 train loss: 2.3063 valid loss: 2.3192, valid accuracy: 0.0898\n",
      "Iter-700 train loss: 2.3385 valid loss: 2.3179, valid accuracy: 0.0906\n",
      "Iter-800 train loss: 2.2998 valid loss: 2.3166, valid accuracy: 0.0920\n",
      "Iter-900 train loss: 2.3183 valid loss: 2.3152, valid accuracy: 0.0940\n",
      "Iter-1000 train loss: 2.3129 valid loss: 2.3139, valid accuracy: 0.0960\n",
      "Iter-1100 train loss: 2.3401 valid loss: 2.3125, valid accuracy: 0.0978\n",
      "Iter-1200 train loss: 2.3081 valid loss: 2.3112, valid accuracy: 0.1002\n",
      "Iter-1300 train loss: 2.3136 valid loss: 2.3098, valid accuracy: 0.1012\n",
      "Iter-1400 train loss: 2.3020 valid loss: 2.3085, valid accuracy: 0.1026\n",
      "Iter-1500 train loss: 2.3105 valid loss: 2.3072, valid accuracy: 0.1028\n",
      "Iter-1600 train loss: 2.2992 valid loss: 2.3059, valid accuracy: 0.1056\n",
      "Iter-1700 train loss: 2.2984 valid loss: 2.3046, valid accuracy: 0.1074\n",
      "Iter-1800 train loss: 2.3025 valid loss: 2.3032, valid accuracy: 0.1092\n",
      "Iter-1900 train loss: 2.2888 valid loss: 2.3019, valid accuracy: 0.1110\n",
      "Iter-2000 train loss: 2.3128 valid loss: 2.3006, valid accuracy: 0.1132\n",
      "Iter-2100 train loss: 2.2914 valid loss: 2.2993, valid accuracy: 0.1160\n",
      "Iter-2200 train loss: 2.3088 valid loss: 2.2980, valid accuracy: 0.1174\n",
      "Iter-2300 train loss: 2.3024 valid loss: 2.2967, valid accuracy: 0.1190\n",
      "Iter-2400 train loss: 2.3079 valid loss: 2.2954, valid accuracy: 0.1220\n",
      "Iter-2500 train loss: 2.2931 valid loss: 2.2941, valid accuracy: 0.1230\n",
      "Iter-2600 train loss: 2.3055 valid loss: 2.2928, valid accuracy: 0.1266\n",
      "Iter-2700 train loss: 2.2663 valid loss: 2.2915, valid accuracy: 0.1282\n",
      "Iter-2800 train loss: 2.3062 valid loss: 2.2902, valid accuracy: 0.1306\n",
      "Iter-2900 train loss: 2.2542 valid loss: 2.2889, valid accuracy: 0.1342\n",
      "Iter-3000 train loss: 2.2750 valid loss: 2.2877, valid accuracy: 0.1358\n",
      "Iter-3100 train loss: 2.2955 valid loss: 2.2864, valid accuracy: 0.1390\n",
      "Iter-3200 train loss: 2.2898 valid loss: 2.2851, valid accuracy: 0.1422\n",
      "Iter-3300 train loss: 2.2647 valid loss: 2.2838, valid accuracy: 0.1446\n",
      "Iter-3400 train loss: 2.2583 valid loss: 2.2825, valid accuracy: 0.1486\n",
      "Iter-3500 train loss: 2.2826 valid loss: 2.2813, valid accuracy: 0.1504\n",
      "Iter-3600 train loss: 2.2751 valid loss: 2.2800, valid accuracy: 0.1524\n",
      "Iter-3700 train loss: 2.2877 valid loss: 2.2787, valid accuracy: 0.1540\n",
      "Iter-3800 train loss: 2.2519 valid loss: 2.2775, valid accuracy: 0.1568\n",
      "Iter-3900 train loss: 2.2690 valid loss: 2.2762, valid accuracy: 0.1618\n",
      "Iter-4000 train loss: 2.2763 valid loss: 2.2749, valid accuracy: 0.1662\n",
      "Iter-4100 train loss: 2.2573 valid loss: 2.2737, valid accuracy: 0.1692\n",
      "Iter-4200 train loss: 2.2662 valid loss: 2.2724, valid accuracy: 0.1730\n",
      "Iter-4300 train loss: 2.2742 valid loss: 2.2712, valid accuracy: 0.1762\n",
      "Iter-4400 train loss: 2.2653 valid loss: 2.2699, valid accuracy: 0.1784\n",
      "Iter-4500 train loss: 2.2834 valid loss: 2.2686, valid accuracy: 0.1822\n",
      "Iter-4600 train loss: 2.2643 valid loss: 2.2674, valid accuracy: 0.1880\n",
      "Iter-4700 train loss: 2.2845 valid loss: 2.2662, valid accuracy: 0.1904\n",
      "Iter-4800 train loss: 2.2638 valid loss: 2.2649, valid accuracy: 0.1970\n",
      "Iter-4900 train loss: 2.2564 valid loss: 2.2636, valid accuracy: 0.2032\n",
      "Iter-5000 train loss: 2.2642 valid loss: 2.2624, valid accuracy: 0.2060\n",
      "Iter-5100 train loss: 2.2742 valid loss: 2.2612, valid accuracy: 0.2094\n",
      "Iter-5200 train loss: 2.2331 valid loss: 2.2599, valid accuracy: 0.2138\n",
      "Iter-5300 train loss: 2.2421 valid loss: 2.2587, valid accuracy: 0.2178\n",
      "Iter-5400 train loss: 2.2663 valid loss: 2.2575, valid accuracy: 0.2214\n",
      "Iter-5500 train loss: 2.2442 valid loss: 2.2562, valid accuracy: 0.2252\n",
      "Iter-5600 train loss: 2.2745 valid loss: 2.2550, valid accuracy: 0.2296\n",
      "Iter-5700 train loss: 2.2308 valid loss: 2.2538, valid accuracy: 0.2336\n",
      "Iter-5800 train loss: 2.2681 valid loss: 2.2526, valid accuracy: 0.2362\n",
      "Iter-5900 train loss: 2.2614 valid loss: 2.2514, valid accuracy: 0.2386\n",
      "Iter-6000 train loss: 2.2623 valid loss: 2.2502, valid accuracy: 0.2438\n",
      "Iter-6100 train loss: 2.2461 valid loss: 2.2489, valid accuracy: 0.2466\n",
      "Iter-6200 train loss: 2.2618 valid loss: 2.2477, valid accuracy: 0.2512\n",
      "Iter-6300 train loss: 2.2543 valid loss: 2.2465, valid accuracy: 0.2550\n",
      "Iter-6400 train loss: 2.2469 valid loss: 2.2453, valid accuracy: 0.2582\n",
      "Iter-6500 train loss: 2.2426 valid loss: 2.2441, valid accuracy: 0.2594\n",
      "Iter-6600 train loss: 2.2625 valid loss: 2.2429, valid accuracy: 0.2636\n",
      "Iter-6700 train loss: 2.2535 valid loss: 2.2417, valid accuracy: 0.2660\n",
      "Iter-6800 train loss: 2.2322 valid loss: 2.2405, valid accuracy: 0.2696\n",
      "Iter-6900 train loss: 2.2307 valid loss: 2.2393, valid accuracy: 0.2718\n",
      "Iter-7000 train loss: 2.2408 valid loss: 2.2381, valid accuracy: 0.2746\n",
      "Iter-7100 train loss: 2.2244 valid loss: 2.2369, valid accuracy: 0.2778\n",
      "Iter-7200 train loss: 2.2391 valid loss: 2.2357, valid accuracy: 0.2802\n",
      "Iter-7300 train loss: 2.2234 valid loss: 2.2345, valid accuracy: 0.2824\n",
      "Iter-7400 train loss: 2.2425 valid loss: 2.2333, valid accuracy: 0.2854\n",
      "Iter-7500 train loss: 2.2271 valid loss: 2.2322, valid accuracy: 0.2878\n",
      "Iter-7600 train loss: 2.2410 valid loss: 2.2310, valid accuracy: 0.2904\n",
      "Iter-7700 train loss: 2.2247 valid loss: 2.2298, valid accuracy: 0.2934\n",
      "Iter-7800 train loss: 2.2248 valid loss: 2.2286, valid accuracy: 0.2966\n",
      "Iter-7900 train loss: 2.2275 valid loss: 2.2274, valid accuracy: 0.2996\n",
      "Iter-8000 train loss: 2.2314 valid loss: 2.2263, valid accuracy: 0.3018\n",
      "Iter-8100 train loss: 2.2371 valid loss: 2.2251, valid accuracy: 0.3042\n",
      "Iter-8200 train loss: 2.2448 valid loss: 2.2239, valid accuracy: 0.3060\n",
      "Iter-8300 train loss: 2.2244 valid loss: 2.2228, valid accuracy: 0.3086\n",
      "Iter-8400 train loss: 2.2098 valid loss: 2.2216, valid accuracy: 0.3100\n",
      "Iter-8500 train loss: 2.2333 valid loss: 2.2204, valid accuracy: 0.3130\n",
      "Iter-8600 train loss: 2.2190 valid loss: 2.2193, valid accuracy: 0.3144\n",
      "Iter-8700 train loss: 2.2098 valid loss: 2.2182, valid accuracy: 0.3162\n",
      "Iter-8800 train loss: 2.2090 valid loss: 2.2170, valid accuracy: 0.3178\n",
      "Iter-8900 train loss: 2.1898 valid loss: 2.2159, valid accuracy: 0.3200\n",
      "Iter-9000 train loss: 2.2003 valid loss: 2.2147, valid accuracy: 0.3226\n",
      "Iter-9100 train loss: 2.2299 valid loss: 2.2135, valid accuracy: 0.3238\n",
      "Iter-9200 train loss: 2.2058 valid loss: 2.2124, valid accuracy: 0.3256\n",
      "Iter-9300 train loss: 2.2091 valid loss: 2.2112, valid accuracy: 0.3270\n",
      "Iter-9400 train loss: 2.2400 valid loss: 2.2101, valid accuracy: 0.3290\n",
      "Iter-9500 train loss: 2.2345 valid loss: 2.2090, valid accuracy: 0.3314\n",
      "Iter-9600 train loss: 2.1897 valid loss: 2.2078, valid accuracy: 0.3328\n",
      "Iter-9700 train loss: 2.2327 valid loss: 2.2067, valid accuracy: 0.3340\n",
      "Iter-9800 train loss: 2.2157 valid loss: 2.2055, valid accuracy: 0.3344\n",
      "Iter-9900 train loss: 2.1918 valid loss: 2.2044, valid accuracy: 0.3356\n",
      "Iter-10000 train loss: 2.1956 valid loss: 2.2033, valid accuracy: 0.3370\n",
      "Iter-10100 train loss: 2.2497 valid loss: 2.2021, valid accuracy: 0.3376\n",
      "Iter-10200 train loss: 2.2032 valid loss: 2.2010, valid accuracy: 0.3384\n",
      "Iter-10300 train loss: 2.1976 valid loss: 2.1999, valid accuracy: 0.3392\n",
      "Iter-10400 train loss: 2.1914 valid loss: 2.1987, valid accuracy: 0.3416\n",
      "Iter-10500 train loss: 2.1734 valid loss: 2.1976, valid accuracy: 0.3436\n",
      "Iter-10600 train loss: 2.1951 valid loss: 2.1965, valid accuracy: 0.3452\n",
      "Iter-10700 train loss: 2.1884 valid loss: 2.1953, valid accuracy: 0.3452\n",
      "Iter-10800 train loss: 2.1673 valid loss: 2.1942, valid accuracy: 0.3466\n",
      "Iter-10900 train loss: 2.2132 valid loss: 2.1931, valid accuracy: 0.3478\n",
      "Iter-11000 train loss: 2.1939 valid loss: 2.1920, valid accuracy: 0.3490\n",
      "Iter-11100 train loss: 2.1930 valid loss: 2.1909, valid accuracy: 0.3516\n",
      "Iter-11200 train loss: 2.1856 valid loss: 2.1898, valid accuracy: 0.3526\n",
      "Iter-11300 train loss: 2.1787 valid loss: 2.1887, valid accuracy: 0.3540\n",
      "Iter-11400 train loss: 2.1911 valid loss: 2.1876, valid accuracy: 0.3550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.1842 valid loss: 2.1865, valid accuracy: 0.3556\n",
      "Iter-11600 train loss: 2.1905 valid loss: 2.1854, valid accuracy: 0.3568\n",
      "Iter-11700 train loss: 2.1845 valid loss: 2.1843, valid accuracy: 0.3574\n",
      "Iter-11800 train loss: 2.2014 valid loss: 2.1832, valid accuracy: 0.3596\n",
      "Iter-11900 train loss: 2.1917 valid loss: 2.1822, valid accuracy: 0.3600\n",
      "Iter-12000 train loss: 2.1782 valid loss: 2.1811, valid accuracy: 0.3610\n",
      "Iter-12100 train loss: 2.1583 valid loss: 2.1800, valid accuracy: 0.3618\n",
      "Iter-12200 train loss: 2.2089 valid loss: 2.1789, valid accuracy: 0.3618\n",
      "Iter-12300 train loss: 2.1592 valid loss: 2.1778, valid accuracy: 0.3628\n",
      "Iter-12400 train loss: 2.2015 valid loss: 2.1767, valid accuracy: 0.3638\n",
      "Iter-12500 train loss: 2.1420 valid loss: 2.1757, valid accuracy: 0.3642\n",
      "Iter-12600 train loss: 2.1810 valid loss: 2.1745, valid accuracy: 0.3654\n",
      "Iter-12700 train loss: 2.2232 valid loss: 2.1735, valid accuracy: 0.3668\n",
      "Iter-12800 train loss: 2.2077 valid loss: 2.1724, valid accuracy: 0.3682\n",
      "Iter-12900 train loss: 2.1458 valid loss: 2.1713, valid accuracy: 0.3692\n",
      "Iter-13000 train loss: 2.1540 valid loss: 2.1702, valid accuracy: 0.3700\n",
      "Iter-13100 train loss: 2.1878 valid loss: 2.1692, valid accuracy: 0.3716\n",
      "Iter-13200 train loss: 2.1484 valid loss: 2.1681, valid accuracy: 0.3722\n",
      "Iter-13300 train loss: 2.1989 valid loss: 2.1671, valid accuracy: 0.3728\n",
      "Iter-13400 train loss: 2.1339 valid loss: 2.1660, valid accuracy: 0.3740\n",
      "Iter-13500 train loss: 2.1975 valid loss: 2.1649, valid accuracy: 0.3762\n",
      "Iter-13600 train loss: 2.1713 valid loss: 2.1638, valid accuracy: 0.3772\n",
      "Iter-13700 train loss: 2.1635 valid loss: 2.1628, valid accuracy: 0.3774\n",
      "Iter-13800 train loss: 2.1303 valid loss: 2.1617, valid accuracy: 0.3776\n",
      "Iter-13900 train loss: 2.1359 valid loss: 2.1607, valid accuracy: 0.3786\n",
      "Iter-14000 train loss: 2.1573 valid loss: 2.1596, valid accuracy: 0.3792\n",
      "Iter-14100 train loss: 2.1623 valid loss: 2.1586, valid accuracy: 0.3806\n",
      "Iter-14200 train loss: 2.1760 valid loss: 2.1575, valid accuracy: 0.3826\n",
      "Iter-14300 train loss: 2.1871 valid loss: 2.1565, valid accuracy: 0.3836\n",
      "Iter-14400 train loss: 2.1908 valid loss: 2.1555, valid accuracy: 0.3850\n",
      "Iter-14500 train loss: 2.1316 valid loss: 2.1544, valid accuracy: 0.3858\n",
      "Iter-14600 train loss: 2.1156 valid loss: 2.1534, valid accuracy: 0.3868\n",
      "Iter-14700 train loss: 2.1783 valid loss: 2.1523, valid accuracy: 0.3870\n",
      "Iter-14800 train loss: 2.1719 valid loss: 2.1513, valid accuracy: 0.3880\n",
      "Iter-14900 train loss: 2.1718 valid loss: 2.1503, valid accuracy: 0.3890\n",
      "Iter-15000 train loss: 2.1087 valid loss: 2.1492, valid accuracy: 0.3902\n",
      "Iter-15100 train loss: 2.1764 valid loss: 2.1482, valid accuracy: 0.3920\n",
      "Iter-15200 train loss: 2.1742 valid loss: 2.1472, valid accuracy: 0.3924\n",
      "Iter-15300 train loss: 2.1147 valid loss: 2.1461, valid accuracy: 0.3922\n",
      "Iter-15400 train loss: 2.1763 valid loss: 2.1451, valid accuracy: 0.3932\n",
      "Iter-15500 train loss: 2.1435 valid loss: 2.1441, valid accuracy: 0.3942\n",
      "Iter-15600 train loss: 2.1463 valid loss: 2.1431, valid accuracy: 0.3960\n",
      "Iter-15700 train loss: 2.1517 valid loss: 2.1421, valid accuracy: 0.3964\n",
      "Iter-15800 train loss: 2.1485 valid loss: 2.1410, valid accuracy: 0.3974\n",
      "Iter-15900 train loss: 2.1496 valid loss: 2.1400, valid accuracy: 0.3976\n",
      "Iter-16000 train loss: 2.1458 valid loss: 2.1390, valid accuracy: 0.3992\n",
      "Iter-16100 train loss: 2.1426 valid loss: 2.1380, valid accuracy: 0.4004\n",
      "Iter-16200 train loss: 2.1442 valid loss: 2.1370, valid accuracy: 0.4010\n",
      "Iter-16300 train loss: 2.1223 valid loss: 2.1360, valid accuracy: 0.4014\n",
      "Iter-16400 train loss: 2.1362 valid loss: 2.1350, valid accuracy: 0.4022\n",
      "Iter-16500 train loss: 2.1448 valid loss: 2.1340, valid accuracy: 0.4028\n",
      "Iter-16600 train loss: 2.2013 valid loss: 2.1330, valid accuracy: 0.4034\n",
      "Iter-16700 train loss: 2.1063 valid loss: 2.1320, valid accuracy: 0.4044\n",
      "Iter-16800 train loss: 2.1566 valid loss: 2.1310, valid accuracy: 0.4050\n",
      "Iter-16900 train loss: 2.1126 valid loss: 2.1300, valid accuracy: 0.4052\n",
      "Iter-17000 train loss: 2.1255 valid loss: 2.1290, valid accuracy: 0.4060\n",
      "Iter-17100 train loss: 2.1556 valid loss: 2.1280, valid accuracy: 0.4060\n",
      "Iter-17200 train loss: 2.1221 valid loss: 2.1270, valid accuracy: 0.4062\n",
      "Iter-17300 train loss: 2.1186 valid loss: 2.1260, valid accuracy: 0.4066\n",
      "Iter-17400 train loss: 2.1997 valid loss: 2.1250, valid accuracy: 0.4068\n",
      "Iter-17500 train loss: 2.1010 valid loss: 2.1240, valid accuracy: 0.4076\n",
      "Iter-17600 train loss: 2.1619 valid loss: 2.1230, valid accuracy: 0.4094\n",
      "Iter-17700 train loss: 2.1123 valid loss: 2.1220, valid accuracy: 0.4098\n",
      "Iter-17800 train loss: 2.1285 valid loss: 2.1210, valid accuracy: 0.4110\n",
      "Iter-17900 train loss: 2.1440 valid loss: 2.1200, valid accuracy: 0.4110\n",
      "Iter-18000 train loss: 2.1162 valid loss: 2.1190, valid accuracy: 0.4108\n",
      "Iter-18100 train loss: 2.1382 valid loss: 2.1180, valid accuracy: 0.4108\n",
      "Iter-18200 train loss: 2.0926 valid loss: 2.1170, valid accuracy: 0.4116\n",
      "Iter-18300 train loss: 2.1056 valid loss: 2.1160, valid accuracy: 0.4126\n",
      "Iter-18400 train loss: 2.1133 valid loss: 2.1150, valid accuracy: 0.4132\n",
      "Iter-18500 train loss: 2.0807 valid loss: 2.1140, valid accuracy: 0.4140\n",
      "Iter-18600 train loss: 2.1029 valid loss: 2.1131, valid accuracy: 0.4144\n",
      "Iter-18700 train loss: 2.0811 valid loss: 2.1121, valid accuracy: 0.4146\n",
      "Iter-18800 train loss: 2.0873 valid loss: 2.1111, valid accuracy: 0.4160\n",
      "Iter-18900 train loss: 2.1413 valid loss: 2.1101, valid accuracy: 0.4162\n",
      "Iter-19000 train loss: 2.0566 valid loss: 2.1092, valid accuracy: 0.4168\n",
      "Iter-19100 train loss: 2.1293 valid loss: 2.1082, valid accuracy: 0.4166\n",
      "Iter-19200 train loss: 2.1385 valid loss: 2.1072, valid accuracy: 0.4174\n",
      "Iter-19300 train loss: 2.1183 valid loss: 2.1062, valid accuracy: 0.4178\n",
      "Iter-19400 train loss: 2.1044 valid loss: 2.1052, valid accuracy: 0.4184\n",
      "Iter-19500 train loss: 2.0541 valid loss: 2.1043, valid accuracy: 0.4198\n",
      "Iter-19600 train loss: 2.0659 valid loss: 2.1033, valid accuracy: 0.4204\n",
      "Iter-19700 train loss: 2.0902 valid loss: 2.1024, valid accuracy: 0.4212\n",
      "Iter-19800 train loss: 2.1457 valid loss: 2.1014, valid accuracy: 0.4214\n",
      "Iter-19900 train loss: 2.1316 valid loss: 2.1004, valid accuracy: 0.4218\n",
      "Iter-20000 train loss: 2.1014 valid loss: 2.0994, valid accuracy: 0.4222\n",
      "Iter-20100 train loss: 2.0966 valid loss: 2.0985, valid accuracy: 0.4222\n",
      "Iter-20200 train loss: 2.0918 valid loss: 2.0975, valid accuracy: 0.4226\n",
      "Iter-20300 train loss: 2.0850 valid loss: 2.0966, valid accuracy: 0.4228\n",
      "Iter-20400 train loss: 2.1349 valid loss: 2.0956, valid accuracy: 0.4234\n",
      "Iter-20500 train loss: 2.0684 valid loss: 2.0947, valid accuracy: 0.4234\n",
      "Iter-20600 train loss: 2.0721 valid loss: 2.0937, valid accuracy: 0.4248\n",
      "Iter-20700 train loss: 2.1287 valid loss: 2.0927, valid accuracy: 0.4256\n",
      "Iter-20800 train loss: 2.0673 valid loss: 2.0918, valid accuracy: 0.4250\n",
      "Iter-20900 train loss: 2.0634 valid loss: 2.0909, valid accuracy: 0.4260\n",
      "Iter-21000 train loss: 2.0701 valid loss: 2.0899, valid accuracy: 0.4266\n",
      "Iter-21100 train loss: 2.0939 valid loss: 2.0890, valid accuracy: 0.4266\n",
      "Iter-21200 train loss: 2.0948 valid loss: 2.0880, valid accuracy: 0.4276\n",
      "Iter-21300 train loss: 2.0580 valid loss: 2.0871, valid accuracy: 0.4280\n",
      "Iter-21400 train loss: 2.0864 valid loss: 2.0861, valid accuracy: 0.4286\n",
      "Iter-21500 train loss: 2.1090 valid loss: 2.0852, valid accuracy: 0.4292\n",
      "Iter-21600 train loss: 2.0978 valid loss: 2.0842, valid accuracy: 0.4288\n",
      "Iter-21700 train loss: 2.0723 valid loss: 2.0833, valid accuracy: 0.4294\n",
      "Iter-21800 train loss: 2.1071 valid loss: 2.0823, valid accuracy: 0.4296\n",
      "Iter-21900 train loss: 2.0523 valid loss: 2.0814, valid accuracy: 0.4292\n",
      "Iter-22000 train loss: 2.1442 valid loss: 2.0804, valid accuracy: 0.4300\n",
      "Iter-22100 train loss: 2.0669 valid loss: 2.0795, valid accuracy: 0.4298\n",
      "Iter-22200 train loss: 2.1048 valid loss: 2.0786, valid accuracy: 0.4312\n",
      "Iter-22300 train loss: 2.0839 valid loss: 2.0777, valid accuracy: 0.4314\n",
      "Iter-22400 train loss: 2.0568 valid loss: 2.0768, valid accuracy: 0.4324\n",
      "Iter-22500 train loss: 2.0849 valid loss: 2.0759, valid accuracy: 0.4332\n",
      "Iter-22600 train loss: 2.0262 valid loss: 2.0750, valid accuracy: 0.4328\n",
      "Iter-22700 train loss: 2.1019 valid loss: 2.0741, valid accuracy: 0.4334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.1386 valid loss: 2.0732, valid accuracy: 0.4340\n",
      "Iter-22900 train loss: 2.0572 valid loss: 2.0722, valid accuracy: 0.4338\n",
      "Iter-23000 train loss: 2.0463 valid loss: 2.0713, valid accuracy: 0.4338\n",
      "Iter-23100 train loss: 2.0540 valid loss: 2.0704, valid accuracy: 0.4340\n",
      "Iter-23200 train loss: 2.0402 valid loss: 2.0695, valid accuracy: 0.4340\n",
      "Iter-23300 train loss: 2.1311 valid loss: 2.0685, valid accuracy: 0.4344\n",
      "Iter-23400 train loss: 2.0538 valid loss: 2.0676, valid accuracy: 0.4352\n",
      "Iter-23500 train loss: 2.0961 valid loss: 2.0667, valid accuracy: 0.4350\n",
      "Iter-23600 train loss: 2.0675 valid loss: 2.0658, valid accuracy: 0.4354\n",
      "Iter-23700 train loss: 2.0720 valid loss: 2.0649, valid accuracy: 0.4360\n",
      "Iter-23800 train loss: 2.0682 valid loss: 2.0640, valid accuracy: 0.4354\n",
      "Iter-23900 train loss: 2.0509 valid loss: 2.0631, valid accuracy: 0.4362\n",
      "Iter-24000 train loss: 2.0900 valid loss: 2.0622, valid accuracy: 0.4370\n",
      "Iter-24100 train loss: 2.0466 valid loss: 2.0613, valid accuracy: 0.4378\n",
      "Iter-24200 train loss: 2.0697 valid loss: 2.0605, valid accuracy: 0.4378\n",
      "Iter-24300 train loss: 2.0773 valid loss: 2.0595, valid accuracy: 0.4396\n",
      "Iter-24400 train loss: 2.0749 valid loss: 2.0586, valid accuracy: 0.4408\n",
      "Iter-24500 train loss: 2.0701 valid loss: 2.0577, valid accuracy: 0.4408\n",
      "Iter-24600 train loss: 2.0843 valid loss: 2.0568, valid accuracy: 0.4422\n",
      "Iter-24700 train loss: 2.0333 valid loss: 2.0559, valid accuracy: 0.4422\n",
      "Iter-24800 train loss: 2.0553 valid loss: 2.0550, valid accuracy: 0.4428\n",
      "Iter-24900 train loss: 2.0760 valid loss: 2.0541, valid accuracy: 0.4424\n",
      "Iter-25000 train loss: 2.0542 valid loss: 2.0532, valid accuracy: 0.4428\n",
      "Iter-25100 train loss: 2.0421 valid loss: 2.0523, valid accuracy: 0.4428\n",
      "Iter-25200 train loss: 2.0553 valid loss: 2.0514, valid accuracy: 0.4432\n",
      "Iter-25300 train loss: 2.1083 valid loss: 2.0505, valid accuracy: 0.4434\n",
      "Iter-25400 train loss: 2.0450 valid loss: 2.0496, valid accuracy: 0.4442\n",
      "Iter-25500 train loss: 2.0714 valid loss: 2.0487, valid accuracy: 0.4444\n",
      "Iter-25600 train loss: 2.0752 valid loss: 2.0478, valid accuracy: 0.4446\n",
      "Iter-25700 train loss: 2.0498 valid loss: 2.0469, valid accuracy: 0.4446\n",
      "Iter-25800 train loss: 2.0436 valid loss: 2.0460, valid accuracy: 0.4454\n",
      "Iter-25900 train loss: 2.0395 valid loss: 2.0452, valid accuracy: 0.4466\n",
      "Iter-26000 train loss: 2.0729 valid loss: 2.0443, valid accuracy: 0.4466\n",
      "Iter-26100 train loss: 2.0201 valid loss: 2.0434, valid accuracy: 0.4474\n",
      "Iter-26200 train loss: 2.0417 valid loss: 2.0426, valid accuracy: 0.4478\n",
      "Iter-26300 train loss: 2.0673 valid loss: 2.0417, valid accuracy: 0.4480\n",
      "Iter-26400 train loss: 2.0012 valid loss: 2.0408, valid accuracy: 0.4478\n",
      "Iter-26500 train loss: 2.0825 valid loss: 2.0400, valid accuracy: 0.4482\n",
      "Iter-26600 train loss: 2.0330 valid loss: 2.0391, valid accuracy: 0.4478\n",
      "Iter-26700 train loss: 2.0666 valid loss: 2.0382, valid accuracy: 0.4482\n",
      "Iter-26800 train loss: 2.0320 valid loss: 2.0374, valid accuracy: 0.4488\n",
      "Iter-26900 train loss: 2.0913 valid loss: 2.0365, valid accuracy: 0.4490\n",
      "Iter-27000 train loss: 2.0325 valid loss: 2.0356, valid accuracy: 0.4490\n",
      "Iter-27100 train loss: 2.0238 valid loss: 2.0347, valid accuracy: 0.4498\n",
      "Iter-27200 train loss: 2.0575 valid loss: 2.0339, valid accuracy: 0.4500\n",
      "Iter-27300 train loss: 2.0189 valid loss: 2.0330, valid accuracy: 0.4506\n",
      "Iter-27400 train loss: 2.0454 valid loss: 2.0321, valid accuracy: 0.4504\n",
      "Iter-27500 train loss: 2.0485 valid loss: 2.0312, valid accuracy: 0.4504\n",
      "Iter-27600 train loss: 2.1107 valid loss: 2.0304, valid accuracy: 0.4506\n",
      "Iter-27700 train loss: 2.0441 valid loss: 2.0295, valid accuracy: 0.4514\n",
      "Iter-27800 train loss: 2.0200 valid loss: 2.0287, valid accuracy: 0.4516\n",
      "Iter-27900 train loss: 2.0200 valid loss: 2.0278, valid accuracy: 0.4510\n",
      "Iter-28000 train loss: 2.0277 valid loss: 2.0269, valid accuracy: 0.4522\n",
      "Iter-28100 train loss: 2.0194 valid loss: 2.0261, valid accuracy: 0.4526\n",
      "Iter-28200 train loss: 2.0533 valid loss: 2.0252, valid accuracy: 0.4530\n",
      "Iter-28300 train loss: 2.0122 valid loss: 2.0244, valid accuracy: 0.4534\n",
      "Iter-28400 train loss: 2.0949 valid loss: 2.0235, valid accuracy: 0.4542\n",
      "Iter-28500 train loss: 2.0331 valid loss: 2.0226, valid accuracy: 0.4544\n",
      "Iter-28600 train loss: 2.0632 valid loss: 2.0218, valid accuracy: 0.4546\n",
      "Iter-28700 train loss: 2.0445 valid loss: 2.0209, valid accuracy: 0.4554\n",
      "Iter-28800 train loss: 2.0813 valid loss: 2.0201, valid accuracy: 0.4556\n",
      "Iter-28900 train loss: 1.9968 valid loss: 2.0192, valid accuracy: 0.4562\n",
      "Iter-29000 train loss: 1.9832 valid loss: 2.0184, valid accuracy: 0.4568\n",
      "Iter-29100 train loss: 2.0131 valid loss: 2.0175, valid accuracy: 0.4578\n",
      "Iter-29200 train loss: 2.0134 valid loss: 2.0167, valid accuracy: 0.4592\n",
      "Iter-29300 train loss: 1.9977 valid loss: 2.0159, valid accuracy: 0.4594\n",
      "Iter-29400 train loss: 2.0309 valid loss: 2.0151, valid accuracy: 0.4598\n",
      "Iter-29500 train loss: 1.9967 valid loss: 2.0143, valid accuracy: 0.4596\n",
      "Iter-29600 train loss: 2.0136 valid loss: 2.0134, valid accuracy: 0.4602\n",
      "Iter-29700 train loss: 2.0694 valid loss: 2.0126, valid accuracy: 0.4610\n",
      "Iter-29800 train loss: 2.0097 valid loss: 2.0118, valid accuracy: 0.4616\n",
      "Iter-29900 train loss: 2.0178 valid loss: 2.0109, valid accuracy: 0.4610\n",
      "Iter-30000 train loss: 1.9820 valid loss: 2.0101, valid accuracy: 0.4616\n",
      "Iter-30100 train loss: 2.0205 valid loss: 2.0093, valid accuracy: 0.4616\n",
      "Iter-30200 train loss: 1.9927 valid loss: 2.0084, valid accuracy: 0.4626\n",
      "Iter-30300 train loss: 2.0006 valid loss: 2.0076, valid accuracy: 0.4628\n",
      "Iter-30400 train loss: 1.9992 valid loss: 2.0067, valid accuracy: 0.4634\n",
      "Iter-30500 train loss: 1.9932 valid loss: 2.0059, valid accuracy: 0.4636\n",
      "Iter-30600 train loss: 2.0446 valid loss: 2.0050, valid accuracy: 0.4638\n",
      "Iter-30700 train loss: 1.9754 valid loss: 2.0042, valid accuracy: 0.4638\n",
      "Iter-30800 train loss: 2.0001 valid loss: 2.0034, valid accuracy: 0.4644\n",
      "Iter-30900 train loss: 2.0011 valid loss: 2.0026, valid accuracy: 0.4648\n",
      "Iter-31000 train loss: 2.0254 valid loss: 2.0018, valid accuracy: 0.4654\n",
      "Iter-31100 train loss: 2.0215 valid loss: 2.0009, valid accuracy: 0.4654\n",
      "Iter-31200 train loss: 1.9862 valid loss: 2.0001, valid accuracy: 0.4652\n",
      "Iter-31300 train loss: 1.9073 valid loss: 1.9993, valid accuracy: 0.4654\n",
      "Iter-31400 train loss: 2.0324 valid loss: 1.9984, valid accuracy: 0.4656\n",
      "Iter-31500 train loss: 2.0600 valid loss: 1.9977, valid accuracy: 0.4654\n",
      "Iter-31600 train loss: 2.0188 valid loss: 1.9968, valid accuracy: 0.4656\n",
      "Iter-31700 train loss: 1.9760 valid loss: 1.9960, valid accuracy: 0.4660\n",
      "Iter-31800 train loss: 2.0081 valid loss: 1.9952, valid accuracy: 0.4662\n",
      "Iter-31900 train loss: 2.0924 valid loss: 1.9944, valid accuracy: 0.4674\n",
      "Iter-32000 train loss: 1.9796 valid loss: 1.9936, valid accuracy: 0.4676\n",
      "Iter-32100 train loss: 1.9490 valid loss: 1.9928, valid accuracy: 0.4686\n",
      "Iter-32200 train loss: 2.0636 valid loss: 1.9919, valid accuracy: 0.4680\n",
      "Iter-32300 train loss: 2.0311 valid loss: 1.9911, valid accuracy: 0.4686\n",
      "Iter-32400 train loss: 1.9581 valid loss: 1.9903, valid accuracy: 0.4688\n",
      "Iter-32500 train loss: 2.0431 valid loss: 1.9895, valid accuracy: 0.4690\n",
      "Iter-32600 train loss: 1.9523 valid loss: 1.9887, valid accuracy: 0.4696\n",
      "Iter-32700 train loss: 2.0502 valid loss: 1.9879, valid accuracy: 0.4690\n",
      "Iter-32800 train loss: 1.9472 valid loss: 1.9871, valid accuracy: 0.4692\n",
      "Iter-32900 train loss: 2.0354 valid loss: 1.9862, valid accuracy: 0.4702\n",
      "Iter-33000 train loss: 1.9723 valid loss: 1.9854, valid accuracy: 0.4710\n",
      "Iter-33100 train loss: 2.0059 valid loss: 1.9846, valid accuracy: 0.4706\n",
      "Iter-33200 train loss: 2.0490 valid loss: 1.9838, valid accuracy: 0.4716\n",
      "Iter-33300 train loss: 2.0257 valid loss: 1.9830, valid accuracy: 0.4720\n",
      "Iter-33400 train loss: 1.9548 valid loss: 1.9822, valid accuracy: 0.4722\n",
      "Iter-33500 train loss: 1.9990 valid loss: 1.9814, valid accuracy: 0.4732\n",
      "Iter-33600 train loss: 1.9311 valid loss: 1.9806, valid accuracy: 0.4724\n",
      "Iter-33700 train loss: 1.9897 valid loss: 1.9798, valid accuracy: 0.4732\n",
      "Iter-33800 train loss: 1.9345 valid loss: 1.9790, valid accuracy: 0.4730\n",
      "Iter-33900 train loss: 2.0000 valid loss: 1.9783, valid accuracy: 0.4728\n",
      "Iter-34000 train loss: 1.9988 valid loss: 1.9775, valid accuracy: 0.4746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.0080 valid loss: 1.9767, valid accuracy: 0.4742\n",
      "Iter-34200 train loss: 1.9638 valid loss: 1.9759, valid accuracy: 0.4746\n",
      "Iter-34300 train loss: 1.9618 valid loss: 1.9751, valid accuracy: 0.4744\n",
      "Iter-34400 train loss: 2.0501 valid loss: 1.9743, valid accuracy: 0.4748\n",
      "Iter-34500 train loss: 1.8634 valid loss: 1.9735, valid accuracy: 0.4758\n",
      "Iter-34600 train loss: 2.0218 valid loss: 1.9727, valid accuracy: 0.4756\n",
      "Iter-34700 train loss: 1.8818 valid loss: 1.9719, valid accuracy: 0.4756\n",
      "Iter-34800 train loss: 2.0003 valid loss: 1.9711, valid accuracy: 0.4754\n",
      "Iter-34900 train loss: 1.9841 valid loss: 1.9703, valid accuracy: 0.4756\n",
      "Iter-35000 train loss: 1.9650 valid loss: 1.9695, valid accuracy: 0.4760\n",
      "Iter-35100 train loss: 2.0139 valid loss: 1.9688, valid accuracy: 0.4762\n",
      "Iter-35200 train loss: 1.9789 valid loss: 1.9680, valid accuracy: 0.4768\n",
      "Iter-35300 train loss: 1.9168 valid loss: 1.9672, valid accuracy: 0.4770\n",
      "Iter-35400 train loss: 1.9648 valid loss: 1.9664, valid accuracy: 0.4764\n",
      "Iter-35500 train loss: 1.9551 valid loss: 1.9657, valid accuracy: 0.4770\n",
      "Iter-35600 train loss: 1.9426 valid loss: 1.9649, valid accuracy: 0.4774\n",
      "Iter-35700 train loss: 1.9647 valid loss: 1.9641, valid accuracy: 0.4780\n",
      "Iter-35800 train loss: 1.9062 valid loss: 1.9633, valid accuracy: 0.4784\n",
      "Iter-35900 train loss: 2.0154 valid loss: 1.9625, valid accuracy: 0.4786\n",
      "Iter-36000 train loss: 1.9575 valid loss: 1.9618, valid accuracy: 0.4786\n",
      "Iter-36100 train loss: 1.9280 valid loss: 1.9610, valid accuracy: 0.4786\n",
      "Iter-36200 train loss: 1.8905 valid loss: 1.9602, valid accuracy: 0.4792\n",
      "Iter-36300 train loss: 2.0440 valid loss: 1.9595, valid accuracy: 0.4796\n",
      "Iter-36400 train loss: 1.9987 valid loss: 1.9587, valid accuracy: 0.4806\n",
      "Iter-36500 train loss: 1.9651 valid loss: 1.9579, valid accuracy: 0.4806\n",
      "Iter-36600 train loss: 1.9715 valid loss: 1.9571, valid accuracy: 0.4814\n",
      "Iter-36700 train loss: 1.9662 valid loss: 1.9564, valid accuracy: 0.4818\n",
      "Iter-36800 train loss: 1.9595 valid loss: 1.9556, valid accuracy: 0.4820\n",
      "Iter-36900 train loss: 2.0751 valid loss: 1.9548, valid accuracy: 0.4814\n",
      "Iter-37000 train loss: 1.9697 valid loss: 1.9540, valid accuracy: 0.4812\n",
      "Iter-37100 train loss: 1.9706 valid loss: 1.9533, valid accuracy: 0.4818\n",
      "Iter-37200 train loss: 1.9220 valid loss: 1.9525, valid accuracy: 0.4820\n",
      "Iter-37300 train loss: 2.0050 valid loss: 1.9517, valid accuracy: 0.4818\n",
      "Iter-37400 train loss: 1.8784 valid loss: 1.9509, valid accuracy: 0.4824\n",
      "Iter-37500 train loss: 1.8479 valid loss: 1.9502, valid accuracy: 0.4828\n",
      "Iter-37600 train loss: 1.8760 valid loss: 1.9494, valid accuracy: 0.4824\n",
      "Iter-37700 train loss: 1.9220 valid loss: 1.9487, valid accuracy: 0.4822\n",
      "Iter-37800 train loss: 1.9810 valid loss: 1.9480, valid accuracy: 0.4826\n",
      "Iter-37900 train loss: 1.9366 valid loss: 1.9472, valid accuracy: 0.4838\n",
      "Iter-38000 train loss: 1.9163 valid loss: 1.9464, valid accuracy: 0.4840\n",
      "Iter-38100 train loss: 1.9077 valid loss: 1.9456, valid accuracy: 0.4842\n",
      "Iter-38200 train loss: 1.9564 valid loss: 1.9449, valid accuracy: 0.4844\n",
      "Iter-38300 train loss: 1.9354 valid loss: 1.9441, valid accuracy: 0.4854\n",
      "Iter-38400 train loss: 2.0101 valid loss: 1.9433, valid accuracy: 0.4862\n",
      "Iter-38500 train loss: 1.8764 valid loss: 1.9426, valid accuracy: 0.4856\n",
      "Iter-38600 train loss: 1.8896 valid loss: 1.9418, valid accuracy: 0.4852\n",
      "Iter-38700 train loss: 1.9553 valid loss: 1.9410, valid accuracy: 0.4858\n",
      "Iter-38800 train loss: 1.9721 valid loss: 1.9403, valid accuracy: 0.4858\n",
      "Iter-38900 train loss: 1.9006 valid loss: 1.9395, valid accuracy: 0.4868\n",
      "Iter-39000 train loss: 2.0416 valid loss: 1.9388, valid accuracy: 0.4872\n",
      "Iter-39100 train loss: 1.8588 valid loss: 1.9380, valid accuracy: 0.4866\n",
      "Iter-39200 train loss: 1.8763 valid loss: 1.9373, valid accuracy: 0.4874\n",
      "Iter-39300 train loss: 1.9220 valid loss: 1.9366, valid accuracy: 0.4874\n",
      "Iter-39400 train loss: 1.9458 valid loss: 1.9358, valid accuracy: 0.4880\n",
      "Iter-39500 train loss: 1.9780 valid loss: 1.9351, valid accuracy: 0.4888\n",
      "Iter-39600 train loss: 1.9266 valid loss: 1.9343, valid accuracy: 0.4886\n",
      "Iter-39700 train loss: 1.9667 valid loss: 1.9336, valid accuracy: 0.4890\n",
      "Iter-39800 train loss: 1.8956 valid loss: 1.9328, valid accuracy: 0.4886\n",
      "Iter-39900 train loss: 1.9689 valid loss: 1.9321, valid accuracy: 0.4886\n",
      "Iter-40000 train loss: 1.9072 valid loss: 1.9313, valid accuracy: 0.4898\n",
      "Iter-40100 train loss: 1.9136 valid loss: 1.9306, valid accuracy: 0.4900\n",
      "Iter-40200 train loss: 1.8645 valid loss: 1.9299, valid accuracy: 0.4904\n",
      "Iter-40300 train loss: 1.9235 valid loss: 1.9291, valid accuracy: 0.4906\n",
      "Iter-40400 train loss: 1.9044 valid loss: 1.9284, valid accuracy: 0.4906\n",
      "Iter-40500 train loss: 1.9864 valid loss: 1.9276, valid accuracy: 0.4908\n",
      "Iter-40600 train loss: 1.9057 valid loss: 1.9269, valid accuracy: 0.4904\n",
      "Iter-40700 train loss: 1.8242 valid loss: 1.9261, valid accuracy: 0.4910\n",
      "Iter-40800 train loss: 1.8555 valid loss: 1.9254, valid accuracy: 0.4916\n",
      "Iter-40900 train loss: 1.8605 valid loss: 1.9247, valid accuracy: 0.4918\n",
      "Iter-41000 train loss: 1.8634 valid loss: 1.9239, valid accuracy: 0.4916\n",
      "Iter-41100 train loss: 1.9653 valid loss: 1.9232, valid accuracy: 0.4916\n",
      "Iter-41200 train loss: 2.0162 valid loss: 1.9225, valid accuracy: 0.4914\n",
      "Iter-41300 train loss: 1.8730 valid loss: 1.9217, valid accuracy: 0.4922\n",
      "Iter-41400 train loss: 2.0090 valid loss: 1.9210, valid accuracy: 0.4924\n",
      "Iter-41500 train loss: 1.9633 valid loss: 1.9202, valid accuracy: 0.4932\n",
      "Iter-41600 train loss: 1.9369 valid loss: 1.9195, valid accuracy: 0.4932\n",
      "Iter-41700 train loss: 1.9034 valid loss: 1.9188, valid accuracy: 0.4936\n",
      "Iter-41800 train loss: 1.9259 valid loss: 1.9180, valid accuracy: 0.4942\n",
      "Iter-41900 train loss: 1.9089 valid loss: 1.9173, valid accuracy: 0.4936\n",
      "Iter-42000 train loss: 1.8501 valid loss: 1.9165, valid accuracy: 0.4938\n",
      "Iter-42100 train loss: 1.9558 valid loss: 1.9158, valid accuracy: 0.4940\n",
      "Iter-42200 train loss: 1.9807 valid loss: 1.9151, valid accuracy: 0.4942\n",
      "Iter-42300 train loss: 1.9647 valid loss: 1.9143, valid accuracy: 0.4942\n",
      "Iter-42400 train loss: 1.9198 valid loss: 1.9136, valid accuracy: 0.4948\n",
      "Iter-42500 train loss: 1.9636 valid loss: 1.9129, valid accuracy: 0.4944\n",
      "Iter-42600 train loss: 1.9278 valid loss: 1.9122, valid accuracy: 0.4948\n",
      "Iter-42700 train loss: 1.8894 valid loss: 1.9115, valid accuracy: 0.4954\n",
      "Iter-42800 train loss: 1.9809 valid loss: 1.9107, valid accuracy: 0.4958\n",
      "Iter-42900 train loss: 1.9314 valid loss: 1.9100, valid accuracy: 0.4954\n",
      "Iter-43000 train loss: 1.9212 valid loss: 1.9093, valid accuracy: 0.4960\n",
      "Iter-43100 train loss: 1.8882 valid loss: 1.9086, valid accuracy: 0.4960\n",
      "Iter-43200 train loss: 1.9169 valid loss: 1.9079, valid accuracy: 0.4964\n",
      "Iter-43300 train loss: 1.8532 valid loss: 1.9071, valid accuracy: 0.4964\n",
      "Iter-43400 train loss: 1.8400 valid loss: 1.9064, valid accuracy: 0.4972\n",
      "Iter-43500 train loss: 1.9582 valid loss: 1.9057, valid accuracy: 0.4970\n",
      "Iter-43600 train loss: 1.8468 valid loss: 1.9049, valid accuracy: 0.4972\n",
      "Iter-43700 train loss: 1.9125 valid loss: 1.9042, valid accuracy: 0.4976\n",
      "Iter-43800 train loss: 1.9210 valid loss: 1.9035, valid accuracy: 0.4974\n",
      "Iter-43900 train loss: 1.8840 valid loss: 1.9028, valid accuracy: 0.4976\n",
      "Iter-44000 train loss: 1.9145 valid loss: 1.9021, valid accuracy: 0.4974\n",
      "Iter-44100 train loss: 1.8312 valid loss: 1.9014, valid accuracy: 0.4978\n",
      "Iter-44200 train loss: 1.9756 valid loss: 1.9007, valid accuracy: 0.4980\n",
      "Iter-44300 train loss: 1.9499 valid loss: 1.8999, valid accuracy: 0.4976\n",
      "Iter-44400 train loss: 1.9190 valid loss: 1.8992, valid accuracy: 0.4974\n",
      "Iter-44500 train loss: 1.8902 valid loss: 1.8985, valid accuracy: 0.4974\n",
      "Iter-44600 train loss: 1.8772 valid loss: 1.8978, valid accuracy: 0.4980\n",
      "Iter-44700 train loss: 1.8597 valid loss: 1.8971, valid accuracy: 0.4984\n",
      "Iter-44800 train loss: 1.8178 valid loss: 1.8964, valid accuracy: 0.4986\n",
      "Iter-44900 train loss: 1.8792 valid loss: 1.8957, valid accuracy: 0.4982\n",
      "Iter-45000 train loss: 1.9010 valid loss: 1.8950, valid accuracy: 0.4994\n",
      "Iter-45100 train loss: 1.8482 valid loss: 1.8943, valid accuracy: 0.4998\n",
      "Iter-45200 train loss: 1.9555 valid loss: 1.8935, valid accuracy: 0.4996\n",
      "Iter-45300 train loss: 1.8632 valid loss: 1.8929, valid accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 1.9221 valid loss: 1.8921, valid accuracy: 0.4994\n",
      "Iter-45500 train loss: 1.9022 valid loss: 1.8914, valid accuracy: 0.4998\n",
      "Iter-45600 train loss: 1.8864 valid loss: 1.8908, valid accuracy: 0.5002\n",
      "Iter-45700 train loss: 1.8939 valid loss: 1.8900, valid accuracy: 0.5008\n",
      "Iter-45800 train loss: 1.9228 valid loss: 1.8893, valid accuracy: 0.5008\n",
      "Iter-45900 train loss: 1.8643 valid loss: 1.8886, valid accuracy: 0.5010\n",
      "Iter-46000 train loss: 1.8932 valid loss: 1.8879, valid accuracy: 0.5010\n",
      "Iter-46100 train loss: 1.9249 valid loss: 1.8872, valid accuracy: 0.5010\n",
      "Iter-46200 train loss: 1.9112 valid loss: 1.8865, valid accuracy: 0.5008\n",
      "Iter-46300 train loss: 1.9050 valid loss: 1.8858, valid accuracy: 0.5014\n",
      "Iter-46400 train loss: 1.9266 valid loss: 1.8851, valid accuracy: 0.5018\n",
      "Iter-46500 train loss: 2.0244 valid loss: 1.8844, valid accuracy: 0.5022\n",
      "Iter-46600 train loss: 1.9565 valid loss: 1.8837, valid accuracy: 0.5024\n",
      "Iter-46700 train loss: 1.8720 valid loss: 1.8830, valid accuracy: 0.5026\n",
      "Iter-46800 train loss: 1.9249 valid loss: 1.8823, valid accuracy: 0.5030\n",
      "Iter-46900 train loss: 1.8960 valid loss: 1.8816, valid accuracy: 0.5038\n",
      "Iter-47000 train loss: 1.7987 valid loss: 1.8809, valid accuracy: 0.5032\n",
      "Iter-47100 train loss: 1.9841 valid loss: 1.8802, valid accuracy: 0.5034\n",
      "Iter-47200 train loss: 1.9097 valid loss: 1.8795, valid accuracy: 0.5036\n",
      "Iter-47300 train loss: 1.8661 valid loss: 1.8788, valid accuracy: 0.5038\n",
      "Iter-47400 train loss: 1.8704 valid loss: 1.8781, valid accuracy: 0.5040\n",
      "Iter-47500 train loss: 1.9058 valid loss: 1.8774, valid accuracy: 0.5040\n",
      "Iter-47600 train loss: 1.8928 valid loss: 1.8767, valid accuracy: 0.5042\n",
      "Iter-47700 train loss: 1.8343 valid loss: 1.8760, valid accuracy: 0.5036\n",
      "Iter-47800 train loss: 1.8700 valid loss: 1.8754, valid accuracy: 0.5038\n",
      "Iter-47900 train loss: 1.8179 valid loss: 1.8747, valid accuracy: 0.5036\n",
      "Iter-48000 train loss: 1.9089 valid loss: 1.8740, valid accuracy: 0.5038\n",
      "Iter-48100 train loss: 1.9184 valid loss: 1.8733, valid accuracy: 0.5042\n",
      "Iter-48200 train loss: 1.8162 valid loss: 1.8726, valid accuracy: 0.5040\n",
      "Iter-48300 train loss: 1.8783 valid loss: 1.8719, valid accuracy: 0.5046\n",
      "Iter-48400 train loss: 1.8836 valid loss: 1.8712, valid accuracy: 0.5042\n",
      "Iter-48500 train loss: 1.9133 valid loss: 1.8705, valid accuracy: 0.5038\n",
      "Iter-48600 train loss: 1.8644 valid loss: 1.8698, valid accuracy: 0.5040\n",
      "Iter-48700 train loss: 1.9368 valid loss: 1.8692, valid accuracy: 0.5044\n",
      "Iter-48800 train loss: 1.8746 valid loss: 1.8685, valid accuracy: 0.5042\n",
      "Iter-48900 train loss: 1.8295 valid loss: 1.8678, valid accuracy: 0.5044\n",
      "Iter-49000 train loss: 1.8296 valid loss: 1.8672, valid accuracy: 0.5044\n",
      "Iter-49100 train loss: 1.8281 valid loss: 1.8665, valid accuracy: 0.5048\n",
      "Iter-49200 train loss: 1.8786 valid loss: 1.8658, valid accuracy: 0.5046\n",
      "Iter-49300 train loss: 1.8190 valid loss: 1.8651, valid accuracy: 0.5048\n",
      "Iter-49400 train loss: 1.7763 valid loss: 1.8644, valid accuracy: 0.5052\n",
      "Iter-49500 train loss: 1.8960 valid loss: 1.8637, valid accuracy: 0.5052\n",
      "Iter-49600 train loss: 1.8208 valid loss: 1.8631, valid accuracy: 0.5052\n",
      "Iter-49700 train loss: 1.9373 valid loss: 1.8624, valid accuracy: 0.5054\n",
      "Iter-49800 train loss: 1.8456 valid loss: 1.8617, valid accuracy: 0.5058\n",
      "Iter-49900 train loss: 2.0179 valid loss: 1.8610, valid accuracy: 0.5058\n",
      "Iter-50000 train loss: 1.7721 valid loss: 1.8604, valid accuracy: 0.5062\n",
      "Iter-50100 train loss: 1.8732 valid loss: 1.8597, valid accuracy: 0.5064\n",
      "Iter-50200 train loss: 1.8466 valid loss: 1.8590, valid accuracy: 0.5064\n",
      "Iter-50300 train loss: 1.8547 valid loss: 1.8584, valid accuracy: 0.5064\n",
      "Iter-50400 train loss: 1.7960 valid loss: 1.8577, valid accuracy: 0.5060\n",
      "Iter-50500 train loss: 1.9517 valid loss: 1.8570, valid accuracy: 0.5064\n",
      "Iter-50600 train loss: 1.7736 valid loss: 1.8563, valid accuracy: 0.5068\n",
      "Iter-50700 train loss: 1.7842 valid loss: 1.8556, valid accuracy: 0.5072\n",
      "Iter-50800 train loss: 1.8703 valid loss: 1.8549, valid accuracy: 0.5070\n",
      "Iter-50900 train loss: 1.8923 valid loss: 1.8542, valid accuracy: 0.5072\n",
      "Iter-51000 train loss: 1.7992 valid loss: 1.8535, valid accuracy: 0.5074\n",
      "Iter-51100 train loss: 1.8274 valid loss: 1.8528, valid accuracy: 0.5086\n",
      "Iter-51200 train loss: 1.7826 valid loss: 1.8522, valid accuracy: 0.5076\n",
      "Iter-51300 train loss: 1.8563 valid loss: 1.8515, valid accuracy: 0.5078\n",
      "Iter-51400 train loss: 1.8697 valid loss: 1.8509, valid accuracy: 0.5080\n",
      "Iter-51500 train loss: 1.7760 valid loss: 1.8502, valid accuracy: 0.5082\n",
      "Iter-51600 train loss: 1.8864 valid loss: 1.8496, valid accuracy: 0.5078\n",
      "Iter-51700 train loss: 1.7880 valid loss: 1.8489, valid accuracy: 0.5078\n",
      "Iter-51800 train loss: 1.9649 valid loss: 1.8483, valid accuracy: 0.5084\n",
      "Iter-51900 train loss: 1.8341 valid loss: 1.8476, valid accuracy: 0.5084\n",
      "Iter-52000 train loss: 1.8535 valid loss: 1.8470, valid accuracy: 0.5084\n",
      "Iter-52100 train loss: 1.8364 valid loss: 1.8463, valid accuracy: 0.5086\n",
      "Iter-52200 train loss: 1.7285 valid loss: 1.8456, valid accuracy: 0.5084\n",
      "Iter-52300 train loss: 1.8635 valid loss: 1.8450, valid accuracy: 0.5092\n",
      "Iter-52400 train loss: 1.7406 valid loss: 1.8443, valid accuracy: 0.5090\n",
      "Iter-52500 train loss: 1.8011 valid loss: 1.8436, valid accuracy: 0.5092\n",
      "Iter-52600 train loss: 1.8739 valid loss: 1.8430, valid accuracy: 0.5096\n",
      "Iter-52700 train loss: 1.8445 valid loss: 1.8423, valid accuracy: 0.5098\n",
      "Iter-52800 train loss: 1.9142 valid loss: 1.8417, valid accuracy: 0.5096\n",
      "Iter-52900 train loss: 1.9963 valid loss: 1.8409, valid accuracy: 0.5098\n",
      "Iter-53000 train loss: 1.8993 valid loss: 1.8403, valid accuracy: 0.5096\n",
      "Iter-53100 train loss: 1.9028 valid loss: 1.8397, valid accuracy: 0.5098\n",
      "Iter-53200 train loss: 1.8488 valid loss: 1.8390, valid accuracy: 0.5102\n",
      "Iter-53300 train loss: 1.8891 valid loss: 1.8384, valid accuracy: 0.5100\n",
      "Iter-53400 train loss: 1.8172 valid loss: 1.8378, valid accuracy: 0.5102\n",
      "Iter-53500 train loss: 1.9280 valid loss: 1.8371, valid accuracy: 0.5098\n",
      "Iter-53600 train loss: 1.8104 valid loss: 1.8364, valid accuracy: 0.5098\n",
      "Iter-53700 train loss: 1.7751 valid loss: 1.8358, valid accuracy: 0.5098\n",
      "Iter-53800 train loss: 1.8510 valid loss: 1.8351, valid accuracy: 0.5104\n",
      "Iter-53900 train loss: 1.9189 valid loss: 1.8345, valid accuracy: 0.5106\n",
      "Iter-54000 train loss: 1.8304 valid loss: 1.8339, valid accuracy: 0.5110\n",
      "Iter-54100 train loss: 1.8901 valid loss: 1.8332, valid accuracy: 0.5112\n",
      "Iter-54200 train loss: 1.8303 valid loss: 1.8325, valid accuracy: 0.5108\n",
      "Iter-54300 train loss: 1.9409 valid loss: 1.8319, valid accuracy: 0.5118\n",
      "Iter-54400 train loss: 1.8307 valid loss: 1.8312, valid accuracy: 0.5116\n",
      "Iter-54500 train loss: 1.9107 valid loss: 1.8306, valid accuracy: 0.5110\n",
      "Iter-54600 train loss: 1.9618 valid loss: 1.8299, valid accuracy: 0.5114\n",
      "Iter-54700 train loss: 1.9537 valid loss: 1.8293, valid accuracy: 0.5110\n",
      "Iter-54800 train loss: 1.9020 valid loss: 1.8286, valid accuracy: 0.5112\n",
      "Iter-54900 train loss: 1.9168 valid loss: 1.8280, valid accuracy: 0.5110\n",
      "Iter-55000 train loss: 1.8525 valid loss: 1.8273, valid accuracy: 0.5114\n",
      "Iter-55100 train loss: 1.8727 valid loss: 1.8267, valid accuracy: 0.5116\n",
      "Iter-55200 train loss: 1.8562 valid loss: 1.8260, valid accuracy: 0.5114\n",
      "Iter-55300 train loss: 1.8078 valid loss: 1.8254, valid accuracy: 0.5120\n",
      "Iter-55400 train loss: 1.8719 valid loss: 1.8247, valid accuracy: 0.5120\n",
      "Iter-55500 train loss: 1.8051 valid loss: 1.8241, valid accuracy: 0.5114\n",
      "Iter-55600 train loss: 1.7842 valid loss: 1.8234, valid accuracy: 0.5112\n",
      "Iter-55700 train loss: 1.8030 valid loss: 1.8228, valid accuracy: 0.5120\n",
      "Iter-55800 train loss: 1.8435 valid loss: 1.8221, valid accuracy: 0.5124\n",
      "Iter-55900 train loss: 1.7494 valid loss: 1.8215, valid accuracy: 0.5124\n",
      "Iter-56000 train loss: 1.7588 valid loss: 1.8208, valid accuracy: 0.5120\n",
      "Iter-56100 train loss: 1.8453 valid loss: 1.8202, valid accuracy: 0.5118\n",
      "Iter-56200 train loss: 1.7331 valid loss: 1.8195, valid accuracy: 0.5126\n",
      "Iter-56300 train loss: 1.8019 valid loss: 1.8189, valid accuracy: 0.5126\n",
      "Iter-56400 train loss: 1.7550 valid loss: 1.8182, valid accuracy: 0.5124\n",
      "Iter-56500 train loss: 1.7784 valid loss: 1.8176, valid accuracy: 0.5124\n",
      "Iter-56600 train loss: 1.7927 valid loss: 1.8170, valid accuracy: 0.5130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 1.8259 valid loss: 1.8163, valid accuracy: 0.5130\n",
      "Iter-56800 train loss: 1.6985 valid loss: 1.8156, valid accuracy: 0.5126\n",
      "Iter-56900 train loss: 1.8382 valid loss: 1.8150, valid accuracy: 0.5134\n",
      "Iter-57000 train loss: 1.7917 valid loss: 1.8144, valid accuracy: 0.5132\n",
      "Iter-57100 train loss: 1.8342 valid loss: 1.8138, valid accuracy: 0.5142\n",
      "Iter-57200 train loss: 1.8786 valid loss: 1.8131, valid accuracy: 0.5140\n",
      "Iter-57300 train loss: 1.8374 valid loss: 1.8125, valid accuracy: 0.5148\n",
      "Iter-57400 train loss: 1.7723 valid loss: 1.8118, valid accuracy: 0.5148\n",
      "Iter-57500 train loss: 1.9050 valid loss: 1.8112, valid accuracy: 0.5152\n",
      "Iter-57600 train loss: 1.8699 valid loss: 1.8106, valid accuracy: 0.5158\n",
      "Iter-57700 train loss: 1.7802 valid loss: 1.8099, valid accuracy: 0.5158\n",
      "Iter-57800 train loss: 1.8211 valid loss: 1.8093, valid accuracy: 0.5172\n",
      "Iter-57900 train loss: 1.7496 valid loss: 1.8087, valid accuracy: 0.5176\n",
      "Iter-58000 train loss: 1.7706 valid loss: 1.8080, valid accuracy: 0.5176\n",
      "Iter-58100 train loss: 1.7992 valid loss: 1.8074, valid accuracy: 0.5180\n",
      "Iter-58200 train loss: 1.7510 valid loss: 1.8068, valid accuracy: 0.5182\n",
      "Iter-58300 train loss: 1.8295 valid loss: 1.8061, valid accuracy: 0.5180\n",
      "Iter-58400 train loss: 1.7568 valid loss: 1.8055, valid accuracy: 0.5182\n",
      "Iter-58500 train loss: 1.7585 valid loss: 1.8049, valid accuracy: 0.5188\n",
      "Iter-58600 train loss: 1.7665 valid loss: 1.8043, valid accuracy: 0.5194\n",
      "Iter-58700 train loss: 1.7560 valid loss: 1.8036, valid accuracy: 0.5196\n",
      "Iter-58800 train loss: 1.8169 valid loss: 1.8030, valid accuracy: 0.5202\n",
      "Iter-58900 train loss: 1.8379 valid loss: 1.8024, valid accuracy: 0.5206\n",
      "Iter-59000 train loss: 1.8217 valid loss: 1.8017, valid accuracy: 0.5202\n",
      "Iter-59100 train loss: 1.7998 valid loss: 1.8011, valid accuracy: 0.5208\n",
      "Iter-59200 train loss: 1.8032 valid loss: 1.8005, valid accuracy: 0.5200\n",
      "Iter-59300 train loss: 1.9116 valid loss: 1.7999, valid accuracy: 0.5212\n",
      "Iter-59400 train loss: 1.7345 valid loss: 1.7993, valid accuracy: 0.5212\n",
      "Iter-59500 train loss: 1.8408 valid loss: 1.7986, valid accuracy: 0.5214\n",
      "Iter-59600 train loss: 1.9065 valid loss: 1.7980, valid accuracy: 0.5214\n",
      "Iter-59700 train loss: 1.7737 valid loss: 1.7974, valid accuracy: 0.5214\n",
      "Iter-59800 train loss: 1.8846 valid loss: 1.7968, valid accuracy: 0.5216\n",
      "Iter-59900 train loss: 1.8995 valid loss: 1.7962, valid accuracy: 0.5216\n",
      "Iter-60000 train loss: 1.7613 valid loss: 1.7956, valid accuracy: 0.5214\n",
      "Iter-60100 train loss: 1.8595 valid loss: 1.7950, valid accuracy: 0.5214\n",
      "Iter-60200 train loss: 1.7921 valid loss: 1.7944, valid accuracy: 0.5222\n",
      "Iter-60300 train loss: 1.7893 valid loss: 1.7937, valid accuracy: 0.5218\n",
      "Iter-60400 train loss: 1.8120 valid loss: 1.7931, valid accuracy: 0.5222\n",
      "Iter-60500 train loss: 1.7993 valid loss: 1.7925, valid accuracy: 0.5220\n",
      "Iter-60600 train loss: 1.6956 valid loss: 1.7919, valid accuracy: 0.5226\n",
      "Iter-60700 train loss: 1.7796 valid loss: 1.7912, valid accuracy: 0.5226\n",
      "Iter-60800 train loss: 1.8526 valid loss: 1.7906, valid accuracy: 0.5228\n",
      "Iter-60900 train loss: 1.8004 valid loss: 1.7900, valid accuracy: 0.5224\n",
      "Iter-61000 train loss: 1.6745 valid loss: 1.7894, valid accuracy: 0.5226\n",
      "Iter-61100 train loss: 1.8365 valid loss: 1.7888, valid accuracy: 0.5222\n",
      "Iter-61200 train loss: 1.7898 valid loss: 1.7881, valid accuracy: 0.5226\n",
      "Iter-61300 train loss: 1.8071 valid loss: 1.7875, valid accuracy: 0.5228\n",
      "Iter-61400 train loss: 1.8215 valid loss: 1.7869, valid accuracy: 0.5226\n",
      "Iter-61500 train loss: 1.8112 valid loss: 1.7863, valid accuracy: 0.5230\n",
      "Iter-61600 train loss: 1.7797 valid loss: 1.7857, valid accuracy: 0.5232\n",
      "Iter-61700 train loss: 1.7335 valid loss: 1.7851, valid accuracy: 0.5224\n",
      "Iter-61800 train loss: 1.7341 valid loss: 1.7845, valid accuracy: 0.5234\n",
      "Iter-61900 train loss: 1.9035 valid loss: 1.7839, valid accuracy: 0.5236\n",
      "Iter-62000 train loss: 1.5889 valid loss: 1.7833, valid accuracy: 0.5232\n",
      "Iter-62100 train loss: 1.7832 valid loss: 1.7827, valid accuracy: 0.5228\n",
      "Iter-62200 train loss: 1.7832 valid loss: 1.7821, valid accuracy: 0.5238\n",
      "Iter-62300 train loss: 1.7172 valid loss: 1.7815, valid accuracy: 0.5234\n",
      "Iter-62400 train loss: 1.7919 valid loss: 1.7809, valid accuracy: 0.5238\n",
      "Iter-62500 train loss: 1.7380 valid loss: 1.7803, valid accuracy: 0.5238\n",
      "Iter-62600 train loss: 1.7671 valid loss: 1.7796, valid accuracy: 0.5238\n",
      "Iter-62700 train loss: 1.8055 valid loss: 1.7791, valid accuracy: 0.5232\n",
      "Iter-62800 train loss: 1.7719 valid loss: 1.7784, valid accuracy: 0.5240\n",
      "Iter-62900 train loss: 1.7549 valid loss: 1.7778, valid accuracy: 0.5242\n",
      "Iter-63000 train loss: 1.8245 valid loss: 1.7772, valid accuracy: 0.5252\n",
      "Iter-63100 train loss: 1.8015 valid loss: 1.7766, valid accuracy: 0.5246\n",
      "Iter-63200 train loss: 1.7528 valid loss: 1.7760, valid accuracy: 0.5244\n",
      "Iter-63300 train loss: 1.8372 valid loss: 1.7753, valid accuracy: 0.5252\n",
      "Iter-63400 train loss: 1.7761 valid loss: 1.7747, valid accuracy: 0.5246\n",
      "Iter-63500 train loss: 1.8182 valid loss: 1.7741, valid accuracy: 0.5248\n",
      "Iter-63600 train loss: 1.7638 valid loss: 1.7736, valid accuracy: 0.5250\n",
      "Iter-63700 train loss: 1.8338 valid loss: 1.7730, valid accuracy: 0.5246\n",
      "Iter-63800 train loss: 1.8228 valid loss: 1.7724, valid accuracy: 0.5252\n",
      "Iter-63900 train loss: 1.8004 valid loss: 1.7718, valid accuracy: 0.5252\n",
      "Iter-64000 train loss: 1.7047 valid loss: 1.7712, valid accuracy: 0.5252\n",
      "Iter-64100 train loss: 1.8015 valid loss: 1.7706, valid accuracy: 0.5256\n",
      "Iter-64200 train loss: 1.7303 valid loss: 1.7700, valid accuracy: 0.5252\n",
      "Iter-64300 train loss: 1.7951 valid loss: 1.7694, valid accuracy: 0.5250\n",
      "Iter-64400 train loss: 1.7601 valid loss: 1.7688, valid accuracy: 0.5256\n",
      "Iter-64500 train loss: 1.7701 valid loss: 1.7682, valid accuracy: 0.5256\n",
      "Iter-64600 train loss: 1.7766 valid loss: 1.7676, valid accuracy: 0.5254\n",
      "Iter-64700 train loss: 1.8321 valid loss: 1.7670, valid accuracy: 0.5254\n",
      "Iter-64800 train loss: 1.8172 valid loss: 1.7664, valid accuracy: 0.5252\n",
      "Iter-64900 train loss: 1.8919 valid loss: 1.7658, valid accuracy: 0.5254\n",
      "Iter-65000 train loss: 1.7767 valid loss: 1.7652, valid accuracy: 0.5258\n",
      "Iter-65100 train loss: 1.8126 valid loss: 1.7646, valid accuracy: 0.5260\n",
      "Iter-65200 train loss: 1.7748 valid loss: 1.7640, valid accuracy: 0.5254\n",
      "Iter-65300 train loss: 1.8085 valid loss: 1.7634, valid accuracy: 0.5254\n",
      "Iter-65400 train loss: 1.6762 valid loss: 1.7628, valid accuracy: 0.5258\n",
      "Iter-65500 train loss: 1.7107 valid loss: 1.7622, valid accuracy: 0.5262\n",
      "Iter-65600 train loss: 1.7465 valid loss: 1.7617, valid accuracy: 0.5262\n",
      "Iter-65700 train loss: 1.6826 valid loss: 1.7611, valid accuracy: 0.5264\n",
      "Iter-65800 train loss: 1.7104 valid loss: 1.7605, valid accuracy: 0.5266\n",
      "Iter-65900 train loss: 1.7004 valid loss: 1.7599, valid accuracy: 0.5264\n",
      "Iter-66000 train loss: 1.7174 valid loss: 1.7593, valid accuracy: 0.5270\n",
      "Iter-66100 train loss: 1.8369 valid loss: 1.7587, valid accuracy: 0.5276\n",
      "Iter-66200 train loss: 1.7688 valid loss: 1.7581, valid accuracy: 0.5274\n",
      "Iter-66300 train loss: 1.8538 valid loss: 1.7576, valid accuracy: 0.5280\n",
      "Iter-66400 train loss: 1.6962 valid loss: 1.7570, valid accuracy: 0.5278\n",
      "Iter-66500 train loss: 1.7092 valid loss: 1.7564, valid accuracy: 0.5280\n",
      "Iter-66600 train loss: 1.7751 valid loss: 1.7558, valid accuracy: 0.5280\n",
      "Iter-66700 train loss: 1.7160 valid loss: 1.7552, valid accuracy: 0.5278\n",
      "Iter-66800 train loss: 1.8502 valid loss: 1.7546, valid accuracy: 0.5278\n",
      "Iter-66900 train loss: 1.7003 valid loss: 1.7540, valid accuracy: 0.5284\n",
      "Iter-67000 train loss: 1.6979 valid loss: 1.7534, valid accuracy: 0.5282\n",
      "Iter-67100 train loss: 1.7986 valid loss: 1.7528, valid accuracy: 0.5286\n",
      "Iter-67200 train loss: 1.6664 valid loss: 1.7522, valid accuracy: 0.5282\n",
      "Iter-67300 train loss: 1.7142 valid loss: 1.7517, valid accuracy: 0.5286\n",
      "Iter-67400 train loss: 1.8281 valid loss: 1.7511, valid accuracy: 0.5290\n",
      "Iter-67500 train loss: 1.7341 valid loss: 1.7505, valid accuracy: 0.5292\n",
      "Iter-67600 train loss: 1.8756 valid loss: 1.7499, valid accuracy: 0.5294\n",
      "Iter-67700 train loss: 1.6583 valid loss: 1.7494, valid accuracy: 0.5292\n",
      "Iter-67800 train loss: 1.7186 valid loss: 1.7488, valid accuracy: 0.5298\n",
      "Iter-67900 train loss: 1.8391 valid loss: 1.7482, valid accuracy: 0.5296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 1.7301 valid loss: 1.7476, valid accuracy: 0.5290\n",
      "Iter-68100 train loss: 1.7162 valid loss: 1.7470, valid accuracy: 0.5294\n",
      "Iter-68200 train loss: 1.8594 valid loss: 1.7464, valid accuracy: 0.5292\n",
      "Iter-68300 train loss: 1.7353 valid loss: 1.7458, valid accuracy: 0.5294\n",
      "Iter-68400 train loss: 1.7444 valid loss: 1.7452, valid accuracy: 0.5286\n",
      "Iter-68500 train loss: 1.6184 valid loss: 1.7447, valid accuracy: 0.5284\n",
      "Iter-68600 train loss: 1.9659 valid loss: 1.7441, valid accuracy: 0.5284\n",
      "Iter-68700 train loss: 1.9950 valid loss: 1.7434, valid accuracy: 0.5292\n",
      "Iter-68800 train loss: 1.7950 valid loss: 1.7429, valid accuracy: 0.5288\n",
      "Iter-68900 train loss: 1.6963 valid loss: 1.7423, valid accuracy: 0.5288\n",
      "Iter-69000 train loss: 1.6925 valid loss: 1.7417, valid accuracy: 0.5292\n",
      "Iter-69100 train loss: 1.7196 valid loss: 1.7411, valid accuracy: 0.5296\n",
      "Iter-69200 train loss: 1.7823 valid loss: 1.7405, valid accuracy: 0.5294\n",
      "Iter-69300 train loss: 1.6013 valid loss: 1.7400, valid accuracy: 0.5298\n",
      "Iter-69400 train loss: 1.8309 valid loss: 1.7394, valid accuracy: 0.5298\n",
      "Iter-69500 train loss: 1.7416 valid loss: 1.7388, valid accuracy: 0.5300\n",
      "Iter-69600 train loss: 1.7819 valid loss: 1.7383, valid accuracy: 0.5298\n",
      "Iter-69700 train loss: 1.6496 valid loss: 1.7377, valid accuracy: 0.5304\n",
      "Iter-69800 train loss: 1.6961 valid loss: 1.7371, valid accuracy: 0.5296\n",
      "Iter-69900 train loss: 1.6883 valid loss: 1.7365, valid accuracy: 0.5300\n",
      "Iter-70000 train loss: 1.7107 valid loss: 1.7360, valid accuracy: 0.5294\n",
      "Iter-70100 train loss: 1.7548 valid loss: 1.7354, valid accuracy: 0.5300\n",
      "Iter-70200 train loss: 1.7167 valid loss: 1.7349, valid accuracy: 0.5300\n",
      "Iter-70300 train loss: 1.7651 valid loss: 1.7343, valid accuracy: 0.5302\n",
      "Iter-70400 train loss: 1.8746 valid loss: 1.7338, valid accuracy: 0.5306\n",
      "Iter-70500 train loss: 1.7988 valid loss: 1.7332, valid accuracy: 0.5302\n",
      "Iter-70600 train loss: 1.7577 valid loss: 1.7326, valid accuracy: 0.5306\n",
      "Iter-70700 train loss: 1.8281 valid loss: 1.7320, valid accuracy: 0.5302\n",
      "Iter-70800 train loss: 1.7465 valid loss: 1.7315, valid accuracy: 0.5298\n",
      "Iter-70900 train loss: 1.6235 valid loss: 1.7309, valid accuracy: 0.5298\n",
      "Iter-71000 train loss: 1.6246 valid loss: 1.7303, valid accuracy: 0.5314\n",
      "Iter-71100 train loss: 1.6981 valid loss: 1.7298, valid accuracy: 0.5318\n",
      "Iter-71200 train loss: 1.7113 valid loss: 1.7292, valid accuracy: 0.5318\n",
      "Iter-71300 train loss: 1.7088 valid loss: 1.7286, valid accuracy: 0.5322\n",
      "Iter-71400 train loss: 1.7460 valid loss: 1.7280, valid accuracy: 0.5324\n",
      "Iter-71500 train loss: 1.7839 valid loss: 1.7275, valid accuracy: 0.5320\n",
      "Iter-71600 train loss: 1.7404 valid loss: 1.7269, valid accuracy: 0.5322\n",
      "Iter-71700 train loss: 1.7558 valid loss: 1.7263, valid accuracy: 0.5322\n",
      "Iter-71800 train loss: 1.6685 valid loss: 1.7258, valid accuracy: 0.5322\n",
      "Iter-71900 train loss: 1.6759 valid loss: 1.7252, valid accuracy: 0.5322\n",
      "Iter-72000 train loss: 1.8469 valid loss: 1.7247, valid accuracy: 0.5326\n",
      "Iter-72100 train loss: 1.7651 valid loss: 1.7241, valid accuracy: 0.5324\n",
      "Iter-72200 train loss: 1.8256 valid loss: 1.7235, valid accuracy: 0.5318\n",
      "Iter-72300 train loss: 1.8243 valid loss: 1.7230, valid accuracy: 0.5330\n",
      "Iter-72400 train loss: 1.7262 valid loss: 1.7224, valid accuracy: 0.5322\n",
      "Iter-72500 train loss: 1.7805 valid loss: 1.7218, valid accuracy: 0.5332\n",
      "Iter-72600 train loss: 1.6248 valid loss: 1.7212, valid accuracy: 0.5332\n",
      "Iter-72700 train loss: 1.7422 valid loss: 1.7207, valid accuracy: 0.5334\n",
      "Iter-72800 train loss: 1.8215 valid loss: 1.7201, valid accuracy: 0.5332\n",
      "Iter-72900 train loss: 1.6307 valid loss: 1.7196, valid accuracy: 0.5332\n",
      "Iter-73000 train loss: 1.6324 valid loss: 1.7190, valid accuracy: 0.5330\n",
      "Iter-73100 train loss: 1.7944 valid loss: 1.7184, valid accuracy: 0.5336\n",
      "Iter-73200 train loss: 1.7095 valid loss: 1.7179, valid accuracy: 0.5340\n",
      "Iter-73300 train loss: 1.6971 valid loss: 1.7173, valid accuracy: 0.5340\n",
      "Iter-73400 train loss: 1.7812 valid loss: 1.7167, valid accuracy: 0.5342\n",
      "Iter-73500 train loss: 1.7116 valid loss: 1.7162, valid accuracy: 0.5338\n",
      "Iter-73600 train loss: 1.8098 valid loss: 1.7157, valid accuracy: 0.5336\n",
      "Iter-73700 train loss: 1.5897 valid loss: 1.7151, valid accuracy: 0.5338\n",
      "Iter-73800 train loss: 1.6225 valid loss: 1.7146, valid accuracy: 0.5338\n",
      "Iter-73900 train loss: 1.8098 valid loss: 1.7140, valid accuracy: 0.5340\n",
      "Iter-74000 train loss: 1.7666 valid loss: 1.7135, valid accuracy: 0.5344\n",
      "Iter-74100 train loss: 1.5781 valid loss: 1.7130, valid accuracy: 0.5344\n",
      "Iter-74200 train loss: 1.6536 valid loss: 1.7124, valid accuracy: 0.5342\n",
      "Iter-74300 train loss: 1.6980 valid loss: 1.7119, valid accuracy: 0.5338\n",
      "Iter-74400 train loss: 1.6905 valid loss: 1.7113, valid accuracy: 0.5342\n",
      "Iter-74500 train loss: 1.7417 valid loss: 1.7107, valid accuracy: 0.5338\n",
      "Iter-74600 train loss: 1.7610 valid loss: 1.7102, valid accuracy: 0.5348\n",
      "Iter-74700 train loss: 1.7685 valid loss: 1.7097, valid accuracy: 0.5348\n",
      "Iter-74800 train loss: 1.6725 valid loss: 1.7092, valid accuracy: 0.5350\n",
      "Iter-74900 train loss: 1.7313 valid loss: 1.7086, valid accuracy: 0.5344\n",
      "Iter-75000 train loss: 1.6825 valid loss: 1.7080, valid accuracy: 0.5348\n",
      "Iter-75100 train loss: 1.7775 valid loss: 1.7075, valid accuracy: 0.5348\n",
      "Iter-75200 train loss: 1.6778 valid loss: 1.7069, valid accuracy: 0.5350\n",
      "Iter-75300 train loss: 1.5754 valid loss: 1.7064, valid accuracy: 0.5354\n",
      "Iter-75400 train loss: 1.7182 valid loss: 1.7058, valid accuracy: 0.5354\n",
      "Iter-75500 train loss: 1.6793 valid loss: 1.7053, valid accuracy: 0.5348\n",
      "Iter-75600 train loss: 1.7013 valid loss: 1.7047, valid accuracy: 0.5352\n",
      "Iter-75700 train loss: 1.7684 valid loss: 1.7042, valid accuracy: 0.5356\n",
      "Iter-75800 train loss: 1.5979 valid loss: 1.7036, valid accuracy: 0.5356\n",
      "Iter-75900 train loss: 1.7483 valid loss: 1.7031, valid accuracy: 0.5356\n",
      "Iter-76000 train loss: 1.7232 valid loss: 1.7025, valid accuracy: 0.5358\n",
      "Iter-76100 train loss: 1.8398 valid loss: 1.7020, valid accuracy: 0.5358\n",
      "Iter-76200 train loss: 1.7734 valid loss: 1.7015, valid accuracy: 0.5366\n",
      "Iter-76300 train loss: 1.7117 valid loss: 1.7009, valid accuracy: 0.5374\n",
      "Iter-76400 train loss: 1.6336 valid loss: 1.7004, valid accuracy: 0.5374\n",
      "Iter-76500 train loss: 1.7533 valid loss: 1.6998, valid accuracy: 0.5370\n",
      "Iter-76600 train loss: 1.6249 valid loss: 1.6993, valid accuracy: 0.5374\n",
      "Iter-76700 train loss: 1.6901 valid loss: 1.6987, valid accuracy: 0.5376\n",
      "Iter-76800 train loss: 1.7291 valid loss: 1.6982, valid accuracy: 0.5374\n",
      "Iter-76900 train loss: 1.6516 valid loss: 1.6977, valid accuracy: 0.5370\n",
      "Iter-77000 train loss: 1.6139 valid loss: 1.6971, valid accuracy: 0.5370\n",
      "Iter-77100 train loss: 1.7051 valid loss: 1.6965, valid accuracy: 0.5374\n",
      "Iter-77200 train loss: 1.7359 valid loss: 1.6960, valid accuracy: 0.5378\n",
      "Iter-77300 train loss: 1.6252 valid loss: 1.6955, valid accuracy: 0.5378\n",
      "Iter-77400 train loss: 1.7144 valid loss: 1.6949, valid accuracy: 0.5376\n",
      "Iter-77500 train loss: 1.7757 valid loss: 1.6944, valid accuracy: 0.5376\n",
      "Iter-77600 train loss: 1.7103 valid loss: 1.6939, valid accuracy: 0.5364\n",
      "Iter-77700 train loss: 1.6335 valid loss: 1.6934, valid accuracy: 0.5366\n",
      "Iter-77800 train loss: 1.8144 valid loss: 1.6928, valid accuracy: 0.5374\n",
      "Iter-77900 train loss: 1.9772 valid loss: 1.6923, valid accuracy: 0.5366\n",
      "Iter-78000 train loss: 1.6748 valid loss: 1.6918, valid accuracy: 0.5370\n",
      "Iter-78100 train loss: 1.6586 valid loss: 1.6913, valid accuracy: 0.5372\n",
      "Iter-78200 train loss: 1.6528 valid loss: 1.6907, valid accuracy: 0.5378\n",
      "Iter-78300 train loss: 1.7631 valid loss: 1.6901, valid accuracy: 0.5388\n",
      "Iter-78400 train loss: 1.6373 valid loss: 1.6896, valid accuracy: 0.5382\n",
      "Iter-78500 train loss: 1.8389 valid loss: 1.6890, valid accuracy: 0.5382\n",
      "Iter-78600 train loss: 1.6313 valid loss: 1.6885, valid accuracy: 0.5390\n",
      "Iter-78700 train loss: 1.6545 valid loss: 1.6880, valid accuracy: 0.5386\n",
      "Iter-78800 train loss: 1.6979 valid loss: 1.6874, valid accuracy: 0.5384\n",
      "Iter-78900 train loss: 1.6609 valid loss: 1.6868, valid accuracy: 0.5388\n",
      "Iter-79000 train loss: 1.8437 valid loss: 1.6863, valid accuracy: 0.5386\n",
      "Iter-79100 train loss: 1.6173 valid loss: 1.6857, valid accuracy: 0.5388\n",
      "Iter-79200 train loss: 1.5693 valid loss: 1.6852, valid accuracy: 0.5394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 1.6351 valid loss: 1.6847, valid accuracy: 0.5394\n",
      "Iter-79400 train loss: 1.6662 valid loss: 1.6842, valid accuracy: 0.5396\n",
      "Iter-79500 train loss: 1.6797 valid loss: 1.6836, valid accuracy: 0.5390\n",
      "Iter-79600 train loss: 1.5986 valid loss: 1.6831, valid accuracy: 0.5396\n",
      "Iter-79700 train loss: 1.6612 valid loss: 1.6826, valid accuracy: 0.5398\n",
      "Iter-79800 train loss: 1.7280 valid loss: 1.6821, valid accuracy: 0.5396\n",
      "Iter-79900 train loss: 1.6789 valid loss: 1.6816, valid accuracy: 0.5396\n",
      "Iter-80000 train loss: 1.7156 valid loss: 1.6811, valid accuracy: 0.5402\n",
      "Iter-80100 train loss: 1.6420 valid loss: 1.6806, valid accuracy: 0.5400\n",
      "Iter-80200 train loss: 1.5138 valid loss: 1.6801, valid accuracy: 0.5400\n",
      "Iter-80300 train loss: 1.7009 valid loss: 1.6795, valid accuracy: 0.5408\n",
      "Iter-80400 train loss: 1.6541 valid loss: 1.6790, valid accuracy: 0.5408\n",
      "Iter-80500 train loss: 1.7597 valid loss: 1.6784, valid accuracy: 0.5408\n",
      "Iter-80600 train loss: 1.7574 valid loss: 1.6779, valid accuracy: 0.5404\n",
      "Iter-80700 train loss: 1.7312 valid loss: 1.6774, valid accuracy: 0.5408\n",
      "Iter-80800 train loss: 1.7603 valid loss: 1.6768, valid accuracy: 0.5408\n",
      "Iter-80900 train loss: 1.7345 valid loss: 1.6763, valid accuracy: 0.5406\n",
      "Iter-81000 train loss: 1.6974 valid loss: 1.6758, valid accuracy: 0.5410\n",
      "Iter-81100 train loss: 1.5966 valid loss: 1.6752, valid accuracy: 0.5408\n",
      "Iter-81200 train loss: 1.7010 valid loss: 1.6747, valid accuracy: 0.5416\n",
      "Iter-81300 train loss: 1.6085 valid loss: 1.6742, valid accuracy: 0.5416\n",
      "Iter-81400 train loss: 1.6711 valid loss: 1.6737, valid accuracy: 0.5416\n",
      "Iter-81500 train loss: 1.7707 valid loss: 1.6731, valid accuracy: 0.5416\n",
      "Iter-81600 train loss: 1.5812 valid loss: 1.6726, valid accuracy: 0.5414\n",
      "Iter-81700 train loss: 1.6941 valid loss: 1.6721, valid accuracy: 0.5414\n",
      "Iter-81800 train loss: 1.7510 valid loss: 1.6716, valid accuracy: 0.5414\n",
      "Iter-81900 train loss: 1.7164 valid loss: 1.6711, valid accuracy: 0.5416\n",
      "Iter-82000 train loss: 1.6833 valid loss: 1.6706, valid accuracy: 0.5420\n",
      "Iter-82100 train loss: 1.8166 valid loss: 1.6700, valid accuracy: 0.5422\n",
      "Iter-82200 train loss: 1.5032 valid loss: 1.6695, valid accuracy: 0.5422\n",
      "Iter-82300 train loss: 1.8025 valid loss: 1.6689, valid accuracy: 0.5418\n",
      "Iter-82400 train loss: 1.7216 valid loss: 1.6684, valid accuracy: 0.5418\n",
      "Iter-82500 train loss: 1.6721 valid loss: 1.6679, valid accuracy: 0.5418\n",
      "Iter-82600 train loss: 1.8039 valid loss: 1.6674, valid accuracy: 0.5418\n",
      "Iter-82700 train loss: 1.7049 valid loss: 1.6669, valid accuracy: 0.5418\n",
      "Iter-82800 train loss: 1.6468 valid loss: 1.6663, valid accuracy: 0.5420\n",
      "Iter-82900 train loss: 1.5130 valid loss: 1.6658, valid accuracy: 0.5422\n",
      "Iter-83000 train loss: 1.6405 valid loss: 1.6653, valid accuracy: 0.5422\n",
      "Iter-83100 train loss: 1.6932 valid loss: 1.6648, valid accuracy: 0.5426\n",
      "Iter-83200 train loss: 1.7074 valid loss: 1.6643, valid accuracy: 0.5428\n",
      "Iter-83300 train loss: 1.6891 valid loss: 1.6638, valid accuracy: 0.5430\n",
      "Iter-83400 train loss: 1.5717 valid loss: 1.6633, valid accuracy: 0.5430\n",
      "Iter-83500 train loss: 1.6360 valid loss: 1.6627, valid accuracy: 0.5428\n",
      "Iter-83600 train loss: 1.7264 valid loss: 1.6622, valid accuracy: 0.5424\n",
      "Iter-83700 train loss: 1.7045 valid loss: 1.6617, valid accuracy: 0.5424\n",
      "Iter-83800 train loss: 1.6657 valid loss: 1.6612, valid accuracy: 0.5430\n",
      "Iter-83900 train loss: 1.5968 valid loss: 1.6607, valid accuracy: 0.5434\n",
      "Iter-84000 train loss: 1.6909 valid loss: 1.6602, valid accuracy: 0.5428\n",
      "Iter-84100 train loss: 1.6077 valid loss: 1.6597, valid accuracy: 0.5434\n",
      "Iter-84200 train loss: 1.7174 valid loss: 1.6591, valid accuracy: 0.5432\n",
      "Iter-84300 train loss: 1.5441 valid loss: 1.6587, valid accuracy: 0.5430\n",
      "Iter-84400 train loss: 1.8058 valid loss: 1.6582, valid accuracy: 0.5432\n",
      "Iter-84500 train loss: 1.5670 valid loss: 1.6577, valid accuracy: 0.5428\n",
      "Iter-84600 train loss: 1.7090 valid loss: 1.6572, valid accuracy: 0.5432\n",
      "Iter-84700 train loss: 1.5941 valid loss: 1.6567, valid accuracy: 0.5428\n",
      "Iter-84800 train loss: 1.6146 valid loss: 1.6562, valid accuracy: 0.5428\n",
      "Iter-84900 train loss: 1.6240 valid loss: 1.6556, valid accuracy: 0.5432\n",
      "Iter-85000 train loss: 1.6691 valid loss: 1.6552, valid accuracy: 0.5434\n",
      "Iter-85100 train loss: 1.5976 valid loss: 1.6547, valid accuracy: 0.5434\n",
      "Iter-85200 train loss: 1.6762 valid loss: 1.6542, valid accuracy: 0.5434\n",
      "Iter-85300 train loss: 1.5734 valid loss: 1.6537, valid accuracy: 0.5438\n",
      "Iter-85400 train loss: 1.6192 valid loss: 1.6532, valid accuracy: 0.5442\n",
      "Iter-85500 train loss: 1.7022 valid loss: 1.6527, valid accuracy: 0.5438\n",
      "Iter-85600 train loss: 1.6683 valid loss: 1.6522, valid accuracy: 0.5436\n",
      "Iter-85700 train loss: 1.8254 valid loss: 1.6517, valid accuracy: 0.5438\n",
      "Iter-85800 train loss: 1.6571 valid loss: 1.6512, valid accuracy: 0.5440\n",
      "Iter-85900 train loss: 1.7021 valid loss: 1.6507, valid accuracy: 0.5442\n",
      "Iter-86000 train loss: 1.7146 valid loss: 1.6502, valid accuracy: 0.5446\n",
      "Iter-86100 train loss: 1.7077 valid loss: 1.6497, valid accuracy: 0.5450\n",
      "Iter-86200 train loss: 1.5525 valid loss: 1.6492, valid accuracy: 0.5446\n",
      "Iter-86300 train loss: 1.7789 valid loss: 1.6486, valid accuracy: 0.5444\n",
      "Iter-86400 train loss: 1.7848 valid loss: 1.6482, valid accuracy: 0.5442\n",
      "Iter-86500 train loss: 1.8347 valid loss: 1.6477, valid accuracy: 0.5442\n",
      "Iter-86600 train loss: 1.5364 valid loss: 1.6472, valid accuracy: 0.5444\n",
      "Iter-86700 train loss: 1.5173 valid loss: 1.6467, valid accuracy: 0.5440\n",
      "Iter-86800 train loss: 1.7385 valid loss: 1.6462, valid accuracy: 0.5444\n",
      "Iter-86900 train loss: 1.6953 valid loss: 1.6456, valid accuracy: 0.5438\n",
      "Iter-87000 train loss: 1.6714 valid loss: 1.6451, valid accuracy: 0.5444\n",
      "Iter-87100 train loss: 1.6646 valid loss: 1.6445, valid accuracy: 0.5442\n",
      "Iter-87200 train loss: 1.5288 valid loss: 1.6441, valid accuracy: 0.5438\n",
      "Iter-87300 train loss: 1.5616 valid loss: 1.6436, valid accuracy: 0.5444\n",
      "Iter-87400 train loss: 1.7864 valid loss: 1.6431, valid accuracy: 0.5442\n",
      "Iter-87500 train loss: 1.6741 valid loss: 1.6426, valid accuracy: 0.5442\n",
      "Iter-87600 train loss: 1.4671 valid loss: 1.6421, valid accuracy: 0.5442\n",
      "Iter-87700 train loss: 1.6586 valid loss: 1.6416, valid accuracy: 0.5438\n",
      "Iter-87800 train loss: 1.6107 valid loss: 1.6411, valid accuracy: 0.5452\n",
      "Iter-87900 train loss: 1.5939 valid loss: 1.6406, valid accuracy: 0.5448\n",
      "Iter-88000 train loss: 1.6654 valid loss: 1.6401, valid accuracy: 0.5444\n",
      "Iter-88100 train loss: 1.7309 valid loss: 1.6395, valid accuracy: 0.5444\n",
      "Iter-88200 train loss: 1.7600 valid loss: 1.6390, valid accuracy: 0.5442\n",
      "Iter-88300 train loss: 1.8062 valid loss: 1.6385, valid accuracy: 0.5448\n",
      "Iter-88400 train loss: 1.5117 valid loss: 1.6380, valid accuracy: 0.5450\n",
      "Iter-88500 train loss: 1.6524 valid loss: 1.6375, valid accuracy: 0.5452\n",
      "Iter-88600 train loss: 1.7027 valid loss: 1.6370, valid accuracy: 0.5458\n",
      "Iter-88700 train loss: 1.6506 valid loss: 1.6365, valid accuracy: 0.5456\n",
      "Iter-88800 train loss: 1.6671 valid loss: 1.6360, valid accuracy: 0.5450\n",
      "Iter-88900 train loss: 1.4746 valid loss: 1.6355, valid accuracy: 0.5458\n",
      "Iter-89000 train loss: 1.6501 valid loss: 1.6350, valid accuracy: 0.5462\n",
      "Iter-89100 train loss: 1.6511 valid loss: 1.6345, valid accuracy: 0.5452\n",
      "Iter-89200 train loss: 1.6177 valid loss: 1.6340, valid accuracy: 0.5456\n",
      "Iter-89300 train loss: 1.6659 valid loss: 1.6335, valid accuracy: 0.5458\n",
      "Iter-89400 train loss: 1.7067 valid loss: 1.6330, valid accuracy: 0.5458\n",
      "Iter-89500 train loss: 1.6553 valid loss: 1.6325, valid accuracy: 0.5456\n",
      "Iter-89600 train loss: 1.5614 valid loss: 1.6320, valid accuracy: 0.5458\n",
      "Iter-89700 train loss: 1.5839 valid loss: 1.6315, valid accuracy: 0.5462\n",
      "Iter-89800 train loss: 1.6348 valid loss: 1.6310, valid accuracy: 0.5464\n",
      "Iter-89900 train loss: 1.6696 valid loss: 1.6305, valid accuracy: 0.5464\n",
      "Iter-90000 train loss: 1.7343 valid loss: 1.6300, valid accuracy: 0.5462\n",
      "Iter-90100 train loss: 1.7146 valid loss: 1.6295, valid accuracy: 0.5470\n",
      "Iter-90200 train loss: 1.5533 valid loss: 1.6290, valid accuracy: 0.5470\n",
      "Iter-90300 train loss: 1.6596 valid loss: 1.6285, valid accuracy: 0.5464\n",
      "Iter-90400 train loss: 1.6670 valid loss: 1.6281, valid accuracy: 0.5464\n",
      "Iter-90500 train loss: 1.6328 valid loss: 1.6276, valid accuracy: 0.5466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 1.5047 valid loss: 1.6271, valid accuracy: 0.5466\n",
      "Iter-90700 train loss: 1.6197 valid loss: 1.6265, valid accuracy: 0.5462\n",
      "Iter-90800 train loss: 1.6976 valid loss: 1.6261, valid accuracy: 0.5460\n",
      "Iter-90900 train loss: 1.6290 valid loss: 1.6256, valid accuracy: 0.5464\n",
      "Iter-91000 train loss: 1.7106 valid loss: 1.6251, valid accuracy: 0.5466\n",
      "Iter-91100 train loss: 1.4722 valid loss: 1.6246, valid accuracy: 0.5468\n",
      "Iter-91200 train loss: 1.6737 valid loss: 1.6241, valid accuracy: 0.5466\n",
      "Iter-91300 train loss: 1.6617 valid loss: 1.6236, valid accuracy: 0.5468\n",
      "Iter-91400 train loss: 1.6113 valid loss: 1.6232, valid accuracy: 0.5476\n",
      "Iter-91500 train loss: 1.6325 valid loss: 1.6227, valid accuracy: 0.5474\n",
      "Iter-91600 train loss: 1.7192 valid loss: 1.6222, valid accuracy: 0.5476\n",
      "Iter-91700 train loss: 1.6256 valid loss: 1.6218, valid accuracy: 0.5474\n",
      "Iter-91800 train loss: 1.6559 valid loss: 1.6213, valid accuracy: 0.5472\n",
      "Iter-91900 train loss: 1.6291 valid loss: 1.6207, valid accuracy: 0.5466\n",
      "Iter-92000 train loss: 1.7055 valid loss: 1.6203, valid accuracy: 0.5476\n",
      "Iter-92100 train loss: 1.5541 valid loss: 1.6198, valid accuracy: 0.5472\n",
      "Iter-92200 train loss: 1.5518 valid loss: 1.6193, valid accuracy: 0.5478\n",
      "Iter-92300 train loss: 1.5027 valid loss: 1.6188, valid accuracy: 0.5476\n",
      "Iter-92400 train loss: 1.5998 valid loss: 1.6183, valid accuracy: 0.5476\n",
      "Iter-92500 train loss: 1.5594 valid loss: 1.6178, valid accuracy: 0.5480\n",
      "Iter-92600 train loss: 1.6572 valid loss: 1.6173, valid accuracy: 0.5478\n",
      "Iter-92700 train loss: 1.6358 valid loss: 1.6168, valid accuracy: 0.5472\n",
      "Iter-92800 train loss: 1.7306 valid loss: 1.6164, valid accuracy: 0.5478\n",
      "Iter-92900 train loss: 1.6890 valid loss: 1.6159, valid accuracy: 0.5476\n",
      "Iter-93000 train loss: 1.5500 valid loss: 1.6154, valid accuracy: 0.5476\n",
      "Iter-93100 train loss: 1.4770 valid loss: 1.6149, valid accuracy: 0.5480\n",
      "Iter-93200 train loss: 1.7090 valid loss: 1.6145, valid accuracy: 0.5484\n",
      "Iter-93300 train loss: 1.7183 valid loss: 1.6140, valid accuracy: 0.5480\n",
      "Iter-93400 train loss: 1.6505 valid loss: 1.6135, valid accuracy: 0.5482\n",
      "Iter-93500 train loss: 1.6723 valid loss: 1.6131, valid accuracy: 0.5484\n",
      "Iter-93600 train loss: 1.6327 valid loss: 1.6126, valid accuracy: 0.5486\n",
      "Iter-93700 train loss: 1.6368 valid loss: 1.6121, valid accuracy: 0.5490\n",
      "Iter-93800 train loss: 1.6309 valid loss: 1.6117, valid accuracy: 0.5488\n",
      "Iter-93900 train loss: 1.5788 valid loss: 1.6112, valid accuracy: 0.5496\n",
      "Iter-94000 train loss: 1.6319 valid loss: 1.6107, valid accuracy: 0.5496\n",
      "Iter-94100 train loss: 1.6609 valid loss: 1.6102, valid accuracy: 0.5496\n",
      "Iter-94200 train loss: 1.5654 valid loss: 1.6097, valid accuracy: 0.5496\n",
      "Iter-94300 train loss: 1.5747 valid loss: 1.6092, valid accuracy: 0.5502\n",
      "Iter-94400 train loss: 1.6117 valid loss: 1.6088, valid accuracy: 0.5500\n",
      "Iter-94500 train loss: 1.5717 valid loss: 1.6083, valid accuracy: 0.5504\n",
      "Iter-94600 train loss: 1.6472 valid loss: 1.6078, valid accuracy: 0.5506\n",
      "Iter-94700 train loss: 1.5764 valid loss: 1.6073, valid accuracy: 0.5502\n",
      "Iter-94800 train loss: 1.6998 valid loss: 1.6069, valid accuracy: 0.5506\n",
      "Iter-94900 train loss: 1.5432 valid loss: 1.6064, valid accuracy: 0.5510\n",
      "Iter-95000 train loss: 1.5836 valid loss: 1.6059, valid accuracy: 0.5512\n",
      "Iter-95100 train loss: 1.5896 valid loss: 1.6055, valid accuracy: 0.5518\n",
      "Iter-95200 train loss: 1.5420 valid loss: 1.6050, valid accuracy: 0.5518\n",
      "Iter-95300 train loss: 1.6423 valid loss: 1.6046, valid accuracy: 0.5518\n",
      "Iter-95400 train loss: 1.6433 valid loss: 1.6041, valid accuracy: 0.5514\n",
      "Iter-95500 train loss: 1.5235 valid loss: 1.6036, valid accuracy: 0.5518\n",
      "Iter-95600 train loss: 1.5336 valid loss: 1.6031, valid accuracy: 0.5518\n",
      "Iter-95700 train loss: 1.5824 valid loss: 1.6026, valid accuracy: 0.5518\n",
      "Iter-95800 train loss: 1.7383 valid loss: 1.6022, valid accuracy: 0.5512\n",
      "Iter-95900 train loss: 1.5018 valid loss: 1.6017, valid accuracy: 0.5508\n",
      "Iter-96000 train loss: 1.7288 valid loss: 1.6012, valid accuracy: 0.5512\n",
      "Iter-96100 train loss: 1.6637 valid loss: 1.6007, valid accuracy: 0.5514\n",
      "Iter-96200 train loss: 1.5369 valid loss: 1.6003, valid accuracy: 0.5518\n",
      "Iter-96300 train loss: 1.6692 valid loss: 1.5998, valid accuracy: 0.5520\n",
      "Iter-96400 train loss: 1.5397 valid loss: 1.5992, valid accuracy: 0.5516\n",
      "Iter-96500 train loss: 1.6034 valid loss: 1.5987, valid accuracy: 0.5516\n",
      "Iter-96600 train loss: 1.6053 valid loss: 1.5983, valid accuracy: 0.5516\n",
      "Iter-96700 train loss: 1.6806 valid loss: 1.5978, valid accuracy: 0.5526\n",
      "Iter-96800 train loss: 1.4768 valid loss: 1.5973, valid accuracy: 0.5526\n",
      "Iter-96900 train loss: 1.6338 valid loss: 1.5969, valid accuracy: 0.5522\n",
      "Iter-97000 train loss: 1.6399 valid loss: 1.5964, valid accuracy: 0.5520\n",
      "Iter-97100 train loss: 1.6937 valid loss: 1.5959, valid accuracy: 0.5524\n",
      "Iter-97200 train loss: 1.6395 valid loss: 1.5954, valid accuracy: 0.5524\n",
      "Iter-97300 train loss: 1.6508 valid loss: 1.5950, valid accuracy: 0.5530\n",
      "Iter-97400 train loss: 1.5845 valid loss: 1.5946, valid accuracy: 0.5524\n",
      "Iter-97500 train loss: 1.6445 valid loss: 1.5941, valid accuracy: 0.5526\n",
      "Iter-97600 train loss: 1.5282 valid loss: 1.5936, valid accuracy: 0.5520\n",
      "Iter-97700 train loss: 1.7229 valid loss: 1.5932, valid accuracy: 0.5524\n",
      "Iter-97800 train loss: 1.7832 valid loss: 1.5927, valid accuracy: 0.5524\n",
      "Iter-97900 train loss: 1.5333 valid loss: 1.5923, valid accuracy: 0.5528\n",
      "Iter-98000 train loss: 1.5776 valid loss: 1.5918, valid accuracy: 0.5532\n",
      "Iter-98100 train loss: 1.5619 valid loss: 1.5914, valid accuracy: 0.5532\n",
      "Iter-98200 train loss: 1.5819 valid loss: 1.5909, valid accuracy: 0.5528\n",
      "Iter-98300 train loss: 1.6376 valid loss: 1.5904, valid accuracy: 0.5532\n",
      "Iter-98400 train loss: 1.5994 valid loss: 1.5900, valid accuracy: 0.5532\n",
      "Iter-98500 train loss: 1.5100 valid loss: 1.5895, valid accuracy: 0.5534\n",
      "Iter-98600 train loss: 1.4513 valid loss: 1.5891, valid accuracy: 0.5532\n",
      "Iter-98700 train loss: 1.4132 valid loss: 1.5886, valid accuracy: 0.5532\n",
      "Iter-98800 train loss: 1.6727 valid loss: 1.5881, valid accuracy: 0.5532\n",
      "Iter-98900 train loss: 1.6008 valid loss: 1.5877, valid accuracy: 0.5528\n",
      "Iter-99000 train loss: 1.6495 valid loss: 1.5873, valid accuracy: 0.5530\n",
      "Iter-99100 train loss: 1.5283 valid loss: 1.5868, valid accuracy: 0.5532\n",
      "Iter-99200 train loss: 1.6147 valid loss: 1.5863, valid accuracy: 0.5532\n",
      "Iter-99300 train loss: 1.7048 valid loss: 1.5859, valid accuracy: 0.5530\n",
      "Iter-99400 train loss: 1.6002 valid loss: 1.5854, valid accuracy: 0.5530\n",
      "Iter-99500 train loss: 1.5970 valid loss: 1.5849, valid accuracy: 0.5536\n",
      "Iter-99600 train loss: 1.6659 valid loss: 1.5845, valid accuracy: 0.5536\n",
      "Iter-99700 train loss: 1.6104 valid loss: 1.5840, valid accuracy: 0.5538\n",
      "Iter-99800 train loss: 1.4953 valid loss: 1.5836, valid accuracy: 0.5538\n",
      "Iter-99900 train loss: 1.7779 valid loss: 1.5831, valid accuracy: 0.5540\n",
      "Iter-100000 train loss: 1.6208 valid loss: 1.5827, valid accuracy: 0.5540\n",
      "Last iteration - Test accuracy mean: 0.5394, std: 0.0000, loss: 1.5781\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYQQNhIQhBBCAPD//Y4yB4Vu1QcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9262f8d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc9262f88d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
