{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = np.tanh(y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = np.tanh(y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3390 valid loss: 2.3261, valid accuracy: 0.0822\n",
      "Iter-200 train loss: 2.3115 valid loss: 2.3247, valid accuracy: 0.0848\n",
      "Iter-300 train loss: 2.3029 valid loss: 2.3233, valid accuracy: 0.0860\n",
      "Iter-400 train loss: 2.3123 valid loss: 2.3220, valid accuracy: 0.0860\n",
      "Iter-500 train loss: 2.3120 valid loss: 2.3206, valid accuracy: 0.0890\n",
      "Iter-600 train loss: 2.3063 valid loss: 2.3192, valid accuracy: 0.0898\n",
      "Iter-700 train loss: 2.3385 valid loss: 2.3179, valid accuracy: 0.0906\n",
      "Iter-800 train loss: 2.2998 valid loss: 2.3166, valid accuracy: 0.0920\n",
      "Iter-900 train loss: 2.3183 valid loss: 2.3152, valid accuracy: 0.0940\n",
      "Iter-1000 train loss: 2.3129 valid loss: 2.3139, valid accuracy: 0.0960\n",
      "Iter-1100 train loss: 2.3401 valid loss: 2.3125, valid accuracy: 0.0978\n",
      "Iter-1200 train loss: 2.3081 valid loss: 2.3112, valid accuracy: 0.1002\n",
      "Iter-1300 train loss: 2.3136 valid loss: 2.3098, valid accuracy: 0.1012\n",
      "Iter-1400 train loss: 2.3020 valid loss: 2.3085, valid accuracy: 0.1026\n",
      "Iter-1500 train loss: 2.3105 valid loss: 2.3072, valid accuracy: 0.1028\n",
      "Iter-1600 train loss: 2.2992 valid loss: 2.3059, valid accuracy: 0.1056\n",
      "Iter-1700 train loss: 2.2984 valid loss: 2.3046, valid accuracy: 0.1074\n",
      "Iter-1800 train loss: 2.3025 valid loss: 2.3032, valid accuracy: 0.1092\n",
      "Iter-1900 train loss: 2.2888 valid loss: 2.3019, valid accuracy: 0.1110\n",
      "Iter-2000 train loss: 2.3128 valid loss: 2.3006, valid accuracy: 0.1132\n",
      "Iter-2100 train loss: 2.2914 valid loss: 2.2993, valid accuracy: 0.1160\n",
      "Iter-2200 train loss: 2.3088 valid loss: 2.2980, valid accuracy: 0.1174\n",
      "Iter-2300 train loss: 2.3024 valid loss: 2.2967, valid accuracy: 0.1190\n",
      "Iter-2400 train loss: 2.3079 valid loss: 2.2954, valid accuracy: 0.1220\n",
      "Iter-2500 train loss: 2.2931 valid loss: 2.2941, valid accuracy: 0.1230\n",
      "Iter-2600 train loss: 2.3055 valid loss: 2.2928, valid accuracy: 0.1266\n",
      "Iter-2700 train loss: 2.2663 valid loss: 2.2915, valid accuracy: 0.1282\n",
      "Iter-2800 train loss: 2.3062 valid loss: 2.2902, valid accuracy: 0.1306\n",
      "Iter-2900 train loss: 2.2542 valid loss: 2.2889, valid accuracy: 0.1342\n",
      "Iter-3000 train loss: 2.2750 valid loss: 2.2877, valid accuracy: 0.1358\n",
      "Iter-3100 train loss: 2.2955 valid loss: 2.2864, valid accuracy: 0.1390\n",
      "Iter-3200 train loss: 2.2898 valid loss: 2.2851, valid accuracy: 0.1422\n",
      "Iter-3300 train loss: 2.2647 valid loss: 2.2838, valid accuracy: 0.1446\n",
      "Iter-3400 train loss: 2.2583 valid loss: 2.2825, valid accuracy: 0.1486\n",
      "Iter-3500 train loss: 2.2826 valid loss: 2.2813, valid accuracy: 0.1504\n",
      "Iter-3600 train loss: 2.2751 valid loss: 2.2800, valid accuracy: 0.1524\n",
      "Iter-3700 train loss: 2.2877 valid loss: 2.2787, valid accuracy: 0.1540\n",
      "Iter-3800 train loss: 2.2519 valid loss: 2.2775, valid accuracy: 0.1568\n",
      "Iter-3900 train loss: 2.2690 valid loss: 2.2762, valid accuracy: 0.1618\n",
      "Iter-4000 train loss: 2.2763 valid loss: 2.2749, valid accuracy: 0.1662\n",
      "Iter-4100 train loss: 2.2573 valid loss: 2.2737, valid accuracy: 0.1692\n",
      "Iter-4200 train loss: 2.2662 valid loss: 2.2724, valid accuracy: 0.1730\n",
      "Iter-4300 train loss: 2.2742 valid loss: 2.2712, valid accuracy: 0.1762\n",
      "Iter-4400 train loss: 2.2653 valid loss: 2.2699, valid accuracy: 0.1784\n",
      "Iter-4500 train loss: 2.2834 valid loss: 2.2686, valid accuracy: 0.1822\n",
      "Iter-4600 train loss: 2.2643 valid loss: 2.2674, valid accuracy: 0.1880\n",
      "Iter-4700 train loss: 2.2845 valid loss: 2.2662, valid accuracy: 0.1904\n",
      "Iter-4800 train loss: 2.2638 valid loss: 2.2649, valid accuracy: 0.1970\n",
      "Iter-4900 train loss: 2.2564 valid loss: 2.2636, valid accuracy: 0.2032\n",
      "Iter-5000 train loss: 2.2642 valid loss: 2.2624, valid accuracy: 0.2060\n",
      "Iter-5100 train loss: 2.2742 valid loss: 2.2612, valid accuracy: 0.2094\n",
      "Iter-5200 train loss: 2.2331 valid loss: 2.2599, valid accuracy: 0.2138\n",
      "Iter-5300 train loss: 2.2421 valid loss: 2.2587, valid accuracy: 0.2178\n",
      "Iter-5400 train loss: 2.2663 valid loss: 2.2575, valid accuracy: 0.2214\n",
      "Iter-5500 train loss: 2.2442 valid loss: 2.2562, valid accuracy: 0.2252\n",
      "Iter-5600 train loss: 2.2745 valid loss: 2.2550, valid accuracy: 0.2296\n",
      "Iter-5700 train loss: 2.2308 valid loss: 2.2538, valid accuracy: 0.2336\n",
      "Iter-5800 train loss: 2.2681 valid loss: 2.2526, valid accuracy: 0.2362\n",
      "Iter-5900 train loss: 2.2614 valid loss: 2.2514, valid accuracy: 0.2386\n",
      "Iter-6000 train loss: 2.2623 valid loss: 2.2502, valid accuracy: 0.2438\n",
      "Iter-6100 train loss: 2.2461 valid loss: 2.2489, valid accuracy: 0.2466\n",
      "Iter-6200 train loss: 2.2618 valid loss: 2.2477, valid accuracy: 0.2512\n",
      "Iter-6300 train loss: 2.2543 valid loss: 2.2465, valid accuracy: 0.2550\n",
      "Iter-6400 train loss: 2.2469 valid loss: 2.2453, valid accuracy: 0.2582\n",
      "Iter-6500 train loss: 2.2426 valid loss: 2.2441, valid accuracy: 0.2594\n",
      "Iter-6600 train loss: 2.2625 valid loss: 2.2429, valid accuracy: 0.2636\n",
      "Iter-6700 train loss: 2.2535 valid loss: 2.2417, valid accuracy: 0.2660\n",
      "Iter-6800 train loss: 2.2322 valid loss: 2.2405, valid accuracy: 0.2696\n",
      "Iter-6900 train loss: 2.2307 valid loss: 2.2393, valid accuracy: 0.2718\n",
      "Iter-7000 train loss: 2.2408 valid loss: 2.2381, valid accuracy: 0.2746\n",
      "Iter-7100 train loss: 2.2244 valid loss: 2.2369, valid accuracy: 0.2778\n",
      "Iter-7200 train loss: 2.2391 valid loss: 2.2357, valid accuracy: 0.2802\n",
      "Iter-7300 train loss: 2.2234 valid loss: 2.2345, valid accuracy: 0.2824\n",
      "Iter-7400 train loss: 2.2425 valid loss: 2.2333, valid accuracy: 0.2854\n",
      "Iter-7500 train loss: 2.2271 valid loss: 2.2322, valid accuracy: 0.2878\n",
      "Iter-7600 train loss: 2.2410 valid loss: 2.2310, valid accuracy: 0.2904\n",
      "Iter-7700 train loss: 2.2247 valid loss: 2.2298, valid accuracy: 0.2934\n",
      "Iter-7800 train loss: 2.2248 valid loss: 2.2286, valid accuracy: 0.2966\n",
      "Iter-7900 train loss: 2.2275 valid loss: 2.2274, valid accuracy: 0.2996\n",
      "Iter-8000 train loss: 2.2314 valid loss: 2.2263, valid accuracy: 0.3018\n",
      "Iter-8100 train loss: 2.2371 valid loss: 2.2251, valid accuracy: 0.3042\n",
      "Iter-8200 train loss: 2.2448 valid loss: 2.2239, valid accuracy: 0.3060\n",
      "Iter-8300 train loss: 2.2244 valid loss: 2.2228, valid accuracy: 0.3086\n",
      "Iter-8400 train loss: 2.2098 valid loss: 2.2216, valid accuracy: 0.3100\n",
      "Iter-8500 train loss: 2.2333 valid loss: 2.2204, valid accuracy: 0.3130\n",
      "Iter-8600 train loss: 2.2190 valid loss: 2.2193, valid accuracy: 0.3144\n",
      "Iter-8700 train loss: 2.2098 valid loss: 2.2182, valid accuracy: 0.3162\n",
      "Iter-8800 train loss: 2.2090 valid loss: 2.2170, valid accuracy: 0.3178\n",
      "Iter-8900 train loss: 2.1898 valid loss: 2.2159, valid accuracy: 0.3200\n",
      "Iter-9000 train loss: 2.2003 valid loss: 2.2147, valid accuracy: 0.3226\n",
      "Iter-9100 train loss: 2.2299 valid loss: 2.2135, valid accuracy: 0.3238\n",
      "Iter-9200 train loss: 2.2058 valid loss: 2.2124, valid accuracy: 0.3256\n",
      "Iter-9300 train loss: 2.2091 valid loss: 2.2112, valid accuracy: 0.3270\n",
      "Iter-9400 train loss: 2.2400 valid loss: 2.2101, valid accuracy: 0.3290\n",
      "Iter-9500 train loss: 2.2345 valid loss: 2.2090, valid accuracy: 0.3314\n",
      "Iter-9600 train loss: 2.1897 valid loss: 2.2078, valid accuracy: 0.3328\n",
      "Iter-9700 train loss: 2.2327 valid loss: 2.2067, valid accuracy: 0.3340\n",
      "Iter-9800 train loss: 2.2157 valid loss: 2.2055, valid accuracy: 0.3344\n",
      "Iter-9900 train loss: 2.1918 valid loss: 2.2044, valid accuracy: 0.3356\n",
      "Iter-10000 train loss: 2.1956 valid loss: 2.2033, valid accuracy: 0.3370\n",
      "Iter-10100 train loss: 2.2497 valid loss: 2.2021, valid accuracy: 0.3376\n",
      "Iter-10200 train loss: 2.2032 valid loss: 2.2010, valid accuracy: 0.3384\n",
      "Iter-10300 train loss: 2.1976 valid loss: 2.1999, valid accuracy: 0.3392\n",
      "Iter-10400 train loss: 2.1914 valid loss: 2.1987, valid accuracy: 0.3416\n",
      "Iter-10500 train loss: 2.1734 valid loss: 2.1976, valid accuracy: 0.3436\n",
      "Iter-10600 train loss: 2.1951 valid loss: 2.1965, valid accuracy: 0.3452\n",
      "Iter-10700 train loss: 2.1884 valid loss: 2.1953, valid accuracy: 0.3452\n",
      "Iter-10800 train loss: 2.1673 valid loss: 2.1942, valid accuracy: 0.3466\n",
      "Iter-10900 train loss: 2.2132 valid loss: 2.1931, valid accuracy: 0.3478\n",
      "Iter-11000 train loss: 2.1939 valid loss: 2.1920, valid accuracy: 0.3490\n",
      "Iter-11100 train loss: 2.1930 valid loss: 2.1909, valid accuracy: 0.3516\n",
      "Iter-11200 train loss: 2.1856 valid loss: 2.1898, valid accuracy: 0.3526\n",
      "Iter-11300 train loss: 2.1787 valid loss: 2.1887, valid accuracy: 0.3540\n",
      "Iter-11400 train loss: 2.1911 valid loss: 2.1876, valid accuracy: 0.3550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.1842 valid loss: 2.1865, valid accuracy: 0.3556\n",
      "Iter-11600 train loss: 2.1905 valid loss: 2.1854, valid accuracy: 0.3568\n",
      "Iter-11700 train loss: 2.1845 valid loss: 2.1843, valid accuracy: 0.3574\n",
      "Iter-11800 train loss: 2.2014 valid loss: 2.1832, valid accuracy: 0.3596\n",
      "Iter-11900 train loss: 2.1917 valid loss: 2.1822, valid accuracy: 0.3600\n",
      "Iter-12000 train loss: 2.1782 valid loss: 2.1811, valid accuracy: 0.3610\n",
      "Iter-12100 train loss: 2.1583 valid loss: 2.1800, valid accuracy: 0.3618\n",
      "Iter-12200 train loss: 2.2089 valid loss: 2.1789, valid accuracy: 0.3618\n",
      "Iter-12300 train loss: 2.1592 valid loss: 2.1778, valid accuracy: 0.3628\n",
      "Iter-12400 train loss: 2.2015 valid loss: 2.1767, valid accuracy: 0.3638\n",
      "Iter-12500 train loss: 2.1420 valid loss: 2.1757, valid accuracy: 0.3642\n",
      "Iter-12600 train loss: 2.1810 valid loss: 2.1745, valid accuracy: 0.3654\n",
      "Iter-12700 train loss: 2.2232 valid loss: 2.1735, valid accuracy: 0.3668\n",
      "Iter-12800 train loss: 2.2077 valid loss: 2.1724, valid accuracy: 0.3682\n",
      "Iter-12900 train loss: 2.1458 valid loss: 2.1713, valid accuracy: 0.3692\n",
      "Iter-13000 train loss: 2.1540 valid loss: 2.1702, valid accuracy: 0.3700\n",
      "Iter-13100 train loss: 2.1878 valid loss: 2.1692, valid accuracy: 0.3716\n",
      "Iter-13200 train loss: 2.1484 valid loss: 2.1681, valid accuracy: 0.3722\n",
      "Iter-13300 train loss: 2.1989 valid loss: 2.1671, valid accuracy: 0.3728\n",
      "Iter-13400 train loss: 2.1339 valid loss: 2.1660, valid accuracy: 0.3740\n",
      "Iter-13500 train loss: 2.1975 valid loss: 2.1649, valid accuracy: 0.3762\n",
      "Iter-13600 train loss: 2.1713 valid loss: 2.1638, valid accuracy: 0.3772\n",
      "Iter-13700 train loss: 2.1635 valid loss: 2.1628, valid accuracy: 0.3774\n",
      "Iter-13800 train loss: 2.1303 valid loss: 2.1617, valid accuracy: 0.3776\n",
      "Iter-13900 train loss: 2.1359 valid loss: 2.1607, valid accuracy: 0.3786\n",
      "Iter-14000 train loss: 2.1573 valid loss: 2.1596, valid accuracy: 0.3792\n",
      "Iter-14100 train loss: 2.1623 valid loss: 2.1586, valid accuracy: 0.3806\n",
      "Iter-14200 train loss: 2.1760 valid loss: 2.1575, valid accuracy: 0.3826\n",
      "Iter-14300 train loss: 2.1871 valid loss: 2.1565, valid accuracy: 0.3836\n",
      "Iter-14400 train loss: 2.1908 valid loss: 2.1555, valid accuracy: 0.3850\n",
      "Iter-14500 train loss: 2.1316 valid loss: 2.1544, valid accuracy: 0.3858\n",
      "Iter-14600 train loss: 2.1156 valid loss: 2.1534, valid accuracy: 0.3868\n",
      "Iter-14700 train loss: 2.1783 valid loss: 2.1523, valid accuracy: 0.3870\n",
      "Iter-14800 train loss: 2.1719 valid loss: 2.1513, valid accuracy: 0.3880\n",
      "Iter-14900 train loss: 2.1718 valid loss: 2.1503, valid accuracy: 0.3890\n",
      "Iter-15000 train loss: 2.1087 valid loss: 2.1492, valid accuracy: 0.3902\n",
      "Iter-15100 train loss: 2.1764 valid loss: 2.1482, valid accuracy: 0.3920\n",
      "Iter-15200 train loss: 2.1742 valid loss: 2.1472, valid accuracy: 0.3924\n",
      "Iter-15300 train loss: 2.1147 valid loss: 2.1461, valid accuracy: 0.3922\n",
      "Iter-15400 train loss: 2.1763 valid loss: 2.1451, valid accuracy: 0.3932\n",
      "Iter-15500 train loss: 2.1435 valid loss: 2.1441, valid accuracy: 0.3942\n",
      "Iter-15600 train loss: 2.1463 valid loss: 2.1431, valid accuracy: 0.3960\n",
      "Iter-15700 train loss: 2.1517 valid loss: 2.1421, valid accuracy: 0.3964\n",
      "Iter-15800 train loss: 2.1485 valid loss: 2.1410, valid accuracy: 0.3974\n",
      "Iter-15900 train loss: 2.1496 valid loss: 2.1400, valid accuracy: 0.3976\n",
      "Iter-16000 train loss: 2.1458 valid loss: 2.1390, valid accuracy: 0.3992\n",
      "Iter-16100 train loss: 2.1426 valid loss: 2.1380, valid accuracy: 0.4004\n",
      "Iter-16200 train loss: 2.1442 valid loss: 2.1370, valid accuracy: 0.4010\n",
      "Iter-16300 train loss: 2.1223 valid loss: 2.1360, valid accuracy: 0.4014\n",
      "Iter-16400 train loss: 2.1362 valid loss: 2.1350, valid accuracy: 0.4022\n",
      "Iter-16500 train loss: 2.1448 valid loss: 2.1340, valid accuracy: 0.4028\n",
      "Iter-16600 train loss: 2.2013 valid loss: 2.1330, valid accuracy: 0.4034\n",
      "Iter-16700 train loss: 2.1063 valid loss: 2.1320, valid accuracy: 0.4044\n",
      "Iter-16800 train loss: 2.1566 valid loss: 2.1310, valid accuracy: 0.4050\n",
      "Iter-16900 train loss: 2.1126 valid loss: 2.1300, valid accuracy: 0.4052\n",
      "Iter-17000 train loss: 2.1255 valid loss: 2.1290, valid accuracy: 0.4060\n",
      "Iter-17100 train loss: 2.1556 valid loss: 2.1280, valid accuracy: 0.4060\n",
      "Iter-17200 train loss: 2.1221 valid loss: 2.1270, valid accuracy: 0.4062\n",
      "Iter-17300 train loss: 2.1186 valid loss: 2.1260, valid accuracy: 0.4066\n",
      "Iter-17400 train loss: 2.1997 valid loss: 2.1250, valid accuracy: 0.4068\n",
      "Iter-17500 train loss: 2.1010 valid loss: 2.1240, valid accuracy: 0.4076\n",
      "Iter-17600 train loss: 2.1619 valid loss: 2.1230, valid accuracy: 0.4094\n",
      "Iter-17700 train loss: 2.1123 valid loss: 2.1220, valid accuracy: 0.4098\n",
      "Iter-17800 train loss: 2.1285 valid loss: 2.1210, valid accuracy: 0.4110\n",
      "Iter-17900 train loss: 2.1440 valid loss: 2.1200, valid accuracy: 0.4110\n",
      "Iter-18000 train loss: 2.1162 valid loss: 2.1190, valid accuracy: 0.4108\n",
      "Iter-18100 train loss: 2.1382 valid loss: 2.1180, valid accuracy: 0.4108\n",
      "Iter-18200 train loss: 2.0926 valid loss: 2.1170, valid accuracy: 0.4116\n",
      "Iter-18300 train loss: 2.1056 valid loss: 2.1160, valid accuracy: 0.4126\n",
      "Iter-18400 train loss: 2.1133 valid loss: 2.1150, valid accuracy: 0.4132\n",
      "Iter-18500 train loss: 2.0807 valid loss: 2.1140, valid accuracy: 0.4140\n",
      "Iter-18600 train loss: 2.1029 valid loss: 2.1131, valid accuracy: 0.4144\n",
      "Iter-18700 train loss: 2.0811 valid loss: 2.1121, valid accuracy: 0.4146\n",
      "Iter-18800 train loss: 2.0873 valid loss: 2.1111, valid accuracy: 0.4160\n",
      "Iter-18900 train loss: 2.1413 valid loss: 2.1101, valid accuracy: 0.4162\n",
      "Iter-19000 train loss: 2.0566 valid loss: 2.1092, valid accuracy: 0.4168\n",
      "Iter-19100 train loss: 2.1293 valid loss: 2.1082, valid accuracy: 0.4166\n",
      "Iter-19200 train loss: 2.1385 valid loss: 2.1072, valid accuracy: 0.4174\n",
      "Iter-19300 train loss: 2.1183 valid loss: 2.1062, valid accuracy: 0.4178\n",
      "Iter-19400 train loss: 2.1044 valid loss: 2.1052, valid accuracy: 0.4184\n",
      "Iter-19500 train loss: 2.0541 valid loss: 2.1043, valid accuracy: 0.4198\n",
      "Iter-19600 train loss: 2.0659 valid loss: 2.1033, valid accuracy: 0.4204\n",
      "Iter-19700 train loss: 2.0902 valid loss: 2.1024, valid accuracy: 0.4212\n",
      "Iter-19800 train loss: 2.1457 valid loss: 2.1014, valid accuracy: 0.4214\n",
      "Iter-19900 train loss: 2.1316 valid loss: 2.1004, valid accuracy: 0.4218\n",
      "Iter-20000 train loss: 2.1014 valid loss: 2.0994, valid accuracy: 0.4222\n",
      "Iter-20100 train loss: 2.0966 valid loss: 2.0985, valid accuracy: 0.4222\n",
      "Iter-20200 train loss: 2.0918 valid loss: 2.0975, valid accuracy: 0.4226\n",
      "Iter-20300 train loss: 2.0850 valid loss: 2.0966, valid accuracy: 0.4228\n",
      "Iter-20400 train loss: 2.1349 valid loss: 2.0956, valid accuracy: 0.4234\n",
      "Iter-20500 train loss: 2.0684 valid loss: 2.0947, valid accuracy: 0.4234\n",
      "Iter-20600 train loss: 2.0721 valid loss: 2.0937, valid accuracy: 0.4248\n",
      "Iter-20700 train loss: 2.1287 valid loss: 2.0927, valid accuracy: 0.4256\n",
      "Iter-20800 train loss: 2.0673 valid loss: 2.0918, valid accuracy: 0.4250\n",
      "Iter-20900 train loss: 2.0634 valid loss: 2.0909, valid accuracy: 0.4260\n",
      "Iter-21000 train loss: 2.0701 valid loss: 2.0899, valid accuracy: 0.4266\n",
      "Iter-21100 train loss: 2.0939 valid loss: 2.0890, valid accuracy: 0.4266\n",
      "Iter-21200 train loss: 2.0948 valid loss: 2.0880, valid accuracy: 0.4276\n",
      "Iter-21300 train loss: 2.0580 valid loss: 2.0871, valid accuracy: 0.4280\n",
      "Iter-21400 train loss: 2.0864 valid loss: 2.0861, valid accuracy: 0.4286\n",
      "Iter-21500 train loss: 2.1090 valid loss: 2.0852, valid accuracy: 0.4292\n",
      "Iter-21600 train loss: 2.0978 valid loss: 2.0842, valid accuracy: 0.4288\n",
      "Iter-21700 train loss: 2.0723 valid loss: 2.0833, valid accuracy: 0.4294\n",
      "Iter-21800 train loss: 2.1071 valid loss: 2.0823, valid accuracy: 0.4296\n",
      "Iter-21900 train loss: 2.0523 valid loss: 2.0814, valid accuracy: 0.4292\n",
      "Iter-22000 train loss: 2.1442 valid loss: 2.0804, valid accuracy: 0.4300\n",
      "Iter-22100 train loss: 2.0669 valid loss: 2.0795, valid accuracy: 0.4298\n",
      "Iter-22200 train loss: 2.1048 valid loss: 2.0786, valid accuracy: 0.4312\n",
      "Iter-22300 train loss: 2.0839 valid loss: 2.0777, valid accuracy: 0.4314\n",
      "Iter-22400 train loss: 2.0568 valid loss: 2.0768, valid accuracy: 0.4324\n",
      "Iter-22500 train loss: 2.0849 valid loss: 2.0759, valid accuracy: 0.4332\n",
      "Iter-22600 train loss: 2.0262 valid loss: 2.0750, valid accuracy: 0.4328\n",
      "Iter-22700 train loss: 2.1019 valid loss: 2.0741, valid accuracy: 0.4334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.1386 valid loss: 2.0732, valid accuracy: 0.4340\n",
      "Iter-22900 train loss: 2.0572 valid loss: 2.0722, valid accuracy: 0.4338\n",
      "Iter-23000 train loss: 2.0463 valid loss: 2.0713, valid accuracy: 0.4338\n",
      "Iter-23100 train loss: 2.0540 valid loss: 2.0704, valid accuracy: 0.4340\n",
      "Iter-23200 train loss: 2.0402 valid loss: 2.0695, valid accuracy: 0.4340\n",
      "Iter-23300 train loss: 2.1311 valid loss: 2.0685, valid accuracy: 0.4344\n",
      "Iter-23400 train loss: 2.0538 valid loss: 2.0676, valid accuracy: 0.4352\n",
      "Iter-23500 train loss: 2.0961 valid loss: 2.0667, valid accuracy: 0.4350\n",
      "Iter-23600 train loss: 2.0675 valid loss: 2.0658, valid accuracy: 0.4354\n",
      "Iter-23700 train loss: 2.0720 valid loss: 2.0649, valid accuracy: 0.4360\n",
      "Iter-23800 train loss: 2.0682 valid loss: 2.0640, valid accuracy: 0.4354\n",
      "Iter-23900 train loss: 2.0509 valid loss: 2.0631, valid accuracy: 0.4362\n",
      "Iter-24000 train loss: 2.0900 valid loss: 2.0622, valid accuracy: 0.4370\n",
      "Iter-24100 train loss: 2.0466 valid loss: 2.0613, valid accuracy: 0.4378\n",
      "Iter-24200 train loss: 2.0697 valid loss: 2.0605, valid accuracy: 0.4378\n",
      "Iter-24300 train loss: 2.0773 valid loss: 2.0595, valid accuracy: 0.4396\n",
      "Iter-24400 train loss: 2.0749 valid loss: 2.0586, valid accuracy: 0.4408\n",
      "Iter-24500 train loss: 2.0701 valid loss: 2.0577, valid accuracy: 0.4408\n",
      "Iter-24600 train loss: 2.0843 valid loss: 2.0568, valid accuracy: 0.4422\n",
      "Iter-24700 train loss: 2.0333 valid loss: 2.0559, valid accuracy: 0.4422\n",
      "Iter-24800 train loss: 2.0553 valid loss: 2.0550, valid accuracy: 0.4428\n",
      "Iter-24900 train loss: 2.0760 valid loss: 2.0541, valid accuracy: 0.4424\n",
      "Iter-25000 train loss: 2.0542 valid loss: 2.0532, valid accuracy: 0.4428\n",
      "Iter-25100 train loss: 2.0421 valid loss: 2.0523, valid accuracy: 0.4428\n",
      "Iter-25200 train loss: 2.0553 valid loss: 2.0514, valid accuracy: 0.4432\n",
      "Iter-25300 train loss: 2.1083 valid loss: 2.0505, valid accuracy: 0.4434\n",
      "Iter-25400 train loss: 2.0450 valid loss: 2.0496, valid accuracy: 0.4442\n",
      "Iter-25500 train loss: 2.0714 valid loss: 2.0487, valid accuracy: 0.4444\n",
      "Iter-25600 train loss: 2.0752 valid loss: 2.0478, valid accuracy: 0.4446\n",
      "Iter-25700 train loss: 2.0498 valid loss: 2.0469, valid accuracy: 0.4446\n",
      "Iter-25800 train loss: 2.0436 valid loss: 2.0460, valid accuracy: 0.4454\n",
      "Iter-25900 train loss: 2.0395 valid loss: 2.0452, valid accuracy: 0.4466\n",
      "Iter-26000 train loss: 2.0729 valid loss: 2.0443, valid accuracy: 0.4466\n",
      "Iter-26100 train loss: 2.0201 valid loss: 2.0434, valid accuracy: 0.4474\n",
      "Iter-26200 train loss: 2.0417 valid loss: 2.0426, valid accuracy: 0.4478\n",
      "Iter-26300 train loss: 2.0673 valid loss: 2.0417, valid accuracy: 0.4480\n",
      "Iter-26400 train loss: 2.0012 valid loss: 2.0408, valid accuracy: 0.4478\n",
      "Iter-26500 train loss: 2.0825 valid loss: 2.0400, valid accuracy: 0.4482\n",
      "Iter-26600 train loss: 2.0330 valid loss: 2.0391, valid accuracy: 0.4478\n",
      "Iter-26700 train loss: 2.0666 valid loss: 2.0382, valid accuracy: 0.4482\n",
      "Iter-26800 train loss: 2.0320 valid loss: 2.0374, valid accuracy: 0.4488\n",
      "Iter-26900 train loss: 2.0913 valid loss: 2.0365, valid accuracy: 0.4490\n",
      "Iter-27000 train loss: 2.0325 valid loss: 2.0356, valid accuracy: 0.4490\n",
      "Iter-27100 train loss: 2.0238 valid loss: 2.0347, valid accuracy: 0.4498\n",
      "Iter-27200 train loss: 2.0575 valid loss: 2.0339, valid accuracy: 0.4500\n",
      "Iter-27300 train loss: 2.0189 valid loss: 2.0330, valid accuracy: 0.4506\n",
      "Iter-27400 train loss: 2.0454 valid loss: 2.0321, valid accuracy: 0.4504\n",
      "Iter-27500 train loss: 2.0485 valid loss: 2.0312, valid accuracy: 0.4504\n",
      "Iter-27600 train loss: 2.1107 valid loss: 2.0304, valid accuracy: 0.4506\n",
      "Iter-27700 train loss: 2.0441 valid loss: 2.0295, valid accuracy: 0.4514\n",
      "Iter-27800 train loss: 2.0200 valid loss: 2.0287, valid accuracy: 0.4516\n",
      "Iter-27900 train loss: 2.0200 valid loss: 2.0278, valid accuracy: 0.4510\n",
      "Iter-28000 train loss: 2.0277 valid loss: 2.0269, valid accuracy: 0.4522\n",
      "Iter-28100 train loss: 2.0194 valid loss: 2.0261, valid accuracy: 0.4526\n",
      "Iter-28200 train loss: 2.0533 valid loss: 2.0252, valid accuracy: 0.4530\n",
      "Iter-28300 train loss: 2.0122 valid loss: 2.0244, valid accuracy: 0.4534\n",
      "Iter-28400 train loss: 2.0949 valid loss: 2.0235, valid accuracy: 0.4542\n",
      "Iter-28500 train loss: 2.0331 valid loss: 2.0226, valid accuracy: 0.4544\n",
      "Iter-28600 train loss: 2.0632 valid loss: 2.0218, valid accuracy: 0.4546\n",
      "Iter-28700 train loss: 2.0445 valid loss: 2.0209, valid accuracy: 0.4554\n",
      "Iter-28800 train loss: 2.0813 valid loss: 2.0201, valid accuracy: 0.4556\n",
      "Iter-28900 train loss: 1.9968 valid loss: 2.0192, valid accuracy: 0.4562\n",
      "Iter-29000 train loss: 1.9832 valid loss: 2.0184, valid accuracy: 0.4568\n",
      "Iter-29100 train loss: 2.0131 valid loss: 2.0175, valid accuracy: 0.4578\n",
      "Iter-29200 train loss: 2.0134 valid loss: 2.0167, valid accuracy: 0.4592\n",
      "Iter-29300 train loss: 1.9977 valid loss: 2.0159, valid accuracy: 0.4594\n",
      "Iter-29400 train loss: 2.0309 valid loss: 2.0151, valid accuracy: 0.4598\n",
      "Iter-29500 train loss: 1.9967 valid loss: 2.0143, valid accuracy: 0.4596\n",
      "Iter-29600 train loss: 2.0136 valid loss: 2.0134, valid accuracy: 0.4602\n",
      "Iter-29700 train loss: 2.0694 valid loss: 2.0126, valid accuracy: 0.4610\n",
      "Iter-29800 train loss: 2.0097 valid loss: 2.0118, valid accuracy: 0.4616\n",
      "Iter-29900 train loss: 2.0178 valid loss: 2.0109, valid accuracy: 0.4610\n",
      "Iter-30000 train loss: 1.9820 valid loss: 2.0101, valid accuracy: 0.4616\n",
      "Iter-30100 train loss: 2.0205 valid loss: 2.0093, valid accuracy: 0.4616\n",
      "Iter-30200 train loss: 1.9927 valid loss: 2.0084, valid accuracy: 0.4626\n",
      "Iter-30300 train loss: 2.0006 valid loss: 2.0076, valid accuracy: 0.4628\n",
      "Iter-30400 train loss: 1.9992 valid loss: 2.0067, valid accuracy: 0.4634\n",
      "Iter-30500 train loss: 1.9932 valid loss: 2.0059, valid accuracy: 0.4636\n",
      "Iter-30600 train loss: 2.0446 valid loss: 2.0050, valid accuracy: 0.4638\n",
      "Iter-30700 train loss: 1.9754 valid loss: 2.0042, valid accuracy: 0.4638\n",
      "Iter-30800 train loss: 2.0001 valid loss: 2.0034, valid accuracy: 0.4644\n",
      "Iter-30900 train loss: 2.0011 valid loss: 2.0026, valid accuracy: 0.4648\n",
      "Iter-31000 train loss: 2.0254 valid loss: 2.0018, valid accuracy: 0.4654\n",
      "Iter-31100 train loss: 2.0215 valid loss: 2.0009, valid accuracy: 0.4654\n",
      "Iter-31200 train loss: 1.9862 valid loss: 2.0001, valid accuracy: 0.4652\n",
      "Iter-31300 train loss: 1.9073 valid loss: 1.9993, valid accuracy: 0.4654\n",
      "Iter-31400 train loss: 2.0324 valid loss: 1.9984, valid accuracy: 0.4656\n",
      "Iter-31500 train loss: 2.0600 valid loss: 1.9977, valid accuracy: 0.4654\n",
      "Iter-31600 train loss: 2.0188 valid loss: 1.9968, valid accuracy: 0.4656\n",
      "Iter-31700 train loss: 1.9760 valid loss: 1.9960, valid accuracy: 0.4660\n",
      "Iter-31800 train loss: 2.0081 valid loss: 1.9952, valid accuracy: 0.4662\n",
      "Iter-31900 train loss: 2.0924 valid loss: 1.9944, valid accuracy: 0.4674\n",
      "Iter-32000 train loss: 1.9796 valid loss: 1.9936, valid accuracy: 0.4676\n",
      "Iter-32100 train loss: 1.9490 valid loss: 1.9928, valid accuracy: 0.4686\n",
      "Iter-32200 train loss: 2.0636 valid loss: 1.9919, valid accuracy: 0.4680\n",
      "Iter-32300 train loss: 2.0311 valid loss: 1.9911, valid accuracy: 0.4686\n",
      "Iter-32400 train loss: 1.9581 valid loss: 1.9903, valid accuracy: 0.4688\n",
      "Iter-32500 train loss: 2.0431 valid loss: 1.9895, valid accuracy: 0.4690\n",
      "Iter-32600 train loss: 1.9523 valid loss: 1.9887, valid accuracy: 0.4696\n",
      "Iter-32700 train loss: 2.0502 valid loss: 1.9879, valid accuracy: 0.4690\n",
      "Iter-32800 train loss: 1.9472 valid loss: 1.9871, valid accuracy: 0.4692\n",
      "Iter-32900 train loss: 2.0354 valid loss: 1.9862, valid accuracy: 0.4702\n",
      "Iter-33000 train loss: 1.9723 valid loss: 1.9854, valid accuracy: 0.4710\n",
      "Iter-33100 train loss: 2.0059 valid loss: 1.9846, valid accuracy: 0.4706\n",
      "Iter-33200 train loss: 2.0490 valid loss: 1.9838, valid accuracy: 0.4716\n",
      "Iter-33300 train loss: 2.0257 valid loss: 1.9830, valid accuracy: 0.4720\n",
      "Iter-33400 train loss: 1.9548 valid loss: 1.9822, valid accuracy: 0.4722\n",
      "Iter-33500 train loss: 1.9990 valid loss: 1.9814, valid accuracy: 0.4732\n",
      "Iter-33600 train loss: 1.9311 valid loss: 1.9806, valid accuracy: 0.4724\n",
      "Iter-33700 train loss: 1.9897 valid loss: 1.9798, valid accuracy: 0.4732\n",
      "Iter-33800 train loss: 1.9345 valid loss: 1.9790, valid accuracy: 0.4730\n",
      "Iter-33900 train loss: 2.0000 valid loss: 1.9783, valid accuracy: 0.4728\n",
      "Iter-34000 train loss: 1.9988 valid loss: 1.9775, valid accuracy: 0.4746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.0080 valid loss: 1.9767, valid accuracy: 0.4742\n",
      "Iter-34200 train loss: 1.9638 valid loss: 1.9759, valid accuracy: 0.4746\n",
      "Iter-34300 train loss: 1.9618 valid loss: 1.9751, valid accuracy: 0.4744\n",
      "Iter-34400 train loss: 2.0501 valid loss: 1.9743, valid accuracy: 0.4748\n",
      "Iter-34500 train loss: 1.8634 valid loss: 1.9735, valid accuracy: 0.4758\n",
      "Iter-34600 train loss: 2.0218 valid loss: 1.9727, valid accuracy: 0.4756\n",
      "Iter-34700 train loss: 1.8818 valid loss: 1.9719, valid accuracy: 0.4756\n",
      "Iter-34800 train loss: 2.0003 valid loss: 1.9711, valid accuracy: 0.4754\n",
      "Iter-34900 train loss: 1.9841 valid loss: 1.9703, valid accuracy: 0.4756\n",
      "Iter-35000 train loss: 1.9650 valid loss: 1.9695, valid accuracy: 0.4760\n",
      "Iter-35100 train loss: 2.0139 valid loss: 1.9688, valid accuracy: 0.4762\n",
      "Iter-35200 train loss: 1.9789 valid loss: 1.9680, valid accuracy: 0.4768\n",
      "Iter-35300 train loss: 1.9168 valid loss: 1.9672, valid accuracy: 0.4770\n",
      "Iter-35400 train loss: 1.9648 valid loss: 1.9664, valid accuracy: 0.4764\n",
      "Iter-35500 train loss: 1.9551 valid loss: 1.9657, valid accuracy: 0.4770\n",
      "Iter-35600 train loss: 1.9426 valid loss: 1.9649, valid accuracy: 0.4774\n",
      "Iter-35700 train loss: 1.9647 valid loss: 1.9641, valid accuracy: 0.4780\n",
      "Iter-35800 train loss: 1.9062 valid loss: 1.9633, valid accuracy: 0.4784\n",
      "Iter-35900 train loss: 2.0154 valid loss: 1.9625, valid accuracy: 0.4786\n",
      "Iter-36000 train loss: 1.9575 valid loss: 1.9618, valid accuracy: 0.4786\n",
      "Iter-36100 train loss: 1.9280 valid loss: 1.9610, valid accuracy: 0.4786\n",
      "Iter-36200 train loss: 1.8905 valid loss: 1.9602, valid accuracy: 0.4792\n",
      "Iter-36300 train loss: 2.0440 valid loss: 1.9595, valid accuracy: 0.4796\n",
      "Iter-36400 train loss: 1.9987 valid loss: 1.9587, valid accuracy: 0.4806\n",
      "Iter-36500 train loss: 1.9651 valid loss: 1.9579, valid accuracy: 0.4806\n",
      "Iter-36600 train loss: 1.9715 valid loss: 1.9571, valid accuracy: 0.4814\n",
      "Iter-36700 train loss: 1.9662 valid loss: 1.9564, valid accuracy: 0.4818\n",
      "Iter-36800 train loss: 1.9595 valid loss: 1.9556, valid accuracy: 0.4820\n",
      "Iter-36900 train loss: 2.0751 valid loss: 1.9548, valid accuracy: 0.4814\n",
      "Iter-37000 train loss: 1.9697 valid loss: 1.9540, valid accuracy: 0.4812\n",
      "Iter-37100 train loss: 1.9706 valid loss: 1.9533, valid accuracy: 0.4818\n",
      "Iter-37200 train loss: 1.9220 valid loss: 1.9525, valid accuracy: 0.4820\n",
      "Iter-37300 train loss: 2.0050 valid loss: 1.9517, valid accuracy: 0.4818\n",
      "Iter-37400 train loss: 1.8784 valid loss: 1.9509, valid accuracy: 0.4824\n",
      "Iter-37500 train loss: 1.8479 valid loss: 1.9502, valid accuracy: 0.4828\n",
      "Iter-37600 train loss: 1.8760 valid loss: 1.9494, valid accuracy: 0.4824\n",
      "Iter-37700 train loss: 1.9220 valid loss: 1.9487, valid accuracy: 0.4822\n",
      "Iter-37800 train loss: 1.9810 valid loss: 1.9480, valid accuracy: 0.4826\n",
      "Iter-37900 train loss: 1.9366 valid loss: 1.9472, valid accuracy: 0.4838\n",
      "Iter-38000 train loss: 1.9163 valid loss: 1.9464, valid accuracy: 0.4840\n",
      "Iter-38100 train loss: 1.9077 valid loss: 1.9456, valid accuracy: 0.4842\n",
      "Iter-38200 train loss: 1.9564 valid loss: 1.9449, valid accuracy: 0.4844\n",
      "Iter-38300 train loss: 1.9354 valid loss: 1.9441, valid accuracy: 0.4854\n",
      "Iter-38400 train loss: 2.0101 valid loss: 1.9433, valid accuracy: 0.4862\n",
      "Iter-38500 train loss: 1.8764 valid loss: 1.9426, valid accuracy: 0.4856\n",
      "Iter-38600 train loss: 1.8896 valid loss: 1.9418, valid accuracy: 0.4852\n",
      "Iter-38700 train loss: 1.9553 valid loss: 1.9410, valid accuracy: 0.4858\n",
      "Iter-38800 train loss: 1.9721 valid loss: 1.9403, valid accuracy: 0.4858\n",
      "Iter-38900 train loss: 1.9006 valid loss: 1.9395, valid accuracy: 0.4868\n",
      "Iter-39000 train loss: 2.0416 valid loss: 1.9388, valid accuracy: 0.4872\n",
      "Iter-39100 train loss: 1.8588 valid loss: 1.9380, valid accuracy: 0.4866\n",
      "Iter-39200 train loss: 1.8763 valid loss: 1.9373, valid accuracy: 0.4874\n",
      "Iter-39300 train loss: 1.9220 valid loss: 1.9366, valid accuracy: 0.4874\n",
      "Iter-39400 train loss: 1.9458 valid loss: 1.9358, valid accuracy: 0.4880\n",
      "Iter-39500 train loss: 1.9780 valid loss: 1.9351, valid accuracy: 0.4888\n",
      "Iter-39600 train loss: 1.9266 valid loss: 1.9343, valid accuracy: 0.4886\n",
      "Iter-39700 train loss: 1.9667 valid loss: 1.9336, valid accuracy: 0.4890\n",
      "Iter-39800 train loss: 1.8956 valid loss: 1.9328, valid accuracy: 0.4886\n",
      "Iter-39900 train loss: 1.9689 valid loss: 1.9321, valid accuracy: 0.4886\n",
      "Iter-40000 train loss: 1.9072 valid loss: 1.9313, valid accuracy: 0.4898\n",
      "Iter-40100 train loss: 1.9136 valid loss: 1.9306, valid accuracy: 0.4900\n",
      "Iter-40200 train loss: 1.8645 valid loss: 1.9299, valid accuracy: 0.4904\n",
      "Iter-40300 train loss: 1.9235 valid loss: 1.9291, valid accuracy: 0.4906\n",
      "Iter-40400 train loss: 1.9044 valid loss: 1.9284, valid accuracy: 0.4906\n",
      "Iter-40500 train loss: 1.9864 valid loss: 1.9276, valid accuracy: 0.4908\n",
      "Iter-40600 train loss: 1.9057 valid loss: 1.9269, valid accuracy: 0.4904\n",
      "Iter-40700 train loss: 1.8242 valid loss: 1.9261, valid accuracy: 0.4910\n",
      "Iter-40800 train loss: 1.8555 valid loss: 1.9254, valid accuracy: 0.4916\n",
      "Iter-40900 train loss: 1.8605 valid loss: 1.9247, valid accuracy: 0.4918\n",
      "Iter-41000 train loss: 1.8634 valid loss: 1.9239, valid accuracy: 0.4916\n",
      "Iter-41100 train loss: 1.9653 valid loss: 1.9232, valid accuracy: 0.4916\n",
      "Iter-41200 train loss: 2.0162 valid loss: 1.9225, valid accuracy: 0.4914\n",
      "Iter-41300 train loss: 1.8730 valid loss: 1.9217, valid accuracy: 0.4922\n",
      "Iter-41400 train loss: 2.0090 valid loss: 1.9210, valid accuracy: 0.4924\n",
      "Iter-41500 train loss: 1.9633 valid loss: 1.9202, valid accuracy: 0.4932\n",
      "Iter-41600 train loss: 1.9369 valid loss: 1.9195, valid accuracy: 0.4932\n",
      "Iter-41700 train loss: 1.9034 valid loss: 1.9188, valid accuracy: 0.4936\n",
      "Iter-41800 train loss: 1.9259 valid loss: 1.9180, valid accuracy: 0.4942\n",
      "Iter-41900 train loss: 1.9089 valid loss: 1.9173, valid accuracy: 0.4936\n",
      "Iter-42000 train loss: 1.8501 valid loss: 1.9165, valid accuracy: 0.4938\n",
      "Iter-42100 train loss: 1.9558 valid loss: 1.9158, valid accuracy: 0.4940\n",
      "Iter-42200 train loss: 1.9807 valid loss: 1.9151, valid accuracy: 0.4942\n",
      "Iter-42300 train loss: 1.9647 valid loss: 1.9143, valid accuracy: 0.4942\n",
      "Iter-42400 train loss: 1.9198 valid loss: 1.9136, valid accuracy: 0.4948\n",
      "Iter-42500 train loss: 1.9636 valid loss: 1.9129, valid accuracy: 0.4944\n",
      "Iter-42600 train loss: 1.9278 valid loss: 1.9122, valid accuracy: 0.4948\n",
      "Iter-42700 train loss: 1.8894 valid loss: 1.9115, valid accuracy: 0.4954\n",
      "Iter-42800 train loss: 1.9809 valid loss: 1.9107, valid accuracy: 0.4958\n",
      "Iter-42900 train loss: 1.9314 valid loss: 1.9100, valid accuracy: 0.4954\n",
      "Iter-43000 train loss: 1.9212 valid loss: 1.9093, valid accuracy: 0.4960\n",
      "Iter-43100 train loss: 1.8882 valid loss: 1.9086, valid accuracy: 0.4960\n",
      "Iter-43200 train loss: 1.9169 valid loss: 1.9079, valid accuracy: 0.4964\n",
      "Iter-43300 train loss: 1.8532 valid loss: 1.9071, valid accuracy: 0.4964\n",
      "Iter-43400 train loss: 1.8400 valid loss: 1.9064, valid accuracy: 0.4972\n",
      "Iter-43500 train loss: 1.9582 valid loss: 1.9057, valid accuracy: 0.4970\n",
      "Iter-43600 train loss: 1.8468 valid loss: 1.9049, valid accuracy: 0.4972\n",
      "Iter-43700 train loss: 1.9125 valid loss: 1.9042, valid accuracy: 0.4976\n",
      "Iter-43800 train loss: 1.9210 valid loss: 1.9035, valid accuracy: 0.4974\n",
      "Iter-43900 train loss: 1.8840 valid loss: 1.9028, valid accuracy: 0.4976\n",
      "Iter-44000 train loss: 1.9145 valid loss: 1.9021, valid accuracy: 0.4974\n",
      "Iter-44100 train loss: 1.8312 valid loss: 1.9014, valid accuracy: 0.4978\n",
      "Iter-44200 train loss: 1.9756 valid loss: 1.9007, valid accuracy: 0.4980\n",
      "Iter-44300 train loss: 1.9499 valid loss: 1.8999, valid accuracy: 0.4976\n",
      "Iter-44400 train loss: 1.9190 valid loss: 1.8992, valid accuracy: 0.4974\n",
      "Iter-44500 train loss: 1.8902 valid loss: 1.8985, valid accuracy: 0.4974\n",
      "Iter-44600 train loss: 1.8772 valid loss: 1.8978, valid accuracy: 0.4980\n",
      "Iter-44700 train loss: 1.8597 valid loss: 1.8971, valid accuracy: 0.4984\n",
      "Iter-44800 train loss: 1.8178 valid loss: 1.8964, valid accuracy: 0.4986\n",
      "Iter-44900 train loss: 1.8792 valid loss: 1.8957, valid accuracy: 0.4982\n",
      "Iter-45000 train loss: 1.9010 valid loss: 1.8950, valid accuracy: 0.4994\n",
      "Iter-45100 train loss: 1.8482 valid loss: 1.8943, valid accuracy: 0.4998\n",
      "Iter-45200 train loss: 1.9555 valid loss: 1.8935, valid accuracy: 0.4996\n",
      "Iter-45300 train loss: 1.8632 valid loss: 1.8929, valid accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 1.9221 valid loss: 1.8921, valid accuracy: 0.4994\n",
      "Iter-45500 train loss: 1.9022 valid loss: 1.8914, valid accuracy: 0.4998\n",
      "Iter-45600 train loss: 1.8864 valid loss: 1.8908, valid accuracy: 0.5002\n",
      "Iter-45700 train loss: 1.8939 valid loss: 1.8900, valid accuracy: 0.5008\n",
      "Iter-45800 train loss: 1.9228 valid loss: 1.8893, valid accuracy: 0.5008\n",
      "Iter-45900 train loss: 1.8643 valid loss: 1.8886, valid accuracy: 0.5010\n",
      "Iter-46000 train loss: 1.8932 valid loss: 1.8879, valid accuracy: 0.5010\n",
      "Iter-46100 train loss: 1.9249 valid loss: 1.8872, valid accuracy: 0.5010\n",
      "Iter-46200 train loss: 1.9112 valid loss: 1.8865, valid accuracy: 0.5008\n",
      "Iter-46300 train loss: 1.9050 valid loss: 1.8858, valid accuracy: 0.5014\n",
      "Iter-46400 train loss: 1.9266 valid loss: 1.8851, valid accuracy: 0.5018\n",
      "Iter-46500 train loss: 2.0244 valid loss: 1.8844, valid accuracy: 0.5022\n",
      "Iter-46600 train loss: 1.9565 valid loss: 1.8837, valid accuracy: 0.5024\n",
      "Iter-46700 train loss: 1.8720 valid loss: 1.8830, valid accuracy: 0.5026\n",
      "Iter-46800 train loss: 1.9249 valid loss: 1.8823, valid accuracy: 0.5030\n",
      "Iter-46900 train loss: 1.8960 valid loss: 1.8816, valid accuracy: 0.5038\n",
      "Iter-47000 train loss: 1.7987 valid loss: 1.8809, valid accuracy: 0.5032\n",
      "Iter-47100 train loss: 1.9841 valid loss: 1.8802, valid accuracy: 0.5034\n",
      "Iter-47200 train loss: 1.9097 valid loss: 1.8795, valid accuracy: 0.5036\n",
      "Iter-47300 train loss: 1.8661 valid loss: 1.8788, valid accuracy: 0.5038\n",
      "Iter-47400 train loss: 1.8704 valid loss: 1.8781, valid accuracy: 0.5040\n",
      "Iter-47500 train loss: 1.9058 valid loss: 1.8774, valid accuracy: 0.5040\n",
      "Iter-47600 train loss: 1.8928 valid loss: 1.8767, valid accuracy: 0.5042\n",
      "Iter-47700 train loss: 1.8343 valid loss: 1.8760, valid accuracy: 0.5036\n",
      "Iter-47800 train loss: 1.8700 valid loss: 1.8754, valid accuracy: 0.5038\n",
      "Iter-47900 train loss: 1.8179 valid loss: 1.8747, valid accuracy: 0.5036\n",
      "Iter-48000 train loss: 1.9089 valid loss: 1.8740, valid accuracy: 0.5038\n",
      "Iter-48100 train loss: 1.9184 valid loss: 1.8733, valid accuracy: 0.5042\n",
      "Iter-48200 train loss: 1.8162 valid loss: 1.8726, valid accuracy: 0.5040\n",
      "Iter-48300 train loss: 1.8783 valid loss: 1.8719, valid accuracy: 0.5046\n",
      "Iter-48400 train loss: 1.8836 valid loss: 1.8712, valid accuracy: 0.5042\n",
      "Iter-48500 train loss: 1.9133 valid loss: 1.8705, valid accuracy: 0.5038\n",
      "Iter-48600 train loss: 1.8644 valid loss: 1.8698, valid accuracy: 0.5040\n",
      "Iter-48700 train loss: 1.9368 valid loss: 1.8692, valid accuracy: 0.5044\n",
      "Iter-48800 train loss: 1.8746 valid loss: 1.8685, valid accuracy: 0.5042\n",
      "Iter-48900 train loss: 1.8295 valid loss: 1.8678, valid accuracy: 0.5044\n",
      "Iter-49000 train loss: 1.8296 valid loss: 1.8672, valid accuracy: 0.5044\n",
      "Iter-49100 train loss: 1.8281 valid loss: 1.8665, valid accuracy: 0.5048\n",
      "Iter-49200 train loss: 1.8786 valid loss: 1.8658, valid accuracy: 0.5046\n",
      "Iter-49300 train loss: 1.8190 valid loss: 1.8651, valid accuracy: 0.5048\n",
      "Iter-49400 train loss: 1.7763 valid loss: 1.8644, valid accuracy: 0.5052\n",
      "Iter-49500 train loss: 1.8960 valid loss: 1.8637, valid accuracy: 0.5052\n",
      "Iter-49600 train loss: 1.8208 valid loss: 1.8631, valid accuracy: 0.5052\n",
      "Iter-49700 train loss: 1.9373 valid loss: 1.8624, valid accuracy: 0.5054\n",
      "Iter-49800 train loss: 1.8456 valid loss: 1.8617, valid accuracy: 0.5058\n",
      "Iter-49900 train loss: 2.0179 valid loss: 1.8610, valid accuracy: 0.5058\n",
      "Iter-50000 train loss: 1.7721 valid loss: 1.8604, valid accuracy: 0.5062\n",
      "Iter-50100 train loss: 1.8732 valid loss: 1.8597, valid accuracy: 0.5064\n",
      "Iter-50200 train loss: 1.8466 valid loss: 1.8590, valid accuracy: 0.5064\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
