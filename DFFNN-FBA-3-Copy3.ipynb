{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = np.random.uniform(high=10, low=0, size=(1,10))\n",
    "# sample.shape, sample.dtype, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.exp(sample)/np.exp(sample).sum(axis=1)\n",
    "# # # sample.shape, sample.dtype\n",
    "# np.exp(sample)/ np.exp(sample).sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def softmax(X):\n",
    "# #     eX = np.exp((X.T - np.max(X, axis=1)).T)\n",
    "#     eX = np.exp(X.T)\n",
    "#     return (eX.T / eX.sum(axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax(X=sample), np.exp(sample.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad = []\n",
    "        for layer in range(L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        self.grads.append(grad)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append((fc_cache, nl_cache)) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            nl_caches.append(nl_cache)\n",
    "        caches.append((fc_caches, nl_caches)) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3812 valid loss: 2.3893, valid accuracy: 0.0776\n",
      "Iter-20 train loss: 2.3830 valid loss: 2.3878, valid accuracy: 0.0790\n",
      "Iter-30 train loss: 2.3711 valid loss: 2.3864, valid accuracy: 0.0786\n",
      "Iter-40 train loss: 2.3961 valid loss: 2.3846, valid accuracy: 0.0792\n",
      "Iter-50 train loss: 2.3360 valid loss: 2.3829, valid accuracy: 0.0792\n",
      "Iter-60 train loss: 2.3935 valid loss: 2.3815, valid accuracy: 0.0794\n",
      "Iter-70 train loss: 2.3013 valid loss: 2.3798, valid accuracy: 0.0796\n",
      "Iter-80 train loss: 2.3988 valid loss: 2.3783, valid accuracy: 0.0802\n",
      "Iter-90 train loss: 2.3118 valid loss: 2.3766, valid accuracy: 0.0812\n",
      "Iter-100 train loss: 2.3635 valid loss: 2.3749, valid accuracy: 0.0814\n",
      "Iter-110 train loss: 2.3998 valid loss: 2.3733, valid accuracy: 0.0822\n",
      "Iter-120 train loss: 2.3535 valid loss: 2.3715, valid accuracy: 0.0836\n",
      "Iter-130 train loss: 2.3291 valid loss: 2.3697, valid accuracy: 0.0836\n",
      "Iter-140 train loss: 2.3366 valid loss: 2.3682, valid accuracy: 0.0844\n",
      "Iter-150 train loss: 2.3903 valid loss: 2.3661, valid accuracy: 0.0862\n",
      "Iter-160 train loss: 2.4144 valid loss: 2.3644, valid accuracy: 0.0862\n",
      "Iter-170 train loss: 2.4091 valid loss: 2.3626, valid accuracy: 0.0854\n",
      "Iter-180 train loss: 2.4028 valid loss: 2.3603, valid accuracy: 0.0850\n",
      "Iter-190 train loss: 2.3631 valid loss: 2.3583, valid accuracy: 0.0868\n",
      "Iter-200 train loss: 2.3707 valid loss: 2.3562, valid accuracy: 0.0862\n",
      "Iter-210 train loss: 2.3919 valid loss: 2.3548, valid accuracy: 0.0868\n",
      "Iter-220 train loss: 2.3668 valid loss: 2.3530, valid accuracy: 0.0876\n",
      "Iter-230 train loss: 2.3468 valid loss: 2.3511, valid accuracy: 0.0878\n",
      "Iter-240 train loss: 2.3171 valid loss: 2.3492, valid accuracy: 0.0882\n",
      "Iter-250 train loss: 2.3703 valid loss: 2.3472, valid accuracy: 0.0886\n",
      "Iter-260 train loss: 2.3463 valid loss: 2.3453, valid accuracy: 0.0892\n",
      "Iter-270 train loss: 2.3272 valid loss: 2.3431, valid accuracy: 0.0886\n",
      "Iter-280 train loss: 2.3286 valid loss: 2.3413, valid accuracy: 0.0892\n",
      "Iter-290 train loss: 2.3515 valid loss: 2.3394, valid accuracy: 0.0886\n",
      "Iter-300 train loss: 2.3247 valid loss: 2.3374, valid accuracy: 0.0898\n",
      "Iter-310 train loss: 2.3353 valid loss: 2.3354, valid accuracy: 0.0906\n",
      "Iter-320 train loss: 2.3656 valid loss: 2.3336, valid accuracy: 0.0914\n",
      "Iter-330 train loss: 2.3215 valid loss: 2.3316, valid accuracy: 0.0924\n",
      "Iter-340 train loss: 2.3683 valid loss: 2.3297, valid accuracy: 0.0930\n",
      "Iter-350 train loss: 2.3685 valid loss: 2.3276, valid accuracy: 0.0934\n",
      "Iter-360 train loss: 2.3596 valid loss: 2.3254, valid accuracy: 0.0934\n",
      "Iter-370 train loss: 2.3172 valid loss: 2.3232, valid accuracy: 0.0950\n",
      "Iter-380 train loss: 2.2965 valid loss: 2.3210, valid accuracy: 0.0944\n",
      "Iter-390 train loss: 2.3083 valid loss: 2.3191, valid accuracy: 0.0960\n",
      "Iter-400 train loss: 2.3488 valid loss: 2.3170, valid accuracy: 0.0968\n",
      "Iter-410 train loss: 2.3143 valid loss: 2.3150, valid accuracy: 0.0968\n",
      "Iter-420 train loss: 2.2767 valid loss: 2.3128, valid accuracy: 0.0986\n",
      "Iter-430 train loss: 2.3047 valid loss: 2.3102, valid accuracy: 0.0980\n",
      "Iter-440 train loss: 2.3292 valid loss: 2.3078, valid accuracy: 0.0992\n",
      "Iter-450 train loss: 2.3127 valid loss: 2.3055, valid accuracy: 0.0992\n",
      "Iter-460 train loss: 2.3364 valid loss: 2.3030, valid accuracy: 0.1002\n",
      "Iter-470 train loss: 2.3191 valid loss: 2.3008, valid accuracy: 0.0994\n",
      "Iter-480 train loss: 2.3001 valid loss: 2.2981, valid accuracy: 0.1002\n",
      "Iter-490 train loss: 2.2930 valid loss: 2.2955, valid accuracy: 0.1010\n",
      "Iter-500 train loss: 2.2962 valid loss: 2.2931, valid accuracy: 0.1020\n",
      "Iter-510 train loss: 2.2733 valid loss: 2.2903, valid accuracy: 0.1032\n",
      "Iter-520 train loss: 2.3278 valid loss: 2.2873, valid accuracy: 0.1040\n",
      "Iter-530 train loss: 2.2634 valid loss: 2.2850, valid accuracy: 0.1042\n",
      "Iter-540 train loss: 2.2835 valid loss: 2.2823, valid accuracy: 0.1058\n",
      "Iter-550 train loss: 2.3403 valid loss: 2.2794, valid accuracy: 0.1062\n",
      "Iter-560 train loss: 2.2480 valid loss: 2.2767, valid accuracy: 0.1064\n",
      "Iter-570 train loss: 2.2657 valid loss: 2.2740, valid accuracy: 0.1068\n",
      "Iter-580 train loss: 2.2803 valid loss: 2.2711, valid accuracy: 0.1078\n",
      "Iter-590 train loss: 2.2415 valid loss: 2.2682, valid accuracy: 0.1084\n",
      "Iter-600 train loss: 2.2315 valid loss: 2.2655, valid accuracy: 0.1096\n",
      "Iter-610 train loss: 2.3185 valid loss: 2.2627, valid accuracy: 0.1108\n",
      "Iter-620 train loss: 2.3010 valid loss: 2.2599, valid accuracy: 0.1116\n",
      "Iter-630 train loss: 2.2616 valid loss: 2.2568, valid accuracy: 0.1130\n",
      "Iter-640 train loss: 2.2487 valid loss: 2.2538, valid accuracy: 0.1150\n",
      "Iter-650 train loss: 2.3055 valid loss: 2.2510, valid accuracy: 0.1156\n",
      "Iter-660 train loss: 2.2939 valid loss: 2.2481, valid accuracy: 0.1156\n",
      "Iter-670 train loss: 2.2244 valid loss: 2.2451, valid accuracy: 0.1168\n",
      "Iter-680 train loss: 2.2511 valid loss: 2.2422, valid accuracy: 0.1184\n",
      "Iter-690 train loss: 2.2212 valid loss: 2.2391, valid accuracy: 0.1194\n",
      "Iter-700 train loss: 2.2155 valid loss: 2.2359, valid accuracy: 0.1200\n",
      "Iter-710 train loss: 2.3237 valid loss: 2.2329, valid accuracy: 0.1220\n",
      "Iter-720 train loss: 2.2309 valid loss: 2.2295, valid accuracy: 0.1240\n",
      "Iter-730 train loss: 2.2304 valid loss: 2.2261, valid accuracy: 0.1266\n",
      "Iter-740 train loss: 2.2357 valid loss: 2.2229, valid accuracy: 0.1268\n",
      "Iter-750 train loss: 2.2230 valid loss: 2.2196, valid accuracy: 0.1290\n",
      "Iter-760 train loss: 2.2525 valid loss: 2.2159, valid accuracy: 0.1302\n",
      "Iter-770 train loss: 2.2330 valid loss: 2.2124, valid accuracy: 0.1314\n",
      "Iter-780 train loss: 2.2182 valid loss: 2.2092, valid accuracy: 0.1324\n",
      "Iter-790 train loss: 2.2383 valid loss: 2.2057, valid accuracy: 0.1344\n",
      "Iter-800 train loss: 2.2168 valid loss: 2.2025, valid accuracy: 0.1362\n",
      "Iter-810 train loss: 2.1913 valid loss: 2.1992, valid accuracy: 0.1392\n",
      "Iter-820 train loss: 2.1915 valid loss: 2.1958, valid accuracy: 0.1424\n",
      "Iter-830 train loss: 2.1978 valid loss: 2.1922, valid accuracy: 0.1442\n",
      "Iter-840 train loss: 2.2095 valid loss: 2.1887, valid accuracy: 0.1470\n",
      "Iter-850 train loss: 2.2173 valid loss: 2.1851, valid accuracy: 0.1492\n",
      "Iter-860 train loss: 2.1712 valid loss: 2.1813, valid accuracy: 0.1506\n",
      "Iter-870 train loss: 2.1827 valid loss: 2.1773, valid accuracy: 0.1518\n",
      "Iter-880 train loss: 2.1428 valid loss: 2.1735, valid accuracy: 0.1548\n",
      "Iter-890 train loss: 2.1864 valid loss: 2.1698, valid accuracy: 0.1564\n",
      "Iter-900 train loss: 2.1347 valid loss: 2.1662, valid accuracy: 0.1584\n",
      "Iter-910 train loss: 2.1612 valid loss: 2.1627, valid accuracy: 0.1602\n",
      "Iter-920 train loss: 2.1642 valid loss: 2.1588, valid accuracy: 0.1634\n",
      "Iter-930 train loss: 2.1408 valid loss: 2.1547, valid accuracy: 0.1658\n",
      "Iter-940 train loss: 2.1657 valid loss: 2.1514, valid accuracy: 0.1686\n",
      "Iter-950 train loss: 2.1438 valid loss: 2.1477, valid accuracy: 0.1712\n",
      "Iter-960 train loss: 2.1112 valid loss: 2.1438, valid accuracy: 0.1738\n",
      "Iter-970 train loss: 2.1459 valid loss: 2.1400, valid accuracy: 0.1752\n",
      "Iter-980 train loss: 2.1472 valid loss: 2.1360, valid accuracy: 0.1796\n",
      "Iter-990 train loss: 2.0375 valid loss: 2.1318, valid accuracy: 0.1832\n",
      "Iter-1000 train loss: 2.1492 valid loss: 2.1280, valid accuracy: 0.1872\n",
      "Iter-1010 train loss: 2.1174 valid loss: 2.1237, valid accuracy: 0.1906\n",
      "Iter-1020 train loss: 2.1407 valid loss: 2.1194, valid accuracy: 0.1948\n",
      "Iter-1030 train loss: 2.1182 valid loss: 2.1155, valid accuracy: 0.1986\n",
      "Iter-1040 train loss: 2.1166 valid loss: 2.1114, valid accuracy: 0.2032\n",
      "Iter-1050 train loss: 2.1506 valid loss: 2.1071, valid accuracy: 0.2074\n",
      "Iter-1060 train loss: 2.1629 valid loss: 2.1032, valid accuracy: 0.2116\n",
      "Iter-1070 train loss: 2.0840 valid loss: 2.0992, valid accuracy: 0.2166\n",
      "Iter-1080 train loss: 2.0940 valid loss: 2.0953, valid accuracy: 0.2210\n",
      "Iter-1090 train loss: 2.0834 valid loss: 2.0912, valid accuracy: 0.2258\n",
      "Iter-1100 train loss: 2.0736 valid loss: 2.0869, valid accuracy: 0.2296\n",
      "Iter-1110 train loss: 2.0650 valid loss: 2.0828, valid accuracy: 0.2342\n",
      "Iter-1120 train loss: 2.0589 valid loss: 2.0785, valid accuracy: 0.2378\n",
      "Iter-1130 train loss: 2.1143 valid loss: 2.0742, valid accuracy: 0.2424\n",
      "Iter-1140 train loss: 2.0588 valid loss: 2.0700, valid accuracy: 0.2488\n",
      "Iter-1150 train loss: 2.1086 valid loss: 2.0656, valid accuracy: 0.2536\n",
      "Iter-1160 train loss: 2.0127 valid loss: 2.0614, valid accuracy: 0.2586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.0453 valid loss: 2.0572, valid accuracy: 0.2640\n",
      "Iter-1180 train loss: 2.1497 valid loss: 2.0529, valid accuracy: 0.2680\n",
      "Iter-1190 train loss: 2.0813 valid loss: 2.0486, valid accuracy: 0.2740\n",
      "Iter-1200 train loss: 2.0972 valid loss: 2.0443, valid accuracy: 0.2800\n",
      "Iter-1210 train loss: 1.9570 valid loss: 2.0400, valid accuracy: 0.2858\n",
      "Iter-1220 train loss: 2.0348 valid loss: 2.0358, valid accuracy: 0.2914\n",
      "Iter-1230 train loss: 2.0167 valid loss: 2.0313, valid accuracy: 0.2960\n",
      "Iter-1240 train loss: 1.9763 valid loss: 2.0273, valid accuracy: 0.3002\n",
      "Iter-1250 train loss: 2.0703 valid loss: 2.0231, valid accuracy: 0.3034\n",
      "Iter-1260 train loss: 1.9835 valid loss: 2.0186, valid accuracy: 0.3076\n",
      "Iter-1270 train loss: 2.0061 valid loss: 2.0143, valid accuracy: 0.3104\n",
      "Iter-1280 train loss: 2.0538 valid loss: 2.0099, valid accuracy: 0.3160\n",
      "Iter-1290 train loss: 2.0621 valid loss: 2.0056, valid accuracy: 0.3184\n",
      "Iter-1300 train loss: 2.0793 valid loss: 2.0012, valid accuracy: 0.3214\n",
      "Iter-1310 train loss: 2.0095 valid loss: 1.9970, valid accuracy: 0.3258\n",
      "Iter-1320 train loss: 2.0818 valid loss: 1.9930, valid accuracy: 0.3282\n",
      "Iter-1330 train loss: 1.9996 valid loss: 1.9885, valid accuracy: 0.3322\n",
      "Iter-1340 train loss: 2.0890 valid loss: 1.9843, valid accuracy: 0.3364\n",
      "Iter-1350 train loss: 2.0066 valid loss: 1.9801, valid accuracy: 0.3398\n",
      "Iter-1360 train loss: 2.0032 valid loss: 1.9760, valid accuracy: 0.3428\n",
      "Iter-1370 train loss: 2.0098 valid loss: 1.9714, valid accuracy: 0.3472\n",
      "Iter-1380 train loss: 1.9994 valid loss: 1.9671, valid accuracy: 0.3524\n",
      "Iter-1390 train loss: 1.9906 valid loss: 1.9627, valid accuracy: 0.3554\n",
      "Iter-1400 train loss: 1.9818 valid loss: 1.9580, valid accuracy: 0.3596\n",
      "Iter-1410 train loss: 1.9555 valid loss: 1.9535, valid accuracy: 0.3644\n",
      "Iter-1420 train loss: 1.9630 valid loss: 1.9492, valid accuracy: 0.3658\n",
      "Iter-1430 train loss: 1.9788 valid loss: 1.9451, valid accuracy: 0.3682\n",
      "Iter-1440 train loss: 1.9109 valid loss: 1.9408, valid accuracy: 0.3706\n",
      "Iter-1450 train loss: 1.9024 valid loss: 1.9364, valid accuracy: 0.3730\n",
      "Iter-1460 train loss: 1.9971 valid loss: 1.9321, valid accuracy: 0.3760\n",
      "Iter-1470 train loss: 1.9034 valid loss: 1.9277, valid accuracy: 0.3786\n",
      "Iter-1480 train loss: 1.9575 valid loss: 1.9232, valid accuracy: 0.3810\n",
      "Iter-1490 train loss: 1.8729 valid loss: 1.9188, valid accuracy: 0.3850\n",
      "Iter-1500 train loss: 1.9047 valid loss: 1.9146, valid accuracy: 0.3886\n",
      "Iter-1510 train loss: 1.9542 valid loss: 1.9102, valid accuracy: 0.3900\n",
      "Iter-1520 train loss: 1.9849 valid loss: 1.9057, valid accuracy: 0.3916\n",
      "Iter-1530 train loss: 1.9259 valid loss: 1.9015, valid accuracy: 0.3934\n",
      "Iter-1540 train loss: 1.9789 valid loss: 1.8972, valid accuracy: 0.3956\n",
      "Iter-1550 train loss: 1.8823 valid loss: 1.8929, valid accuracy: 0.3984\n",
      "Iter-1560 train loss: 1.9138 valid loss: 1.8886, valid accuracy: 0.3992\n",
      "Iter-1570 train loss: 1.9125 valid loss: 1.8843, valid accuracy: 0.4020\n",
      "Iter-1580 train loss: 1.9710 valid loss: 1.8801, valid accuracy: 0.4034\n",
      "Iter-1590 train loss: 1.9254 valid loss: 1.8759, valid accuracy: 0.4060\n",
      "Iter-1600 train loss: 1.7903 valid loss: 1.8716, valid accuracy: 0.4076\n",
      "Iter-1610 train loss: 1.8424 valid loss: 1.8674, valid accuracy: 0.4084\n",
      "Iter-1620 train loss: 1.8353 valid loss: 1.8633, valid accuracy: 0.4106\n",
      "Iter-1630 train loss: 1.8567 valid loss: 1.8592, valid accuracy: 0.4124\n",
      "Iter-1640 train loss: 1.8604 valid loss: 1.8551, valid accuracy: 0.4154\n",
      "Iter-1650 train loss: 1.9260 valid loss: 1.8511, valid accuracy: 0.4162\n",
      "Iter-1660 train loss: 1.8489 valid loss: 1.8470, valid accuracy: 0.4156\n",
      "Iter-1670 train loss: 1.7983 valid loss: 1.8426, valid accuracy: 0.4176\n",
      "Iter-1680 train loss: 1.9945 valid loss: 1.8383, valid accuracy: 0.4196\n",
      "Iter-1690 train loss: 1.7913 valid loss: 1.8339, valid accuracy: 0.4194\n",
      "Iter-1700 train loss: 1.8495 valid loss: 1.8299, valid accuracy: 0.4204\n",
      "Iter-1710 train loss: 1.8719 valid loss: 1.8258, valid accuracy: 0.4232\n",
      "Iter-1720 train loss: 1.8912 valid loss: 1.8217, valid accuracy: 0.4238\n",
      "Iter-1730 train loss: 1.8262 valid loss: 1.8177, valid accuracy: 0.4244\n",
      "Iter-1740 train loss: 1.8432 valid loss: 1.8137, valid accuracy: 0.4252\n",
      "Iter-1750 train loss: 1.7768 valid loss: 1.8095, valid accuracy: 0.4262\n",
      "Iter-1760 train loss: 1.8893 valid loss: 1.8056, valid accuracy: 0.4278\n",
      "Iter-1770 train loss: 1.8160 valid loss: 1.8016, valid accuracy: 0.4292\n",
      "Iter-1780 train loss: 1.7188 valid loss: 1.7975, valid accuracy: 0.4304\n",
      "Iter-1790 train loss: 1.8117 valid loss: 1.7935, valid accuracy: 0.4318\n",
      "Iter-1800 train loss: 1.7775 valid loss: 1.7894, valid accuracy: 0.4326\n",
      "Iter-1810 train loss: 1.7123 valid loss: 1.7856, valid accuracy: 0.4344\n",
      "Iter-1820 train loss: 1.7464 valid loss: 1.7818, valid accuracy: 0.4352\n",
      "Iter-1830 train loss: 1.7587 valid loss: 1.7778, valid accuracy: 0.4364\n",
      "Iter-1840 train loss: 1.7342 valid loss: 1.7739, valid accuracy: 0.4378\n",
      "Iter-1850 train loss: 1.9157 valid loss: 1.7699, valid accuracy: 0.4394\n",
      "Iter-1860 train loss: 1.9437 valid loss: 1.7662, valid accuracy: 0.4402\n",
      "Iter-1870 train loss: 1.7364 valid loss: 1.7623, valid accuracy: 0.4416\n",
      "Iter-1880 train loss: 1.7592 valid loss: 1.7583, valid accuracy: 0.4418\n",
      "Iter-1890 train loss: 1.6578 valid loss: 1.7543, valid accuracy: 0.4422\n",
      "Iter-1900 train loss: 1.7498 valid loss: 1.7506, valid accuracy: 0.4430\n",
      "Iter-1910 train loss: 1.8279 valid loss: 1.7468, valid accuracy: 0.4436\n",
      "Iter-1920 train loss: 1.7144 valid loss: 1.7429, valid accuracy: 0.4442\n",
      "Iter-1930 train loss: 1.6798 valid loss: 1.7389, valid accuracy: 0.4458\n",
      "Iter-1940 train loss: 1.8763 valid loss: 1.7350, valid accuracy: 0.4462\n",
      "Iter-1950 train loss: 1.7728 valid loss: 1.7310, valid accuracy: 0.4470\n",
      "Iter-1960 train loss: 1.6788 valid loss: 1.7271, valid accuracy: 0.4482\n",
      "Iter-1970 train loss: 1.7578 valid loss: 1.7230, valid accuracy: 0.4508\n",
      "Iter-1980 train loss: 1.7272 valid loss: 1.7192, valid accuracy: 0.4516\n",
      "Iter-1990 train loss: 1.6450 valid loss: 1.7154, valid accuracy: 0.4520\n",
      "Iter-2000 train loss: 1.7520 valid loss: 1.7116, valid accuracy: 0.4526\n",
      "Iter-2010 train loss: 1.7596 valid loss: 1.7079, valid accuracy: 0.4544\n",
      "Iter-2020 train loss: 1.8273 valid loss: 1.7041, valid accuracy: 0.4562\n",
      "Iter-2030 train loss: 1.7337 valid loss: 1.7002, valid accuracy: 0.4562\n",
      "Iter-2040 train loss: 1.6235 valid loss: 1.6964, valid accuracy: 0.4602\n",
      "Iter-2050 train loss: 1.7041 valid loss: 1.6926, valid accuracy: 0.4624\n",
      "Iter-2060 train loss: 1.6540 valid loss: 1.6888, valid accuracy: 0.4638\n",
      "Iter-2070 train loss: 1.6669 valid loss: 1.6850, valid accuracy: 0.4650\n",
      "Iter-2080 train loss: 1.6573 valid loss: 1.6812, valid accuracy: 0.4656\n",
      "Iter-2090 train loss: 1.6933 valid loss: 1.6774, valid accuracy: 0.4668\n",
      "Iter-2100 train loss: 1.6925 valid loss: 1.6736, valid accuracy: 0.4698\n",
      "Iter-2110 train loss: 1.6792 valid loss: 1.6698, valid accuracy: 0.4714\n",
      "Iter-2120 train loss: 1.6028 valid loss: 1.6660, valid accuracy: 0.4744\n",
      "Iter-2130 train loss: 1.6099 valid loss: 1.6623, valid accuracy: 0.4760\n",
      "Iter-2140 train loss: 1.7085 valid loss: 1.6586, valid accuracy: 0.4766\n",
      "Iter-2150 train loss: 1.5668 valid loss: 1.6549, valid accuracy: 0.4780\n",
      "Iter-2160 train loss: 1.6936 valid loss: 1.6515, valid accuracy: 0.4796\n",
      "Iter-2170 train loss: 1.6777 valid loss: 1.6480, valid accuracy: 0.4810\n",
      "Iter-2180 train loss: 1.5385 valid loss: 1.6445, valid accuracy: 0.4816\n",
      "Iter-2190 train loss: 1.8185 valid loss: 1.6409, valid accuracy: 0.4830\n",
      "Iter-2200 train loss: 1.6821 valid loss: 1.6374, valid accuracy: 0.4856\n",
      "Iter-2210 train loss: 1.6962 valid loss: 1.6339, valid accuracy: 0.4886\n",
      "Iter-2220 train loss: 1.5796 valid loss: 1.6304, valid accuracy: 0.4904\n",
      "Iter-2230 train loss: 1.7034 valid loss: 1.6269, valid accuracy: 0.4922\n",
      "Iter-2240 train loss: 1.6718 valid loss: 1.6234, valid accuracy: 0.4930\n",
      "Iter-2250 train loss: 1.6953 valid loss: 1.6200, valid accuracy: 0.4952\n",
      "Iter-2260 train loss: 1.5220 valid loss: 1.6167, valid accuracy: 0.4960\n",
      "Iter-2270 train loss: 1.5234 valid loss: 1.6133, valid accuracy: 0.4970\n",
      "Iter-2280 train loss: 1.6552 valid loss: 1.6099, valid accuracy: 0.4984\n",
      "Iter-2290 train loss: 1.7549 valid loss: 1.6065, valid accuracy: 0.5002\n",
      "Iter-2300 train loss: 1.5616 valid loss: 1.6031, valid accuracy: 0.5024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 1.4578 valid loss: 1.5996, valid accuracy: 0.5048\n",
      "Iter-2320 train loss: 1.7523 valid loss: 1.5962, valid accuracy: 0.5056\n",
      "Iter-2330 train loss: 1.6443 valid loss: 1.5929, valid accuracy: 0.5068\n",
      "Iter-2340 train loss: 1.6515 valid loss: 1.5895, valid accuracy: 0.5082\n",
      "Iter-2350 train loss: 1.5935 valid loss: 1.5862, valid accuracy: 0.5096\n",
      "Iter-2360 train loss: 1.6877 valid loss: 1.5829, valid accuracy: 0.5120\n",
      "Iter-2370 train loss: 1.5240 valid loss: 1.5794, valid accuracy: 0.5144\n",
      "Iter-2380 train loss: 1.5491 valid loss: 1.5761, valid accuracy: 0.5158\n",
      "Iter-2390 train loss: 1.6150 valid loss: 1.5728, valid accuracy: 0.5174\n",
      "Iter-2400 train loss: 1.6468 valid loss: 1.5695, valid accuracy: 0.5190\n",
      "Iter-2410 train loss: 1.5510 valid loss: 1.5661, valid accuracy: 0.5230\n",
      "Iter-2420 train loss: 1.6942 valid loss: 1.5628, valid accuracy: 0.5242\n",
      "Iter-2430 train loss: 1.7203 valid loss: 1.5595, valid accuracy: 0.5258\n",
      "Iter-2440 train loss: 1.6890 valid loss: 1.5562, valid accuracy: 0.5268\n",
      "Iter-2450 train loss: 1.6041 valid loss: 1.5532, valid accuracy: 0.5284\n",
      "Iter-2460 train loss: 1.6120 valid loss: 1.5500, valid accuracy: 0.5286\n",
      "Iter-2470 train loss: 1.6323 valid loss: 1.5466, valid accuracy: 0.5306\n",
      "Iter-2480 train loss: 1.5777 valid loss: 1.5434, valid accuracy: 0.5326\n",
      "Iter-2490 train loss: 1.4850 valid loss: 1.5402, valid accuracy: 0.5338\n",
      "Iter-2500 train loss: 1.4981 valid loss: 1.5371, valid accuracy: 0.5346\n",
      "Iter-2510 train loss: 1.5250 valid loss: 1.5337, valid accuracy: 0.5380\n",
      "Iter-2520 train loss: 1.5142 valid loss: 1.5304, valid accuracy: 0.5390\n",
      "Iter-2530 train loss: 1.5431 valid loss: 1.5271, valid accuracy: 0.5402\n",
      "Iter-2540 train loss: 1.5974 valid loss: 1.5240, valid accuracy: 0.5418\n",
      "Iter-2550 train loss: 1.4627 valid loss: 1.5207, valid accuracy: 0.5440\n",
      "Iter-2560 train loss: 1.5398 valid loss: 1.5176, valid accuracy: 0.5458\n",
      "Iter-2570 train loss: 1.5452 valid loss: 1.5145, valid accuracy: 0.5456\n",
      "Iter-2580 train loss: 1.5970 valid loss: 1.5113, valid accuracy: 0.5478\n",
      "Iter-2590 train loss: 1.5206 valid loss: 1.5081, valid accuracy: 0.5482\n",
      "Iter-2600 train loss: 1.5454 valid loss: 1.5050, valid accuracy: 0.5502\n",
      "Iter-2610 train loss: 1.5259 valid loss: 1.5020, valid accuracy: 0.5506\n",
      "Iter-2620 train loss: 1.3604 valid loss: 1.4990, valid accuracy: 0.5514\n",
      "Iter-2630 train loss: 1.5718 valid loss: 1.4958, valid accuracy: 0.5530\n",
      "Iter-2640 train loss: 1.5263 valid loss: 1.4927, valid accuracy: 0.5542\n",
      "Iter-2650 train loss: 1.5040 valid loss: 1.4896, valid accuracy: 0.5546\n",
      "Iter-2660 train loss: 1.4106 valid loss: 1.4865, valid accuracy: 0.5552\n",
      "Iter-2670 train loss: 1.3863 valid loss: 1.4835, valid accuracy: 0.5562\n",
      "Iter-2680 train loss: 1.4495 valid loss: 1.4804, valid accuracy: 0.5570\n",
      "Iter-2690 train loss: 1.4333 valid loss: 1.4773, valid accuracy: 0.5578\n",
      "Iter-2700 train loss: 1.5300 valid loss: 1.4743, valid accuracy: 0.5588\n",
      "Iter-2710 train loss: 1.4997 valid loss: 1.4713, valid accuracy: 0.5598\n",
      "Iter-2720 train loss: 1.5057 valid loss: 1.4684, valid accuracy: 0.5606\n",
      "Iter-2730 train loss: 1.5857 valid loss: 1.4655, valid accuracy: 0.5620\n",
      "Iter-2740 train loss: 1.4553 valid loss: 1.4626, valid accuracy: 0.5628\n",
      "Iter-2750 train loss: 1.5536 valid loss: 1.4597, valid accuracy: 0.5640\n",
      "Iter-2760 train loss: 1.5799 valid loss: 1.4567, valid accuracy: 0.5650\n",
      "Iter-2770 train loss: 1.4466 valid loss: 1.4539, valid accuracy: 0.5666\n",
      "Iter-2780 train loss: 1.5022 valid loss: 1.4509, valid accuracy: 0.5686\n",
      "Iter-2790 train loss: 1.4652 valid loss: 1.4480, valid accuracy: 0.5708\n",
      "Iter-2800 train loss: 1.4147 valid loss: 1.4451, valid accuracy: 0.5716\n",
      "Iter-2810 train loss: 1.5966 valid loss: 1.4421, valid accuracy: 0.5722\n",
      "Iter-2820 train loss: 1.5098 valid loss: 1.4392, valid accuracy: 0.5738\n",
      "Iter-2830 train loss: 1.3342 valid loss: 1.4362, valid accuracy: 0.5746\n",
      "Iter-2840 train loss: 1.4385 valid loss: 1.4335, valid accuracy: 0.5748\n",
      "Iter-2850 train loss: 1.4478 valid loss: 1.4306, valid accuracy: 0.5748\n",
      "Iter-2860 train loss: 1.4143 valid loss: 1.4279, valid accuracy: 0.5760\n",
      "Iter-2870 train loss: 1.3742 valid loss: 1.4251, valid accuracy: 0.5770\n",
      "Iter-2880 train loss: 1.4246 valid loss: 1.4222, valid accuracy: 0.5772\n",
      "Iter-2890 train loss: 1.3655 valid loss: 1.4194, valid accuracy: 0.5788\n",
      "Iter-2900 train loss: 1.5296 valid loss: 1.4165, valid accuracy: 0.5794\n",
      "Iter-2910 train loss: 1.4809 valid loss: 1.4135, valid accuracy: 0.5808\n",
      "Iter-2920 train loss: 1.4479 valid loss: 1.4107, valid accuracy: 0.5814\n",
      "Iter-2930 train loss: 1.4264 valid loss: 1.4078, valid accuracy: 0.5824\n",
      "Iter-2940 train loss: 1.4925 valid loss: 1.4050, valid accuracy: 0.5828\n",
      "Iter-2950 train loss: 1.3517 valid loss: 1.4023, valid accuracy: 0.5828\n",
      "Iter-2960 train loss: 1.4326 valid loss: 1.3995, valid accuracy: 0.5842\n",
      "Iter-2970 train loss: 1.3429 valid loss: 1.3967, valid accuracy: 0.5854\n",
      "Iter-2980 train loss: 1.4228 valid loss: 1.3938, valid accuracy: 0.5860\n",
      "Iter-2990 train loss: 1.4683 valid loss: 1.3911, valid accuracy: 0.5872\n",
      "Iter-3000 train loss: 1.2830 valid loss: 1.3883, valid accuracy: 0.5872\n",
      "Iter-3010 train loss: 1.4875 valid loss: 1.3854, valid accuracy: 0.5898\n",
      "Iter-3020 train loss: 1.2939 valid loss: 1.3826, valid accuracy: 0.5908\n",
      "Iter-3030 train loss: 1.4178 valid loss: 1.3800, valid accuracy: 0.5920\n",
      "Iter-3040 train loss: 1.2777 valid loss: 1.3773, valid accuracy: 0.5926\n",
      "Iter-3050 train loss: 1.5008 valid loss: 1.3746, valid accuracy: 0.5938\n",
      "Iter-3060 train loss: 1.3775 valid loss: 1.3719, valid accuracy: 0.5952\n",
      "Iter-3070 train loss: 1.4055 valid loss: 1.3692, valid accuracy: 0.5974\n",
      "Iter-3080 train loss: 1.3865 valid loss: 1.3666, valid accuracy: 0.5976\n",
      "Iter-3090 train loss: 1.4527 valid loss: 1.3638, valid accuracy: 0.5986\n",
      "Iter-3100 train loss: 1.3359 valid loss: 1.3612, valid accuracy: 0.5986\n",
      "Iter-3110 train loss: 1.2678 valid loss: 1.3586, valid accuracy: 0.5994\n",
      "Iter-3120 train loss: 1.3053 valid loss: 1.3558, valid accuracy: 0.6006\n",
      "Iter-3130 train loss: 1.4226 valid loss: 1.3531, valid accuracy: 0.6016\n",
      "Iter-3140 train loss: 1.3762 valid loss: 1.3504, valid accuracy: 0.6014\n",
      "Iter-3150 train loss: 1.4513 valid loss: 1.3477, valid accuracy: 0.6028\n",
      "Iter-3160 train loss: 1.3366 valid loss: 1.3451, valid accuracy: 0.6034\n",
      "Iter-3170 train loss: 1.3495 valid loss: 1.3427, valid accuracy: 0.6044\n",
      "Iter-3180 train loss: 1.3332 valid loss: 1.3400, valid accuracy: 0.6060\n",
      "Iter-3190 train loss: 1.3284 valid loss: 1.3374, valid accuracy: 0.6076\n",
      "Iter-3200 train loss: 1.3648 valid loss: 1.3348, valid accuracy: 0.6088\n",
      "Iter-3210 train loss: 1.3518 valid loss: 1.3322, valid accuracy: 0.6100\n",
      "Iter-3220 train loss: 1.3384 valid loss: 1.3295, valid accuracy: 0.6104\n",
      "Iter-3230 train loss: 1.2769 valid loss: 1.3268, valid accuracy: 0.6126\n",
      "Iter-3240 train loss: 1.3746 valid loss: 1.3243, valid accuracy: 0.6138\n",
      "Iter-3250 train loss: 1.3433 valid loss: 1.3218, valid accuracy: 0.6142\n",
      "Iter-3260 train loss: 1.4038 valid loss: 1.3192, valid accuracy: 0.6156\n",
      "Iter-3270 train loss: 1.3376 valid loss: 1.3168, valid accuracy: 0.6164\n",
      "Iter-3280 train loss: 1.4199 valid loss: 1.3143, valid accuracy: 0.6166\n",
      "Iter-3290 train loss: 1.2079 valid loss: 1.3117, valid accuracy: 0.6164\n",
      "Iter-3300 train loss: 1.2201 valid loss: 1.3091, valid accuracy: 0.6174\n",
      "Iter-3310 train loss: 1.2554 valid loss: 1.3067, valid accuracy: 0.6178\n",
      "Iter-3320 train loss: 1.1075 valid loss: 1.3042, valid accuracy: 0.6182\n",
      "Iter-3330 train loss: 1.2201 valid loss: 1.3016, valid accuracy: 0.6188\n",
      "Iter-3340 train loss: 1.2883 valid loss: 1.2990, valid accuracy: 0.6196\n",
      "Iter-3350 train loss: 1.4288 valid loss: 1.2967, valid accuracy: 0.6210\n",
      "Iter-3360 train loss: 1.5249 valid loss: 1.2941, valid accuracy: 0.6216\n",
      "Iter-3370 train loss: 1.3221 valid loss: 1.2918, valid accuracy: 0.6224\n",
      "Iter-3380 train loss: 1.3473 valid loss: 1.2894, valid accuracy: 0.6230\n",
      "Iter-3390 train loss: 1.2785 valid loss: 1.2868, valid accuracy: 0.6230\n",
      "Iter-3400 train loss: 1.2542 valid loss: 1.2844, valid accuracy: 0.6238\n",
      "Iter-3410 train loss: 1.3709 valid loss: 1.2820, valid accuracy: 0.6242\n",
      "Iter-3420 train loss: 1.3168 valid loss: 1.2795, valid accuracy: 0.6250\n",
      "Iter-3430 train loss: 1.2397 valid loss: 1.2770, valid accuracy: 0.6264\n",
      "Iter-3440 train loss: 1.3578 valid loss: 1.2747, valid accuracy: 0.6276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 1.1492 valid loss: 1.2724, valid accuracy: 0.6280\n",
      "Iter-3460 train loss: 1.2521 valid loss: 1.2699, valid accuracy: 0.6286\n",
      "Iter-3470 train loss: 1.1968 valid loss: 1.2677, valid accuracy: 0.6292\n",
      "Iter-3480 train loss: 1.3446 valid loss: 1.2653, valid accuracy: 0.6302\n",
      "Iter-3490 train loss: 1.1629 valid loss: 1.2630, valid accuracy: 0.6310\n",
      "Iter-3500 train loss: 1.2744 valid loss: 1.2606, valid accuracy: 0.6316\n",
      "Iter-3510 train loss: 1.2444 valid loss: 1.2581, valid accuracy: 0.6324\n",
      "Iter-3520 train loss: 1.2237 valid loss: 1.2559, valid accuracy: 0.6326\n",
      "Iter-3530 train loss: 1.3421 valid loss: 1.2536, valid accuracy: 0.6336\n",
      "Iter-3540 train loss: 1.2574 valid loss: 1.2513, valid accuracy: 0.6340\n",
      "Iter-3550 train loss: 1.1390 valid loss: 1.2490, valid accuracy: 0.6344\n",
      "Iter-3560 train loss: 1.1556 valid loss: 1.2467, valid accuracy: 0.6356\n",
      "Iter-3570 train loss: 1.2349 valid loss: 1.2444, valid accuracy: 0.6364\n",
      "Iter-3580 train loss: 1.2515 valid loss: 1.2422, valid accuracy: 0.6366\n",
      "Iter-3590 train loss: 1.2230 valid loss: 1.2399, valid accuracy: 0.6376\n",
      "Iter-3600 train loss: 1.2516 valid loss: 1.2376, valid accuracy: 0.6382\n",
      "Iter-3610 train loss: 1.2801 valid loss: 1.2353, valid accuracy: 0.6388\n",
      "Iter-3620 train loss: 1.3091 valid loss: 1.2330, valid accuracy: 0.6404\n",
      "Iter-3630 train loss: 1.1619 valid loss: 1.2309, valid accuracy: 0.6412\n",
      "Iter-3640 train loss: 1.1862 valid loss: 1.2287, valid accuracy: 0.6414\n",
      "Iter-3650 train loss: 1.1938 valid loss: 1.2266, valid accuracy: 0.6426\n",
      "Iter-3660 train loss: 1.2806 valid loss: 1.2243, valid accuracy: 0.6434\n",
      "Iter-3670 train loss: 1.2694 valid loss: 1.2222, valid accuracy: 0.6446\n",
      "Iter-3680 train loss: 1.2897 valid loss: 1.2199, valid accuracy: 0.6462\n",
      "Iter-3690 train loss: 1.1928 valid loss: 1.2178, valid accuracy: 0.6468\n",
      "Iter-3700 train loss: 1.2398 valid loss: 1.2157, valid accuracy: 0.6488\n",
      "Iter-3710 train loss: 1.2040 valid loss: 1.2136, valid accuracy: 0.6492\n",
      "Iter-3720 train loss: 1.2430 valid loss: 1.2115, valid accuracy: 0.6496\n",
      "Iter-3730 train loss: 1.1680 valid loss: 1.2093, valid accuracy: 0.6504\n",
      "Iter-3740 train loss: 1.2546 valid loss: 1.2073, valid accuracy: 0.6502\n",
      "Iter-3750 train loss: 1.1772 valid loss: 1.2052, valid accuracy: 0.6508\n",
      "Iter-3760 train loss: 1.2661 valid loss: 1.2031, valid accuracy: 0.6514\n",
      "Iter-3770 train loss: 1.1943 valid loss: 1.2010, valid accuracy: 0.6518\n",
      "Iter-3780 train loss: 1.0989 valid loss: 1.1988, valid accuracy: 0.6528\n",
      "Iter-3790 train loss: 1.1979 valid loss: 1.1968, valid accuracy: 0.6534\n",
      "Iter-3800 train loss: 1.3068 valid loss: 1.1947, valid accuracy: 0.6536\n",
      "Iter-3810 train loss: 1.2271 valid loss: 1.1925, valid accuracy: 0.6542\n",
      "Iter-3820 train loss: 1.2044 valid loss: 1.1906, valid accuracy: 0.6540\n",
      "Iter-3830 train loss: 1.3432 valid loss: 1.1885, valid accuracy: 0.6546\n",
      "Iter-3840 train loss: 1.1763 valid loss: 1.1863, valid accuracy: 0.6560\n",
      "Iter-3850 train loss: 1.1134 valid loss: 1.1843, valid accuracy: 0.6564\n",
      "Iter-3860 train loss: 1.0496 valid loss: 1.1822, valid accuracy: 0.6570\n",
      "Iter-3870 train loss: 1.1974 valid loss: 1.1801, valid accuracy: 0.6572\n",
      "Iter-3880 train loss: 1.1995 valid loss: 1.1781, valid accuracy: 0.6572\n",
      "Iter-3890 train loss: 1.0441 valid loss: 1.1760, valid accuracy: 0.6584\n",
      "Iter-3900 train loss: 1.1155 valid loss: 1.1740, valid accuracy: 0.6584\n",
      "Iter-3910 train loss: 1.2516 valid loss: 1.1719, valid accuracy: 0.6586\n",
      "Iter-3920 train loss: 1.1790 valid loss: 1.1699, valid accuracy: 0.6586\n",
      "Iter-3930 train loss: 1.0076 valid loss: 1.1679, valid accuracy: 0.6596\n",
      "Iter-3940 train loss: 1.1635 valid loss: 1.1659, valid accuracy: 0.6606\n",
      "Iter-3950 train loss: 1.1558 valid loss: 1.1640, valid accuracy: 0.6614\n",
      "Iter-3960 train loss: 1.0856 valid loss: 1.1619, valid accuracy: 0.6628\n",
      "Iter-3970 train loss: 1.1473 valid loss: 1.1599, valid accuracy: 0.6632\n",
      "Iter-3980 train loss: 1.2928 valid loss: 1.1578, valid accuracy: 0.6634\n",
      "Iter-3990 train loss: 1.1077 valid loss: 1.1558, valid accuracy: 0.6634\n",
      "Iter-4000 train loss: 1.1855 valid loss: 1.1539, valid accuracy: 0.6644\n",
      "Iter-4010 train loss: 1.2882 valid loss: 1.1519, valid accuracy: 0.6648\n",
      "Iter-4020 train loss: 1.3262 valid loss: 1.1499, valid accuracy: 0.6664\n",
      "Iter-4030 train loss: 1.1628 valid loss: 1.1480, valid accuracy: 0.6672\n",
      "Iter-4040 train loss: 1.1772 valid loss: 1.1460, valid accuracy: 0.6682\n",
      "Iter-4050 train loss: 1.1881 valid loss: 1.1441, valid accuracy: 0.6682\n",
      "Iter-4060 train loss: 1.0868 valid loss: 1.1421, valid accuracy: 0.6686\n",
      "Iter-4070 train loss: 1.0993 valid loss: 1.1401, valid accuracy: 0.6690\n",
      "Iter-4080 train loss: 1.2340 valid loss: 1.1383, valid accuracy: 0.6694\n",
      "Iter-4090 train loss: 1.2943 valid loss: 1.1364, valid accuracy: 0.6694\n",
      "Iter-4100 train loss: 1.1729 valid loss: 1.1346, valid accuracy: 0.6694\n",
      "Iter-4110 train loss: 1.2816 valid loss: 1.1326, valid accuracy: 0.6698\n",
      "Iter-4120 train loss: 1.1493 valid loss: 1.1307, valid accuracy: 0.6694\n",
      "Iter-4130 train loss: 1.2092 valid loss: 1.1288, valid accuracy: 0.6704\n",
      "Iter-4140 train loss: 1.1395 valid loss: 1.1268, valid accuracy: 0.6704\n",
      "Iter-4150 train loss: 1.2072 valid loss: 1.1250, valid accuracy: 0.6710\n",
      "Iter-4160 train loss: 1.0994 valid loss: 1.1232, valid accuracy: 0.6716\n",
      "Iter-4170 train loss: 1.2170 valid loss: 1.1213, valid accuracy: 0.6728\n",
      "Iter-4180 train loss: 1.3188 valid loss: 1.1196, valid accuracy: 0.6742\n",
      "Iter-4190 train loss: 1.2245 valid loss: 1.1178, valid accuracy: 0.6748\n",
      "Iter-4200 train loss: 1.1147 valid loss: 1.1159, valid accuracy: 0.6756\n",
      "Iter-4210 train loss: 1.0707 valid loss: 1.1141, valid accuracy: 0.6760\n",
      "Iter-4220 train loss: 1.0803 valid loss: 1.1125, valid accuracy: 0.6762\n",
      "Iter-4230 train loss: 1.1478 valid loss: 1.1107, valid accuracy: 0.6764\n",
      "Iter-4240 train loss: 1.1136 valid loss: 1.1088, valid accuracy: 0.6766\n",
      "Iter-4250 train loss: 1.1162 valid loss: 1.1070, valid accuracy: 0.6772\n",
      "Iter-4260 train loss: 1.2570 valid loss: 1.1052, valid accuracy: 0.6772\n",
      "Iter-4270 train loss: 1.3026 valid loss: 1.1035, valid accuracy: 0.6782\n",
      "Iter-4280 train loss: 1.0225 valid loss: 1.1017, valid accuracy: 0.6784\n",
      "Iter-4290 train loss: 1.2352 valid loss: 1.0999, valid accuracy: 0.6792\n",
      "Iter-4300 train loss: 1.2891 valid loss: 1.0981, valid accuracy: 0.6802\n",
      "Iter-4310 train loss: 1.0131 valid loss: 1.0963, valid accuracy: 0.6810\n",
      "Iter-4320 train loss: 1.2256 valid loss: 1.0945, valid accuracy: 0.6808\n",
      "Iter-4330 train loss: 0.9764 valid loss: 1.0928, valid accuracy: 0.6812\n",
      "Iter-4340 train loss: 1.2790 valid loss: 1.0911, valid accuracy: 0.6820\n",
      "Iter-4350 train loss: 1.0387 valid loss: 1.0893, valid accuracy: 0.6826\n",
      "Iter-4360 train loss: 1.1529 valid loss: 1.0875, valid accuracy: 0.6832\n",
      "Iter-4370 train loss: 0.9319 valid loss: 1.0858, valid accuracy: 0.6834\n",
      "Iter-4380 train loss: 1.0585 valid loss: 1.0841, valid accuracy: 0.6834\n",
      "Iter-4390 train loss: 1.2173 valid loss: 1.0825, valid accuracy: 0.6844\n",
      "Iter-4400 train loss: 1.1180 valid loss: 1.0808, valid accuracy: 0.6846\n",
      "Iter-4410 train loss: 1.2201 valid loss: 1.0792, valid accuracy: 0.6850\n",
      "Iter-4420 train loss: 1.2012 valid loss: 1.0776, valid accuracy: 0.6860\n",
      "Iter-4430 train loss: 1.1468 valid loss: 1.0759, valid accuracy: 0.6862\n",
      "Iter-4440 train loss: 1.1645 valid loss: 1.0743, valid accuracy: 0.6876\n",
      "Iter-4450 train loss: 1.0925 valid loss: 1.0728, valid accuracy: 0.6884\n",
      "Iter-4460 train loss: 1.1484 valid loss: 1.0711, valid accuracy: 0.6892\n",
      "Iter-4470 train loss: 1.1020 valid loss: 1.0695, valid accuracy: 0.6896\n",
      "Iter-4480 train loss: 1.1577 valid loss: 1.0678, valid accuracy: 0.6898\n",
      "Iter-4490 train loss: 1.2685 valid loss: 1.0661, valid accuracy: 0.6908\n",
      "Iter-4500 train loss: 1.1668 valid loss: 1.0643, valid accuracy: 0.6918\n",
      "Iter-4510 train loss: 0.9676 valid loss: 1.0626, valid accuracy: 0.6922\n",
      "Iter-4520 train loss: 1.1495 valid loss: 1.0610, valid accuracy: 0.6928\n",
      "Iter-4530 train loss: 1.0117 valid loss: 1.0595, valid accuracy: 0.6928\n",
      "Iter-4540 train loss: 1.0744 valid loss: 1.0579, valid accuracy: 0.6932\n",
      "Iter-4550 train loss: 1.0588 valid loss: 1.0563, valid accuracy: 0.6938\n",
      "Iter-4560 train loss: 1.0420 valid loss: 1.0548, valid accuracy: 0.6942\n",
      "Iter-4570 train loss: 1.0406 valid loss: 1.0531, valid accuracy: 0.6954\n",
      "Iter-4580 train loss: 1.1961 valid loss: 1.0516, valid accuracy: 0.6950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 1.0516 valid loss: 1.0499, valid accuracy: 0.6956\n",
      "Iter-4600 train loss: 1.0400 valid loss: 1.0484, valid accuracy: 0.6960\n",
      "Iter-4610 train loss: 0.9795 valid loss: 1.0469, valid accuracy: 0.6964\n",
      "Iter-4620 train loss: 1.0389 valid loss: 1.0453, valid accuracy: 0.6962\n",
      "Iter-4630 train loss: 1.0772 valid loss: 1.0437, valid accuracy: 0.6968\n",
      "Iter-4640 train loss: 1.1042 valid loss: 1.0421, valid accuracy: 0.6972\n",
      "Iter-4650 train loss: 1.2031 valid loss: 1.0406, valid accuracy: 0.6982\n",
      "Iter-4660 train loss: 0.9503 valid loss: 1.0390, valid accuracy: 0.6986\n",
      "Iter-4670 train loss: 1.0057 valid loss: 1.0374, valid accuracy: 0.6998\n",
      "Iter-4680 train loss: 1.0404 valid loss: 1.0359, valid accuracy: 0.6998\n",
      "Iter-4690 train loss: 1.0975 valid loss: 1.0344, valid accuracy: 0.7004\n",
      "Iter-4700 train loss: 0.9923 valid loss: 1.0329, valid accuracy: 0.7010\n",
      "Iter-4710 train loss: 1.2054 valid loss: 1.0315, valid accuracy: 0.7016\n",
      "Iter-4720 train loss: 1.2837 valid loss: 1.0300, valid accuracy: 0.7020\n",
      "Iter-4730 train loss: 1.1333 valid loss: 1.0284, valid accuracy: 0.7024\n",
      "Iter-4740 train loss: 1.0169 valid loss: 1.0270, valid accuracy: 0.7030\n",
      "Iter-4750 train loss: 0.9556 valid loss: 1.0255, valid accuracy: 0.7032\n",
      "Iter-4760 train loss: 1.2579 valid loss: 1.0239, valid accuracy: 0.7046\n",
      "Iter-4770 train loss: 1.0363 valid loss: 1.0224, valid accuracy: 0.7050\n",
      "Iter-4780 train loss: 1.1368 valid loss: 1.0209, valid accuracy: 0.7048\n",
      "Iter-4790 train loss: 1.0502 valid loss: 1.0193, valid accuracy: 0.7048\n",
      "Iter-4800 train loss: 0.9800 valid loss: 1.0179, valid accuracy: 0.7056\n",
      "Iter-4810 train loss: 1.0382 valid loss: 1.0163, valid accuracy: 0.7058\n",
      "Iter-4820 train loss: 1.0006 valid loss: 1.0149, valid accuracy: 0.7068\n",
      "Iter-4830 train loss: 1.0676 valid loss: 1.0134, valid accuracy: 0.7072\n",
      "Iter-4840 train loss: 1.0972 valid loss: 1.0120, valid accuracy: 0.7074\n",
      "Iter-4850 train loss: 1.0835 valid loss: 1.0104, valid accuracy: 0.7082\n",
      "Iter-4860 train loss: 0.8733 valid loss: 1.0090, valid accuracy: 0.7084\n",
      "Iter-4870 train loss: 1.1306 valid loss: 1.0076, valid accuracy: 0.7086\n",
      "Iter-4880 train loss: 1.1687 valid loss: 1.0060, valid accuracy: 0.7094\n",
      "Iter-4890 train loss: 0.9693 valid loss: 1.0045, valid accuracy: 0.7100\n",
      "Iter-4900 train loss: 1.0130 valid loss: 1.0031, valid accuracy: 0.7104\n",
      "Iter-4910 train loss: 0.9258 valid loss: 1.0016, valid accuracy: 0.7108\n",
      "Iter-4920 train loss: 1.0538 valid loss: 1.0001, valid accuracy: 0.7108\n",
      "Iter-4930 train loss: 1.0101 valid loss: 0.9988, valid accuracy: 0.7110\n",
      "Iter-4940 train loss: 1.0729 valid loss: 0.9973, valid accuracy: 0.7128\n",
      "Iter-4950 train loss: 1.0372 valid loss: 0.9960, valid accuracy: 0.7138\n",
      "Iter-4960 train loss: 1.0002 valid loss: 0.9947, valid accuracy: 0.7136\n",
      "Iter-4970 train loss: 1.3198 valid loss: 0.9933, valid accuracy: 0.7146\n",
      "Iter-4980 train loss: 1.1155 valid loss: 0.9919, valid accuracy: 0.7156\n",
      "Iter-4990 train loss: 1.0122 valid loss: 0.9904, valid accuracy: 0.7162\n",
      "Iter-5000 train loss: 0.8417 valid loss: 0.9889, valid accuracy: 0.7166\n",
      "Iter-5010 train loss: 1.0951 valid loss: 0.9876, valid accuracy: 0.7164\n",
      "Iter-5020 train loss: 0.9896 valid loss: 0.9862, valid accuracy: 0.7168\n",
      "Iter-5030 train loss: 0.9141 valid loss: 0.9848, valid accuracy: 0.7178\n",
      "Iter-5040 train loss: 1.0379 valid loss: 0.9834, valid accuracy: 0.7182\n",
      "Iter-5050 train loss: 0.9992 valid loss: 0.9821, valid accuracy: 0.7186\n",
      "Iter-5060 train loss: 1.1474 valid loss: 0.9808, valid accuracy: 0.7190\n",
      "Iter-5070 train loss: 1.1070 valid loss: 0.9795, valid accuracy: 0.7190\n",
      "Iter-5080 train loss: 1.1173 valid loss: 0.9782, valid accuracy: 0.7194\n",
      "Iter-5090 train loss: 1.1914 valid loss: 0.9768, valid accuracy: 0.7208\n",
      "Iter-5100 train loss: 1.1163 valid loss: 0.9754, valid accuracy: 0.7220\n",
      "Iter-5110 train loss: 1.0482 valid loss: 0.9740, valid accuracy: 0.7228\n",
      "Iter-5120 train loss: 0.8426 valid loss: 0.9727, valid accuracy: 0.7238\n",
      "Iter-5130 train loss: 1.0283 valid loss: 0.9713, valid accuracy: 0.7240\n",
      "Iter-5140 train loss: 0.9776 valid loss: 0.9701, valid accuracy: 0.7242\n",
      "Iter-5150 train loss: 0.9998 valid loss: 0.9687, valid accuracy: 0.7244\n",
      "Iter-5160 train loss: 1.0647 valid loss: 0.9675, valid accuracy: 0.7258\n",
      "Iter-5170 train loss: 0.8837 valid loss: 0.9661, valid accuracy: 0.7264\n",
      "Iter-5180 train loss: 0.8439 valid loss: 0.9648, valid accuracy: 0.7268\n",
      "Iter-5190 train loss: 0.9520 valid loss: 0.9635, valid accuracy: 0.7270\n",
      "Iter-5200 train loss: 0.9154 valid loss: 0.9623, valid accuracy: 0.7272\n",
      "Iter-5210 train loss: 0.9820 valid loss: 0.9610, valid accuracy: 0.7282\n",
      "Iter-5220 train loss: 0.9828 valid loss: 0.9597, valid accuracy: 0.7290\n",
      "Iter-5230 train loss: 0.7953 valid loss: 0.9584, valid accuracy: 0.7296\n",
      "Iter-5240 train loss: 0.9599 valid loss: 0.9571, valid accuracy: 0.7300\n",
      "Iter-5250 train loss: 0.9369 valid loss: 0.9557, valid accuracy: 0.7302\n",
      "Iter-5260 train loss: 0.8974 valid loss: 0.9545, valid accuracy: 0.7306\n",
      "Iter-5270 train loss: 0.8627 valid loss: 0.9532, valid accuracy: 0.7306\n",
      "Iter-5280 train loss: 1.0122 valid loss: 0.9520, valid accuracy: 0.7316\n",
      "Iter-5290 train loss: 0.8418 valid loss: 0.9506, valid accuracy: 0.7320\n",
      "Iter-5300 train loss: 0.9235 valid loss: 0.9494, valid accuracy: 0.7324\n",
      "Iter-5310 train loss: 0.8598 valid loss: 0.9481, valid accuracy: 0.7330\n",
      "Iter-5320 train loss: 0.9812 valid loss: 0.9468, valid accuracy: 0.7344\n",
      "Iter-5330 train loss: 1.0517 valid loss: 0.9456, valid accuracy: 0.7352\n",
      "Iter-5340 train loss: 0.9414 valid loss: 0.9444, valid accuracy: 0.7352\n",
      "Iter-5350 train loss: 0.8402 valid loss: 0.9432, valid accuracy: 0.7358\n",
      "Iter-5360 train loss: 0.8209 valid loss: 0.9419, valid accuracy: 0.7362\n",
      "Iter-5370 train loss: 1.0815 valid loss: 0.9406, valid accuracy: 0.7368\n",
      "Iter-5380 train loss: 0.9560 valid loss: 0.9393, valid accuracy: 0.7368\n",
      "Iter-5390 train loss: 0.8858 valid loss: 0.9381, valid accuracy: 0.7374\n",
      "Iter-5400 train loss: 1.0874 valid loss: 0.9369, valid accuracy: 0.7378\n",
      "Iter-5410 train loss: 0.9873 valid loss: 0.9356, valid accuracy: 0.7384\n",
      "Iter-5420 train loss: 0.9427 valid loss: 0.9343, valid accuracy: 0.7390\n",
      "Iter-5430 train loss: 1.1092 valid loss: 0.9331, valid accuracy: 0.7392\n",
      "Iter-5440 train loss: 1.0547 valid loss: 0.9318, valid accuracy: 0.7396\n",
      "Iter-5450 train loss: 0.9298 valid loss: 0.9306, valid accuracy: 0.7412\n",
      "Iter-5460 train loss: 0.9539 valid loss: 0.9293, valid accuracy: 0.7418\n",
      "Iter-5470 train loss: 1.0406 valid loss: 0.9282, valid accuracy: 0.7432\n",
      "Iter-5480 train loss: 0.9884 valid loss: 0.9269, valid accuracy: 0.7434\n",
      "Iter-5490 train loss: 0.9195 valid loss: 0.9257, valid accuracy: 0.7442\n",
      "Iter-5500 train loss: 0.7942 valid loss: 0.9245, valid accuracy: 0.7440\n",
      "Iter-5510 train loss: 0.9331 valid loss: 0.9234, valid accuracy: 0.7442\n",
      "Iter-5520 train loss: 0.9273 valid loss: 0.9222, valid accuracy: 0.7440\n",
      "Iter-5530 train loss: 1.0284 valid loss: 0.9209, valid accuracy: 0.7446\n",
      "Iter-5540 train loss: 1.0925 valid loss: 0.9198, valid accuracy: 0.7454\n",
      "Iter-5550 train loss: 0.9281 valid loss: 0.9185, valid accuracy: 0.7470\n",
      "Iter-5560 train loss: 1.0669 valid loss: 0.9174, valid accuracy: 0.7474\n",
      "Iter-5570 train loss: 0.9462 valid loss: 0.9162, valid accuracy: 0.7476\n",
      "Iter-5580 train loss: 1.0322 valid loss: 0.9150, valid accuracy: 0.7484\n",
      "Iter-5590 train loss: 1.0675 valid loss: 0.9139, valid accuracy: 0.7480\n",
      "Iter-5600 train loss: 0.7348 valid loss: 0.9128, valid accuracy: 0.7482\n",
      "Iter-5610 train loss: 1.0015 valid loss: 0.9117, valid accuracy: 0.7486\n",
      "Iter-5620 train loss: 1.0244 valid loss: 0.9105, valid accuracy: 0.7490\n",
      "Iter-5630 train loss: 0.8821 valid loss: 0.9094, valid accuracy: 0.7498\n",
      "Iter-5640 train loss: 0.8144 valid loss: 0.9083, valid accuracy: 0.7496\n",
      "Iter-5650 train loss: 0.8529 valid loss: 0.9071, valid accuracy: 0.7502\n",
      "Iter-5660 train loss: 0.9959 valid loss: 0.9059, valid accuracy: 0.7504\n",
      "Iter-5670 train loss: 1.0272 valid loss: 0.9047, valid accuracy: 0.7506\n",
      "Iter-5680 train loss: 0.8517 valid loss: 0.9036, valid accuracy: 0.7512\n",
      "Iter-5690 train loss: 0.9484 valid loss: 0.9025, valid accuracy: 0.7512\n",
      "Iter-5700 train loss: 0.8950 valid loss: 0.9014, valid accuracy: 0.7512\n",
      "Iter-5710 train loss: 0.9559 valid loss: 0.9004, valid accuracy: 0.7520\n",
      "Iter-5720 train loss: 0.9927 valid loss: 0.8992, valid accuracy: 0.7522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 0.8255 valid loss: 0.8980, valid accuracy: 0.7530\n",
      "Iter-5740 train loss: 0.8866 valid loss: 0.8968, valid accuracy: 0.7536\n",
      "Iter-5750 train loss: 0.8419 valid loss: 0.8958, valid accuracy: 0.7542\n",
      "Iter-5760 train loss: 0.8467 valid loss: 0.8946, valid accuracy: 0.7542\n",
      "Iter-5770 train loss: 1.0066 valid loss: 0.8935, valid accuracy: 0.7544\n",
      "Iter-5780 train loss: 0.9241 valid loss: 0.8925, valid accuracy: 0.7548\n",
      "Iter-5790 train loss: 1.0234 valid loss: 0.8914, valid accuracy: 0.7546\n",
      "Iter-5800 train loss: 0.8632 valid loss: 0.8903, valid accuracy: 0.7562\n",
      "Iter-5810 train loss: 0.8253 valid loss: 0.8892, valid accuracy: 0.7560\n",
      "Iter-5820 train loss: 0.9944 valid loss: 0.8881, valid accuracy: 0.7570\n",
      "Iter-5830 train loss: 1.0219 valid loss: 0.8871, valid accuracy: 0.7570\n",
      "Iter-5840 train loss: 0.9421 valid loss: 0.8860, valid accuracy: 0.7576\n",
      "Iter-5850 train loss: 1.1752 valid loss: 0.8849, valid accuracy: 0.7578\n",
      "Iter-5860 train loss: 1.0283 valid loss: 0.8839, valid accuracy: 0.7582\n",
      "Iter-5870 train loss: 0.7954 valid loss: 0.8828, valid accuracy: 0.7582\n",
      "Iter-5880 train loss: 0.8101 valid loss: 0.8817, valid accuracy: 0.7586\n",
      "Iter-5890 train loss: 0.9438 valid loss: 0.8806, valid accuracy: 0.7590\n",
      "Iter-5900 train loss: 0.9904 valid loss: 0.8796, valid accuracy: 0.7592\n",
      "Iter-5910 train loss: 1.1689 valid loss: 0.8786, valid accuracy: 0.7598\n",
      "Iter-5920 train loss: 0.9186 valid loss: 0.8775, valid accuracy: 0.7600\n",
      "Iter-5930 train loss: 0.8682 valid loss: 0.8765, valid accuracy: 0.7606\n",
      "Iter-5940 train loss: 0.8518 valid loss: 0.8754, valid accuracy: 0.7610\n",
      "Iter-5950 train loss: 0.8028 valid loss: 0.8743, valid accuracy: 0.7614\n",
      "Iter-5960 train loss: 0.7532 valid loss: 0.8732, valid accuracy: 0.7616\n",
      "Iter-5970 train loss: 0.8511 valid loss: 0.8722, valid accuracy: 0.7622\n",
      "Iter-5980 train loss: 0.7700 valid loss: 0.8713, valid accuracy: 0.7622\n",
      "Iter-5990 train loss: 0.9095 valid loss: 0.8702, valid accuracy: 0.7634\n",
      "Iter-6000 train loss: 0.8671 valid loss: 0.8691, valid accuracy: 0.7638\n",
      "Iter-6010 train loss: 0.6511 valid loss: 0.8680, valid accuracy: 0.7640\n",
      "Iter-6020 train loss: 0.8837 valid loss: 0.8669, valid accuracy: 0.7642\n",
      "Iter-6030 train loss: 0.8764 valid loss: 0.8659, valid accuracy: 0.7646\n",
      "Iter-6040 train loss: 0.6717 valid loss: 0.8649, valid accuracy: 0.7660\n",
      "Iter-6050 train loss: 0.9165 valid loss: 0.8639, valid accuracy: 0.7664\n",
      "Iter-6060 train loss: 0.9069 valid loss: 0.8629, valid accuracy: 0.7670\n",
      "Iter-6070 train loss: 0.8110 valid loss: 0.8619, valid accuracy: 0.7670\n",
      "Iter-6080 train loss: 0.8250 valid loss: 0.8609, valid accuracy: 0.7670\n",
      "Iter-6090 train loss: 0.8663 valid loss: 0.8598, valid accuracy: 0.7678\n",
      "Iter-6100 train loss: 0.8116 valid loss: 0.8587, valid accuracy: 0.7684\n",
      "Iter-6110 train loss: 0.9072 valid loss: 0.8578, valid accuracy: 0.7686\n",
      "Iter-6120 train loss: 0.9414 valid loss: 0.8568, valid accuracy: 0.7690\n",
      "Iter-6130 train loss: 1.1093 valid loss: 0.8558, valid accuracy: 0.7690\n",
      "Iter-6140 train loss: 0.8172 valid loss: 0.8548, valid accuracy: 0.7694\n",
      "Iter-6150 train loss: 0.9080 valid loss: 0.8538, valid accuracy: 0.7696\n",
      "Iter-6160 train loss: 0.8501 valid loss: 0.8528, valid accuracy: 0.7696\n",
      "Iter-6170 train loss: 0.8633 valid loss: 0.8518, valid accuracy: 0.7702\n",
      "Iter-6180 train loss: 0.8135 valid loss: 0.8508, valid accuracy: 0.7706\n",
      "Iter-6190 train loss: 0.7539 valid loss: 0.8498, valid accuracy: 0.7714\n",
      "Iter-6200 train loss: 0.8488 valid loss: 0.8488, valid accuracy: 0.7718\n",
      "Iter-6210 train loss: 0.8451 valid loss: 0.8478, valid accuracy: 0.7718\n",
      "Iter-6220 train loss: 0.7474 valid loss: 0.8468, valid accuracy: 0.7720\n",
      "Iter-6230 train loss: 1.0884 valid loss: 0.8458, valid accuracy: 0.7720\n",
      "Iter-6240 train loss: 0.8199 valid loss: 0.8448, valid accuracy: 0.7716\n",
      "Iter-6250 train loss: 1.0091 valid loss: 0.8438, valid accuracy: 0.7724\n",
      "Iter-6260 train loss: 0.8657 valid loss: 0.8428, valid accuracy: 0.7724\n",
      "Iter-6270 train loss: 0.7956 valid loss: 0.8418, valid accuracy: 0.7732\n",
      "Iter-6280 train loss: 0.8071 valid loss: 0.8408, valid accuracy: 0.7734\n",
      "Iter-6290 train loss: 0.8717 valid loss: 0.8398, valid accuracy: 0.7748\n",
      "Iter-6300 train loss: 0.8818 valid loss: 0.8388, valid accuracy: 0.7746\n",
      "Iter-6310 train loss: 0.8343 valid loss: 0.8379, valid accuracy: 0.7744\n",
      "Iter-6320 train loss: 0.8217 valid loss: 0.8369, valid accuracy: 0.7752\n",
      "Iter-6330 train loss: 0.8225 valid loss: 0.8360, valid accuracy: 0.7752\n",
      "Iter-6340 train loss: 0.8106 valid loss: 0.8351, valid accuracy: 0.7758\n",
      "Iter-6350 train loss: 0.8352 valid loss: 0.8342, valid accuracy: 0.7758\n",
      "Iter-6360 train loss: 0.8656 valid loss: 0.8333, valid accuracy: 0.7764\n",
      "Iter-6370 train loss: 0.8509 valid loss: 0.8324, valid accuracy: 0.7762\n",
      "Iter-6380 train loss: 0.9149 valid loss: 0.8314, valid accuracy: 0.7764\n",
      "Iter-6390 train loss: 1.0172 valid loss: 0.8304, valid accuracy: 0.7768\n",
      "Iter-6400 train loss: 0.8058 valid loss: 0.8295, valid accuracy: 0.7768\n",
      "Iter-6410 train loss: 0.8065 valid loss: 0.8286, valid accuracy: 0.7774\n",
      "Iter-6420 train loss: 0.8393 valid loss: 0.8277, valid accuracy: 0.7772\n",
      "Iter-6430 train loss: 0.8878 valid loss: 0.8267, valid accuracy: 0.7780\n",
      "Iter-6440 train loss: 0.7987 valid loss: 0.8258, valid accuracy: 0.7784\n",
      "Iter-6450 train loss: 0.8184 valid loss: 0.8249, valid accuracy: 0.7790\n",
      "Iter-6460 train loss: 0.8333 valid loss: 0.8240, valid accuracy: 0.7790\n",
      "Iter-6470 train loss: 0.9755 valid loss: 0.8230, valid accuracy: 0.7796\n",
      "Iter-6480 train loss: 0.8177 valid loss: 0.8221, valid accuracy: 0.7796\n",
      "Iter-6490 train loss: 0.7007 valid loss: 0.8212, valid accuracy: 0.7794\n",
      "Iter-6500 train loss: 0.8067 valid loss: 0.8203, valid accuracy: 0.7794\n",
      "Iter-6510 train loss: 0.8529 valid loss: 0.8194, valid accuracy: 0.7798\n",
      "Iter-6520 train loss: 0.8527 valid loss: 0.8185, valid accuracy: 0.7798\n",
      "Iter-6530 train loss: 0.8240 valid loss: 0.8176, valid accuracy: 0.7796\n",
      "Iter-6540 train loss: 1.0594 valid loss: 0.8167, valid accuracy: 0.7806\n",
      "Iter-6550 train loss: 0.8913 valid loss: 0.8159, valid accuracy: 0.7808\n",
      "Iter-6560 train loss: 0.8802 valid loss: 0.8150, valid accuracy: 0.7810\n",
      "Iter-6570 train loss: 0.9412 valid loss: 0.8140, valid accuracy: 0.7810\n",
      "Iter-6580 train loss: 0.7118 valid loss: 0.8131, valid accuracy: 0.7810\n",
      "Iter-6590 train loss: 0.7709 valid loss: 0.8121, valid accuracy: 0.7816\n",
      "Iter-6600 train loss: 0.7410 valid loss: 0.8113, valid accuracy: 0.7816\n",
      "Iter-6610 train loss: 0.9065 valid loss: 0.8104, valid accuracy: 0.7818\n",
      "Iter-6620 train loss: 0.7764 valid loss: 0.8096, valid accuracy: 0.7820\n",
      "Iter-6630 train loss: 0.8624 valid loss: 0.8087, valid accuracy: 0.7824\n",
      "Iter-6640 train loss: 1.1156 valid loss: 0.8078, valid accuracy: 0.7826\n",
      "Iter-6650 train loss: 0.8075 valid loss: 0.8069, valid accuracy: 0.7828\n",
      "Iter-6660 train loss: 0.8016 valid loss: 0.8060, valid accuracy: 0.7830\n",
      "Iter-6670 train loss: 0.7745 valid loss: 0.8052, valid accuracy: 0.7834\n",
      "Iter-6680 train loss: 0.7511 valid loss: 0.8043, valid accuracy: 0.7836\n",
      "Iter-6690 train loss: 0.5476 valid loss: 0.8034, valid accuracy: 0.7844\n",
      "Iter-6700 train loss: 0.7523 valid loss: 0.8025, valid accuracy: 0.7842\n",
      "Iter-6710 train loss: 0.7593 valid loss: 0.8017, valid accuracy: 0.7848\n",
      "Iter-6720 train loss: 0.7202 valid loss: 0.8008, valid accuracy: 0.7844\n",
      "Iter-6730 train loss: 0.5760 valid loss: 0.8001, valid accuracy: 0.7852\n",
      "Iter-6740 train loss: 0.8249 valid loss: 0.7992, valid accuracy: 0.7848\n",
      "Iter-6750 train loss: 0.6215 valid loss: 0.7983, valid accuracy: 0.7854\n",
      "Iter-6760 train loss: 0.7084 valid loss: 0.7975, valid accuracy: 0.7856\n",
      "Iter-6770 train loss: 0.7039 valid loss: 0.7966, valid accuracy: 0.7860\n",
      "Iter-6780 train loss: 0.7619 valid loss: 0.7957, valid accuracy: 0.7866\n",
      "Iter-6790 train loss: 0.6668 valid loss: 0.7949, valid accuracy: 0.7868\n",
      "Iter-6800 train loss: 0.6357 valid loss: 0.7940, valid accuracy: 0.7868\n",
      "Iter-6810 train loss: 0.7152 valid loss: 0.7931, valid accuracy: 0.7874\n",
      "Iter-6820 train loss: 0.7107 valid loss: 0.7923, valid accuracy: 0.7876\n",
      "Iter-6830 train loss: 0.8420 valid loss: 0.7914, valid accuracy: 0.7880\n",
      "Iter-6840 train loss: 0.8920 valid loss: 0.7906, valid accuracy: 0.7880\n",
      "Iter-6850 train loss: 0.7975 valid loss: 0.7897, valid accuracy: 0.7888\n",
      "Iter-6860 train loss: 0.8725 valid loss: 0.7889, valid accuracy: 0.7884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 0.8189 valid loss: 0.7881, valid accuracy: 0.7892\n",
      "Iter-6880 train loss: 0.8745 valid loss: 0.7873, valid accuracy: 0.7896\n",
      "Iter-6890 train loss: 0.9308 valid loss: 0.7865, valid accuracy: 0.7900\n",
      "Iter-6900 train loss: 0.7469 valid loss: 0.7856, valid accuracy: 0.7900\n",
      "Iter-6910 train loss: 0.7531 valid loss: 0.7848, valid accuracy: 0.7910\n",
      "Iter-6920 train loss: 0.6530 valid loss: 0.7840, valid accuracy: 0.7916\n",
      "Iter-6930 train loss: 0.7159 valid loss: 0.7831, valid accuracy: 0.7920\n",
      "Iter-6940 train loss: 0.7393 valid loss: 0.7824, valid accuracy: 0.7918\n",
      "Iter-6950 train loss: 0.8705 valid loss: 0.7815, valid accuracy: 0.7918\n",
      "Iter-6960 train loss: 0.7897 valid loss: 0.7807, valid accuracy: 0.7920\n",
      "Iter-6970 train loss: 0.8125 valid loss: 0.7799, valid accuracy: 0.7930\n",
      "Iter-6980 train loss: 0.8035 valid loss: 0.7791, valid accuracy: 0.7922\n",
      "Iter-6990 train loss: 0.8507 valid loss: 0.7783, valid accuracy: 0.7928\n",
      "Iter-7000 train loss: 1.0587 valid loss: 0.7776, valid accuracy: 0.7932\n",
      "Iter-7010 train loss: 0.9259 valid loss: 0.7767, valid accuracy: 0.7936\n",
      "Iter-7020 train loss: 0.7159 valid loss: 0.7759, valid accuracy: 0.7942\n",
      "Iter-7030 train loss: 0.8076 valid loss: 0.7751, valid accuracy: 0.7942\n",
      "Iter-7040 train loss: 0.9016 valid loss: 0.7743, valid accuracy: 0.7942\n",
      "Iter-7050 train loss: 0.7847 valid loss: 0.7734, valid accuracy: 0.7948\n",
      "Iter-7060 train loss: 0.6736 valid loss: 0.7726, valid accuracy: 0.7950\n",
      "Iter-7070 train loss: 0.7106 valid loss: 0.7718, valid accuracy: 0.7954\n",
      "Iter-7080 train loss: 0.8291 valid loss: 0.7710, valid accuracy: 0.7958\n",
      "Iter-7090 train loss: 0.7617 valid loss: 0.7702, valid accuracy: 0.7962\n",
      "Iter-7100 train loss: 0.7948 valid loss: 0.7694, valid accuracy: 0.7962\n",
      "Iter-7110 train loss: 0.8156 valid loss: 0.7686, valid accuracy: 0.7960\n",
      "Iter-7120 train loss: 0.8440 valid loss: 0.7678, valid accuracy: 0.7972\n",
      "Iter-7130 train loss: 0.6936 valid loss: 0.7671, valid accuracy: 0.7970\n",
      "Iter-7140 train loss: 0.7584 valid loss: 0.7662, valid accuracy: 0.7972\n",
      "Iter-7150 train loss: 0.6954 valid loss: 0.7654, valid accuracy: 0.7978\n",
      "Iter-7160 train loss: 0.7512 valid loss: 0.7647, valid accuracy: 0.7982\n",
      "Iter-7170 train loss: 0.7631 valid loss: 0.7639, valid accuracy: 0.7980\n",
      "Iter-7180 train loss: 0.8624 valid loss: 0.7631, valid accuracy: 0.7982\n",
      "Iter-7190 train loss: 0.7492 valid loss: 0.7623, valid accuracy: 0.7988\n",
      "Iter-7200 train loss: 0.7244 valid loss: 0.7615, valid accuracy: 0.7990\n",
      "Iter-7210 train loss: 0.7830 valid loss: 0.7607, valid accuracy: 0.7998\n",
      "Iter-7220 train loss: 0.8182 valid loss: 0.7599, valid accuracy: 0.7998\n",
      "Iter-7230 train loss: 0.7385 valid loss: 0.7592, valid accuracy: 0.8000\n",
      "Iter-7240 train loss: 0.7134 valid loss: 0.7584, valid accuracy: 0.7994\n",
      "Iter-7250 train loss: 0.9687 valid loss: 0.7577, valid accuracy: 0.7998\n",
      "Iter-7260 train loss: 0.5923 valid loss: 0.7569, valid accuracy: 0.7996\n",
      "Iter-7270 train loss: 0.7510 valid loss: 0.7562, valid accuracy: 0.7998\n",
      "Iter-7280 train loss: 0.9090 valid loss: 0.7555, valid accuracy: 0.8002\n",
      "Iter-7290 train loss: 0.7249 valid loss: 0.7547, valid accuracy: 0.8004\n",
      "Iter-7300 train loss: 0.9225 valid loss: 0.7541, valid accuracy: 0.8010\n",
      "Iter-7310 train loss: 0.7442 valid loss: 0.7533, valid accuracy: 0.8012\n",
      "Iter-7320 train loss: 0.7461 valid loss: 0.7527, valid accuracy: 0.8012\n",
      "Iter-7330 train loss: 0.8044 valid loss: 0.7520, valid accuracy: 0.8012\n",
      "Iter-7340 train loss: 0.8011 valid loss: 0.7513, valid accuracy: 0.8014\n",
      "Iter-7350 train loss: 0.8888 valid loss: 0.7505, valid accuracy: 0.8018\n",
      "Iter-7360 train loss: 0.6707 valid loss: 0.7498, valid accuracy: 0.8020\n",
      "Iter-7370 train loss: 0.7897 valid loss: 0.7491, valid accuracy: 0.8026\n",
      "Iter-7380 train loss: 0.8625 valid loss: 0.7483, valid accuracy: 0.8032\n",
      "Iter-7390 train loss: 0.9676 valid loss: 0.7476, valid accuracy: 0.8032\n",
      "Iter-7400 train loss: 0.7090 valid loss: 0.7468, valid accuracy: 0.8034\n",
      "Iter-7410 train loss: 0.7032 valid loss: 0.7461, valid accuracy: 0.8036\n",
      "Iter-7420 train loss: 0.6146 valid loss: 0.7454, valid accuracy: 0.8034\n",
      "Iter-7430 train loss: 0.8028 valid loss: 0.7447, valid accuracy: 0.8040\n",
      "Iter-7440 train loss: 0.6646 valid loss: 0.7440, valid accuracy: 0.8040\n",
      "Iter-7450 train loss: 0.8192 valid loss: 0.7433, valid accuracy: 0.8044\n",
      "Iter-7460 train loss: 0.9632 valid loss: 0.7427, valid accuracy: 0.8048\n",
      "Iter-7470 train loss: 0.8131 valid loss: 0.7420, valid accuracy: 0.8050\n",
      "Iter-7480 train loss: 0.6771 valid loss: 0.7413, valid accuracy: 0.8050\n",
      "Iter-7490 train loss: 0.8043 valid loss: 0.7407, valid accuracy: 0.8052\n",
      "Iter-7500 train loss: 0.9603 valid loss: 0.7400, valid accuracy: 0.8054\n",
      "Iter-7510 train loss: 0.7768 valid loss: 0.7393, valid accuracy: 0.8062\n",
      "Iter-7520 train loss: 0.8288 valid loss: 0.7386, valid accuracy: 0.8060\n",
      "Iter-7530 train loss: 0.7640 valid loss: 0.7379, valid accuracy: 0.8060\n",
      "Iter-7540 train loss: 0.6389 valid loss: 0.7372, valid accuracy: 0.8066\n",
      "Iter-7550 train loss: 0.7956 valid loss: 0.7365, valid accuracy: 0.8068\n",
      "Iter-7560 train loss: 0.7492 valid loss: 0.7358, valid accuracy: 0.8074\n",
      "Iter-7570 train loss: 0.6540 valid loss: 0.7351, valid accuracy: 0.8074\n",
      "Iter-7580 train loss: 0.6778 valid loss: 0.7344, valid accuracy: 0.8074\n",
      "Iter-7590 train loss: 0.8598 valid loss: 0.7338, valid accuracy: 0.8080\n",
      "Iter-7600 train loss: 0.7960 valid loss: 0.7331, valid accuracy: 0.8078\n",
      "Iter-7610 train loss: 0.7554 valid loss: 0.7324, valid accuracy: 0.8082\n",
      "Iter-7620 train loss: 0.7076 valid loss: 0.7318, valid accuracy: 0.8082\n",
      "Iter-7630 train loss: 0.6253 valid loss: 0.7310, valid accuracy: 0.8084\n",
      "Iter-7640 train loss: 0.9110 valid loss: 0.7305, valid accuracy: 0.8082\n",
      "Iter-7650 train loss: 0.6844 valid loss: 0.7298, valid accuracy: 0.8086\n",
      "Iter-7660 train loss: 0.6270 valid loss: 0.7291, valid accuracy: 0.8080\n",
      "Iter-7670 train loss: 0.8070 valid loss: 0.7285, valid accuracy: 0.8084\n",
      "Iter-7680 train loss: 0.7119 valid loss: 0.7277, valid accuracy: 0.8088\n",
      "Iter-7690 train loss: 0.8324 valid loss: 0.7270, valid accuracy: 0.8086\n",
      "Iter-7700 train loss: 0.8052 valid loss: 0.7264, valid accuracy: 0.8090\n",
      "Iter-7710 train loss: 0.6324 valid loss: 0.7257, valid accuracy: 0.8092\n",
      "Iter-7720 train loss: 0.6718 valid loss: 0.7250, valid accuracy: 0.8100\n",
      "Iter-7730 train loss: 0.6497 valid loss: 0.7243, valid accuracy: 0.8104\n",
      "Iter-7740 train loss: 0.7192 valid loss: 0.7237, valid accuracy: 0.8104\n",
      "Iter-7750 train loss: 0.7436 valid loss: 0.7230, valid accuracy: 0.8100\n",
      "Iter-7760 train loss: 0.6413 valid loss: 0.7223, valid accuracy: 0.8100\n",
      "Iter-7770 train loss: 0.7011 valid loss: 0.7216, valid accuracy: 0.8100\n",
      "Iter-7780 train loss: 0.6103 valid loss: 0.7209, valid accuracy: 0.8106\n",
      "Iter-7790 train loss: 0.7647 valid loss: 0.7202, valid accuracy: 0.8106\n",
      "Iter-7800 train loss: 0.7502 valid loss: 0.7196, valid accuracy: 0.8106\n",
      "Iter-7810 train loss: 0.7529 valid loss: 0.7190, valid accuracy: 0.8110\n",
      "Iter-7820 train loss: 0.7789 valid loss: 0.7183, valid accuracy: 0.8108\n",
      "Iter-7830 train loss: 0.7273 valid loss: 0.7177, valid accuracy: 0.8114\n",
      "Iter-7840 train loss: 0.6754 valid loss: 0.7170, valid accuracy: 0.8116\n",
      "Iter-7850 train loss: 0.7094 valid loss: 0.7163, valid accuracy: 0.8124\n",
      "Iter-7860 train loss: 0.7623 valid loss: 0.7157, valid accuracy: 0.8126\n",
      "Iter-7870 train loss: 0.5577 valid loss: 0.7151, valid accuracy: 0.8122\n",
      "Iter-7880 train loss: 0.8581 valid loss: 0.7144, valid accuracy: 0.8122\n",
      "Iter-7890 train loss: 0.5776 valid loss: 0.7138, valid accuracy: 0.8126\n",
      "Iter-7900 train loss: 0.8092 valid loss: 0.7132, valid accuracy: 0.8126\n",
      "Iter-7910 train loss: 0.6847 valid loss: 0.7126, valid accuracy: 0.8128\n",
      "Iter-7920 train loss: 0.7189 valid loss: 0.7119, valid accuracy: 0.8132\n",
      "Iter-7930 train loss: 0.7300 valid loss: 0.7113, valid accuracy: 0.8136\n",
      "Iter-7940 train loss: 0.8141 valid loss: 0.7107, valid accuracy: 0.8134\n",
      "Iter-7950 train loss: 0.8031 valid loss: 0.7101, valid accuracy: 0.8134\n",
      "Iter-7960 train loss: 0.7099 valid loss: 0.7094, valid accuracy: 0.8136\n",
      "Iter-7970 train loss: 0.7794 valid loss: 0.7088, valid accuracy: 0.8140\n",
      "Iter-7980 train loss: 0.6371 valid loss: 0.7082, valid accuracy: 0.8138\n",
      "Iter-7990 train loss: 0.7686 valid loss: 0.7076, valid accuracy: 0.8140\n",
      "Iter-8000 train loss: 0.7136 valid loss: 0.7070, valid accuracy: 0.8144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 0.5993 valid loss: 0.7064, valid accuracy: 0.8144\n",
      "Iter-8020 train loss: 0.8032 valid loss: 0.7057, valid accuracy: 0.8146\n",
      "Iter-8030 train loss: 0.5193 valid loss: 0.7051, valid accuracy: 0.8148\n",
      "Iter-8040 train loss: 0.6670 valid loss: 0.7045, valid accuracy: 0.8158\n",
      "Iter-8050 train loss: 0.6666 valid loss: 0.7038, valid accuracy: 0.8154\n",
      "Iter-8060 train loss: 0.6524 valid loss: 0.7032, valid accuracy: 0.8160\n",
      "Iter-8070 train loss: 0.6769 valid loss: 0.7026, valid accuracy: 0.8156\n",
      "Iter-8080 train loss: 0.7221 valid loss: 0.7020, valid accuracy: 0.8158\n",
      "Iter-8090 train loss: 0.7239 valid loss: 0.7014, valid accuracy: 0.8156\n",
      "Iter-8100 train loss: 0.7172 valid loss: 0.7008, valid accuracy: 0.8158\n",
      "Iter-8110 train loss: 0.8008 valid loss: 0.7002, valid accuracy: 0.8160\n",
      "Iter-8120 train loss: 0.7570 valid loss: 0.6996, valid accuracy: 0.8166\n",
      "Iter-8130 train loss: 0.6627 valid loss: 0.6990, valid accuracy: 0.8168\n",
      "Iter-8140 train loss: 0.7389 valid loss: 0.6984, valid accuracy: 0.8170\n",
      "Iter-8150 train loss: 0.6950 valid loss: 0.6977, valid accuracy: 0.8168\n",
      "Iter-8160 train loss: 0.6148 valid loss: 0.6971, valid accuracy: 0.8172\n",
      "Iter-8170 train loss: 0.5724 valid loss: 0.6965, valid accuracy: 0.8168\n",
      "Iter-8180 train loss: 0.6724 valid loss: 0.6959, valid accuracy: 0.8170\n",
      "Iter-8190 train loss: 0.7827 valid loss: 0.6953, valid accuracy: 0.8172\n",
      "Iter-8200 train loss: 0.6751 valid loss: 0.6947, valid accuracy: 0.8174\n",
      "Iter-8210 train loss: 0.6870 valid loss: 0.6942, valid accuracy: 0.8174\n",
      "Iter-8220 train loss: 0.6637 valid loss: 0.6935, valid accuracy: 0.8176\n",
      "Iter-8230 train loss: 0.6357 valid loss: 0.6929, valid accuracy: 0.8178\n",
      "Iter-8240 train loss: 0.6815 valid loss: 0.6924, valid accuracy: 0.8180\n",
      "Iter-8250 train loss: 0.9096 valid loss: 0.6918, valid accuracy: 0.8180\n",
      "Iter-8260 train loss: 0.8650 valid loss: 0.6912, valid accuracy: 0.8188\n",
      "Iter-8270 train loss: 0.7723 valid loss: 0.6906, valid accuracy: 0.8190\n",
      "Iter-8280 train loss: 0.6709 valid loss: 0.6901, valid accuracy: 0.8192\n",
      "Iter-8290 train loss: 0.8094 valid loss: 0.6895, valid accuracy: 0.8188\n",
      "Iter-8300 train loss: 0.6348 valid loss: 0.6890, valid accuracy: 0.8190\n",
      "Iter-8310 train loss: 0.8396 valid loss: 0.6885, valid accuracy: 0.8194\n",
      "Iter-8320 train loss: 0.7592 valid loss: 0.6879, valid accuracy: 0.8192\n",
      "Iter-8330 train loss: 0.8593 valid loss: 0.6873, valid accuracy: 0.8194\n",
      "Iter-8340 train loss: 0.6167 valid loss: 0.6867, valid accuracy: 0.8196\n",
      "Iter-8350 train loss: 0.8536 valid loss: 0.6862, valid accuracy: 0.8200\n",
      "Iter-8360 train loss: 0.7021 valid loss: 0.6856, valid accuracy: 0.8202\n",
      "Iter-8370 train loss: 0.6948 valid loss: 0.6851, valid accuracy: 0.8200\n",
      "Iter-8380 train loss: 0.7546 valid loss: 0.6845, valid accuracy: 0.8200\n",
      "Iter-8390 train loss: 0.7202 valid loss: 0.6839, valid accuracy: 0.8196\n",
      "Iter-8400 train loss: 0.7713 valid loss: 0.6834, valid accuracy: 0.8198\n",
      "Iter-8410 train loss: 0.7419 valid loss: 0.6828, valid accuracy: 0.8200\n",
      "Iter-8420 train loss: 0.6544 valid loss: 0.6823, valid accuracy: 0.8206\n",
      "Iter-8430 train loss: 0.5920 valid loss: 0.6817, valid accuracy: 0.8202\n",
      "Iter-8440 train loss: 0.7224 valid loss: 0.6812, valid accuracy: 0.8212\n",
      "Iter-8450 train loss: 0.7881 valid loss: 0.6806, valid accuracy: 0.8218\n",
      "Iter-8460 train loss: 0.6191 valid loss: 0.6801, valid accuracy: 0.8220\n",
      "Iter-8470 train loss: 0.6066 valid loss: 0.6795, valid accuracy: 0.8222\n",
      "Iter-8480 train loss: 0.6134 valid loss: 0.6789, valid accuracy: 0.8222\n",
      "Iter-8490 train loss: 0.7964 valid loss: 0.6784, valid accuracy: 0.8226\n",
      "Iter-8500 train loss: 0.7025 valid loss: 0.6778, valid accuracy: 0.8224\n",
      "Iter-8510 train loss: 0.6910 valid loss: 0.6773, valid accuracy: 0.8226\n",
      "Iter-8520 train loss: 0.6998 valid loss: 0.6767, valid accuracy: 0.8228\n",
      "Iter-8530 train loss: 0.7102 valid loss: 0.6762, valid accuracy: 0.8232\n",
      "Iter-8540 train loss: 0.7059 valid loss: 0.6757, valid accuracy: 0.8230\n",
      "Iter-8550 train loss: 0.5936 valid loss: 0.6752, valid accuracy: 0.8230\n",
      "Iter-8560 train loss: 0.6472 valid loss: 0.6747, valid accuracy: 0.8232\n",
      "Iter-8570 train loss: 0.8313 valid loss: 0.6741, valid accuracy: 0.8232\n",
      "Iter-8580 train loss: 0.6398 valid loss: 0.6736, valid accuracy: 0.8232\n",
      "Iter-8590 train loss: 0.7127 valid loss: 0.6731, valid accuracy: 0.8234\n",
      "Iter-8600 train loss: 0.5078 valid loss: 0.6726, valid accuracy: 0.8230\n",
      "Iter-8610 train loss: 0.7269 valid loss: 0.6720, valid accuracy: 0.8236\n",
      "Iter-8620 train loss: 0.6523 valid loss: 0.6715, valid accuracy: 0.8236\n",
      "Iter-8630 train loss: 0.6698 valid loss: 0.6710, valid accuracy: 0.8234\n",
      "Iter-8640 train loss: 0.6374 valid loss: 0.6704, valid accuracy: 0.8236\n",
      "Iter-8650 train loss: 0.6443 valid loss: 0.6700, valid accuracy: 0.8238\n",
      "Iter-8660 train loss: 0.5841 valid loss: 0.6695, valid accuracy: 0.8236\n",
      "Iter-8670 train loss: 0.6691 valid loss: 0.6690, valid accuracy: 0.8236\n",
      "Iter-8680 train loss: 0.7263 valid loss: 0.6685, valid accuracy: 0.8236\n",
      "Iter-8690 train loss: 0.4871 valid loss: 0.6680, valid accuracy: 0.8244\n",
      "Iter-8700 train loss: 0.6287 valid loss: 0.6675, valid accuracy: 0.8240\n",
      "Iter-8710 train loss: 0.8784 valid loss: 0.6669, valid accuracy: 0.8240\n",
      "Iter-8720 train loss: 0.7963 valid loss: 0.6664, valid accuracy: 0.8246\n",
      "Iter-8730 train loss: 0.7898 valid loss: 0.6659, valid accuracy: 0.8254\n",
      "Iter-8740 train loss: 0.7672 valid loss: 0.6653, valid accuracy: 0.8256\n",
      "Iter-8750 train loss: 0.6480 valid loss: 0.6648, valid accuracy: 0.8254\n",
      "Iter-8760 train loss: 0.7087 valid loss: 0.6643, valid accuracy: 0.8254\n",
      "Iter-8770 train loss: 0.7127 valid loss: 0.6638, valid accuracy: 0.8254\n",
      "Iter-8780 train loss: 0.8040 valid loss: 0.6633, valid accuracy: 0.8254\n",
      "Iter-8790 train loss: 0.5892 valid loss: 0.6628, valid accuracy: 0.8250\n",
      "Iter-8800 train loss: 0.5440 valid loss: 0.6623, valid accuracy: 0.8252\n",
      "Iter-8810 train loss: 0.7645 valid loss: 0.6618, valid accuracy: 0.8254\n",
      "Iter-8820 train loss: 0.6659 valid loss: 0.6612, valid accuracy: 0.8254\n",
      "Iter-8830 train loss: 0.7406 valid loss: 0.6606, valid accuracy: 0.8256\n",
      "Iter-8840 train loss: 0.7227 valid loss: 0.6601, valid accuracy: 0.8254\n",
      "Iter-8850 train loss: 0.6274 valid loss: 0.6596, valid accuracy: 0.8254\n",
      "Iter-8860 train loss: 0.5221 valid loss: 0.6591, valid accuracy: 0.8256\n",
      "Iter-8870 train loss: 0.7907 valid loss: 0.6586, valid accuracy: 0.8258\n",
      "Iter-8880 train loss: 0.8192 valid loss: 0.6581, valid accuracy: 0.8260\n",
      "Iter-8890 train loss: 0.8352 valid loss: 0.6576, valid accuracy: 0.8260\n",
      "Iter-8900 train loss: 0.7131 valid loss: 0.6571, valid accuracy: 0.8260\n",
      "Iter-8910 train loss: 0.7698 valid loss: 0.6566, valid accuracy: 0.8260\n",
      "Iter-8920 train loss: 0.5042 valid loss: 0.6562, valid accuracy: 0.8258\n",
      "Iter-8930 train loss: 0.6472 valid loss: 0.6557, valid accuracy: 0.8262\n",
      "Iter-8940 train loss: 0.7213 valid loss: 0.6551, valid accuracy: 0.8262\n",
      "Iter-8950 train loss: 0.6714 valid loss: 0.6547, valid accuracy: 0.8260\n",
      "Iter-8960 train loss: 0.6990 valid loss: 0.6542, valid accuracy: 0.8262\n",
      "Iter-8970 train loss: 0.7880 valid loss: 0.6537, valid accuracy: 0.8264\n",
      "Iter-8980 train loss: 0.7681 valid loss: 0.6532, valid accuracy: 0.8262\n",
      "Iter-8990 train loss: 0.6431 valid loss: 0.6527, valid accuracy: 0.8264\n",
      "Iter-9000 train loss: 0.6507 valid loss: 0.6522, valid accuracy: 0.8264\n",
      "Iter-9010 train loss: 0.7175 valid loss: 0.6517, valid accuracy: 0.8266\n",
      "Iter-9020 train loss: 0.5771 valid loss: 0.6512, valid accuracy: 0.8268\n",
      "Iter-9030 train loss: 0.6872 valid loss: 0.6507, valid accuracy: 0.8268\n",
      "Iter-9040 train loss: 0.7246 valid loss: 0.6502, valid accuracy: 0.8268\n",
      "Iter-9050 train loss: 0.5613 valid loss: 0.6498, valid accuracy: 0.8268\n",
      "Iter-9060 train loss: 0.6164 valid loss: 0.6493, valid accuracy: 0.8270\n",
      "Iter-9070 train loss: 0.6440 valid loss: 0.6488, valid accuracy: 0.8268\n",
      "Iter-9080 train loss: 0.7057 valid loss: 0.6483, valid accuracy: 0.8268\n",
      "Iter-9090 train loss: 0.5401 valid loss: 0.6479, valid accuracy: 0.8268\n",
      "Iter-9100 train loss: 0.6566 valid loss: 0.6474, valid accuracy: 0.8268\n",
      "Iter-9110 train loss: 0.6896 valid loss: 0.6469, valid accuracy: 0.8270\n",
      "Iter-9120 train loss: 0.9389 valid loss: 0.6465, valid accuracy: 0.8270\n",
      "Iter-9130 train loss: 0.6425 valid loss: 0.6460, valid accuracy: 0.8272\n",
      "Iter-9140 train loss: 0.5252 valid loss: 0.6455, valid accuracy: 0.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 0.7116 valid loss: 0.6450, valid accuracy: 0.8274\n",
      "Iter-9160 train loss: 0.6105 valid loss: 0.6446, valid accuracy: 0.8274\n",
      "Iter-9170 train loss: 0.6353 valid loss: 0.6441, valid accuracy: 0.8276\n",
      "Iter-9180 train loss: 0.5755 valid loss: 0.6436, valid accuracy: 0.8278\n",
      "Iter-9190 train loss: 0.7739 valid loss: 0.6431, valid accuracy: 0.8282\n",
      "Iter-9200 train loss: 0.6435 valid loss: 0.6426, valid accuracy: 0.8280\n",
      "Iter-9210 train loss: 0.6544 valid loss: 0.6421, valid accuracy: 0.8286\n",
      "Iter-9220 train loss: 0.7121 valid loss: 0.6417, valid accuracy: 0.8286\n",
      "Iter-9230 train loss: 0.5049 valid loss: 0.6412, valid accuracy: 0.8288\n",
      "Iter-9240 train loss: 0.8670 valid loss: 0.6408, valid accuracy: 0.8290\n",
      "Iter-9250 train loss: 0.7369 valid loss: 0.6403, valid accuracy: 0.8294\n",
      "Iter-9260 train loss: 0.7439 valid loss: 0.6398, valid accuracy: 0.8296\n",
      "Iter-9270 train loss: 0.6615 valid loss: 0.6394, valid accuracy: 0.8296\n",
      "Iter-9280 train loss: 0.5520 valid loss: 0.6390, valid accuracy: 0.8296\n",
      "Iter-9290 train loss: 0.7386 valid loss: 0.6385, valid accuracy: 0.8298\n",
      "Iter-9300 train loss: 0.6729 valid loss: 0.6381, valid accuracy: 0.8304\n",
      "Iter-9310 train loss: 0.6335 valid loss: 0.6377, valid accuracy: 0.8304\n",
      "Iter-9320 train loss: 0.6809 valid loss: 0.6373, valid accuracy: 0.8302\n",
      "Iter-9330 train loss: 0.7132 valid loss: 0.6368, valid accuracy: 0.8300\n",
      "Iter-9340 train loss: 0.7806 valid loss: 0.6363, valid accuracy: 0.8302\n",
      "Iter-9350 train loss: 0.5567 valid loss: 0.6358, valid accuracy: 0.8306\n",
      "Iter-9360 train loss: 0.5418 valid loss: 0.6354, valid accuracy: 0.8310\n",
      "Iter-9370 train loss: 0.6331 valid loss: 0.6350, valid accuracy: 0.8310\n",
      "Iter-9380 train loss: 0.6039 valid loss: 0.6345, valid accuracy: 0.8316\n",
      "Iter-9390 train loss: 0.7402 valid loss: 0.6341, valid accuracy: 0.8314\n",
      "Iter-9400 train loss: 0.6114 valid loss: 0.6337, valid accuracy: 0.8314\n",
      "Iter-9410 train loss: 0.5767 valid loss: 0.6332, valid accuracy: 0.8314\n",
      "Iter-9420 train loss: 0.7385 valid loss: 0.6328, valid accuracy: 0.8314\n",
      "Iter-9430 train loss: 0.7508 valid loss: 0.6323, valid accuracy: 0.8316\n",
      "Iter-9440 train loss: 0.5470 valid loss: 0.6319, valid accuracy: 0.8314\n",
      "Iter-9450 train loss: 0.6363 valid loss: 0.6314, valid accuracy: 0.8316\n",
      "Iter-9460 train loss: 0.5348 valid loss: 0.6310, valid accuracy: 0.8316\n",
      "Iter-9470 train loss: 0.6839 valid loss: 0.6305, valid accuracy: 0.8316\n",
      "Iter-9480 train loss: 0.5174 valid loss: 0.6301, valid accuracy: 0.8316\n",
      "Iter-9490 train loss: 0.7467 valid loss: 0.6296, valid accuracy: 0.8316\n",
      "Iter-9500 train loss: 0.4311 valid loss: 0.6292, valid accuracy: 0.8320\n",
      "Iter-9510 train loss: 0.6503 valid loss: 0.6288, valid accuracy: 0.8316\n",
      "Iter-9520 train loss: 0.5616 valid loss: 0.6284, valid accuracy: 0.8320\n",
      "Iter-9530 train loss: 0.5665 valid loss: 0.6280, valid accuracy: 0.8326\n",
      "Iter-9540 train loss: 0.8350 valid loss: 0.6275, valid accuracy: 0.8326\n",
      "Iter-9550 train loss: 0.5368 valid loss: 0.6270, valid accuracy: 0.8326\n",
      "Iter-9560 train loss: 0.5496 valid loss: 0.6266, valid accuracy: 0.8330\n",
      "Iter-9570 train loss: 0.5841 valid loss: 0.6262, valid accuracy: 0.8330\n",
      "Iter-9580 train loss: 0.6342 valid loss: 0.6258, valid accuracy: 0.8332\n",
      "Iter-9590 train loss: 0.7806 valid loss: 0.6254, valid accuracy: 0.8334\n",
      "Iter-9600 train loss: 0.6329 valid loss: 0.6250, valid accuracy: 0.8336\n",
      "Iter-9610 train loss: 0.7405 valid loss: 0.6245, valid accuracy: 0.8338\n",
      "Iter-9620 train loss: 0.5370 valid loss: 0.6242, valid accuracy: 0.8338\n",
      "Iter-9630 train loss: 0.7933 valid loss: 0.6238, valid accuracy: 0.8338\n",
      "Iter-9640 train loss: 0.4220 valid loss: 0.6234, valid accuracy: 0.8338\n",
      "Iter-9650 train loss: 0.6645 valid loss: 0.6230, valid accuracy: 0.8340\n",
      "Iter-9660 train loss: 0.6263 valid loss: 0.6226, valid accuracy: 0.8340\n",
      "Iter-9670 train loss: 0.6653 valid loss: 0.6222, valid accuracy: 0.8344\n",
      "Iter-9680 train loss: 0.7133 valid loss: 0.6218, valid accuracy: 0.8344\n",
      "Iter-9690 train loss: 0.7420 valid loss: 0.6214, valid accuracy: 0.8344\n",
      "Iter-9700 train loss: 0.5414 valid loss: 0.6210, valid accuracy: 0.8348\n",
      "Iter-9710 train loss: 0.5441 valid loss: 0.6206, valid accuracy: 0.8348\n",
      "Iter-9720 train loss: 0.8700 valid loss: 0.6201, valid accuracy: 0.8348\n",
      "Iter-9730 train loss: 0.5539 valid loss: 0.6197, valid accuracy: 0.8348\n",
      "Iter-9740 train loss: 0.5661 valid loss: 0.6193, valid accuracy: 0.8348\n",
      "Iter-9750 train loss: 0.5858 valid loss: 0.6189, valid accuracy: 0.8350\n",
      "Iter-9760 train loss: 0.6372 valid loss: 0.6185, valid accuracy: 0.8350\n",
      "Iter-9770 train loss: 0.5493 valid loss: 0.6181, valid accuracy: 0.8350\n",
      "Iter-9780 train loss: 0.6191 valid loss: 0.6177, valid accuracy: 0.8350\n",
      "Iter-9790 train loss: 0.5034 valid loss: 0.6173, valid accuracy: 0.8350\n",
      "Iter-9800 train loss: 0.5804 valid loss: 0.6169, valid accuracy: 0.8354\n",
      "Iter-9810 train loss: 0.5109 valid loss: 0.6165, valid accuracy: 0.8354\n",
      "Iter-9820 train loss: 0.6866 valid loss: 0.6161, valid accuracy: 0.8358\n",
      "Iter-9830 train loss: 0.6416 valid loss: 0.6157, valid accuracy: 0.8360\n",
      "Iter-9840 train loss: 0.5216 valid loss: 0.6153, valid accuracy: 0.8360\n",
      "Iter-9850 train loss: 0.7445 valid loss: 0.6149, valid accuracy: 0.8360\n",
      "Iter-9860 train loss: 0.5932 valid loss: 0.6145, valid accuracy: 0.8364\n",
      "Iter-9870 train loss: 0.5746 valid loss: 0.6141, valid accuracy: 0.8362\n",
      "Iter-9880 train loss: 0.6539 valid loss: 0.6138, valid accuracy: 0.8364\n",
      "Iter-9890 train loss: 0.8781 valid loss: 0.6134, valid accuracy: 0.8368\n",
      "Iter-9900 train loss: 0.5195 valid loss: 0.6130, valid accuracy: 0.8368\n",
      "Iter-9910 train loss: 0.6963 valid loss: 0.6126, valid accuracy: 0.8366\n",
      "Iter-9920 train loss: 0.4184 valid loss: 0.6122, valid accuracy: 0.8368\n",
      "Iter-9930 train loss: 0.6730 valid loss: 0.6118, valid accuracy: 0.8370\n",
      "Iter-9940 train loss: 0.7166 valid loss: 0.6114, valid accuracy: 0.8372\n",
      "Iter-9950 train loss: 0.7632 valid loss: 0.6111, valid accuracy: 0.8372\n",
      "Iter-9960 train loss: 0.6395 valid loss: 0.6107, valid accuracy: 0.8374\n",
      "Iter-9970 train loss: 0.6811 valid loss: 0.6103, valid accuracy: 0.8370\n",
      "Iter-9980 train loss: 0.6135 valid loss: 0.6099, valid accuracy: 0.8370\n",
      "Iter-9990 train loss: 0.8204 valid loss: 0.6095, valid accuracy: 0.8370\n",
      "Iter-10000 train loss: 0.5887 valid loss: 0.6091, valid accuracy: 0.8374\n",
      "Last iteration - Test accuracy mean: 0.8380, std: 0.0000, loss: 0.6126\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VFX6wPHvCQRCS0BASogJVUFABEUUlLCLUhRFepHi\n+lNsK6Iu2JEVXXBdCyssFgTpYAcBC1IUkKLSe0dASgihaGjJ+f1x5mZKpiYzk0nm/TzPPHPLuXfO\nXMh9556qtNYIIYSITjEFnQEhhBAFR4KAEEJEMQkCQggRxSQICCFEFJMgIIQQUUyCgBBCRDGfQUAp\nVUMptUgptVkptVEp9ZibNK2VUhlKqV9tr+dDk10hhBDBVNyPNJeAJ7TW65RSZYFflFLfaq23uaT7\nQWt9Z/CzKIQQIlR8PglorY9ordfZls8CW4FEN0lVkPMmhBAixAKqE1BKpQBNgFVudt+olFqnlJqn\nlGoQhLwJIYQIMX+KgwCwFQV9Agy2PRE4+gW4Qmv9p1KqA/AFUC942RRCCBEKyp+xg5RSxYGvgAVa\n67f9SL8XaKa1TnfZLgMVCSFEHmitQ1Lk7m9x0IfAFk8BQClVxWG5OSa4pLtLq7WWl9YMHz68wPMQ\nKS+5FnIt5Fp4f4WSz+IgpVRLoC+wUSm1FtDAs0Cyuafr94BuSqmHgItAJtAzdFkWQggRLD6DgNZ6\nOVDMR5qxwFh/P/Tnn2HECJg7198jhBBChILfFcPB8ssvMGQILFsW7k+OLKmpqQWdhYgh18JOroWd\nXIvw8KtiOGgfppQ2pUmG1nD6NHz1FXTrBiVKhC0rQghRaCil0CGqGC7QINC+PVx+OUyebNaTkmD/\nflDS7UyIkEtJSWH//v0FnQ3hIDk5mX379uXaXmSDgDvXXQdr1oQpQ0JEMduNpaCzIRx4+jeJqiAA\npphICBFaEgQiT0EEgfAPJd21DySu9pokI8MUCW3ZYtbPnoVNm8KQNyGEiDLhDwKHm0G3nvB/LaDh\nTFDZuZJUqGDely6FrCx4/nlo1CjM+RRCiChQMMVBKgvqzYObX4HYP2HJCNh6N74GIpUnVyGCJ1qK\ng7Kzs0lISGDr1q3UqFEjoGN3795N3bp1yc7O/WM1FKKjOAhAF4Ptd8IHK+H7f8EtI2FQM6g7D291\nBkrBrl3mPSsrfNkVQoRPuXLliI+PJz4+nmLFilG6dOmcbTNmzAj4fDExMZw5cybgAGBRRby5YmRU\nDKtsuOoLaPMiXCgLi16GPW1x92RQvrypM8jIgISE0OdZiKKqMDwJ1KpViwkTJtCmTRuPabKysihW\nzOugBnm2e/du6tWrR1aYfnVGz5OAKx0DW7vA/9bDysHQ8VEYmOq2Ajkjw7yXL++8PTMTLlywr58/\nL08LQhR27gZQe+GFF+jVqxd9+vQhISGBadOmsXLlSm688UYqVKhAYmIigwcPzrlxZ2VlERMTw4ED\nBwDo168fgwcPpmPHjsTHx9OyZUu/+0scOnSITp06UbFiRa688komTpyYs2/VqlU0a9aMhIQEqlWr\nxrBhwwDIzMykb9++VKpUiQoVKtCiRQvS092Or1kgwh4EZs/2slMXg029YdxmWD8Aet0FHR6DEmfc\nJu/eHf74wyyXLg2OPxYqVIBHHw1evoUQkeOLL77gnnvu4dSpU/Ts2ZPY2FjGjBlDeno6y5cv55tv\nvuHdd9/NSe9apDNjxgxeeeUVTp48SVJSEi+88IJfn9uzZ09q167NkSNHmDlzJkOHDuXHH38E4O9/\n/ztDhw7l1KlT7Nq1i27dugEwceJEMjMzOXz4MOnp6YwbN464uLggXYn8C3sQ6N7dvNeta94fyzVt\nPZBdHNb+DcZtgtg/4JGr4covcyX75BMoW9a0HgJYsQJq1ICPPjJPBuvXh+Y7CBENlArOKxRatWpF\nx44dAShZsiTNmjXj+uuvRylFSkoK999/P0uXLs1J7/o00a1bN6699lqKFStG3759Wbdunc/P3Lt3\nL2vWrGHUqFHExsZy7bXXcu+99zJlyhQASpQowc6dO0lPT6dMmTJcf/31AMTGxpKWlsaOHTtQStG0\naVNKly4drEuRbwVWHLRtG8TEmB7CHmVWhDkT4PPJcOtQ6NkFyh3KleyVV+zLhw7BDz+Y5Qgv7hQi\nomkdnFcoJCUlOa1v376dO+64g2rVqpGQkMDw4cNJS0vzeHzVqlVzlkuXLs3Zs66TJeb2+++/U6lS\nJadf8cnJyRw6ZO5JEydOZPPmzVx55ZW0aNGCBQsWADBw4EDatm1Ljx49SEpK4tlnnw1bayN/FEgQ\n0NoEgKws6NcP/vlP+3arAv/mmx0O2JcK49fDsYbwYBNoMsnr+WNi7OcTQhQ9rsU7gwYNolGjRuzZ\ns4dTp04xYsSIoFd6V69enbS0NDIzM3O2HThwgMTERADq1q3LjBkzOH78OE888QRdu3blwoULxMbG\n8uKLL7JlyxaWLVvGZ599xrRp04Kat/yIiIrhK6+0L//8s+kdvHQpPPigQ6JLcbD4n/DRYmg5Gjo9\nAMXPuT2f9W8vQUCI6HDmzBkSEhIoVaoUW7dudaoPyC8rmKSkpHDdddfx7LPPcuHCBdatW8fEiRPp\n168fAFOnTuXEiRMAxMfHExMTQ0xMDIsXL2bz5s1orSlbtiyxsbHExETErReIkCBw113w+edmuUoV\nuPpqU5b4v//B8uUuiY81hPdXQ9xJ+FsrKL8v1/kmTDDvEgSEKNz8baP/n//8h0mTJhEfH89DDz1E\nr169PJ4n0Hb/julnzZrFjh07qFq1Kj169GDUqFHcbCu2mD9/PvXr1ychIYGhQ4cye/ZsihcvzuHD\nh+nSpQsJCQk0atSI2267jT59+gSUh1AKez+BQD9Pa5gzBzp3zrUHWrwFrUbDF5NgV3u3xx87BpUq\nyfDUQrgqDP0Eok309hPwQinzpGB1FOzSJWcPrBwCsz+GO++D1iPcjkN0+eWmjsCas0AIIYRdxD8J\nuHP6tEtv4bJHzKB05+Phk5lwsYzb47Ky7JXGQkQ7eRKIPPIk4Kf4eOeOYZytCpMXmial/dtCKfe9\n8W68UeoJhBDCUaF8EgA4csSU919zjeNWDbcOMwPRTf0aTiflOm7VKmjePChZEKJQkyeByCNPAgGo\nWhUaNzbLAwdaWxV895rpbXxfS6i8OddxN9xg5jEWQghRiIOAoyTXH/w/PQnfv2qKhi7fmCv9a6/B\nwYNga9LL+fOhz6MQQkSiQlscZFmzxvQrGDQIpk512Xn1LGj/OEz9Bo42dnv8ggXQoYPUFYjoI8VB\nkScqJpoP5ee57Qtw9WwzEumUb+DoNW4SGCkpsHdvyLImRMSRIBB5pE4gn/78E44edRmuenMPmP9f\n6NcOqq71eOy+fWYMI5mDQIjCbf/+/cTExOQM0taxY8eckT59pXVVs2ZNFi1aFLK8RoIiFQRKlTKd\nw7p3N62AcmzpDvPGQt+OUHG7x+OHDzfTVwohCk6HDh146aWXcm3/8ssvqVatml8jcDoO9TB//vyc\n8X18pY1GRSoIOGre3Py6z7G1q6ks7ncbxP/m8TjpTCZEwRowYABTc1XwmQHa+vXrF1GDrxUFRfpq\nlipl3u++27Zh3b2w6jETCEofd3vMhg3O65s2meGuhRDh0blzZ06cOMGyZctytmVkZPDVV1/Rv39/\nwPy6b9q0KQkJCSQnJzNixAiP52vTpg0ffvghANnZ2Tz11FNUrlyZOnXqMG/ePL/zdeHCBR5//HES\nExOpUaMGQ4YM4eLFiwCcOHGCTp06UaFCBSpWrEjr1q1zjhs9ejQ1atQgPj6e+vXrs3jx4oCuR6gV\n6SBQvrx5InB62vvpSdh2tykaKpF7Iolu3WD+fDPz2Zw50KiRm1ZHQoiQiYuLo3v37kx2GPBr1qxZ\n1K9fn4YNGwJQtmxZpkyZwqlTp5g3bx7jx49nzpw5Ps/93nvvMX/+fNavX8/PP//MJ5984ne+Ro4c\nyerVq9mwYQPr169n9erVjBw5EjCjmCYlJXHixAmOHTvGq6++CsCOHTsYO3Ysv/zyC6dPn+abb74h\nJSUlgKsResULOgOhVKKEqRuwTfVp9/0rtvGGesHML8x0lg6mTzd1A7Z/XyGikhoRnLJyPTzwFkgD\nBgzgjjvu4J133qFEiRJMmTKFAQMG5Oy/5ZZbcpYbNmxIr169WLp0KXfeeafX83788cc8/vjjVK9e\nHYBnnnnGaRpKb6ZPn87YsWOpWLEiAMOHD+fBBx9kxIgRxMbG8vvvv7N3715q165Ny5YtAShWrBgX\nLlxg06ZNVKxYkSuuuCKg6xAWWuuwvczHhd/997uZ9C7mguae2zR3DNKQ7XOSvORkrY8dK5DsCxES\nBfX36K+6devqWbNm6d27d+sSJUroYw5/gKtWrdJt2rTRlStX1gkJCbpUqVK6f//+Wmut9+3bp2Ni\nYnRWVpbWWuvU1FQ9YcIErbXWV111lZ4/f37OebZv3+6U1lVKSor+/vvvtdZalypVSm/ZsiVn37Zt\n23TJkiW11lqfOXNGP/nkk7pWrVq6du3aetSoUTnpZsyYoVu1aqUvu+wy3bt3b3348GGP39nTv4lt\ne0juy0W6OMjy5puml/DgwQ4bs2Ph44+hxkpoNcrnOfbvh927Q5dHIYSzfv368dFHHzF16lTatWtH\n5cqVc/b16dOHzp07c+jQITIyMhg0aJBffR6qVavGb7/ZG4bsD2AMmerVqzul379/f84TRdmyZXn9\n9dfZvXs3c+bM4Y033sgp++/Vqxc//vhjzrFPP/20358ZDlERBMqUgX/8w5TvA9SrZ9txPh6mzYfr\nxkMj33N+Sr8aIcKnf//+LFy4kA8++MCpKAjg7NmzVKhQgdjYWFavXs306dOd9nsKCD169GDMmDEc\nOnSIkydPMnr0aL/z07t3b0aOHElaWhppaWm8/PLLOU1P582bx27br8Ry5cpRvHhxYmJi2LFjB4sX\nL+bChQuUKFGCUqVKRVzrpsjKTYhZFcRO/wZnqptA0H4I1PTeKUSCgBDhk5yczE033cSff/6Zq6x/\n3LhxvPDCCyQkJDBy5Eh69uzptN/TdJL3338/7dq145prruG6666ja9euXvPgeOzzzz/PddddR+PG\njXOOf+655wDYuXMnbdu2pVy5crRs2ZJHHnmE1q1bc/78eZ5++mkqV65M9erVOX78OP/617/yfE1C\nweewEUqpGsBkoAqQDbyvtR7jJt0YoAPwBzBQa73OTRrtzyNbqEybBvfcA6mpsGSJy86UxdC9J3y0\nyMxj7MawYTDKoeTo8GETWKpVC1WOhQgdGTYi8kTqsBGXgCe01lcDNwKPKKWucslgB6C21rouMAgY\nH/ScBkHPnqa1UHy8WU9Odti5rw18/ZZpOlrukNvjR482cxhYrr7adT4DIYQoXHwGAa31EetXvdb6\nLLAVSHRJdhfmaQGt9SogQSlVJch5zbfixU2/gWefhX//G155xUxCn2NjH1jzMPS9HUqednsOq//K\niy9CRoZ9OGohhCiMAhpFVCmVAiwBGtoCgrV9LvAvrfUK2/pCYKjW+leX4wu0OMiThQvh1lutNQ13\nPAQV9sC0eaYVkYs33oAnnjDLMTEy6JwonKQ4KPIURHGQ353FlFJlgU+AwY4BIFCOA0OlpqaSmpqa\n11MFTdu2jmsK5r8DPe+GTg/Alx+abQ6sAODo9Gm4+WZYvz6UORVCRIMlS5awJFfFZWj49SSglCoO\nfAUs0Fq/7Wb/eGCx1nqWbX0b0FprfdQlXUQ+CYCbuQhi/4CBqbDzdljyksfjrCeBjRvNdJfW11u7\nFpo2lRZFInLJk0DkidSKYYAPgS3uAoDNHKA/gFKqBZDhGgAKnYtlYPpXcM1kuPZDj8mys83LdYrK\nnTtDnD8hhAgCn0FAKdUS6Av8RSm1Vin1q1KqvVJqkFLqAQCt9Xxgr1JqF/Au8HBIcx0CVl+TaY59\nxv6oAlMXwF+fhdrfeDx26FC4/nrnba5PFr/8AuPGBSevQgRDcnIySil5RdAr2anJYngUqekl82vM\nGBgwwFQUOw06l7Qcet1tpqg8cq3Xc2RmQlwcfPKJmdxGa9Of4P/+z8xnHMFfXwgRoSKhOCgqPPYY\nJCRAgwYuO35rCfPGQZ9OkHDA6zmsOQysJ4Hff4fERLh0Kfj5FUKI/JIg4Eb9+m42bukGKx+H3p2g\nxBmf57CCQGamef/uu+DlTwghgkWCgA+/OvZ0WPEkHLwRenaFYhc8HjNjBkyaZJbnzg1p9oQQIl+k\nTsCD+HhTtr9rlykiyhFzCXp0gwtl4fPJoL3H0YoVnXsVF5KvL4SIIKGsE5Ag4MGhQ6ZIp3p1N30I\nimdC/7amruC71wI6byH5+kKICCIVwwUgMdEEALculYIZc6HeV9DizYDOu2OHDDMhhIgcEgT8cNNN\nbjZmXgZTv4Yb34CGM/0+15VXwrvvBi9vQgiRHxIE/LBsGaSnw4euHYdPXWEmpOnwGL4mpHFkDUet\ntUvFsxBChJkEAT8oBRUqwL33utl5rBF8PBu69YKquebR8WrjRmjWzJx/1arg5FUIIQIhQSBA99zj\nZuO+VJg3FvrcAeX3+TzH27YRmC5etG974QV46qlg5FAIIfwnrYPyIFdrIUvz/0Lzd+DD5fBnJQ+J\njNtug1tugeefd95eBC6PECLIpIlohPEYBAD++gzUXAwffW9GIs2DInCJhBBBJE1EI8zKlc7rr77q\nsPL9q5B2pakjiJEBg4QQkU2CQB7ccIN5r1PHvBcr5rhXwZwPoNhFuGMQqOyAz791Kxw5As89l++s\nCiGEVxIE8kEpWLQIHnnEZUd2LMz+BCpvhfaDgcDKdxo0gC++ME8Y1lPHkSPm81avDkrWhRACkCCQ\nb23aQBl3Rf8XypoJaWqshFuHEmggsOoFbrzRNE21ZipzmvRGCCHySYJAHrVoAR072tePH3eT6HwC\nTP0Gan8HbYYHdP6HHeZmmzTJR2W0EELkUfGCzkBh9dNPzuuVPLUIzbwMpnxrJq2/UAaWD8vT51lB\nQFoOCSGCSZ4Egqy4u7D6x+Uw+Tto9r7pS5AHrkHguedg+/a85VEIISwSBIJs9Wo4d87NjjOJMHkh\n3PQ6XDsh4PP+9pt5373bvL/6qn3iGiGEyCspDgqykiXNKyYGsl1bh2akmEAwMBUuxcHGvn6ft1cv\n875gAfz5p1mWoiEhRH7Jk0CIZGXBjz+62ZFeF6Z8B7f9AxrOyNO51651Xj961E3AEUIIP0gQCKLO\nnSElxb7eqpWHX+vHG5jK4nZPwNWzA/6cV14x79a5q1aF998P+DRCCCFBIJg+/xxKl/Yz8bGGpvlo\nh8egwScBfc6CBeb9u+/sFcbWHAVCCBEICQIF6WhjMztZx0eh/mcBH+5YLKQUpKXB+fOe02/eLPUI\nQghnEgQK2pEmpmfx7Q9D46l5Ps3s2VC5Mgwe7DlNw4awZk2eP0IIUQRJEIgER66FjxbBX5+FG8bk\n6RQbN5p3qymp1nDwYO50Fy7kMY9CiCJJgkAYNGli3n/+2Uui4w3gwx/h+rHQ5kUCHWvIYtURfPYZ\nJCXl3i/FQUIIRxIEwuDRR817s2bwX28dhk8lw8Qfoe48UzyksgL+rIwMU/afnp63vGotzU2FiCYS\nBMIgLs6+bAUEj/64HD5aDJW2QbfeUMxLTa8by5ebsn/HX/y7dsFUW3XDrbd6P/6ee+DqqwP6SCFE\nISbTS4ZBVhZs2ADXXmvW/RoRtPg56NoHSpyFWZ+Zoanz4P77c/ch8PZPkJwMBw5IsZEQkUTmGC5i\n/B4WOuYS3PEgXL4Rps2HzIpB+Xzrn2DFCmjZ0vmGn5IC+/dLEBAiksgcw0XM6NF+JswuDnPeh31t\n4G+tIGF/UPNhTVQjhIheEgQKwNCh5r17d39SK1g4Cn55AO5rCVXWBy0f7oa9lslrhIguEgQKSGYm\ndOtmlt94w48DVg6Bb/4D/W+FWguDkgcrCBw/DuPGmWUJAkJEF59BQCk1QSl1VCm1wcP+1kqpDKXU\nr7bX88HPZtETF2e/4Q4Z4udBm3uaCey73ANN8z5i3EsvmXcrCCQnwyOPwMmTcOKE2fbEE3k+vRCi\nEPHnSWAi0M5Hmh+01k1tr5FByFfUmT/fz4T7bzGdylr+G9oOAxV4o/4RI8y7FQQyM817q1Zw+rRZ\nfvNNe/rTp00zUyFE0eMzCGitlwEnfSSTQoQ8aNgQYmPNcocOpqI2O9uPljnpdeGDnyDpJ+jeA2L/\nDPizP/00d8Xwvn3u0zZqBHXrmieXPXsC/ighRAQLVp3AjUqpdUqpeUqpBkE6Z5FXv77zWD516tiL\niF5+2cfBmRXNvMUXS8GANlD2SECf3a0b/OMfzttcewpXrWreDxywb1u3LqCPEUJEuGAEgV+AK7TW\nTYB3gC+CcM6o16KFH4mySsLnk2Hn7XB/c6ievyFCXedGPno094Bzrk8pBw7IXAZCFGb5nmNYa33W\nYXmBUmqcUuoyrbXb0WtesmolgdTUVFJTU/ObhSLJ8WZ7++0wb56nlAqWvmjmJuh7u2lBtKFf0PJR\nsqT3/cnJ0KCBGa9ICBEcS5YsYcmSJWH5LL96DCulUoC5WutGbvZV0VoftS03B2ZrrVM8nEd6DPtp\n40Zo3NgsnzwJM2fCQw/5OOjyTdCrM2zrbPoWZOc7xucyezYkJsJNN5l1paB6dTh0KOgfJYSwKdAe\nw0qp6cAKoJ5S6oBS6l6l1CCl1AO2JN2UUpuUUmuBt4CeochotGnkEG7j482N1qdjDeH91aZDWd+O\nUDot6PlassQMNSGEKBp8/lTUWvfxsX8sMDZoORK5BNSBK/MymLbATFDzQDP4eDYcuiFoeckKfHRr\nIUQEkx7DEax1azMRjVJQr14AB2YXh+9egwVjoPed0OIt8jpJjSsrIN15Z+5tgVi3TnonCxEJJAhE\nsCVLzEQ0AFddZTpzWcsZGX6cYPtd8MFKaDQdenaBOF/dPfw3d64ZbdSiNfzpobuCNfWlZc8e2Ls3\naFkRQuSDBIFC5M03YfJk2LoVSpTw86CMmvDhMjh1BQxqlu9mpOPH25cdm7GOGwdlypib+6uvOh/T\nuLFz89Patd3PfyyECD8JAoXIdddBP1vrz1KlAhjzP6sEfP02fPtv04z0hjEEo3joiEP/NOuX/aRJ\n8Nxzprjn7bftRT6TJsELL9jTO/Y/2LcPfv/dfJ/t2/OdrVy0hn/+M/jnFaIokEllCrmAy9Ur7DZD\nTWSkwJwJcK58UPOTkACnTnner7XJ8+uvw1NP2dfr1DGBolWr4E9ok5VlxkmS/3qisJJJZUTwnKwN\nE1bA2Wqm9VC1X4J6em8BwJvTpz3XKQghQkeCQBHQzs0Yr5cueTkgqyTMfwe+/xfc0x6uH0uwWg/l\nlVKh+6VunVeeBITITYJAEeBYJDR1qnkvVgxu8NU9YHMP81TQ9APo0R1KnQhZHi1WXrdsyb091EFA\nCJGbBIEiwFO9gF/1Bel1YcJPcCoJHroGan0X1Lx58uGH5t1q9nrkiPNopcEkTwJCeCZBoAhQygzi\nZi07bvfLpTj45k34YiJ0vhfaDYHi53wfFwTLl9uXH3jAc7r8kCAghGcSBIqYpk2hUiWz3LGjfXt5\nfxoB7bkV/rceEn4zlcb57FOQHxcumCA2Mgjz1MnNXwjPJAgUAY6/+K+6ykwcDzB4sH37okVw881+\nnCyzIsz+GH58DvrcYcYgKnY+qPn1JSsLRo82y459C3zZs8f904+vJ4EPP8w9oY4Q0UKCQBHgqdgn\nLs7MB/D442ZU0vf9nptewcY+MH49VN5i62n8c7Cy61PDhvDii+73HT3q+Thr6kurvsHiKwjcdx/8\n9ltgecyLyZP9HO5DiDCSIFAEWMUm//qX8/bYWDNcw5tvms5StWsHeOKzVWHm5/Djs9DndvjLc2F5\nKti2zXm9bVuoXNn0Kq5aFZ55xmx//32zzdV99zmvR0px0IABMG1aQedCCGcSBIoApeDuu+Hpp72n\nK17cPuxEAGe3PxVcvjnsTwUA338PaWlQs6ZZHzUKPvjAVCS/955DTt08EZ07Z4alAO/BIFyT4kRK\nQBLCIkGgCAhk6IixY+H55/PwIe6eCsLUgshy3uEh5P77nfd5KtN/7jlTvATeb8ChnChn7177PAwS\nBESkkSBQyN1xB/Tu7X/6cuXg5Zft61WrBvJpjnUFW+HBayBlSSAnCLrsbBMEixVzHpn0wQdNJXm6\n25muw6tWLXt9jAQBEWkkCBRyc+dCz3xM6FmxonlfsMAECL+crQqzPoOFo+HuftB5AJQ5lvdM5IPj\nE8DAgfblr77KPSJpQd6A8zqmkhChJkEgSmVlmQDQo4e5ObZvD0OGBHiSbZ1h7Bb4sxI83BCavQcq\nMuafdFfGf+hQ+Mr+PZEnARFpJAhEqZgYU9nq2BTTsW6hf38/T3ShHHz7H5i8EK6ZDPffADV+Cmpe\nvQnkptqoEdSt6/w9Dx8Ofp68kSAgIo0EAZHD8ebYsSPMmRPAwUcbw4c/wk9DoEc3uLs/lAv9z+5X\nXvE/bWameVmWLYPEROc09erBO+8EJ2+OrGsrQUBEGgkCIodrK6NOnUwb/QDOABv7wjvb4XQNeKix\n6XFcsuAKxL0NSueunH7nTli40L5+/nzuDmrZ2dCrV97yozWcOZOHyYCECBEJAiKHuxvTvHl5ONGF\nsvD9qzB+HZQ9An+vBy3eCvvwE2CGy3AnPd3zjTg72z4n8pNP2ltQZWbC2bMmMMyaBd98k7c8uU6e\nc8UV5rxCFAQJAiKHu5ui3xPau3M6Cb78ECZ/DzW/h0frQ6PpoAp+oJ6KFXMPFWG15Z8718zhvHWr\nvUey1mZgvnLlzPAPYCrT/eVYHGQVCZ08Cc2bm3x4Gw5DiFCSICByuJaPB82xhjBjrhmq+oa34YHr\nwjZvgTcnXObQKV7ceX3tWlOBDjBliv0XvOvYRIFYv96+/MYbsKbgBmoVApAgIBwMHGjvXOXuqeBY\nfrsC7G8NH6w0vY5vfwT63QZV1+bzpHn39tve9/fta1pQgRn3x5Kfyl3HDm3BGCZbiPySICByKAUV\nKnjeHxeEnAKiAAAdDUlEQVSXe1vgLWkUbOkGYzebfgZ9O0KXvlB+b6Anyjd/bubumpA6/nr3dI5e\nvWD27LzlS4hwkiAg3KpRI/e2cuVMpejf/mbf1qlTHj8gOxbWPAz/3QEn6pkionZDoHRaHk8YuGLF\nfKfZscP7/hgPf0GzZsGzz7rfF85mop98Yq/rEMIdCQIilwsX4Kab3O8rUQL+7//sU0Hmu6njhXKw\ndLjpeVzsAjx6pRmcrlToB/3xdAMPlt27TZPTgtS9O6xbV7B5EJFNgoDIJTbW+/4bb4R33zW/aC9e\nDNKH/lEF5o+F934x4xD9vS60fRrKhK7ZTDB7C1+44P4Xt+sAdp5GOw3l04HrGEpCOJIgIHzy1kzU\nChgffWTeHae0zJOMFJj7vgkGJc6YZqUd/g4JXnp9FbDJk6F0adO6qFs35315nbbyzBnTma1NG9Nn\nIT9PXH375v1YUfRJEBA+nT0LP/zgfl9SkmlBc9llZv2tt9zXJwQsI8U8GYzdAhdLw6Br4a57odLW\nIJw8uAYMsD8FfPqp8z7rF751E1+61P05XG/y8fFQvjwsWZL3py0ZokL4Q4KA8Ck21vsk9RUrwqVL\n9vWgztd7tqoZsnrMLjhZCwammkltai0EIvMu53hDv+UW8+7aSzgQ1s28fn348Uf7+U6fdj8q6smT\nZsY1x+EvAnHpkqnPCNS4cfDvf+ftM0XBkSAggsLdr9Xu3YP4AecqwA8vwFv7YWsXaD8YHroGmkwq\nkOEo/GU9IQwfbt/m6xe6p6KfbdvMjRZMMVFCgnnq+vVX53TTp8OgQbkntX/wQf8G3Bs/HurU8Z3O\n1VNPwdChgR8XjX780cw4FwkkCIigSE52Xj940LlnbZUqQfqgS3Gw9j4Ytwm+fR0azoTHU+CWl6H0\n8SB9SPh5CwwJCfblmTPN+1aHUrFmzZzTe3rqePdd+O9/nbe99lrueaddK5KVsg+fIYLjllsCGK49\nxCQIiKBo3tz5RpaYCGXLwr33mvXgj9uvYPdtMPVrM5dBwgEzUN3d/WzzGURmURHAm2/m3la3bmDj\nB3mrKLZ+jftTJ/D++zB1qvM2dx0A3Y3GOm4ctGjh+zNEZPMZBJRSE5RSR5VSG7ykGaOU2qmUWqeU\nahLcLIrCzJqyMqRt8o9fbVoUjdkNR641cxkMagpN34fYP0L4wXnz+uvut9esaZqa+sPT9XQtAvLF\n31ZH7gLKl1/CqlWBfZ6IPP78aU4E2nnaqZTqANTWWtcFBgHjg5Q3UQQ4NpGcPdv0YHWsRA6qzMvg\npyfMfAYLR0O9eTDkCrj9Yaj2q+/jC1hmpn9zEX/5peebd+PG9mXXYh7IfTO3ztOypX95dHesp3MX\nJKVg48aCzkXh4DMIaK2XASe9JLkLmGxLuwpIUEoFqwRYFHKOFcbdu0PXrp6Ha3jjjSB9qI4xRUUz\nv4B315oWRj27mGamzf8blt7IeZXmx6gZn37qPghMmeLcMsvxqcL1Br15s/n3sIbFWLEi8Lx6e4p4\n8cUgdiTMo3BPHRqoSAmawXhITwQcGwUesm0Twu/ijWbNTIAIulNXwNIX4e098N2/IWkFDK4FXfuY\nOQ4iYG4DRw0a+E7jqQOat4pG64ZjjQQ7d655KvOX6w1rwQLPafv1g5df9q+p8IwZMGaM//mIZDt3\nmk5+hY1UDIuQuvtuaOehMLFPH0hNhVGj4OefzQxbSUkhyoiOgT1t4dMZJiD8diO0exIeq2NaFsUH\ns3NDaB07Fni/A9chLfwZPM+d994zTysdO8LXX7tP421uaq3tk/IAPP54EHqZR4h69QrndynuO4lP\nhwDHP90atm1uvfTSSznLqamppKamBiELIlJ16uR+pNFffzXl1643owMHwjD/buZlsPrvsPpRU1fQ\n9APT5+DgDab56Y47TFPUCPWdm/l4li3zfozr0B/+XmPXiXYGDcr9a/fhh01LIX+LNwYMiJzmkcF2\n+rT/ab1dryVLlrBkyZJ858fPjGifLyAF2OhhX0dgnm25BbDSy3m0EL4oZU3CGMZX7B+axpM1A9po\nhlXQ3Pk3Tc3vNepS+PMS4pfWnrc77nNcXrHCvv7SS7mPvfNOrUuWdN4WF6f1+vX28/7yi9Z//GH2\nZWebbVWqOH+2o//+V+u0NN//X9auzX0O0Prrr30fG0ygddeu/qe96aZAzo3Wftyr8/Lyp4nodGAF\nUE8pdUApda9SapBS6gHbXX0+sFcptQt4F3g42IFKRJdatQrgQy+Whg394KNF8L8NcLwB3PaUaV3U\n7glIXE0k9z0Ih3/+07w7PMznmDPHzDXh6Nw5U1m9ZImpH2jWDMqUMfu0H5fy73+3d47zZvNm32mC\neZw37r7XyZPw00+5t4f8iddPSvvzrxGsD1NKh/PzROE0bRrs2mWGrPZUnxA2lbZCw1mmZ3KxC2ZW\ntM3d4fB1QIT8FQdIa/c3IOtP09rnmC411dzMg+XSJXPu6tVNJzl3twWlTD3RhQveeyxPmwb33GM/\nR9eu8Nlnps7C2/8fpUxlbl6GyPB0vi5dcg8i+OCD9qHXz50zfTxKljTNcn0V49nPrdBah+Q/nFQM\ni4jTt68Za8fqaOavjz8OQWbS6sOSl+CdrTDzc8gqAV3ugcdrmieFxFUUticET79Ap03zXNkb7OLp\nsWNNfZCnXtLWMCMHDsCRI/btjz5qfyKxuAaQzz6zL995p/cOdIE2Y500KfCmnY6V8mXKmAAA7s+j\ndfib1koQEBHL3z+2PXvMWEXdujnP/xtcCo5eA4tGwjvbYPpcU4TUeSAMSTZFRkkrIq7JaSDuuQc6\ndLCvh3JIiA0u4w+4FpdYTVldjR0Lb7/t+bzWvBaWuXN9F/ucPAmrV3tPY7GGQfHE0xONxdf8ElOn\nep+/IxQkCIiI949/2Je1zt33oGZNM1YRwFVXhSNHCo41gsX/hLFbYeoCOB8Pne43dQgdH4Xa35ri\no0IslENCuN4MrelMv/oqbzday8CBubcpZYbGPngw975z52DYMLjhBuf/Z9ZxwejdHkjZv685rUNB\ngoCIWNYf+2uvOQ9gFhtrbhr+/noLueNXmyKjcZth8ndwugakvgT/uBy69YRG0yK6l3JBmDjR/fYH\nHzRFLt5kZcFzzwV2c61Tx/3QGE2b2n9UuBvTySqa+cNhCCpvQch1X1oaTJhglsdH6IA6EgREoeDa\n2Wn5cjNKqSvHP0J3rVjc+etf85yt3NLqw7KnYcIKU2y0+zZTsfx4CgxsDTe9DpU3U9jqEcLF3SQ5\nYG741jAQp0/Dq6+a5Tgv3TnOnrUf67gOUKqU+2P27DHv1v+j9983fVrc/V9z54svTAc4y8MP2//v\nvvaac9pIaSMjQUAUCu7KUsuXz73NmvMYzPDM/hg5Mm958ulsVdP5bMYc+PdRWD4UKuyGPneYYqNO\n90P9TyEuwKE/o1Sim8Fozp+339w//9x5X2amebdaFjnedM+dsy87bl+71nnboUNw3DZNhWMQcXTx\novNnO9ZZ7Nvn/nN8CWflcDB6DAsREvHx9uWUFOfhBgCqVcv9hxUXZ355ZWWZ3q6eJlm/6y4zGqej\nChVMJWFIXCoFO283LzRU3AF1voamE6DzvXDkGtjV3ryOXAs6+n6fbdmSt+OsITS6dHG/3xqTyvq/\n8oeX0cWHDzeBYMQIs+7448NqrZaWBpdfbpa9FUlt3+7cUMHdD5k1a+C66+xFUlYeS5QI35NC9P1P\nE4VGo0b2ViIxMe6HRnYnJsY8EXj6A923z7Qiad7crFvpwtd5R8GJK2HVYJg23zwl/PAClD4BXfrB\nU1WhS18zH8JlO4mWoqOrr87bce4qfN2xbqreinY2bzZTcFo3bNdiSPC/74prIwXXm7pS5v/g5s3Q\nqpW9ctzRDz/491n5IU8CIqJVrhz8c1pTYVrnLvCem5dKmbqD3bfBN29Awn6otRBSlkLrf5pmp/tS\nYX9r836iLoW1o1oouJupDeCXX5zXs7P9/7d2DAKjRzvv82e4b3dcg4BV/3HpkhlAEaB9e+c0rVsH\nNh5RXkiPYVGkWX/0f/mLGfhszRr7H+PJk5CeboJCbKypbNy1y/wq+9vfCi7PzjRU2AspS2yvxRCT\nZYKB9ZKg4JeyZT2X67vKzDSVx3fe6XlUVE89rz1JTHRf8b12rWmlpLX5YWLVQVjnz8iA8uVD12NY\nngREkde+vRn//tZbnbdXqGBeYH5tWWW+rh2OHA0blvuXIZghlh94IDj5dabgZC3zWvs3nIJC8lK4\nZSTEXHQJCvWQoJBbIL8/rScBb8NiB8pTyycwN3ut7QHAUah/N0sQEEXa3r32VkSzZnkeQsBxiAqr\nhVGJErk7pnkqTw5rfYJrUCi/z/6kcPOrppOaY/FR2pVIUPBeIezKV89esFcO55enm7z11GL9UAkV\nCQKiSEtJsS9fdpl5+dK9u2lV5G4yd/vjedCymE8KMmrCupqwztbV1jEotPoXFD/v/KQgQcEn1zb9\n7rj71Z5X7n5E3H578M7vjQQBIVzExpoipHLlcg9KpxQkJOQ+JtzjvXiVkQLrBpoXuASFUVA8E35r\naWZXO9gCfm9mxkESOV5+OXyf5aluYdeu8Hy+VAwL4YX1x7luHTRpYka0rFIl9x/txYvOHdUiWsIB\nSFoONVZBjZ/g8k2QdpUJCNYrvQ7ytBAe3bq5n+/Zud9K6CqGJQgI4YXj2Pqu2x9+2DwVfPGF6ejk\nqV7AqvSLWMXPQdW1UGOleSX9BLF/muk2D7aAQzeY+RMy/ShLEyEiQUCIAjFlCsyfDzNmOG8/d878\n8necI9lTENizp4BmS8uPcofsTwqJq81czGerwuHr4dD18HtTONIEzrspGxMhIEFAiIhnBYFLl5wn\naN+71wx3XbEitG1rWhxZY820awfffJP7XCVL5p6usUCpLKi0DRLXQPU1UG0tVNkAZ6qZYS5+v9b+\n/keVgs5tESRBQIiI525aRrAHgdWr4frr4bvv4LbbzL5du8wwxxs3QqdO9gHH/OmIFBfnPBBa2Kks\nqLTdFCVV+9UEhqrrICsWjja2va4x4yKl1Tezsok8kiAgRMTr1cv0RfAVBMDs79bNufXR0qVmLl/w\nLwgsXmyGwfanTXv4aCh32DwlVF0PVdab9/J7Ib2uQ3Cwvc5UQyqg/SE9hoWIeFYQAPsQFX/5i+f0\njz3mvO7rpv/WW85j1Rf4mEduKTiTaF67HOaqLJ4JlbeaoFBlo5l5rcp6My7S8QbmSeF4Azhuez9d\nAwkO4SFBQIgQKFsW2rRx3ubY+eyZZ8x4Mf5q3RoGD4YePaB6dfv2e++1z1wV0S6VMpXJvzt+aQ1l\njpngUHkLVNoK9eaa9RJnbIGhvnOQOFkTdDGPHyMCJ8VBQgTJ55+bMe0d/4srZYqDDh400xt6+/X+\nww/mZg/OxUH33QcffOB8TjDn3LbNVDYD1K5t5tItEuJOmoroylucg0TZo2bAPNcnh/S6RbzOQYqD\nhCjUWrUK/JgVK9yPMW9JTIQTJ+zrVq/lW24Jzzj0IXWuAhy80bwcxf5hKqOtoNBougkS5feZIqT0\nOpBeG07WdliuZZ5EhFsSBIQIkvr1g3u+G233P29PD40bm45qHTvan0DmzTOjorqbjrHQu1jGTbES\nUOy8CQSX7YbLdplpPGt+b9bL74M/K5mAkF7HJUDUhnMRMxBUgZAgIESQXHWV+57B/lbgekrnOM0m\nQOnS9ikVwQSfvXvNjFirV5v6CH8nRveka1f49NP8nSOsskqa2dpOXJl7n8qC+IPOAeLqWWa9wm5z\nrFOAcFg+W4WiXkEtdQJChNDChaYZpz+BYNMmM6Um2IPJjh1Qo4a58VvKlDFBwNefUoUKnofOtmRm\nwt13w9dfO29310Q1NRWWLPH1LQobW+W0Y4BwXI7NtD8xZCTDqWQzQJ+1nFmB8AQJqRMQolCyKm39\n0bChGWLizBn7tnr1cqcbNcr/GbJ8iYszvZb374etWz2nGzbMzLZ2pZsf2oWbMj2c/6gCv7mpgCl5\nyv7EUH6/rZhpkZkCtPw+08T1VLJDgHB4z0gx59WRPZW7PAkIUUQ5PgkUK+Y8afpXX5mgY823DPZf\n/k8+Ca+/7vwkYP3ZRmbfhAIUl2ELCPvdv8edglNJcOoKU3F9Osn2butLcbqGqa/wGSikx7AQIkCO\nQaBmTVNvAGbCnKlTc6dXCiZNgt69TUujkyftk/AEEgR69IDZs/Od/aKheKYZujvhgKmXiD8ICb+Z\nAfriD5n3kqfhbDWHwOD6XgNO1pbiICFE3jnevN0FAMhdx+DPtIZvv206sXn6LHA/TWfUuFTKc4W1\npfg5M9SGFRjiD5rlxNX2bW+FLosSBIQo4saPN5OW7NkT/HO7m0inc2f78BldupjK7YwM07lNuHEp\nzj5vtEehK4eL7BoLIUSeLV5sOpwNGuQ8tHUgunTxnWbzZudJ160Obs2amWamK1aYNC1b+j5X7972\n5TJlAsurpWfPvB0XrSQICFFENWli73CWV59+CocP29fT0kxHNOuG3qULNGjgPLmOpWJF816unHNf\nh7VroW7d3OljYmD6dPu6v9N1OhZjHT8Ot97q33HCkCAgRBTwNE2mP6pVsy9XrGhu6p9+anoqV6ni\nfN7XX4ekJDhwwF4k5KpJE/fbv/jCed2xNZPl009N0ZI15LZl/37z7k89hnAmQUCIKFAqyEPnVKni\nfpiMJ580AScpCcq7jMbgGICsORAGDDDvDRua/gqO3M2T0KWLmdfZekqwznnFFSY4uHsiCfZwHkWN\nX0FAKdVeKbVNKbVDKTXMzf7WSqkMpdSvttfzwc+qECKv3nsPNmwo2Dw4dpz74ANT9DNpklm/+277\nAHiWQCfLSfAw3bE8HXjnMwgopWKAd4B2wNVAb6XUVW6S/qC1bmp7jQxyPoUQ+VCxon1IioLSwWGO\nmdRU50pg12alI0Y4Fwe9/DKMHes5vTdz5gSUzRzNmuXtuMLGnzYDzYGdWuv9AEqpmcBdwDaXdNKX\nUIgo9dRT8Ntv3tMkJXne53hT790b+veHo0fNfAmLFsHz+ShbsCqoXQ0YAB99lPfzFhX+FAclAo7/\nvAdt21zdqJRap5Sap5RqEJTcCSEKhSefNNNfepOY6LliumpV+/L06ZCSYn75f/+9+2NefdUUcblj\nVWRbvZ09seZ79sTxc19/3XtaXyK5SMrnsBFKqa5AO631A7b1e4DmWuvHHNKUBbK11n8qpToAb2ut\ncw19pZTSw4cPz1lPTU0l1bWaXwgRVY4fN7/WY4LYTOXUKec6AnfFR9nZpidzXJzz9kcfhXfesU/O\nM3gwvPmm+/wNGwajRztvO38eSpZ03qY1pKebeo9y5fz5BktsL8uIghs7SCnVAnhJa93etv40oLXW\no70csxdoprVOd9kuYwcJIcKuaVPTP8HSv7+9KMg1QKxcCS1awLffmmG8rdZFrul+/dV0aHMdWVVr\nc1xamgkI1jaLdZ7mzc38D/4J3QBy/sTeNUAdpVSyUqoE0AtwqmpRSlVxWG6OCS7pCCFEBFi92sy/\nvHat6cHsrS7ghhvMe+nSzs1LBw50TnfttZ6LtzZtguXLveepb1/7smtz2nDyGQS01lnAo8C3wGZg\nptZ6q1JqkFLqAVuybkqpTUqptZihjqTjthAiYhQvDrVqBdaL2rXJ6cSJpvf000/nTjtpkqnDsIJC\n+fK5Z4Rz1KiR85AcJ0/6l6dQkKGkhRBRr39/mDLFLGsNJ054blX01VfQqZNJt32752lFd+ywFxW5\nFgc1amT6bfTtayrC3c3kZp0jMRHKlCnY4iAhhCjSJk829QYWTwEA4Pbb4dgx3+f01tnNCgrXXJO7\nEtlR3brOU4uGggQBIYTADD3hD6WgcmXf6awhLBw7ybkaOhTOnTPLBVVIIvMJCCEEMG0a/Pln8M5X\npw789JNpaeRo+nTPASc93TSZ/eyz/HWQC4TUCQghRB4dPGh6Qof6tqaUzDEshBAR6eTJ0PcIliAg\nhBBRLJRBQCqGhRAiikkQEEKIKCZBQAghopgEASGEiGISBIQQIopJEBBCiCgmQUAIIaKYBAEhhIhi\nEgSEECKKSRAQQogoJkFACCGimAQBIYSIYhIEhBAiikkQEEKIKCZBQAghopgEASGEiGISBIQQIopJ\nEBBCiCgmQUAIIaKYBAEhhIhiEgSEECKKSRAQQogoJkFACCGimAQBIYSIYhIEhBAiikkQEEKIKCZB\nQAghopgEASGEiGISBIQQIor5FQSUUu2VUtuUUjuUUsM8pBmjlNqplFqnlGoS3GwKIYQIBZ9BQCkV\nA7wDtAOuBnorpa5ySdMBqK21rgsMAsaHIK9FypIlSwo6CxFDroWdXAs7uRbh4c+TQHNgp9Z6v9b6\nIjATuMslzV3AZACt9SogQSlVJag5LWLkP7idXAs7uRZ2ci3Cw58gkAj85rB+0LbNW5pDbtIIIYSI\nMFIxLIQQUUxprb0nUKoF8JLWur1t/WlAa61HO6QZDyzWWs+yrW8DWmutj7qcy/uHCSGEcEtrrUJx\n3uJ+pFkD1FFKJQO/A72A3i5p5gCPALNsQSPDNQBA6L6EEEKIvPEZBLTWWUqpR4FvMcVHE7TWW5VS\ng8xu/Z7Wer5SqqNSahfwB3BvaLMthBAiGHwWBwkhhCi6wlYx7E+Hs8JMKVVDKbVIKbVZKbVRKfWY\nbXsFpdS3SqntSqlvlFIJDsc8Y+tgt1UpdZvD9qZKqQ22a/VWQXyfYFBKxSilflVKzbGtR+W1UEol\nKKU+tn23zUqpG6L4WgxRSm2yfY9pSqkS0XItlFITlFJHlVIbHLYF7bvbruVM2zE/KaWu8CtjWuuQ\nvzDBZheQDMQC64CrwvHZ4XoBVYEmtuWywHbgKmA0MNS2fRgwyrbcAFiLKZJLsV0f68lsFXC9bXk+\n0K6gv18er8kQYCowx7YeldcCmATca1suDiRE47UAqgN7gBK29VnAgGi5FkAroAmwwWFb0L478BAw\nzrbcE5jpT77C9STgT4ezQk1rfURrvc62fBbYCtTAfM+PbMk+Ajrblu/E/CNd0lrvA3YCzZVSVYFy\nWus1tnSTHY4pNJRSNYCOwAcOm6PuWiil4oGbtdYTAWzf8RRReC1sigFllFLFgVKYPkVRcS201suA\nky6bg/ndHc/1CfBXf/IVriDgT4ezIkMplYKJ+CuBKtrWUkprfQS43JbMUwe7RMz1sRTWa/Um8A/A\nsdIpGq9FTSBNKTXRVjT2nlKqNFF4LbTWh4H/AAcw3+uU1nohUXgtHFwexO+ec4zWOgvIUEpd5isD\n0lksyJRSZTFReLDticC15r3I18QrpW4HjtqejLw1Cy7y1wLzON8UGKu1boppPfc00fn/ojzm12oy\npmiojFKqL1F4LbwI5nf3q0l+uILAIcCxkqKGbVuRYnvE/QSYorX+0rb5qDWOku1R7pht+yEgyeFw\n65p42l6YtATuVErtAWYAf1FKTQGOROG1OAj8prX+2bb+KSYoROP/i7bAHq11uu2X6ufATUTntbAE\n87vn7FNKFQPitdbpvjIQriCQ0+FMKVUC0+FsTpg+O5w+BLZord922DYHGGhbHgB86bC9l61GvyZQ\nB1hteyQ8pZRqrpRSQH+HYwoFrfWzWusrtNa1MP/Wi7TW/YC5RN+1OAr8ppSqZ9v0V2AzUfj/AlMM\n1EIpFWf7Dn8FthBd10Lh/As9mN99ju0cAN2BRX7lKIw14+0xLWZ2Ak8XRO18iL9fSyAL0/JpLfCr\n7TtfBiy0ffdvgfIOxzyDqfXfCtzmsL0ZsNF2rd4u6O+Wz+vSGnvroKi8FsA1mB9C64DPMK2DovVa\nDLd9rw2YSszYaLkWwHTgMHAeExDvBSoE67sDJYHZtu0rgRR/8iWdxYQQIopJxbAQQkQxCQJCCBHF\nJAgIIUQUkyAghBBRTIKAEEJEMQkCQggRxSQICCFEFJMgIIQQUez/AV5R6ADiHNoQAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119ab36d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8VNW5//HPE4S2QomgIAgYVGyVioAVRMR2RI5ErYKC\ngthAPS1Se6iX2hba2mNsbX9yTn8t0koL1nqjiooFokKNt4AISARRLgkg10DUiiItULkkz/ljD2GI\nuQw4MzuZ+b5fr7zcl5W9n72J88xea+21zN0REZHMlBV2ACIiEh4lARGRDKYkICKSwZQEREQymJKA\niEgGUxIQEclgcSUBM8s1s1IzW2tm42rYf5yZ/c3M3jKzxWbWNfGhiohIotWbBMwsC/gDMBD4CnCd\nmZ1RrdhPgTfdvTswCpiU6EBFRCTx4nkS6A2sc/fN7r4fmA4MqlamK/AygLuvATqbWZuERioiIgkX\nTxLoAJTFrG+Nbov1FnA1gJn1Bk4GOiYiQBERSZ5ENQzfA7Qys2XAfwFvAhUJOraIiCTJMXGU2Ubw\nzf6gjtFtVdz9X8B/Hlw3s43AhuoHMjMNVCQichTc3ZJx3HieBIqBLmaWY2bNgOFAQWwBM8s2s6bR\n5dHAPHffVdPB3F0/7tx5552hx9BQfnQvdC90L+r+SaZ6nwTcvcLMxgKFBEnjAXcvMbMxwW6fCpwJ\nPGxmlcAq4NvJDFpERBIjnuog3P3vwJerbZsSs7y4+n4REWn49MZwSCKRSNghNBi6F4foXhyie5Ea\nluz6psNOZuapPJ+ISDowMzxJDcNxVQeJSPJ07tyZzZs3hx2GNAA5OTls2rQppefUk4BIyKLf8sIO\nQxqA2v4WkvkkoDYBEZEMpiQgIpLBlARERDKYkoCIJMXmzZvJysqisrISgMsuu4xHH300rrKSOkoC\nIlKjSy+9lPz8/E9tnz17Nu3bt4/rA9vsUFvmnDlzyMvLi6uspI6SgIjUaNSoUUybNu1T26dNm0Ze\nXh5ZWZnz8ZHOvbcy519RRI7I4MGD+fDDD1mwYEHVto8//phnn32WkSNHAsG3+3POOYfs7GxycnK4\n6667aj3eRRddxF/+8hcAKisr+eEPf0ibNm3o0qULzz33XJ2xTJgwgS5dutCyZUvOOussZs2addj+\n+++/n65du1btX758OQBbt25lyJAhtG3bljZt2nDzzTcDcNdddx32VFK9Ouqiiy7ijjvuoF+/fjRv\n3pyNGzfy0EMPVZ2jS5cuTJ069bAYZs+eTc+ePcnOzub000+nsLCQGTNmcO655x5W7re//S1XXXVV\nrde6fj389a/wzDPw0EPBT1KleCQ8F5HDNeT/L0aPHu2jR4+uWv/Tn/7kPXv2rFqfN2+er1y50t3d\nV6xY4e3atfPZs2e7u/umTZs8KyvLKyoq3N09Eon4Aw884O7uf/zjH/3MM8/0bdu2+Y4dO/yiiy46\nrGx1M2bM8Pfee8/d3Z988klv3rz5YesdO3b0pUuXurv7+vXrfcuWLV5RUeHdu3f322+/3f/973/7\n3r17/bXXXnN39/z8fM/Ly6s6fk2x5uTkeElJiVdUVPj+/ft9zpw5vnHjRnd3nz9/vh977LH+5ptv\nurv766+/7tnZ2f7SSy+5u3t5ebmvWbPG9+7d68cff7yXlpZWnatnz54+c+bMGq8T8JNPdgf3Pn3c\nr7/efdSoqr+R5HwuJ+vANZ6sAf+xi4Slvv8vIDE/R2PBggV+3HHH+d69e93d/YILLvCJEyfWWv7W\nW2/1H/zgB+5edxLo37+/T5kyper3CgsL60wC1fXo0cMLCgrc3X3gwIE+adKkT5VZtGiRt23btsZj\nxpME7rzzzjpjGDx4cNV5x4wZU3Xd1X3ve9/zO+64w93dV65c6a1bt/Z9+/bVWLa2v4VkJgFVB4k0\ncIlKA0fjggsuoE2bNsyaNYsNGzZQXFzMiBEjqvYvWbKE/v3707ZtW4477jimTJnC9u3b6z1ueXk5\nnTp1qlrPycmps/wjjzxCz549adWqFa1atWLVqlVV5ykrK+O000771O+UlZWRk5Nz1G0XsfEBzJ07\nl/PPP5/jjz+eVq1aMXfu3HpjABg5ciSPPfYYELSnXHvttTRt2vSoYkoGJQERqVNeXh4PP/ww06ZN\nY+DAgbRp06Zq34gRIxg8eDDbtm3j448/ZsyYMQef+uvUvn17ysoOTV1e19hJW7Zs4cYbb2Ty5Mns\n2LGDHTt28JWvfKXqPJ06dWL9+vWf+r1OnTqxZcuWGnsxNW/enD179lStv/vuu58qE9tbad++fQwd\nOpQf//jHfPDBB+zYsYNLL7203hgAzjvvPJo1a8arr77KY489VmcPqTAoCYhInUaOHMmLL77In//8\nZ0aNGnXYvl27dtGqVSuaNm3KkiVLqr7xHlRbQrj22muZNGkS27ZtY8eOHUyYMKHW8+/evZusrCxO\nOOEEKisrefDBB1m5cmXV/u985zv85je/YdmyZQCsX7+esrIyevfuTfv27Rk/fjx79uxh7969LFy4\nEIAePXowf/58ysrK2LlzJ/fcc0+d92Dfvn3s27ePE044gaysLObOnUthYWHV/m9/+9s8+OCDvPLK\nK7g75eXlrFmzpmp/Xl4eY8eOpVmzZvTt27fOc6VaXEnAzHLNrNTM1prZuBr2tzSzAjNbbmYrzOxb\nCY9UREKRk5ND37592bNnD1deeeVh+yZPnszPf/5zsrOzufvuuxk2bNhh+2O/Tccujx49moEDB9K9\ne3fOPfdchgwZUuv5zzzzTG6//Xb69OlDu3btWLVqFf369avaP3ToUH72s58xYsQIWrZsyVVXXcVH\nH31EVlYWzzzzDOvWrePkk0+mU6dOPPnkkwAMGDCAYcOGcfbZZ9OrVy+uuOKKWuMGaNGiBZMmTeKa\na66hdevWTJ8+nUGDBlXt79WrFw8++CC33nor2dnZRCIRtmzZUrU/Ly+PlStXNrinAIhjFFEzywLW\nAhcD5QRzDg9399KYMj8BWrr7T8zsBGANcKK7H6h2LI/nUVEkk2gU0fT3ySefcOKJJ7Js2bJa2w6g\n4Y4i2htY5+6b3X0/MB0YVK2MA1+MLn8R+LB6AhARyVSTJ0+mV69edSaAsMQzqUwHoCxmfStBYoj1\nB6DAzMqBFsAwRESEU045BeBTL7g1FImaWWwg8Ka79zez04AXzOxsd99VvWDsWCSRSETziIpIWtu4\nceMR/05RURFFRUWJD6YG8bQJ9AHy3T03uj6e4MWFCTFlngX+n7u/Fl1/CRjn7m9UO5baBESqUZuA\nHNRQ2wSKgS5mlmNmzYDhQEG1MpuBAQBmdiLwJWBDIgMVEZHEq7c6yN0rzGwsUEiQNB5w9xIzGxPs\n9qnA3cBDZvZ29Nd+7O4fJS1qERFJCE00LxKyzp071/nGrGSOnJwcNm3a9KntyawOUhIQaaQ2bIDy\ncpg/H6ZNC9b37g32mcHYsZCdDd27wxVXwOc+F268cvSSmQQS1TtIRJJo+XI49lgoKYHf/x5eeinY\nfu650KYNfPnLMG8eHHccHHNMkARE4qEnAZEG6L33gg/11auhsBAWLw62f/nLcPXVMHgwfPWr0KRJ\nuHFKauhJQCQD7N0Lf/87/PrXsGRJsG3kSDjlFHjkETj99HDjk/SkJCASon374IUXgqkEn34aOnWC\noUPh8cfh1FPDjk4ygZKASAq5Bx/6zz0X1PMvXgzdusFVV8HcuUEdv0gqqU1AJMkOVvNMmQKrVgUN\nt9/+NrRrBxdfDPVMqiWiLqIijUlxMbzyCixdGvxs3QqnnQb/8R9w3XXQsyc0axZ2lNKYKAmINHBb\ntwYf/ldffWjb3XdDv37Bh37LluHFJo2fkoBIA7V3L3zzmzBjBnTtCjt3whtvBFU9IomiLqIiDci+\nfVBUBPffH3z4Q9DAe955oYYlclQ00bxInHbuhDvugLZtYdy44L/z5kFlpRKANF56EhCpg3swTMMt\ntwTrnTrB3/4G/fuHG5dIouhJQKQGq1bB8OGQlRUkgJEj4e23YcsWJQBJL3oSEIlyh/374bLL4OWX\n4cILg4Ha9KEv6UxJQDLexx9DQQHcd18wZo8ZvP9+MDqnSLqLqzrIzHLNrNTM1prZuBr2/9DM3jSz\nZWa2wswOmNlxiQ9XJHF27YKLLgoGZpsxA66/Phi9s7JSCUAyRzwTzWcBa4GLgXKCOYeHu3tpLeW/\nAdzq7gNq2Kf3BCRUu3bB1KnBh/6iRcG2hQvh/PPDjUukLmFPNN8bWOfum919PzAdGFRH+euAxxMR\nnEgiHDgAv/pV8EH/xS8G3Tx79QqqfNyVACSzxZMEOgBlMetbo9s+xcy+AOQCT3/20EQ+m4ULg2Ec\nTjoJHnsMzjoLXn8d9uyBe+8N+vmLZLpENwxfASxw949rK5Cfn1+1HIlEiEQiCQ5BMtWBA8EsXHPn\nBi9x7dgBZ54Jc+YEs3BpykVpLIqKiigqKkrJueJpE+gD5Lt7bnR9PODuPqGGsn8DnnT36bUcS20C\nknB//zvMnBnU83/0EfToEUzMcvvt8PnPhx2dyGcX6gByZtYEWEPQMPwusAS4zt1LqpXLBjYAHd39\n37UcS0lAEmbxYhg/PvjWf9pp8LvfwaWXBuP1i6STUBuG3b0CGAsUAquA6e5eYmZjzOzGmKKDgedr\nSwAiiXDgAMyfD6NGBQ26LVsGM3S98w5ccYUSgMiR0lDS0iisWRP08Hn00WCY5vPOg//+bzjnnLAj\nE0m+sLuIioRi+XL4n/8JevGccQZkZwc9ft59F2bNUgIQSQQ9CUiDc+AA3HADTJsWjOPTrh38+tdw\n4olhRyYSDk0qI2lt9+7gLd433gj68x+0alUwW5eIJI+eBCQ0H3wQ9Oy55pqgV8/llwfTNQ4YAN26\nqV+/yEGaY1jSyoEDMHEi/OhHQdfOAQOCETybNAk7MpGGSdVBkhYKC+Huu+HVV4P1yZPhppvCjUkk\n06l3kCSNe1Dlc8cdcOyxMHAgfP3r8NprwT4lAJHwKQlIUjzySDA1Y9u2sHUrvPJKMGvXL38JffuG\nHZ2IHKQ2AUm4OXOCRt4JE4KunpqgReSzUZuANArbtwdDN2zYoLl5RRoLVQfJZ1ZZGQze1qYN/OMf\nsGKFEoBIY6EkIEftX/8Kvvk3aQJPPx3U+69fr8laRBoTVQfJEduzB4YMCcbxb9UKHn8chg3Ty10i\njZGSgMStsjKo67/hhmCylueeC8b2EZHGS72DJC4LF8Lo0cG3/dzcYHTPLFUmiqSEegdJKPbsgT59\ngoZegIcfhrw8VfuIpJO4vsuZWa6ZlZrZWjMbV0uZiJm9aWYrzeyVxIYpqfbuu9C8eZAAfvObYO7e\nkSOVAETSTTxzDGcBawnmGC4HioHh7l4aUyYbWAhc4u7bzOwEd99ew7FUHdQILFgAF14YNP4+9ZQ+\n+EXCFvbMYr2Bde6+2d33A9OBQdXKjACedvdtADUlAGkciouDBPDd78KMGUoAIukuniTQASiLWd8a\n3RbrS0BrM3vFzIrNLC9RAUpqrF8PJ50EvXsH4/788Y9hRyQiqZCohuFjgHOA/kBzYJGZLXL3d6oX\nzM/Pr1qORCJEIpEEhSBH66c/hXvvhTPPDF76Ov/8sCMSyWxFRUUUFRWl5FzxtAn0AfLdPTe6Ph5w\nd58QU2Yc8Hl3vyu6/mdgrrs/Xe1YahNoQCorg+Gcp06F1auDJCAiDU/YbQLFQBczyzGzZsBwoKBa\nmdlAPzNrYmbHAucBJYkNVRJp9+5gWscnnoBNm5QARDJVvUnA3SuAsUAhsAqY7u4lZjbGzG6MlikF\nngfeBhYDU919dfLClqNVWQkzZ0KLFrBmDZSWQk5O2FGJSFj0xnAG+fDDYEL34uJgtq9f/jLsiEQk\nHmFXB0kjV1oK118Pp58eVPvs3q0EICIBDRuR5iZMgPHjoWXLoP4/NzfsiESkIVF1UBpbsgTOOw9u\nuQUmTgw7GhE5WsmsDlISSFOrVsHZZweDvn3zm2FHIyKfhdoE5Ij88Idw1lnwjW8oAYhI3dQmkEYO\nVv9AMPLn7beHG4+INHxKAmnAHQYNgmeeCdZ37gwagkVE6qMk0Mh98gl84QvB8gsvwIAB4cYjIo2L\nkkAjduAAHHtssLxoUTALmIjIkVASaKQqK6FpUzj1VHjnHY37LyJHR72DGqlbbw3++8ILSgAicvT0\nnkAj9PzzwZu/r70GffuGHY2IJJveE5Aqv/99kABmzlQCEJHPTk8CjchLLwW9fyZODIaCEJHMoGEj\nhMrKYBTQiRPhiivCjkZEUin06iAzyzWzUjNbG51Ksvr+r5vZx2a2LPpzR+JDzWzXXgsbNgRDQYiI\nJEq9XUTNLAv4A3AxUA4Um9ns6Gxisea7+5VJiDHjPfdcMAH822+rJ5CIJFY8TwK9gXXuvtnd9wPT\ngUE1lNPHUxJs3Bh8+7/3XujWLexoRCTdxJMEOgBlMetbo9uqO9/MlpvZc2bWNSHRZbjt24OXwX7+\nc7j55rCjEZF0lKg3hpcCJ7v7HjO7FJgFfClBx85Yt98eNAb/4hdhRyIi6SqeJLANODlmvWN0WxV3\n3xWzPNfMJptZa3f/qPrB8vPzq5YjkQiRSOQIQ05/+/dD//5BG8D69WFHIyKpVlRURFFRUUrOVW8X\nUTNrAqwhaBh+F1gCXOfuJTFlTnT396PLvYEn3b1zDcdSF9F6uMOwYUFj8Lp1cNJJYUckImFLZhfR\nep8E3L3CzMYChQRtCA+4e4mZjQl2+1RgqJndBOwH/g0MS0awmeCJJ2DBAiguVgIQkeTTy2INyD//\nCdnZMGtWMEmMiAjojeGMUF4OHToE3UDfekvvA4jIIUoCaW7LFsjJgdat4YMPIEvD+olIDCWBNDd8\nePAk8PLLcIym+RGRakJtGJbkKiyE11+HVauUAEQk9VTxEKKf/QwGDoT77z80V7CISCqpOigk+/bB\n5z4HU6bAjTeGHY2INGRqE0gz7jBiBGzaBIsWhR2NiDR0ahNIIzNmwDXXBPX/a9eGHY2IZDo9CaRQ\nSQl07Qr9+sErr6ghWETiE/rMYvLZLFkCl1wSJIAbb4T585UARKRh0EdRkhUUHBoC4qmnYOjQcOMR\nEYmlJJBE06ZBXh7Mmwdf+1rY0YiIfJqqg5Lkf/83SAAzZyoBiEjDpSeBBDtwAPr2DXr+rFsHXbqE\nHZGISO2UBBJo925o0SJYXrBACUBEGj5VByXIhg3QuTP06QOffAIXXBB2RCIi9YsrCZhZrpmVmtla\nMxtXR7leZrbfzK5OXIgN3+TJcNpp0L07PP98MByEiEhjEM8cw1nAWoI5hsuBYmC4u5fWUO4Fgukl\n/+Luf6vhWGn3stiuXcFkMN/8Jtx7r/r/i0jihf2yWG9gnbtvdvf9wHSgpskPvw/MAP6RwPgatMpK\nOPts6NkT7rtPCUBEGp94kkAHoCxmfWt0WxUzOwkY7O5/BDJmYsT77oM2bYIqIBGRxihR310nArFt\nBWmfCF5/HW6+OegKqjYAEWms4kkC24CTY9Y7RrfFOheYbmYGnABcamb73b2g+sHy8/OrliORCJFI\n5AhDDl9FBVx3HQwZAqefHnY0IpJuioqKKCoqSsm54mkYbgKsIWgYfhdYAlzn7iW1lH8QeCadG4Zv\nuw2Ki+HVV8HS/plHRMIW6nwC7l5hZmOBQoI2hAfcvcTMxgS7fWr1X0lCnA3G0qUwcSKsXq0EICKN\nn+YTOEKXXx70Cpo7N+xIRCRTaGaxBqK8HObMgRRV1YmIJJ2eBI5AJBK0BezeHXYkIpJJwn5ZTIAd\nO+Dtt4MkICKSLvQkEKebbgqeAB55JOxIRCTTJPNJQEkgDuXlwfhAZWXQsWPY0YhIplF1UMimT4dv\nfUsJQETSj5JAHKZPh2HDwo5CRCTxlATqsWIFvPceDBgQdiQiIomnJFCPWbNg6FANEy0i6UlJoB7P\nPhu8JSwiko6UBOqwdi2sXw8XXhh2JCIiyaEkUIfJk+G734VmzcKOREQkOfSeQC0qK6FJE3jtNejb\nN+xoRCST6T2BELzwAnTrpgQgIulNSaAWU6bA974XdhQiIsml6qAafPQRdOoE77wD7duHHY2IZLrQ\nq4PMLNfMSs1srZmNq2H/lWb2lpm9aWZLzOyCxIeaOo89Bt/4hhKAiKS/eOYYzgLWEswxXA4UA8Pd\nvTSmzLHuvie63A140t3PrOFYjeJJoHNnmDoVLrkk7EhERMJ/EugNrHP3ze6+H5gODIotcDABRLUA\nKhMXYmotWQKbN8PFF4cdiYhI8sWTBDoAZTHrW6PbDmNmg82sBHgG+M/EhJd6s2bBrbcG3UNFRNJd\nwkbEcfdZwCwz6wfcDfxHTeXy8/OrliORCJFIJFEhfGbu8NRTwaihIiJhKSoqoihFk5nH0ybQB8h3\n99zo+njA3X1CHb+zHujl7h9V296g2wSWL4erroING8CSUvsmInLkwm4TKAa6mFmOmTUDhgMF1QI8\nLWb5HKBZ9QTQGDz+OAwapAQgIpmj3uogd68ws7FAIUHSeMDdS8xsTLDbpwJDzGwksA/4N3BtMoNO\nlqVL4bbbwo5CRCR19LJYVEUFtGoFGzfC8ceHHY2IyCFhVwdlhNWr4aSTlABEJLMoCUS98AJ89ath\nRyEiklpKAlFvvQW9e4cdhYhIaqlNgGDugJYtYcEC6NEj7GhERA6nNoEke/VV2L0buncPOxIRkdRS\nEgCKiuCmm/R+gIhkHiUBYN68YOhoEZFMk/FtAvv2Bd1Ct26F7OywoxER+TS1CSRRcTF86UtKACKS\nmTI+CRQVwde+FnYUIiLhyPgkUFAAl18edhQiIuHI6DaBbdugWzd4/31o2jTsaEREaqY2gSQ5+BSg\nBCAimSqjk8CsWTB4cNhRiIiEJ2Org3bsgJwcKC+HFi3CjkZEpHaqDkqCOXOgf38lABHJbHElATPL\nNbNSM1trZuNq2D/CzN6K/iwws26JDzWxXnsN+vULOwoRkXDFM9F8FrAWuBgoJ5hzeLi7l8aU6QOU\nuPtOM8slmJi+Tw3HahDVQRUV0L49LFwIXbqEHY2ISN3Crg7qDaxz983uvh+YDgyKLeDui919Z3R1\nMdAhsWEm1ty50KGDEoCISDxJoANQFrO+lbo/5L8DzP0sQSXbiy/C8OFhRyEiEr5jEnkwM7sIuAGo\ntbY9Pz+/ajkSiRCJRBIZQr0qK4P3A556KqWnFRGJW1FREUVFRSk5VzxtAn0I6vhzo+vjAXf3CdXK\nnQ08DeS6+/pajhV6m8DChXDjjbBiheYPEJHGIew2gWKgi5nlmFkzYDhQUC3AkwkSQF5tCaCh+Otf\n4eqrlQBERCCO6iB3rzCzsUAhQdJ4wN1LzGxMsNunAj8HWgOTzcyA/e7e4KZtr6iAZ5+FmTPDjkRE\npGHIqDeGCwrgV7+CxYv1JCAijUfY1UFp45lnYNgwJQARkYMy5klg3bpgBrG1a+H000MJQUTkqCTz\nSSBjksDIkZCVBQ89FMrpRUSOWjKTQELfE2ioiouDF8RKS+svKyKSSTKiTeCee+BHP4KWLcOORESk\nYUn76qCXXoLrr4f166F585SeWkQkIVQddJT+9S8YMQKeeEIJQESkJmn7JOAOV10FbdvC1KkpOaWI\nSFLoSeAoPP10UAWkgeJERGqXlk8C+/bBqafC44/DhRcm/XQiIkmlN4aP0BNPBC+GKQGIiNQt7aqD\nVq6E226DJ58MOxIRkYYvrZ4Eli2Dbt1g/Hjo3z/saEREGr60aRP45z+DeYO//3349a+TcgoRkVBo\n7KB67NgBN9yg7qAikp7UMFyHefOgVy/Yvh1+97uwoxERaVziSgJmlmtmpWa21szG1bD/y2a20Mw+\nMbMfJD7Mmk2fDpEIXHttkAz0VrCIyJGpt3eQmWUBfwAuBsqBYjOb7e6xY3J+CHwfGJyUKKvZsQPy\n82HSJJgxA4YMScVZRUTSTzxPAr2Bde6+2d33A9OBQbEF3H27uy8FDiQhxip79gSTxHfsGHzzLylR\nAhAR+SziSQIdgLKY9a3RbSlRWRm8/NWuXVDds2oVLF8e/JxxRqqiEBFJTyl/WSw/P79qORKJEIlE\nai27cCHk5cGGDcH6HXfAL3+Z3PhERMJWVFREUVFRSs5VbxdRM+sD5Lt7bnR9PODuPqGGsncC/3L3\n39ZyrLi6iL74ItxyC6xeDTfdBL/4BZxwQhxXIyKShsLuIloMdDGzHDNrBgwHCuooH3eg7rBzJyxa\nFAz73KoVmMEVVwRzAm/fDpMnKwGIiCRLXC+LmVkucC9B0njA3e8xszEETwRTzexE4A3gi0AlsAvo\n6u67qh2n6kmgvBx694Zt2w7ugz59YPRoGDQIWrdO2DWKiDRqafXGcG6us2LFoQ//desgKysY+llE\nRD4trZIAOJ//PCxdCl27puzUIiKNVlrNLJbieeZFRKQOjX7sIBEROXpKAiIiGUxJQEQkgykJiIhk\nMCUBEZEMpiQgIpLBlARERDKYkoCISAZTEhARyWBKAiIiGUxJQEQkgykJiIhkMCUBEZEMFlcSMLNc\nMys1s7VmNq6WMpPMbJ2ZLTezHokNU0REkqHeJGBmWcAfgIHAV4DrzOyMamUuBU5z99OBMcCfkhBr\nWknVJNKNge7FIboXh+hepEY8TwK9gXXuvtnd9wPTgUHVygwCHgFw99eB7OiUk1IL/YEfontxiO7F\nIboXqRFPEugAlMWsb41uq6vMthrKiIhIA6OGYRGRDFbvHMNm1gfId/fc6Pp4wN19QkyZPwGvuPsT\n0fVS4Ovu/n61Y2lySRGRoxDmHMPFQBczywHeBYYD11UrUwD8F/BENGl8XD0BQPIuQkREjk69ScDd\nK8xsLFBIUH30gLuXmNmYYLdPdfc5ZnaZmb0D7AZuSG7YIiKSCPVWB4mISPpKWcNwPC+cNWZm1tHM\nXjazVWa2wsxujm5vZWaFZrbGzJ43s+yY3/lJ9AW7EjO7JGb7OWb2dvReTQzjehLBzLLMbJmZFUTX\nM/JemFm2mT0VvbZVZnZeBt+L28xsZfQ6/mpmzTLlXpjZA2b2vpm9HbMtYdcevZfTo7+zyMxOjisw\nd0/6D0FDu+rxAAADPklEQVSyeQfIAZoCy4EzUnHuVP0A7YAe0eUWwBrgDGAC8OPo9nHAPdHlrsCb\nBFVynaP35+CT2etAr+jyHGBg2Nd3lPfkNmAaUBBdz8h7ATwE3BBdPgbIzsR7AZwEbACaRdefAEZl\nyr0A+gE9gLdjtiXs2oGbgMnR5WHA9HjiStWTQDwvnDVq7v6euy+PLu8CSoCOBNf5cLTYw8Dg6PKV\nBP9IB9x9E7AO6G1m7YAvuntxtNwjMb/TaJhZR+Ay4M8xmzPuXphZS+BCd38QIHqNO8nAexHVBGhu\nZscAXyB4pygj7oW7LwB2VNucyGuPPdYM4OJ44kpVEojnhbO0YWadCTL+YuBEj/aUcvf3gLbRYrW9\nYNeB4P4c1Fjv1e+AHwGxjU6ZeC9OAbab2YPRqrGpZnYsGXgv3L0c+P/AFoLr2unuL5KB9yJG2wRe\ne9XvuHsF8LGZta4vAL0slmBm1oIgC98SfSKo3vKe9i3xZnY58H70yaiubsFpfy8IHufPAe5z93MI\nes+NJzP/Lo4j+LaaQ1A11NzMricD70UdEnntcXXJT1US2AbENlJ0jG5LK9FH3BnAo+4+O7r5/YPj\nKEUf5f4R3b4N6BTz6wfvSW3bG5MLgCvNbAPwONDfzB4F3svAe7EVKHP3N6LrTxMkhUz8uxgAbHD3\nj6LfVGcCfcnMe3FQIq+9ap+ZNQFauvtH9QWQqiRQ9cKZmTUjeOGsIEXnTqW/AKvd/d6YbQXAt6LL\no4DZMduHR1v0TwG6AEuij4Q7zay3mRkwMuZ3GgV3/6m7n+zupxL8W7/s7nnAM2TevXgfKDOzL0U3\nXQysIgP/LgiqgfqY2eej13AxsJrMuhfG4d/QE3ntBdFjAFwDvBxXRClsGc8l6DGzDhgfRut8kq/v\nAqCCoOfTm8Cy6DW3Bl6MXnshcFzM7/yEoNW/BLgkZvtXgRXRe3Vv2Nf2Ge/L1znUOygj7wXQneCL\n0HLgbwS9gzL1XtwZva63CRoxm2bKvQAeA8qBvQQJ8QagVaKuHfgc8GR0+2Kgczxx6WUxEZEMpoZh\nEZEMpiQgIpLBlARERDKYkoCISAZTEhARyWBKAiIiGUxJQEQkgykJiIhksP8DaLC8ttXRRHkAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119ab3630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
