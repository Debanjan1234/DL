{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # MNIST Data\n",
    "# import numpy as np\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import impl.layer as l\n",
    "\n",
    "# # Dataset preparation and pre-processing\n",
    "# mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "# X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "# X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "# X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Latin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# # Count the freq of words in the text/collection of words\n",
    "# word_counts = Counter(text)\n",
    "# # Having counted the frequency of the words in collection, sort them from most to least/top to bottom/descendng\n",
    "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "# # first enumerating for vocab to int\n",
    "# vocab_to_int = {words: ii for ii, words in enumerate(sorted_vocab)}\n",
    "# # into_to_vocab after enumerating through the sorted vocab\n",
    "# int_to_vocab = {ii: words for words, ii in vocab_to_int.items()}\n",
    "\n",
    "counted_labels = Counter(train_Y)\n",
    "key_to_val = {key: val for val, key in enumerate(counted_labels)}\n",
    "key_to_val['Country']\n",
    "\n",
    "val_to_key = {val: key for val, key in enumerate(counted_labels)}\n",
    "val_to_key[1]\n",
    "\n",
    "# labels = []\n",
    "# for val, key in enumerate(counted_labels):\n",
    "#     print(val, key)\n",
    "#     labels.append(val)\n",
    "  \n",
    "# labels = np.array(labels, dtype=int)\n",
    "# labels.size, np.max(labels), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from impl.layer import onehot\n",
    "\n",
    "# labels_onehot = onehot(labels)\n",
    "\n",
    "# labels, labels_onehot, counted_labels.keys()\n",
    "# key_to_vec = {key: vec for key, vec in zip(counted_labels.keys(), labels_onehot)}\n",
    "# key_to_vec, key_to_vec['Vocal']\n",
    "\n",
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.5994 valid loss: 2.5901, valid accuracy: 0.0692\n",
      "Iter-200 train loss: 2.5835 valid loss: 2.5888, valid accuracy: 0.0708\n",
      "Iter-300 train loss: 2.5844 valid loss: 2.5874, valid accuracy: 0.0746\n",
      "Iter-400 train loss: 2.5930 valid loss: 2.5861, valid accuracy: 0.0777\n",
      "Iter-500 train loss: 2.5927 valid loss: 2.5849, valid accuracy: 0.0792\n",
      "Iter-600 train loss: 2.5865 valid loss: 2.5836, valid accuracy: 0.0808\n",
      "Iter-700 train loss: 2.6473 valid loss: 2.5823, valid accuracy: 0.0808\n",
      "Iter-800 train loss: 2.5328 valid loss: 2.5810, valid accuracy: 0.0838\n",
      "Iter-900 train loss: 2.5692 valid loss: 2.5798, valid accuracy: 0.0869\n",
      "Iter-1000 train loss: 2.5427 valid loss: 2.5785, valid accuracy: 0.0877\n",
      "Iter-1100 train loss: 2.5510 valid loss: 2.5773, valid accuracy: 0.0892\n",
      "Iter-1200 train loss: 2.5782 valid loss: 2.5760, valid accuracy: 0.0900\n",
      "Iter-1300 train loss: 2.5566 valid loss: 2.5749, valid accuracy: 0.0885\n",
      "Iter-1400 train loss: 2.5438 valid loss: 2.5737, valid accuracy: 0.0885\n",
      "Iter-1500 train loss: 2.5731 valid loss: 2.5725, valid accuracy: 0.0877\n",
      "Iter-1600 train loss: 2.6086 valid loss: 2.5714, valid accuracy: 0.0877\n",
      "Iter-1700 train loss: 2.5514 valid loss: 2.5702, valid accuracy: 0.0885\n",
      "Iter-1800 train loss: 2.5302 valid loss: 2.5690, valid accuracy: 0.0892\n",
      "Iter-1900 train loss: 2.5765 valid loss: 2.5679, valid accuracy: 0.0900\n",
      "Iter-2000 train loss: 2.5560 valid loss: 2.5667, valid accuracy: 0.0908\n",
      "Iter-2100 train loss: 2.5703 valid loss: 2.5655, valid accuracy: 0.0908\n",
      "Iter-2200 train loss: 2.5723 valid loss: 2.5644, valid accuracy: 0.0931\n",
      "Iter-2300 train loss: 2.5506 valid loss: 2.5632, valid accuracy: 0.0946\n",
      "Iter-2400 train loss: 2.5593 valid loss: 2.5621, valid accuracy: 0.0938\n",
      "Iter-2500 train loss: 2.5635 valid loss: 2.5610, valid accuracy: 0.0931\n",
      "Iter-2600 train loss: 2.5384 valid loss: 2.5599, valid accuracy: 0.0938\n",
      "Iter-2700 train loss: 2.5415 valid loss: 2.5588, valid accuracy: 0.0946\n",
      "Iter-2800 train loss: 2.5060 valid loss: 2.5578, valid accuracy: 0.0969\n",
      "Iter-2900 train loss: 2.5152 valid loss: 2.5567, valid accuracy: 0.0977\n",
      "Iter-3000 train loss: 2.5550 valid loss: 2.5556, valid accuracy: 0.1008\n",
      "Iter-3100 train loss: 2.5722 valid loss: 2.5545, valid accuracy: 0.1023\n",
      "Iter-3200 train loss: 2.5016 valid loss: 2.5535, valid accuracy: 0.1031\n",
      "Iter-3300 train loss: 2.5484 valid loss: 2.5525, valid accuracy: 0.1038\n",
      "Iter-3400 train loss: 2.5921 valid loss: 2.5514, valid accuracy: 0.1023\n",
      "Iter-3500 train loss: 2.5566 valid loss: 2.5504, valid accuracy: 0.1031\n",
      "Iter-3600 train loss: 2.5515 valid loss: 2.5494, valid accuracy: 0.1046\n",
      "Iter-3700 train loss: 2.5177 valid loss: 2.5483, valid accuracy: 0.1062\n",
      "Iter-3800 train loss: 2.5183 valid loss: 2.5473, valid accuracy: 0.1062\n",
      "Iter-3900 train loss: 2.5563 valid loss: 2.5463, valid accuracy: 0.1077\n",
      "Iter-4000 train loss: 2.5337 valid loss: 2.5453, valid accuracy: 0.1085\n",
      "Iter-4100 train loss: 2.5575 valid loss: 2.5442, valid accuracy: 0.1077\n",
      "Iter-4200 train loss: 2.5029 valid loss: 2.5432, valid accuracy: 0.1077\n",
      "Iter-4300 train loss: 2.4940 valid loss: 2.5422, valid accuracy: 0.1077\n",
      "Iter-4400 train loss: 2.5315 valid loss: 2.5413, valid accuracy: 0.1085\n",
      "Iter-4500 train loss: 2.5859 valid loss: 2.5403, valid accuracy: 0.1092\n",
      "Iter-4600 train loss: 2.5338 valid loss: 2.5394, valid accuracy: 0.1092\n",
      "Iter-4700 train loss: 2.5254 valid loss: 2.5384, valid accuracy: 0.1092\n",
      "Iter-4800 train loss: 2.4779 valid loss: 2.5375, valid accuracy: 0.1108\n",
      "Iter-4900 train loss: 2.5280 valid loss: 2.5365, valid accuracy: 0.1100\n",
      "Iter-5000 train loss: 2.5385 valid loss: 2.5356, valid accuracy: 0.1123\n",
      "Iter-5100 train loss: 2.5460 valid loss: 2.5346, valid accuracy: 0.1131\n",
      "Iter-5200 train loss: 2.5093 valid loss: 2.5337, valid accuracy: 0.1146\n",
      "Iter-5300 train loss: 2.5483 valid loss: 2.5328, valid accuracy: 0.1146\n",
      "Iter-5400 train loss: 2.5680 valid loss: 2.5319, valid accuracy: 0.1162\n",
      "Iter-5500 train loss: 2.4844 valid loss: 2.5310, valid accuracy: 0.1162\n",
      "Iter-5600 train loss: 2.5109 valid loss: 2.5300, valid accuracy: 0.1177\n",
      "Iter-5700 train loss: 2.5170 valid loss: 2.5291, valid accuracy: 0.1169\n",
      "Iter-5800 train loss: 2.5192 valid loss: 2.5282, valid accuracy: 0.1185\n",
      "Iter-5900 train loss: 2.5248 valid loss: 2.5274, valid accuracy: 0.1177\n",
      "Iter-6000 train loss: 2.5038 valid loss: 2.5266, valid accuracy: 0.1185\n",
      "Iter-6100 train loss: 2.5710 valid loss: 2.5257, valid accuracy: 0.1208\n",
      "Iter-6200 train loss: 2.5438 valid loss: 2.5248, valid accuracy: 0.1223\n",
      "Iter-6300 train loss: 2.5757 valid loss: 2.5239, valid accuracy: 0.1223\n",
      "Iter-6400 train loss: 2.4707 valid loss: 2.5231, valid accuracy: 0.1238\n",
      "Iter-6500 train loss: 2.4894 valid loss: 2.5222, valid accuracy: 0.1246\n",
      "Iter-6600 train loss: 2.5585 valid loss: 2.5214, valid accuracy: 0.1254\n",
      "Iter-6700 train loss: 2.5457 valid loss: 2.5205, valid accuracy: 0.1238\n",
      "Iter-6800 train loss: 2.5147 valid loss: 2.5197, valid accuracy: 0.1238\n",
      "Iter-6900 train loss: 2.4455 valid loss: 2.5188, valid accuracy: 0.1238\n",
      "Iter-7000 train loss: 2.4817 valid loss: 2.5180, valid accuracy: 0.1231\n",
      "Iter-7100 train loss: 2.4585 valid loss: 2.5172, valid accuracy: 0.1231\n",
      "Iter-7200 train loss: 2.5003 valid loss: 2.5164, valid accuracy: 0.1231\n",
      "Iter-7300 train loss: 2.5771 valid loss: 2.5156, valid accuracy: 0.1254\n",
      "Iter-7400 train loss: 2.5010 valid loss: 2.5148, valid accuracy: 0.1262\n",
      "Iter-7500 train loss: 2.5099 valid loss: 2.5140, valid accuracy: 0.1262\n",
      "Iter-7600 train loss: 2.5177 valid loss: 2.5131, valid accuracy: 0.1277\n",
      "Iter-7700 train loss: 2.5465 valid loss: 2.5123, valid accuracy: 0.1277\n",
      "Iter-7800 train loss: 2.5116 valid loss: 2.5115, valid accuracy: 0.1277\n",
      "Iter-7900 train loss: 2.4708 valid loss: 2.5108, valid accuracy: 0.1269\n",
      "Iter-8000 train loss: 2.5009 valid loss: 2.5100, valid accuracy: 0.1269\n",
      "Iter-8100 train loss: 2.4835 valid loss: 2.5092, valid accuracy: 0.1285\n",
      "Iter-8200 train loss: 2.5286 valid loss: 2.5084, valid accuracy: 0.1292\n",
      "Iter-8300 train loss: 2.4641 valid loss: 2.5076, valid accuracy: 0.1308\n",
      "Iter-8400 train loss: 2.4690 valid loss: 2.5069, valid accuracy: 0.1308\n",
      "Iter-8500 train loss: 2.4319 valid loss: 2.5061, valid accuracy: 0.1308\n",
      "Iter-8600 train loss: 2.4679 valid loss: 2.5053, valid accuracy: 0.1315\n",
      "Iter-8700 train loss: 2.4819 valid loss: 2.5046, valid accuracy: 0.1315\n",
      "Iter-8800 train loss: 2.5000 valid loss: 2.5038, valid accuracy: 0.1308\n",
      "Iter-8900 train loss: 2.5547 valid loss: 2.5031, valid accuracy: 0.1315\n",
      "Iter-9000 train loss: 2.4334 valid loss: 2.5023, valid accuracy: 0.1315\n",
      "Iter-9100 train loss: 2.4812 valid loss: 2.5016, valid accuracy: 0.1315\n",
      "Iter-9200 train loss: 2.4742 valid loss: 2.5008, valid accuracy: 0.1315\n",
      "Iter-9300 train loss: 2.4921 valid loss: 2.5001, valid accuracy: 0.1315\n",
      "Iter-9400 train loss: 2.5177 valid loss: 2.4993, valid accuracy: 0.1315\n",
      "Iter-9500 train loss: 2.5144 valid loss: 2.4986, valid accuracy: 0.1308\n",
      "Iter-9600 train loss: 2.4316 valid loss: 2.4979, valid accuracy: 0.1315\n",
      "Iter-9700 train loss: 2.4828 valid loss: 2.4972, valid accuracy: 0.1315\n",
      "Iter-9800 train loss: 2.4562 valid loss: 2.4965, valid accuracy: 0.1323\n",
      "Iter-9900 train loss: 2.5139 valid loss: 2.4958, valid accuracy: 0.1323\n",
      "Iter-10000 train loss: 2.4603 valid loss: 2.4951, valid accuracy: 0.1331\n",
      "Iter-10100 train loss: 2.4941 valid loss: 2.4943, valid accuracy: 0.1331\n",
      "Iter-10200 train loss: 2.4256 valid loss: 2.4936, valid accuracy: 0.1338\n",
      "Iter-10300 train loss: 2.5293 valid loss: 2.4929, valid accuracy: 0.1338\n",
      "Iter-10400 train loss: 2.4977 valid loss: 2.4922, valid accuracy: 0.1354\n",
      "Iter-10500 train loss: 2.5301 valid loss: 2.4916, valid accuracy: 0.1354\n",
      "Iter-10600 train loss: 2.4481 valid loss: 2.4909, valid accuracy: 0.1346\n",
      "Iter-10700 train loss: 2.4977 valid loss: 2.4902, valid accuracy: 0.1354\n",
      "Iter-10800 train loss: 2.4956 valid loss: 2.4895, valid accuracy: 0.1346\n",
      "Iter-10900 train loss: 2.4577 valid loss: 2.4889, valid accuracy: 0.1354\n",
      "Iter-11000 train loss: 2.4834 valid loss: 2.4882, valid accuracy: 0.1362\n",
      "Iter-11100 train loss: 2.4869 valid loss: 2.4875, valid accuracy: 0.1369\n",
      "Iter-11200 train loss: 2.4639 valid loss: 2.4868, valid accuracy: 0.1369\n",
      "Iter-11300 train loss: 2.4842 valid loss: 2.4861, valid accuracy: 0.1369\n",
      "Iter-11400 train loss: 2.5035 valid loss: 2.4855, valid accuracy: 0.1369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.4837 valid loss: 2.4849, valid accuracy: 0.1400\n",
      "Iter-11600 train loss: 2.5039 valid loss: 2.4843, valid accuracy: 0.1400\n",
      "Iter-11700 train loss: 2.5044 valid loss: 2.4836, valid accuracy: 0.1400\n",
      "Iter-11800 train loss: 2.4273 valid loss: 2.4829, valid accuracy: 0.1408\n",
      "Iter-11900 train loss: 2.4457 valid loss: 2.4822, valid accuracy: 0.1408\n",
      "Iter-12000 train loss: 2.4171 valid loss: 2.4816, valid accuracy: 0.1408\n",
      "Iter-12100 train loss: 2.4229 valid loss: 2.4810, valid accuracy: 0.1408\n",
      "Iter-12200 train loss: 2.4975 valid loss: 2.4804, valid accuracy: 0.1408\n",
      "Iter-12300 train loss: 2.4448 valid loss: 2.4798, valid accuracy: 0.1423\n",
      "Iter-12400 train loss: 2.4630 valid loss: 2.4791, valid accuracy: 0.1415\n",
      "Iter-12500 train loss: 2.4874 valid loss: 2.4785, valid accuracy: 0.1431\n",
      "Iter-12600 train loss: 2.5110 valid loss: 2.4779, valid accuracy: 0.1438\n",
      "Iter-12700 train loss: 2.4943 valid loss: 2.4773, valid accuracy: 0.1423\n",
      "Iter-12800 train loss: 2.4683 valid loss: 2.4767, valid accuracy: 0.1438\n",
      "Iter-12900 train loss: 2.4787 valid loss: 2.4761, valid accuracy: 0.1438\n",
      "Iter-13000 train loss: 2.4968 valid loss: 2.4754, valid accuracy: 0.1438\n",
      "Iter-13100 train loss: 2.4662 valid loss: 2.4748, valid accuracy: 0.1446\n",
      "Iter-13200 train loss: 2.4598 valid loss: 2.4742, valid accuracy: 0.1446\n",
      "Iter-13300 train loss: 2.4263 valid loss: 2.4736, valid accuracy: 0.1438\n",
      "Iter-13400 train loss: 2.4940 valid loss: 2.4730, valid accuracy: 0.1454\n",
      "Iter-13500 train loss: 2.4637 valid loss: 2.4724, valid accuracy: 0.1462\n",
      "Iter-13600 train loss: 2.4460 valid loss: 2.4718, valid accuracy: 0.1485\n",
      "Iter-13700 train loss: 2.4158 valid loss: 2.4712, valid accuracy: 0.1485\n",
      "Iter-13800 train loss: 2.5234 valid loss: 2.4706, valid accuracy: 0.1492\n",
      "Iter-13900 train loss: 2.5090 valid loss: 2.4700, valid accuracy: 0.1492\n",
      "Iter-14000 train loss: 2.4692 valid loss: 2.4695, valid accuracy: 0.1492\n",
      "Iter-14100 train loss: 2.4654 valid loss: 2.4688, valid accuracy: 0.1500\n",
      "Iter-14200 train loss: 2.4801 valid loss: 2.4683, valid accuracy: 0.1492\n",
      "Iter-14300 train loss: 2.3644 valid loss: 2.4677, valid accuracy: 0.1500\n",
      "Iter-14400 train loss: 2.4171 valid loss: 2.4671, valid accuracy: 0.1500\n",
      "Iter-14500 train loss: 2.4525 valid loss: 2.4665, valid accuracy: 0.1508\n",
      "Iter-14600 train loss: 2.4384 valid loss: 2.4660, valid accuracy: 0.1500\n",
      "Iter-14700 train loss: 2.4472 valid loss: 2.4654, valid accuracy: 0.1523\n",
      "Iter-14800 train loss: 2.5243 valid loss: 2.4649, valid accuracy: 0.1515\n",
      "Iter-14900 train loss: 2.4957 valid loss: 2.4643, valid accuracy: 0.1523\n",
      "Iter-15000 train loss: 2.4135 valid loss: 2.4637, valid accuracy: 0.1523\n",
      "Iter-15100 train loss: 2.4912 valid loss: 2.4632, valid accuracy: 0.1515\n",
      "Iter-15200 train loss: 2.4651 valid loss: 2.4627, valid accuracy: 0.1523\n",
      "Iter-15300 train loss: 2.4227 valid loss: 2.4621, valid accuracy: 0.1531\n",
      "Iter-15400 train loss: 2.3712 valid loss: 2.4615, valid accuracy: 0.1531\n",
      "Iter-15500 train loss: 2.4606 valid loss: 2.4610, valid accuracy: 0.1546\n",
      "Iter-15600 train loss: 2.4475 valid loss: 2.4605, valid accuracy: 0.1538\n",
      "Iter-15700 train loss: 2.4372 valid loss: 2.4599, valid accuracy: 0.1546\n",
      "Iter-15800 train loss: 2.4621 valid loss: 2.4593, valid accuracy: 0.1546\n",
      "Iter-15900 train loss: 2.4909 valid loss: 2.4588, valid accuracy: 0.1546\n",
      "Iter-16000 train loss: 2.4469 valid loss: 2.4583, valid accuracy: 0.1562\n",
      "Iter-16100 train loss: 2.4324 valid loss: 2.4577, valid accuracy: 0.1577\n",
      "Iter-16200 train loss: 2.4491 valid loss: 2.4572, valid accuracy: 0.1585\n",
      "Iter-16300 train loss: 2.4158 valid loss: 2.4566, valid accuracy: 0.1577\n",
      "Iter-16400 train loss: 2.4296 valid loss: 2.4561, valid accuracy: 0.1569\n",
      "Iter-16500 train loss: 2.3798 valid loss: 2.4555, valid accuracy: 0.1577\n",
      "Iter-16600 train loss: 2.4248 valid loss: 2.4550, valid accuracy: 0.1592\n",
      "Iter-16700 train loss: 2.4016 valid loss: 2.4545, valid accuracy: 0.1592\n",
      "Iter-16800 train loss: 2.4072 valid loss: 2.4540, valid accuracy: 0.1585\n",
      "Iter-16900 train loss: 2.4923 valid loss: 2.4534, valid accuracy: 0.1600\n",
      "Iter-17000 train loss: 2.4600 valid loss: 2.4529, valid accuracy: 0.1592\n",
      "Iter-17100 train loss: 2.4693 valid loss: 2.4524, valid accuracy: 0.1592\n",
      "Iter-17200 train loss: 2.3502 valid loss: 2.4519, valid accuracy: 0.1592\n",
      "Iter-17300 train loss: 2.3970 valid loss: 2.4514, valid accuracy: 0.1592\n",
      "Iter-17400 train loss: 2.3865 valid loss: 2.4509, valid accuracy: 0.1592\n",
      "Iter-17500 train loss: 2.4132 valid loss: 2.4504, valid accuracy: 0.1592\n",
      "Iter-17600 train loss: 2.4554 valid loss: 2.4499, valid accuracy: 0.1600\n",
      "Iter-17700 train loss: 2.4254 valid loss: 2.4494, valid accuracy: 0.1600\n",
      "Iter-17800 train loss: 2.4837 valid loss: 2.4489, valid accuracy: 0.1600\n",
      "Iter-17900 train loss: 2.4135 valid loss: 2.4484, valid accuracy: 0.1600\n",
      "Iter-18000 train loss: 2.3805 valid loss: 2.4479, valid accuracy: 0.1608\n",
      "Iter-18100 train loss: 2.4618 valid loss: 2.4474, valid accuracy: 0.1623\n",
      "Iter-18200 train loss: 2.4048 valid loss: 2.4469, valid accuracy: 0.1623\n",
      "Iter-18300 train loss: 2.4049 valid loss: 2.4464, valid accuracy: 0.1631\n",
      "Iter-18400 train loss: 2.3677 valid loss: 2.4459, valid accuracy: 0.1615\n",
      "Iter-18500 train loss: 2.4455 valid loss: 2.4455, valid accuracy: 0.1608\n",
      "Iter-18600 train loss: 2.3691 valid loss: 2.4450, valid accuracy: 0.1615\n",
      "Iter-18700 train loss: 2.4045 valid loss: 2.4445, valid accuracy: 0.1615\n",
      "Iter-18800 train loss: 2.3429 valid loss: 2.4440, valid accuracy: 0.1615\n",
      "Iter-18900 train loss: 2.4007 valid loss: 2.4435, valid accuracy: 0.1608\n",
      "Iter-19000 train loss: 2.3657 valid loss: 2.4430, valid accuracy: 0.1608\n",
      "Iter-19100 train loss: 2.3873 valid loss: 2.4426, valid accuracy: 0.1623\n",
      "Iter-19200 train loss: 2.3717 valid loss: 2.4421, valid accuracy: 0.1623\n",
      "Iter-19300 train loss: 2.4671 valid loss: 2.4416, valid accuracy: 0.1638\n",
      "Iter-19400 train loss: 2.4701 valid loss: 2.4412, valid accuracy: 0.1646\n",
      "Iter-19500 train loss: 2.3558 valid loss: 2.4407, valid accuracy: 0.1646\n",
      "Iter-19600 train loss: 2.5050 valid loss: 2.4402, valid accuracy: 0.1638\n",
      "Iter-19700 train loss: 2.4131 valid loss: 2.4397, valid accuracy: 0.1638\n",
      "Iter-19800 train loss: 2.4272 valid loss: 2.4392, valid accuracy: 0.1638\n",
      "Iter-19900 train loss: 2.3859 valid loss: 2.4388, valid accuracy: 0.1638\n",
      "Iter-20000 train loss: 2.4038 valid loss: 2.4384, valid accuracy: 0.1646\n",
      "Iter-20100 train loss: 2.4183 valid loss: 2.4379, valid accuracy: 0.1646\n",
      "Iter-20200 train loss: 2.4452 valid loss: 2.4374, valid accuracy: 0.1638\n",
      "Iter-20300 train loss: 2.3657 valid loss: 2.4370, valid accuracy: 0.1638\n",
      "Iter-20400 train loss: 2.4598 valid loss: 2.4365, valid accuracy: 0.1638\n",
      "Iter-20500 train loss: 2.4341 valid loss: 2.4361, valid accuracy: 0.1654\n",
      "Iter-20600 train loss: 2.4068 valid loss: 2.4356, valid accuracy: 0.1654\n",
      "Iter-20700 train loss: 2.3754 valid loss: 2.4352, valid accuracy: 0.1662\n",
      "Iter-20800 train loss: 2.4752 valid loss: 2.4348, valid accuracy: 0.1669\n",
      "Iter-20900 train loss: 2.4085 valid loss: 2.4343, valid accuracy: 0.1669\n",
      "Iter-21000 train loss: 2.4024 valid loss: 2.4339, valid accuracy: 0.1669\n",
      "Iter-21100 train loss: 2.3864 valid loss: 2.4334, valid accuracy: 0.1677\n",
      "Iter-21200 train loss: 2.3534 valid loss: 2.4330, valid accuracy: 0.1677\n",
      "Iter-21300 train loss: 2.3975 valid loss: 2.4325, valid accuracy: 0.1685\n",
      "Iter-21400 train loss: 2.3835 valid loss: 2.4320, valid accuracy: 0.1685\n",
      "Iter-21500 train loss: 2.4244 valid loss: 2.4316, valid accuracy: 0.1692\n",
      "Iter-21600 train loss: 2.3630 valid loss: 2.4311, valid accuracy: 0.1685\n",
      "Iter-21700 train loss: 2.3045 valid loss: 2.4307, valid accuracy: 0.1685\n",
      "Iter-21800 train loss: 2.4911 valid loss: 2.4303, valid accuracy: 0.1685\n",
      "Iter-21900 train loss: 2.5018 valid loss: 2.4298, valid accuracy: 0.1685\n",
      "Iter-22000 train loss: 2.4109 valid loss: 2.4294, valid accuracy: 0.1677\n",
      "Iter-22100 train loss: 2.3985 valid loss: 2.4289, valid accuracy: 0.1677\n",
      "Iter-22200 train loss: 2.4140 valid loss: 2.4285, valid accuracy: 0.1677\n",
      "Iter-22300 train loss: 2.3701 valid loss: 2.4281, valid accuracy: 0.1669\n",
      "Iter-22400 train loss: 2.4348 valid loss: 2.4277, valid accuracy: 0.1669\n",
      "Iter-22500 train loss: 2.3780 valid loss: 2.4273, valid accuracy: 0.1677\n",
      "Iter-22600 train loss: 2.4093 valid loss: 2.4269, valid accuracy: 0.1685\n",
      "Iter-22700 train loss: 2.4326 valid loss: 2.4264, valid accuracy: 0.1677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.4469 valid loss: 2.4261, valid accuracy: 0.1685\n",
      "Iter-22900 train loss: 2.3472 valid loss: 2.4256, valid accuracy: 0.1685\n",
      "Iter-23000 train loss: 2.4239 valid loss: 2.4252, valid accuracy: 0.1685\n",
      "Iter-23100 train loss: 2.3704 valid loss: 2.4247, valid accuracy: 0.1692\n",
      "Iter-23200 train loss: 2.4536 valid loss: 2.4243, valid accuracy: 0.1715\n",
      "Iter-23300 train loss: 2.3849 valid loss: 2.4239, valid accuracy: 0.1692\n",
      "Iter-23400 train loss: 2.3525 valid loss: 2.4235, valid accuracy: 0.1692\n",
      "Iter-23500 train loss: 2.4275 valid loss: 2.4231, valid accuracy: 0.1692\n",
      "Iter-23600 train loss: 2.4432 valid loss: 2.4227, valid accuracy: 0.1700\n",
      "Iter-23700 train loss: 2.3503 valid loss: 2.4223, valid accuracy: 0.1692\n",
      "Iter-23800 train loss: 2.4013 valid loss: 2.4218, valid accuracy: 0.1708\n",
      "Iter-23900 train loss: 2.3399 valid loss: 2.4214, valid accuracy: 0.1700\n",
      "Iter-24000 train loss: 2.4399 valid loss: 2.4210, valid accuracy: 0.1708\n",
      "Iter-24100 train loss: 2.3650 valid loss: 2.4207, valid accuracy: 0.1700\n",
      "Iter-24200 train loss: 2.4259 valid loss: 2.4203, valid accuracy: 0.1700\n",
      "Iter-24300 train loss: 2.3985 valid loss: 2.4199, valid accuracy: 0.1700\n",
      "Iter-24400 train loss: 2.3938 valid loss: 2.4195, valid accuracy: 0.1715\n",
      "Iter-24500 train loss: 2.3335 valid loss: 2.4192, valid accuracy: 0.1700\n",
      "Iter-24600 train loss: 2.4273 valid loss: 2.4187, valid accuracy: 0.1723\n",
      "Iter-24700 train loss: 2.3363 valid loss: 2.4184, valid accuracy: 0.1731\n",
      "Iter-24800 train loss: 2.3214 valid loss: 2.4180, valid accuracy: 0.1723\n",
      "Iter-24900 train loss: 2.4635 valid loss: 2.4176, valid accuracy: 0.1723\n",
      "Iter-25000 train loss: 2.4400 valid loss: 2.4172, valid accuracy: 0.1731\n",
      "Iter-25100 train loss: 2.4670 valid loss: 2.4168, valid accuracy: 0.1723\n",
      "Iter-25200 train loss: 2.3305 valid loss: 2.4164, valid accuracy: 0.1723\n",
      "Iter-25300 train loss: 2.4282 valid loss: 2.4160, valid accuracy: 0.1731\n",
      "Iter-25400 train loss: 2.3880 valid loss: 2.4157, valid accuracy: 0.1738\n",
      "Iter-25500 train loss: 2.4625 valid loss: 2.4153, valid accuracy: 0.1746\n",
      "Iter-25600 train loss: 2.3536 valid loss: 2.4149, valid accuracy: 0.1746\n",
      "Iter-25700 train loss: 2.3107 valid loss: 2.4145, valid accuracy: 0.1738\n",
      "Iter-25800 train loss: 2.3822 valid loss: 2.4141, valid accuracy: 0.1738\n",
      "Iter-25900 train loss: 2.4376 valid loss: 2.4137, valid accuracy: 0.1738\n",
      "Iter-26000 train loss: 2.3888 valid loss: 2.4134, valid accuracy: 0.1731\n",
      "Iter-26100 train loss: 2.4433 valid loss: 2.4129, valid accuracy: 0.1738\n",
      "Iter-26200 train loss: 2.4336 valid loss: 2.4126, valid accuracy: 0.1746\n",
      "Iter-26300 train loss: 2.3433 valid loss: 2.4122, valid accuracy: 0.1738\n",
      "Iter-26400 train loss: 2.3135 valid loss: 2.4118, valid accuracy: 0.1746\n",
      "Iter-26500 train loss: 2.3419 valid loss: 2.4114, valid accuracy: 0.1754\n",
      "Iter-26600 train loss: 2.3923 valid loss: 2.4111, valid accuracy: 0.1769\n",
      "Iter-26700 train loss: 2.4066 valid loss: 2.4107, valid accuracy: 0.1769\n",
      "Iter-26800 train loss: 2.3665 valid loss: 2.4103, valid accuracy: 0.1769\n",
      "Iter-26900 train loss: 2.3951 valid loss: 2.4099, valid accuracy: 0.1777\n",
      "Iter-27000 train loss: 2.3538 valid loss: 2.4096, valid accuracy: 0.1785\n",
      "Iter-27100 train loss: 2.4651 valid loss: 2.4092, valid accuracy: 0.1792\n",
      "Iter-27200 train loss: 2.4234 valid loss: 2.4089, valid accuracy: 0.1792\n",
      "Iter-27300 train loss: 2.4713 valid loss: 2.4085, valid accuracy: 0.1792\n",
      "Iter-27400 train loss: 2.4093 valid loss: 2.4082, valid accuracy: 0.1800\n",
      "Iter-27500 train loss: 2.3080 valid loss: 2.4079, valid accuracy: 0.1800\n",
      "Iter-27600 train loss: 2.3143 valid loss: 2.4076, valid accuracy: 0.1800\n",
      "Iter-27700 train loss: 2.4095 valid loss: 2.4072, valid accuracy: 0.1792\n",
      "Iter-27800 train loss: 2.3619 valid loss: 2.4068, valid accuracy: 0.1792\n",
      "Iter-27900 train loss: 2.3342 valid loss: 2.4065, valid accuracy: 0.1785\n",
      "Iter-28000 train loss: 2.3355 valid loss: 2.4061, valid accuracy: 0.1792\n",
      "Iter-28100 train loss: 2.4431 valid loss: 2.4058, valid accuracy: 0.1792\n",
      "Iter-28200 train loss: 2.3408 valid loss: 2.4054, valid accuracy: 0.1800\n",
      "Iter-28300 train loss: 2.4440 valid loss: 2.4051, valid accuracy: 0.1800\n",
      "Iter-28400 train loss: 2.3733 valid loss: 2.4048, valid accuracy: 0.1800\n",
      "Iter-28500 train loss: 2.4680 valid loss: 2.4044, valid accuracy: 0.1800\n",
      "Iter-28600 train loss: 2.3774 valid loss: 2.4041, valid accuracy: 0.1800\n",
      "Iter-28700 train loss: 2.4018 valid loss: 2.4038, valid accuracy: 0.1808\n",
      "Iter-28800 train loss: 2.3951 valid loss: 2.4035, valid accuracy: 0.1800\n",
      "Iter-28900 train loss: 2.3649 valid loss: 2.4031, valid accuracy: 0.1800\n",
      "Iter-29000 train loss: 2.3586 valid loss: 2.4028, valid accuracy: 0.1792\n",
      "Iter-29100 train loss: 2.4362 valid loss: 2.4024, valid accuracy: 0.1792\n",
      "Iter-29200 train loss: 2.4220 valid loss: 2.4021, valid accuracy: 0.1792\n",
      "Iter-29300 train loss: 2.2478 valid loss: 2.4017, valid accuracy: 0.1792\n",
      "Iter-29400 train loss: 2.4732 valid loss: 2.4014, valid accuracy: 0.1792\n",
      "Iter-29500 train loss: 2.4617 valid loss: 2.4011, valid accuracy: 0.1792\n",
      "Iter-29600 train loss: 2.3618 valid loss: 2.4008, valid accuracy: 0.1808\n",
      "Iter-29700 train loss: 2.3382 valid loss: 2.4005, valid accuracy: 0.1815\n",
      "Iter-29800 train loss: 2.4490 valid loss: 2.4001, valid accuracy: 0.1815\n",
      "Iter-29900 train loss: 2.3993 valid loss: 2.3998, valid accuracy: 0.1815\n",
      "Iter-30000 train loss: 2.3686 valid loss: 2.3995, valid accuracy: 0.1815\n",
      "Iter-30100 train loss: 2.4657 valid loss: 2.3992, valid accuracy: 0.1808\n",
      "Iter-30200 train loss: 2.3485 valid loss: 2.3988, valid accuracy: 0.1808\n",
      "Iter-30300 train loss: 2.3598 valid loss: 2.3986, valid accuracy: 0.1808\n",
      "Iter-30400 train loss: 2.3604 valid loss: 2.3982, valid accuracy: 0.1808\n",
      "Iter-30500 train loss: 2.4845 valid loss: 2.3979, valid accuracy: 0.1808\n",
      "Iter-30600 train loss: 2.3591 valid loss: 2.3976, valid accuracy: 0.1808\n",
      "Iter-30700 train loss: 2.3808 valid loss: 2.3972, valid accuracy: 0.1808\n",
      "Iter-30800 train loss: 2.4656 valid loss: 2.3969, valid accuracy: 0.1808\n",
      "Iter-30900 train loss: 2.3216 valid loss: 2.3966, valid accuracy: 0.1815\n",
      "Iter-31000 train loss: 2.2981 valid loss: 2.3962, valid accuracy: 0.1823\n",
      "Iter-31100 train loss: 2.4533 valid loss: 2.3959, valid accuracy: 0.1831\n",
      "Iter-31200 train loss: 2.4551 valid loss: 2.3957, valid accuracy: 0.1838\n",
      "Iter-31300 train loss: 2.3591 valid loss: 2.3953, valid accuracy: 0.1846\n",
      "Iter-31400 train loss: 2.3849 valid loss: 2.3950, valid accuracy: 0.1846\n",
      "Iter-31500 train loss: 2.4577 valid loss: 2.3946, valid accuracy: 0.1854\n",
      "Iter-31600 train loss: 2.3594 valid loss: 2.3944, valid accuracy: 0.1862\n",
      "Iter-31700 train loss: 2.3293 valid loss: 2.3940, valid accuracy: 0.1862\n",
      "Iter-31800 train loss: 2.3546 valid loss: 2.3937, valid accuracy: 0.1862\n",
      "Iter-31900 train loss: 2.3086 valid loss: 2.3933, valid accuracy: 0.1869\n",
      "Iter-32000 train loss: 2.3186 valid loss: 2.3930, valid accuracy: 0.1869\n",
      "Iter-32100 train loss: 2.3819 valid loss: 2.3927, valid accuracy: 0.1877\n",
      "Iter-32200 train loss: 2.3853 valid loss: 2.3924, valid accuracy: 0.1877\n",
      "Iter-32300 train loss: 2.5001 valid loss: 2.3921, valid accuracy: 0.1877\n",
      "Iter-32400 train loss: 2.4504 valid loss: 2.3918, valid accuracy: 0.1877\n",
      "Iter-32500 train loss: 2.3927 valid loss: 2.3915, valid accuracy: 0.1877\n",
      "Iter-32600 train loss: 2.3505 valid loss: 2.3912, valid accuracy: 0.1877\n",
      "Iter-32700 train loss: 2.4544 valid loss: 2.3909, valid accuracy: 0.1885\n",
      "Iter-32800 train loss: 2.3104 valid loss: 2.3906, valid accuracy: 0.1885\n",
      "Iter-32900 train loss: 2.3237 valid loss: 2.3903, valid accuracy: 0.1885\n",
      "Iter-33000 train loss: 2.4285 valid loss: 2.3900, valid accuracy: 0.1877\n",
      "Iter-33100 train loss: 2.3452 valid loss: 2.3897, valid accuracy: 0.1892\n",
      "Iter-33200 train loss: 2.4342 valid loss: 2.3894, valid accuracy: 0.1892\n",
      "Iter-33300 train loss: 2.3477 valid loss: 2.3891, valid accuracy: 0.1892\n",
      "Iter-33400 train loss: 2.3355 valid loss: 2.3888, valid accuracy: 0.1908\n",
      "Iter-33500 train loss: 2.4851 valid loss: 2.3885, valid accuracy: 0.1900\n",
      "Iter-33600 train loss: 2.3883 valid loss: 2.3882, valid accuracy: 0.1908\n",
      "Iter-33700 train loss: 2.3968 valid loss: 2.3879, valid accuracy: 0.1908\n",
      "Iter-33800 train loss: 2.3497 valid loss: 2.3876, valid accuracy: 0.1908\n",
      "Iter-33900 train loss: 2.3746 valid loss: 2.3873, valid accuracy: 0.1915\n",
      "Iter-34000 train loss: 2.3310 valid loss: 2.3870, valid accuracy: 0.1915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.3192 valid loss: 2.3867, valid accuracy: 0.1915\n",
      "Iter-34200 train loss: 2.4264 valid loss: 2.3864, valid accuracy: 0.1915\n",
      "Iter-34300 train loss: 2.3511 valid loss: 2.3861, valid accuracy: 0.1915\n",
      "Iter-34400 train loss: 2.3253 valid loss: 2.3858, valid accuracy: 0.1923\n",
      "Iter-34500 train loss: 2.2637 valid loss: 2.3855, valid accuracy: 0.1923\n",
      "Iter-34600 train loss: 2.3113 valid loss: 2.3852, valid accuracy: 0.1923\n",
      "Iter-34700 train loss: 2.3570 valid loss: 2.3849, valid accuracy: 0.1908\n",
      "Iter-34800 train loss: 2.3450 valid loss: 2.3846, valid accuracy: 0.1923\n",
      "Iter-34900 train loss: 2.4201 valid loss: 2.3843, valid accuracy: 0.1923\n",
      "Iter-35000 train loss: 2.2871 valid loss: 2.3841, valid accuracy: 0.1923\n",
      "Iter-35100 train loss: 2.4766 valid loss: 2.3838, valid accuracy: 0.1923\n",
      "Iter-35200 train loss: 2.2940 valid loss: 2.3835, valid accuracy: 0.1931\n",
      "Iter-35300 train loss: 2.3423 valid loss: 2.3831, valid accuracy: 0.1938\n",
      "Iter-35400 train loss: 2.3931 valid loss: 2.3828, valid accuracy: 0.1946\n",
      "Iter-35500 train loss: 2.3244 valid loss: 2.3825, valid accuracy: 0.1954\n",
      "Iter-35600 train loss: 2.4398 valid loss: 2.3823, valid accuracy: 0.1946\n",
      "Iter-35700 train loss: 2.2995 valid loss: 2.3820, valid accuracy: 0.1954\n",
      "Iter-35800 train loss: 2.3006 valid loss: 2.3817, valid accuracy: 0.1954\n",
      "Iter-35900 train loss: 2.3200 valid loss: 2.3814, valid accuracy: 0.1954\n",
      "Iter-36000 train loss: 2.3737 valid loss: 2.3811, valid accuracy: 0.1954\n",
      "Iter-36100 train loss: 2.4348 valid loss: 2.3808, valid accuracy: 0.1954\n",
      "Iter-36200 train loss: 2.3351 valid loss: 2.3806, valid accuracy: 0.1946\n",
      "Iter-36300 train loss: 2.2668 valid loss: 2.3803, valid accuracy: 0.1946\n",
      "Iter-36400 train loss: 2.2582 valid loss: 2.3800, valid accuracy: 0.1962\n",
      "Iter-36500 train loss: 2.3022 valid loss: 2.3798, valid accuracy: 0.1946\n",
      "Iter-36600 train loss: 2.3809 valid loss: 2.3795, valid accuracy: 0.1954\n",
      "Iter-36700 train loss: 2.3717 valid loss: 2.3792, valid accuracy: 0.1954\n",
      "Iter-36800 train loss: 2.3276 valid loss: 2.3789, valid accuracy: 0.1962\n",
      "Iter-36900 train loss: 2.4303 valid loss: 2.3786, valid accuracy: 0.1962\n",
      "Iter-37000 train loss: 2.4078 valid loss: 2.3783, valid accuracy: 0.1969\n",
      "Iter-37100 train loss: 2.3655 valid loss: 2.3780, valid accuracy: 0.1969\n",
      "Iter-37200 train loss: 2.3527 valid loss: 2.3778, valid accuracy: 0.1962\n",
      "Iter-37300 train loss: 2.3809 valid loss: 2.3775, valid accuracy: 0.1962\n",
      "Iter-37400 train loss: 2.4265 valid loss: 2.3773, valid accuracy: 0.1962\n",
      "Iter-37500 train loss: 2.5249 valid loss: 2.3770, valid accuracy: 0.1962\n",
      "Iter-37600 train loss: 2.5032 valid loss: 2.3767, valid accuracy: 0.1962\n",
      "Iter-37700 train loss: 2.4061 valid loss: 2.3764, valid accuracy: 0.1962\n",
      "Iter-37800 train loss: 2.2538 valid loss: 2.3761, valid accuracy: 0.1954\n",
      "Iter-37900 train loss: 2.3739 valid loss: 2.3759, valid accuracy: 0.1962\n",
      "Iter-38000 train loss: 2.2644 valid loss: 2.3756, valid accuracy: 0.1977\n",
      "Iter-38100 train loss: 2.3233 valid loss: 2.3754, valid accuracy: 0.1977\n",
      "Iter-38200 train loss: 2.3072 valid loss: 2.3751, valid accuracy: 0.1977\n",
      "Iter-38300 train loss: 2.3674 valid loss: 2.3748, valid accuracy: 0.1977\n",
      "Iter-38400 train loss: 2.3066 valid loss: 2.3746, valid accuracy: 0.1977\n",
      "Iter-38500 train loss: 2.3432 valid loss: 2.3743, valid accuracy: 0.1969\n",
      "Iter-38600 train loss: 2.3220 valid loss: 2.3741, valid accuracy: 0.1969\n",
      "Iter-38700 train loss: 2.4353 valid loss: 2.3738, valid accuracy: 0.1969\n",
      "Iter-38800 train loss: 2.4112 valid loss: 2.3735, valid accuracy: 0.1969\n",
      "Iter-38900 train loss: 2.4480 valid loss: 2.3733, valid accuracy: 0.1962\n",
      "Iter-39000 train loss: 2.4038 valid loss: 2.3730, valid accuracy: 0.1962\n",
      "Iter-39100 train loss: 2.2870 valid loss: 2.3727, valid accuracy: 0.1962\n",
      "Iter-39200 train loss: 2.3609 valid loss: 2.3725, valid accuracy: 0.1962\n",
      "Iter-39300 train loss: 2.4813 valid loss: 2.3722, valid accuracy: 0.1962\n",
      "Iter-39400 train loss: 2.3335 valid loss: 2.3719, valid accuracy: 0.1962\n",
      "Iter-39500 train loss: 2.4535 valid loss: 2.3716, valid accuracy: 0.1954\n",
      "Iter-39600 train loss: 2.3443 valid loss: 2.3714, valid accuracy: 0.1954\n",
      "Iter-39700 train loss: 2.4925 valid loss: 2.3711, valid accuracy: 0.1962\n",
      "Iter-39800 train loss: 2.3577 valid loss: 2.3708, valid accuracy: 0.1969\n",
      "Iter-39900 train loss: 2.3113 valid loss: 2.3706, valid accuracy: 0.1969\n",
      "Iter-40000 train loss: 2.3052 valid loss: 2.3703, valid accuracy: 0.1969\n",
      "Iter-40100 train loss: 2.3800 valid loss: 2.3701, valid accuracy: 0.1992\n",
      "Iter-40200 train loss: 2.3431 valid loss: 2.3699, valid accuracy: 0.1985\n",
      "Iter-40300 train loss: 2.3861 valid loss: 2.3696, valid accuracy: 0.1985\n",
      "Iter-40400 train loss: 2.3073 valid loss: 2.3694, valid accuracy: 0.1969\n",
      "Iter-40500 train loss: 2.2883 valid loss: 2.3692, valid accuracy: 0.1977\n",
      "Iter-40600 train loss: 2.3763 valid loss: 2.3689, valid accuracy: 0.1977\n",
      "Iter-40700 train loss: 2.3700 valid loss: 2.3687, valid accuracy: 0.1985\n",
      "Iter-40800 train loss: 2.3186 valid loss: 2.3684, valid accuracy: 0.1985\n",
      "Iter-40900 train loss: 2.3900 valid loss: 2.3682, valid accuracy: 0.1977\n",
      "Iter-41000 train loss: 2.3404 valid loss: 2.3679, valid accuracy: 0.1969\n",
      "Iter-41100 train loss: 2.2805 valid loss: 2.3677, valid accuracy: 0.1969\n",
      "Iter-41200 train loss: 2.3734 valid loss: 2.3675, valid accuracy: 0.1969\n",
      "Iter-41300 train loss: 2.2899 valid loss: 2.3672, valid accuracy: 0.1969\n",
      "Iter-41400 train loss: 2.2457 valid loss: 2.3670, valid accuracy: 0.1969\n",
      "Iter-41500 train loss: 2.3360 valid loss: 2.3667, valid accuracy: 0.1985\n",
      "Iter-41600 train loss: 2.2588 valid loss: 2.3665, valid accuracy: 0.1985\n",
      "Iter-41700 train loss: 2.2506 valid loss: 2.3663, valid accuracy: 0.1985\n",
      "Iter-41800 train loss: 2.4017 valid loss: 2.3660, valid accuracy: 0.1977\n",
      "Iter-41900 train loss: 2.4281 valid loss: 2.3657, valid accuracy: 0.1977\n",
      "Iter-42000 train loss: 2.3671 valid loss: 2.3655, valid accuracy: 0.1977\n",
      "Iter-42100 train loss: 2.1849 valid loss: 2.3653, valid accuracy: 0.1977\n",
      "Iter-42200 train loss: 2.4761 valid loss: 2.3650, valid accuracy: 0.1977\n",
      "Iter-42300 train loss: 2.3794 valid loss: 2.3648, valid accuracy: 0.1985\n",
      "Iter-42400 train loss: 2.2939 valid loss: 2.3645, valid accuracy: 0.1977\n",
      "Iter-42500 train loss: 2.4318 valid loss: 2.3642, valid accuracy: 0.1977\n",
      "Iter-42600 train loss: 2.4732 valid loss: 2.3639, valid accuracy: 0.1977\n",
      "Iter-42700 train loss: 2.2822 valid loss: 2.3637, valid accuracy: 0.1985\n",
      "Iter-42800 train loss: 2.2964 valid loss: 2.3635, valid accuracy: 0.1985\n",
      "Iter-42900 train loss: 2.3915 valid loss: 2.3632, valid accuracy: 0.1977\n",
      "Iter-43000 train loss: 2.2881 valid loss: 2.3630, valid accuracy: 0.1985\n",
      "Iter-43100 train loss: 2.2943 valid loss: 2.3628, valid accuracy: 0.1985\n",
      "Iter-43200 train loss: 2.3805 valid loss: 2.3625, valid accuracy: 0.1992\n",
      "Iter-43300 train loss: 2.1744 valid loss: 2.3623, valid accuracy: 0.2000\n",
      "Iter-43400 train loss: 2.3013 valid loss: 2.3621, valid accuracy: 0.1992\n",
      "Iter-43500 train loss: 2.3890 valid loss: 2.3619, valid accuracy: 0.1992\n",
      "Iter-43600 train loss: 2.3869 valid loss: 2.3617, valid accuracy: 0.1992\n",
      "Iter-43700 train loss: 2.4083 valid loss: 2.3615, valid accuracy: 0.1985\n",
      "Iter-43800 train loss: 2.4831 valid loss: 2.3612, valid accuracy: 0.1985\n",
      "Iter-43900 train loss: 2.4436 valid loss: 2.3610, valid accuracy: 0.1992\n",
      "Iter-44000 train loss: 2.3790 valid loss: 2.3607, valid accuracy: 0.2000\n",
      "Iter-44100 train loss: 2.2776 valid loss: 2.3604, valid accuracy: 0.2000\n",
      "Iter-44200 train loss: 2.3506 valid loss: 2.3602, valid accuracy: 0.1992\n",
      "Iter-44300 train loss: 2.3129 valid loss: 2.3600, valid accuracy: 0.1992\n",
      "Iter-44400 train loss: 2.3801 valid loss: 2.3597, valid accuracy: 0.1985\n",
      "Iter-44500 train loss: 2.4037 valid loss: 2.3595, valid accuracy: 0.1992\n",
      "Iter-44600 train loss: 2.3766 valid loss: 2.3593, valid accuracy: 0.1992\n",
      "Iter-44700 train loss: 2.2701 valid loss: 2.3590, valid accuracy: 0.1992\n",
      "Iter-44800 train loss: 2.2943 valid loss: 2.3588, valid accuracy: 0.2000\n",
      "Iter-44900 train loss: 2.2781 valid loss: 2.3586, valid accuracy: 0.2000\n",
      "Iter-45000 train loss: 2.3220 valid loss: 2.3583, valid accuracy: 0.2000\n",
      "Iter-45100 train loss: 2.4237 valid loss: 2.3581, valid accuracy: 0.2000\n",
      "Iter-45200 train loss: 2.4173 valid loss: 2.3578, valid accuracy: 0.2008\n",
      "Iter-45300 train loss: 2.3802 valid loss: 2.3577, valid accuracy: 0.2008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 2.3055 valid loss: 2.3574, valid accuracy: 0.2008\n",
      "Iter-45500 train loss: 2.4145 valid loss: 2.3572, valid accuracy: 0.2008\n",
      "Iter-45600 train loss: 2.4828 valid loss: 2.3570, valid accuracy: 0.2008\n",
      "Iter-45700 train loss: 2.3777 valid loss: 2.3568, valid accuracy: 0.2008\n",
      "Iter-45800 train loss: 2.2684 valid loss: 2.3565, valid accuracy: 0.2000\n",
      "Iter-45900 train loss: 2.3032 valid loss: 2.3562, valid accuracy: 0.2000\n",
      "Iter-46000 train loss: 2.2667 valid loss: 2.3560, valid accuracy: 0.2000\n",
      "Iter-46100 train loss: 2.4043 valid loss: 2.3558, valid accuracy: 0.2000\n",
      "Iter-46200 train loss: 2.3697 valid loss: 2.3556, valid accuracy: 0.2000\n",
      "Iter-46300 train loss: 2.2583 valid loss: 2.3553, valid accuracy: 0.2000\n",
      "Iter-46400 train loss: 2.4438 valid loss: 2.3551, valid accuracy: 0.2000\n",
      "Iter-46500 train loss: 2.1363 valid loss: 2.3549, valid accuracy: 0.2000\n",
      "Iter-46600 train loss: 2.2939 valid loss: 2.3547, valid accuracy: 0.2000\n",
      "Iter-46700 train loss: 2.3323 valid loss: 2.3545, valid accuracy: 0.2000\n",
      "Iter-46800 train loss: 2.4136 valid loss: 2.3543, valid accuracy: 0.2000\n",
      "Iter-46900 train loss: 2.2660 valid loss: 2.3541, valid accuracy: 0.2000\n",
      "Iter-47000 train loss: 2.4313 valid loss: 2.3539, valid accuracy: 0.2000\n",
      "Iter-47100 train loss: 2.3636 valid loss: 2.3537, valid accuracy: 0.2000\n",
      "Iter-47200 train loss: 2.3424 valid loss: 2.3535, valid accuracy: 0.2000\n",
      "Iter-47300 train loss: 2.2644 valid loss: 2.3533, valid accuracy: 0.2000\n",
      "Iter-47400 train loss: 2.3002 valid loss: 2.3531, valid accuracy: 0.2000\n",
      "Iter-47500 train loss: 2.3533 valid loss: 2.3528, valid accuracy: 0.2008\n",
      "Iter-47600 train loss: 2.3740 valid loss: 2.3526, valid accuracy: 0.2000\n",
      "Iter-47700 train loss: 2.2404 valid loss: 2.3524, valid accuracy: 0.2000\n",
      "Iter-47800 train loss: 2.2627 valid loss: 2.3522, valid accuracy: 0.2000\n",
      "Iter-47900 train loss: 2.3817 valid loss: 2.3520, valid accuracy: 0.2008\n",
      "Iter-48000 train loss: 2.4619 valid loss: 2.3517, valid accuracy: 0.2008\n",
      "Iter-48100 train loss: 2.1632 valid loss: 2.3516, valid accuracy: 0.2015\n",
      "Iter-48200 train loss: 2.3732 valid loss: 2.3513, valid accuracy: 0.2015\n",
      "Iter-48300 train loss: 2.3265 valid loss: 2.3511, valid accuracy: 0.2015\n",
      "Iter-48400 train loss: 2.1774 valid loss: 2.3509, valid accuracy: 0.2023\n",
      "Iter-48500 train loss: 2.3399 valid loss: 2.3507, valid accuracy: 0.2023\n",
      "Iter-48600 train loss: 2.1936 valid loss: 2.3505, valid accuracy: 0.2015\n",
      "Iter-48700 train loss: 2.3880 valid loss: 2.3503, valid accuracy: 0.2008\n",
      "Iter-48800 train loss: 2.4675 valid loss: 2.3500, valid accuracy: 0.2015\n",
      "Iter-48900 train loss: 2.2644 valid loss: 2.3499, valid accuracy: 0.2015\n",
      "Iter-49000 train loss: 2.3440 valid loss: 2.3497, valid accuracy: 0.2008\n",
      "Iter-49100 train loss: 2.4266 valid loss: 2.3495, valid accuracy: 0.2015\n",
      "Iter-49200 train loss: 2.2118 valid loss: 2.3492, valid accuracy: 0.2031\n",
      "Iter-49300 train loss: 2.4101 valid loss: 2.3490, valid accuracy: 0.2031\n",
      "Iter-49400 train loss: 2.3139 valid loss: 2.3488, valid accuracy: 0.2031\n",
      "Iter-49500 train loss: 2.3186 valid loss: 2.3486, valid accuracy: 0.2031\n",
      "Iter-49600 train loss: 2.4034 valid loss: 2.3484, valid accuracy: 0.2015\n",
      "Iter-49700 train loss: 2.4138 valid loss: 2.3482, valid accuracy: 0.2023\n",
      "Iter-49800 train loss: 2.2571 valid loss: 2.3480, valid accuracy: 0.2008\n",
      "Iter-49900 train loss: 2.3854 valid loss: 2.3478, valid accuracy: 0.2023\n",
      "Iter-50000 train loss: 2.1378 valid loss: 2.3476, valid accuracy: 0.2031\n",
      "Iter-50100 train loss: 2.4662 valid loss: 2.3474, valid accuracy: 0.2023\n",
      "Iter-50200 train loss: 2.3252 valid loss: 2.3471, valid accuracy: 0.2023\n",
      "Iter-50300 train loss: 2.2398 valid loss: 2.3469, valid accuracy: 0.2023\n",
      "Iter-50400 train loss: 2.1189 valid loss: 2.3467, valid accuracy: 0.2031\n",
      "Iter-50500 train loss: 2.2285 valid loss: 2.3465, valid accuracy: 0.2046\n",
      "Iter-50600 train loss: 2.3696 valid loss: 2.3462, valid accuracy: 0.2054\n",
      "Iter-50700 train loss: 2.2800 valid loss: 2.3460, valid accuracy: 0.2046\n",
      "Iter-50800 train loss: 2.3700 valid loss: 2.3458, valid accuracy: 0.2046\n",
      "Iter-50900 train loss: 2.2995 valid loss: 2.3456, valid accuracy: 0.2046\n",
      "Iter-51000 train loss: 2.3717 valid loss: 2.3454, valid accuracy: 0.2046\n",
      "Iter-51100 train loss: 2.2914 valid loss: 2.3452, valid accuracy: 0.2046\n",
      "Iter-51200 train loss: 2.3413 valid loss: 2.3450, valid accuracy: 0.2046\n",
      "Iter-51300 train loss: 2.3148 valid loss: 2.3448, valid accuracy: 0.2046\n",
      "Iter-51400 train loss: 2.3206 valid loss: 2.3446, valid accuracy: 0.2054\n",
      "Iter-51500 train loss: 2.3406 valid loss: 2.3444, valid accuracy: 0.2069\n",
      "Iter-51600 train loss: 2.4458 valid loss: 2.3442, valid accuracy: 0.2069\n",
      "Iter-51700 train loss: 2.3485 valid loss: 2.3440, valid accuracy: 0.2077\n",
      "Iter-51800 train loss: 2.3649 valid loss: 2.3438, valid accuracy: 0.2069\n",
      "Iter-51900 train loss: 2.3467 valid loss: 2.3436, valid accuracy: 0.2054\n",
      "Iter-52000 train loss: 2.2740 valid loss: 2.3433, valid accuracy: 0.2062\n",
      "Iter-52100 train loss: 2.3335 valid loss: 2.3431, valid accuracy: 0.2054\n",
      "Iter-52200 train loss: 2.3435 valid loss: 2.3429, valid accuracy: 0.2062\n",
      "Iter-52300 train loss: 2.3551 valid loss: 2.3427, valid accuracy: 0.2054\n",
      "Iter-52400 train loss: 2.3252 valid loss: 2.3426, valid accuracy: 0.2054\n",
      "Iter-52500 train loss: 2.3683 valid loss: 2.3424, valid accuracy: 0.2062\n",
      "Iter-52600 train loss: 2.3239 valid loss: 2.3422, valid accuracy: 0.2069\n",
      "Iter-52700 train loss: 2.3037 valid loss: 2.3420, valid accuracy: 0.2069\n",
      "Iter-52800 train loss: 2.3579 valid loss: 2.3417, valid accuracy: 0.2069\n",
      "Iter-52900 train loss: 2.2769 valid loss: 2.3416, valid accuracy: 0.2069\n",
      "Iter-53000 train loss: 2.2576 valid loss: 2.3414, valid accuracy: 0.2069\n",
      "Iter-53100 train loss: 2.3583 valid loss: 2.3412, valid accuracy: 0.2069\n",
      "Iter-53200 train loss: 2.3256 valid loss: 2.3409, valid accuracy: 0.2069\n",
      "Iter-53300 train loss: 2.4333 valid loss: 2.3407, valid accuracy: 0.2077\n",
      "Iter-53400 train loss: 2.3423 valid loss: 2.3405, valid accuracy: 0.2085\n",
      "Iter-53500 train loss: 2.2606 valid loss: 2.3404, valid accuracy: 0.2085\n",
      "Iter-53600 train loss: 2.2703 valid loss: 2.3401, valid accuracy: 0.2077\n",
      "Iter-53700 train loss: 2.3370 valid loss: 2.3399, valid accuracy: 0.2077\n",
      "Iter-53800 train loss: 2.4011 valid loss: 2.3397, valid accuracy: 0.2077\n",
      "Iter-53900 train loss: 2.2507 valid loss: 2.3395, valid accuracy: 0.2077\n",
      "Iter-54000 train loss: 2.4379 valid loss: 2.3393, valid accuracy: 0.2069\n",
      "Iter-54100 train loss: 2.3842 valid loss: 2.3391, valid accuracy: 0.2077\n",
      "Iter-54200 train loss: 2.4605 valid loss: 2.3388, valid accuracy: 0.2077\n",
      "Iter-54300 train loss: 2.2777 valid loss: 2.3387, valid accuracy: 0.2069\n",
      "Iter-54400 train loss: 2.2849 valid loss: 2.3385, valid accuracy: 0.2069\n",
      "Iter-54500 train loss: 2.2726 valid loss: 2.3383, valid accuracy: 0.2069\n",
      "Iter-54600 train loss: 2.4201 valid loss: 2.3381, valid accuracy: 0.2069\n",
      "Iter-54700 train loss: 2.1931 valid loss: 2.3379, valid accuracy: 0.2077\n",
      "Iter-54800 train loss: 2.3722 valid loss: 2.3376, valid accuracy: 0.2077\n",
      "Iter-54900 train loss: 2.3416 valid loss: 2.3375, valid accuracy: 0.2077\n",
      "Iter-55000 train loss: 2.3448 valid loss: 2.3372, valid accuracy: 0.2077\n",
      "Iter-55100 train loss: 2.2645 valid loss: 2.3370, valid accuracy: 0.2077\n",
      "Iter-55200 train loss: 2.3325 valid loss: 2.3368, valid accuracy: 0.2085\n",
      "Iter-55300 train loss: 2.3130 valid loss: 2.3366, valid accuracy: 0.2092\n",
      "Iter-55400 train loss: 2.3992 valid loss: 2.3364, valid accuracy: 0.2092\n",
      "Iter-55500 train loss: 2.2488 valid loss: 2.3363, valid accuracy: 0.2085\n",
      "Iter-55600 train loss: 2.3308 valid loss: 2.3361, valid accuracy: 0.2085\n",
      "Iter-55700 train loss: 2.2606 valid loss: 2.3359, valid accuracy: 0.2085\n",
      "Iter-55800 train loss: 2.3139 valid loss: 2.3356, valid accuracy: 0.2092\n",
      "Iter-55900 train loss: 2.2469 valid loss: 2.3355, valid accuracy: 0.2092\n",
      "Iter-56000 train loss: 2.1708 valid loss: 2.3353, valid accuracy: 0.2092\n",
      "Iter-56100 train loss: 2.3929 valid loss: 2.3351, valid accuracy: 0.2092\n",
      "Iter-56200 train loss: 2.2460 valid loss: 2.3349, valid accuracy: 0.2092\n",
      "Iter-56300 train loss: 2.3496 valid loss: 2.3347, valid accuracy: 0.2085\n",
      "Iter-56400 train loss: 2.1686 valid loss: 2.3345, valid accuracy: 0.2092\n",
      "Iter-56500 train loss: 2.3236 valid loss: 2.3343, valid accuracy: 0.2092\n",
      "Iter-56600 train loss: 2.3174 valid loss: 2.3341, valid accuracy: 0.2100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 2.2627 valid loss: 2.3339, valid accuracy: 0.2085\n",
      "Iter-56800 train loss: 2.3109 valid loss: 2.3337, valid accuracy: 0.2092\n",
      "Iter-56900 train loss: 2.2457 valid loss: 2.3335, valid accuracy: 0.2092\n",
      "Iter-57000 train loss: 2.2639 valid loss: 2.3334, valid accuracy: 0.2100\n",
      "Iter-57100 train loss: 2.3677 valid loss: 2.3331, valid accuracy: 0.2092\n",
      "Iter-57200 train loss: 2.1736 valid loss: 2.3329, valid accuracy: 0.2085\n",
      "Iter-57300 train loss: 2.2506 valid loss: 2.3328, valid accuracy: 0.2085\n",
      "Iter-57400 train loss: 2.3590 valid loss: 2.3326, valid accuracy: 0.2092\n",
      "Iter-57500 train loss: 2.1793 valid loss: 2.3324, valid accuracy: 0.2092\n",
      "Iter-57600 train loss: 2.2530 valid loss: 2.3322, valid accuracy: 0.2092\n",
      "Iter-57700 train loss: 2.2072 valid loss: 2.3320, valid accuracy: 0.2092\n",
      "Iter-57800 train loss: 2.2233 valid loss: 2.3319, valid accuracy: 0.2085\n",
      "Iter-57900 train loss: 2.1639 valid loss: 2.3316, valid accuracy: 0.2092\n",
      "Iter-58000 train loss: 2.2572 valid loss: 2.3315, valid accuracy: 0.2100\n",
      "Iter-58100 train loss: 2.3137 valid loss: 2.3313, valid accuracy: 0.2092\n",
      "Iter-58200 train loss: 2.2874 valid loss: 2.3310, valid accuracy: 0.2100\n",
      "Iter-58300 train loss: 2.3508 valid loss: 2.3308, valid accuracy: 0.2100\n",
      "Iter-58400 train loss: 2.3088 valid loss: 2.3307, valid accuracy: 0.2092\n",
      "Iter-58500 train loss: 2.2516 valid loss: 2.3305, valid accuracy: 0.2100\n",
      "Iter-58600 train loss: 2.4193 valid loss: 2.3303, valid accuracy: 0.2100\n",
      "Iter-58700 train loss: 2.2473 valid loss: 2.3301, valid accuracy: 0.2100\n",
      "Iter-58800 train loss: 2.2659 valid loss: 2.3299, valid accuracy: 0.2085\n",
      "Iter-58900 train loss: 2.3551 valid loss: 2.3298, valid accuracy: 0.2092\n",
      "Iter-59000 train loss: 2.1708 valid loss: 2.3296, valid accuracy: 0.2077\n",
      "Iter-59100 train loss: 2.3580 valid loss: 2.3294, valid accuracy: 0.2085\n",
      "Iter-59200 train loss: 2.2063 valid loss: 2.3292, valid accuracy: 0.2092\n",
      "Iter-59300 train loss: 2.3385 valid loss: 2.3290, valid accuracy: 0.2092\n",
      "Iter-59400 train loss: 2.1551 valid loss: 2.3288, valid accuracy: 0.2092\n",
      "Iter-59500 train loss: 2.2894 valid loss: 2.3286, valid accuracy: 0.2100\n",
      "Iter-59600 train loss: 2.1880 valid loss: 2.3284, valid accuracy: 0.2092\n",
      "Iter-59700 train loss: 2.3217 valid loss: 2.3283, valid accuracy: 0.2077\n",
      "Iter-59800 train loss: 2.1828 valid loss: 2.3281, valid accuracy: 0.2100\n",
      "Iter-59900 train loss: 2.1861 valid loss: 2.3279, valid accuracy: 0.2100\n",
      "Iter-60000 train loss: 2.3423 valid loss: 2.3277, valid accuracy: 0.2100\n",
      "Iter-60100 train loss: 2.4633 valid loss: 2.3276, valid accuracy: 0.2085\n",
      "Iter-60200 train loss: 2.3116 valid loss: 2.3274, valid accuracy: 0.2092\n",
      "Iter-60300 train loss: 2.3474 valid loss: 2.3272, valid accuracy: 0.2108\n",
      "Iter-60400 train loss: 2.3861 valid loss: 2.3270, valid accuracy: 0.2108\n",
      "Iter-60500 train loss: 2.3757 valid loss: 2.3268, valid accuracy: 0.2108\n",
      "Iter-60600 train loss: 2.1714 valid loss: 2.3266, valid accuracy: 0.2108\n",
      "Iter-60700 train loss: 2.2692 valid loss: 2.3265, valid accuracy: 0.2108\n",
      "Iter-60800 train loss: 2.3469 valid loss: 2.3263, valid accuracy: 0.2100\n",
      "Iter-60900 train loss: 2.4511 valid loss: 2.3261, valid accuracy: 0.2100\n",
      "Iter-61000 train loss: 2.2105 valid loss: 2.3259, valid accuracy: 0.2108\n",
      "Iter-61100 train loss: 2.2180 valid loss: 2.3257, valid accuracy: 0.2108\n",
      "Iter-61200 train loss: 2.3145 valid loss: 2.3256, valid accuracy: 0.2100\n",
      "Iter-61300 train loss: 2.2489 valid loss: 2.3254, valid accuracy: 0.2108\n",
      "Iter-61400 train loss: 2.2063 valid loss: 2.3252, valid accuracy: 0.2100\n",
      "Iter-61500 train loss: 2.2295 valid loss: 2.3250, valid accuracy: 0.2108\n",
      "Iter-61600 train loss: 2.3840 valid loss: 2.3249, valid accuracy: 0.2108\n",
      "Iter-61700 train loss: 2.1741 valid loss: 2.3247, valid accuracy: 0.2100\n",
      "Iter-61800 train loss: 2.2619 valid loss: 2.3245, valid accuracy: 0.2115\n",
      "Iter-61900 train loss: 2.3330 valid loss: 2.3244, valid accuracy: 0.2123\n",
      "Iter-62000 train loss: 2.1359 valid loss: 2.3242, valid accuracy: 0.2115\n",
      "Iter-62100 train loss: 2.2382 valid loss: 2.3241, valid accuracy: 0.2123\n",
      "Iter-62200 train loss: 2.2868 valid loss: 2.3239, valid accuracy: 0.2123\n",
      "Iter-62300 train loss: 2.3991 valid loss: 2.3238, valid accuracy: 0.2131\n",
      "Iter-62400 train loss: 2.3459 valid loss: 2.3236, valid accuracy: 0.2115\n",
      "Iter-62500 train loss: 2.3189 valid loss: 2.3235, valid accuracy: 0.2115\n",
      "Iter-62600 train loss: 2.2429 valid loss: 2.3233, valid accuracy: 0.2115\n",
      "Iter-62700 train loss: 2.2441 valid loss: 2.3231, valid accuracy: 0.2115\n",
      "Iter-62800 train loss: 2.2499 valid loss: 2.3229, valid accuracy: 0.2100\n",
      "Iter-62900 train loss: 2.3465 valid loss: 2.3228, valid accuracy: 0.2115\n",
      "Iter-63000 train loss: 2.3279 valid loss: 2.3226, valid accuracy: 0.2108\n",
      "Iter-63100 train loss: 2.2340 valid loss: 2.3224, valid accuracy: 0.2100\n",
      "Iter-63200 train loss: 2.3822 valid loss: 2.3222, valid accuracy: 0.2123\n",
      "Iter-63300 train loss: 2.2165 valid loss: 2.3220, valid accuracy: 0.2108\n",
      "Iter-63400 train loss: 2.3913 valid loss: 2.3219, valid accuracy: 0.2100\n",
      "Iter-63500 train loss: 2.2118 valid loss: 2.3217, valid accuracy: 0.2100\n",
      "Iter-63600 train loss: 2.2588 valid loss: 2.3215, valid accuracy: 0.2100\n",
      "Iter-63700 train loss: 2.3184 valid loss: 2.3214, valid accuracy: 0.2092\n",
      "Iter-63800 train loss: 2.2164 valid loss: 2.3212, valid accuracy: 0.2100\n",
      "Iter-63900 train loss: 2.2254 valid loss: 2.3210, valid accuracy: 0.2115\n",
      "Iter-64000 train loss: 2.3707 valid loss: 2.3208, valid accuracy: 0.2100\n",
      "Iter-64100 train loss: 2.1985 valid loss: 2.3207, valid accuracy: 0.2100\n",
      "Iter-64200 train loss: 2.3266 valid loss: 2.3205, valid accuracy: 0.2100\n",
      "Iter-64300 train loss: 2.3388 valid loss: 2.3203, valid accuracy: 0.2108\n",
      "Iter-64400 train loss: 2.2334 valid loss: 2.3201, valid accuracy: 0.2100\n",
      "Iter-64500 train loss: 2.2196 valid loss: 2.3199, valid accuracy: 0.2108\n",
      "Iter-64600 train loss: 2.3483 valid loss: 2.3197, valid accuracy: 0.2115\n",
      "Iter-64700 train loss: 2.3892 valid loss: 2.3195, valid accuracy: 0.2115\n",
      "Iter-64800 train loss: 2.3742 valid loss: 2.3194, valid accuracy: 0.2115\n",
      "Iter-64900 train loss: 2.2120 valid loss: 2.3192, valid accuracy: 0.2115\n",
      "Iter-65000 train loss: 2.2799 valid loss: 2.3191, valid accuracy: 0.2115\n",
      "Iter-65100 train loss: 2.1144 valid loss: 2.3189, valid accuracy: 0.2123\n",
      "Iter-65200 train loss: 2.3591 valid loss: 2.3187, valid accuracy: 0.2108\n",
      "Iter-65300 train loss: 2.3300 valid loss: 2.3185, valid accuracy: 0.2115\n",
      "Iter-65400 train loss: 2.1068 valid loss: 2.3184, valid accuracy: 0.2115\n",
      "Iter-65500 train loss: 2.3066 valid loss: 2.3182, valid accuracy: 0.2115\n",
      "Iter-65600 train loss: 2.2499 valid loss: 2.3180, valid accuracy: 0.2115\n",
      "Iter-65700 train loss: 2.3029 valid loss: 2.3179, valid accuracy: 0.2108\n",
      "Iter-65800 train loss: 2.3179 valid loss: 2.3177, valid accuracy: 0.2100\n",
      "Iter-65900 train loss: 2.2858 valid loss: 2.3176, valid accuracy: 0.2108\n",
      "Iter-66000 train loss: 2.3005 valid loss: 2.3174, valid accuracy: 0.2108\n",
      "Iter-66100 train loss: 2.3476 valid loss: 2.3172, valid accuracy: 0.2108\n",
      "Iter-66200 train loss: 2.3479 valid loss: 2.3170, valid accuracy: 0.2115\n",
      "Iter-66300 train loss: 2.2722 valid loss: 2.3168, valid accuracy: 0.2115\n",
      "Iter-66400 train loss: 2.2433 valid loss: 2.3167, valid accuracy: 0.2123\n",
      "Iter-66500 train loss: 2.2238 valid loss: 2.3166, valid accuracy: 0.2092\n",
      "Iter-66600 train loss: 2.1944 valid loss: 2.3164, valid accuracy: 0.2108\n",
      "Iter-66700 train loss: 2.3925 valid loss: 2.3162, valid accuracy: 0.2108\n",
      "Iter-66800 train loss: 2.3409 valid loss: 2.3161, valid accuracy: 0.2108\n",
      "Iter-66900 train loss: 2.2767 valid loss: 2.3159, valid accuracy: 0.2115\n",
      "Iter-67000 train loss: 2.3967 valid loss: 2.3157, valid accuracy: 0.2115\n",
      "Iter-67100 train loss: 2.2866 valid loss: 2.3155, valid accuracy: 0.2123\n",
      "Iter-67200 train loss: 2.2962 valid loss: 2.3154, valid accuracy: 0.2138\n",
      "Iter-67300 train loss: 2.3707 valid loss: 2.3152, valid accuracy: 0.2131\n",
      "Iter-67400 train loss: 2.2443 valid loss: 2.3150, valid accuracy: 0.2146\n",
      "Iter-67500 train loss: 2.3285 valid loss: 2.3149, valid accuracy: 0.2146\n",
      "Iter-67600 train loss: 2.1940 valid loss: 2.3146, valid accuracy: 0.2146\n",
      "Iter-67700 train loss: 2.1472 valid loss: 2.3145, valid accuracy: 0.2154\n",
      "Iter-67800 train loss: 2.2768 valid loss: 2.3143, valid accuracy: 0.2154\n",
      "Iter-67900 train loss: 2.2946 valid loss: 2.3141, valid accuracy: 0.2154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 2.3180 valid loss: 2.3140, valid accuracy: 0.2138\n",
      "Iter-68100 train loss: 2.3403 valid loss: 2.3138, valid accuracy: 0.2138\n",
      "Iter-68200 train loss: 2.4024 valid loss: 2.3137, valid accuracy: 0.2131\n",
      "Iter-68300 train loss: 2.3633 valid loss: 2.3135, valid accuracy: 0.2169\n",
      "Iter-68400 train loss: 2.1561 valid loss: 2.3134, valid accuracy: 0.2154\n",
      "Iter-68500 train loss: 2.3721 valid loss: 2.3132, valid accuracy: 0.2146\n",
      "Iter-68600 train loss: 2.3418 valid loss: 2.3129, valid accuracy: 0.2169\n",
      "Iter-68700 train loss: 2.3103 valid loss: 2.3128, valid accuracy: 0.2169\n",
      "Iter-68800 train loss: 2.2315 valid loss: 2.3126, valid accuracy: 0.2177\n",
      "Iter-68900 train loss: 2.2181 valid loss: 2.3124, valid accuracy: 0.2169\n",
      "Iter-69000 train loss: 2.1883 valid loss: 2.3123, valid accuracy: 0.2169\n",
      "Iter-69100 train loss: 2.2808 valid loss: 2.3121, valid accuracy: 0.2162\n",
      "Iter-69200 train loss: 2.1901 valid loss: 2.3120, valid accuracy: 0.2177\n",
      "Iter-69300 train loss: 2.2613 valid loss: 2.3118, valid accuracy: 0.2177\n",
      "Iter-69400 train loss: 2.2095 valid loss: 2.3116, valid accuracy: 0.2185\n",
      "Iter-69500 train loss: 2.3092 valid loss: 2.3114, valid accuracy: 0.2185\n",
      "Iter-69600 train loss: 2.2751 valid loss: 2.3113, valid accuracy: 0.2169\n",
      "Iter-69700 train loss: 2.3833 valid loss: 2.3111, valid accuracy: 0.2177\n",
      "Iter-69800 train loss: 2.2933 valid loss: 2.3109, valid accuracy: 0.2177\n",
      "Iter-69900 train loss: 2.1289 valid loss: 2.3107, valid accuracy: 0.2169\n",
      "Iter-70000 train loss: 2.3352 valid loss: 2.3106, valid accuracy: 0.2177\n",
      "Iter-70100 train loss: 2.4023 valid loss: 2.3104, valid accuracy: 0.2177\n",
      "Iter-70200 train loss: 2.3529 valid loss: 2.3103, valid accuracy: 0.2192\n",
      "Iter-70300 train loss: 2.2806 valid loss: 2.3101, valid accuracy: 0.2200\n",
      "Iter-70400 train loss: 2.2342 valid loss: 2.3100, valid accuracy: 0.2200\n",
      "Iter-70500 train loss: 2.2401 valid loss: 2.3098, valid accuracy: 0.2185\n",
      "Iter-70600 train loss: 2.2408 valid loss: 2.3096, valid accuracy: 0.2192\n",
      "Iter-70700 train loss: 2.1690 valid loss: 2.3095, valid accuracy: 0.2192\n",
      "Iter-70800 train loss: 2.2884 valid loss: 2.3093, valid accuracy: 0.2200\n",
      "Iter-70900 train loss: 2.3310 valid loss: 2.3091, valid accuracy: 0.2192\n",
      "Iter-71000 train loss: 2.2901 valid loss: 2.3090, valid accuracy: 0.2185\n",
      "Iter-71100 train loss: 2.2759 valid loss: 2.3088, valid accuracy: 0.2192\n",
      "Iter-71200 train loss: 2.3491 valid loss: 2.3086, valid accuracy: 0.2192\n",
      "Iter-71300 train loss: 2.2727 valid loss: 2.3085, valid accuracy: 0.2192\n",
      "Iter-71400 train loss: 2.3156 valid loss: 2.3083, valid accuracy: 0.2169\n",
      "Iter-71500 train loss: 2.4968 valid loss: 2.3082, valid accuracy: 0.2177\n",
      "Iter-71600 train loss: 2.2285 valid loss: 2.3080, valid accuracy: 0.2185\n",
      "Iter-71700 train loss: 2.1886 valid loss: 2.3078, valid accuracy: 0.2185\n",
      "Iter-71800 train loss: 2.3574 valid loss: 2.3076, valid accuracy: 0.2177\n",
      "Iter-71900 train loss: 2.1270 valid loss: 2.3075, valid accuracy: 0.2177\n",
      "Iter-72000 train loss: 2.1535 valid loss: 2.3073, valid accuracy: 0.2185\n",
      "Iter-72100 train loss: 2.2008 valid loss: 2.3072, valid accuracy: 0.2192\n",
      "Iter-72200 train loss: 2.1756 valid loss: 2.3070, valid accuracy: 0.2192\n",
      "Iter-72300 train loss: 2.0381 valid loss: 2.3069, valid accuracy: 0.2192\n",
      "Iter-72400 train loss: 2.2800 valid loss: 2.3067, valid accuracy: 0.2192\n",
      "Iter-72500 train loss: 2.3423 valid loss: 2.3065, valid accuracy: 0.2192\n",
      "Iter-72600 train loss: 2.2515 valid loss: 2.3064, valid accuracy: 0.2200\n",
      "Iter-72700 train loss: 2.3211 valid loss: 2.3062, valid accuracy: 0.2192\n",
      "Iter-72800 train loss: 2.3252 valid loss: 2.3061, valid accuracy: 0.2208\n",
      "Iter-72900 train loss: 2.3078 valid loss: 2.3059, valid accuracy: 0.2192\n",
      "Iter-73000 train loss: 2.1470 valid loss: 2.3058, valid accuracy: 0.2200\n",
      "Iter-73100 train loss: 2.3162 valid loss: 2.3056, valid accuracy: 0.2192\n",
      "Iter-73200 train loss: 2.1363 valid loss: 2.3054, valid accuracy: 0.2200\n",
      "Iter-73300 train loss: 2.2635 valid loss: 2.3053, valid accuracy: 0.2208\n",
      "Iter-73400 train loss: 2.1883 valid loss: 2.3051, valid accuracy: 0.2208\n",
      "Iter-73500 train loss: 2.2617 valid loss: 2.3049, valid accuracy: 0.2192\n",
      "Iter-73600 train loss: 2.2376 valid loss: 2.3048, valid accuracy: 0.2200\n",
      "Iter-73700 train loss: 2.3693 valid loss: 2.3046, valid accuracy: 0.2200\n",
      "Iter-73800 train loss: 2.1764 valid loss: 2.3044, valid accuracy: 0.2208\n",
      "Iter-73900 train loss: 2.2371 valid loss: 2.3042, valid accuracy: 0.2208\n",
      "Iter-74000 train loss: 2.1445 valid loss: 2.3041, valid accuracy: 0.2215\n",
      "Iter-74100 train loss: 2.1807 valid loss: 2.3039, valid accuracy: 0.2200\n",
      "Iter-74200 train loss: 2.1389 valid loss: 2.3037, valid accuracy: 0.2208\n",
      "Iter-74300 train loss: 2.1369 valid loss: 2.3036, valid accuracy: 0.2200\n",
      "Iter-74400 train loss: 2.2033 valid loss: 2.3034, valid accuracy: 0.2200\n",
      "Iter-74500 train loss: 2.2682 valid loss: 2.3033, valid accuracy: 0.2208\n",
      "Iter-74600 train loss: 2.1930 valid loss: 2.3031, valid accuracy: 0.2200\n",
      "Iter-74700 train loss: 2.2803 valid loss: 2.3029, valid accuracy: 0.2200\n",
      "Iter-74800 train loss: 2.2330 valid loss: 2.3027, valid accuracy: 0.2208\n",
      "Iter-74900 train loss: 2.4258 valid loss: 2.3026, valid accuracy: 0.2215\n",
      "Iter-75000 train loss: 2.2555 valid loss: 2.3024, valid accuracy: 0.2208\n",
      "Iter-75100 train loss: 2.3129 valid loss: 2.3022, valid accuracy: 0.2208\n",
      "Iter-75200 train loss: 2.3411 valid loss: 2.3020, valid accuracy: 0.2208\n",
      "Iter-75300 train loss: 2.1900 valid loss: 2.3018, valid accuracy: 0.2223\n",
      "Iter-75400 train loss: 2.3421 valid loss: 2.3017, valid accuracy: 0.2223\n",
      "Iter-75500 train loss: 2.2878 valid loss: 2.3016, valid accuracy: 0.2223\n",
      "Iter-75600 train loss: 2.2472 valid loss: 2.3014, valid accuracy: 0.2215\n",
      "Iter-75700 train loss: 2.2395 valid loss: 2.3013, valid accuracy: 0.2215\n",
      "Iter-75800 train loss: 2.1973 valid loss: 2.3011, valid accuracy: 0.2215\n",
      "Iter-75900 train loss: 2.4072 valid loss: 2.3009, valid accuracy: 0.2215\n",
      "Iter-76000 train loss: 2.2278 valid loss: 2.3007, valid accuracy: 0.2223\n",
      "Iter-76100 train loss: 2.3662 valid loss: 2.3006, valid accuracy: 0.2223\n",
      "Iter-76200 train loss: 2.1800 valid loss: 2.3005, valid accuracy: 0.2215\n",
      "Iter-76300 train loss: 2.3448 valid loss: 2.3003, valid accuracy: 0.2215\n",
      "Iter-76400 train loss: 2.4574 valid loss: 2.3001, valid accuracy: 0.2223\n",
      "Iter-76500 train loss: 2.2539 valid loss: 2.2999, valid accuracy: 0.2223\n",
      "Iter-76600 train loss: 2.1392 valid loss: 2.2998, valid accuracy: 0.2215\n",
      "Iter-76700 train loss: 2.2684 valid loss: 2.2996, valid accuracy: 0.2223\n",
      "Iter-76800 train loss: 2.2273 valid loss: 2.2994, valid accuracy: 0.2231\n",
      "Iter-76900 train loss: 2.3258 valid loss: 2.2993, valid accuracy: 0.2215\n",
      "Iter-77000 train loss: 2.0803 valid loss: 2.2991, valid accuracy: 0.2208\n",
      "Iter-77100 train loss: 2.3153 valid loss: 2.2989, valid accuracy: 0.2215\n",
      "Iter-77200 train loss: 2.3238 valid loss: 2.2988, valid accuracy: 0.2215\n",
      "Iter-77300 train loss: 2.1696 valid loss: 2.2986, valid accuracy: 0.2208\n",
      "Iter-77400 train loss: 2.1718 valid loss: 2.2985, valid accuracy: 0.2208\n",
      "Iter-77500 train loss: 2.3217 valid loss: 2.2983, valid accuracy: 0.2215\n",
      "Iter-77600 train loss: 2.2375 valid loss: 2.2981, valid accuracy: 0.2208\n",
      "Iter-77700 train loss: 2.3283 valid loss: 2.2980, valid accuracy: 0.2223\n",
      "Iter-77800 train loss: 2.3377 valid loss: 2.2979, valid accuracy: 0.2223\n",
      "Iter-77900 train loss: 2.1528 valid loss: 2.2977, valid accuracy: 0.2231\n",
      "Iter-78000 train loss: 2.2333 valid loss: 2.2975, valid accuracy: 0.2223\n",
      "Iter-78100 train loss: 2.2233 valid loss: 2.2974, valid accuracy: 0.2223\n",
      "Iter-78200 train loss: 2.2554 valid loss: 2.2972, valid accuracy: 0.2223\n",
      "Iter-78300 train loss: 2.2509 valid loss: 2.2971, valid accuracy: 0.2223\n",
      "Iter-78400 train loss: 2.3907 valid loss: 2.2969, valid accuracy: 0.2231\n",
      "Iter-78500 train loss: 2.3565 valid loss: 2.2968, valid accuracy: 0.2223\n",
      "Iter-78600 train loss: 2.2328 valid loss: 2.2966, valid accuracy: 0.2231\n",
      "Iter-78700 train loss: 2.2821 valid loss: 2.2965, valid accuracy: 0.2231\n",
      "Iter-78800 train loss: 2.3927 valid loss: 2.2963, valid accuracy: 0.2231\n",
      "Iter-78900 train loss: 2.1844 valid loss: 2.2961, valid accuracy: 0.2231\n",
      "Iter-79000 train loss: 2.2457 valid loss: 2.2960, valid accuracy: 0.2246\n",
      "Iter-79100 train loss: 2.2546 valid loss: 2.2958, valid accuracy: 0.2223\n",
      "Iter-79200 train loss: 2.4174 valid loss: 2.2956, valid accuracy: 0.2215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 2.1376 valid loss: 2.2955, valid accuracy: 0.2223\n",
      "Iter-79400 train loss: 2.3726 valid loss: 2.2953, valid accuracy: 0.2231\n",
      "Iter-79500 train loss: 2.2613 valid loss: 2.2951, valid accuracy: 0.2231\n",
      "Iter-79600 train loss: 2.3788 valid loss: 2.2950, valid accuracy: 0.2238\n",
      "Iter-79700 train loss: 2.2442 valid loss: 2.2948, valid accuracy: 0.2231\n",
      "Iter-79800 train loss: 2.3254 valid loss: 2.2947, valid accuracy: 0.2246\n",
      "Iter-79900 train loss: 2.3423 valid loss: 2.2945, valid accuracy: 0.2262\n",
      "Iter-80000 train loss: 2.3930 valid loss: 2.2944, valid accuracy: 0.2246\n",
      "Iter-80100 train loss: 2.2759 valid loss: 2.2942, valid accuracy: 0.2254\n",
      "Iter-80200 train loss: 2.3187 valid loss: 2.2941, valid accuracy: 0.2254\n",
      "Iter-80300 train loss: 2.2377 valid loss: 2.2939, valid accuracy: 0.2254\n",
      "Iter-80400 train loss: 2.3559 valid loss: 2.2938, valid accuracy: 0.2254\n",
      "Iter-80500 train loss: 2.1777 valid loss: 2.2936, valid accuracy: 0.2238\n",
      "Iter-80600 train loss: 2.2168 valid loss: 2.2934, valid accuracy: 0.2246\n",
      "Iter-80700 train loss: 2.2535 valid loss: 2.2932, valid accuracy: 0.2246\n",
      "Iter-80800 train loss: 2.1871 valid loss: 2.2930, valid accuracy: 0.2246\n",
      "Iter-80900 train loss: 2.3251 valid loss: 2.2929, valid accuracy: 0.2254\n",
      "Iter-81000 train loss: 2.1810 valid loss: 2.2927, valid accuracy: 0.2246\n",
      "Iter-81100 train loss: 2.4097 valid loss: 2.2925, valid accuracy: 0.2254\n",
      "Iter-81200 train loss: 2.2596 valid loss: 2.2924, valid accuracy: 0.2262\n",
      "Iter-81300 train loss: 2.2128 valid loss: 2.2922, valid accuracy: 0.2269\n",
      "Iter-81400 train loss: 2.3989 valid loss: 2.2921, valid accuracy: 0.2262\n",
      "Iter-81500 train loss: 2.2367 valid loss: 2.2919, valid accuracy: 0.2254\n",
      "Iter-81600 train loss: 2.2372 valid loss: 2.2917, valid accuracy: 0.2262\n",
      "Iter-81700 train loss: 2.1754 valid loss: 2.2916, valid accuracy: 0.2254\n",
      "Iter-81800 train loss: 2.1759 valid loss: 2.2914, valid accuracy: 0.2246\n",
      "Iter-81900 train loss: 2.2840 valid loss: 2.2913, valid accuracy: 0.2238\n",
      "Iter-82000 train loss: 2.4105 valid loss: 2.2911, valid accuracy: 0.2262\n",
      "Iter-82100 train loss: 2.2878 valid loss: 2.2909, valid accuracy: 0.2262\n",
      "Iter-82200 train loss: 2.1899 valid loss: 2.2908, valid accuracy: 0.2262\n",
      "Iter-82300 train loss: 2.4083 valid loss: 2.2906, valid accuracy: 0.2262\n",
      "Iter-82400 train loss: 2.1838 valid loss: 2.2905, valid accuracy: 0.2262\n",
      "Iter-82500 train loss: 2.3452 valid loss: 2.2903, valid accuracy: 0.2254\n",
      "Iter-82600 train loss: 2.3920 valid loss: 2.2902, valid accuracy: 0.2262\n",
      "Iter-82700 train loss: 2.3077 valid loss: 2.2900, valid accuracy: 0.2246\n",
      "Iter-82800 train loss: 2.3047 valid loss: 2.2899, valid accuracy: 0.2262\n",
      "Iter-82900 train loss: 2.3615 valid loss: 2.2897, valid accuracy: 0.2262\n",
      "Iter-83000 train loss: 2.3310 valid loss: 2.2896, valid accuracy: 0.2269\n",
      "Iter-83100 train loss: 2.1599 valid loss: 2.2894, valid accuracy: 0.2246\n",
      "Iter-83200 train loss: 2.3519 valid loss: 2.2892, valid accuracy: 0.2254\n",
      "Iter-83300 train loss: 2.1466 valid loss: 2.2891, valid accuracy: 0.2262\n",
      "Iter-83400 train loss: 2.2066 valid loss: 2.2889, valid accuracy: 0.2262\n",
      "Iter-83500 train loss: 2.4407 valid loss: 2.2888, valid accuracy: 0.2277\n",
      "Iter-83600 train loss: 2.0498 valid loss: 2.2885, valid accuracy: 0.2246\n",
      "Iter-83700 train loss: 2.1044 valid loss: 2.2884, valid accuracy: 0.2254\n",
      "Iter-83800 train loss: 2.2251 valid loss: 2.2882, valid accuracy: 0.2246\n",
      "Iter-83900 train loss: 2.1900 valid loss: 2.2880, valid accuracy: 0.2254\n",
      "Iter-84000 train loss: 2.1374 valid loss: 2.2879, valid accuracy: 0.2246\n",
      "Iter-84100 train loss: 2.2291 valid loss: 2.2877, valid accuracy: 0.2277\n",
      "Iter-84200 train loss: 2.2023 valid loss: 2.2876, valid accuracy: 0.2285\n",
      "Iter-84300 train loss: 2.3541 valid loss: 2.2874, valid accuracy: 0.2277\n",
      "Iter-84400 train loss: 2.3377 valid loss: 2.2873, valid accuracy: 0.2277\n",
      "Iter-84500 train loss: 2.3357 valid loss: 2.2871, valid accuracy: 0.2269\n",
      "Iter-84600 train loss: 2.2450 valid loss: 2.2870, valid accuracy: 0.2262\n",
      "Iter-84700 train loss: 2.3396 valid loss: 2.2867, valid accuracy: 0.2277\n",
      "Iter-84800 train loss: 2.2188 valid loss: 2.2866, valid accuracy: 0.2269\n",
      "Iter-84900 train loss: 2.1820 valid loss: 2.2864, valid accuracy: 0.2269\n",
      "Iter-85000 train loss: 2.3102 valid loss: 2.2863, valid accuracy: 0.2285\n",
      "Iter-85100 train loss: 2.2416 valid loss: 2.2861, valid accuracy: 0.2285\n",
      "Iter-85200 train loss: 2.2623 valid loss: 2.2859, valid accuracy: 0.2277\n",
      "Iter-85300 train loss: 2.2415 valid loss: 2.2858, valid accuracy: 0.2277\n",
      "Iter-85400 train loss: 2.2143 valid loss: 2.2857, valid accuracy: 0.2277\n",
      "Iter-85500 train loss: 2.1904 valid loss: 2.2855, valid accuracy: 0.2277\n",
      "Iter-85600 train loss: 2.0975 valid loss: 2.2854, valid accuracy: 0.2277\n",
      "Iter-85700 train loss: 2.1774 valid loss: 2.2852, valid accuracy: 0.2269\n",
      "Iter-85800 train loss: 2.2836 valid loss: 2.2851, valid accuracy: 0.2277\n",
      "Iter-85900 train loss: 2.2305 valid loss: 2.2849, valid accuracy: 0.2277\n",
      "Iter-86000 train loss: 2.2233 valid loss: 2.2848, valid accuracy: 0.2269\n",
      "Iter-86100 train loss: 2.1221 valid loss: 2.2846, valid accuracy: 0.2262\n",
      "Iter-86200 train loss: 2.1914 valid loss: 2.2844, valid accuracy: 0.2262\n",
      "Iter-86300 train loss: 2.0474 valid loss: 2.2842, valid accuracy: 0.2254\n",
      "Iter-86400 train loss: 2.1929 valid loss: 2.2840, valid accuracy: 0.2254\n",
      "Iter-86500 train loss: 2.1107 valid loss: 2.2839, valid accuracy: 0.2254\n",
      "Iter-86600 train loss: 2.3475 valid loss: 2.2837, valid accuracy: 0.2254\n",
      "Iter-86700 train loss: 2.3512 valid loss: 2.2835, valid accuracy: 0.2262\n",
      "Iter-86800 train loss: 2.3401 valid loss: 2.2833, valid accuracy: 0.2254\n",
      "Iter-86900 train loss: 2.4108 valid loss: 2.2833, valid accuracy: 0.2254\n",
      "Iter-87000 train loss: 2.1532 valid loss: 2.2832, valid accuracy: 0.2269\n",
      "Iter-87100 train loss: 2.2444 valid loss: 2.2830, valid accuracy: 0.2269\n",
      "Iter-87200 train loss: 2.0556 valid loss: 2.2828, valid accuracy: 0.2277\n",
      "Iter-87300 train loss: 2.3429 valid loss: 2.2827, valid accuracy: 0.2269\n",
      "Iter-87400 train loss: 2.3921 valid loss: 2.2825, valid accuracy: 0.2269\n",
      "Iter-87500 train loss: 2.2433 valid loss: 2.2824, valid accuracy: 0.2269\n",
      "Iter-87600 train loss: 2.1554 valid loss: 2.2823, valid accuracy: 0.2292\n",
      "Iter-87700 train loss: 2.1996 valid loss: 2.2821, valid accuracy: 0.2300\n",
      "Iter-87800 train loss: 2.3112 valid loss: 2.2820, valid accuracy: 0.2300\n",
      "Iter-87900 train loss: 2.2104 valid loss: 2.2818, valid accuracy: 0.2300\n",
      "Iter-88000 train loss: 2.3419 valid loss: 2.2817, valid accuracy: 0.2300\n",
      "Iter-88100 train loss: 2.1436 valid loss: 2.2816, valid accuracy: 0.2308\n",
      "Iter-88200 train loss: 2.2170 valid loss: 2.2814, valid accuracy: 0.2308\n",
      "Iter-88300 train loss: 2.2720 valid loss: 2.2813, valid accuracy: 0.2308\n",
      "Iter-88400 train loss: 2.2651 valid loss: 2.2811, valid accuracy: 0.2308\n",
      "Iter-88500 train loss: 2.0585 valid loss: 2.2810, valid accuracy: 0.2308\n",
      "Iter-88600 train loss: 2.0355 valid loss: 2.2809, valid accuracy: 0.2308\n",
      "Iter-88700 train loss: 2.2282 valid loss: 2.2807, valid accuracy: 0.2308\n",
      "Iter-88800 train loss: 2.1642 valid loss: 2.2806, valid accuracy: 0.2315\n",
      "Iter-88900 train loss: 2.2865 valid loss: 2.2805, valid accuracy: 0.2315\n",
      "Iter-89000 train loss: 2.2310 valid loss: 2.2804, valid accuracy: 0.2315\n",
      "Iter-89100 train loss: 2.2026 valid loss: 2.2802, valid accuracy: 0.2315\n",
      "Iter-89200 train loss: 2.2920 valid loss: 2.2801, valid accuracy: 0.2315\n",
      "Iter-89300 train loss: 2.2182 valid loss: 2.2800, valid accuracy: 0.2315\n",
      "Iter-89400 train loss: 2.1716 valid loss: 2.2798, valid accuracy: 0.2315\n",
      "Iter-89500 train loss: 2.1695 valid loss: 2.2797, valid accuracy: 0.2315\n",
      "Iter-89600 train loss: 2.2009 valid loss: 2.2796, valid accuracy: 0.2315\n",
      "Iter-89700 train loss: 2.2974 valid loss: 2.2794, valid accuracy: 0.2308\n",
      "Iter-89800 train loss: 2.2797 valid loss: 2.2792, valid accuracy: 0.2315\n",
      "Iter-89900 train loss: 2.2690 valid loss: 2.2791, valid accuracy: 0.2315\n",
      "Iter-90000 train loss: 2.2206 valid loss: 2.2790, valid accuracy: 0.2315\n",
      "Iter-90100 train loss: 2.1454 valid loss: 2.2788, valid accuracy: 0.2315\n",
      "Iter-90200 train loss: 2.2247 valid loss: 2.2787, valid accuracy: 0.2315\n",
      "Iter-90300 train loss: 2.1888 valid loss: 2.2785, valid accuracy: 0.2315\n",
      "Iter-90400 train loss: 2.1747 valid loss: 2.2784, valid accuracy: 0.2315\n",
      "Iter-90500 train loss: 2.2161 valid loss: 2.2783, valid accuracy: 0.2315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 2.2243 valid loss: 2.2782, valid accuracy: 0.2315\n",
      "Iter-90700 train loss: 2.2358 valid loss: 2.2780, valid accuracy: 0.2315\n",
      "Iter-90800 train loss: 2.2084 valid loss: 2.2778, valid accuracy: 0.2315\n",
      "Iter-90900 train loss: 2.0912 valid loss: 2.2777, valid accuracy: 0.2315\n",
      "Iter-91000 train loss: 2.2123 valid loss: 2.2775, valid accuracy: 0.2315\n",
      "Iter-91100 train loss: 2.1179 valid loss: 2.2774, valid accuracy: 0.2315\n",
      "Iter-91200 train loss: 2.1017 valid loss: 2.2772, valid accuracy: 0.2315\n",
      "Iter-91300 train loss: 2.2233 valid loss: 2.2770, valid accuracy: 0.2315\n",
      "Iter-91400 train loss: 2.2046 valid loss: 2.2769, valid accuracy: 0.2315\n",
      "Iter-91500 train loss: 2.1826 valid loss: 2.2768, valid accuracy: 0.2315\n",
      "Iter-91600 train loss: 2.1245 valid loss: 2.2767, valid accuracy: 0.2315\n",
      "Iter-91700 train loss: 2.2784 valid loss: 2.2766, valid accuracy: 0.2323\n",
      "Iter-91800 train loss: 2.3068 valid loss: 2.2764, valid accuracy: 0.2323\n",
      "Iter-91900 train loss: 2.3711 valid loss: 2.2763, valid accuracy: 0.2308\n",
      "Iter-92000 train loss: 2.2050 valid loss: 2.2762, valid accuracy: 0.2323\n",
      "Iter-92100 train loss: 2.1521 valid loss: 2.2761, valid accuracy: 0.2323\n",
      "Iter-92200 train loss: 2.2999 valid loss: 2.2759, valid accuracy: 0.2315\n",
      "Iter-92300 train loss: 2.2594 valid loss: 2.2757, valid accuracy: 0.2315\n",
      "Iter-92400 train loss: 2.1323 valid loss: 2.2756, valid accuracy: 0.2308\n",
      "Iter-92500 train loss: 2.2214 valid loss: 2.2754, valid accuracy: 0.2300\n",
      "Iter-92600 train loss: 2.2284 valid loss: 2.2752, valid accuracy: 0.2300\n",
      "Iter-92700 train loss: 2.0529 valid loss: 2.2751, valid accuracy: 0.2300\n",
      "Iter-92800 train loss: 2.1655 valid loss: 2.2750, valid accuracy: 0.2308\n",
      "Iter-92900 train loss: 2.2770 valid loss: 2.2749, valid accuracy: 0.2315\n",
      "Iter-93000 train loss: 2.1392 valid loss: 2.2747, valid accuracy: 0.2315\n",
      "Iter-93100 train loss: 2.4667 valid loss: 2.2745, valid accuracy: 0.2300\n",
      "Iter-93200 train loss: 2.1877 valid loss: 2.2743, valid accuracy: 0.2315\n",
      "Iter-93300 train loss: 2.2372 valid loss: 2.2743, valid accuracy: 0.2323\n",
      "Iter-93400 train loss: 2.3309 valid loss: 2.2741, valid accuracy: 0.2315\n",
      "Iter-93500 train loss: 2.3166 valid loss: 2.2740, valid accuracy: 0.2315\n",
      "Iter-93600 train loss: 2.1825 valid loss: 2.2738, valid accuracy: 0.2315\n",
      "Iter-93700 train loss: 2.2488 valid loss: 2.2737, valid accuracy: 0.2315\n",
      "Iter-93800 train loss: 2.3504 valid loss: 2.2736, valid accuracy: 0.2315\n",
      "Iter-93900 train loss: 2.2674 valid loss: 2.2734, valid accuracy: 0.2315\n",
      "Iter-94000 train loss: 2.1662 valid loss: 2.2732, valid accuracy: 0.2315\n",
      "Iter-94100 train loss: 2.1433 valid loss: 2.2730, valid accuracy: 0.2315\n",
      "Iter-94200 train loss: 2.0759 valid loss: 2.2730, valid accuracy: 0.2315\n",
      "Iter-94300 train loss: 2.3138 valid loss: 2.2728, valid accuracy: 0.2323\n",
      "Iter-94400 train loss: 2.1487 valid loss: 2.2727, valid accuracy: 0.2315\n",
      "Iter-94500 train loss: 2.2599 valid loss: 2.2726, valid accuracy: 0.2315\n",
      "Iter-94600 train loss: 2.2511 valid loss: 2.2725, valid accuracy: 0.2315\n",
      "Iter-94700 train loss: 2.2650 valid loss: 2.2723, valid accuracy: 0.2308\n",
      "Iter-94800 train loss: 2.1978 valid loss: 2.2722, valid accuracy: 0.2308\n",
      "Iter-94900 train loss: 2.2662 valid loss: 2.2720, valid accuracy: 0.2315\n",
      "Iter-95000 train loss: 2.3625 valid loss: 2.2718, valid accuracy: 0.2323\n",
      "Iter-95100 train loss: 2.2612 valid loss: 2.2717, valid accuracy: 0.2331\n",
      "Iter-95200 train loss: 2.3599 valid loss: 2.2716, valid accuracy: 0.2323\n",
      "Iter-95300 train loss: 2.0731 valid loss: 2.2714, valid accuracy: 0.2323\n",
      "Iter-95400 train loss: 2.4185 valid loss: 2.2713, valid accuracy: 0.2315\n",
      "Iter-95500 train loss: 2.3080 valid loss: 2.2711, valid accuracy: 0.2331\n",
      "Iter-95600 train loss: 2.2710 valid loss: 2.2710, valid accuracy: 0.2338\n",
      "Iter-95700 train loss: 2.2492 valid loss: 2.2708, valid accuracy: 0.2331\n",
      "Iter-95800 train loss: 2.1968 valid loss: 2.2707, valid accuracy: 0.2331\n",
      "Iter-95900 train loss: 2.0867 valid loss: 2.2706, valid accuracy: 0.2338\n",
      "Iter-96000 train loss: 2.3432 valid loss: 2.2704, valid accuracy: 0.2338\n",
      "Iter-96100 train loss: 2.3442 valid loss: 2.2703, valid accuracy: 0.2346\n",
      "Iter-96200 train loss: 1.9847 valid loss: 2.2701, valid accuracy: 0.2338\n",
      "Iter-96300 train loss: 2.3801 valid loss: 2.2700, valid accuracy: 0.2338\n",
      "Iter-96400 train loss: 2.2449 valid loss: 2.2698, valid accuracy: 0.2346\n",
      "Iter-96500 train loss: 2.1798 valid loss: 2.2697, valid accuracy: 0.2346\n",
      "Iter-96600 train loss: 2.1234 valid loss: 2.2695, valid accuracy: 0.2346\n",
      "Iter-96700 train loss: 2.1530 valid loss: 2.2694, valid accuracy: 0.2346\n",
      "Iter-96800 train loss: 2.2350 valid loss: 2.2692, valid accuracy: 0.2338\n",
      "Iter-96900 train loss: 2.1833 valid loss: 2.2691, valid accuracy: 0.2338\n",
      "Iter-97000 train loss: 2.2306 valid loss: 2.2690, valid accuracy: 0.2338\n",
      "Iter-97100 train loss: 2.3240 valid loss: 2.2688, valid accuracy: 0.2338\n",
      "Iter-97200 train loss: 2.1770 valid loss: 2.2687, valid accuracy: 0.2338\n",
      "Iter-97300 train loss: 2.1823 valid loss: 2.2686, valid accuracy: 0.2338\n",
      "Iter-97400 train loss: 2.2026 valid loss: 2.2684, valid accuracy: 0.2346\n",
      "Iter-97500 train loss: 2.1917 valid loss: 2.2683, valid accuracy: 0.2346\n",
      "Iter-97600 train loss: 2.2612 valid loss: 2.2681, valid accuracy: 0.2346\n",
      "Iter-97700 train loss: 2.2826 valid loss: 2.2680, valid accuracy: 0.2346\n",
      "Iter-97800 train loss: 2.1819 valid loss: 2.2679, valid accuracy: 0.2346\n",
      "Iter-97900 train loss: 2.1957 valid loss: 2.2678, valid accuracy: 0.2346\n",
      "Iter-98000 train loss: 2.1314 valid loss: 2.2676, valid accuracy: 0.2338\n",
      "Iter-98100 train loss: 2.1533 valid loss: 2.2675, valid accuracy: 0.2362\n",
      "Iter-98200 train loss: 2.2211 valid loss: 2.2674, valid accuracy: 0.2362\n",
      "Iter-98300 train loss: 2.2730 valid loss: 2.2673, valid accuracy: 0.2362\n",
      "Iter-98400 train loss: 2.2149 valid loss: 2.2672, valid accuracy: 0.2362\n",
      "Iter-98500 train loss: 2.1632 valid loss: 2.2671, valid accuracy: 0.2362\n",
      "Iter-98600 train loss: 2.3407 valid loss: 2.2669, valid accuracy: 0.2362\n",
      "Iter-98700 train loss: 2.2262 valid loss: 2.2668, valid accuracy: 0.2362\n",
      "Iter-98800 train loss: 2.1930 valid loss: 2.2666, valid accuracy: 0.2362\n",
      "Iter-98900 train loss: 2.1198 valid loss: 2.2665, valid accuracy: 0.2369\n",
      "Iter-99000 train loss: 2.1179 valid loss: 2.2663, valid accuracy: 0.2362\n",
      "Iter-99100 train loss: 2.2247 valid loss: 2.2662, valid accuracy: 0.2362\n",
      "Iter-99200 train loss: 2.2806 valid loss: 2.2661, valid accuracy: 0.2362\n",
      "Iter-99300 train loss: 2.2941 valid loss: 2.2660, valid accuracy: 0.2362\n",
      "Iter-99400 train loss: 2.2500 valid loss: 2.2659, valid accuracy: 0.2362\n",
      "Iter-99500 train loss: 2.2266 valid loss: 2.2657, valid accuracy: 0.2362\n",
      "Iter-99600 train loss: 2.0810 valid loss: 2.2656, valid accuracy: 0.2369\n",
      "Iter-99700 train loss: 2.0608 valid loss: 2.2655, valid accuracy: 0.2369\n",
      "Iter-99800 train loss: 2.1470 valid loss: 2.2653, valid accuracy: 0.2362\n",
      "Iter-99900 train loss: 2.1663 valid loss: 2.2652, valid accuracy: 0.2369\n",
      "Iter-100000 train loss: 2.4086 valid loss: 2.2651, valid accuracy: 0.2369\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD7CAYAAABqvuNzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4VFX6wPHvSQg1JHQILaGprCDVAliCoqzYFQFBQFRA\nl1V0LSAuAor1B2tnBdsqTRBdRFBZxd4ABZQiTSAgvXcJkPP748xl7szcacm0ZN7P88wzt98zNzDv\nnK601gghhBAp8U6AEEKIxCABQQghBCABQQghhIsEBCGEEIAEBCGEEC4SEIQQQgBQKpY3U0pJG1ch\nhCgErbWK9j1inkPQWstLa0aMGBH3NCTKS56FPAt5FoFfsSJFRkIIIYA4BISTJ2N9RyGEEKGIeUCY\nOzfWd0xMubm58U5CwpBn4SbPwk2eReypWJZPKaX0uHGaO++M2S2FEKLYU0qhY1CpHPOAAJoY3lII\n4UdOTg55eXnxToawyc7OZsOGDT7bJSAIIaLK9SUT72QIG39/k1gFhLi0Mjp5Eg4fdq+fOGFeQggh\n4icuAeGRRyA93b3euTOcd148UiKEEMISlyIji3VrpTzXhRDRJ0VGiScpi4wsF10E774bzxQIIUq6\ngoICKlasyB9//BH2ub///jspKcnTfzemYxl5+/prqFzZva417Ntntp08CcePQ9my8UufECL2Klas\niHIVGxw+fJgyZcqQmpqKUorx48dz0003hXW9lJQUDh48WOj0WGlJBnEPfQUF7uUpU6BKFbP8wANQ\nrlx80iSEiJ+DBw9y4MABDhw4QHZ2NnPmzDm1zSkYnJThDyIm7gHBXly2bp17+fnn3csFBVK/IJLb\n8eOwbVu8UxF7ToO7DR8+nB49etCzZ08yMzOZPHkyP/74I+3ataNy5crUqVOHwYMHnwoUJ0+eJCUl\nhY0bNwLQu3dvBg8eTJcuXcjIyKBDhw4h98fYvHkzV111FVWrVuX000/nzTffPLVv/vz5tGnThszM\nTLKyshgyZAgAR48epVevXlSrVo3KlStz3nnnsWfPnkg8noiLe0CYPdu9/Mgj5j0jwzPnUL8+DBzo\n2VRViGQyejRkZcU7FYlj5syZ3Hzzzezfv5/u3buTlpbGCy+8wJ49e/juu++YO3cu48ePP3W8d7HP\n1KlTefzxx9m7dy/16tVj+PDhId23e/fuNGrUiG3btvHOO+/w4IMP8s033wBw11138eCDD7J//37W\nrl1L165dAXjzzTc5evQoW7ZsYc+ePYwbN46yCVoWHveA4MRe3Pfhh7B5M8yfb5qqrlgRv3QJES/b\nt8f+nkpF5hUN559/Pl26dAGgTJkytGnThrPPPhulFDk5OfTv35+vvvrq1PHeuYyuXbvSqlUrUlNT\n6dWrF0uWLAl6z/Xr17Nw4UKeeuop0tLSaNWqFf369WPixIkAlC5dmjVr1rBnzx4qVKjA2WefDUBa\nWhq7du1i9erVKKVo3bo15cuXj9SjiKiEDAh2V1/tub53b+DjhwyBOnWilx4hkoXWkXlFQ7169TzW\nV61axZVXXklWVhaZmZmMGDGCXbt2+T2/Vq1ap5bLly/PoUOHgt5z69atVKtWzePXfXZ2Nps3bwZM\nTmD58uWcfvrpnHfeeXz88ccA3HLLLXTq1Ilu3bpRr149hg0bRoG9CCSBJHxA8Gb/B6Y1fPaZ5/5v\nv4UtW8zy3r0mqy2EKFm8i4AGDhxI8+bNWbduHfv372fUqFER72NRu3Ztdu3axdGjR09t27hxI3Vc\nv0CbNGnC1KlT2blzJ//4xz+44YYbyM/PJy0tjUceeYQVK1bw7bff8v777zN58uSIpi1Sil1AsNu8\nGS69FFatcv4l8skn4F00qDVs2hSb9AkhYuPgwYNkZmZSrlw5fvvtN4/6g6KyAktOTg5t27Zl2LBh\n5Ofns2TJEt5880169+4NwKRJk9i9ezcAGRkZpKSkkJKSwhdffMHy5cvRWpOenk5aWlrC9m0Imiql\nVF2l1OdKqeVKqaVKqbv9HJerlFqslFqmlPoi8kk1Jkww9QiLF7srns84Az79NLTzZ80yldT+HDok\nAaMkKyiQeqjiJNQ+AGPHjuU///kPGRkZ3HnnnfTo0cPvdcLtV2A/ftq0aaxevZpatWrRrVs3nnrq\nKS644AIAPvroI5o2bUpmZiYPPvgg06dPp1SpUmzZsoXrr7+ezMxMmjdvzmWXXUbPnj0D3rN8edi/\nP6xkRkYIc3nWAlq6ltOBVcAZXsdkAsuBOq71an6uVeiSyLPO8t2Wl+denjlTa6217tDBrGut9ZQp\n7uW8PK0nT9b6P/9xb9uzR+sRI7SeM0ef0q2be78oeaZOLZ5/34EDI59uiuODKOGsvwlovXq1z/ao\nz90cNIegtd6mtV7iWj4E/AZ4V9v2BN7TWm92Hee/NqeQfv3Vd9v337uXd+wIfP7jj0OvXp6tHqpU\ngVGj4Mkn3dsC1EOJEqAkNl1WCo4di3cqREkQVkGWUioHaAnM99p1GlBFKfWFUmqhUqp3ZJIXmL3T\n4oABsbijEIkpPz/eKRAlQchjGSml0oEZwGBXTsH7Oq2Bi4EKwA9KqR+01mt9rzTStpzrekWOlQOY\nMwfee893e7DiwyQatkQIkbC+5IUXvqRq1djeNaSAoJQqhQkGE7XWHzgc8gewS2v9J/CnUuproAUQ\nJCBElq0XOd26wZEjUbtVkbz6KtSqBVddFe+U+Dd9OnTvbirZK1SId2qESDa53H13Lk2amLVRo0bF\n5K6hFhm9AazQWj/vZ/8HwPlKqVSlVHngXExdg6/06A3Icuut7mXvfh9WK7REyCEMGAB//3v45508\n6du81tUnJuK6dzfviRpUC0tygEL4F0qz0w5AL+BiV7PSRUqpvyqlBiqlBgBorVcCc4FfgR+BCVpr\n58Z9t1wEGeGPSx4q6z/8n3+6t9mHQS/OXwhpaTBypOe2unXhf/+LS3JEgrroIt8fRKL4iceAnqG0\nMvpOa52qtW6ptW6ltW6ttf5Eaz1eaz3BdtwYrfWZWuuztNYv+r3gov7Q70KotD5CHyE4+zgwBw6Y\n98cec2/bt8+8a+3/P9KuXTBjhnv9oYfg998jm85gtAanIVekXX3xsGZN8P/kR4/Czp1Fu8/XX5vR\nUUXxdvHF8M47sb1n7LvLfX8/fH8f9LsIqkf+m8w18KAHe5PUv/3NvFsjqwIsW2Ye/OTJ8Pnnztd9\n9lm48Ub3+lNPmfkbAtEaFizwvy9UixbBjz+GfnyiWLgQli+PdyoCW7YM1jrUdEXDaaeBa3gbvwYM\ngBo1YpMekdg2b4b334/tPePTf3rhIJj3OPTtCPUdvsEjzDUoYkBTpsBvtlqP667z3F+Y7Nvq1XDu\nueGf5+2CC6Bdu6JfJxilTJoL6/BhaNbMvX7OOeZXTiScdx688kpkrmXXvLm5dqwE6wdRiFkehZe8\nvDxSUlJODSDXpUuXUyOSBjvWW4MGDfjc36/EEih+A2r82hvenwTdb4BmU+OWDH/s8zQMHQr/+lf4\n1zhxIvD+goLgxU7ffx+5it0TJ4LXoRSlknrbtujlCObPN0Oh2508CQ8/HN51nD6/TL6UWC6//HJG\neleWAR988AFZWVkhjRRqH27io48+OjXeULBjk118R1hadym8NQ86DYULRwPx+5+5cKHv0NrHjpkv\nkKefdu4JGsKIuaesXu1bPzFlCjRuHPi8Dh2CX3vVKpg2LfhxVoDq2RPeeCP48Ylu1y544gnP+TMK\nY8+ewvVQP3myeDdSSFR9+/Zl0qRJPtsnTZpE7969E3ZguJIg/k92R3N47Uc4/QO4rg+UOhr8nCjY\ntg3+/W/PbcF6fwb7ErH/8jz9dEhN9dxvH7xqxw7zxVQYr7wCXmN5ObKGBZ861bPPRmHk54NrRkJ+\n/dVdWQ/mi9ISzV/fVsOAjIyiX2vxYt9tK1eaEXP9CZYDFIVz7bXXsnv3br799ttT2/bt28fs2bPp\n06cPYH71t27dmszMTLKzswO20+/YsSNvuH4BFRQUcP/991O9enUaN27MnDlzQk5Xfn4+99xzD3Xq\n1KFu3brce++9HHfV3u/evZurrrqKypUrU7VqVS666KJT5z399NPUrVuXjIwMmjZtyhdfhD72Z6xz\nr/EPCACHsuA/X0Hqcbj1AsiI/3CjBQXw6KPBj9uyBcqWhdxc332hDqcxZw7UrAnt24eVRJ9cS7Dy\n+kaN3Mu2/2uF8tRTkJ1tllu0gAcecO+zBwdvkRxz5+WXI3ctJ7fdBpdfHt17+BNqsLF/Ydx7b2g5\nykRXtmxZbrzxRt5+++1T26ZNm0bTpk1p5qqkSk9PZ+LEiezfv585c+bwyiuvMGvWrKDXnjBhAh99\n9BG//PILP/30EzPsTQeDGD16NAsWLODXX3/ll19+YcGCBYx2TbgyduxY6tWrx+7du9mxYwdPPPEE\nAKtXr+bll1/m559/5sCBA8ydO5ecnJwwnkZshTx0RdQdLw8zpkL7MdD/HFO/sP6SuCWnoADGjPHd\nrrW7mGDlSvfsbF99ZbZrbX45168PP//se779y9K6zpVXmvctW8y2xYuhZUv/adMa1q/3/IIHsH54\nDB1qWjcFqws7ftz0bSgM17Dvp9j7fQRStqx5Pl4TXsVMJIp43nnHBO+aNQt3fk4OvPSS++/upG1b\nzybG/gZvtJeezJ4d2RZTalRkysP0iPB/5vbt25crr7ySl156idKlSzNx4kT69u17av+FF154arlZ\ns2b06NGDr776iqu9p1j08u6773LPPfdQu3ZtAB566CGPqTYDmTJlCi+//DJVXeNJjBgxgjvuuINR\no0aRlpbG1q1bWb9+PY0aNaKDKzKnpqaSn5/PsmXLqFq1KvUDjb3vYMYMU3oRK4kTEABQ8P0DsLUN\nXN/LtEb6ZhjoxMjIgCmvtope7KOtWj77zEza89NPzl8+Vp8Ipwpeqyy8VavAWcWcHHdxjZP//je0\n1kKlS5t6k0qVgh8bSYFyEBZrBM/SpaOfnnDddJPJ/b3ov7eNX1pDXp75AeEUEKx/E7/84rn9v/8N\n/15FVZgv8kjp0KED1atXZ+bMmbRt25aFCxfyX9tDWLBgAUOHDmXZsmXk5+eTn5/PjfZ24X5s2bLF\nY/rNbCubG4ItW7Z4fKFnZ2ezxfVl8MADDzBy5Eguu+wylFL079+fIUOG0KhRI5577jlGjhzJihUr\n6Ny5M2PHjiUrKyvk+/7wQ8iHFlnifNParb8YJvwEDT+Fm/8KFYKMbR1D3bp5Nk/1duml5v3w4cC/\nRrduDX6vTp18t82aFTgYvPhieE1Hi9oJysmyZe7PXpQyUKt4adgw333lyvk/7/hxqex1sn178WrW\n2rt3b9566y0mTZpE586dqV69+ql9PXv25Nprr2Xz5s3s27ePgQMHhjRlZlZWFptsM2Dl5eWFnJ7a\ntWt7HJ+Xl3cqp5Gens6YMWP4/fffmTVrFv/6179O1RX06NGDb7755tS5Q4cODfmeAH5azEZFYgYE\ngIN14O15sPkcGNgKcqI2CVtY5s4N7Tit3cUo9hyp/d9ssC+tefPCSxuYgBEOKz1ff+2831/rJe+0\nL1vm7hTo1DmwKOzzVVgCFVFZ+7ybCq9b57nNqdd3OKxn8PvvZnn7dtPQ4Pzzi3bdaDn//PgV1RVG\nnz59+Oyzz3jttdc8iosADh06ROXKlUlLS2PBggVM8eol6i84dOvWjRdeeIHNmzezd+9enn766ZDT\nc9NNNzF69Gh27drFrl27eOyxx041Z50zZw6/u9qQV6xYkVKlSpGSksLq1av54osvyM/Pp3Tp0pQr\nVy7sVlKxzB0mbkAAKCgFn4+GD96E62+GSx+EUiEWVkfR9dcHP8Y+oY+9wjna0+J99pnzdn85Euv/\nja1RhIe77vJcP3jQsxWRZdEi6NfPLNv7BuzaFThHVRiBAumaNe4c0sMPe6b13/+GpUvd6/6Kjn/8\nMbRKXatlpFWEWKuW6Yfx3XfBz402p+/DWPXIjpTs7Gzat2/PkSNHfOoGxo0bx/Dhw8nMzGT06NF0\nt0ZjdPE3ZWb//v3p3LkzLVq0oG3bttxwww0B02A/95///Cdt27blrLPOOnX+w65/7GvWrKFTp05U\nrFiRDh06MGjQIC666CKOHTvG0KFDqV69OrVr12bnzp086fQLJ1HEYlo260URptCk/A5Nt+s1fz9N\nk/NF4a8T59cLL7iXX3nF/3HWNHpFednVret8zMqV5mWtW1ORel9n/nytCwrM+qOPaj14cOjpnDHD\nPR0gaL10qdbHj2u9YYNnGv/8070MWh844Hv9b78N/FnLlPHc/vDD7n333+95/HPPudfnzvW89/Tp\nWrdv7/scrf39+2vdr59Z/vpr93W+/NL5HOu8adPM+/33Ox/TsaPz389pm337gQNaN27sXj961P+x\n7nU/CRVxY/1NfP+No7VOgCk0E8aR6jD9Pfj0GdNf4Zpbodzu4OclMFfLtKixms3OmePbKsiitWcH\nu2uv9f113KSJGYLD+qX/yCPwvGsg9FDK6k+cMOP42L34oqkctytb1rmi3u6ZZwLv9x7ULVBP8A0b\n/O8LJYcQj3oKf2nW2nP9n/+MelJECVR8AoJl1TXw8nI4VhEGnQlnTYI49nAO1913u5cDVQ4//njR\n7zVihHm/8koziqaTpk19B87z/lK1ihoK2xHLqVWRv054HTq4m1g6dQwMt44kkOeecy937uy7317c\npJT/v5f3l3E4jh41fTrs9wmkQYPQrjt2bOD99qIzISzFLyAA5FeET56HKR9Cu7HQ+zKoUswKSIOI\n1C+8UH7Feje0sGZpihTrV/0mh/6GffqY+gf7qLBW+/5q1Zz7cjgJt/VMsF7oWvsGgPXrw7tHKH76\nyQyl7kQpUxFeWF26mKHd7cO/W0Jp+iuST/EMCJYtZ8OrC2HtX+H28+CCJyBVZhsP1//9n+e6vwHu\nvAeXC5WVw7D3ybEC1cSJZipRf6PChtopJ9zWM2XK+G6zB5UnngitabA/bdo4b7fnJm6+OfjwJ07D\npysV2tAj8+bBJZeYym4hQlG8AwKYlkg/3Gf6LdT7Dga2hnpBCqJFoUQq15Kf71kcY7XScRKoN6+3\norbgsgeVYKO2au0Oak5Ba9Gi4PebPNmM4mpZssS3d/lNNzmfG2qRj5WO224L7XiR3BKsp3IR7MuB\nKbPhzHfNTDarroZ5T8KfMe6GK4J65BFT0R1p0e5xrZTnL/rXXzfv9haPwYro7rjD/77C9DsJxJ4b\nKQmj24roK/45BA8KlneDca6fd38/A9r+G1JkWMpE4j0sQzRZUxAqBa++Gvz4ceP879u4EazOsq+9\nFtr99+zxzEFY07VarH5R27bB/fc7X8NfbsU+658Tf9NoWkGrTJlslFLySqBXOENpREMJCwguf1aC\nOf+GSR/DX2bAnc3h9FkUp9ZIJVm8hk8IpUjJaRhsS2Hmrm7fHrKywDUopl+Bxqvx11PVPi/4228H\nn43N27FjG9BaM2mSxvzfMK9p09zLffp47qtZU5OVZZanTjXvrubzgObvf/c83np5t3d3OgY0HTu6\njz//fM99b76padzY83pXX+15zOuvu/fl5rq3Dxjg+zms5erVPdO2bp17X6tW7u09ezp/ltmzzfZO\nndznZWYGfgb+ntGGQG2hYyAuAaEozfTCsq0VvP0Z/G8MXDIM+l2YMENgiMS0cqXvNqtiN5QOpt49\nvletMu/jxxc+TcOH+27z7nV8113hVYKHOqy3dxHY9u3ube+9Z97DHJonoDCmCigSp5ZXe/aYJtgN\nGxbumvYWYU4/Pg4fDjzkyksvFe6+kVQycwgeFKy5Al5ZAj8PgKsGwC25kB3akLciuTjNExGJebGD\nCfdH0v/+V7T7WSPr2vtiWPyNa+XNahgQynBAVmX5wYOhzYnx8cfOf4to/pgcMsR37nKn8a78jXIb\nrIlw48amKfDHHzvv//TT4GmMtpgHBKv9t70zTkwUlDLzOL/8GyzuZ3o6970Y6kd4JDaRcKxf6YnE\ne7yj8It7IpOOe+/1HYzwp5/cy05fwFYOIZxB1y5xTW1SvbqZwjWYwg48aLX+CrcT5b59zk2AnT6/\nvXNpOLZtMzmgLl38H3P8eNGnhC2KmAcEa0KWIUNifWeXglLwS194aSX8ejNc1xd6XypNVUswe1l7\novCuM3DNDBkX3sVZ9qawTgK1pArWyurYsfCGZ/dmH4rk++/9D+boPQBjoABx8CC0bg0zZ4aXllq1\nQmuoEKqdO838H127Ru6a4UqCIiM/CtJg8a3w4ipY3h1u6Al9OkGjuUjlc8kyeXJ87msfnqM4zUMQ\nTDzGcPLWoIEZ5uTIEef91rAtlkDNbv/8M7Re6N732r499GlyQ2GNKRaoX060JVxAuOKK6Eza4ldB\nGiy6HV5cDb/0gc73wR2t4KyJ0utZFIlrpsViLdwv/zVrInPfYHUF/hrjWE10n346tM6B4fA3jlak\np7hctiyy1wtHXAPCN9/AJ5+4Wzz88ouZF7ZatTgk5mRpExDGLYXPnoSW/4HBDaHDM1B2X9DThUgW\ngYJEoB7eVrFNNL/wnn3WvRzJ8Zry8808605uvz1y94m3uPZUtmaWchppMn4UrL3cvGothvZjTWBY\nehMsuAt2nRHvBAoRM06V3f7GugrGqj8MhX2SpXg7fNh/0VRJo3TMOgWAUkqHer9EKKc8peIWOHsc\ntH4VtreAhX+D1VeaCmohRIm2b5/JIdSo4d5WUABhzoQZVLNmgXJPCq111L8VJSCEI/UYnDkd2o6H\nSutN3cOi2+FAMZqoVggRluefN0HBXlG9caPn6L2RcPrpgZpIS0BIbDWWQZvx0HwKbOoAPw00w3Dr\n1HinTAgRZdEICIFJQCge0g5Ds3dMriF9Kyy5xfRz2NM43ikTQkRJXh7Edhy62ASEhGt2arHP83vD\nDfFLR1DHK8Di2+DVBTBlDpQ5CLd2gNvaQ9tXoJyfuSKFEMVWNIZvTwQJGxAsWpuytWJh+1nwyXPw\nrz/g64fNQHqDG0C3G+CM/5o6CCFEsfdVCR0KLeEDAkApV2Oe/Hz49df4piUkBWlmQL0Z0+C5PNOE\n9bzn4f4suPp2aDBP5mgQQiSchK1DOHwY0tNNDuHQIdNprUMH6zpRTGQ0Zfxh6huavQMZm+C362HF\njZB3oTRhFaIY6d4dpk2L5R2TvFLZHhB8rxPhhMVD5d/NdJ9/mQGZG+G368yYShIchBA+JCD4DQgb\nNsCbb8Kjj0Y2fXFTab3p33Dmu5CZZ+aDXnktrOsEJ8rFO3VCiLhL8oBw4oTp6h7o8BKRU/BWaYOp\ngD7jA8haBOsuMcFhzRVwJB6DPAkh4i/JA0Jo14vYpRJTud1w2hw4Y6apiN7eAta4xlna1gIo6Q9A\nCGFIQAjhelCnTuEH2ypWSv1pmrE2+RgafwylD8OaLiZArOsExzLjnUIhRNQkSEBQStUF3gZqAgXA\nq1rrF/wcezbwPdBda/2+w/6IB4QZM0zHNXtuoWtXM29puNMSFitV1kKTOdD4E6j/HWxtZQLD+oth\n8zmm6asQooRInIBQC6iltV6ilEoHfgau0Vqv9DouBfgUOAq8EYuAMHYs3HEHVKjgDghr1phcQ1YW\n7N8fsVsltrQjkPMlNPjcFC1VXgcbzzfBYf0lpsOcLhZdToQQjhIkIPicoNRM4EWt9Tyv7YOBfOBs\nYHYsAoLdtGnQo4e7EjojI76TVcdV+V2eAaL8bljf0R0gdjdB6h+EKE4SMCAopXKAL4FmWutDtu21\ngcla645KqTeBD2MdEDZuNINNWZfv0wcmTozKrYqfjD9cwcEVIFQBbLwA8i4w7zuaSQ5CiISWYAHB\nVVz0JfCY1voDr33TgTFa6wWugDBba/2ewzX0CNug4rm5ueTm5hY+9V527oTq1e33cy+fey7Mnx+x\nWxVjGqr8DvW+g+xvIPtrKL/TDOGdd4EpatrSFk6WiXdChUhiX7pellGJExCUUqWA2cDHWuvnHfav\nsxaBasBhYIDWepbXcVHLITipUME99d306dCtm5muc+7cmCWheEjfBvW/gfrfmiBRdZWpd9jUHv5o\nB5vawcE68U6lEEksgXIISqm3gV1a63+EcGxcioycNGkCa9eaZSsgPP88rFgB48fHLBnFT+lDUHsh\n1PsB6v5g3o+XNwFiUzsTJLa1hJOl451SIZJEbAJC0EFzlFIdgF7AUqXUYkADw4BsQGutJ3idErtv\n/CA++AB274YLL3Rvu+kmU6y0di3Mm+f/3KSWnw4bOpoXABqqrnEHh1ZvmGKnra3cOYg/2sGhWnFN\nthCiaIp1x7TQ7+vOIVi3HzUKRo6MeVJKjjIHoM4Cd5Co+yMcyzD1D5vPNn0htrYx24QQRZQgOQQh\nHB3LMB3h1nUy66rAdJarvRDqLISL/wm1lsChLBMgtraBLW3M8BtHq8Q37UIIR0kTEBoHmOK4dWtY\ntCh2aSmRdArsPs28lvYy21JOQNXVJkjU/tmMyVTzVzhS1R0gtrYx9RGHa8Q3/UKI5AkIzZv7Hzm1\na1cJCFFRUAp2/sW8fulrtqkCqLLGBIisn+GCx02QOFnatGza0Ry2NzfLO8+EE2Xj+xmESCJJExC8\n1a5t3p95xgy1LWJEp8Du081raU9rI2RsNoGhxlJoOA/aPWsCx57GJgexraUJFjuawcEspKe1EJGX\nFJXKP/8Mbdp4bisogNRUGDcO9u2DYcNiniwRTKk/ofoKUxdRawnUWAbVl0PqcRMYtp8FO840yzua\nwZ+V451iIaJEKpUjxjsYAKQ4jNRw6aXw6afu9XLl4OjR6KVLBHGiLGxtbV52FXaYnETNpWYSoRZv\nQ43lcKyiKZ7a1RR2NnUvS/2EECFJioAQjDXERefOngHhnXfgmmvikyYRwOEaZpC+9Ze4t6kCMzd1\n9RVQ7TdTR3HWJLOuU2HXGe7XzqamyGpfjsxfLYRN0v9vCFSCddVVsUuHKCKdYr7g9+WYiYPcOyB9\nuxmOo9pKEyAafmpaP6VvN3UUu86AXa56Detd+k+IJJTUAWHOHMjNhRdf9NyekQEHDiTBFJ1JQZke\n1IdqQd7C7greAAAaRElEQVRFnrvSDkO1Ve5gcdpsaDfWBItjGZ4BYtfpJnDsyzE5DiFKoKSoVA7m\n88/hssvgm2+gfXszlPaJE9CgAdxyC7z1VrxTKGJKFUDFzZ7BwlqusAP2NrQFiaauoHGaVGqLKEqg\nwe0idrMEDQiBvPEG3Habe71NG9NqaexYuO+++KVLxEnaEdMc1goQ1X8zOYqqq+FEGRMY9jSGfQ1M\n4LBeh2rJnBOiCCQgJAStzciod95p1v/4wzRTPfNMKVISdtrkHqquNgMBVlpvpjK1XmUOmuIme5A4\n9WoAxyvE+wOIhCbNThOCUqa/giUry8zZLIQnBYdrmtfGC3x3lz7kFSR+N5XblddBpQ1wLNMzSOxp\nZMtdZEnuQsSE5BBCsHo1DBgAX30FJ0+6+zA45RCeeQb69fOcuU2IgFQBpG/1zFHYX2X3+c9d7Gtg\nhisXJZwUGSUcpTwDApj5FmrWNNvBVFB37CjFSSKC0g6bXIRjwFhvOuQ5FkU1hIO1pVVUiSABIeEo\nZYa88P6yL1VKAoKIF22mQPWXuyi/G/bXN6+9DT1zGvvrmSIuKY4qBiQgJJxQAsKGDZCdLQFBJIhS\nR03uolKeqcM4ldNYDxmbzERHexu6gka2qeDe18AdRKR1VIKQgJBwggUE+0eTgCCKhbTDrortPMjM\nM4Gi0gazXCkPyuyHA3VNsNiXA/uy4UA9s7y/nlmWIcpjQAJCwvEXEFJTzXYJCKLEKfWnGSPKChCn\n3jeaHEbGZjMDnhUsrMCxt4FZ3l8fjpeP96coASQgJJxIBQStJWCIEkKdhIpbPIOFVTRVKc8EjWOZ\nJjexP9sWNLLdxVRHKyPzWwQjASHhOLUygsABITMT9u/3PF4CgkgaVpPajD/cQcI7x6EK3HUWB+q5\niqXqmmVr+8nS8f4kcSYBIeHs2weVKvlub9QIjh0zvZgtEhCECFGZ/SZIWDmKzI2QuckEkcw8kwM5\nXMMEhn057iBhBZADdZMglyEBodjYu9d8yVep4t6mFPTtCzNnBg8ITZvCb7/FJq1CFDspx01dhXf9\nReZGV9DYCCknPQPFqSIpK3DUhYK0eH+SIpCAUKwpBTt2QJMmwQOC5BiEKKIy+02uItMqkrK9KuWZ\nvhqnchnZnjmMg7VNwDhcPYE78UlAKNasgNC4sZlbwc5fQMjJMf0YhBARlnLCFD15BIs8E0QqbjY5\njbL7TNA4WNuMH3WwtufrQB04WAeOVCX2xVMyuF2J1qoVLF7suS01UX+cCFHcFZRy5wr8STluchIV\nt5rgYb3qfe9a3myKrtKOuIJElgkQ++u5i6Ws9UO1Eji34Z8EhChSCtq1g7lzffc5BYTXXjPDXgRT\nqpSZwEcIEUEFaa5K6nqBj0s7YoJD+jZXHcYmM+R5zpdmPeMPqLDT9M84mOUuljr1suU+DtdIqMAh\nASFKfvgBqlWD2bNhxQp4+20zqU4gNWt6rivlPOfzli3QsqV5F0LE2PHysKeJefmTcgLK7zI5i4xN\n7lxHnQWmGa6V+yi3B45U9w0U3kVWh2Lz0aQOIYaUMjmDRYvMLGxvvGG2W3UIK1bAX/7iPr5MGdOc\n9fXXPWdt09q0TFq5MrbpF0JEWMpxSN/uGSTsRVbW9jE7pA6hJMrKCv+cW2+FwYPh0CHo3Nls++gj\n6N8f5s0L7RqtW5tAJIRIIAVprk54dYMcGJtKbBnGMIHdfjuMGWOW27c371brpAYNzPzOobrppsim\nTQhR8khAiKH+/c3MawDXXgs1avge8/777uXsbLjvPrP80UeFu2ebNmaOhq5dC3e+ECJ5SECIoQkT\n4JprzPJVV8GXX7r3XXAB1Kvnv4Oa1SQ1UJHTk0/Cp596bvvww9BaLgkhhNQhJIivvw7tuBtv9L+v\nZk0o7TUGWMWKhU+TECK5SA4hwQQbwiLQ/uxsk9OwaA3pXvOvhxp4hBDJRwJCHJV3mDfE/os+1PGN\n1q6F48fh4ovNOVdeCWl+xvGyBwwhhLCTIqM4ys6Gbds8t3XsCOvWmTGNzj3Xc9+775ovfW+NGnmu\nT58O+fkRTaoQIglIDiHOnHonN2hgAoN3DqJrV986Aiflypl5GPx59dXw0ymEKPkkICQJewfxpk3j\nlw4hROKSgFCMXXgh1KoV2WtOnuy5/vDDkb2+ECJxBQ0ISqm6SqnPlVLLlVJLlVJ3OxzTUyn1i+v1\nrVKqeXSSK+yuuAK2bg3/vHPOMaOwOunZE/Ly3OstWhQubUKI4ieUHMIJ4B9a6zOBdsAgpdQZXses\nAy7UWrcARgNSSp3A0tLMF7/l6ac999cPMGS8EKLkChoQtNbbtNZLXMuHgN+AOl7H/Ki1tiaK/NF7\nv0g89joFp+avlnLlzPt115me1kKIkiusOgSlVA7QEpgf4LDbgY8LnySRSK64wrxff70Zi0kIUXKF\nHBCUUunADGCwK6fgdExHoB8wJDLJE7FQqZL/fUqZiXh69TLrd97pe8xnn0UnXUKI2AqpY5pSqhQm\nGEzUWn/g55izgAnAX7XWe/1da+TIkaeWc3Nzyc3NDSO5orBycjxHUj3zTPM+b57p8/DxxzBlint/\nr17uFkf2AfVSHH5CXHJJxJMrRJL70vWKrVB7Kr8BrNBaP++0UylVH3gP6K21/j3QhewBQcSOUqYe\nwHLxxZ71CHW95uc4/XTn69jPefxx+Mc/gt97wACpfxAiPLmul2VUTO4aSrPTDkAv4GKl1GKl1CKl\n1F+VUgOVUq7R/RkOVAHGuY5ZEMU0iziyB47LLoOyZYOfM3589NIjhIicUFoZfae1TtVat9Rat9Ja\nt9Zaf6K1Hq+1nuA6pr/WuqprXyut9TnRT7qIJO96hNq1nY978EHYvdss2wff0xruuSf0+3XpAg0b\nhpfGQL75JnLXEiJZSU9lAZiZ2dauda/feivs2eN7XGoqVKkS+nVzcpy3t2zpeb+i8hfAhBChk4Ag\nADNonn3UVKWgcmX/xw8a5DsmUo8enh3eggl1eG8hRGxIQBCF8tJLvh3azj3Xdywkuw0bAl9z4sTC\np0eCixBFJwFBxIX3oHyffQbdusUnLUIIQwKCiKmrrzbvgwZ5br/kktDmehBCRI8EBBE1ffr4H+7C\n3sHtllt89w8bFpUkCSECkIAgoqJOHXjrLfcX++zZ/kdRdeoE16OH7zYpUhIiuiQgiKiYOdNz/Yor\nQqv4tfomNG/u28qpTBn/50mlshBFJwFBJBSnwfOs4TGqVw/tGn36eK7f7TOlk+Q2hHAiAUFEhX3M\nI7tQv9QB3nvPvF9+ubneOSH2f//LXzzXa9b0PaZly9DTIUSykIAgYuqFF2DTptCO7djRBIJOncx6\nt27wxx/Bz7NXWNtzB4GKnIQQEhBEjJUv7zuyaqiUMpXV4XjeNj6vU05BCOEmAUHEXWEqhN95x7Ou\nwH4N7+tddZWZ8U0IEZgEBFHsaA3du0O1ap7bL7vM+fjmzd31EaHavz/4MUKUNBIQRIlx0UWRu1aF\nCkW/hnflthCJTgKCKDFuvNG8+yuC8jcUtyUz071sv8Zpp3ked+IEVK3qXn/8cefrOU03KkQik3+y\nIqFcdpmZ3jNcpUv7Dpjnbc6cwPut1k8332y+zJs3dz4uNdUzYAwb5jvZz+efO/e2tktLC7xfiFiT\ngCASyllnwbx54Z2zfj1kZUF6uud27w5q3vu9Vazoud6xY+hpeO0133OD3e/bb0O//oABwY8Roqgk\nIIhia8gQU1lsFQVZv9qt97feCnz+ww8H3v/EE7Bqlee2MWOcjw0neBTG2WdH9/pCgAQEUYzVqFG0\n5qTeOQJvFSr41h/cd595P+OM4NfPyPBcv+uu0NPmzTu3I0Q0SEAQUeFvZFMnrVpF7r4jRsCVVxb+\n/KlTYdQo5332+RrmzjXvd9zh3rZzp+fx3l/iL7zg7nUdLpkrQsRCqXgnQJQ8/sYxKuqxoRg5smjX\nDVQRbK9kLl/eFDlZA++Bb7+I1FTfa3z6qW/RVmENHw6PPVa0awhhJzkEkVRuu815++zZwc/1/gIf\nPRqqVCl6mgrriis815cujU86RMkhAUEkFXtrIHv9gPeXq513XUA4ArUOsgcY79yFP/aAVr48lCpi\nHr9Jk6KdL0oWCQgiaV13HTzzTPDjgvVf8Gb/kh4/3ne/vX7Fu9IazLAc/jRo4F5u3txUrDvd158J\nEzzXe/YMfk5h6z1E8SMBQSS1UHoTW1+6kZqVLS/Pfb2i5D4Aevd2L3v3xH7oId/jneo1grnwwvDP\nEcWTBASRlBJhnKFgHdecWEHpzz/N+1NPuSvCy5b1PPaJJzzXBw3yDQiRrtQXxZu0MhJJp7BfgsGG\nxgjHjh3Os8c9/jhceilMmxb4fPtkP088AV27eu53GujPPv6S5fzz3cutW8OiRb7HSNBIHhIQRFLr\n2zd4BzWArVsL94veH39TiQ4b5rmene0uYvKnQQPPugVwLt5ympjo0kvdyw0bOgcEkTykyEgktWrV\nQhsnqFatyAYEb/6ar3rXcRS2HmPfPrj99sDH2Dv0detWuPuI4k0CghAx4K/3c+3a5v2bb2DjRt/9\nkarIzsw01/I3iRCY3JLFHoiKWvEtig8pMhIiBvwVS02eDIcPezYftStM+f2AAZ5f7nZZWc7bAzVZ\nbd4ctm2LbB2KSEwSEISIoDFjnL9c/X2ZpqdHvijqb3+DFi3COyc313PdO1dQs6bzeRUrmvGfijJ+\nlEgcUmQkRATddx8MHuy5bffu4JPleHvpJfNe1CKjUAfFa9rUc33sWN+hv5107Bi4l7coXiQgCBFl\nVaqE/8U+aFDh72fPcTz6qP/j3nnHvB85As8+63uN006D6dOdO6Z5z8/w009w7rmFS69IHBIQhChB\n9u6FRo3c696d1SwDB7qHyChXzt1hrV0795wPYOapdprqc8EC824FujZt3E1f7XNT20Vi3CRrNFsw\nM+WJyJKAIEQC885ZBMtpVKpUtPt9/73/WeGclC/vXr7xRvPuPS+EJdypUZ0MH170awj/JCAIkYQK\n03rJKmKys1dGX3+9ua5TjgKgXj3z7q+C+vDh4GkIZewpUXjyeIUoRkIdJjsanHpXF6bS21/AsHIb\n/oqcInFvEZgEBCGKkcqV4ze20MUXw3PPhXdOs2ahHWd9pvnzYcqU8O4BzsOIi/BJQBBChCzcGeLC\n/RV/zjlQp07w4+xzWYM7oIwb53tsKGNVhdtvo6QKGhCUUnWVUp8rpZYrpZYqpe72c9wLSqk1Sqkl\nSqmWkU+qECJcTq2Mxozx7SsRLYFyM126FO6ahw/Diy8673MaZuPAgeDXfOSR8NNhn+iosO52/DaN\nn1ByCCeAf2itzwTaAYOUUmfYD1BKXQ400lo3AQYCr0Q8pUIkoaKWk996q+kjYHfffbGfD8Lpi9re\nQsmJNeeD03nevcGtwOM0oqsT74BoBadQO/IBtG8f+rHBBBt4MFaCBgSt9Tat9RLX8iHgN8A7U3cN\n8LbrmPlAplLKT1sCIUSspKWZPgKx9sQTMHq0e/3pp/0f62+KzjJlPHMYQ4cGv6/TPBBOrrrKs8Od\n1Q/jl19CO99JYee3fvVV59ntYpWLswurDkEplQO0BOZ77aoDbLKtb8Y3aAghirn27aFtW/e6vzGa\nHnoIHn7YvW4v8/fuK2HvbBZIuXKe60VtgvrVV7Bli+e2M2xlHz//bN5btw7teqtX+05jGkzFiiZ3\n0LBheOdFS8gxTSmVDswABrtyCoUy0vbXz83NJdd7VC0hRMJq1AgWLjTLe/eG3kTUkpdnvtjto7sW\ntlisbl3zK//rr81QH8uXF+46/oQaCCxVqsDMmdDSTw3qkSOexWSrV7v7Zvj6kh9//DK8BERASAFB\nKVUKEwwmaq0/cDhkM2D/aHVd23yMDPXngBDC48vyxhuhQ4f4pcVbYXpFO1XEFjYgKGWKiL7+Gu65\nJ/TzBg1y53Jq1YJJk0I7r1w5OHrUve5dX2H/HMOHw2OP+Z5vF2gojz17chk5Mpf5p8pi/EyoEWGh\nZrreAFZorZ/3s38W0AdAKXUesE9rvT0C6RMiqV1yifuLY/r04jcnwYcf+lZqF1akOqK99JI7Z6MU\n9OoV2nneHfMaN/acd8KevkcfLXzfiAsvNP1N4iGUZqcdgF7AxUqpxUqpRUqpvyqlBiqlBgBorT8C\n1iul1gLjgb9FNdVCJIlHHzVFC8VVTk7wSm3vL/poTlXqPUqrk9dfD/169maj/gLWrl1mCPRQPPoo\nPPlk6PePtKBFRlrr74DUEI77e0RSJIQo8YYONU09Z8zw/SJt1Mj/AHlFZY3SGg1lyniu338/rFgB\nVauGfo077vDNiWgdu2E6ZMY0IUTM2X8FO33ZOY3Z5G9QvFh59FG45RbnfWlpvmM09e8f9SRFnAxd\nIUQCS4YB3EL5jKNGOXfeisXzse7Rt69nUDr/fPfywYPhXTOaxWJFIQFBiAT17bfxHd00VkL5Uq9e\nvWj9DoYNK/y5TsNv9OsHZ57pXvcuLgrkkUdCm540HiQgCJGgEqmJaTT5Gw47ksLpUxDtXEetWlC7\ndnTvUVgSEIQQcbNoEZx1VvDjwh1lNVqC9b1o1gymTYtNWqJBKpWFEHHTqlXwY7ZsKXr/i1at3HM+\nF0XNmp7FPd7FSamp0K1b0e8TLxIQhBAJLSur6Ndo2BDWrSv6dSwltbJfioyEEMWWUw6jU6fI9vSN\n1wx18bivBAQhRLF17bW+X5yffgqXXmqWrYH4wtG5c/A+BPEKEtEmAUEIUeKcc46Z7MY+VHcg9iKg\n2rVhwgTn46y6jEDzOxRnEhCEECXOfffBsWOhH1+qFGzc6LzPXvz0xhumktt7eImSQgKCEELgPDfB\n/v3QooV7vUKFoldyB5s6NJ6klZEQQvjhNBd0UaxYEXhY7AoVInu/cElAEEKIGGna1P8+74pqaWUk\nhBAibiQgCCFEITVuDBdcEO9URI4EBCGECCBQ0U1mppnTuaSQgCCEEAKQgCCEEAnJqRlstElAEEKI\nBHTffbBnT2zvKQFBCCECqF8/PvdNTY3sIH2hUDqGjV2VUjqW9xNCiKIqKDBzJmdmxi8NSim01lEf\ndFsCghBCJLhYBQQpMhJCCAFIQBBCCOEiAUEIIQQgAUEIIYSLBAQhhBCABAQhhBAuEhCEEEIAEhCE\nEEK4SEAQQggBSEAQQgjhIgFBCCEEIAFBCCGEiwQEIYQQgAQEIYQQLhIQhBBCABIQhBBCuEhAEEII\nAYQQEJRSryultiulfvWzP0MpNUsptUQptVQpdUvEUymEECLqQskhvAl0DrB/ELBca90S6AiMVUqV\nikTiSrIvv/wy3klIGPIs3ORZuMmziL2gAUFr/S2wN9AhQEXXckVgt9b6RATSVqLJP3Y3eRZu8izc\n5FnEXiR+yb8EzFJKbQHSge4RuKYQQogYi0Slcmdgsda6NtAKeFkplR6B6wohhIghpbUOfpBS2cCH\nWuuzHPbNBp7UWn/nWp8HDNFa/+RwbPCbCSGE8KG1VtG+R6hFRsr1cpIHdAK+U0rVBE4D1jkdGIsP\nJIQQonCC5hCUUlOAXKAqsB0YAZQGtNZ6glIqC/gPkOU65Umt9dRoJVgIIUR0hFRkJIQQouSLWU9l\npdRflVIrlVKrlVJDYnXfaFJK1VVKfa6UWu7qlHe3a3tlpdT/lFKrlFJzlVKZtnMeUkqtUUr9ppS6\nzLa9tVLqV9fzec62vbRS6h3XOT8operH9lOGRymVopRapJSa5VpPymehlMpUSr3r+mzLlVLnJvGz\nuFcptcz1OSa70p4Uz8KpY2+sPrtSqq/r+FVKqT4hJVhrHfUXJvCsBbKBNGAJcEYs7h3lz1ULaOla\nTgdWAWcATwMPurYPAZ5yLf8FWIypu8lxPRMrlzYfONu1/BHQ2bV8JzDOtdwdeCfenzvIM7kXmATM\ncq0n5bPAFKP2cy2XAjKT8VkAtTF1iqVd69OAvsnyLIDzgZbAr7ZtUf/sQGXgd9e/u0rWctD0xuih\nnAd8bFsfimmJFPc/WIQ/50xMBftKoKZrWy1gpdPnBj4GznUds8K2vQfwb9fyJ8C5ruVUYGe8P2eA\nz18X+BRT52QFhKR7FkAG8LvD9mR8FrUxDU8qu77oZiXb/xHMD2F7QIjmZ9/hfYxr/d9A92BpjVWR\nUR1gk239D9e2EkMplYP5JfAj5o+9HUBrvQ2o4TrM+zlsdm2rg3kmFvvzOXWO1voksE8pVSUqH6Lo\nngUewPRetyTjs2gA7FJKvekqPpuglCpPEj4LrfUWYCywEfO59mutPyMJn4VNjSh+9v2uz+7vWgHJ\naKcRoExHvBnAYK31ITy/EHFYL9LtInitiFFKXQFs11ovIXAaS/yzwPwSbg28rLVuDRzG/PpLxn8X\nlYBrML+SawMVlFK9SMJnEUDCfPZYBYTNgL2ip65rW7GnzEB+M4CJWusPXJu3K9MnA6VULWCHa/tm\noJ7tdOs5+NvucY5SKhXI0FrvicJHKaoOwNVKqXXAVOBipdREYFsSPos/gE3a3TnzPUyASMZ/F52A\ndVrrPa5fsP8F2pOcz8ISi89eqO/cWAWEhUBjpVS2Uqo0pnxrVozuHW1vYMr3nrdtmwXc4lruC3xg\n297D1TKgAdAYWODKNu5XSp2jlFJAH69z+rqWbwQ+j9onKQKt9TCtdX2tdUPM3/dzrXVv4EOS71ls\nBzYppU5zbboEWE4S/rvAFBWdp5Qq6/oMlwArSK5n4d2xNxaffS5wqTKt3SoDl7q2BRbDipW/Ylrh\nrAGGxruiJ0KfqQNwEtNqajGwyPU5qwCfuT7v/4BKtnMewrQe+A24zLa9DbDU9Xyet20vA0x3bf8R\nyIn35w7huVyEu1I5KZ8F0ALzQ2gJ8D6mtUeyPosRrs/1K/AWpqVhUjwLYAqwBTiGCY79MBXsUf/s\nmKCzBlgN9AklvdIxTQghBCCVykIIIVwkIAghhAAkIAghhHCRgCCEEAKQgCCEEMJFAoIQQghAAoIQ\nQggXCQhCCCEA+H9nYaqqIIVUHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4e85ae390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAD7CAYAAAB0d9PAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRETiUqIQEZG4lChERCQuJQoREYnr/wHBKYwAvPMREwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4e88ca940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "# y_pred, y_logit = nn.test(X_test)\n",
    "# loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "# acc = np.mean(y_pred == y_test)\n",
    "# print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#     acc.mean(), acc.std(), loss))\n",
    "y_pred, _ = nn.test(X_test)\n",
    "# mplot.imsave(y_pred)\n",
    "# pd.DataFrame.to_csv(y_pred)\n",
    "# y_pred.shape\n",
    "# import numpy\n",
    "# a = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "# numpy.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "np.savetxt(X=y_pred, delimiter=\",\", fname='y_predddddddddddddddddd3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBCI-HW1\u001b[0m/                                 \u001b[01;34mDGRUs\u001b[0m/\r\n",
      "\u001b[01;34mBCI-HW2\u001b[0m/                                 \u001b[01;34mDGRUs_old\u001b[0m/\r\n",
      "\u001b[01;34mbin\u001b[0m/                                     environment.yml\r\n",
      "\u001b[01;34mbio-bp-dl\u001b[0m/                               gradient_descent.py\r\n",
      "confusion_mat_cov.ipynb                  \u001b[01;34mimpl\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/                                    \u001b[01;34mimpl_imagernn_karpathy\u001b[0m/\r\n",
      "DCNN.ipynb                               LICENSE\r\n",
      "Deep-FFNN-Tanh-FBA.ipynb                 minimal_net.ipynb\r\n",
      "Deep-FFNN-Tanh-FBA-ITD.ipynb             \u001b[01;34mmisc\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD-Kaggle.ipynb      \u001b[01;34mmisc2\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-Kaggle.ipynb          NOTES\r\n",
      "Deep-FFNN-Tanh-Vanilla.ipynb             numba-cuda-gpu-example.ipynb\r\n",
      "Deep-FFNN-Tanh-Vanilla-Kaggle.ipynb      README.md\r\n",
      "DFFNN-FBA-STDP-Tanh-diff.ipynb           \u001b[01;34mrnn-testing-platform\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-diff-no_FBA.ipynb    \u001b[01;34mtf-based\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff2.ipynb  y_predddddddddddddddddd3.csv\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff.ipynb   y_predddddddddddddddddd.csv\r\n",
      "\u001b[01;34mDFFNNs\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
