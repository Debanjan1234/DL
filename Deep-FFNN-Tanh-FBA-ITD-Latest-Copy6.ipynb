{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "#         dX = dout @ np.linalg.inv(W_fixed) # my backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward include in dcross_entropy\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "#         dy =  dy @ self.W_fixed[2].T # done\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "#             dy =  dy @ self.W_fixed[2].T # done\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[2].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_prob = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.2772 valid loss: 2.2862, valid accuracy: 0.1342\n",
      "Iter-20 train loss: 2.2749 valid loss: 2.2861, valid accuracy: 0.1348\n",
      "Iter-30 train loss: 2.3030 valid loss: 2.2860, valid accuracy: 0.1350\n",
      "Iter-40 train loss: 2.2949 valid loss: 2.2858, valid accuracy: 0.1356\n",
      "Iter-50 train loss: 2.3111 valid loss: 2.2857, valid accuracy: 0.1358\n",
      "Iter-60 train loss: 2.2718 valid loss: 2.2856, valid accuracy: 0.1360\n",
      "Iter-70 train loss: 2.2926 valid loss: 2.2855, valid accuracy: 0.1364\n",
      "Iter-80 train loss: 2.2622 valid loss: 2.2853, valid accuracy: 0.1374\n",
      "Iter-90 train loss: 2.2831 valid loss: 2.2852, valid accuracy: 0.1372\n",
      "Iter-100 train loss: 2.2656 valid loss: 2.2851, valid accuracy: 0.1380\n",
      "Iter-110 train loss: 2.2801 valid loss: 2.2849, valid accuracy: 0.1396\n",
      "Iter-120 train loss: 2.2621 valid loss: 2.2848, valid accuracy: 0.1400\n",
      "Iter-130 train loss: 2.2726 valid loss: 2.2847, valid accuracy: 0.1404\n",
      "Iter-140 train loss: 2.2821 valid loss: 2.2846, valid accuracy: 0.1406\n",
      "Iter-150 train loss: 2.3017 valid loss: 2.2844, valid accuracy: 0.1408\n",
      "Iter-160 train loss: 2.3174 valid loss: 2.2843, valid accuracy: 0.1416\n",
      "Iter-170 train loss: 2.2895 valid loss: 2.2842, valid accuracy: 0.1420\n",
      "Iter-180 train loss: 2.3155 valid loss: 2.2841, valid accuracy: 0.1424\n",
      "Iter-190 train loss: 2.2969 valid loss: 2.2840, valid accuracy: 0.1432\n",
      "Iter-200 train loss: 2.2803 valid loss: 2.2838, valid accuracy: 0.1432\n",
      "Iter-210 train loss: 2.2797 valid loss: 2.2837, valid accuracy: 0.1436\n",
      "Iter-220 train loss: 2.2735 valid loss: 2.2836, valid accuracy: 0.1444\n",
      "Iter-230 train loss: 2.2830 valid loss: 2.2835, valid accuracy: 0.1446\n",
      "Iter-240 train loss: 2.2997 valid loss: 2.2834, valid accuracy: 0.1448\n",
      "Iter-250 train loss: 2.2492 valid loss: 2.2832, valid accuracy: 0.1452\n",
      "Iter-260 train loss: 2.2868 valid loss: 2.2831, valid accuracy: 0.1456\n",
      "Iter-270 train loss: 2.2908 valid loss: 2.2830, valid accuracy: 0.1460\n",
      "Iter-280 train loss: 2.3000 valid loss: 2.2829, valid accuracy: 0.1460\n",
      "Iter-290 train loss: 2.2943 valid loss: 2.2828, valid accuracy: 0.1460\n",
      "Iter-300 train loss: 2.2674 valid loss: 2.2826, valid accuracy: 0.1462\n",
      "Iter-310 train loss: 2.2733 valid loss: 2.2825, valid accuracy: 0.1470\n",
      "Iter-320 train loss: 2.2748 valid loss: 2.2824, valid accuracy: 0.1484\n",
      "Iter-330 train loss: 2.2940 valid loss: 2.2823, valid accuracy: 0.1484\n",
      "Iter-340 train loss: 2.2832 valid loss: 2.2821, valid accuracy: 0.1492\n",
      "Iter-350 train loss: 2.3093 valid loss: 2.2820, valid accuracy: 0.1500\n",
      "Iter-360 train loss: 2.2922 valid loss: 2.2819, valid accuracy: 0.1504\n",
      "Iter-370 train loss: 2.3126 valid loss: 2.2818, valid accuracy: 0.1514\n",
      "Iter-380 train loss: 2.2590 valid loss: 2.2816, valid accuracy: 0.1520\n",
      "Iter-390 train loss: 2.2760 valid loss: 2.2815, valid accuracy: 0.1524\n",
      "Iter-400 train loss: 2.2968 valid loss: 2.2814, valid accuracy: 0.1526\n",
      "Iter-410 train loss: 2.2822 valid loss: 2.2813, valid accuracy: 0.1534\n",
      "Iter-420 train loss: 2.2874 valid loss: 2.2811, valid accuracy: 0.1542\n",
      "Iter-430 train loss: 2.3016 valid loss: 2.2810, valid accuracy: 0.1546\n",
      "Iter-440 train loss: 2.2951 valid loss: 2.2809, valid accuracy: 0.1548\n",
      "Iter-450 train loss: 2.2976 valid loss: 2.2808, valid accuracy: 0.1550\n",
      "Iter-460 train loss: 2.2895 valid loss: 2.2806, valid accuracy: 0.1554\n",
      "Iter-470 train loss: 2.2885 valid loss: 2.2805, valid accuracy: 0.1558\n",
      "Iter-480 train loss: 2.2945 valid loss: 2.2804, valid accuracy: 0.1560\n",
      "Iter-490 train loss: 2.2654 valid loss: 2.2803, valid accuracy: 0.1560\n",
      "Iter-500 train loss: 2.2991 valid loss: 2.2802, valid accuracy: 0.1566\n",
      "Iter-510 train loss: 2.2628 valid loss: 2.2800, valid accuracy: 0.1570\n",
      "Iter-520 train loss: 2.2729 valid loss: 2.2799, valid accuracy: 0.1570\n",
      "Iter-530 train loss: 2.2655 valid loss: 2.2798, valid accuracy: 0.1576\n",
      "Iter-540 train loss: 2.2739 valid loss: 2.2797, valid accuracy: 0.1580\n",
      "Iter-550 train loss: 2.2784 valid loss: 2.2795, valid accuracy: 0.1582\n",
      "Iter-560 train loss: 2.2987 valid loss: 2.2794, valid accuracy: 0.1592\n",
      "Iter-570 train loss: 2.2963 valid loss: 2.2793, valid accuracy: 0.1596\n",
      "Iter-580 train loss: 2.2887 valid loss: 2.2792, valid accuracy: 0.1600\n",
      "Iter-590 train loss: 2.2657 valid loss: 2.2790, valid accuracy: 0.1602\n",
      "Iter-600 train loss: 2.3029 valid loss: 2.2789, valid accuracy: 0.1598\n",
      "Iter-610 train loss: 2.2712 valid loss: 2.2788, valid accuracy: 0.1608\n",
      "Iter-620 train loss: 2.2795 valid loss: 2.2787, valid accuracy: 0.1612\n",
      "Iter-630 train loss: 2.2962 valid loss: 2.2786, valid accuracy: 0.1614\n",
      "Iter-640 train loss: 2.2738 valid loss: 2.2784, valid accuracy: 0.1614\n",
      "Iter-650 train loss: 2.2580 valid loss: 2.2783, valid accuracy: 0.1624\n",
      "Iter-660 train loss: 2.2945 valid loss: 2.2782, valid accuracy: 0.1624\n",
      "Iter-670 train loss: 2.2681 valid loss: 2.2781, valid accuracy: 0.1632\n",
      "Iter-680 train loss: 2.2914 valid loss: 2.2780, valid accuracy: 0.1628\n",
      "Iter-690 train loss: 2.2877 valid loss: 2.2778, valid accuracy: 0.1638\n",
      "Iter-700 train loss: 2.2839 valid loss: 2.2777, valid accuracy: 0.1638\n",
      "Iter-710 train loss: 2.2918 valid loss: 2.2776, valid accuracy: 0.1642\n",
      "Iter-720 train loss: 2.3051 valid loss: 2.2775, valid accuracy: 0.1644\n",
      "Iter-730 train loss: 2.2589 valid loss: 2.2773, valid accuracy: 0.1652\n",
      "Iter-740 train loss: 2.2954 valid loss: 2.2772, valid accuracy: 0.1654\n",
      "Iter-750 train loss: 2.2720 valid loss: 2.2771, valid accuracy: 0.1650\n",
      "Iter-760 train loss: 2.2720 valid loss: 2.2770, valid accuracy: 0.1648\n",
      "Iter-770 train loss: 2.2838 valid loss: 2.2769, valid accuracy: 0.1656\n",
      "Iter-780 train loss: 2.3002 valid loss: 2.2767, valid accuracy: 0.1664\n",
      "Iter-790 train loss: 2.2732 valid loss: 2.2766, valid accuracy: 0.1668\n",
      "Iter-800 train loss: 2.2744 valid loss: 2.2765, valid accuracy: 0.1674\n",
      "Iter-810 train loss: 2.2617 valid loss: 2.2764, valid accuracy: 0.1678\n",
      "Iter-820 train loss: 2.2718 valid loss: 2.2762, valid accuracy: 0.1678\n",
      "Iter-830 train loss: 2.2842 valid loss: 2.2761, valid accuracy: 0.1684\n",
      "Iter-840 train loss: 2.2876 valid loss: 2.2760, valid accuracy: 0.1682\n",
      "Iter-850 train loss: 2.2803 valid loss: 2.2759, valid accuracy: 0.1690\n",
      "Iter-860 train loss: 2.2929 valid loss: 2.2758, valid accuracy: 0.1694\n",
      "Iter-870 train loss: 2.2774 valid loss: 2.2756, valid accuracy: 0.1690\n",
      "Iter-880 train loss: 2.2804 valid loss: 2.2755, valid accuracy: 0.1694\n",
      "Iter-890 train loss: 2.2686 valid loss: 2.2754, valid accuracy: 0.1698\n",
      "Iter-900 train loss: 2.2779 valid loss: 2.2752, valid accuracy: 0.1708\n",
      "Iter-910 train loss: 2.2794 valid loss: 2.2751, valid accuracy: 0.1718\n",
      "Iter-920 train loss: 2.2620 valid loss: 2.2750, valid accuracy: 0.1724\n",
      "Iter-930 train loss: 2.2744 valid loss: 2.2749, valid accuracy: 0.1728\n",
      "Iter-940 train loss: 2.2681 valid loss: 2.2748, valid accuracy: 0.1734\n",
      "Iter-950 train loss: 2.2958 valid loss: 2.2746, valid accuracy: 0.1732\n",
      "Iter-960 train loss: 2.2861 valid loss: 2.2745, valid accuracy: 0.1738\n",
      "Iter-970 train loss: 2.2762 valid loss: 2.2744, valid accuracy: 0.1748\n",
      "Iter-980 train loss: 2.2843 valid loss: 2.2743, valid accuracy: 0.1756\n",
      "Iter-990 train loss: 2.2599 valid loss: 2.2741, valid accuracy: 0.1766\n",
      "Iter-1000 train loss: 2.2563 valid loss: 2.2740, valid accuracy: 0.1766\n",
      "Iter-1010 train loss: 2.2812 valid loss: 2.2739, valid accuracy: 0.1764\n",
      "Iter-1020 train loss: 2.2832 valid loss: 2.2738, valid accuracy: 0.1770\n",
      "Iter-1030 train loss: 2.2663 valid loss: 2.2737, valid accuracy: 0.1778\n",
      "Iter-1040 train loss: 2.3034 valid loss: 2.2735, valid accuracy: 0.1784\n",
      "Iter-1050 train loss: 2.2584 valid loss: 2.2734, valid accuracy: 0.1792\n",
      "Iter-1060 train loss: 2.2946 valid loss: 2.2733, valid accuracy: 0.1798\n",
      "Iter-1070 train loss: 2.2658 valid loss: 2.2732, valid accuracy: 0.1808\n",
      "Iter-1080 train loss: 2.2713 valid loss: 2.2730, valid accuracy: 0.1806\n",
      "Iter-1090 train loss: 2.2544 valid loss: 2.2729, valid accuracy: 0.1810\n",
      "Iter-1100 train loss: 2.2858 valid loss: 2.2728, valid accuracy: 0.1816\n",
      "Iter-1110 train loss: 2.2700 valid loss: 2.2727, valid accuracy: 0.1816\n",
      "Iter-1120 train loss: 2.2665 valid loss: 2.2726, valid accuracy: 0.1824\n",
      "Iter-1130 train loss: 2.2734 valid loss: 2.2724, valid accuracy: 0.1832\n",
      "Iter-1140 train loss: 2.2718 valid loss: 2.2723, valid accuracy: 0.1830\n",
      "Iter-1150 train loss: 2.2606 valid loss: 2.2722, valid accuracy: 0.1838\n",
      "Iter-1160 train loss: 2.2867 valid loss: 2.2721, valid accuracy: 0.1844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.2558 valid loss: 2.2719, valid accuracy: 0.1848\n",
      "Iter-1180 train loss: 2.2690 valid loss: 2.2718, valid accuracy: 0.1848\n",
      "Iter-1190 train loss: 2.2746 valid loss: 2.2717, valid accuracy: 0.1856\n",
      "Iter-1200 train loss: 2.2715 valid loss: 2.2715, valid accuracy: 0.1868\n",
      "Iter-1210 train loss: 2.2791 valid loss: 2.2714, valid accuracy: 0.1870\n",
      "Iter-1220 train loss: 2.2598 valid loss: 2.2713, valid accuracy: 0.1874\n",
      "Iter-1230 train loss: 2.2909 valid loss: 2.2712, valid accuracy: 0.1876\n",
      "Iter-1240 train loss: 2.2883 valid loss: 2.2711, valid accuracy: 0.1886\n",
      "Iter-1250 train loss: 2.2883 valid loss: 2.2709, valid accuracy: 0.1892\n",
      "Iter-1260 train loss: 2.2761 valid loss: 2.2708, valid accuracy: 0.1892\n",
      "Iter-1270 train loss: 2.2792 valid loss: 2.2707, valid accuracy: 0.1896\n",
      "Iter-1280 train loss: 2.2575 valid loss: 2.2706, valid accuracy: 0.1898\n",
      "Iter-1290 train loss: 2.2731 valid loss: 2.2705, valid accuracy: 0.1908\n",
      "Iter-1300 train loss: 2.2745 valid loss: 2.2703, valid accuracy: 0.1910\n",
      "Iter-1310 train loss: 2.2612 valid loss: 2.2702, valid accuracy: 0.1916\n",
      "Iter-1320 train loss: 2.2721 valid loss: 2.2701, valid accuracy: 0.1920\n",
      "Iter-1330 train loss: 2.2938 valid loss: 2.2700, valid accuracy: 0.1926\n",
      "Iter-1340 train loss: 2.2847 valid loss: 2.2698, valid accuracy: 0.1936\n",
      "Iter-1350 train loss: 2.2669 valid loss: 2.2697, valid accuracy: 0.1936\n",
      "Iter-1360 train loss: 2.2684 valid loss: 2.2696, valid accuracy: 0.1944\n",
      "Iter-1370 train loss: 2.2476 valid loss: 2.2695, valid accuracy: 0.1942\n",
      "Iter-1380 train loss: 2.2560 valid loss: 2.2694, valid accuracy: 0.1940\n",
      "Iter-1390 train loss: 2.2657 valid loss: 2.2693, valid accuracy: 0.1946\n",
      "Iter-1400 train loss: 2.2793 valid loss: 2.2691, valid accuracy: 0.1956\n",
      "Iter-1410 train loss: 2.2752 valid loss: 2.2690, valid accuracy: 0.1970\n",
      "Iter-1420 train loss: 2.2812 valid loss: 2.2689, valid accuracy: 0.1972\n",
      "Iter-1430 train loss: 2.2644 valid loss: 2.2688, valid accuracy: 0.1966\n",
      "Iter-1440 train loss: 2.2956 valid loss: 2.2686, valid accuracy: 0.1972\n",
      "Iter-1450 train loss: 2.2627 valid loss: 2.2685, valid accuracy: 0.1976\n",
      "Iter-1460 train loss: 2.2747 valid loss: 2.2684, valid accuracy: 0.1978\n",
      "Iter-1470 train loss: 2.2873 valid loss: 2.2683, valid accuracy: 0.1982\n",
      "Iter-1480 train loss: 2.2533 valid loss: 2.2682, valid accuracy: 0.1982\n",
      "Iter-1490 train loss: 2.2767 valid loss: 2.2680, valid accuracy: 0.1984\n",
      "Iter-1500 train loss: 2.2505 valid loss: 2.2679, valid accuracy: 0.1990\n",
      "Iter-1510 train loss: 2.2575 valid loss: 2.2678, valid accuracy: 0.1992\n",
      "Iter-1520 train loss: 2.2901 valid loss: 2.2677, valid accuracy: 0.2004\n",
      "Iter-1530 train loss: 2.2807 valid loss: 2.2675, valid accuracy: 0.2006\n",
      "Iter-1540 train loss: 2.2672 valid loss: 2.2674, valid accuracy: 0.2010\n",
      "Iter-1550 train loss: 2.2805 valid loss: 2.2673, valid accuracy: 0.2016\n",
      "Iter-1560 train loss: 2.2833 valid loss: 2.2672, valid accuracy: 0.2016\n",
      "Iter-1570 train loss: 2.2691 valid loss: 2.2671, valid accuracy: 0.2016\n",
      "Iter-1580 train loss: 2.2664 valid loss: 2.2670, valid accuracy: 0.2026\n",
      "Iter-1590 train loss: 2.2725 valid loss: 2.2668, valid accuracy: 0.2032\n",
      "Iter-1600 train loss: 2.2416 valid loss: 2.2667, valid accuracy: 0.2032\n",
      "Iter-1610 train loss: 2.2887 valid loss: 2.2666, valid accuracy: 0.2036\n",
      "Iter-1620 train loss: 2.2638 valid loss: 2.2665, valid accuracy: 0.2036\n",
      "Iter-1630 train loss: 2.2785 valid loss: 2.2664, valid accuracy: 0.2044\n",
      "Iter-1640 train loss: 2.2725 valid loss: 2.2662, valid accuracy: 0.2050\n",
      "Iter-1650 train loss: 2.2611 valid loss: 2.2661, valid accuracy: 0.2056\n",
      "Iter-1660 train loss: 2.2763 valid loss: 2.2660, valid accuracy: 0.2060\n",
      "Iter-1670 train loss: 2.2966 valid loss: 2.2659, valid accuracy: 0.2058\n",
      "Iter-1680 train loss: 2.2409 valid loss: 2.2658, valid accuracy: 0.2062\n",
      "Iter-1690 train loss: 2.2916 valid loss: 2.2656, valid accuracy: 0.2062\n",
      "Iter-1700 train loss: 2.2548 valid loss: 2.2655, valid accuracy: 0.2064\n",
      "Iter-1710 train loss: 2.2661 valid loss: 2.2654, valid accuracy: 0.2064\n",
      "Iter-1720 train loss: 2.2581 valid loss: 2.2653, valid accuracy: 0.2066\n",
      "Iter-1730 train loss: 2.2737 valid loss: 2.2652, valid accuracy: 0.2072\n",
      "Iter-1740 train loss: 2.2856 valid loss: 2.2650, valid accuracy: 0.2072\n",
      "Iter-1750 train loss: 2.2664 valid loss: 2.2649, valid accuracy: 0.2080\n",
      "Iter-1760 train loss: 2.2685 valid loss: 2.2648, valid accuracy: 0.2080\n",
      "Iter-1770 train loss: 2.2387 valid loss: 2.2647, valid accuracy: 0.2084\n",
      "Iter-1780 train loss: 2.2872 valid loss: 2.2646, valid accuracy: 0.2088\n",
      "Iter-1790 train loss: 2.2676 valid loss: 2.2645, valid accuracy: 0.2090\n",
      "Iter-1800 train loss: 2.2944 valid loss: 2.2643, valid accuracy: 0.2092\n",
      "Iter-1810 train loss: 2.2593 valid loss: 2.2642, valid accuracy: 0.2090\n",
      "Iter-1820 train loss: 2.2729 valid loss: 2.2641, valid accuracy: 0.2092\n",
      "Iter-1830 train loss: 2.2457 valid loss: 2.2640, valid accuracy: 0.2096\n",
      "Iter-1840 train loss: 2.2663 valid loss: 2.2639, valid accuracy: 0.2102\n",
      "Iter-1850 train loss: 2.2381 valid loss: 2.2637, valid accuracy: 0.2102\n",
      "Iter-1860 train loss: 2.2650 valid loss: 2.2636, valid accuracy: 0.2110\n",
      "Iter-1870 train loss: 2.2633 valid loss: 2.2635, valid accuracy: 0.2112\n",
      "Iter-1880 train loss: 2.2862 valid loss: 2.2634, valid accuracy: 0.2110\n",
      "Iter-1890 train loss: 2.2532 valid loss: 2.2633, valid accuracy: 0.2112\n",
      "Iter-1900 train loss: 2.2387 valid loss: 2.2632, valid accuracy: 0.2114\n",
      "Iter-1910 train loss: 2.2696 valid loss: 2.2630, valid accuracy: 0.2116\n",
      "Iter-1920 train loss: 2.2776 valid loss: 2.2629, valid accuracy: 0.2124\n",
      "Iter-1930 train loss: 2.2856 valid loss: 2.2628, valid accuracy: 0.2134\n",
      "Iter-1940 train loss: 2.2826 valid loss: 2.2627, valid accuracy: 0.2132\n",
      "Iter-1950 train loss: 2.2654 valid loss: 2.2626, valid accuracy: 0.2132\n",
      "Iter-1960 train loss: 2.2678 valid loss: 2.2624, valid accuracy: 0.2134\n",
      "Iter-1970 train loss: 2.2564 valid loss: 2.2623, valid accuracy: 0.2136\n",
      "Iter-1980 train loss: 2.2550 valid loss: 2.2622, valid accuracy: 0.2136\n",
      "Iter-1990 train loss: 2.2648 valid loss: 2.2621, valid accuracy: 0.2142\n",
      "Iter-2000 train loss: 2.2647 valid loss: 2.2620, valid accuracy: 0.2146\n",
      "Iter-2010 train loss: 2.2697 valid loss: 2.2619, valid accuracy: 0.2148\n",
      "Iter-2020 train loss: 2.2917 valid loss: 2.2618, valid accuracy: 0.2150\n",
      "Iter-2030 train loss: 2.2770 valid loss: 2.2616, valid accuracy: 0.2160\n",
      "Iter-2040 train loss: 2.2452 valid loss: 2.2615, valid accuracy: 0.2156\n",
      "Iter-2050 train loss: 2.2602 valid loss: 2.2614, valid accuracy: 0.2162\n",
      "Iter-2060 train loss: 2.2528 valid loss: 2.2613, valid accuracy: 0.2162\n",
      "Iter-2070 train loss: 2.2489 valid loss: 2.2612, valid accuracy: 0.2164\n",
      "Iter-2080 train loss: 2.2665 valid loss: 2.2610, valid accuracy: 0.2170\n",
      "Iter-2090 train loss: 2.2755 valid loss: 2.2609, valid accuracy: 0.2176\n",
      "Iter-2100 train loss: 2.2526 valid loss: 2.2608, valid accuracy: 0.2178\n",
      "Iter-2110 train loss: 2.2745 valid loss: 2.2607, valid accuracy: 0.2178\n",
      "Iter-2120 train loss: 2.2833 valid loss: 2.2606, valid accuracy: 0.2180\n",
      "Iter-2130 train loss: 2.2353 valid loss: 2.2605, valid accuracy: 0.2178\n",
      "Iter-2140 train loss: 2.2631 valid loss: 2.2603, valid accuracy: 0.2178\n",
      "Iter-2150 train loss: 2.2791 valid loss: 2.2602, valid accuracy: 0.2180\n",
      "Iter-2160 train loss: 2.2489 valid loss: 2.2601, valid accuracy: 0.2190\n",
      "Iter-2170 train loss: 2.2596 valid loss: 2.2600, valid accuracy: 0.2196\n",
      "Iter-2180 train loss: 2.2529 valid loss: 2.2599, valid accuracy: 0.2198\n",
      "Iter-2190 train loss: 2.2774 valid loss: 2.2597, valid accuracy: 0.2196\n",
      "Iter-2200 train loss: 2.2708 valid loss: 2.2596, valid accuracy: 0.2196\n",
      "Iter-2210 train loss: 2.2763 valid loss: 2.2595, valid accuracy: 0.2202\n",
      "Iter-2220 train loss: 2.2921 valid loss: 2.2594, valid accuracy: 0.2204\n",
      "Iter-2230 train loss: 2.2682 valid loss: 2.2593, valid accuracy: 0.2204\n",
      "Iter-2240 train loss: 2.2563 valid loss: 2.2591, valid accuracy: 0.2206\n",
      "Iter-2250 train loss: 2.2325 valid loss: 2.2590, valid accuracy: 0.2206\n",
      "Iter-2260 train loss: 2.2570 valid loss: 2.2589, valid accuracy: 0.2210\n",
      "Iter-2270 train loss: 2.2635 valid loss: 2.2588, valid accuracy: 0.2210\n",
      "Iter-2280 train loss: 2.2550 valid loss: 2.2587, valid accuracy: 0.2208\n",
      "Iter-2290 train loss: 2.2632 valid loss: 2.2586, valid accuracy: 0.2212\n",
      "Iter-2300 train loss: 2.2625 valid loss: 2.2584, valid accuracy: 0.2212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.2522 valid loss: 2.2583, valid accuracy: 0.2216\n",
      "Iter-2320 train loss: 2.2631 valid loss: 2.2582, valid accuracy: 0.2218\n",
      "Iter-2330 train loss: 2.2652 valid loss: 2.2581, valid accuracy: 0.2220\n",
      "Iter-2340 train loss: 2.2552 valid loss: 2.2580, valid accuracy: 0.2218\n",
      "Iter-2350 train loss: 2.2698 valid loss: 2.2578, valid accuracy: 0.2228\n",
      "Iter-2360 train loss: 2.2457 valid loss: 2.2577, valid accuracy: 0.2232\n",
      "Iter-2370 train loss: 2.2464 valid loss: 2.2576, valid accuracy: 0.2228\n",
      "Iter-2380 train loss: 2.2505 valid loss: 2.2575, valid accuracy: 0.2234\n",
      "Iter-2390 train loss: 2.2426 valid loss: 2.2574, valid accuracy: 0.2236\n",
      "Iter-2400 train loss: 2.2713 valid loss: 2.2573, valid accuracy: 0.2236\n",
      "Iter-2410 train loss: 2.2661 valid loss: 2.2571, valid accuracy: 0.2240\n",
      "Iter-2420 train loss: 2.2402 valid loss: 2.2570, valid accuracy: 0.2244\n",
      "Iter-2430 train loss: 2.2693 valid loss: 2.2569, valid accuracy: 0.2248\n",
      "Iter-2440 train loss: 2.2740 valid loss: 2.2568, valid accuracy: 0.2252\n",
      "Iter-2450 train loss: 2.2871 valid loss: 2.2567, valid accuracy: 0.2254\n",
      "Iter-2460 train loss: 2.2551 valid loss: 2.2566, valid accuracy: 0.2260\n",
      "Iter-2470 train loss: 2.2439 valid loss: 2.2564, valid accuracy: 0.2264\n",
      "Iter-2480 train loss: 2.2667 valid loss: 2.2563, valid accuracy: 0.2264\n",
      "Iter-2490 train loss: 2.2753 valid loss: 2.2562, valid accuracy: 0.2264\n",
      "Iter-2500 train loss: 2.2539 valid loss: 2.2561, valid accuracy: 0.2270\n",
      "Iter-2510 train loss: 2.2538 valid loss: 2.2560, valid accuracy: 0.2270\n",
      "Iter-2520 train loss: 2.2667 valid loss: 2.2559, valid accuracy: 0.2270\n",
      "Iter-2530 train loss: 2.2679 valid loss: 2.2557, valid accuracy: 0.2272\n",
      "Iter-2540 train loss: 2.2588 valid loss: 2.2556, valid accuracy: 0.2274\n",
      "Iter-2550 train loss: 2.2650 valid loss: 2.2555, valid accuracy: 0.2278\n",
      "Iter-2560 train loss: 2.2658 valid loss: 2.2554, valid accuracy: 0.2282\n",
      "Iter-2570 train loss: 2.2507 valid loss: 2.2553, valid accuracy: 0.2290\n",
      "Iter-2580 train loss: 2.2549 valid loss: 2.2551, valid accuracy: 0.2298\n",
      "Iter-2590 train loss: 2.2472 valid loss: 2.2550, valid accuracy: 0.2304\n",
      "Iter-2600 train loss: 2.2638 valid loss: 2.2549, valid accuracy: 0.2308\n",
      "Iter-2610 train loss: 2.2789 valid loss: 2.2548, valid accuracy: 0.2308\n",
      "Iter-2620 train loss: 2.2514 valid loss: 2.2547, valid accuracy: 0.2314\n",
      "Iter-2630 train loss: 2.2770 valid loss: 2.2546, valid accuracy: 0.2316\n",
      "Iter-2640 train loss: 2.2702 valid loss: 2.2544, valid accuracy: 0.2324\n",
      "Iter-2650 train loss: 2.2687 valid loss: 2.2543, valid accuracy: 0.2320\n",
      "Iter-2660 train loss: 2.2511 valid loss: 2.2542, valid accuracy: 0.2328\n",
      "Iter-2670 train loss: 2.2911 valid loss: 2.2541, valid accuracy: 0.2336\n",
      "Iter-2680 train loss: 2.2605 valid loss: 2.2540, valid accuracy: 0.2338\n",
      "Iter-2690 train loss: 2.2625 valid loss: 2.2538, valid accuracy: 0.2342\n",
      "Iter-2700 train loss: 2.2751 valid loss: 2.2537, valid accuracy: 0.2342\n",
      "Iter-2710 train loss: 2.2533 valid loss: 2.2536, valid accuracy: 0.2344\n",
      "Iter-2720 train loss: 2.2528 valid loss: 2.2535, valid accuracy: 0.2346\n",
      "Iter-2730 train loss: 2.2374 valid loss: 2.2534, valid accuracy: 0.2352\n",
      "Iter-2740 train loss: 2.2736 valid loss: 2.2533, valid accuracy: 0.2354\n",
      "Iter-2750 train loss: 2.2351 valid loss: 2.2531, valid accuracy: 0.2364\n",
      "Iter-2760 train loss: 2.2655 valid loss: 2.2530, valid accuracy: 0.2366\n",
      "Iter-2770 train loss: 2.2569 valid loss: 2.2529, valid accuracy: 0.2368\n",
      "Iter-2780 train loss: 2.2415 valid loss: 2.2528, valid accuracy: 0.2368\n",
      "Iter-2790 train loss: 2.2581 valid loss: 2.2527, valid accuracy: 0.2376\n",
      "Iter-2800 train loss: 2.2568 valid loss: 2.2525, valid accuracy: 0.2380\n",
      "Iter-2810 train loss: 2.2564 valid loss: 2.2524, valid accuracy: 0.2380\n",
      "Iter-2820 train loss: 2.2546 valid loss: 2.2523, valid accuracy: 0.2384\n",
      "Iter-2830 train loss: 2.2624 valid loss: 2.2522, valid accuracy: 0.2388\n",
      "Iter-2840 train loss: 2.2539 valid loss: 2.2521, valid accuracy: 0.2388\n",
      "Iter-2850 train loss: 2.2623 valid loss: 2.2520, valid accuracy: 0.2392\n",
      "Iter-2860 train loss: 2.2629 valid loss: 2.2518, valid accuracy: 0.2398\n",
      "Iter-2870 train loss: 2.2546 valid loss: 2.2517, valid accuracy: 0.2398\n",
      "Iter-2880 train loss: 2.2450 valid loss: 2.2516, valid accuracy: 0.2402\n",
      "Iter-2890 train loss: 2.2781 valid loss: 2.2515, valid accuracy: 0.2404\n",
      "Iter-2900 train loss: 2.2749 valid loss: 2.2514, valid accuracy: 0.2404\n",
      "Iter-2910 train loss: 2.2176 valid loss: 2.2512, valid accuracy: 0.2404\n",
      "Iter-2920 train loss: 2.2739 valid loss: 2.2511, valid accuracy: 0.2408\n",
      "Iter-2930 train loss: 2.2738 valid loss: 2.2510, valid accuracy: 0.2408\n",
      "Iter-2940 train loss: 2.2774 valid loss: 2.2509, valid accuracy: 0.2410\n",
      "Iter-2950 train loss: 2.2256 valid loss: 2.2508, valid accuracy: 0.2410\n",
      "Iter-2960 train loss: 2.2641 valid loss: 2.2507, valid accuracy: 0.2414\n",
      "Iter-2970 train loss: 2.2614 valid loss: 2.2506, valid accuracy: 0.2414\n",
      "Iter-2980 train loss: 2.2336 valid loss: 2.2504, valid accuracy: 0.2420\n",
      "Iter-2990 train loss: 2.2469 valid loss: 2.2503, valid accuracy: 0.2422\n",
      "Iter-3000 train loss: 2.2367 valid loss: 2.2502, valid accuracy: 0.2420\n",
      "Iter-3010 train loss: 2.2504 valid loss: 2.2501, valid accuracy: 0.2434\n",
      "Iter-3020 train loss: 2.2695 valid loss: 2.2500, valid accuracy: 0.2440\n",
      "Iter-3030 train loss: 2.2165 valid loss: 2.2499, valid accuracy: 0.2450\n",
      "Iter-3040 train loss: 2.2439 valid loss: 2.2498, valid accuracy: 0.2450\n",
      "Iter-3050 train loss: 2.2596 valid loss: 2.2497, valid accuracy: 0.2454\n",
      "Iter-3060 train loss: 2.2560 valid loss: 2.2495, valid accuracy: 0.2454\n",
      "Iter-3070 train loss: 2.2460 valid loss: 2.2494, valid accuracy: 0.2462\n",
      "Iter-3080 train loss: 2.2598 valid loss: 2.2493, valid accuracy: 0.2470\n",
      "Iter-3090 train loss: 2.2661 valid loss: 2.2492, valid accuracy: 0.2468\n",
      "Iter-3100 train loss: 2.2430 valid loss: 2.2491, valid accuracy: 0.2472\n",
      "Iter-3110 train loss: 2.2375 valid loss: 2.2490, valid accuracy: 0.2478\n",
      "Iter-3120 train loss: 2.2666 valid loss: 2.2489, valid accuracy: 0.2486\n",
      "Iter-3130 train loss: 2.2297 valid loss: 2.2487, valid accuracy: 0.2486\n",
      "Iter-3140 train loss: 2.2631 valid loss: 2.2486, valid accuracy: 0.2486\n",
      "Iter-3150 train loss: 2.2552 valid loss: 2.2485, valid accuracy: 0.2492\n",
      "Iter-3160 train loss: 2.2335 valid loss: 2.2484, valid accuracy: 0.2492\n",
      "Iter-3170 train loss: 2.2174 valid loss: 2.2483, valid accuracy: 0.2504\n",
      "Iter-3180 train loss: 2.2327 valid loss: 2.2481, valid accuracy: 0.2504\n",
      "Iter-3190 train loss: 2.2348 valid loss: 2.2480, valid accuracy: 0.2504\n",
      "Iter-3200 train loss: 2.2709 valid loss: 2.2479, valid accuracy: 0.2508\n",
      "Iter-3210 train loss: 2.2390 valid loss: 2.2478, valid accuracy: 0.2512\n",
      "Iter-3220 train loss: 2.2634 valid loss: 2.2477, valid accuracy: 0.2514\n",
      "Iter-3230 train loss: 2.2650 valid loss: 2.2476, valid accuracy: 0.2512\n",
      "Iter-3240 train loss: 2.2370 valid loss: 2.2475, valid accuracy: 0.2520\n",
      "Iter-3250 train loss: 2.2672 valid loss: 2.2474, valid accuracy: 0.2520\n",
      "Iter-3260 train loss: 2.2529 valid loss: 2.2472, valid accuracy: 0.2526\n",
      "Iter-3270 train loss: 2.2526 valid loss: 2.2471, valid accuracy: 0.2532\n",
      "Iter-3280 train loss: 2.2601 valid loss: 2.2470, valid accuracy: 0.2546\n",
      "Iter-3290 train loss: 2.2621 valid loss: 2.2469, valid accuracy: 0.2548\n",
      "Iter-3300 train loss: 2.2475 valid loss: 2.2468, valid accuracy: 0.2550\n",
      "Iter-3310 train loss: 2.2827 valid loss: 2.2467, valid accuracy: 0.2554\n",
      "Iter-3320 train loss: 2.2725 valid loss: 2.2466, valid accuracy: 0.2552\n",
      "Iter-3330 train loss: 2.2408 valid loss: 2.2465, valid accuracy: 0.2554\n",
      "Iter-3340 train loss: 2.2627 valid loss: 2.2464, valid accuracy: 0.2562\n",
      "Iter-3350 train loss: 2.2476 valid loss: 2.2462, valid accuracy: 0.2566\n",
      "Iter-3360 train loss: 2.2319 valid loss: 2.2461, valid accuracy: 0.2572\n",
      "Iter-3370 train loss: 2.2326 valid loss: 2.2460, valid accuracy: 0.2578\n",
      "Iter-3380 train loss: 2.2401 valid loss: 2.2459, valid accuracy: 0.2580\n",
      "Iter-3390 train loss: 2.2628 valid loss: 2.2458, valid accuracy: 0.2580\n",
      "Iter-3400 train loss: 2.2388 valid loss: 2.2456, valid accuracy: 0.2594\n",
      "Iter-3410 train loss: 2.2542 valid loss: 2.2455, valid accuracy: 0.2596\n",
      "Iter-3420 train loss: 2.2516 valid loss: 2.2454, valid accuracy: 0.2600\n",
      "Iter-3430 train loss: 2.2644 valid loss: 2.2453, valid accuracy: 0.2604\n",
      "Iter-3440 train loss: 2.2384 valid loss: 2.2452, valid accuracy: 0.2610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.2620 valid loss: 2.2451, valid accuracy: 0.2612\n",
      "Iter-3460 train loss: 2.2546 valid loss: 2.2450, valid accuracy: 0.2616\n",
      "Iter-3470 train loss: 2.2466 valid loss: 2.2449, valid accuracy: 0.2624\n",
      "Iter-3480 train loss: 2.2392 valid loss: 2.2447, valid accuracy: 0.2624\n",
      "Iter-3490 train loss: 2.2220 valid loss: 2.2446, valid accuracy: 0.2626\n",
      "Iter-3500 train loss: 2.2075 valid loss: 2.2445, valid accuracy: 0.2626\n",
      "Iter-3510 train loss: 2.2579 valid loss: 2.2444, valid accuracy: 0.2628\n",
      "Iter-3520 train loss: 2.2587 valid loss: 2.2443, valid accuracy: 0.2630\n",
      "Iter-3530 train loss: 2.2489 valid loss: 2.2442, valid accuracy: 0.2632\n",
      "Iter-3540 train loss: 2.2660 valid loss: 2.2441, valid accuracy: 0.2632\n",
      "Iter-3550 train loss: 2.2712 valid loss: 2.2440, valid accuracy: 0.2636\n",
      "Iter-3560 train loss: 2.2206 valid loss: 2.2439, valid accuracy: 0.2648\n",
      "Iter-3570 train loss: 2.2514 valid loss: 2.2437, valid accuracy: 0.2648\n",
      "Iter-3580 train loss: 2.2468 valid loss: 2.2436, valid accuracy: 0.2650\n",
      "Iter-3590 train loss: 2.2361 valid loss: 2.2435, valid accuracy: 0.2650\n",
      "Iter-3600 train loss: 2.2462 valid loss: 2.2434, valid accuracy: 0.2654\n",
      "Iter-3610 train loss: 2.2435 valid loss: 2.2433, valid accuracy: 0.2650\n",
      "Iter-3620 train loss: 2.2621 valid loss: 2.2432, valid accuracy: 0.2654\n",
      "Iter-3630 train loss: 2.2413 valid loss: 2.2431, valid accuracy: 0.2656\n",
      "Iter-3640 train loss: 2.2384 valid loss: 2.2430, valid accuracy: 0.2656\n",
      "Iter-3650 train loss: 2.2516 valid loss: 2.2428, valid accuracy: 0.2666\n",
      "Iter-3660 train loss: 2.2821 valid loss: 2.2427, valid accuracy: 0.2672\n",
      "Iter-3670 train loss: 2.2179 valid loss: 2.2426, valid accuracy: 0.2670\n",
      "Iter-3680 train loss: 2.2780 valid loss: 2.2425, valid accuracy: 0.2674\n",
      "Iter-3690 train loss: 2.2412 valid loss: 2.2424, valid accuracy: 0.2682\n",
      "Iter-3700 train loss: 2.2198 valid loss: 2.2423, valid accuracy: 0.2680\n",
      "Iter-3710 train loss: 2.2388 valid loss: 2.2422, valid accuracy: 0.2686\n",
      "Iter-3720 train loss: 2.2550 valid loss: 2.2420, valid accuracy: 0.2688\n",
      "Iter-3730 train loss: 2.2525 valid loss: 2.2419, valid accuracy: 0.2694\n",
      "Iter-3740 train loss: 2.2626 valid loss: 2.2418, valid accuracy: 0.2696\n",
      "Iter-3750 train loss: 2.2731 valid loss: 2.2417, valid accuracy: 0.2706\n",
      "Iter-3760 train loss: 2.2583 valid loss: 2.2416, valid accuracy: 0.2714\n",
      "Iter-3770 train loss: 2.2504 valid loss: 2.2415, valid accuracy: 0.2718\n",
      "Iter-3780 train loss: 2.2428 valid loss: 2.2414, valid accuracy: 0.2718\n",
      "Iter-3790 train loss: 2.2197 valid loss: 2.2412, valid accuracy: 0.2718\n",
      "Iter-3800 train loss: 2.2442 valid loss: 2.2411, valid accuracy: 0.2726\n",
      "Iter-3810 train loss: 2.2081 valid loss: 2.2410, valid accuracy: 0.2728\n",
      "Iter-3820 train loss: 2.2781 valid loss: 2.2409, valid accuracy: 0.2734\n",
      "Iter-3830 train loss: 2.2456 valid loss: 2.2408, valid accuracy: 0.2734\n",
      "Iter-3840 train loss: 2.2707 valid loss: 2.2407, valid accuracy: 0.2738\n",
      "Iter-3850 train loss: 2.2446 valid loss: 2.2406, valid accuracy: 0.2744\n",
      "Iter-3860 train loss: 2.2411 valid loss: 2.2404, valid accuracy: 0.2748\n",
      "Iter-3870 train loss: 2.2187 valid loss: 2.2403, valid accuracy: 0.2754\n",
      "Iter-3880 train loss: 2.2453 valid loss: 2.2402, valid accuracy: 0.2758\n",
      "Iter-3890 train loss: 2.2554 valid loss: 2.2401, valid accuracy: 0.2760\n",
      "Iter-3900 train loss: 2.2583 valid loss: 2.2400, valid accuracy: 0.2760\n",
      "Iter-3910 train loss: 2.2479 valid loss: 2.2399, valid accuracy: 0.2764\n",
      "Iter-3920 train loss: 2.2491 valid loss: 2.2398, valid accuracy: 0.2770\n",
      "Iter-3930 train loss: 2.2418 valid loss: 2.2397, valid accuracy: 0.2770\n",
      "Iter-3940 train loss: 2.2517 valid loss: 2.2395, valid accuracy: 0.2770\n",
      "Iter-3950 train loss: 2.2262 valid loss: 2.2394, valid accuracy: 0.2774\n",
      "Iter-3960 train loss: 2.2336 valid loss: 2.2393, valid accuracy: 0.2780\n",
      "Iter-3970 train loss: 2.2502 valid loss: 2.2392, valid accuracy: 0.2784\n",
      "Iter-3980 train loss: 2.2134 valid loss: 2.2391, valid accuracy: 0.2790\n",
      "Iter-3990 train loss: 2.2659 valid loss: 2.2390, valid accuracy: 0.2792\n",
      "Iter-4000 train loss: 2.2247 valid loss: 2.2389, valid accuracy: 0.2792\n",
      "Iter-4010 train loss: 2.2579 valid loss: 2.2388, valid accuracy: 0.2790\n",
      "Iter-4020 train loss: 2.2313 valid loss: 2.2387, valid accuracy: 0.2792\n",
      "Iter-4030 train loss: 2.2525 valid loss: 2.2386, valid accuracy: 0.2792\n",
      "Iter-4040 train loss: 2.2633 valid loss: 2.2384, valid accuracy: 0.2794\n",
      "Iter-4050 train loss: 2.2350 valid loss: 2.2383, valid accuracy: 0.2802\n",
      "Iter-4060 train loss: 2.2375 valid loss: 2.2382, valid accuracy: 0.2808\n",
      "Iter-4070 train loss: 2.2316 valid loss: 2.2381, valid accuracy: 0.2808\n",
      "Iter-4080 train loss: 2.2246 valid loss: 2.2380, valid accuracy: 0.2816\n",
      "Iter-4090 train loss: 2.2315 valid loss: 2.2379, valid accuracy: 0.2812\n",
      "Iter-4100 train loss: 2.2684 valid loss: 2.2378, valid accuracy: 0.2814\n",
      "Iter-4110 train loss: 2.2361 valid loss: 2.2377, valid accuracy: 0.2818\n",
      "Iter-4120 train loss: 2.2529 valid loss: 2.2375, valid accuracy: 0.2818\n",
      "Iter-4130 train loss: 2.2406 valid loss: 2.2374, valid accuracy: 0.2824\n",
      "Iter-4140 train loss: 2.2383 valid loss: 2.2373, valid accuracy: 0.2826\n",
      "Iter-4150 train loss: 2.2528 valid loss: 2.2372, valid accuracy: 0.2828\n",
      "Iter-4160 train loss: 2.2225 valid loss: 2.2371, valid accuracy: 0.2828\n",
      "Iter-4170 train loss: 2.2332 valid loss: 2.2370, valid accuracy: 0.2844\n",
      "Iter-4180 train loss: 2.2356 valid loss: 2.2369, valid accuracy: 0.2844\n",
      "Iter-4190 train loss: 2.2203 valid loss: 2.2367, valid accuracy: 0.2846\n",
      "Iter-4200 train loss: 2.2577 valid loss: 2.2366, valid accuracy: 0.2848\n",
      "Iter-4210 train loss: 2.2303 valid loss: 2.2365, valid accuracy: 0.2850\n",
      "Iter-4220 train loss: 2.2667 valid loss: 2.2364, valid accuracy: 0.2854\n",
      "Iter-4230 train loss: 2.2195 valid loss: 2.2363, valid accuracy: 0.2858\n",
      "Iter-4240 train loss: 2.2737 valid loss: 2.2362, valid accuracy: 0.2858\n",
      "Iter-4250 train loss: 2.2158 valid loss: 2.2361, valid accuracy: 0.2864\n",
      "Iter-4260 train loss: 2.2332 valid loss: 2.2360, valid accuracy: 0.2866\n",
      "Iter-4270 train loss: 2.2456 valid loss: 2.2359, valid accuracy: 0.2870\n",
      "Iter-4280 train loss: 2.2226 valid loss: 2.2357, valid accuracy: 0.2868\n",
      "Iter-4290 train loss: 2.2499 valid loss: 2.2356, valid accuracy: 0.2870\n",
      "Iter-4300 train loss: 2.2432 valid loss: 2.2355, valid accuracy: 0.2870\n",
      "Iter-4310 train loss: 2.2313 valid loss: 2.2354, valid accuracy: 0.2880\n",
      "Iter-4320 train loss: 2.2420 valid loss: 2.2353, valid accuracy: 0.2880\n",
      "Iter-4330 train loss: 2.2547 valid loss: 2.2352, valid accuracy: 0.2886\n",
      "Iter-4340 train loss: 2.2107 valid loss: 2.2351, valid accuracy: 0.2892\n",
      "Iter-4350 train loss: 2.2212 valid loss: 2.2349, valid accuracy: 0.2890\n",
      "Iter-4360 train loss: 2.2516 valid loss: 2.2348, valid accuracy: 0.2894\n",
      "Iter-4370 train loss: 2.2299 valid loss: 2.2347, valid accuracy: 0.2900\n",
      "Iter-4380 train loss: 2.2693 valid loss: 2.2346, valid accuracy: 0.2900\n",
      "Iter-4390 train loss: 2.2517 valid loss: 2.2345, valid accuracy: 0.2908\n",
      "Iter-4400 train loss: 2.2490 valid loss: 2.2344, valid accuracy: 0.2912\n",
      "Iter-4410 train loss: 2.2423 valid loss: 2.2343, valid accuracy: 0.2914\n",
      "Iter-4420 train loss: 2.2200 valid loss: 2.2341, valid accuracy: 0.2916\n",
      "Iter-4430 train loss: 2.2156 valid loss: 2.2340, valid accuracy: 0.2914\n",
      "Iter-4440 train loss: 2.2358 valid loss: 2.2339, valid accuracy: 0.2924\n",
      "Iter-4450 train loss: 2.2419 valid loss: 2.2338, valid accuracy: 0.2928\n",
      "Iter-4460 train loss: 2.2309 valid loss: 2.2337, valid accuracy: 0.2932\n",
      "Iter-4470 train loss: 2.2300 valid loss: 2.2336, valid accuracy: 0.2936\n",
      "Iter-4480 train loss: 2.2755 valid loss: 2.2335, valid accuracy: 0.2936\n",
      "Iter-4490 train loss: 2.2463 valid loss: 2.2334, valid accuracy: 0.2940\n",
      "Iter-4500 train loss: 2.2160 valid loss: 2.2333, valid accuracy: 0.2944\n",
      "Iter-4510 train loss: 2.2250 valid loss: 2.2331, valid accuracy: 0.2944\n",
      "Iter-4520 train loss: 2.2252 valid loss: 2.2330, valid accuracy: 0.2948\n",
      "Iter-4530 train loss: 2.2205 valid loss: 2.2329, valid accuracy: 0.2948\n",
      "Iter-4540 train loss: 2.2163 valid loss: 2.2328, valid accuracy: 0.2948\n",
      "Iter-4550 train loss: 2.2440 valid loss: 2.2327, valid accuracy: 0.2948\n",
      "Iter-4560 train loss: 2.2567 valid loss: 2.2326, valid accuracy: 0.2950\n",
      "Iter-4570 train loss: 2.2091 valid loss: 2.2325, valid accuracy: 0.2948\n",
      "Iter-4580 train loss: 2.2402 valid loss: 2.2323, valid accuracy: 0.2948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.2212 valid loss: 2.2322, valid accuracy: 0.2950\n",
      "Iter-4600 train loss: 2.2173 valid loss: 2.2321, valid accuracy: 0.2958\n",
      "Iter-4610 train loss: 2.2313 valid loss: 2.2320, valid accuracy: 0.2960\n",
      "Iter-4620 train loss: 2.2481 valid loss: 2.2319, valid accuracy: 0.2964\n",
      "Iter-4630 train loss: 2.2516 valid loss: 2.2318, valid accuracy: 0.2964\n",
      "Iter-4640 train loss: 2.2273 valid loss: 2.2317, valid accuracy: 0.2968\n",
      "Iter-4650 train loss: 2.2222 valid loss: 2.2315, valid accuracy: 0.2970\n",
      "Iter-4660 train loss: 2.2325 valid loss: 2.2314, valid accuracy: 0.2970\n",
      "Iter-4670 train loss: 2.2520 valid loss: 2.2313, valid accuracy: 0.2970\n",
      "Iter-4680 train loss: 2.2392 valid loss: 2.2312, valid accuracy: 0.2970\n",
      "Iter-4690 train loss: 2.2213 valid loss: 2.2311, valid accuracy: 0.2966\n",
      "Iter-4700 train loss: 2.2369 valid loss: 2.2310, valid accuracy: 0.2968\n",
      "Iter-4710 train loss: 2.2226 valid loss: 2.2309, valid accuracy: 0.2972\n",
      "Iter-4720 train loss: 2.2161 valid loss: 2.2308, valid accuracy: 0.2970\n",
      "Iter-4730 train loss: 2.2555 valid loss: 2.2306, valid accuracy: 0.2968\n",
      "Iter-4740 train loss: 2.2455 valid loss: 2.2305, valid accuracy: 0.2972\n",
      "Iter-4750 train loss: 2.2722 valid loss: 2.2304, valid accuracy: 0.2970\n",
      "Iter-4760 train loss: 2.2310 valid loss: 2.2303, valid accuracy: 0.2974\n",
      "Iter-4770 train loss: 2.1922 valid loss: 2.2302, valid accuracy: 0.2974\n",
      "Iter-4780 train loss: 2.2124 valid loss: 2.2301, valid accuracy: 0.2976\n",
      "Iter-4790 train loss: 2.1797 valid loss: 2.2299, valid accuracy: 0.2976\n",
      "Iter-4800 train loss: 2.2405 valid loss: 2.2298, valid accuracy: 0.2978\n",
      "Iter-4810 train loss: 2.2148 valid loss: 2.2297, valid accuracy: 0.2982\n",
      "Iter-4820 train loss: 2.2442 valid loss: 2.2296, valid accuracy: 0.2980\n",
      "Iter-4830 train loss: 2.1992 valid loss: 2.2295, valid accuracy: 0.2988\n",
      "Iter-4840 train loss: 2.2661 valid loss: 2.2294, valid accuracy: 0.2988\n",
      "Iter-4850 train loss: 2.2083 valid loss: 2.2293, valid accuracy: 0.2984\n",
      "Iter-4860 train loss: 2.2368 valid loss: 2.2292, valid accuracy: 0.2990\n",
      "Iter-4870 train loss: 2.2223 valid loss: 2.2290, valid accuracy: 0.2998\n",
      "Iter-4880 train loss: 2.2194 valid loss: 2.2289, valid accuracy: 0.3000\n",
      "Iter-4890 train loss: 2.2404 valid loss: 2.2288, valid accuracy: 0.3000\n",
      "Iter-4900 train loss: 2.2449 valid loss: 2.2287, valid accuracy: 0.3002\n",
      "Iter-4910 train loss: 2.2321 valid loss: 2.2286, valid accuracy: 0.3010\n",
      "Iter-4920 train loss: 2.2208 valid loss: 2.2285, valid accuracy: 0.3012\n",
      "Iter-4930 train loss: 2.2387 valid loss: 2.2284, valid accuracy: 0.3016\n",
      "Iter-4940 train loss: 2.2394 valid loss: 2.2282, valid accuracy: 0.3018\n",
      "Iter-4950 train loss: 2.2353 valid loss: 2.2281, valid accuracy: 0.3018\n",
      "Iter-4960 train loss: 2.2448 valid loss: 2.2280, valid accuracy: 0.3014\n",
      "Iter-4970 train loss: 2.2256 valid loss: 2.2279, valid accuracy: 0.3014\n",
      "Iter-4980 train loss: 2.2444 valid loss: 2.2278, valid accuracy: 0.3020\n",
      "Iter-4990 train loss: 2.2357 valid loss: 2.2277, valid accuracy: 0.3024\n",
      "Iter-5000 train loss: 2.2431 valid loss: 2.2276, valid accuracy: 0.3030\n",
      "Iter-5010 train loss: 2.2323 valid loss: 2.2275, valid accuracy: 0.3030\n",
      "Iter-5020 train loss: 2.2264 valid loss: 2.2273, valid accuracy: 0.3038\n",
      "Iter-5030 train loss: 2.2616 valid loss: 2.2272, valid accuracy: 0.3040\n",
      "Iter-5040 train loss: 2.2456 valid loss: 2.2272, valid accuracy: 0.3038\n",
      "Iter-5050 train loss: 2.2548 valid loss: 2.2270, valid accuracy: 0.3044\n",
      "Iter-5060 train loss: 2.2257 valid loss: 2.2269, valid accuracy: 0.3046\n",
      "Iter-5070 train loss: 2.2208 valid loss: 2.2268, valid accuracy: 0.3046\n",
      "Iter-5080 train loss: 2.2500 valid loss: 2.2267, valid accuracy: 0.3048\n",
      "Iter-5090 train loss: 2.1956 valid loss: 2.2266, valid accuracy: 0.3054\n",
      "Iter-5100 train loss: 2.2350 valid loss: 2.2265, valid accuracy: 0.3056\n",
      "Iter-5110 train loss: 2.2030 valid loss: 2.2264, valid accuracy: 0.3064\n",
      "Iter-5120 train loss: 2.1840 valid loss: 2.2262, valid accuracy: 0.3064\n",
      "Iter-5130 train loss: 2.2282 valid loss: 2.2261, valid accuracy: 0.3070\n",
      "Iter-5140 train loss: 2.2419 valid loss: 2.2260, valid accuracy: 0.3072\n",
      "Iter-5150 train loss: 2.2329 valid loss: 2.2259, valid accuracy: 0.3076\n",
      "Iter-5160 train loss: 2.2217 valid loss: 2.2258, valid accuracy: 0.3074\n",
      "Iter-5170 train loss: 2.2321 valid loss: 2.2257, valid accuracy: 0.3078\n",
      "Iter-5180 train loss: 2.2260 valid loss: 2.2256, valid accuracy: 0.3072\n",
      "Iter-5190 train loss: 2.2246 valid loss: 2.2255, valid accuracy: 0.3084\n",
      "Iter-5200 train loss: 2.2217 valid loss: 2.2254, valid accuracy: 0.3088\n",
      "Iter-5210 train loss: 2.2096 valid loss: 2.2252, valid accuracy: 0.3086\n",
      "Iter-5220 train loss: 2.2009 valid loss: 2.2251, valid accuracy: 0.3090\n",
      "Iter-5230 train loss: 2.2275 valid loss: 2.2250, valid accuracy: 0.3090\n",
      "Iter-5240 train loss: 2.1913 valid loss: 2.2249, valid accuracy: 0.3092\n",
      "Iter-5250 train loss: 2.2303 valid loss: 2.2248, valid accuracy: 0.3092\n",
      "Iter-5260 train loss: 2.2077 valid loss: 2.2247, valid accuracy: 0.3098\n",
      "Iter-5270 train loss: 2.2142 valid loss: 2.2246, valid accuracy: 0.3100\n",
      "Iter-5280 train loss: 2.2225 valid loss: 2.2245, valid accuracy: 0.3096\n",
      "Iter-5290 train loss: 2.2181 valid loss: 2.2243, valid accuracy: 0.3106\n",
      "Iter-5300 train loss: 2.2535 valid loss: 2.2242, valid accuracy: 0.3114\n",
      "Iter-5310 train loss: 2.2255 valid loss: 2.2241, valid accuracy: 0.3114\n",
      "Iter-5320 train loss: 2.2052 valid loss: 2.2240, valid accuracy: 0.3116\n",
      "Iter-5330 train loss: 2.2335 valid loss: 2.2239, valid accuracy: 0.3118\n",
      "Iter-5340 train loss: 2.2100 valid loss: 2.2238, valid accuracy: 0.3122\n",
      "Iter-5350 train loss: 2.2234 valid loss: 2.2237, valid accuracy: 0.3124\n",
      "Iter-5360 train loss: 2.2339 valid loss: 2.2236, valid accuracy: 0.3124\n",
      "Iter-5370 train loss: 2.2303 valid loss: 2.2235, valid accuracy: 0.3124\n",
      "Iter-5380 train loss: 2.2176 valid loss: 2.2233, valid accuracy: 0.3124\n",
      "Iter-5390 train loss: 2.2308 valid loss: 2.2232, valid accuracy: 0.3126\n",
      "Iter-5400 train loss: 2.2351 valid loss: 2.2231, valid accuracy: 0.3126\n",
      "Iter-5410 train loss: 2.2243 valid loss: 2.2230, valid accuracy: 0.3130\n",
      "Iter-5420 train loss: 2.2076 valid loss: 2.2229, valid accuracy: 0.3130\n",
      "Iter-5430 train loss: 2.2144 valid loss: 2.2228, valid accuracy: 0.3134\n",
      "Iter-5440 train loss: 2.2343 valid loss: 2.2227, valid accuracy: 0.3130\n",
      "Iter-5450 train loss: 2.2295 valid loss: 2.2226, valid accuracy: 0.3134\n",
      "Iter-5460 train loss: 2.2402 valid loss: 2.2225, valid accuracy: 0.3138\n",
      "Iter-5470 train loss: 2.2621 valid loss: 2.2224, valid accuracy: 0.3138\n",
      "Iter-5480 train loss: 2.2552 valid loss: 2.2222, valid accuracy: 0.3140\n",
      "Iter-5490 train loss: 2.2152 valid loss: 2.2221, valid accuracy: 0.3138\n",
      "Iter-5500 train loss: 2.2294 valid loss: 2.2220, valid accuracy: 0.3144\n",
      "Iter-5510 train loss: 2.2329 valid loss: 2.2219, valid accuracy: 0.3144\n",
      "Iter-5520 train loss: 2.2093 valid loss: 2.2218, valid accuracy: 0.3142\n",
      "Iter-5530 train loss: 2.1855 valid loss: 2.2217, valid accuracy: 0.3144\n",
      "Iter-5540 train loss: 2.2183 valid loss: 2.2216, valid accuracy: 0.3140\n",
      "Iter-5550 train loss: 2.2203 valid loss: 2.2215, valid accuracy: 0.3140\n",
      "Iter-5560 train loss: 2.2284 valid loss: 2.2214, valid accuracy: 0.3142\n",
      "Iter-5570 train loss: 2.2137 valid loss: 2.2212, valid accuracy: 0.3142\n",
      "Iter-5580 train loss: 2.2362 valid loss: 2.2211, valid accuracy: 0.3140\n",
      "Iter-5590 train loss: 2.2024 valid loss: 2.2210, valid accuracy: 0.3142\n",
      "Iter-5600 train loss: 2.2480 valid loss: 2.2209, valid accuracy: 0.3142\n",
      "Iter-5610 train loss: 2.2182 valid loss: 2.2208, valid accuracy: 0.3146\n",
      "Iter-5620 train loss: 2.2238 valid loss: 2.2207, valid accuracy: 0.3148\n",
      "Iter-5630 train loss: 2.2198 valid loss: 2.2206, valid accuracy: 0.3148\n",
      "Iter-5640 train loss: 2.2363 valid loss: 2.2205, valid accuracy: 0.3150\n",
      "Iter-5650 train loss: 2.2702 valid loss: 2.2204, valid accuracy: 0.3150\n",
      "Iter-5660 train loss: 2.2346 valid loss: 2.2203, valid accuracy: 0.3160\n",
      "Iter-5670 train loss: 2.2529 valid loss: 2.2202, valid accuracy: 0.3160\n",
      "Iter-5680 train loss: 2.2518 valid loss: 2.2201, valid accuracy: 0.3166\n",
      "Iter-5690 train loss: 2.2296 valid loss: 2.2199, valid accuracy: 0.3164\n",
      "Iter-5700 train loss: 2.2323 valid loss: 2.2198, valid accuracy: 0.3160\n",
      "Iter-5710 train loss: 2.2205 valid loss: 2.2197, valid accuracy: 0.3164\n",
      "Iter-5720 train loss: 2.2247 valid loss: 2.2196, valid accuracy: 0.3168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.2477 valid loss: 2.2195, valid accuracy: 0.3166\n",
      "Iter-5740 train loss: 2.2567 valid loss: 2.2194, valid accuracy: 0.3172\n",
      "Iter-5750 train loss: 2.2024 valid loss: 2.2193, valid accuracy: 0.3170\n",
      "Iter-5760 train loss: 2.2410 valid loss: 2.2192, valid accuracy: 0.3172\n",
      "Iter-5770 train loss: 2.2327 valid loss: 2.2191, valid accuracy: 0.3178\n",
      "Iter-5780 train loss: 2.2389 valid loss: 2.2189, valid accuracy: 0.3176\n",
      "Iter-5790 train loss: 2.2559 valid loss: 2.2188, valid accuracy: 0.3188\n",
      "Iter-5800 train loss: 2.2474 valid loss: 2.2187, valid accuracy: 0.3196\n",
      "Iter-5810 train loss: 2.2471 valid loss: 2.2186, valid accuracy: 0.3200\n",
      "Iter-5820 train loss: 2.2229 valid loss: 2.2185, valid accuracy: 0.3202\n",
      "Iter-5830 train loss: 2.2101 valid loss: 2.2184, valid accuracy: 0.3202\n",
      "Iter-5840 train loss: 2.2281 valid loss: 2.2183, valid accuracy: 0.3208\n",
      "Iter-5850 train loss: 2.1969 valid loss: 2.2182, valid accuracy: 0.3208\n",
      "Iter-5860 train loss: 2.2215 valid loss: 2.2181, valid accuracy: 0.3210\n",
      "Iter-5870 train loss: 2.2196 valid loss: 2.2180, valid accuracy: 0.3212\n",
      "Iter-5880 train loss: 2.1968 valid loss: 2.2178, valid accuracy: 0.3214\n",
      "Iter-5890 train loss: 2.2247 valid loss: 2.2177, valid accuracy: 0.3212\n",
      "Iter-5900 train loss: 2.2455 valid loss: 2.2176, valid accuracy: 0.3216\n",
      "Iter-5910 train loss: 2.2171 valid loss: 2.2175, valid accuracy: 0.3214\n",
      "Iter-5920 train loss: 2.2425 valid loss: 2.2174, valid accuracy: 0.3218\n",
      "Iter-5930 train loss: 2.2311 valid loss: 2.2173, valid accuracy: 0.3218\n",
      "Iter-5940 train loss: 2.2431 valid loss: 2.2172, valid accuracy: 0.3224\n",
      "Iter-5950 train loss: 2.2268 valid loss: 2.2171, valid accuracy: 0.3228\n",
      "Iter-5960 train loss: 2.2176 valid loss: 2.2170, valid accuracy: 0.3236\n",
      "Iter-5970 train loss: 2.2030 valid loss: 2.2169, valid accuracy: 0.3240\n",
      "Iter-5980 train loss: 2.2030 valid loss: 2.2168, valid accuracy: 0.3238\n",
      "Iter-5990 train loss: 2.1993 valid loss: 2.2166, valid accuracy: 0.3244\n",
      "Iter-6000 train loss: 2.2165 valid loss: 2.2165, valid accuracy: 0.3250\n",
      "Iter-6010 train loss: 2.2454 valid loss: 2.2164, valid accuracy: 0.3248\n",
      "Iter-6020 train loss: 2.2469 valid loss: 2.2163, valid accuracy: 0.3250\n",
      "Iter-6030 train loss: 2.2346 valid loss: 2.2162, valid accuracy: 0.3246\n",
      "Iter-6040 train loss: 2.2131 valid loss: 2.2161, valid accuracy: 0.3248\n",
      "Iter-6050 train loss: 2.2187 valid loss: 2.2160, valid accuracy: 0.3250\n",
      "Iter-6060 train loss: 2.2093 valid loss: 2.2159, valid accuracy: 0.3254\n",
      "Iter-6070 train loss: 2.2038 valid loss: 2.2158, valid accuracy: 0.3256\n",
      "Iter-6080 train loss: 2.2048 valid loss: 2.2157, valid accuracy: 0.3258\n",
      "Iter-6090 train loss: 2.2238 valid loss: 2.2156, valid accuracy: 0.3256\n",
      "Iter-6100 train loss: 2.2308 valid loss: 2.2155, valid accuracy: 0.3260\n",
      "Iter-6110 train loss: 2.2399 valid loss: 2.2153, valid accuracy: 0.3260\n",
      "Iter-6120 train loss: 2.2587 valid loss: 2.2152, valid accuracy: 0.3258\n",
      "Iter-6130 train loss: 2.1922 valid loss: 2.2151, valid accuracy: 0.3264\n",
      "Iter-6140 train loss: 2.2046 valid loss: 2.2150, valid accuracy: 0.3264\n",
      "Iter-6150 train loss: 2.2393 valid loss: 2.2149, valid accuracy: 0.3262\n",
      "Iter-6160 train loss: 2.2035 valid loss: 2.2148, valid accuracy: 0.3268\n",
      "Iter-6170 train loss: 2.2314 valid loss: 2.2147, valid accuracy: 0.3272\n",
      "Iter-6180 train loss: 2.2044 valid loss: 2.2146, valid accuracy: 0.3274\n",
      "Iter-6190 train loss: 2.2347 valid loss: 2.2145, valid accuracy: 0.3272\n",
      "Iter-6200 train loss: 2.2335 valid loss: 2.2144, valid accuracy: 0.3276\n",
      "Iter-6210 train loss: 2.1942 valid loss: 2.2143, valid accuracy: 0.3276\n",
      "Iter-6220 train loss: 2.2264 valid loss: 2.2141, valid accuracy: 0.3280\n",
      "Iter-6230 train loss: 2.2510 valid loss: 2.2140, valid accuracy: 0.3276\n",
      "Iter-6240 train loss: 2.2211 valid loss: 2.2139, valid accuracy: 0.3278\n",
      "Iter-6250 train loss: 2.2151 valid loss: 2.2138, valid accuracy: 0.3280\n",
      "Iter-6260 train loss: 2.2289 valid loss: 2.2137, valid accuracy: 0.3286\n",
      "Iter-6270 train loss: 2.2143 valid loss: 2.2136, valid accuracy: 0.3284\n",
      "Iter-6280 train loss: 2.2249 valid loss: 2.2135, valid accuracy: 0.3292\n",
      "Iter-6290 train loss: 2.2191 valid loss: 2.2134, valid accuracy: 0.3292\n",
      "Iter-6300 train loss: 2.2090 valid loss: 2.2132, valid accuracy: 0.3300\n",
      "Iter-6310 train loss: 2.2251 valid loss: 2.2131, valid accuracy: 0.3296\n",
      "Iter-6320 train loss: 2.2235 valid loss: 2.2130, valid accuracy: 0.3298\n",
      "Iter-6330 train loss: 2.1874 valid loss: 2.2129, valid accuracy: 0.3308\n",
      "Iter-6340 train loss: 2.1989 valid loss: 2.2128, valid accuracy: 0.3308\n",
      "Iter-6350 train loss: 2.2468 valid loss: 2.2127, valid accuracy: 0.3306\n",
      "Iter-6360 train loss: 2.2233 valid loss: 2.2126, valid accuracy: 0.3310\n",
      "Iter-6370 train loss: 2.1955 valid loss: 2.2125, valid accuracy: 0.3310\n",
      "Iter-6380 train loss: 2.2296 valid loss: 2.2124, valid accuracy: 0.3308\n",
      "Iter-6390 train loss: 2.2099 valid loss: 2.2123, valid accuracy: 0.3310\n",
      "Iter-6400 train loss: 2.1786 valid loss: 2.2121, valid accuracy: 0.3306\n",
      "Iter-6410 train loss: 2.2009 valid loss: 2.2120, valid accuracy: 0.3310\n",
      "Iter-6420 train loss: 2.2195 valid loss: 2.2119, valid accuracy: 0.3310\n",
      "Iter-6430 train loss: 2.1989 valid loss: 2.2118, valid accuracy: 0.3316\n",
      "Iter-6440 train loss: 2.2251 valid loss: 2.2117, valid accuracy: 0.3318\n",
      "Iter-6450 train loss: 2.2179 valid loss: 2.2116, valid accuracy: 0.3320\n",
      "Iter-6460 train loss: 2.1818 valid loss: 2.2115, valid accuracy: 0.3320\n",
      "Iter-6470 train loss: 2.2370 valid loss: 2.2114, valid accuracy: 0.3322\n",
      "Iter-6480 train loss: 2.1937 valid loss: 2.2113, valid accuracy: 0.3326\n",
      "Iter-6490 train loss: 2.1971 valid loss: 2.2112, valid accuracy: 0.3326\n",
      "Iter-6500 train loss: 2.2285 valid loss: 2.2111, valid accuracy: 0.3328\n",
      "Iter-6510 train loss: 2.2209 valid loss: 2.2110, valid accuracy: 0.3326\n",
      "Iter-6520 train loss: 2.2303 valid loss: 2.2108, valid accuracy: 0.3324\n",
      "Iter-6530 train loss: 2.1985 valid loss: 2.2107, valid accuracy: 0.3328\n",
      "Iter-6540 train loss: 2.2402 valid loss: 2.2106, valid accuracy: 0.3328\n",
      "Iter-6550 train loss: 2.2520 valid loss: 2.2105, valid accuracy: 0.3332\n",
      "Iter-6560 train loss: 2.2021 valid loss: 2.2104, valid accuracy: 0.3330\n",
      "Iter-6570 train loss: 2.1918 valid loss: 2.2103, valid accuracy: 0.3332\n",
      "Iter-6580 train loss: 2.2313 valid loss: 2.2102, valid accuracy: 0.3330\n",
      "Iter-6590 train loss: 2.2153 valid loss: 2.2101, valid accuracy: 0.3330\n",
      "Iter-6600 train loss: 2.1943 valid loss: 2.2100, valid accuracy: 0.3332\n",
      "Iter-6610 train loss: 2.2337 valid loss: 2.2099, valid accuracy: 0.3336\n",
      "Iter-6620 train loss: 2.2309 valid loss: 2.2098, valid accuracy: 0.3336\n",
      "Iter-6630 train loss: 2.2393 valid loss: 2.2097, valid accuracy: 0.3338\n",
      "Iter-6640 train loss: 2.2252 valid loss: 2.2095, valid accuracy: 0.3342\n",
      "Iter-6650 train loss: 2.2088 valid loss: 2.2094, valid accuracy: 0.3342\n",
      "Iter-6660 train loss: 2.1646 valid loss: 2.2093, valid accuracy: 0.3348\n",
      "Iter-6670 train loss: 2.2228 valid loss: 2.2092, valid accuracy: 0.3350\n",
      "Iter-6680 train loss: 2.1892 valid loss: 2.2091, valid accuracy: 0.3352\n",
      "Iter-6690 train loss: 2.2292 valid loss: 2.2090, valid accuracy: 0.3348\n",
      "Iter-6700 train loss: 2.2120 valid loss: 2.2089, valid accuracy: 0.3358\n",
      "Iter-6710 train loss: 2.2206 valid loss: 2.2088, valid accuracy: 0.3358\n",
      "Iter-6720 train loss: 2.2298 valid loss: 2.2087, valid accuracy: 0.3356\n",
      "Iter-6730 train loss: 2.1875 valid loss: 2.2086, valid accuracy: 0.3360\n",
      "Iter-6740 train loss: 2.2029 valid loss: 2.2085, valid accuracy: 0.3358\n",
      "Iter-6750 train loss: 2.2129 valid loss: 2.2083, valid accuracy: 0.3366\n",
      "Iter-6760 train loss: 2.2548 valid loss: 2.2082, valid accuracy: 0.3366\n",
      "Iter-6770 train loss: 2.2093 valid loss: 2.2081, valid accuracy: 0.3370\n",
      "Iter-6780 train loss: 2.2161 valid loss: 2.2080, valid accuracy: 0.3374\n",
      "Iter-6790 train loss: 2.2192 valid loss: 2.2079, valid accuracy: 0.3372\n",
      "Iter-6800 train loss: 2.1970 valid loss: 2.2078, valid accuracy: 0.3372\n",
      "Iter-6810 train loss: 2.2054 valid loss: 2.2077, valid accuracy: 0.3370\n",
      "Iter-6820 train loss: 2.1912 valid loss: 2.2076, valid accuracy: 0.3374\n",
      "Iter-6830 train loss: 2.2333 valid loss: 2.2075, valid accuracy: 0.3380\n",
      "Iter-6840 train loss: 2.2056 valid loss: 2.2074, valid accuracy: 0.3382\n",
      "Iter-6850 train loss: 2.2167 valid loss: 2.2073, valid accuracy: 0.3382\n",
      "Iter-6860 train loss: 2.2080 valid loss: 2.2072, valid accuracy: 0.3386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.1986 valid loss: 2.2071, valid accuracy: 0.3386\n",
      "Iter-6880 train loss: 2.2239 valid loss: 2.2070, valid accuracy: 0.3386\n",
      "Iter-6890 train loss: 2.2075 valid loss: 2.2069, valid accuracy: 0.3384\n",
      "Iter-6900 train loss: 2.2156 valid loss: 2.2068, valid accuracy: 0.3392\n",
      "Iter-6910 train loss: 2.2087 valid loss: 2.2067, valid accuracy: 0.3392\n",
      "Iter-6920 train loss: 2.2343 valid loss: 2.2066, valid accuracy: 0.3396\n",
      "Iter-6930 train loss: 2.2029 valid loss: 2.2064, valid accuracy: 0.3396\n",
      "Iter-6940 train loss: 2.2148 valid loss: 2.2063, valid accuracy: 0.3400\n",
      "Iter-6950 train loss: 2.2238 valid loss: 2.2062, valid accuracy: 0.3396\n",
      "Iter-6960 train loss: 2.1820 valid loss: 2.2061, valid accuracy: 0.3396\n",
      "Iter-6970 train loss: 2.2148 valid loss: 2.2060, valid accuracy: 0.3394\n",
      "Iter-6980 train loss: 2.1941 valid loss: 2.2059, valid accuracy: 0.3394\n",
      "Iter-6990 train loss: 2.2267 valid loss: 2.2058, valid accuracy: 0.3392\n",
      "Iter-7000 train loss: 2.2499 valid loss: 2.2057, valid accuracy: 0.3394\n",
      "Iter-7010 train loss: 2.1970 valid loss: 2.2056, valid accuracy: 0.3398\n",
      "Iter-7020 train loss: 2.2149 valid loss: 2.2055, valid accuracy: 0.3390\n",
      "Iter-7030 train loss: 2.1946 valid loss: 2.2054, valid accuracy: 0.3392\n",
      "Iter-7040 train loss: 2.1853 valid loss: 2.2052, valid accuracy: 0.3392\n",
      "Iter-7050 train loss: 2.2258 valid loss: 2.2051, valid accuracy: 0.3394\n",
      "Iter-7060 train loss: 2.2222 valid loss: 2.2050, valid accuracy: 0.3396\n",
      "Iter-7070 train loss: 2.2212 valid loss: 2.2049, valid accuracy: 0.3402\n",
      "Iter-7080 train loss: 2.2104 valid loss: 2.2048, valid accuracy: 0.3408\n",
      "Iter-7090 train loss: 2.2407 valid loss: 2.2047, valid accuracy: 0.3406\n",
      "Iter-7100 train loss: 2.2387 valid loss: 2.2046, valid accuracy: 0.3412\n",
      "Iter-7110 train loss: 2.2064 valid loss: 2.2045, valid accuracy: 0.3412\n",
      "Iter-7120 train loss: 2.2192 valid loss: 2.2044, valid accuracy: 0.3416\n",
      "Iter-7130 train loss: 2.2297 valid loss: 2.2043, valid accuracy: 0.3424\n",
      "Iter-7140 train loss: 2.2091 valid loss: 2.2042, valid accuracy: 0.3424\n",
      "Iter-7150 train loss: 2.2019 valid loss: 2.2041, valid accuracy: 0.3426\n",
      "Iter-7160 train loss: 2.2051 valid loss: 2.2040, valid accuracy: 0.3430\n",
      "Iter-7170 train loss: 2.2282 valid loss: 2.2039, valid accuracy: 0.3432\n",
      "Iter-7180 train loss: 2.2319 valid loss: 2.2038, valid accuracy: 0.3434\n",
      "Iter-7190 train loss: 2.2078 valid loss: 2.2037, valid accuracy: 0.3438\n",
      "Iter-7200 train loss: 2.2017 valid loss: 2.2036, valid accuracy: 0.3440\n",
      "Iter-7210 train loss: 2.1846 valid loss: 2.2034, valid accuracy: 0.3440\n",
      "Iter-7220 train loss: 2.1899 valid loss: 2.2033, valid accuracy: 0.3446\n",
      "Iter-7230 train loss: 2.1866 valid loss: 2.2032, valid accuracy: 0.3442\n",
      "Iter-7240 train loss: 2.2408 valid loss: 2.2031, valid accuracy: 0.3442\n",
      "Iter-7250 train loss: 2.2235 valid loss: 2.2030, valid accuracy: 0.3442\n",
      "Iter-7260 train loss: 2.1990 valid loss: 2.2029, valid accuracy: 0.3446\n",
      "Iter-7270 train loss: 2.2069 valid loss: 2.2028, valid accuracy: 0.3442\n",
      "Iter-7280 train loss: 2.2410 valid loss: 2.2027, valid accuracy: 0.3446\n",
      "Iter-7290 train loss: 2.2302 valid loss: 2.2026, valid accuracy: 0.3448\n",
      "Iter-7300 train loss: 2.2244 valid loss: 2.2025, valid accuracy: 0.3446\n",
      "Iter-7310 train loss: 2.2102 valid loss: 2.2024, valid accuracy: 0.3448\n",
      "Iter-7320 train loss: 2.1969 valid loss: 2.2023, valid accuracy: 0.3452\n",
      "Iter-7330 train loss: 2.1853 valid loss: 2.2022, valid accuracy: 0.3450\n",
      "Iter-7340 train loss: 2.2013 valid loss: 2.2021, valid accuracy: 0.3452\n",
      "Iter-7350 train loss: 2.2134 valid loss: 2.2020, valid accuracy: 0.3456\n",
      "Iter-7360 train loss: 2.1992 valid loss: 2.2019, valid accuracy: 0.3458\n",
      "Iter-7370 train loss: 2.1839 valid loss: 2.2018, valid accuracy: 0.3458\n",
      "Iter-7380 train loss: 2.2009 valid loss: 2.2017, valid accuracy: 0.3458\n",
      "Iter-7390 train loss: 2.1978 valid loss: 2.2015, valid accuracy: 0.3460\n",
      "Iter-7400 train loss: 2.1767 valid loss: 2.2014, valid accuracy: 0.3462\n",
      "Iter-7410 train loss: 2.2224 valid loss: 2.2013, valid accuracy: 0.3460\n",
      "Iter-7420 train loss: 2.2202 valid loss: 2.2012, valid accuracy: 0.3462\n",
      "Iter-7430 train loss: 2.2396 valid loss: 2.2011, valid accuracy: 0.3464\n",
      "Iter-7440 train loss: 2.2041 valid loss: 2.2010, valid accuracy: 0.3466\n",
      "Iter-7450 train loss: 2.1849 valid loss: 2.2009, valid accuracy: 0.3466\n",
      "Iter-7460 train loss: 2.2105 valid loss: 2.2008, valid accuracy: 0.3468\n",
      "Iter-7470 train loss: 2.2243 valid loss: 2.2007, valid accuracy: 0.3472\n",
      "Iter-7480 train loss: 2.2056 valid loss: 2.2006, valid accuracy: 0.3474\n",
      "Iter-7490 train loss: 2.2322 valid loss: 2.2005, valid accuracy: 0.3472\n",
      "Iter-7500 train loss: 2.1824 valid loss: 2.2004, valid accuracy: 0.3470\n",
      "Iter-7510 train loss: 2.2340 valid loss: 2.2003, valid accuracy: 0.3476\n",
      "Iter-7520 train loss: 2.1912 valid loss: 2.2002, valid accuracy: 0.3476\n",
      "Iter-7530 train loss: 2.1845 valid loss: 2.2001, valid accuracy: 0.3476\n",
      "Iter-7540 train loss: 2.2197 valid loss: 2.2000, valid accuracy: 0.3478\n",
      "Iter-7550 train loss: 2.2033 valid loss: 2.1999, valid accuracy: 0.3476\n",
      "Iter-7560 train loss: 2.2292 valid loss: 2.1998, valid accuracy: 0.3474\n",
      "Iter-7570 train loss: 2.2269 valid loss: 2.1997, valid accuracy: 0.3476\n",
      "Iter-7580 train loss: 2.1883 valid loss: 2.1996, valid accuracy: 0.3476\n",
      "Iter-7590 train loss: 2.2144 valid loss: 2.1995, valid accuracy: 0.3476\n",
      "Iter-7600 train loss: 2.2128 valid loss: 2.1994, valid accuracy: 0.3478\n",
      "Iter-7610 train loss: 2.1987 valid loss: 2.1993, valid accuracy: 0.3480\n",
      "Iter-7620 train loss: 2.2359 valid loss: 2.1992, valid accuracy: 0.3484\n",
      "Iter-7630 train loss: 2.2174 valid loss: 2.1991, valid accuracy: 0.3488\n",
      "Iter-7640 train loss: 2.1904 valid loss: 2.1990, valid accuracy: 0.3488\n",
      "Iter-7650 train loss: 2.1859 valid loss: 2.1989, valid accuracy: 0.3488\n",
      "Iter-7660 train loss: 2.2079 valid loss: 2.1987, valid accuracy: 0.3486\n",
      "Iter-7670 train loss: 2.2347 valid loss: 2.1986, valid accuracy: 0.3486\n",
      "Iter-7680 train loss: 2.2041 valid loss: 2.1985, valid accuracy: 0.3486\n",
      "Iter-7690 train loss: 2.2077 valid loss: 2.1984, valid accuracy: 0.3484\n",
      "Iter-7700 train loss: 2.2016 valid loss: 2.1983, valid accuracy: 0.3490\n",
      "Iter-7710 train loss: 2.1816 valid loss: 2.1982, valid accuracy: 0.3494\n",
      "Iter-7720 train loss: 2.2274 valid loss: 2.1981, valid accuracy: 0.3498\n",
      "Iter-7730 train loss: 2.2303 valid loss: 2.1980, valid accuracy: 0.3496\n",
      "Iter-7740 train loss: 2.1806 valid loss: 2.1979, valid accuracy: 0.3496\n",
      "Iter-7750 train loss: 2.2222 valid loss: 2.1978, valid accuracy: 0.3500\n",
      "Iter-7760 train loss: 2.1735 valid loss: 2.1977, valid accuracy: 0.3498\n",
      "Iter-7770 train loss: 2.2236 valid loss: 2.1976, valid accuracy: 0.3500\n",
      "Iter-7780 train loss: 2.1969 valid loss: 2.1975, valid accuracy: 0.3502\n",
      "Iter-7790 train loss: 2.1964 valid loss: 2.1974, valid accuracy: 0.3504\n",
      "Iter-7800 train loss: 2.2046 valid loss: 2.1973, valid accuracy: 0.3506\n",
      "Iter-7810 train loss: 2.1864 valid loss: 2.1972, valid accuracy: 0.3504\n",
      "Iter-7820 train loss: 2.1871 valid loss: 2.1971, valid accuracy: 0.3504\n",
      "Iter-7830 train loss: 2.2396 valid loss: 2.1970, valid accuracy: 0.3504\n",
      "Iter-7840 train loss: 2.1789 valid loss: 2.1969, valid accuracy: 0.3504\n",
      "Iter-7850 train loss: 2.2003 valid loss: 2.1968, valid accuracy: 0.3504\n",
      "Iter-7860 train loss: 2.1901 valid loss: 2.1967, valid accuracy: 0.3506\n",
      "Iter-7870 train loss: 2.2059 valid loss: 2.1966, valid accuracy: 0.3506\n",
      "Iter-7880 train loss: 2.2178 valid loss: 2.1965, valid accuracy: 0.3510\n",
      "Iter-7890 train loss: 2.2345 valid loss: 2.1964, valid accuracy: 0.3512\n",
      "Iter-7900 train loss: 2.2159 valid loss: 2.1962, valid accuracy: 0.3516\n",
      "Iter-7910 train loss: 2.2527 valid loss: 2.1961, valid accuracy: 0.3520\n",
      "Iter-7920 train loss: 2.2092 valid loss: 2.1960, valid accuracy: 0.3526\n",
      "Iter-7930 train loss: 2.2060 valid loss: 2.1959, valid accuracy: 0.3532\n",
      "Iter-7940 train loss: 2.1879 valid loss: 2.1958, valid accuracy: 0.3534\n",
      "Iter-7950 train loss: 2.2215 valid loss: 2.1957, valid accuracy: 0.3538\n",
      "Iter-7960 train loss: 2.1643 valid loss: 2.1956, valid accuracy: 0.3538\n",
      "Iter-7970 train loss: 2.1633 valid loss: 2.1955, valid accuracy: 0.3536\n",
      "Iter-7980 train loss: 2.2223 valid loss: 2.1954, valid accuracy: 0.3540\n",
      "Iter-7990 train loss: 2.1991 valid loss: 2.1953, valid accuracy: 0.3544\n",
      "Iter-8000 train loss: 2.1794 valid loss: 2.1952, valid accuracy: 0.3544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.1754 valid loss: 2.1950, valid accuracy: 0.3550\n",
      "Iter-8020 train loss: 2.2083 valid loss: 2.1949, valid accuracy: 0.3552\n",
      "Iter-8030 train loss: 2.1963 valid loss: 2.1948, valid accuracy: 0.3552\n",
      "Iter-8040 train loss: 2.1785 valid loss: 2.1947, valid accuracy: 0.3554\n",
      "Iter-8050 train loss: 2.2085 valid loss: 2.1946, valid accuracy: 0.3554\n",
      "Iter-8060 train loss: 2.2143 valid loss: 2.1945, valid accuracy: 0.3562\n",
      "Iter-8070 train loss: 2.1881 valid loss: 2.1944, valid accuracy: 0.3564\n",
      "Iter-8080 train loss: 2.1605 valid loss: 2.1943, valid accuracy: 0.3564\n",
      "Iter-8090 train loss: 2.2248 valid loss: 2.1942, valid accuracy: 0.3562\n",
      "Iter-8100 train loss: 2.1654 valid loss: 2.1941, valid accuracy: 0.3564\n",
      "Iter-8110 train loss: 2.1940 valid loss: 2.1940, valid accuracy: 0.3566\n",
      "Iter-8120 train loss: 2.1934 valid loss: 2.1939, valid accuracy: 0.3566\n",
      "Iter-8130 train loss: 2.2026 valid loss: 2.1938, valid accuracy: 0.3568\n",
      "Iter-8140 train loss: 2.2231 valid loss: 2.1937, valid accuracy: 0.3570\n",
      "Iter-8150 train loss: 2.2115 valid loss: 2.1936, valid accuracy: 0.3570\n",
      "Iter-8160 train loss: 2.2199 valid loss: 2.1935, valid accuracy: 0.3572\n",
      "Iter-8170 train loss: 2.1925 valid loss: 2.1934, valid accuracy: 0.3574\n",
      "Iter-8180 train loss: 2.1954 valid loss: 2.1933, valid accuracy: 0.3574\n",
      "Iter-8190 train loss: 2.2212 valid loss: 2.1932, valid accuracy: 0.3576\n",
      "Iter-8200 train loss: 2.2279 valid loss: 2.1931, valid accuracy: 0.3576\n",
      "Iter-8210 train loss: 2.1626 valid loss: 2.1930, valid accuracy: 0.3578\n",
      "Iter-8220 train loss: 2.1844 valid loss: 2.1929, valid accuracy: 0.3580\n",
      "Iter-8230 train loss: 2.1998 valid loss: 2.1928, valid accuracy: 0.3582\n",
      "Iter-8240 train loss: 2.1870 valid loss: 2.1926, valid accuracy: 0.3582\n",
      "Iter-8250 train loss: 2.1802 valid loss: 2.1925, valid accuracy: 0.3588\n",
      "Iter-8260 train loss: 2.2162 valid loss: 2.1924, valid accuracy: 0.3586\n",
      "Iter-8270 train loss: 2.1966 valid loss: 2.1923, valid accuracy: 0.3592\n",
      "Iter-8280 train loss: 2.1713 valid loss: 2.1922, valid accuracy: 0.3594\n",
      "Iter-8290 train loss: 2.2013 valid loss: 2.1921, valid accuracy: 0.3596\n",
      "Iter-8300 train loss: 2.1855 valid loss: 2.1920, valid accuracy: 0.3600\n",
      "Iter-8310 train loss: 2.1959 valid loss: 2.1919, valid accuracy: 0.3600\n",
      "Iter-8320 train loss: 2.2072 valid loss: 2.1918, valid accuracy: 0.3598\n",
      "Iter-8330 train loss: 2.2102 valid loss: 2.1917, valid accuracy: 0.3602\n",
      "Iter-8340 train loss: 2.1625 valid loss: 2.1916, valid accuracy: 0.3602\n",
      "Iter-8350 train loss: 2.1677 valid loss: 2.1915, valid accuracy: 0.3602\n",
      "Iter-8360 train loss: 2.1884 valid loss: 2.1914, valid accuracy: 0.3604\n",
      "Iter-8370 train loss: 2.1888 valid loss: 2.1913, valid accuracy: 0.3606\n",
      "Iter-8380 train loss: 2.1901 valid loss: 2.1912, valid accuracy: 0.3604\n",
      "Iter-8390 train loss: 2.1842 valid loss: 2.1911, valid accuracy: 0.3606\n",
      "Iter-8400 train loss: 2.2033 valid loss: 2.1909, valid accuracy: 0.3606\n",
      "Iter-8410 train loss: 2.1811 valid loss: 2.1908, valid accuracy: 0.3610\n",
      "Iter-8420 train loss: 2.1803 valid loss: 2.1907, valid accuracy: 0.3612\n",
      "Iter-8430 train loss: 2.1860 valid loss: 2.1906, valid accuracy: 0.3612\n",
      "Iter-8440 train loss: 2.1886 valid loss: 2.1905, valid accuracy: 0.3616\n",
      "Iter-8450 train loss: 2.1859 valid loss: 2.1904, valid accuracy: 0.3616\n",
      "Iter-8460 train loss: 2.1821 valid loss: 2.1903, valid accuracy: 0.3616\n",
      "Iter-8470 train loss: 2.1906 valid loss: 2.1902, valid accuracy: 0.3616\n",
      "Iter-8480 train loss: 2.1819 valid loss: 2.1901, valid accuracy: 0.3616\n",
      "Iter-8490 train loss: 2.1870 valid loss: 2.1900, valid accuracy: 0.3620\n",
      "Iter-8500 train loss: 2.1629 valid loss: 2.1899, valid accuracy: 0.3618\n",
      "Iter-8510 train loss: 2.1600 valid loss: 2.1898, valid accuracy: 0.3616\n",
      "Iter-8520 train loss: 2.2175 valid loss: 2.1897, valid accuracy: 0.3620\n",
      "Iter-8530 train loss: 2.1577 valid loss: 2.1896, valid accuracy: 0.3618\n",
      "Iter-8540 train loss: 2.1909 valid loss: 2.1895, valid accuracy: 0.3624\n",
      "Iter-8550 train loss: 2.2117 valid loss: 2.1894, valid accuracy: 0.3624\n",
      "Iter-8560 train loss: 2.1690 valid loss: 2.1893, valid accuracy: 0.3622\n",
      "Iter-8570 train loss: 2.2193 valid loss: 2.1892, valid accuracy: 0.3628\n",
      "Iter-8580 train loss: 2.1698 valid loss: 2.1891, valid accuracy: 0.3632\n",
      "Iter-8590 train loss: 2.1997 valid loss: 2.1890, valid accuracy: 0.3636\n",
      "Iter-8600 train loss: 2.1802 valid loss: 2.1889, valid accuracy: 0.3636\n",
      "Iter-8610 train loss: 2.1982 valid loss: 2.1888, valid accuracy: 0.3634\n",
      "Iter-8620 train loss: 2.1863 valid loss: 2.1887, valid accuracy: 0.3638\n",
      "Iter-8630 train loss: 2.1805 valid loss: 2.1886, valid accuracy: 0.3640\n",
      "Iter-8640 train loss: 2.1725 valid loss: 2.1885, valid accuracy: 0.3636\n",
      "Iter-8650 train loss: 2.1455 valid loss: 2.1884, valid accuracy: 0.3638\n",
      "Iter-8660 train loss: 2.1828 valid loss: 2.1883, valid accuracy: 0.3634\n",
      "Iter-8670 train loss: 2.1888 valid loss: 2.1882, valid accuracy: 0.3636\n",
      "Iter-8680 train loss: 2.1975 valid loss: 2.1881, valid accuracy: 0.3640\n",
      "Iter-8690 train loss: 2.2103 valid loss: 2.1880, valid accuracy: 0.3638\n",
      "Iter-8700 train loss: 2.2178 valid loss: 2.1878, valid accuracy: 0.3640\n",
      "Iter-8710 train loss: 2.2181 valid loss: 2.1877, valid accuracy: 0.3644\n",
      "Iter-8720 train loss: 2.1749 valid loss: 2.1876, valid accuracy: 0.3642\n",
      "Iter-8730 train loss: 2.1956 valid loss: 2.1875, valid accuracy: 0.3644\n",
      "Iter-8740 train loss: 2.1701 valid loss: 2.1874, valid accuracy: 0.3646\n",
      "Iter-8750 train loss: 2.1944 valid loss: 2.1873, valid accuracy: 0.3646\n",
      "Iter-8760 train loss: 2.2026 valid loss: 2.1872, valid accuracy: 0.3646\n",
      "Iter-8770 train loss: 2.2117 valid loss: 2.1871, valid accuracy: 0.3646\n",
      "Iter-8780 train loss: 2.1922 valid loss: 2.1870, valid accuracy: 0.3650\n",
      "Iter-8790 train loss: 2.1826 valid loss: 2.1869, valid accuracy: 0.3652\n",
      "Iter-8800 train loss: 2.1884 valid loss: 2.1868, valid accuracy: 0.3654\n",
      "Iter-8810 train loss: 2.1705 valid loss: 2.1867, valid accuracy: 0.3650\n",
      "Iter-8820 train loss: 2.1946 valid loss: 2.1866, valid accuracy: 0.3652\n",
      "Iter-8830 train loss: 2.1909 valid loss: 2.1865, valid accuracy: 0.3650\n",
      "Iter-8840 train loss: 2.1968 valid loss: 2.1864, valid accuracy: 0.3648\n",
      "Iter-8850 train loss: 2.1841 valid loss: 2.1863, valid accuracy: 0.3650\n",
      "Iter-8860 train loss: 2.1897 valid loss: 2.1862, valid accuracy: 0.3654\n",
      "Iter-8870 train loss: 2.1992 valid loss: 2.1861, valid accuracy: 0.3656\n",
      "Iter-8880 train loss: 2.1807 valid loss: 2.1860, valid accuracy: 0.3652\n",
      "Iter-8890 train loss: 2.1716 valid loss: 2.1859, valid accuracy: 0.3652\n",
      "Iter-8900 train loss: 2.1599 valid loss: 2.1858, valid accuracy: 0.3652\n",
      "Iter-8910 train loss: 2.1890 valid loss: 2.1857, valid accuracy: 0.3648\n",
      "Iter-8920 train loss: 2.1837 valid loss: 2.1856, valid accuracy: 0.3652\n",
      "Iter-8930 train loss: 2.1740 valid loss: 2.1855, valid accuracy: 0.3650\n",
      "Iter-8940 train loss: 2.1861 valid loss: 2.1854, valid accuracy: 0.3650\n",
      "Iter-8950 train loss: 2.1735 valid loss: 2.1853, valid accuracy: 0.3648\n",
      "Iter-8960 train loss: 2.1690 valid loss: 2.1852, valid accuracy: 0.3650\n",
      "Iter-8970 train loss: 2.1777 valid loss: 2.1851, valid accuracy: 0.3650\n",
      "Iter-8980 train loss: 2.2282 valid loss: 2.1850, valid accuracy: 0.3652\n",
      "Iter-8990 train loss: 2.1613 valid loss: 2.1849, valid accuracy: 0.3650\n",
      "Iter-9000 train loss: 2.2052 valid loss: 2.1848, valid accuracy: 0.3648\n",
      "Iter-9010 train loss: 2.1893 valid loss: 2.1847, valid accuracy: 0.3648\n",
      "Iter-9020 train loss: 2.1626 valid loss: 2.1846, valid accuracy: 0.3650\n",
      "Iter-9030 train loss: 2.2003 valid loss: 2.1845, valid accuracy: 0.3652\n",
      "Iter-9040 train loss: 2.2076 valid loss: 2.1843, valid accuracy: 0.3654\n",
      "Iter-9050 train loss: 2.2022 valid loss: 2.1842, valid accuracy: 0.3654\n",
      "Iter-9060 train loss: 2.2235 valid loss: 2.1842, valid accuracy: 0.3660\n",
      "Iter-9070 train loss: 2.1959 valid loss: 2.1841, valid accuracy: 0.3664\n",
      "Iter-9080 train loss: 2.2026 valid loss: 2.1839, valid accuracy: 0.3658\n",
      "Iter-9090 train loss: 2.1894 valid loss: 2.1838, valid accuracy: 0.3658\n",
      "Iter-9100 train loss: 2.1462 valid loss: 2.1837, valid accuracy: 0.3658\n",
      "Iter-9110 train loss: 2.1939 valid loss: 2.1836, valid accuracy: 0.3658\n",
      "Iter-9120 train loss: 2.1663 valid loss: 2.1835, valid accuracy: 0.3658\n",
      "Iter-9130 train loss: 2.2036 valid loss: 2.1834, valid accuracy: 0.3658\n",
      "Iter-9140 train loss: 2.1570 valid loss: 2.1833, valid accuracy: 0.3660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.1328 valid loss: 2.1832, valid accuracy: 0.3658\n",
      "Iter-9160 train loss: 2.1683 valid loss: 2.1831, valid accuracy: 0.3662\n",
      "Iter-9170 train loss: 2.2239 valid loss: 2.1830, valid accuracy: 0.3664\n",
      "Iter-9180 train loss: 2.1992 valid loss: 2.1829, valid accuracy: 0.3668\n",
      "Iter-9190 train loss: 2.1522 valid loss: 2.1828, valid accuracy: 0.3666\n",
      "Iter-9200 train loss: 2.2019 valid loss: 2.1827, valid accuracy: 0.3664\n",
      "Iter-9210 train loss: 2.1848 valid loss: 2.1826, valid accuracy: 0.3670\n",
      "Iter-9220 train loss: 2.1597 valid loss: 2.1825, valid accuracy: 0.3674\n",
      "Iter-9230 train loss: 2.1762 valid loss: 2.1824, valid accuracy: 0.3674\n",
      "Iter-9240 train loss: 2.1606 valid loss: 2.1823, valid accuracy: 0.3666\n",
      "Iter-9250 train loss: 2.1555 valid loss: 2.1822, valid accuracy: 0.3672\n",
      "Iter-9260 train loss: 2.1702 valid loss: 2.1821, valid accuracy: 0.3668\n",
      "Iter-9270 train loss: 2.1872 valid loss: 2.1820, valid accuracy: 0.3670\n",
      "Iter-9280 train loss: 2.2244 valid loss: 2.1819, valid accuracy: 0.3676\n",
      "Iter-9290 train loss: 2.1948 valid loss: 2.1818, valid accuracy: 0.3670\n",
      "Iter-9300 train loss: 2.2141 valid loss: 2.1817, valid accuracy: 0.3674\n",
      "Iter-9310 train loss: 2.2596 valid loss: 2.1816, valid accuracy: 0.3678\n",
      "Iter-9320 train loss: 2.2015 valid loss: 2.1815, valid accuracy: 0.3672\n",
      "Iter-9330 train loss: 2.1684 valid loss: 2.1814, valid accuracy: 0.3676\n",
      "Iter-9340 train loss: 2.1831 valid loss: 2.1813, valid accuracy: 0.3682\n",
      "Iter-9350 train loss: 2.1385 valid loss: 2.1812, valid accuracy: 0.3676\n",
      "Iter-9360 train loss: 2.1877 valid loss: 2.1811, valid accuracy: 0.3678\n",
      "Iter-9370 train loss: 2.1911 valid loss: 2.1810, valid accuracy: 0.3672\n",
      "Iter-9380 train loss: 2.2001 valid loss: 2.1808, valid accuracy: 0.3674\n",
      "Iter-9390 train loss: 2.2150 valid loss: 2.1808, valid accuracy: 0.3678\n",
      "Iter-9400 train loss: 2.1404 valid loss: 2.1806, valid accuracy: 0.3680\n",
      "Iter-9410 train loss: 2.2158 valid loss: 2.1805, valid accuracy: 0.3682\n",
      "Iter-9420 train loss: 2.1911 valid loss: 2.1804, valid accuracy: 0.3688\n",
      "Iter-9430 train loss: 2.1778 valid loss: 2.1803, valid accuracy: 0.3688\n",
      "Iter-9440 train loss: 2.2038 valid loss: 2.1802, valid accuracy: 0.3690\n",
      "Iter-9450 train loss: 2.1764 valid loss: 2.1801, valid accuracy: 0.3692\n",
      "Iter-9460 train loss: 2.1662 valid loss: 2.1800, valid accuracy: 0.3692\n",
      "Iter-9470 train loss: 2.1609 valid loss: 2.1799, valid accuracy: 0.3692\n",
      "Iter-9480 train loss: 2.1893 valid loss: 2.1798, valid accuracy: 0.3694\n",
      "Iter-9490 train loss: 2.1671 valid loss: 2.1797, valid accuracy: 0.3696\n",
      "Iter-9500 train loss: 2.1749 valid loss: 2.1796, valid accuracy: 0.3696\n",
      "Iter-9510 train loss: 2.1657 valid loss: 2.1795, valid accuracy: 0.3700\n",
      "Iter-9520 train loss: 2.1767 valid loss: 2.1794, valid accuracy: 0.3702\n",
      "Iter-9530 train loss: 2.1973 valid loss: 2.1793, valid accuracy: 0.3704\n",
      "Iter-9540 train loss: 2.1962 valid loss: 2.1792, valid accuracy: 0.3706\n",
      "Iter-9550 train loss: 2.1897 valid loss: 2.1791, valid accuracy: 0.3706\n",
      "Iter-9560 train loss: 2.2091 valid loss: 2.1790, valid accuracy: 0.3708\n",
      "Iter-9570 train loss: 2.1387 valid loss: 2.1789, valid accuracy: 0.3708\n",
      "Iter-9580 train loss: 2.1795 valid loss: 2.1788, valid accuracy: 0.3712\n",
      "Iter-9590 train loss: 2.2026 valid loss: 2.1787, valid accuracy: 0.3710\n",
      "Iter-9600 train loss: 2.1868 valid loss: 2.1786, valid accuracy: 0.3710\n",
      "Iter-9610 train loss: 2.2005 valid loss: 2.1785, valid accuracy: 0.3708\n",
      "Iter-9620 train loss: 2.1681 valid loss: 2.1784, valid accuracy: 0.3714\n",
      "Iter-9630 train loss: 2.2192 valid loss: 2.1783, valid accuracy: 0.3710\n",
      "Iter-9640 train loss: 2.1998 valid loss: 2.1782, valid accuracy: 0.3708\n",
      "Iter-9650 train loss: 2.1800 valid loss: 2.1781, valid accuracy: 0.3716\n",
      "Iter-9660 train loss: 2.1486 valid loss: 2.1780, valid accuracy: 0.3712\n",
      "Iter-9670 train loss: 2.1606 valid loss: 2.1779, valid accuracy: 0.3718\n",
      "Iter-9680 train loss: 2.1978 valid loss: 2.1777, valid accuracy: 0.3712\n",
      "Iter-9690 train loss: 2.1927 valid loss: 2.1777, valid accuracy: 0.3716\n",
      "Iter-9700 train loss: 2.1815 valid loss: 2.1775, valid accuracy: 0.3718\n",
      "Iter-9710 train loss: 2.1692 valid loss: 2.1774, valid accuracy: 0.3718\n",
      "Iter-9720 train loss: 2.2023 valid loss: 2.1773, valid accuracy: 0.3720\n",
      "Iter-9730 train loss: 2.1863 valid loss: 2.1772, valid accuracy: 0.3718\n",
      "Iter-9740 train loss: 2.1900 valid loss: 2.1771, valid accuracy: 0.3716\n",
      "Iter-9750 train loss: 2.1765 valid loss: 2.1770, valid accuracy: 0.3720\n",
      "Iter-9760 train loss: 2.1976 valid loss: 2.1769, valid accuracy: 0.3718\n",
      "Iter-9770 train loss: 2.1812 valid loss: 2.1768, valid accuracy: 0.3718\n",
      "Iter-9780 train loss: 2.1734 valid loss: 2.1767, valid accuracy: 0.3722\n",
      "Iter-9790 train loss: 2.1880 valid loss: 2.1766, valid accuracy: 0.3720\n",
      "Iter-9800 train loss: 2.1704 valid loss: 2.1765, valid accuracy: 0.3718\n",
      "Iter-9810 train loss: 2.2128 valid loss: 2.1764, valid accuracy: 0.3720\n",
      "Iter-9820 train loss: 2.1562 valid loss: 2.1763, valid accuracy: 0.3724\n",
      "Iter-9830 train loss: 2.1641 valid loss: 2.1762, valid accuracy: 0.3726\n",
      "Iter-9840 train loss: 2.1641 valid loss: 2.1761, valid accuracy: 0.3726\n",
      "Iter-9850 train loss: 2.2219 valid loss: 2.1760, valid accuracy: 0.3728\n",
      "Iter-9860 train loss: 2.1541 valid loss: 2.1759, valid accuracy: 0.3732\n",
      "Iter-9870 train loss: 2.1850 valid loss: 2.1758, valid accuracy: 0.3732\n",
      "Iter-9880 train loss: 2.1735 valid loss: 2.1757, valid accuracy: 0.3730\n",
      "Iter-9890 train loss: 2.1719 valid loss: 2.1756, valid accuracy: 0.3738\n",
      "Iter-9900 train loss: 2.1798 valid loss: 2.1755, valid accuracy: 0.3734\n",
      "Iter-9910 train loss: 2.1730 valid loss: 2.1754, valid accuracy: 0.3740\n",
      "Iter-9920 train loss: 2.1910 valid loss: 2.1753, valid accuracy: 0.3740\n",
      "Iter-9930 train loss: 2.1829 valid loss: 2.1752, valid accuracy: 0.3744\n",
      "Iter-9940 train loss: 2.1753 valid loss: 2.1751, valid accuracy: 0.3746\n",
      "Iter-9950 train loss: 2.2016 valid loss: 2.1750, valid accuracy: 0.3746\n",
      "Iter-9960 train loss: 2.1872 valid loss: 2.1749, valid accuracy: 0.3744\n",
      "Iter-9970 train loss: 2.1959 valid loss: 2.1748, valid accuracy: 0.3746\n",
      "Iter-9980 train loss: 2.1719 valid loss: 2.1747, valid accuracy: 0.3746\n",
      "Iter-9990 train loss: 2.1627 valid loss: 2.1746, valid accuracy: 0.3746\n",
      "Iter-10000 train loss: 2.2261 valid loss: 2.1745, valid accuracy: 0.3748\n",
      "Last iteration - Test accuracy mean: 0.3647, std: 0.0000, loss: 2.1780\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7cYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc19cf19d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncBARkST/H4JO57+oZM1gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc19cf19198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
