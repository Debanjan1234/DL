{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.selu_forward(X=y)\n",
    "        if train:\n",
    "            y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "            caches.append((fc_cache, do_cache)) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.selu_forward(X=y)\n",
    "            if train:\n",
    "                y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "    #             y = l.sigmoid(X=y) # non-linearity\n",
    "                fc_caches.append(fc_cache)\n",
    "                do_caches.append(do_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "        if train:\n",
    "            caches.append((fc_caches, do_caches)) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "            caches.append((fc_cache, do_cache)) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache, do_cache = caches[2]\n",
    "        dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, do_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.dropout_backward(cache=do_caches[layer], dout=dy)\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, do_cache = caches[0]\n",
    "        dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-20 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-30 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-40 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-50 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-60 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-70 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-80 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-90 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-100 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-110 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-120 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-130 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-140 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-150 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-160 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-170 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-180 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-190 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-200 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-210 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-220 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-230 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-240 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-250 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-260 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-270 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-280 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-290 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-300 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-310 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-320 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-330 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-340 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-350 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-360 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-370 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-380 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-390 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-400 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-410 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-420 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-430 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-440 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-450 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-460 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-470 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-480 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-490 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-500 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-510 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-520 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-530 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-540 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-550 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-560 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-570 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-580 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-590 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-600 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-610 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-620 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-630 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-640 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-650 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-660 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-670 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-680 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-690 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-700 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-710 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-720 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-730 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-740 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-750 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-760 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-770 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-780 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-790 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-800 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-810 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-820 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-830 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-840 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-850 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-860 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-870 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-880 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-890 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-900 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-910 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-920 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-930 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-940 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-950 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-960 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-970 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-980 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-990 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Iter-1000 train loss: 2.3026 valid loss: 2.2924, valid accuracy: 0.1234\n",
      "Last iteration - Test accuracy mean: 0.1124, std: 0.0000, loss: 2.2968\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067374e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11385ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
