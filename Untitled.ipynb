{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-c6f739d4e232>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-c6f739d4e232>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print \"We're on time %d\" % (x)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 3):\n",
    "    print \"We're on time %d\" % (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'patsy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-109153fb3792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# import statsmodels.api as sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpatsy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdmatrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'patsy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn import svm\n",
    "np.set_printoptions(suppress=True)\n",
    "import numpy as np\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ggplot import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # coding: utf-8\n",
    "\n",
    "# # In[1]:\n",
    "\n",
    "# cd Desktop\n",
    "\n",
    "\n",
    "# # In[2]:\n",
    "\n",
    "# cd Deep\n",
    "\n",
    "\n",
    "# # In[204]:\n",
    "\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "from numpy import genfromtxt\n",
    "my_trainx=  pd.read_csv('train.x.csv')\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "my_trainy=  pd.read_csv('trainy.csv')\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "my_test=  pd.read_csv('test.x.csv')\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "my_trainx.head()\n",
    "\n",
    "\n",
    "# In[150]:\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(my_trainx, my_trainy, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "# In[135]:\n",
    "\n",
    "my_trainy.head()\n",
    "\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "model = model.fit(my_trainx, my_trainy)\n",
    "\n",
    "\n",
    "# In[193]:\n",
    "\n",
    "model = LogisticRegression(penalty='l2',C =1.19)\n",
    "\n",
    "\n",
    "# In[199]:\n",
    "\n",
    "predicted = model.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# In[196]:\n",
    "\n",
    "predicted\n",
    "\n",
    "\n",
    "# In[197]:\n",
    "\n",
    "y_score=accuracy_score(y_test,predicted)\n",
    "\n",
    "\n",
    "# In[198]:\n",
    "\n",
    "y_score\n",
    "\n",
    "\n",
    "# In[203]:\n",
    "\n",
    "log_loss (my_trainy,predicted)\n",
    "\n",
    "\n",
    "# In[208]:\n",
    "\n",
    "predicted1 = model.predict_proba(my_test)\n",
    "\n",
    "\n",
    "# In[201]:\n",
    "\n",
    "for type in ['l1','l2']:\n",
    "    for c in [1.47,1.48,1.15,2.4,1.16,2.39]: #list of C values to evaluate\n",
    "        scores = cross_val_score(LogisticRegression(C=c, penalty=type),\n",
    "        my_trainx, my_trainy, scoring='log_loss', cv=10)\n",
    "        print (scores.mean(), \"C=\", c, \"type=\", type)\n",
    "\n",
    "\n",
    "# In[209]:\n",
    "\n",
    "predicted1\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# In[200]:\n",
    "\n",
    "np.savetxt('deep4.csv', predicted, delimiter=',',fmt=\"%s\") \n",
    "\n",
    "\n",
    "# In[174]:\n",
    "\n",
    "binarizer = preprocessing.Binarizer(threshold=0)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# In[136]:\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(27,))\n",
    "\n",
    "\n",
    "# In[138]:\n",
    "\n",
    "\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# In[142]:\n",
    "\n",
    "predictions = mlp.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# In[140]:\n",
    "\n",
    "predictions\n",
    "\n",
    "\n",
    "# In[141]:\n",
    "\n",
    "y_score=accuracy_score(y_test,predictions)\n",
    "y_score\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "scaler = StandardScaler()\n",
    "y_score\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(my_trainx)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "p = scaler.transform(my_trainx)\n",
    "q = scaler.transform(my_test)\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "#####Logistic###\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic('matplotlib inline')\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))\n",
    "\n",
    "\n",
    "# In[113]:\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)\n",
    "\n",
    "\n",
    "# In[114]:\n",
    "\n",
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "\n",
    "# In[115]:\n",
    "\n",
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )\n",
    "    return ll\n",
    "\n",
    "\n",
    "# In[119]:\n",
    "\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        scores = np.dot(features, weights)\n",
    "        predictions = sigmoid(scores)\n",
    "\n",
    "        # Update weights with gradient\n",
    "        output_error_signal = target - predictions\n",
    "        gradient = np.dot(features.T, output_error_signal)\n",
    "        weights += learning_rate * gradient\n",
    "        \n",
    "        # Print log-likelihood every so often\n",
    "        if step % 10000 == 0:\n",
    "            print (log_likelihood(features, target, weights))\n",
    "        \n",
    "    return weights\n",
    "\n",
    "\n",
    "# In[130]:\n",
    "\n",
    "weights = logistic_regression(simulated_separableish_features, simulated_labels,\n",
    "                     num_steps = 300, learning_rate = 5e-5, add_intercept=True)\n",
    "\n",
    "\n",
    "# In[131]:\n",
    "\n",
    "data_with_intercept = np.hstack((np.ones((simulated_separableish_features.shape[0], 1)),\n",
    "                                 simulated_separableish_features))\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "format((preds == simulated_labels).sum().astype(float) / len(preds))\n",
    "\n",
    "\n",
    "# In[132]:\n",
    "\n",
    "preds\n",
    "\n",
    "\n",
    "# In[133]:\n",
    "\n",
    "data_with_intercept\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
