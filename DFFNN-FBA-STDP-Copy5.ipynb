{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "#         y, _ = l.relu_forward(X=y)\n",
    "        y, _ = l.lrelu_forward(X=y)\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "#             y, _ = l.relu_forward(X=y)\n",
    "            y, _ = l.lrelu_forward(X=y)\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.0880\n",
      "Iter-20 train loss: 2.3027 valid loss: 2.3026, valid accuracy: 0.0966\n",
      "Iter-30 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1084\n",
      "Iter-40 train loss: 2.3027 valid loss: 2.3026, valid accuracy: 0.0754\n",
      "Iter-50 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.0810\n",
      "Iter-60 train loss: 2.3027 valid loss: 2.3026, valid accuracy: 0.1144\n",
      "Iter-70 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-80 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-90 train loss: 2.3028 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-100 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-110 train loss: 2.3024 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-120 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-130 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-140 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-150 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-160 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-170 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-180 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-190 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-200 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-210 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-220 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-230 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-240 train loss: 2.3028 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-250 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-260 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-270 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-280 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-290 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-300 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-310 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-320 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-330 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-340 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-350 train loss: 2.3030 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-360 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-370 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-380 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-390 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-400 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-410 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-420 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-430 train loss: 2.3022 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-440 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-450 train loss: 2.3030 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-460 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-470 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-480 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-490 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-500 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-510 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-520 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-530 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-540 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-550 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-560 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-570 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-580 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-590 train loss: 2.3018 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-600 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-610 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-620 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-630 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-640 train loss: 2.3020 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-650 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-660 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-670 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-680 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-690 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-700 train loss: 2.3018 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-710 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-720 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-730 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-740 train loss: 2.3020 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-750 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-760 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-770 train loss: 2.3028 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-780 train loss: 2.3017 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-790 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-800 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-810 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-820 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-830 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-840 train loss: 2.3036 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-850 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-860 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-870 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-880 train loss: 2.3003 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-890 train loss: 2.3035 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-900 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-910 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-920 train loss: 2.3028 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-930 train loss: 2.3034 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-940 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-950 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-960 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-970 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-980 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-990 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1000 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1010 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1020 train loss: 2.3022 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1030 train loss: 2.3017 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1040 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1050 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1060 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1070 train loss: 2.3035 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1080 train loss: 2.3029 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1090 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1100 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1110 train loss: 2.3035 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1120 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1130 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1140 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1150 train loss: 2.3007 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1160 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1180 train loss: 2.3029 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1190 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.3007 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1210 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1220 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1230 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1240 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1250 train loss: 2.3035 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1260 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1270 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1280 train loss: 2.3046 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1290 train loss: 2.3007 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1310 train loss: 2.3028 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1320 train loss: 2.3031 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1330 train loss: 2.3026 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1340 train loss: 2.3030 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1350 train loss: 2.3030 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1360 train loss: 2.3015 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1370 train loss: 2.3014 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1380 train loss: 2.3035 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1390 train loss: 2.3018 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3011 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1410 train loss: 2.3009 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1420 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1430 train loss: 2.3034 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1440 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1450 train loss: 2.3038 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1460 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1470 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1480 train loss: 2.3015 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1490 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.3012 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1510 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1520 train loss: 2.3013 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1530 train loss: 2.3012 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1540 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1550 train loss: 2.3017 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1560 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1570 train loss: 2.3023 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1580 train loss: 2.3029 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1590 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3001 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1610 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1620 train loss: 2.3030 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1630 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1640 train loss: 2.3027 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1650 train loss: 2.3000 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1660 train loss: 2.3033 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1670 train loss: 2.3029 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1680 train loss: 2.3027 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1690 train loss: 2.3000 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3020 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1710 train loss: 2.3016 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1720 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1730 train loss: 2.3036 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1740 train loss: 2.3032 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1750 train loss: 2.3008 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1760 train loss: 2.3019 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1770 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1780 train loss: 2.3035 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1790 train loss: 2.3017 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1810 train loss: 2.3014 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1820 train loss: 2.3035 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1830 train loss: 2.3025 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1840 train loss: 2.3032 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1850 train loss: 2.2985 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1860 train loss: 2.3037 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1870 train loss: 2.3040 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1880 train loss: 2.3010 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1890 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3024 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1910 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1920 train loss: 2.3018 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1930 train loss: 2.3028 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1940 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1950 train loss: 2.3051 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1960 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1970 train loss: 2.3032 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1980 train loss: 2.3000 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1990 train loss: 2.3013 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3008 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2010 train loss: 2.3022 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2020 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2030 train loss: 2.3007 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2040 train loss: 2.3002 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2050 train loss: 2.3006 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2060 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2070 train loss: 2.3039 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2080 train loss: 2.3011 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2090 train loss: 2.3010 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3022 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2110 train loss: 2.3013 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2120 train loss: 2.3019 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2130 train loss: 2.3026 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2140 train loss: 2.2997 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2150 train loss: 2.3004 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2160 train loss: 2.3044 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2170 train loss: 2.3024 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2180 train loss: 2.3005 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2190 train loss: 2.3015 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3013 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2210 train loss: 2.3037 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2220 train loss: 2.3052 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2230 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2240 train loss: 2.3023 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2250 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2260 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2270 train loss: 2.3032 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2280 train loss: 2.3023 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2290 train loss: 2.3024 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3029 valid loss: 2.3020, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3030 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2320 train loss: 2.3050 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2330 train loss: 2.3034 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2340 train loss: 2.3005 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2350 train loss: 2.3031 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2360 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2370 train loss: 2.2995 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2380 train loss: 2.3013 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2390 train loss: 2.3005 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.3030 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2410 train loss: 2.3012 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2420 train loss: 2.3016 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2430 train loss: 2.3028 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2440 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2450 train loss: 2.3031 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2460 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2470 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2480 train loss: 2.3047 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2490 train loss: 2.3063 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2510 train loss: 2.3042 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2520 train loss: 2.3013 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2530 train loss: 2.3027 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2540 train loss: 2.3027 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2550 train loss: 2.2994 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2560 train loss: 2.3016 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2570 train loss: 2.3044 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2580 train loss: 2.3018 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2590 train loss: 2.3015 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.3001 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2610 train loss: 2.3001 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2620 train loss: 2.3047 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2630 train loss: 2.3016 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2640 train loss: 2.2991 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2650 train loss: 2.3016 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2660 train loss: 2.3029 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2670 train loss: 2.3031 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2680 train loss: 2.3029 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2690 train loss: 2.3049 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.3057 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2710 train loss: 2.3017 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2720 train loss: 2.3063 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2730 train loss: 2.3012 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2740 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2750 train loss: 2.3047 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2760 train loss: 2.3032 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2770 train loss: 2.2995 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2780 train loss: 2.3031 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2790 train loss: 2.3036 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2810 train loss: 2.3056 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2820 train loss: 2.3040 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2830 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2840 train loss: 2.2978 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2850 train loss: 2.3021 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2860 train loss: 2.3033 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2870 train loss: 2.3036 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2880 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2890 train loss: 2.2991 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.2972 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2910 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2920 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2930 train loss: 2.2993 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2940 train loss: 2.3015 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2950 train loss: 2.3045 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2960 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2970 train loss: 2.2988 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2980 train loss: 2.3006 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2990 train loss: 2.3052 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.3016 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3010 train loss: 2.3016 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3020 train loss: 2.3008 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3030 train loss: 2.3030 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3040 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3050 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3060 train loss: 2.3017 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3070 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3080 train loss: 2.2999 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3090 train loss: 2.3047 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3046 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3110 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3120 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3130 train loss: 2.3040 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3140 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3150 train loss: 2.3024 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3160 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3170 train loss: 2.3007 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3180 train loss: 2.3023 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3190 train loss: 2.3031 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3062 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3210 train loss: 2.3007 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3220 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3230 train loss: 2.3002 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3240 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3250 train loss: 2.3020 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3260 train loss: 2.3034 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3270 train loss: 2.2991 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3280 train loss: 2.3017 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3290 train loss: 2.3014 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.2980 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3310 train loss: 2.2979 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3320 train loss: 2.3022 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3330 train loss: 2.3032 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3340 train loss: 2.2977 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3350 train loss: 2.3010 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3360 train loss: 2.3058 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3370 train loss: 2.3026 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3380 train loss: 2.2980 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3390 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.3026 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3410 train loss: 2.3030 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3420 train loss: 2.3008 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3430 train loss: 2.3028 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3440 train loss: 2.3005 valid loss: 2.3018, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3058 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3460 train loss: 2.3015 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3470 train loss: 2.2997 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3480 train loss: 2.3047 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3490 train loss: 2.3015 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.3009 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3510 train loss: 2.3029 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3520 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3530 train loss: 2.3043 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3540 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3550 train loss: 2.3012 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3560 train loss: 2.3005 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3570 train loss: 2.3004 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3580 train loss: 2.3017 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3590 train loss: 2.2998 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.3033 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3610 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3620 train loss: 2.2989 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3630 train loss: 2.3006 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3640 train loss: 2.2991 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3650 train loss: 2.3000 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3660 train loss: 2.2984 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3670 train loss: 2.3010 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3680 train loss: 2.3011 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3690 train loss: 2.2996 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.2990 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3710 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3720 train loss: 2.3057 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3730 train loss: 2.3058 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3740 train loss: 2.2997 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3750 train loss: 2.3003 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3760 train loss: 2.3019 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3770 train loss: 2.3017 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3780 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3790 train loss: 2.3070 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3046 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3810 train loss: 2.3039 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3820 train loss: 2.3039 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3830 train loss: 2.3010 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3840 train loss: 2.3044 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3850 train loss: 2.2985 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3860 train loss: 2.3050 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3870 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3880 train loss: 2.2995 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3890 train loss: 2.3054 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3022 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3910 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3920 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3930 train loss: 2.3060 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3940 train loss: 2.3025 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3950 train loss: 2.3030 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3960 train loss: 2.3007 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3970 train loss: 2.3062 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3980 train loss: 2.3037 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3990 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3014 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4010 train loss: 2.2988 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4020 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4030 train loss: 2.3023 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4040 train loss: 2.3044 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4050 train loss: 2.2949 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4060 train loss: 2.3032 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4070 train loss: 2.2979 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4080 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4090 train loss: 2.3036 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3041 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4110 train loss: 2.3002 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4120 train loss: 2.2991 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4130 train loss: 2.3044 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4140 train loss: 2.3028 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4150 train loss: 2.3020 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4160 train loss: 2.2994 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4170 train loss: 2.3026 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4180 train loss: 2.3042 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4190 train loss: 2.2989 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3003 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4210 train loss: 2.3059 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4220 train loss: 2.3011 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4230 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4240 train loss: 2.2992 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4250 train loss: 2.3063 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4260 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4270 train loss: 2.3039 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4280 train loss: 2.3010 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4290 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.3008 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4310 train loss: 2.2994 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4320 train loss: 2.3043 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4330 train loss: 2.3007 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4340 train loss: 2.3007 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4350 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4360 train loss: 2.3068 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4370 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4380 train loss: 2.2991 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4390 train loss: 2.3013 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.3033 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4410 train loss: 2.3007 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4420 train loss: 2.3053 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4430 train loss: 2.2991 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4440 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4450 train loss: 2.3026 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4460 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4470 train loss: 2.2952 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4480 train loss: 2.3015 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4490 train loss: 2.3033 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.2992 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4510 train loss: 2.3039 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4520 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4530 train loss: 2.3022 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4540 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4550 train loss: 2.3036 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4560 train loss: 2.2981 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4570 train loss: 2.3048 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4580 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3028 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.2997 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4610 train loss: 2.2999 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4620 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4630 train loss: 2.3052 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4640 train loss: 2.3009 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4650 train loss: 2.2930 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4660 train loss: 2.3002 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4670 train loss: 2.3059 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4680 train loss: 2.3044 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4690 train loss: 2.2978 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.3007 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4710 train loss: 2.3045 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4720 train loss: 2.2977 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4730 train loss: 2.2964 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4740 train loss: 2.3043 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4750 train loss: 2.2962 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4760 train loss: 2.3044 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4770 train loss: 2.2988 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4780 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4790 train loss: 2.2990 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.2989 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4810 train loss: 2.3011 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4820 train loss: 2.2978 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4830 train loss: 2.2979 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4840 train loss: 2.2987 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4850 train loss: 2.2937 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4860 train loss: 2.3081 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4870 train loss: 2.3021 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4880 train loss: 2.3048 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4890 train loss: 2.3057 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.3031 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4910 train loss: 2.3008 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4920 train loss: 2.3073 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4930 train loss: 2.3035 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4940 train loss: 2.2996 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4950 train loss: 2.2990 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4960 train loss: 2.3006 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4970 train loss: 2.3038 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4980 train loss: 2.3047 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4990 train loss: 2.3014 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3013 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5010 train loss: 2.2981 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5020 train loss: 2.3049 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5030 train loss: 2.3027 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5040 train loss: 2.2993 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5050 train loss: 2.3062 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5060 train loss: 2.3095 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5070 train loss: 2.3056 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5080 train loss: 2.3030 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5090 train loss: 2.3048 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3029 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5110 train loss: 2.2996 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5120 train loss: 2.3053 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5130 train loss: 2.2998 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5140 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5150 train loss: 2.2926 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5160 train loss: 2.3028 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5170 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5180 train loss: 2.3025 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5190 train loss: 2.3025 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5210 train loss: 2.3020 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5220 train loss: 2.3016 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5230 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5240 train loss: 2.2999 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5250 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5260 train loss: 2.3007 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5270 train loss: 2.3035 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5280 train loss: 2.2986 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5290 train loss: 2.2967 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.2987 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5310 train loss: 2.2986 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5320 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5330 train loss: 2.2991 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5340 train loss: 2.3019 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5350 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5360 train loss: 2.3020 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5370 train loss: 2.3013 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5380 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5390 train loss: 2.2983 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5410 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5420 train loss: 2.3026 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5430 train loss: 2.3023 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5440 train loss: 2.3052 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5450 train loss: 2.3012 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5460 train loss: 2.3063 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5470 train loss: 2.3021 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5480 train loss: 2.3005 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5490 train loss: 2.2992 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3052 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5510 train loss: 2.2988 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5520 train loss: 2.3075 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5530 train loss: 2.3027 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5540 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5550 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5560 train loss: 2.2986 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5570 train loss: 2.3061 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5580 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5590 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.3018 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5610 train loss: 2.3024 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5620 train loss: 2.3023 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5630 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5640 train loss: 2.3058 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5650 train loss: 2.2998 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5660 train loss: 2.3102 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5670 train loss: 2.3023 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5680 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5690 train loss: 2.3026 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5710 train loss: 2.2960 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5720 train loss: 2.2969 valid loss: 2.3015, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.3048 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5740 train loss: 2.3075 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5750 train loss: 2.3013 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5760 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5770 train loss: 2.3055 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5780 train loss: 2.3048 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5790 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3046 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5810 train loss: 2.2986 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5820 train loss: 2.3068 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5830 train loss: 2.3038 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5840 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5850 train loss: 2.3004 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5860 train loss: 2.3044 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5870 train loss: 2.3042 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5880 train loss: 2.2976 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5890 train loss: 2.3046 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.3012 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5910 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5920 train loss: 2.2958 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5930 train loss: 2.2971 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5940 train loss: 2.2995 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5950 train loss: 2.3020 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5960 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5970 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5980 train loss: 2.3019 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5990 train loss: 2.2999 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3065 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6010 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6020 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6030 train loss: 2.3035 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6040 train loss: 2.2958 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6050 train loss: 2.2975 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6060 train loss: 2.3051 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6070 train loss: 2.2969 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6080 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6090 train loss: 2.3019 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3060 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6110 train loss: 2.3069 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6120 train loss: 2.2995 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6130 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6140 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6150 train loss: 2.3012 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6160 train loss: 2.2982 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6170 train loss: 2.3017 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6180 train loss: 2.2985 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6190 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3050 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6210 train loss: 2.3034 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6220 train loss: 2.2995 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6230 train loss: 2.3025 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6240 train loss: 2.2990 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6250 train loss: 2.2986 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6260 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6270 train loss: 2.3001 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6280 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6290 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6310 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6320 train loss: 2.3016 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6330 train loss: 2.3081 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6340 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6350 train loss: 2.2953 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6360 train loss: 2.2988 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6370 train loss: 2.3021 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6380 train loss: 2.3024 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6390 train loss: 2.3044 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6410 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6420 train loss: 2.3050 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6430 train loss: 2.2943 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6440 train loss: 2.2948 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6450 train loss: 2.3036 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6460 train loss: 2.2999 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6470 train loss: 2.2979 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6480 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6490 train loss: 2.3044 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.2995 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6510 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6520 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6530 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6540 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6550 train loss: 2.2982 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6560 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6570 train loss: 2.3032 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6580 train loss: 2.2915 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6590 train loss: 2.3005 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6610 train loss: 2.2953 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6620 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6630 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6640 train loss: 2.2988 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6650 train loss: 2.3086 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6660 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6670 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6680 train loss: 2.3002 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6690 train loss: 2.3104 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.2934 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6710 train loss: 2.3050 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6720 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6730 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6740 train loss: 2.3054 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6750 train loss: 2.3030 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6760 train loss: 2.3010 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6770 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6780 train loss: 2.3018 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6790 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.3034 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6810 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6820 train loss: 2.2985 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6830 train loss: 2.3077 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6840 train loss: 2.3008 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6850 train loss: 2.2985 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6860 train loss: 2.2967 valid loss: 2.3014, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.2987 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6880 train loss: 2.3046 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6890 train loss: 2.3042 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.3088 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6910 train loss: 2.3052 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6920 train loss: 2.2966 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6930 train loss: 2.3104 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6940 train loss: 2.3001 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6950 train loss: 2.3023 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6960 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6970 train loss: 2.3069 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6980 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6990 train loss: 2.3071 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.2958 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7010 train loss: 2.2963 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7020 train loss: 2.2936 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7030 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7040 train loss: 2.2991 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7050 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7060 train loss: 2.3085 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7070 train loss: 2.3064 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7080 train loss: 2.3048 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7090 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7110 train loss: 2.3078 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7120 train loss: 2.3007 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7130 train loss: 2.2951 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7140 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7150 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7160 train loss: 2.3029 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7170 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7180 train loss: 2.3005 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7190 train loss: 2.2977 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.2970 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7210 train loss: 2.3000 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7220 train loss: 2.2988 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7230 train loss: 2.2996 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7240 train loss: 2.3005 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7250 train loss: 2.3074 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7260 train loss: 2.2958 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7270 train loss: 2.2984 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7280 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7290 train loss: 2.2906 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.3022 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7310 train loss: 2.3054 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7320 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7330 train loss: 2.3071 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7340 train loss: 2.3027 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7350 train loss: 2.3019 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7360 train loss: 2.3081 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7370 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7380 train loss: 2.2937 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7390 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.2975 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7410 train loss: 2.2941 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7420 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7430 train loss: 2.3030 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7440 train loss: 2.3002 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7450 train loss: 2.2970 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7460 train loss: 2.3027 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7470 train loss: 2.3091 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7480 train loss: 2.3046 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7490 train loss: 2.3051 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.2992 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7510 train loss: 2.3057 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7520 train loss: 2.3047 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7530 train loss: 2.2972 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7540 train loss: 2.2968 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7550 train loss: 2.3083 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7560 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7570 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7580 train loss: 2.3068 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7590 train loss: 2.3005 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.3046 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7610 train loss: 2.2962 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7620 train loss: 2.2998 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7630 train loss: 2.3041 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7640 train loss: 2.2962 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7650 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7660 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7670 train loss: 2.3090 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7680 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7690 train loss: 2.3053 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7710 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7720 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7730 train loss: 2.3020 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7740 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7750 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7760 train loss: 2.2990 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7770 train loss: 2.2959 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7780 train loss: 2.2952 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7790 train loss: 2.2973 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7810 train loss: 2.3048 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7820 train loss: 2.3008 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7830 train loss: 2.3001 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7840 train loss: 2.3074 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7850 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7860 train loss: 2.2969 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7870 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7880 train loss: 2.3041 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7890 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.3052 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7910 train loss: 2.3054 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7920 train loss: 2.3022 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7930 train loss: 2.3007 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7940 train loss: 2.3047 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7950 train loss: 2.2915 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7960 train loss: 2.3029 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7970 train loss: 2.2961 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7980 train loss: 2.2990 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7990 train loss: 2.3023 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.3027 valid loss: 2.3014, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.3082 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8020 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8030 train loss: 2.2992 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8040 train loss: 2.3046 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8050 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8060 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8070 train loss: 2.3084 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8080 train loss: 2.2954 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8090 train loss: 2.2940 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8110 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8120 train loss: 2.2999 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8130 train loss: 2.3034 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8140 train loss: 2.2976 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8150 train loss: 2.3045 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8160 train loss: 2.2997 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8170 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8180 train loss: 2.3002 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8190 train loss: 2.2966 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.3041 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8210 train loss: 2.3072 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8220 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8230 train loss: 2.3063 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8240 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8250 train loss: 2.2915 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8260 train loss: 2.3001 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8270 train loss: 2.3047 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8280 train loss: 2.3069 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8290 train loss: 2.3053 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.2981 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8310 train loss: 2.2921 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8320 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8330 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8340 train loss: 2.2959 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8350 train loss: 2.3008 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8360 train loss: 2.3087 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8370 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8380 train loss: 2.3092 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8390 train loss: 2.2995 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.3032 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8410 train loss: 2.3050 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-8420 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8430 train loss: 2.3053 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8440 train loss: 2.3043 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8450 train loss: 2.2929 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8460 train loss: 2.3045 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8470 train loss: 2.3045 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8480 train loss: 2.2964 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8490 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8510 train loss: 2.3053 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8520 train loss: 2.2991 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8530 train loss: 2.3013 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8540 train loss: 2.3085 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8550 train loss: 2.3009 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8560 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8570 train loss: 2.2977 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8580 train loss: 2.2983 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8590 train loss: 2.3092 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8610 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8620 train loss: 2.3067 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8630 train loss: 2.2987 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8640 train loss: 2.3102 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8650 train loss: 2.3024 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8660 train loss: 2.3073 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8670 train loss: 2.2979 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8680 train loss: 2.2945 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8690 train loss: 2.3162 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.3090 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8710 train loss: 2.3015 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8720 train loss: 2.3036 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8730 train loss: 2.2903 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8740 train loss: 2.3042 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8750 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8760 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8770 train loss: 2.2954 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8780 train loss: 2.2955 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8790 train loss: 2.3032 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.2955 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8810 train loss: 2.2987 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8820 train loss: 2.3065 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8830 train loss: 2.3001 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8840 train loss: 2.3045 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8850 train loss: 2.3052 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8860 train loss: 2.3036 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8870 train loss: 2.2972 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8880 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8890 train loss: 2.3083 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.3064 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8910 train loss: 2.3061 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8920 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8930 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8940 train loss: 2.3044 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8950 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8960 train loss: 2.3019 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8970 train loss: 2.2965 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8980 train loss: 2.3046 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8990 train loss: 2.3024 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9010 train loss: 2.3056 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9020 train loss: 2.3030 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9030 train loss: 2.2948 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9040 train loss: 2.3055 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9050 train loss: 2.2929 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9060 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9070 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9080 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9090 train loss: 2.2968 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.2983 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9110 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9120 train loss: 2.2939 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9130 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9140 train loss: 2.3069 valid loss: 2.3013, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.3032 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9160 train loss: 2.2990 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9170 train loss: 2.2934 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9180 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9190 train loss: 2.3079 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.2990 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9210 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9220 train loss: 2.3076 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9230 train loss: 2.3018 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9240 train loss: 2.3049 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9250 train loss: 2.3078 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9260 train loss: 2.3046 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9270 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9280 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9290 train loss: 2.3008 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9310 train loss: 2.3111 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9320 train loss: 2.2998 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9330 train loss: 2.2976 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9340 train loss: 2.3008 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9350 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9360 train loss: 2.3040 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9370 train loss: 2.3034 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9380 train loss: 2.3005 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9390 train loss: 2.3053 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.3081 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9410 train loss: 2.2981 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9420 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-9430 train loss: 2.3007 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9440 train loss: 2.3025 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9450 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9460 train loss: 2.3015 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9470 train loss: 2.2918 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9480 train loss: 2.2998 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9490 train loss: 2.3065 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.2976 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9510 train loss: 2.2966 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9520 train loss: 2.2993 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9530 train loss: 2.2975 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9540 train loss: 2.3006 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9550 train loss: 2.3046 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9560 train loss: 2.3016 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9570 train loss: 2.2947 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9580 train loss: 2.2982 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9590 train loss: 2.2994 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.3060 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9610 train loss: 2.3057 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9620 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9630 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9640 train loss: 2.2973 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9650 train loss: 2.3095 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9660 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9670 train loss: 2.3070 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9680 train loss: 2.3108 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9690 train loss: 2.3083 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.3106 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9710 train loss: 2.3004 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9720 train loss: 2.3065 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9730 train loss: 2.3059 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9740 train loss: 2.2926 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9750 train loss: 2.2978 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9760 train loss: 2.3028 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9770 train loss: 2.3039 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9780 train loss: 2.3076 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9790 train loss: 2.2958 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.3049 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9810 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9820 train loss: 2.3007 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9830 train loss: 2.3044 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9840 train loss: 2.3019 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9850 train loss: 2.3000 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9860 train loss: 2.2966 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9870 train loss: 2.3027 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9880 train loss: 2.2942 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9890 train loss: 2.2992 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3004 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9910 train loss: 2.3020 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9920 train loss: 2.3018 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9930 train loss: 2.3014 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9940 train loss: 2.2992 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9950 train loss: 2.3028 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9960 train loss: 2.3115 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9970 train loss: 2.3020 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9980 train loss: 2.2942 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9990 train loss: 2.3057 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.2963 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3013\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120f26a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH3BJREFUeJzt3XuUFeWd7vHv0yonRxxbVC6GSxvFeCGj4CgSNStbJeGS\nRPASBc4CY4ySY4jXnIgnydhkOWsFJ2EMy0MQJQRjGOKQRDBBZYzpGGOOEoGlOFxaFGgaIV7AHDUC\ngd/5YxfNpu3eXcDeVG/7+axVy11vvVX1vmXrs+utql2KCMzMzNKoyroBZmZWORwaZmaWmkPDzMxS\nc2iYmVlqDg0zM0vNoWFmZqmlCg1JQyWtlLRa0m0tLD9Z0jOS3pd0S7NlMyVtlvRCs/I7JG2QtCSZ\nhh5YV8zMrNzaDA1JVcA9wBCgHzBa0inNqr0JfB341xY2MStZtyVTIuLMZHosfbPNzCwLac40BgL1\nEbEuInYAc4ERhRUi4o2IeB74e/OVI+JpYEsr29Y+ttfMzDKUJjR6Ag0F8xuSslKYIGmZpPslVZdo\nm2ZmViZZXgifBpwQEf2BTcCUDNtiZmYpHJqiTiPQp2C+V1J2QCLi9YLZ+4BHWqonyT+OZWa2HyKi\n5JcA0pxpLAb6SqqR1AkYBSwoUr+lRqp5uaQeBbOXAstb22BEeIrgjjvuyLwN7WXysfCx8LEoPpVL\nm6ERETuBCcAi4CVgbkSskDRe0nUAkrpLagBuBr4lab2kI5Jlc4BngI8n5Vcnm75L0guSlgGfTtZt\n0SWXwHe/ewC9NDOzkkgzPEXkb4c9uVnZvQWfNwO9W1l3TCvl49I28uGH89M//3PaNczMrBz8RHgF\nyeVyWTeh3fCx2MPHYg8fi/JTOce+SiF/ITzfxnbeVDOzdkMSUYYL4amGp8ys/Tj++ONZt25d1s2w\ndqKmpoa1a9cetP35TMOswiTfILNuhrUTrf09lOtMo6Kuacg/OmJmlqmKCg0zM8uWQ8PMzFKruNCQ\nPHnq2NOH2bp166iqqmLXrl0ADB8+nJ/+9Kep6nZkB/PvpGLunho8GBYtyroVZtmrasdf9YYNG8Y5\n55xDbW3tXuXz58/nq1/9Ko2NjVS10QEV/B9v4cKFqet2ZC3lZrn+Ttrxn9/esv5258lTe5nas6uu\nuooHH3zwA+UPPvggY8eObTMwPkwO5h1uB/PvpGL+DfoOQ7P2b+TIkbz55ps8/fTTTWVbt27l17/+\nNePG5X85aOHChZx55plUV1dTU1PDpEmTWt3eBRdcwI9//GMAdu3axTe+8Q26du1K3759+c1vflO0\nLZMnT6Zv374ceeSRfOITn+Dhhx/ea/l9993Haaed1rR82bJlAGzYsIHLLruMbt260bVrV2644QYA\nJk2axNixY5vWbz48dsEFF/Dtb3+b888/n86dO/Pqq6/yk5/8pGkfffv2ZcaMGXu1Yf78+QwYMIDq\n6mpOOukkFi1axLx58zjrrLP2qjdlyhQuueSSov09aLL+JcYUv9QYEDF4cJhZROT/s22/rr322rj2\n2mub5qdPnx4DBgxomv/9738fy5cvj4iIF198MXr06BHz58+PiIi1a9dGVVVV7Ny5MyIicrlczJw5\nMyIifvSjH8Wpp54ajY2NsWXLlrjgggv2qtvcvHnzYtOmTRER8dBDD0Xnzp33mu/Vq1c8//zzERGx\nZs2aWL9+fezcuTPOOOOMuPXWW+Nvf/tbbNu2Lf74xz9GRERtbW2MHTu2afsttbWmpiZWrFgRO3fu\njB07dsTChQvj1VdfjYiIp556Kg4//PBYunRpREQ8++yzUV1dHb/97W8jImLjxo2xatWq2LZtWxxz\nzDGxcuXKpn0NGDAgfvWrX7XYz9b+HpLy0v8/uRwbLWkDHRpme0kTGvlz8wOf9sfTTz8dRx11VGzb\nti0iIs4777y4++67W61/0003xS233BIRxUPjwgsvjHvvvbdpvUWLFhUNjeb69+8fCxYsiIiIIUOG\nxNSpUz9Q509/+lN069atxW2mCY077rijaBtGjhzZtN/x48c39bu566+/Pr797W9HRMTy5cvj6KOP\nju3bt7dY92CHRsUMT7X3sVyz9qRUsbE/zjvvPLp27crDDz/MK6+8wuLFixkzZs+PXT/33HNceOGF\ndOvWjaOOOop7772XN954o83tbty4kd699/yYdk1NTdH6DzzwAAMGDKBLly506dKFl156qWk/DQ0N\nnHjiiR9Yp6GhgZqamv2+9lLYPoBHH32UT37ykxxzzDF06dKFRx99tM02AIwbN445c+YA+etBV1xx\nBYcddth+tanUKiY0zKxyjB07ltmzZ/Pggw8yZMgQunbt2rRszJgxjBw5ksbGRrZu3cr48eN3jyoU\nddxxx9HQ0NA0X+z3t9avX891113HtGnT2LJlC1u2bKFfv35N++nduzdr1qz5wHq9e/dm/fr1Ld7G\n27lzZ957772m+ddee+0DdQrv5tq+fTuXX3453/zmN3n99dfZsmULw4YNa7MNAOeccw6dOnXiD3/4\nA3PmzNnrWkrWKiY09vdbj5kdfOPGjeOJJ57g/vvv56qrrtpr2TvvvEOXLl047LDDeO6555q+Ue/W\nWoBcccUVTJ06lcbGRrZs2cLkyZNb3f+7775LVVUVxx57LLt27WLWrFksX77n5aBf+cpX+P73v8+S\nJUsAWLNmDQ0NDQwcOJDjjjuOiRMn8t5777Ft2zaeeeYZAPr3789TTz1FQ0MDb7/9Nt/73veKHoPt\n27ezfft2jj32WKqqqnj00UdZVPDcwDXXXMOsWbP43e9+R0SwceNGVq1a1bR87NixTJgwgU6dOnHu\nuecW3dfBlCo0JA2VtFLSakm3tbD8ZEnPSHpf0i3Nls2UtFnSC83Ku0haJGmVpMclVR9YV8ysvaip\nqeHcc8/lvffe4+KLL95r2bRp0/jOd75DdXU1d955J1deeeVeywu/rRd+vvbaaxkyZAhnnHEGZ511\nFpdddlmr+z/11FO59dZbGTRoED169OCll17i/PPPb1p++eWX861vfYsxY8Zw5JFHcskll/DWW29R\nVVXFI488Qn19PX369KF379489NBDAAwePJgrr7yS008/nbPPPpsvfOELrbYb4IgjjmDq1Kl88Ytf\n5Oijj2bu3LmMGDGiafnZZ5/NrFmzuOmmm6iuriaXy7F+/fqm5WPHjmX58uXt6iwDaPtXbiVVAauB\ni4CN5N8ZPioiVhbUORaoAUYCWyJiSsGy84F3gAci4vSC8snAmxFxVxJEXSJiYgv7Dwg+8xk/3GcG\n+f85pRnOscr2/vvv0717d5YsWdLqtQ9o/e8hy1+5HQjUR8S6iNgBzAVGFFaIiDci4nng781Xjoin\ngS0tbHcEMDv5PJt84JiZGfkzsrPPPrtoYGQhzc+I9AQaCuY3kA+SA9Ut8u8WJyI2SepWgm2amVW8\nj33sYwAfeCCxPWhPvz1V5Hy7ljVroLY2/w5gvwfYzD7MXn311X1ep66ujrq6utI3ppk0odEI9CmY\n75WUHajNkrpHxGZJPYC/tF61lhNPzIeGmZl9UPMv1MV+nuVApLmmsRjoK6lGUidgFLCgSP2WLryo\nhfIFwJeSz1cB81O0xczMMtRmaETETmACsAh4CZgbESskjZd0HYCk7pIagJuBb0laL+mIZNkc4Bng\n40n51cmmJwOfkbSK/J1ZRW969s0iZmbZS3VNIyIeA05uVnZvwefNQO/m6yXLxrRS/hYwOHVLzQzI\nPwPh90jYbm39nEqptacL4UX5vxGzvLVr12bdBOvAKuZnRBwaZmbZq5jQMDOz7FVMaPhCuJlZ9iom\nNMzMLHsODTMzS61iQsMXws3MslcxoeFrGmZm2auY0DAzs+w5NMzMLLWKCQ1f0zAzy17FhIaZmWXP\noWFmZqk5NMzMLDWHhpmZpebQMDOz1FKFhqShklZKWi3pthaWnyzpGUnvS7olzbqS7pC0QdKSZBpa\nrA1+uM/MLHttvoRJUhVwD/lXsm4EFkuaHxErC6q9CXwdGLmP606JiCkH3g0zMzsY0pxpDATqI2Jd\nROwA5gIjCitExBsR8Tzw931cN/XTF35Ow8wse2lCoyfQUDC/ISlLo611J0haJul+SdXFNuTQMDPL\nXpbvCJ8GfDciQtKdwBTgmpar1lJfD7W1kMvlyOVyB62RZmaVoK6ujrq6urLvJ01oNAJ9CuZ7JWVp\ntLpuRLxeUH4f8Ejrm6mlb998aJiZ2Qc1/0I9adKksuwnzfDUYqCvpBpJnYBRwIIi9QsHklpdV1KP\ngnqXAsv3qeVmZnbQtXmmERE7JU0AFpEPmZkRsULS+PzimCGpO/Bn4B+AXZJuBE6LiHdaWjfZ9F2S\n+gO7gLXA+FJ3zszMSkvRzh+AkBQQfPaz8PjjWbfGzKwySCIiSn4LkZ8INzOz1BwaZmaWmkPDzMxS\nq5jQ8MN9ZmbZq5jQaOfX683MOoSKCQ0zM8texYSGh6fMzLJXMaFhZmbZq5jQ8DUNM7PsVUxomJlZ\n9hwaZmaWWsWEhi+Em5llr2JCw8zMsufQMDOz1BwaZmaWmkPDzMxSSxUakoZKWilptaTbWlh+sqRn\nJL0v6ZY060rqImmRpFWSHpdUXawNfk7DzCx7bYaGpCrgHmAI0A8YLemUZtXeBL4O/Os+rDsReCIi\nTgaeBG4/gH6YmdlBkOZMYyBQHxHrImIHMBcYUVghIt6IiOeBv+/DuiOA2cnn2cDIYo3wLbdmZtlL\nExo9gYaC+Q1JWRrF1u0eEZsBImIT0C3lNs3MLCOHZt2AAkWuWtTy8stQWwu5XI5cLnew2mRmVhHq\n6uqoq6sr+37ShEYj0KdgvldSlkaxdTdJ6h4RmyX1AP7S+mZq6ds3HxpmZvZBzb9QT5o0qSz7STM8\ntRjoK6lGUidgFLCgSP3Cqw/F1l0AfCn5fBUwf18abmZmB1+bZxoRsVPSBGAR+ZCZGRErJI3PL44Z\nkroDfwb+Adgl6UbgtIh4p6V1k01PBh6S9GVgHXBFyXtnZmYlpWjnD0BICgiGDoVHH826NWZmlUES\nEVHy+04r5onwdp5tZmYdQsWEhpmZZc+hYWZmqTk0zMwsNYeGmZmlVjGh4d+eMjPLXsWEhpmZZc+h\nYWZmqTk0zMwsNYeGmZml5tAwM7PUHBpmZpaaQ8PMzFJzaJiZWWoVExp+uM/MLHupQkPSUEkrJa2W\ndFsrdaZKqpe0TFL/gvIbJb2YTDcWlN8haYOkJck09MC7Y2Zm5dRmaEiqAu4BhgD9gNGSTmlWZxhw\nYkScBIwHpifl/YBrgLOA/sDnJZ1QsOqUiDgzmR4rRYfMzKx80pxpDATqI2JdROwA5gIjmtUZATwA\nEBHPAtXJK2BPBZ6NiG0RsRP4PXBpwXoedDIzqyBpQqMn0FAwvyEpK1anMSlbDnxKUhdJhwPDgd4F\n9SYkw1n3S6re59abmdlBVdYL4RGxEpgM/CewEFgK7EwWTwNOiIj+wCZgSjnbYmZmB+7QFHUagT4F\n872SsuZ1erdUJyJmAbMAJP0LyRlJRLxeUP8+4JHWm1BLfT3U1kIulyOXy6VotplZx1FXV0ddXV3Z\n96OIKF5BOgRYBVwEvAY8B4yOiBUFdYYDX4uIz0kaBNwdEYOSZV0j4nVJfYDHgEER8VdJPSJiU1Ln\nZuDsiBjTwv4DgmHDYOHCkvTZzOxDTxIRUfLrxm2eaUTETkkTgEXkh7NmRsQKSePzi2NGRCyUNFzS\ny8C7wNUFm/iFpKOBHcD1EfHXpPyu5NbcXcBa8nddtcrPaZiZZa/NM42s7T7TGD4cfvObrFtjZlYZ\nynWmUTFPhJuZWfYcGmZmlppDw8zMUnNomJlZag4NMzNLzaFhZmapOTTMzCw1h4aZmaVWMaHhJ8LN\nzLJXMaFhZmbZc2iYmVlqDg0zM0vNoWFmZqk5NMzMLDWHhpmZpebQMDOz1FKFhqShklZKWi3ptlbq\nTJVUL2lZ8ka+3eU3SnoxmW4oKO8iaZGkVZIel1RdvA1pu2RmZuXSZmhIqgLuAYYA/YDRkk5pVmcY\ncGJEnET+ta3Tk/J+wDXAWUB/4AuSTkhWmwg8EREnA08Ct5ekR2ZmVjZpzjQGAvURsS4idgBzgRHN\n6owAHgCIiGeBakndgVOBZyNiW0TsBH4PXFqwzuzk82xg5AH1xMzMyi5NaPQEGgrmNyRlxeo0JmXL\ngU8lQ1GHA8OB3kmd7hGxGSAiNgHd9r35ZmZ2MB1azo1HxEpJk4H/BN4BlgI7W6tezraYmdmBSxMa\njUCfgvleSVnzOr1bqhMRs4BZAJL+hT1nJJskdY+IzZJ6AH9pvQm1rFoFtbWQy+XI5XIpmm1m1nHU\n1dVRV1dX9v0oovgXfEmHAKuAi4DXgOeA0RGxoqDOcOBrEfE5SYOAuyNiULKsa0S8LqkP8BgwKCL+\nmpyBvBURk5M7srpExMQW9h8QfP7z8Mgjpem0mdmHnSQiouT3nbZ5phEROyVNABaRvwYyMyJWSBqf\nXxwzImKhpOGSXgbeBa4u2MQvJB0N7ACuj4i/JuWTgYckfRlYB1xRwn6ZmVkZtHmmkTWfaZiZ7bty\nnWn4iXAzM0utYkLDT4SbmWWvYkLDzMyy59AwM7PUHBpmZpaaQ8PMzFJzaJiZWWoODTMzS82hYWZm\nqVVMaPg5DTOz7FVMaJiZWfYcGmZmlppDw8zMUnNomJlZag4NMzNLzaFhZmappQoNSUMlrZS0Onk1\na0t1pkqql7RMUv+C8pslLZf0gqSfSeqUlN8haYOkJck0tDRdMjOzcmkzNCRVAfcAQ4B+wGhJpzSr\nMww4MSJOAsYD05PyjwJfB86MiNPJv152VMGqUyLizGR6rBQdMjOz8klzpjEQqI+IdRGxA5gLjGhW\nZwTwAEBEPAtUS+qeLDsE6CzpUOBwYGPBen5kz8ysgqQJjZ5AQ8H8hqSsWJ1GoGdEbAR+AKxPyrZG\nxBMF9SYkw1n3S6ou1gg/EW5mlr1Dy7lxSUeRPwupAd4G5kkaExFzgGnAdyMiJN0JTAGuaXlLtaxc\nCbW1kMvlyOVy5Wy2mVnFqauro66uruz7UUQUryANAmojYmgyPxGIiJhcUGc68LuI+HkyvxL4NPAp\nYEhEXJuUjwXOiYgJzfZRAzySXPdovv+AYMQIePjhA+ipmVkHIomIKPkYTZrhqcVAX0k1yZ1Po4AF\nzeosAMZBU8hsjYjN5IelBkn6iCQBFwErkno9Cta/FFh+QD0xM7Oya3N4KiJ2SpoALCIfMjMjYoWk\n8fnFMSMiFkoaLull4F3g6mTd5yTNA5YCO5J/zkg2fVdya+4uYC35u67MzKwda3N4KmsenjIz23dZ\nDk+ZmZkBDg0zM9sHDg0zM0utYkLDD/eZmWXPoWFmZqlVTGiYmVn2HBpmZpZaxYSGh6fMzLJXMaFh\nZmbZq5jQ8JmGmVn2KiY0zMwsew4NMzNLrWJCw8NTZmbZq5jQMDOz7FVMaPhMw8wsew4NMzNLLVVo\nSBoqaaWk1ZJua6XOVEn1kpYlb+TbXX6zpOWSXpD0s+SVsUjqImmRpFWSHpdUXZoumZlZubQZGpKq\ngHuAIUA/YLSkU5rVGQacGBEnkX9t6/Sk/KPA14EzI+J08q+XHZWsNhF4IiJOBp4Ebi9Jj8zMrGzS\nnGkMBOojYl1E7ADmAiOa1RkBPAAQEc8C1ZK6J8sOATpLOhQ4HGgsWGd28nk2MHK/e2FmZgdFmtDo\nCTQUzG9IyorVaQR6RsRG4AfA+qRsa0T8NqnTLSI2A0TEJqBbsUb4moaZWfYOLefGJR1F/oyiBngb\nmCdpTETMaaF6tL6lWpYvh9payOVy5HK5MrTWzKxy1dXVUVdXV/b9pAmNRqBPwXwv9gwxFdbp3UKd\nwcArEfEWgKRfAucCc4DNkrpHxGZJPYC/tN6EWv7xH/OhYWZmH9T8C/WkSZPKsp80w1OLgb6SapI7\nn0YBC5rVWQCMA5A0iPww1Gbyw1KDJH1EkoCLgBUF63wp+XwVML9YIzw8ZWaWvTbPNCJip6QJwCLy\nITMzIlZIGp9fHDMiYqGk4ZJeBt4Frk7WfU7SPGApsCP554xk05OBhyR9GVgHXFHqzpmZWWkposil\nhHZAUkAwejTMaelKiJmZfYAkIqLkYzQV80S4mZllr2JCw9c0zMyy59AwM7PUKiY0zMwsew4NMzNL\nrWJCw8NTZmbZq5jQMDOz7FVMaPhMw8wsexUTGmZmlr2KCQ2faZiZZa9iQsPMzLLn0DAzs9QqJjQ8\nPGVmlr2KCQ0zM8texYSGzzTMzLLn0DAzs9RShYakoZJWSlot6bZW6kyVVC9pmaT+SdnHJS2VtCT5\n59uSbkiW3SFpQ7JsiaShxdrwne/sa9fMzKzU2nzdq6Qq4B7y7/feCCyWND8iVhbUGQacGBEnSToH\nmA4MiojVwICC7WwAflmw+SkRMSVNQ3v0SNkjMzMrmzRnGgOB+ohYFxE7gLnAiGZ1RgAPAETEs0C1\npO7N6gwG1kTEhoIyDzqZmVWQNKHRE2gomN+QlBWr09hCnSuBf29WNiEZzrpfUnWKtpiZWYbaHJ4q\nBUmHARcDEwuKpwHfjYiQdCcwBbim5S3UcuedcNhhkMvlyOVyZW6xmVllqauro66uruz7UUQUryAN\nAmojYmgyPxGIiJhcUGc68LuI+HkyvxL4dERsTuYvBq7fvY0W9lEDPBIRp7ewLCD429/gIx/Zrz6a\nmXU4koiIkl8CSDM8tRjoK6lGUidgFLCgWZ0FwDhoCpmtuwMjMZpmQ1OSCi9tXwos38e2m5nZQdbm\n8FRE7JQ0AVhEPmRmRsQKSePzi2NGRCyUNFzSy8C7wNW715d0OPmL4Nc12/Rdya25u4C1wPiS9MjM\nzMqmzeGprHl4ysxs32U5PNUutPNsMzPrEComNMzMLHsVExr+7Skzs+xVTGh4eMrMLHsVExpmZpa9\nigkNn2mYmWWvYkLDzMyy59AwM7PUHBpmZpaaQ8PMzFJzaJiZWWoVExq+e8rMLHsVExpmZpa9igkN\n/4yImVn2KiI0li6Fww/PuhVmZpYqNCQNlbRS0mpJt7VSZ6qkeknLkpcrIenjkpZKWpL8821JNyTL\nukhaJGmVpMclVbe2//7996drZmZWam2GhqQq4B5gCNAPGC3plGZ1hgEnRsRJ5N/ANx0gIlZHxICI\nOBP4J/Jv9ftlstpE4ImIOBl4Eri9NF368DoYL42vFD4We/hY7OFjUX5pzjQGAvURsS4idgBzgRHN\n6owAHgCIiGeBakndm9UZDKyJiA0F68xOPs8GRu5H+zsU/wexh4/FHj4We/hYlF+a0OgJNBTMb0jK\nitVpbKHOlcC/F8x3i4jNABGxCeiWpsFmZpadg3IhXNJhwMXAfxSp5icxzMzau4goOgGDgMcK5icC\ntzWrMx24smB+JdC9YP7iwm0kZSt21wF6ACta2X948uTJk6d9n9r6//v+TIfStsVAX0k1wGvAKGB0\nszoLgK8BP5c0CNi6e+gpMZq9h6Z2r/MlYDJwFTC/pZ1HhJ/QMDNrJxQpfp9D0lDgh+SHs2ZGxPck\njSefZDOSOvcAQ8nfIXV1RCxJyg8H1gEnRMT/K9jm0cBDQO9k+RURsbWUnTMzs9JKFRpmZmbQjp8I\nT/NAYaWT1EvSk5JekvRimgcfJd2ePES5QtJnC8rPlPRCcrzuzqI/pSCpKnkYdEEy3yGPhaRqSf+R\n9O0lSed04GNxs6TlST9+JqlTRzkWkmZK2izphYKykvU9OZZzk3X+JKlPm40qx4WSA53Ih9nLQA1w\nGLAMOCXrdpWhnz2A/snnI4BVwCnkr/N8Mym/Dfhe8vk0YClwKHB8cox2ny0+C5ydfF4IDMm6f/t5\nTG4GHgQWJPMd8lgAPyE/zEvSx+qOeCyAjwKvAJ2S+Z+TvwbaIY4FcD7QH3ihoKxkfQf+JzAt+Xwl\nMLetNrXXM400DxRWvIjYFBHLks/vkL+jrBetP/h4Mfl/qX+PiLVAPTBQUg/gHyJicVLvASrwYUlJ\nvYDhwP0FxR3uWEg6EvhURMwCSPr4Nh3wWCQOATpLOhT47+SfA+sQxyIinga2NCsuZd8LtzUPuKit\nNrXX0EjzQOGHiqTjyX+j+L/kb0Vu6cHH1h6i7En+GO1Wqcfr34D/Rf52wd064rH4GPCGpFnJUN2M\n5IaSDncsImIj8ANgPfl+vR0RT9ABj0WB1h6M3p++N60TETuBrclNSq1qr6HRoUg6gnzK35iccTS/\nO+FDf7eCpM8Bm5Mzr2K3WX/ojwX54YUzgf8T+d9te5f881Ed8e/iKPLfhmvID1V1lvQ/6IDHoohS\n9r3NRxzaa2g0AoUXZHolZR86ySn3POCnEbH7WZXNu3+7Kzm1/EtS3kj+FuXddh+X1soryXnAxZJe\nIf9Mz4WSfgps6oDHYgPQEBF/TuZ/QT5EOuLfxWDglYh4K/km/CvgXDrmsditlH1vWibpEODIiHir\n2M7ba2g0PVAoqRP5BwoXZNymcvkx8F8R8cOCst0PPsLeDz4uAEYldzx8DOgLPJecor4taaAkAeNo\n5WHJ9ioi/ndE9ImIE8j/+34yIsYCj9DxjsVmoEHSx5Oii4CX6IB/F+SHpQZJ+kjSh4uA/6JjHQux\n9xlAKfu+INkGwBfJ/+J4cVnfHVDkroGh5O8mqgcmZt2eMvXxPGAn+bvDlgJLkn4fDTyR9H8RcFTB\nOreTvytiBfDZgvJ/Al5MjtcPs+7bAR6XT7Pn7qkOeSyAM8h/eVpG/nUC1R34WNyR9OsF8hdtD+so\nxwKYA2wEtpEP0KuBLqXqO/DfyD9kXU/+eurxbbXJD/eZmVlq7XV4yszM2iGHhpmZpebQMDOz1Bwa\nZmaWmkPDzMxSc2iYmVlqDg0zM0vNoWFmZqn9f/HG8lYkMrZ8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119135ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
