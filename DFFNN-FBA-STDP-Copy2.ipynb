{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.2770 valid loss: 2.2898, valid accuracy: 0.1474\n",
      "Iter-20 train loss: 2.2606 valid loss: 2.2894, valid accuracy: 0.1482\n",
      "Iter-30 train loss: 2.3129 valid loss: 2.2890, valid accuracy: 0.1490\n",
      "Iter-40 train loss: 2.3884 valid loss: 2.2886, valid accuracy: 0.1494\n",
      "Iter-50 train loss: 2.2874 valid loss: 2.2882, valid accuracy: 0.1502\n",
      "Iter-60 train loss: 2.2748 valid loss: 2.2878, valid accuracy: 0.1506\n",
      "Iter-70 train loss: 2.2766 valid loss: 2.2874, valid accuracy: 0.1516\n",
      "Iter-80 train loss: 2.3113 valid loss: 2.2870, valid accuracy: 0.1524\n",
      "Iter-90 train loss: 2.3051 valid loss: 2.2866, valid accuracy: 0.1528\n",
      "Iter-100 train loss: 2.2945 valid loss: 2.2862, valid accuracy: 0.1536\n",
      "Iter-110 train loss: 2.2623 valid loss: 2.2858, valid accuracy: 0.1546\n",
      "Iter-120 train loss: 2.2779 valid loss: 2.2854, valid accuracy: 0.1558\n",
      "Iter-130 train loss: 2.2915 valid loss: 2.2850, valid accuracy: 0.1566\n",
      "Iter-140 train loss: 2.2998 valid loss: 2.2846, valid accuracy: 0.1576\n",
      "Iter-150 train loss: 2.2586 valid loss: 2.2842, valid accuracy: 0.1582\n",
      "Iter-160 train loss: 2.2884 valid loss: 2.2838, valid accuracy: 0.1588\n",
      "Iter-170 train loss: 2.3359 valid loss: 2.2834, valid accuracy: 0.1600\n",
      "Iter-180 train loss: 2.2891 valid loss: 2.2829, valid accuracy: 0.1614\n",
      "Iter-190 train loss: 2.2977 valid loss: 2.2825, valid accuracy: 0.1622\n",
      "Iter-200 train loss: 2.2699 valid loss: 2.2821, valid accuracy: 0.1636\n",
      "Iter-210 train loss: 2.3032 valid loss: 2.2817, valid accuracy: 0.1650\n",
      "Iter-220 train loss: 2.2312 valid loss: 2.2813, valid accuracy: 0.1650\n",
      "Iter-230 train loss: 2.3418 valid loss: 2.2809, valid accuracy: 0.1662\n",
      "Iter-240 train loss: 2.2428 valid loss: 2.2805, valid accuracy: 0.1666\n",
      "Iter-250 train loss: 2.2393 valid loss: 2.2801, valid accuracy: 0.1668\n",
      "Iter-260 train loss: 2.2510 valid loss: 2.2798, valid accuracy: 0.1674\n",
      "Iter-270 train loss: 2.2606 valid loss: 2.2794, valid accuracy: 0.1688\n",
      "Iter-280 train loss: 2.2522 valid loss: 2.2790, valid accuracy: 0.1688\n",
      "Iter-290 train loss: 2.2828 valid loss: 2.2786, valid accuracy: 0.1696\n",
      "Iter-300 train loss: 2.2791 valid loss: 2.2782, valid accuracy: 0.1704\n",
      "Iter-310 train loss: 2.2965 valid loss: 2.2778, valid accuracy: 0.1714\n",
      "Iter-320 train loss: 2.2723 valid loss: 2.2775, valid accuracy: 0.1722\n",
      "Iter-330 train loss: 2.2554 valid loss: 2.2770, valid accuracy: 0.1732\n",
      "Iter-340 train loss: 2.2620 valid loss: 2.2767, valid accuracy: 0.1740\n",
      "Iter-350 train loss: 2.2923 valid loss: 2.2763, valid accuracy: 0.1746\n",
      "Iter-360 train loss: 2.3344 valid loss: 2.2759, valid accuracy: 0.1756\n",
      "Iter-370 train loss: 2.2896 valid loss: 2.2755, valid accuracy: 0.1768\n",
      "Iter-380 train loss: 2.2444 valid loss: 2.2751, valid accuracy: 0.1770\n",
      "Iter-390 train loss: 2.2855 valid loss: 2.2747, valid accuracy: 0.1774\n",
      "Iter-400 train loss: 2.2757 valid loss: 2.2743, valid accuracy: 0.1776\n",
      "Iter-410 train loss: 2.2426 valid loss: 2.2739, valid accuracy: 0.1782\n",
      "Iter-420 train loss: 2.3042 valid loss: 2.2735, valid accuracy: 0.1792\n",
      "Iter-430 train loss: 2.2840 valid loss: 2.2731, valid accuracy: 0.1804\n",
      "Iter-440 train loss: 2.2763 valid loss: 2.2727, valid accuracy: 0.1808\n",
      "Iter-450 train loss: 2.2648 valid loss: 2.2723, valid accuracy: 0.1816\n",
      "Iter-460 train loss: 2.2718 valid loss: 2.2719, valid accuracy: 0.1826\n",
      "Iter-470 train loss: 2.3197 valid loss: 2.2716, valid accuracy: 0.1836\n",
      "Iter-480 train loss: 2.2916 valid loss: 2.2712, valid accuracy: 0.1842\n",
      "Iter-490 train loss: 2.2631 valid loss: 2.2708, valid accuracy: 0.1850\n",
      "Iter-500 train loss: 2.3246 valid loss: 2.2704, valid accuracy: 0.1854\n",
      "Iter-510 train loss: 2.2786 valid loss: 2.2701, valid accuracy: 0.1858\n",
      "Iter-520 train loss: 2.2543 valid loss: 2.2697, valid accuracy: 0.1870\n",
      "Iter-530 train loss: 2.2622 valid loss: 2.2693, valid accuracy: 0.1880\n",
      "Iter-540 train loss: 2.2313 valid loss: 2.2689, valid accuracy: 0.1890\n",
      "Iter-550 train loss: 2.3148 valid loss: 2.2685, valid accuracy: 0.1908\n",
      "Iter-560 train loss: 2.3130 valid loss: 2.2682, valid accuracy: 0.1914\n",
      "Iter-570 train loss: 2.2629 valid loss: 2.2678, valid accuracy: 0.1920\n",
      "Iter-580 train loss: 2.2497 valid loss: 2.2674, valid accuracy: 0.1928\n",
      "Iter-590 train loss: 2.2947 valid loss: 2.2670, valid accuracy: 0.1936\n",
      "Iter-600 train loss: 2.2770 valid loss: 2.2666, valid accuracy: 0.1940\n",
      "Iter-610 train loss: 2.2946 valid loss: 2.2662, valid accuracy: 0.1944\n",
      "Iter-620 train loss: 2.2402 valid loss: 2.2659, valid accuracy: 0.1958\n",
      "Iter-630 train loss: 2.2517 valid loss: 2.2655, valid accuracy: 0.1964\n",
      "Iter-640 train loss: 2.2770 valid loss: 2.2651, valid accuracy: 0.1966\n",
      "Iter-650 train loss: 2.2940 valid loss: 2.2647, valid accuracy: 0.1970\n",
      "Iter-660 train loss: 2.2119 valid loss: 2.2643, valid accuracy: 0.1984\n",
      "Iter-670 train loss: 2.2309 valid loss: 2.2639, valid accuracy: 0.1998\n",
      "Iter-680 train loss: 2.3153 valid loss: 2.2635, valid accuracy: 0.2004\n",
      "Iter-690 train loss: 2.2381 valid loss: 2.2632, valid accuracy: 0.2014\n",
      "Iter-700 train loss: 2.2570 valid loss: 2.2628, valid accuracy: 0.2024\n",
      "Iter-710 train loss: 2.2372 valid loss: 2.2624, valid accuracy: 0.2036\n",
      "Iter-720 train loss: 2.2849 valid loss: 2.2620, valid accuracy: 0.2038\n",
      "Iter-730 train loss: 2.3099 valid loss: 2.2616, valid accuracy: 0.2044\n",
      "Iter-740 train loss: 2.2930 valid loss: 2.2612, valid accuracy: 0.2046\n",
      "Iter-750 train loss: 2.2771 valid loss: 2.2609, valid accuracy: 0.2050\n",
      "Iter-760 train loss: 2.2586 valid loss: 2.2605, valid accuracy: 0.2056\n",
      "Iter-770 train loss: 2.2646 valid loss: 2.2601, valid accuracy: 0.2070\n",
      "Iter-780 train loss: 2.2948 valid loss: 2.2598, valid accuracy: 0.2086\n",
      "Iter-790 train loss: 2.2864 valid loss: 2.2594, valid accuracy: 0.2094\n",
      "Iter-800 train loss: 2.2318 valid loss: 2.2590, valid accuracy: 0.2108\n",
      "Iter-810 train loss: 2.2214 valid loss: 2.2586, valid accuracy: 0.2108\n",
      "Iter-820 train loss: 2.2629 valid loss: 2.2583, valid accuracy: 0.2110\n",
      "Iter-830 train loss: 2.2280 valid loss: 2.2579, valid accuracy: 0.2116\n",
      "Iter-840 train loss: 2.2463 valid loss: 2.2576, valid accuracy: 0.2126\n",
      "Iter-850 train loss: 2.2426 valid loss: 2.2572, valid accuracy: 0.2124\n",
      "Iter-860 train loss: 2.1688 valid loss: 2.2568, valid accuracy: 0.2138\n",
      "Iter-870 train loss: 2.2678 valid loss: 2.2565, valid accuracy: 0.2140\n",
      "Iter-880 train loss: 2.2539 valid loss: 2.2561, valid accuracy: 0.2148\n",
      "Iter-890 train loss: 2.2538 valid loss: 2.2557, valid accuracy: 0.2148\n",
      "Iter-900 train loss: 2.2934 valid loss: 2.2553, valid accuracy: 0.2160\n",
      "Iter-910 train loss: 2.3167 valid loss: 2.2549, valid accuracy: 0.2164\n",
      "Iter-920 train loss: 2.2386 valid loss: 2.2545, valid accuracy: 0.2174\n",
      "Iter-930 train loss: 2.2253 valid loss: 2.2542, valid accuracy: 0.2184\n",
      "Iter-940 train loss: 2.2905 valid loss: 2.2538, valid accuracy: 0.2190\n",
      "Iter-950 train loss: 2.2609 valid loss: 2.2534, valid accuracy: 0.2200\n",
      "Iter-960 train loss: 2.2547 valid loss: 2.2530, valid accuracy: 0.2206\n",
      "Iter-970 train loss: 2.2720 valid loss: 2.2526, valid accuracy: 0.2212\n",
      "Iter-980 train loss: 2.2970 valid loss: 2.2523, valid accuracy: 0.2216\n",
      "Iter-990 train loss: 2.2371 valid loss: 2.2518, valid accuracy: 0.2218\n",
      "Iter-1000 train loss: 2.2714 valid loss: 2.2515, valid accuracy: 0.2220\n",
      "Iter-1010 train loss: 2.2781 valid loss: 2.2512, valid accuracy: 0.2228\n",
      "Iter-1020 train loss: 2.2378 valid loss: 2.2508, valid accuracy: 0.2230\n",
      "Iter-1030 train loss: 2.2319 valid loss: 2.2504, valid accuracy: 0.2234\n",
      "Iter-1040 train loss: 2.2328 valid loss: 2.2501, valid accuracy: 0.2238\n",
      "Iter-1050 train loss: 2.2707 valid loss: 2.2497, valid accuracy: 0.2240\n",
      "Iter-1060 train loss: 2.3035 valid loss: 2.2493, valid accuracy: 0.2238\n",
      "Iter-1070 train loss: 2.2262 valid loss: 2.2489, valid accuracy: 0.2244\n",
      "Iter-1080 train loss: 2.2604 valid loss: 2.2486, valid accuracy: 0.2246\n",
      "Iter-1090 train loss: 2.2911 valid loss: 2.2482, valid accuracy: 0.2254\n",
      "Iter-1100 train loss: 2.2563 valid loss: 2.2479, valid accuracy: 0.2258\n",
      "Iter-1110 train loss: 2.2392 valid loss: 2.2475, valid accuracy: 0.2254\n",
      "Iter-1120 train loss: 2.2560 valid loss: 2.2472, valid accuracy: 0.2248\n",
      "Iter-1130 train loss: 2.3016 valid loss: 2.2468, valid accuracy: 0.2252\n",
      "Iter-1140 train loss: 2.2704 valid loss: 2.2465, valid accuracy: 0.2252\n",
      "Iter-1150 train loss: 2.2089 valid loss: 2.2461, valid accuracy: 0.2262\n",
      "Iter-1160 train loss: 2.2500 valid loss: 2.2457, valid accuracy: 0.2264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3149 valid loss: 2.2454, valid accuracy: 0.2266\n",
      "Iter-1180 train loss: 2.2376 valid loss: 2.2450, valid accuracy: 0.2274\n",
      "Iter-1190 train loss: 2.2703 valid loss: 2.2446, valid accuracy: 0.2280\n",
      "Iter-1200 train loss: 2.2862 valid loss: 2.2443, valid accuracy: 0.2284\n",
      "Iter-1210 train loss: 2.2631 valid loss: 2.2439, valid accuracy: 0.2290\n",
      "Iter-1220 train loss: 2.2755 valid loss: 2.2435, valid accuracy: 0.2292\n",
      "Iter-1230 train loss: 2.2326 valid loss: 2.2431, valid accuracy: 0.2296\n",
      "Iter-1240 train loss: 2.2267 valid loss: 2.2427, valid accuracy: 0.2304\n",
      "Iter-1250 train loss: 2.2686 valid loss: 2.2423, valid accuracy: 0.2306\n",
      "Iter-1260 train loss: 2.3100 valid loss: 2.2420, valid accuracy: 0.2310\n",
      "Iter-1270 train loss: 2.2674 valid loss: 2.2417, valid accuracy: 0.2316\n",
      "Iter-1280 train loss: 2.2625 valid loss: 2.2413, valid accuracy: 0.2320\n",
      "Iter-1290 train loss: 2.2780 valid loss: 2.2410, valid accuracy: 0.2322\n",
      "Iter-1300 train loss: 2.2461 valid loss: 2.2406, valid accuracy: 0.2322\n",
      "Iter-1310 train loss: 2.2140 valid loss: 2.2403, valid accuracy: 0.2324\n",
      "Iter-1320 train loss: 2.2493 valid loss: 2.2399, valid accuracy: 0.2328\n",
      "Iter-1330 train loss: 2.2618 valid loss: 2.2396, valid accuracy: 0.2328\n",
      "Iter-1340 train loss: 2.2692 valid loss: 2.2393, valid accuracy: 0.2330\n",
      "Iter-1350 train loss: 2.1959 valid loss: 2.2389, valid accuracy: 0.2334\n",
      "Iter-1360 train loss: 2.2120 valid loss: 2.2385, valid accuracy: 0.2344\n",
      "Iter-1370 train loss: 2.2352 valid loss: 2.2381, valid accuracy: 0.2352\n",
      "Iter-1380 train loss: 2.2842 valid loss: 2.2377, valid accuracy: 0.2354\n",
      "Iter-1390 train loss: 2.2353 valid loss: 2.2374, valid accuracy: 0.2364\n",
      "Iter-1400 train loss: 2.2350 valid loss: 2.2371, valid accuracy: 0.2364\n",
      "Iter-1410 train loss: 2.2834 valid loss: 2.2367, valid accuracy: 0.2368\n",
      "Iter-1420 train loss: 2.2611 valid loss: 2.2364, valid accuracy: 0.2376\n",
      "Iter-1430 train loss: 2.2657 valid loss: 2.2360, valid accuracy: 0.2374\n",
      "Iter-1440 train loss: 2.2105 valid loss: 2.2357, valid accuracy: 0.2374\n",
      "Iter-1450 train loss: 2.2660 valid loss: 2.2354, valid accuracy: 0.2380\n",
      "Iter-1460 train loss: 2.2365 valid loss: 2.2350, valid accuracy: 0.2386\n",
      "Iter-1470 train loss: 2.2279 valid loss: 2.2346, valid accuracy: 0.2390\n",
      "Iter-1480 train loss: 2.2031 valid loss: 2.2343, valid accuracy: 0.2388\n",
      "Iter-1490 train loss: 2.2615 valid loss: 2.2339, valid accuracy: 0.2396\n",
      "Iter-1500 train loss: 2.2279 valid loss: 2.2336, valid accuracy: 0.2402\n",
      "Iter-1510 train loss: 2.2355 valid loss: 2.2333, valid accuracy: 0.2408\n",
      "Iter-1520 train loss: 2.2207 valid loss: 2.2329, valid accuracy: 0.2412\n",
      "Iter-1530 train loss: 2.2795 valid loss: 2.2326, valid accuracy: 0.2420\n",
      "Iter-1540 train loss: 2.2535 valid loss: 2.2322, valid accuracy: 0.2426\n",
      "Iter-1550 train loss: 2.2379 valid loss: 2.2319, valid accuracy: 0.2430\n",
      "Iter-1560 train loss: 2.2447 valid loss: 2.2316, valid accuracy: 0.2428\n",
      "Iter-1570 train loss: 2.2507 valid loss: 2.2312, valid accuracy: 0.2428\n",
      "Iter-1580 train loss: 2.1761 valid loss: 2.2309, valid accuracy: 0.2432\n",
      "Iter-1590 train loss: 2.2570 valid loss: 2.2305, valid accuracy: 0.2434\n",
      "Iter-1600 train loss: 2.2497 valid loss: 2.2302, valid accuracy: 0.2436\n",
      "Iter-1610 train loss: 2.2121 valid loss: 2.2298, valid accuracy: 0.2444\n",
      "Iter-1620 train loss: 2.2236 valid loss: 2.2295, valid accuracy: 0.2450\n",
      "Iter-1630 train loss: 2.2221 valid loss: 2.2292, valid accuracy: 0.2450\n",
      "Iter-1640 train loss: 2.2913 valid loss: 2.2288, valid accuracy: 0.2454\n",
      "Iter-1650 train loss: 2.2465 valid loss: 2.2285, valid accuracy: 0.2464\n",
      "Iter-1660 train loss: 2.2297 valid loss: 2.2282, valid accuracy: 0.2464\n",
      "Iter-1670 train loss: 2.2450 valid loss: 2.2278, valid accuracy: 0.2464\n",
      "Iter-1680 train loss: 2.2189 valid loss: 2.2275, valid accuracy: 0.2464\n",
      "Iter-1690 train loss: 2.2575 valid loss: 2.2272, valid accuracy: 0.2464\n",
      "Iter-1700 train loss: 2.2552 valid loss: 2.2268, valid accuracy: 0.2468\n",
      "Iter-1710 train loss: 2.3161 valid loss: 2.2265, valid accuracy: 0.2470\n",
      "Iter-1720 train loss: 2.2707 valid loss: 2.2261, valid accuracy: 0.2476\n",
      "Iter-1730 train loss: 2.2540 valid loss: 2.2258, valid accuracy: 0.2480\n",
      "Iter-1740 train loss: 2.2242 valid loss: 2.2255, valid accuracy: 0.2488\n",
      "Iter-1750 train loss: 2.2845 valid loss: 2.2251, valid accuracy: 0.2500\n",
      "Iter-1760 train loss: 2.2384 valid loss: 2.2248, valid accuracy: 0.2508\n",
      "Iter-1770 train loss: 2.2228 valid loss: 2.2244, valid accuracy: 0.2508\n",
      "Iter-1780 train loss: 2.2251 valid loss: 2.2241, valid accuracy: 0.2514\n",
      "Iter-1790 train loss: 2.2728 valid loss: 2.2238, valid accuracy: 0.2522\n",
      "Iter-1800 train loss: 2.2230 valid loss: 2.2235, valid accuracy: 0.2528\n",
      "Iter-1810 train loss: 2.2317 valid loss: 2.2231, valid accuracy: 0.2544\n",
      "Iter-1820 train loss: 2.2349 valid loss: 2.2228, valid accuracy: 0.2542\n",
      "Iter-1830 train loss: 2.1715 valid loss: 2.2224, valid accuracy: 0.2544\n",
      "Iter-1840 train loss: 2.2251 valid loss: 2.2220, valid accuracy: 0.2548\n",
      "Iter-1850 train loss: 2.2497 valid loss: 2.2217, valid accuracy: 0.2552\n",
      "Iter-1860 train loss: 2.2369 valid loss: 2.2213, valid accuracy: 0.2556\n",
      "Iter-1870 train loss: 2.2075 valid loss: 2.2210, valid accuracy: 0.2558\n",
      "Iter-1880 train loss: 2.2324 valid loss: 2.2206, valid accuracy: 0.2568\n",
      "Iter-1890 train loss: 2.2062 valid loss: 2.2203, valid accuracy: 0.2568\n",
      "Iter-1900 train loss: 2.2188 valid loss: 2.2199, valid accuracy: 0.2568\n",
      "Iter-1910 train loss: 2.2223 valid loss: 2.2196, valid accuracy: 0.2572\n",
      "Iter-1920 train loss: 2.2150 valid loss: 2.2193, valid accuracy: 0.2578\n",
      "Iter-1930 train loss: 2.2074 valid loss: 2.2189, valid accuracy: 0.2578\n",
      "Iter-1940 train loss: 2.1979 valid loss: 2.2186, valid accuracy: 0.2580\n",
      "Iter-1950 train loss: 2.2112 valid loss: 2.2183, valid accuracy: 0.2586\n",
      "Iter-1960 train loss: 2.1932 valid loss: 2.2180, valid accuracy: 0.2584\n",
      "Iter-1970 train loss: 2.2078 valid loss: 2.2177, valid accuracy: 0.2586\n",
      "Iter-1980 train loss: 2.1995 valid loss: 2.2173, valid accuracy: 0.2592\n",
      "Iter-1990 train loss: 2.1671 valid loss: 2.2170, valid accuracy: 0.2592\n",
      "Iter-2000 train loss: 2.1923 valid loss: 2.2166, valid accuracy: 0.2592\n",
      "Iter-2010 train loss: 2.2031 valid loss: 2.2163, valid accuracy: 0.2592\n",
      "Iter-2020 train loss: 2.2015 valid loss: 2.2160, valid accuracy: 0.2596\n",
      "Iter-2030 train loss: 2.2570 valid loss: 2.2156, valid accuracy: 0.2592\n",
      "Iter-2040 train loss: 2.1876 valid loss: 2.2153, valid accuracy: 0.2598\n",
      "Iter-2050 train loss: 2.2752 valid loss: 2.2150, valid accuracy: 0.2600\n",
      "Iter-2060 train loss: 2.2173 valid loss: 2.2146, valid accuracy: 0.2600\n",
      "Iter-2070 train loss: 2.1818 valid loss: 2.2143, valid accuracy: 0.2600\n",
      "Iter-2080 train loss: 2.2750 valid loss: 2.2140, valid accuracy: 0.2608\n",
      "Iter-2090 train loss: 2.2111 valid loss: 2.2137, valid accuracy: 0.2608\n",
      "Iter-2100 train loss: 2.2031 valid loss: 2.2133, valid accuracy: 0.2610\n",
      "Iter-2110 train loss: 2.2076 valid loss: 2.2130, valid accuracy: 0.2616\n",
      "Iter-2120 train loss: 2.1717 valid loss: 2.2126, valid accuracy: 0.2622\n",
      "Iter-2130 train loss: 2.2476 valid loss: 2.2123, valid accuracy: 0.2624\n",
      "Iter-2140 train loss: 2.2281 valid loss: 2.2120, valid accuracy: 0.2628\n",
      "Iter-2150 train loss: 2.2626 valid loss: 2.2116, valid accuracy: 0.2630\n",
      "Iter-2160 train loss: 2.2150 valid loss: 2.2113, valid accuracy: 0.2634\n",
      "Iter-2170 train loss: 2.1920 valid loss: 2.2110, valid accuracy: 0.2636\n",
      "Iter-2180 train loss: 2.2872 valid loss: 2.2106, valid accuracy: 0.2636\n",
      "Iter-2190 train loss: 2.2150 valid loss: 2.2103, valid accuracy: 0.2636\n",
      "Iter-2200 train loss: 2.1644 valid loss: 2.2099, valid accuracy: 0.2638\n",
      "Iter-2210 train loss: 2.2123 valid loss: 2.2096, valid accuracy: 0.2638\n",
      "Iter-2220 train loss: 2.2264 valid loss: 2.2093, valid accuracy: 0.2640\n",
      "Iter-2230 train loss: 2.2313 valid loss: 2.2090, valid accuracy: 0.2638\n",
      "Iter-2240 train loss: 2.2257 valid loss: 2.2087, valid accuracy: 0.2642\n",
      "Iter-2250 train loss: 2.2317 valid loss: 2.2083, valid accuracy: 0.2642\n",
      "Iter-2260 train loss: 2.2377 valid loss: 2.2080, valid accuracy: 0.2646\n",
      "Iter-2270 train loss: 2.1789 valid loss: 2.2077, valid accuracy: 0.2658\n",
      "Iter-2280 train loss: 2.2010 valid loss: 2.2073, valid accuracy: 0.2660\n",
      "Iter-2290 train loss: 2.2399 valid loss: 2.2070, valid accuracy: 0.2664\n",
      "Iter-2300 train loss: 2.2377 valid loss: 2.2067, valid accuracy: 0.2664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.2148 valid loss: 2.2063, valid accuracy: 0.2666\n",
      "Iter-2320 train loss: 2.2003 valid loss: 2.2060, valid accuracy: 0.2670\n",
      "Iter-2330 train loss: 2.1873 valid loss: 2.2057, valid accuracy: 0.2672\n",
      "Iter-2340 train loss: 2.2023 valid loss: 2.2054, valid accuracy: 0.2672\n",
      "Iter-2350 train loss: 2.2622 valid loss: 2.2050, valid accuracy: 0.2676\n",
      "Iter-2360 train loss: 2.2077 valid loss: 2.2047, valid accuracy: 0.2678\n",
      "Iter-2370 train loss: 2.2189 valid loss: 2.2044, valid accuracy: 0.2686\n",
      "Iter-2380 train loss: 2.1819 valid loss: 2.2041, valid accuracy: 0.2692\n",
      "Iter-2390 train loss: 2.2171 valid loss: 2.2038, valid accuracy: 0.2694\n",
      "Iter-2400 train loss: 2.1959 valid loss: 2.2035, valid accuracy: 0.2692\n",
      "Iter-2410 train loss: 2.2154 valid loss: 2.2031, valid accuracy: 0.2692\n",
      "Iter-2420 train loss: 2.1804 valid loss: 2.2028, valid accuracy: 0.2696\n",
      "Iter-2430 train loss: 2.1856 valid loss: 2.2025, valid accuracy: 0.2700\n",
      "Iter-2440 train loss: 2.1495 valid loss: 2.2022, valid accuracy: 0.2702\n",
      "Iter-2450 train loss: 2.2209 valid loss: 2.2018, valid accuracy: 0.2710\n",
      "Iter-2460 train loss: 2.1713 valid loss: 2.2015, valid accuracy: 0.2714\n",
      "Iter-2470 train loss: 2.2173 valid loss: 2.2012, valid accuracy: 0.2716\n",
      "Iter-2480 train loss: 2.2023 valid loss: 2.2008, valid accuracy: 0.2718\n",
      "Iter-2490 train loss: 2.1190 valid loss: 2.2005, valid accuracy: 0.2726\n",
      "Iter-2500 train loss: 2.2015 valid loss: 2.2002, valid accuracy: 0.2726\n",
      "Iter-2510 train loss: 2.2784 valid loss: 2.1999, valid accuracy: 0.2734\n",
      "Iter-2520 train loss: 2.2155 valid loss: 2.1996, valid accuracy: 0.2734\n",
      "Iter-2530 train loss: 2.2190 valid loss: 2.1993, valid accuracy: 0.2734\n",
      "Iter-2540 train loss: 2.1935 valid loss: 2.1990, valid accuracy: 0.2738\n",
      "Iter-2550 train loss: 2.1828 valid loss: 2.1986, valid accuracy: 0.2742\n",
      "Iter-2560 train loss: 2.1475 valid loss: 2.1983, valid accuracy: 0.2746\n",
      "Iter-2570 train loss: 2.1925 valid loss: 2.1980, valid accuracy: 0.2744\n",
      "Iter-2580 train loss: 2.1842 valid loss: 2.1977, valid accuracy: 0.2748\n",
      "Iter-2590 train loss: 2.2672 valid loss: 2.1974, valid accuracy: 0.2754\n",
      "Iter-2600 train loss: 2.2102 valid loss: 2.1970, valid accuracy: 0.2764\n",
      "Iter-2610 train loss: 2.2185 valid loss: 2.1967, valid accuracy: 0.2770\n",
      "Iter-2620 train loss: 2.2276 valid loss: 2.1964, valid accuracy: 0.2776\n",
      "Iter-2630 train loss: 2.1774 valid loss: 2.1961, valid accuracy: 0.2776\n",
      "Iter-2640 train loss: 2.2166 valid loss: 2.1958, valid accuracy: 0.2778\n",
      "Iter-2650 train loss: 2.2228 valid loss: 2.1955, valid accuracy: 0.2784\n",
      "Iter-2660 train loss: 2.1959 valid loss: 2.1952, valid accuracy: 0.2784\n",
      "Iter-2670 train loss: 2.2516 valid loss: 2.1949, valid accuracy: 0.2788\n",
      "Iter-2680 train loss: 2.2101 valid loss: 2.1945, valid accuracy: 0.2792\n",
      "Iter-2690 train loss: 2.2033 valid loss: 2.1942, valid accuracy: 0.2796\n",
      "Iter-2700 train loss: 2.2061 valid loss: 2.1939, valid accuracy: 0.2802\n",
      "Iter-2710 train loss: 2.2041 valid loss: 2.1936, valid accuracy: 0.2806\n",
      "Iter-2720 train loss: 2.2155 valid loss: 2.1933, valid accuracy: 0.2804\n",
      "Iter-2730 train loss: 2.1481 valid loss: 2.1931, valid accuracy: 0.2804\n",
      "Iter-2740 train loss: 2.1876 valid loss: 2.1927, valid accuracy: 0.2812\n",
      "Iter-2750 train loss: 2.2059 valid loss: 2.1924, valid accuracy: 0.2816\n",
      "Iter-2760 train loss: 2.2036 valid loss: 2.1921, valid accuracy: 0.2816\n",
      "Iter-2770 train loss: 2.1916 valid loss: 2.1918, valid accuracy: 0.2822\n",
      "Iter-2780 train loss: 2.1751 valid loss: 2.1915, valid accuracy: 0.2826\n",
      "Iter-2790 train loss: 2.1838 valid loss: 2.1912, valid accuracy: 0.2824\n",
      "Iter-2800 train loss: 2.2014 valid loss: 2.1909, valid accuracy: 0.2830\n",
      "Iter-2810 train loss: 2.1801 valid loss: 2.1906, valid accuracy: 0.2838\n",
      "Iter-2820 train loss: 2.1897 valid loss: 2.1903, valid accuracy: 0.2848\n",
      "Iter-2830 train loss: 2.2140 valid loss: 2.1900, valid accuracy: 0.2852\n",
      "Iter-2840 train loss: 2.2221 valid loss: 2.1897, valid accuracy: 0.2860\n",
      "Iter-2850 train loss: 2.1672 valid loss: 2.1894, valid accuracy: 0.2862\n",
      "Iter-2860 train loss: 2.1842 valid loss: 2.1890, valid accuracy: 0.2874\n",
      "Iter-2870 train loss: 2.1971 valid loss: 2.1888, valid accuracy: 0.2880\n",
      "Iter-2880 train loss: 2.1894 valid loss: 2.1885, valid accuracy: 0.2884\n",
      "Iter-2890 train loss: 2.2120 valid loss: 2.1882, valid accuracy: 0.2882\n",
      "Iter-2900 train loss: 2.1719 valid loss: 2.1879, valid accuracy: 0.2890\n",
      "Iter-2910 train loss: 2.2093 valid loss: 2.1876, valid accuracy: 0.2892\n",
      "Iter-2920 train loss: 2.1792 valid loss: 2.1873, valid accuracy: 0.2896\n",
      "Iter-2930 train loss: 2.1994 valid loss: 2.1870, valid accuracy: 0.2902\n",
      "Iter-2940 train loss: 2.1937 valid loss: 2.1867, valid accuracy: 0.2912\n",
      "Iter-2950 train loss: 2.2291 valid loss: 2.1863, valid accuracy: 0.2916\n",
      "Iter-2960 train loss: 2.2404 valid loss: 2.1861, valid accuracy: 0.2920\n",
      "Iter-2970 train loss: 2.2063 valid loss: 2.1858, valid accuracy: 0.2924\n",
      "Iter-2980 train loss: 2.1749 valid loss: 2.1855, valid accuracy: 0.2928\n",
      "Iter-2990 train loss: 2.1666 valid loss: 2.1851, valid accuracy: 0.2934\n",
      "Iter-3000 train loss: 2.2811 valid loss: 2.1848, valid accuracy: 0.2932\n",
      "Iter-3010 train loss: 2.2074 valid loss: 2.1845, valid accuracy: 0.2932\n",
      "Iter-3020 train loss: 2.2267 valid loss: 2.1843, valid accuracy: 0.2938\n",
      "Iter-3030 train loss: 2.2255 valid loss: 2.1839, valid accuracy: 0.2942\n",
      "Iter-3040 train loss: 2.2433 valid loss: 2.1837, valid accuracy: 0.2944\n",
      "Iter-3050 train loss: 2.1728 valid loss: 2.1834, valid accuracy: 0.2946\n",
      "Iter-3060 train loss: 2.1626 valid loss: 2.1831, valid accuracy: 0.2946\n",
      "Iter-3070 train loss: 2.2247 valid loss: 2.1828, valid accuracy: 0.2952\n",
      "Iter-3080 train loss: 2.1641 valid loss: 2.1825, valid accuracy: 0.2952\n",
      "Iter-3090 train loss: 2.1828 valid loss: 2.1821, valid accuracy: 0.2952\n",
      "Iter-3100 train loss: 2.2081 valid loss: 2.1818, valid accuracy: 0.2954\n",
      "Iter-3110 train loss: 2.1431 valid loss: 2.1815, valid accuracy: 0.2958\n",
      "Iter-3120 train loss: 2.1421 valid loss: 2.1812, valid accuracy: 0.2964\n",
      "Iter-3130 train loss: 2.1585 valid loss: 2.1809, valid accuracy: 0.2962\n",
      "Iter-3140 train loss: 2.1518 valid loss: 2.1806, valid accuracy: 0.2964\n",
      "Iter-3150 train loss: 2.1611 valid loss: 2.1803, valid accuracy: 0.2964\n",
      "Iter-3160 train loss: 2.1835 valid loss: 2.1800, valid accuracy: 0.2968\n",
      "Iter-3170 train loss: 2.2074 valid loss: 2.1797, valid accuracy: 0.2972\n",
      "Iter-3180 train loss: 2.1757 valid loss: 2.1794, valid accuracy: 0.2982\n",
      "Iter-3190 train loss: 2.2110 valid loss: 2.1791, valid accuracy: 0.2988\n",
      "Iter-3200 train loss: 2.1811 valid loss: 2.1788, valid accuracy: 0.2998\n",
      "Iter-3210 train loss: 2.2277 valid loss: 2.1786, valid accuracy: 0.3002\n",
      "Iter-3220 train loss: 2.1774 valid loss: 2.1783, valid accuracy: 0.3000\n",
      "Iter-3230 train loss: 2.1979 valid loss: 2.1780, valid accuracy: 0.3006\n",
      "Iter-3240 train loss: 2.1869 valid loss: 2.1777, valid accuracy: 0.3012\n",
      "Iter-3250 train loss: 2.1592 valid loss: 2.1774, valid accuracy: 0.3012\n",
      "Iter-3260 train loss: 2.1462 valid loss: 2.1771, valid accuracy: 0.3014\n",
      "Iter-3270 train loss: 2.1788 valid loss: 2.1768, valid accuracy: 0.3022\n",
      "Iter-3280 train loss: 2.2601 valid loss: 2.1765, valid accuracy: 0.3022\n",
      "Iter-3290 train loss: 2.1372 valid loss: 2.1762, valid accuracy: 0.3026\n",
      "Iter-3300 train loss: 2.2053 valid loss: 2.1759, valid accuracy: 0.3026\n",
      "Iter-3310 train loss: 2.1576 valid loss: 2.1757, valid accuracy: 0.3028\n",
      "Iter-3320 train loss: 2.2108 valid loss: 2.1754, valid accuracy: 0.3032\n",
      "Iter-3330 train loss: 2.2010 valid loss: 2.1751, valid accuracy: 0.3032\n",
      "Iter-3340 train loss: 2.2162 valid loss: 2.1748, valid accuracy: 0.3036\n",
      "Iter-3350 train loss: 2.1428 valid loss: 2.1745, valid accuracy: 0.3042\n",
      "Iter-3360 train loss: 2.1710 valid loss: 2.1742, valid accuracy: 0.3044\n",
      "Iter-3370 train loss: 2.2576 valid loss: 2.1739, valid accuracy: 0.3050\n",
      "Iter-3380 train loss: 2.2057 valid loss: 2.1736, valid accuracy: 0.3052\n",
      "Iter-3390 train loss: 2.1704 valid loss: 2.1733, valid accuracy: 0.3048\n",
      "Iter-3400 train loss: 2.2384 valid loss: 2.1730, valid accuracy: 0.3050\n",
      "Iter-3410 train loss: 2.0997 valid loss: 2.1727, valid accuracy: 0.3054\n",
      "Iter-3420 train loss: 2.2165 valid loss: 2.1724, valid accuracy: 0.3058\n",
      "Iter-3430 train loss: 2.1554 valid loss: 2.1721, valid accuracy: 0.3052\n",
      "Iter-3440 train loss: 2.1426 valid loss: 2.1719, valid accuracy: 0.3060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.1754 valid loss: 2.1716, valid accuracy: 0.3078\n",
      "Iter-3460 train loss: 2.1776 valid loss: 2.1713, valid accuracy: 0.3082\n",
      "Iter-3470 train loss: 2.1602 valid loss: 2.1710, valid accuracy: 0.3082\n",
      "Iter-3480 train loss: 2.1946 valid loss: 2.1707, valid accuracy: 0.3090\n",
      "Iter-3490 train loss: 2.1421 valid loss: 2.1705, valid accuracy: 0.3090\n",
      "Iter-3500 train loss: 2.1625 valid loss: 2.1701, valid accuracy: 0.3090\n",
      "Iter-3510 train loss: 2.2187 valid loss: 2.1699, valid accuracy: 0.3098\n",
      "Iter-3520 train loss: 2.2258 valid loss: 2.1696, valid accuracy: 0.3102\n",
      "Iter-3530 train loss: 2.1683 valid loss: 2.1693, valid accuracy: 0.3106\n",
      "Iter-3540 train loss: 2.1853 valid loss: 2.1690, valid accuracy: 0.3108\n",
      "Iter-3550 train loss: 2.1385 valid loss: 2.1687, valid accuracy: 0.3108\n",
      "Iter-3560 train loss: 2.1126 valid loss: 2.1684, valid accuracy: 0.3110\n",
      "Iter-3570 train loss: 2.1289 valid loss: 2.1681, valid accuracy: 0.3114\n",
      "Iter-3580 train loss: 2.1964 valid loss: 2.1679, valid accuracy: 0.3114\n",
      "Iter-3590 train loss: 2.2821 valid loss: 2.1676, valid accuracy: 0.3116\n",
      "Iter-3600 train loss: 2.1446 valid loss: 2.1673, valid accuracy: 0.3126\n",
      "Iter-3610 train loss: 2.2359 valid loss: 2.1670, valid accuracy: 0.3126\n",
      "Iter-3620 train loss: 2.1620 valid loss: 2.1667, valid accuracy: 0.3128\n",
      "Iter-3630 train loss: 2.1351 valid loss: 2.1665, valid accuracy: 0.3132\n",
      "Iter-3640 train loss: 2.1593 valid loss: 2.1662, valid accuracy: 0.3132\n",
      "Iter-3650 train loss: 2.2641 valid loss: 2.1659, valid accuracy: 0.3134\n",
      "Iter-3660 train loss: 2.1835 valid loss: 2.1656, valid accuracy: 0.3140\n",
      "Iter-3670 train loss: 2.2087 valid loss: 2.1653, valid accuracy: 0.3142\n",
      "Iter-3680 train loss: 2.2139 valid loss: 2.1650, valid accuracy: 0.3148\n",
      "Iter-3690 train loss: 2.1385 valid loss: 2.1647, valid accuracy: 0.3154\n",
      "Iter-3700 train loss: 2.1456 valid loss: 2.1644, valid accuracy: 0.3152\n",
      "Iter-3710 train loss: 2.1720 valid loss: 2.1641, valid accuracy: 0.3150\n",
      "Iter-3720 train loss: 2.1696 valid loss: 2.1639, valid accuracy: 0.3150\n",
      "Iter-3730 train loss: 2.1484 valid loss: 2.1636, valid accuracy: 0.3156\n",
      "Iter-3740 train loss: 2.1669 valid loss: 2.1633, valid accuracy: 0.3160\n",
      "Iter-3750 train loss: 2.1354 valid loss: 2.1630, valid accuracy: 0.3164\n",
      "Iter-3760 train loss: 2.1599 valid loss: 2.1628, valid accuracy: 0.3168\n",
      "Iter-3770 train loss: 2.1321 valid loss: 2.1625, valid accuracy: 0.3170\n",
      "Iter-3780 train loss: 2.2118 valid loss: 2.1622, valid accuracy: 0.3176\n",
      "Iter-3790 train loss: 2.1604 valid loss: 2.1619, valid accuracy: 0.3174\n",
      "Iter-3800 train loss: 2.1582 valid loss: 2.1616, valid accuracy: 0.3174\n",
      "Iter-3810 train loss: 2.1755 valid loss: 2.1613, valid accuracy: 0.3178\n",
      "Iter-3820 train loss: 2.0964 valid loss: 2.1610, valid accuracy: 0.3178\n",
      "Iter-3830 train loss: 2.1503 valid loss: 2.1607, valid accuracy: 0.3178\n",
      "Iter-3840 train loss: 2.1165 valid loss: 2.1604, valid accuracy: 0.3176\n",
      "Iter-3850 train loss: 2.1481 valid loss: 2.1602, valid accuracy: 0.3186\n",
      "Iter-3860 train loss: 2.1795 valid loss: 2.1599, valid accuracy: 0.3186\n",
      "Iter-3870 train loss: 2.1202 valid loss: 2.1596, valid accuracy: 0.3192\n",
      "Iter-3880 train loss: 2.2228 valid loss: 2.1593, valid accuracy: 0.3198\n",
      "Iter-3890 train loss: 2.1499 valid loss: 2.1591, valid accuracy: 0.3204\n",
      "Iter-3900 train loss: 2.1284 valid loss: 2.1588, valid accuracy: 0.3204\n",
      "Iter-3910 train loss: 2.2046 valid loss: 2.1585, valid accuracy: 0.3208\n",
      "Iter-3920 train loss: 2.1593 valid loss: 2.1582, valid accuracy: 0.3210\n",
      "Iter-3930 train loss: 2.1243 valid loss: 2.1579, valid accuracy: 0.3206\n",
      "Iter-3940 train loss: 2.1624 valid loss: 2.1577, valid accuracy: 0.3206\n",
      "Iter-3950 train loss: 2.1534 valid loss: 2.1574, valid accuracy: 0.3204\n",
      "Iter-3960 train loss: 2.1614 valid loss: 2.1571, valid accuracy: 0.3208\n",
      "Iter-3970 train loss: 2.1551 valid loss: 2.1569, valid accuracy: 0.3208\n",
      "Iter-3980 train loss: 2.1767 valid loss: 2.1566, valid accuracy: 0.3212\n",
      "Iter-3990 train loss: 2.1848 valid loss: 2.1563, valid accuracy: 0.3216\n",
      "Iter-4000 train loss: 2.1972 valid loss: 2.1561, valid accuracy: 0.3216\n",
      "Iter-4010 train loss: 2.1256 valid loss: 2.1558, valid accuracy: 0.3226\n",
      "Iter-4020 train loss: 2.1252 valid loss: 2.1555, valid accuracy: 0.3234\n",
      "Iter-4030 train loss: 2.1523 valid loss: 2.1552, valid accuracy: 0.3232\n",
      "Iter-4040 train loss: 2.1797 valid loss: 2.1550, valid accuracy: 0.3232\n",
      "Iter-4050 train loss: 2.1815 valid loss: 2.1547, valid accuracy: 0.3232\n",
      "Iter-4060 train loss: 2.1798 valid loss: 2.1544, valid accuracy: 0.3234\n",
      "Iter-4070 train loss: 2.1295 valid loss: 2.1541, valid accuracy: 0.3236\n",
      "Iter-4080 train loss: 2.1419 valid loss: 2.1539, valid accuracy: 0.3240\n",
      "Iter-4090 train loss: 2.1922 valid loss: 2.1536, valid accuracy: 0.3242\n",
      "Iter-4100 train loss: 2.1605 valid loss: 2.1533, valid accuracy: 0.3252\n",
      "Iter-4110 train loss: 2.1710 valid loss: 2.1530, valid accuracy: 0.3252\n",
      "Iter-4120 train loss: 2.1498 valid loss: 2.1528, valid accuracy: 0.3250\n",
      "Iter-4130 train loss: 2.1058 valid loss: 2.1525, valid accuracy: 0.3252\n",
      "Iter-4140 train loss: 2.1255 valid loss: 2.1522, valid accuracy: 0.3250\n",
      "Iter-4150 train loss: 2.1019 valid loss: 2.1519, valid accuracy: 0.3252\n",
      "Iter-4160 train loss: 2.1830 valid loss: 2.1517, valid accuracy: 0.3256\n",
      "Iter-4170 train loss: 2.1569 valid loss: 2.1514, valid accuracy: 0.3254\n",
      "Iter-4180 train loss: 2.2062 valid loss: 2.1512, valid accuracy: 0.3258\n",
      "Iter-4190 train loss: 2.1543 valid loss: 2.1509, valid accuracy: 0.3256\n",
      "Iter-4200 train loss: 2.1679 valid loss: 2.1506, valid accuracy: 0.3268\n",
      "Iter-4210 train loss: 2.1407 valid loss: 2.1504, valid accuracy: 0.3272\n",
      "Iter-4220 train loss: 2.1745 valid loss: 2.1501, valid accuracy: 0.3272\n",
      "Iter-4230 train loss: 2.1333 valid loss: 2.1498, valid accuracy: 0.3276\n",
      "Iter-4240 train loss: 2.1892 valid loss: 2.1496, valid accuracy: 0.3276\n",
      "Iter-4250 train loss: 2.1824 valid loss: 2.1493, valid accuracy: 0.3276\n",
      "Iter-4260 train loss: 2.1595 valid loss: 2.1490, valid accuracy: 0.3282\n",
      "Iter-4270 train loss: 2.0961 valid loss: 2.1487, valid accuracy: 0.3282\n",
      "Iter-4280 train loss: 2.1321 valid loss: 2.1485, valid accuracy: 0.3282\n",
      "Iter-4290 train loss: 2.1270 valid loss: 2.1482, valid accuracy: 0.3288\n",
      "Iter-4300 train loss: 2.1845 valid loss: 2.1480, valid accuracy: 0.3290\n",
      "Iter-4310 train loss: 2.1230 valid loss: 2.1477, valid accuracy: 0.3292\n",
      "Iter-4320 train loss: 2.1588 valid loss: 2.1475, valid accuracy: 0.3294\n",
      "Iter-4330 train loss: 2.1277 valid loss: 2.1472, valid accuracy: 0.3296\n",
      "Iter-4340 train loss: 2.2406 valid loss: 2.1469, valid accuracy: 0.3298\n",
      "Iter-4350 train loss: 2.1104 valid loss: 2.1467, valid accuracy: 0.3300\n",
      "Iter-4360 train loss: 2.1089 valid loss: 2.1464, valid accuracy: 0.3300\n",
      "Iter-4370 train loss: 2.1508 valid loss: 2.1462, valid accuracy: 0.3302\n",
      "Iter-4380 train loss: 2.1235 valid loss: 2.1459, valid accuracy: 0.3314\n",
      "Iter-4390 train loss: 2.1045 valid loss: 2.1456, valid accuracy: 0.3312\n",
      "Iter-4400 train loss: 2.1296 valid loss: 2.1453, valid accuracy: 0.3308\n",
      "Iter-4410 train loss: 2.0702 valid loss: 2.1451, valid accuracy: 0.3308\n",
      "Iter-4420 train loss: 2.1489 valid loss: 2.1448, valid accuracy: 0.3318\n",
      "Iter-4430 train loss: 2.1336 valid loss: 2.1445, valid accuracy: 0.3324\n",
      "Iter-4440 train loss: 2.1143 valid loss: 2.1443, valid accuracy: 0.3318\n",
      "Iter-4450 train loss: 2.1298 valid loss: 2.1440, valid accuracy: 0.3322\n",
      "Iter-4460 train loss: 2.1688 valid loss: 2.1437, valid accuracy: 0.3322\n",
      "Iter-4470 train loss: 2.1318 valid loss: 2.1435, valid accuracy: 0.3318\n",
      "Iter-4480 train loss: 2.1120 valid loss: 2.1432, valid accuracy: 0.3322\n",
      "Iter-4490 train loss: 2.1941 valid loss: 2.1430, valid accuracy: 0.3322\n",
      "Iter-4500 train loss: 2.1273 valid loss: 2.1427, valid accuracy: 0.3320\n",
      "Iter-4510 train loss: 2.1693 valid loss: 2.1425, valid accuracy: 0.3326\n",
      "Iter-4520 train loss: 2.1149 valid loss: 2.1422, valid accuracy: 0.3332\n",
      "Iter-4530 train loss: 2.1177 valid loss: 2.1419, valid accuracy: 0.3336\n",
      "Iter-4540 train loss: 2.1444 valid loss: 2.1417, valid accuracy: 0.3332\n",
      "Iter-4550 train loss: 2.1761 valid loss: 2.1414, valid accuracy: 0.3336\n",
      "Iter-4560 train loss: 2.1129 valid loss: 2.1412, valid accuracy: 0.3334\n",
      "Iter-4570 train loss: 2.2374 valid loss: 2.1409, valid accuracy: 0.3338\n",
      "Iter-4580 train loss: 2.1638 valid loss: 2.1407, valid accuracy: 0.3342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.1725 valid loss: 2.1404, valid accuracy: 0.3338\n",
      "Iter-4600 train loss: 2.0924 valid loss: 2.1402, valid accuracy: 0.3342\n",
      "Iter-4610 train loss: 2.2055 valid loss: 2.1399, valid accuracy: 0.3342\n",
      "Iter-4620 train loss: 2.1796 valid loss: 2.1397, valid accuracy: 0.3342\n",
      "Iter-4630 train loss: 2.1155 valid loss: 2.1394, valid accuracy: 0.3346\n",
      "Iter-4640 train loss: 2.0770 valid loss: 2.1391, valid accuracy: 0.3348\n",
      "Iter-4650 train loss: 2.1399 valid loss: 2.1389, valid accuracy: 0.3346\n",
      "Iter-4660 train loss: 2.1005 valid loss: 2.1386, valid accuracy: 0.3348\n",
      "Iter-4670 train loss: 2.0877 valid loss: 2.1384, valid accuracy: 0.3350\n",
      "Iter-4680 train loss: 2.1663 valid loss: 2.1381, valid accuracy: 0.3354\n",
      "Iter-4690 train loss: 2.1012 valid loss: 2.1378, valid accuracy: 0.3350\n",
      "Iter-4700 train loss: 2.0675 valid loss: 2.1376, valid accuracy: 0.3352\n",
      "Iter-4710 train loss: 2.1721 valid loss: 2.1373, valid accuracy: 0.3352\n",
      "Iter-4720 train loss: 2.1846 valid loss: 2.1371, valid accuracy: 0.3356\n",
      "Iter-4730 train loss: 2.2038 valid loss: 2.1368, valid accuracy: 0.3356\n",
      "Iter-4740 train loss: 2.1572 valid loss: 2.1366, valid accuracy: 0.3360\n",
      "Iter-4750 train loss: 2.0904 valid loss: 2.1363, valid accuracy: 0.3362\n",
      "Iter-4760 train loss: 2.1111 valid loss: 2.1360, valid accuracy: 0.3360\n",
      "Iter-4770 train loss: 2.1434 valid loss: 2.1357, valid accuracy: 0.3354\n",
      "Iter-4780 train loss: 2.2057 valid loss: 2.1355, valid accuracy: 0.3360\n",
      "Iter-4790 train loss: 2.1827 valid loss: 2.1353, valid accuracy: 0.3360\n",
      "Iter-4800 train loss: 2.0488 valid loss: 2.1350, valid accuracy: 0.3360\n",
      "Iter-4810 train loss: 2.1543 valid loss: 2.1348, valid accuracy: 0.3358\n",
      "Iter-4820 train loss: 2.0668 valid loss: 2.1345, valid accuracy: 0.3356\n",
      "Iter-4830 train loss: 2.1687 valid loss: 2.1343, valid accuracy: 0.3358\n",
      "Iter-4840 train loss: 2.1295 valid loss: 2.1340, valid accuracy: 0.3358\n",
      "Iter-4850 train loss: 2.1218 valid loss: 2.1338, valid accuracy: 0.3356\n",
      "Iter-4860 train loss: 2.1814 valid loss: 2.1335, valid accuracy: 0.3354\n",
      "Iter-4870 train loss: 2.1233 valid loss: 2.1333, valid accuracy: 0.3358\n",
      "Iter-4880 train loss: 2.1651 valid loss: 2.1330, valid accuracy: 0.3356\n",
      "Iter-4890 train loss: 2.0994 valid loss: 2.1327, valid accuracy: 0.3362\n",
      "Iter-4900 train loss: 2.1123 valid loss: 2.1325, valid accuracy: 0.3362\n",
      "Iter-4910 train loss: 2.0806 valid loss: 2.1322, valid accuracy: 0.3362\n",
      "Iter-4920 train loss: 2.1443 valid loss: 2.1320, valid accuracy: 0.3362\n",
      "Iter-4930 train loss: 2.1536 valid loss: 2.1317, valid accuracy: 0.3362\n",
      "Iter-4940 train loss: 2.1279 valid loss: 2.1315, valid accuracy: 0.3362\n",
      "Iter-4950 train loss: 2.1927 valid loss: 2.1312, valid accuracy: 0.3370\n",
      "Iter-4960 train loss: 2.1623 valid loss: 2.1310, valid accuracy: 0.3366\n",
      "Iter-4970 train loss: 2.1977 valid loss: 2.1307, valid accuracy: 0.3364\n",
      "Iter-4980 train loss: 2.1372 valid loss: 2.1305, valid accuracy: 0.3360\n",
      "Iter-4990 train loss: 2.1404 valid loss: 2.1302, valid accuracy: 0.3358\n",
      "Iter-5000 train loss: 2.1260 valid loss: 2.1300, valid accuracy: 0.3364\n",
      "Iter-5010 train loss: 2.1413 valid loss: 2.1297, valid accuracy: 0.3366\n",
      "Iter-5020 train loss: 2.1701 valid loss: 2.1295, valid accuracy: 0.3370\n",
      "Iter-5030 train loss: 2.1558 valid loss: 2.1292, valid accuracy: 0.3372\n",
      "Iter-5040 train loss: 2.2137 valid loss: 2.1290, valid accuracy: 0.3372\n",
      "Iter-5050 train loss: 2.1544 valid loss: 2.1287, valid accuracy: 0.3378\n",
      "Iter-5060 train loss: 2.1712 valid loss: 2.1285, valid accuracy: 0.3378\n",
      "Iter-5070 train loss: 2.1450 valid loss: 2.1283, valid accuracy: 0.3376\n",
      "Iter-5080 train loss: 2.1340 valid loss: 2.1280, valid accuracy: 0.3382\n",
      "Iter-5090 train loss: 2.1188 valid loss: 2.1278, valid accuracy: 0.3382\n",
      "Iter-5100 train loss: 2.1113 valid loss: 2.1275, valid accuracy: 0.3382\n",
      "Iter-5110 train loss: 2.1149 valid loss: 2.1273, valid accuracy: 0.3382\n",
      "Iter-5120 train loss: 2.0439 valid loss: 2.1270, valid accuracy: 0.3378\n",
      "Iter-5130 train loss: 2.0706 valid loss: 2.1267, valid accuracy: 0.3378\n",
      "Iter-5140 train loss: 2.1226 valid loss: 2.1265, valid accuracy: 0.3378\n",
      "Iter-5150 train loss: 2.1138 valid loss: 2.1262, valid accuracy: 0.3380\n",
      "Iter-5160 train loss: 2.1439 valid loss: 2.1260, valid accuracy: 0.3380\n",
      "Iter-5170 train loss: 2.0639 valid loss: 2.1257, valid accuracy: 0.3382\n",
      "Iter-5180 train loss: 2.1512 valid loss: 2.1255, valid accuracy: 0.3382\n",
      "Iter-5190 train loss: 2.1035 valid loss: 2.1252, valid accuracy: 0.3378\n",
      "Iter-5200 train loss: 2.1121 valid loss: 2.1250, valid accuracy: 0.3378\n",
      "Iter-5210 train loss: 2.1289 valid loss: 2.1247, valid accuracy: 0.3374\n",
      "Iter-5220 train loss: 2.0884 valid loss: 2.1245, valid accuracy: 0.3378\n",
      "Iter-5230 train loss: 2.1161 valid loss: 2.1243, valid accuracy: 0.3376\n",
      "Iter-5240 train loss: 2.0835 valid loss: 2.1240, valid accuracy: 0.3378\n",
      "Iter-5250 train loss: 2.0417 valid loss: 2.1238, valid accuracy: 0.3380\n",
      "Iter-5260 train loss: 2.0934 valid loss: 2.1235, valid accuracy: 0.3382\n",
      "Iter-5270 train loss: 2.1591 valid loss: 2.1233, valid accuracy: 0.3380\n",
      "Iter-5280 train loss: 2.1156 valid loss: 2.1230, valid accuracy: 0.3382\n",
      "Iter-5290 train loss: 2.0536 valid loss: 2.1228, valid accuracy: 0.3380\n",
      "Iter-5300 train loss: 2.1628 valid loss: 2.1225, valid accuracy: 0.3382\n",
      "Iter-5310 train loss: 2.1952 valid loss: 2.1223, valid accuracy: 0.3382\n",
      "Iter-5320 train loss: 2.1661 valid loss: 2.1221, valid accuracy: 0.3382\n",
      "Iter-5330 train loss: 2.0996 valid loss: 2.1218, valid accuracy: 0.3382\n",
      "Iter-5340 train loss: 2.1669 valid loss: 2.1216, valid accuracy: 0.3376\n",
      "Iter-5350 train loss: 2.0755 valid loss: 2.1214, valid accuracy: 0.3376\n",
      "Iter-5360 train loss: 2.0542 valid loss: 2.1211, valid accuracy: 0.3378\n",
      "Iter-5370 train loss: 2.1622 valid loss: 2.1208, valid accuracy: 0.3378\n",
      "Iter-5380 train loss: 2.1435 valid loss: 2.1206, valid accuracy: 0.3380\n",
      "Iter-5390 train loss: 2.1087 valid loss: 2.1203, valid accuracy: 0.3376\n",
      "Iter-5400 train loss: 2.1077 valid loss: 2.1201, valid accuracy: 0.3374\n",
      "Iter-5410 train loss: 2.1212 valid loss: 2.1199, valid accuracy: 0.3376\n",
      "Iter-5420 train loss: 2.1105 valid loss: 2.1196, valid accuracy: 0.3378\n",
      "Iter-5430 train loss: 2.1658 valid loss: 2.1194, valid accuracy: 0.3380\n",
      "Iter-5440 train loss: 2.1001 valid loss: 2.1191, valid accuracy: 0.3378\n",
      "Iter-5450 train loss: 2.1357 valid loss: 2.1189, valid accuracy: 0.3376\n",
      "Iter-5460 train loss: 2.0952 valid loss: 2.1186, valid accuracy: 0.3378\n",
      "Iter-5470 train loss: 2.0516 valid loss: 2.1184, valid accuracy: 0.3378\n",
      "Iter-5480 train loss: 2.1315 valid loss: 2.1182, valid accuracy: 0.3378\n",
      "Iter-5490 train loss: 2.1165 valid loss: 2.1179, valid accuracy: 0.3378\n",
      "Iter-5500 train loss: 2.0809 valid loss: 2.1177, valid accuracy: 0.3378\n",
      "Iter-5510 train loss: 2.1120 valid loss: 2.1175, valid accuracy: 0.3378\n",
      "Iter-5520 train loss: 2.0971 valid loss: 2.1172, valid accuracy: 0.3376\n",
      "Iter-5530 train loss: 2.1133 valid loss: 2.1169, valid accuracy: 0.3374\n",
      "Iter-5540 train loss: 2.1026 valid loss: 2.1167, valid accuracy: 0.3374\n",
      "Iter-5550 train loss: 2.1493 valid loss: 2.1164, valid accuracy: 0.3376\n",
      "Iter-5560 train loss: 2.1553 valid loss: 2.1162, valid accuracy: 0.3374\n",
      "Iter-5570 train loss: 2.1224 valid loss: 2.1160, valid accuracy: 0.3378\n",
      "Iter-5580 train loss: 2.1139 valid loss: 2.1158, valid accuracy: 0.3376\n",
      "Iter-5590 train loss: 2.1542 valid loss: 2.1155, valid accuracy: 0.3374\n",
      "Iter-5600 train loss: 2.0716 valid loss: 2.1153, valid accuracy: 0.3376\n",
      "Iter-5610 train loss: 2.0979 valid loss: 2.1151, valid accuracy: 0.3372\n",
      "Iter-5620 train loss: 2.1608 valid loss: 2.1148, valid accuracy: 0.3374\n",
      "Iter-5630 train loss: 2.1387 valid loss: 2.1146, valid accuracy: 0.3376\n",
      "Iter-5640 train loss: 2.0648 valid loss: 2.1144, valid accuracy: 0.3376\n",
      "Iter-5650 train loss: 2.1317 valid loss: 2.1142, valid accuracy: 0.3376\n",
      "Iter-5660 train loss: 2.1764 valid loss: 2.1139, valid accuracy: 0.3376\n",
      "Iter-5670 train loss: 2.1035 valid loss: 2.1137, valid accuracy: 0.3376\n",
      "Iter-5680 train loss: 2.0514 valid loss: 2.1134, valid accuracy: 0.3382\n",
      "Iter-5690 train loss: 2.1901 valid loss: 2.1132, valid accuracy: 0.3378\n",
      "Iter-5700 train loss: 2.1094 valid loss: 2.1129, valid accuracy: 0.3380\n",
      "Iter-5710 train loss: 2.0700 valid loss: 2.1127, valid accuracy: 0.3384\n",
      "Iter-5720 train loss: 2.0678 valid loss: 2.1125, valid accuracy: 0.3386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.1115 valid loss: 2.1122, valid accuracy: 0.3384\n",
      "Iter-5740 train loss: 2.0929 valid loss: 2.1120, valid accuracy: 0.3382\n",
      "Iter-5750 train loss: 2.1009 valid loss: 2.1117, valid accuracy: 0.3378\n",
      "Iter-5760 train loss: 2.2108 valid loss: 2.1115, valid accuracy: 0.3380\n",
      "Iter-5770 train loss: 2.1487 valid loss: 2.1112, valid accuracy: 0.3386\n",
      "Iter-5780 train loss: 2.0886 valid loss: 2.1110, valid accuracy: 0.3388\n",
      "Iter-5790 train loss: 2.0226 valid loss: 2.1107, valid accuracy: 0.3386\n",
      "Iter-5800 train loss: 2.0888 valid loss: 2.1105, valid accuracy: 0.3388\n",
      "Iter-5810 train loss: 2.1631 valid loss: 2.1103, valid accuracy: 0.3380\n",
      "Iter-5820 train loss: 2.1079 valid loss: 2.1101, valid accuracy: 0.3378\n",
      "Iter-5830 train loss: 2.0696 valid loss: 2.1098, valid accuracy: 0.3384\n",
      "Iter-5840 train loss: 2.1423 valid loss: 2.1096, valid accuracy: 0.3380\n",
      "Iter-5850 train loss: 2.0712 valid loss: 2.1093, valid accuracy: 0.3382\n",
      "Iter-5860 train loss: 2.0752 valid loss: 2.1091, valid accuracy: 0.3382\n",
      "Iter-5870 train loss: 2.1511 valid loss: 2.1089, valid accuracy: 0.3380\n",
      "Iter-5880 train loss: 2.1555 valid loss: 2.1086, valid accuracy: 0.3382\n",
      "Iter-5890 train loss: 2.1149 valid loss: 2.1084, valid accuracy: 0.3382\n",
      "Iter-5900 train loss: 2.0940 valid loss: 2.1082, valid accuracy: 0.3380\n",
      "Iter-5910 train loss: 2.1436 valid loss: 2.1080, valid accuracy: 0.3384\n",
      "Iter-5920 train loss: 2.1355 valid loss: 2.1077, valid accuracy: 0.3384\n",
      "Iter-5930 train loss: 2.1631 valid loss: 2.1075, valid accuracy: 0.3392\n",
      "Iter-5940 train loss: 2.1256 valid loss: 2.1072, valid accuracy: 0.3386\n",
      "Iter-5950 train loss: 2.1206 valid loss: 2.1070, valid accuracy: 0.3390\n",
      "Iter-5960 train loss: 2.1076 valid loss: 2.1068, valid accuracy: 0.3388\n",
      "Iter-5970 train loss: 2.2013 valid loss: 2.1065, valid accuracy: 0.3384\n",
      "Iter-5980 train loss: 2.0677 valid loss: 2.1063, valid accuracy: 0.3384\n",
      "Iter-5990 train loss: 2.0619 valid loss: 2.1060, valid accuracy: 0.3382\n",
      "Iter-6000 train loss: 2.1173 valid loss: 2.1058, valid accuracy: 0.3384\n",
      "Iter-6010 train loss: 2.1548 valid loss: 2.1056, valid accuracy: 0.3388\n",
      "Iter-6020 train loss: 2.0630 valid loss: 2.1054, valid accuracy: 0.3384\n",
      "Iter-6030 train loss: 2.0980 valid loss: 2.1051, valid accuracy: 0.3386\n",
      "Iter-6040 train loss: 2.1598 valid loss: 2.1049, valid accuracy: 0.3394\n",
      "Iter-6050 train loss: 2.0604 valid loss: 2.1047, valid accuracy: 0.3390\n",
      "Iter-6060 train loss: 2.0830 valid loss: 2.1044, valid accuracy: 0.3392\n",
      "Iter-6070 train loss: 2.1434 valid loss: 2.1042, valid accuracy: 0.3390\n",
      "Iter-6080 train loss: 2.1084 valid loss: 2.1040, valid accuracy: 0.3390\n",
      "Iter-6090 train loss: 2.1213 valid loss: 2.1038, valid accuracy: 0.3388\n",
      "Iter-6100 train loss: 2.1260 valid loss: 2.1035, valid accuracy: 0.3384\n",
      "Iter-6110 train loss: 2.0875 valid loss: 2.1033, valid accuracy: 0.3386\n",
      "Iter-6120 train loss: 2.0872 valid loss: 2.1031, valid accuracy: 0.3376\n",
      "Iter-6130 train loss: 2.1102 valid loss: 2.1029, valid accuracy: 0.3378\n",
      "Iter-6140 train loss: 2.0491 valid loss: 2.1027, valid accuracy: 0.3386\n",
      "Iter-6150 train loss: 2.1020 valid loss: 2.1025, valid accuracy: 0.3386\n",
      "Iter-6160 train loss: 2.0392 valid loss: 2.1022, valid accuracy: 0.3390\n",
      "Iter-6170 train loss: 2.0989 valid loss: 2.1020, valid accuracy: 0.3382\n",
      "Iter-6180 train loss: 2.1096 valid loss: 2.1017, valid accuracy: 0.3388\n",
      "Iter-6190 train loss: 2.1039 valid loss: 2.1015, valid accuracy: 0.3386\n",
      "Iter-6200 train loss: 2.0862 valid loss: 2.1012, valid accuracy: 0.3386\n",
      "Iter-6210 train loss: 2.1041 valid loss: 2.1010, valid accuracy: 0.3388\n",
      "Iter-6220 train loss: 2.1157 valid loss: 2.1008, valid accuracy: 0.3392\n",
      "Iter-6230 train loss: 2.1385 valid loss: 2.1006, valid accuracy: 0.3396\n",
      "Iter-6240 train loss: 2.0822 valid loss: 2.1004, valid accuracy: 0.3398\n",
      "Iter-6250 train loss: 2.0702 valid loss: 2.1002, valid accuracy: 0.3404\n",
      "Iter-6260 train loss: 2.1288 valid loss: 2.0999, valid accuracy: 0.3402\n",
      "Iter-6270 train loss: 2.0919 valid loss: 2.0997, valid accuracy: 0.3400\n",
      "Iter-6280 train loss: 2.1508 valid loss: 2.0995, valid accuracy: 0.3398\n",
      "Iter-6290 train loss: 2.1060 valid loss: 2.0992, valid accuracy: 0.3396\n",
      "Iter-6300 train loss: 2.0886 valid loss: 2.0990, valid accuracy: 0.3398\n",
      "Iter-6310 train loss: 1.9985 valid loss: 2.0988, valid accuracy: 0.3402\n",
      "Iter-6320 train loss: 2.1167 valid loss: 2.0985, valid accuracy: 0.3402\n",
      "Iter-6330 train loss: 2.1611 valid loss: 2.0983, valid accuracy: 0.3400\n",
      "Iter-6340 train loss: 2.1364 valid loss: 2.0981, valid accuracy: 0.3402\n",
      "Iter-6350 train loss: 2.1502 valid loss: 2.0979, valid accuracy: 0.3400\n",
      "Iter-6360 train loss: 2.0967 valid loss: 2.0977, valid accuracy: 0.3394\n",
      "Iter-6370 train loss: 2.0813 valid loss: 2.0974, valid accuracy: 0.3398\n",
      "Iter-6380 train loss: 2.0519 valid loss: 2.0972, valid accuracy: 0.3398\n",
      "Iter-6390 train loss: 2.0552 valid loss: 2.0970, valid accuracy: 0.3396\n",
      "Iter-6400 train loss: 2.0557 valid loss: 2.0968, valid accuracy: 0.3394\n",
      "Iter-6410 train loss: 2.1371 valid loss: 2.0966, valid accuracy: 0.3396\n",
      "Iter-6420 train loss: 2.1418 valid loss: 2.0964, valid accuracy: 0.3396\n",
      "Iter-6430 train loss: 2.1468 valid loss: 2.0962, valid accuracy: 0.3396\n",
      "Iter-6440 train loss: 2.1060 valid loss: 2.0960, valid accuracy: 0.3394\n",
      "Iter-6450 train loss: 2.0775 valid loss: 2.0957, valid accuracy: 0.3390\n",
      "Iter-6460 train loss: 2.0682 valid loss: 2.0955, valid accuracy: 0.3394\n",
      "Iter-6470 train loss: 2.0965 valid loss: 2.0953, valid accuracy: 0.3394\n",
      "Iter-6480 train loss: 2.0916 valid loss: 2.0951, valid accuracy: 0.3394\n",
      "Iter-6490 train loss: 2.1247 valid loss: 2.0949, valid accuracy: 0.3398\n",
      "Iter-6500 train loss: 2.0699 valid loss: 2.0947, valid accuracy: 0.3390\n",
      "Iter-6510 train loss: 2.0783 valid loss: 2.0944, valid accuracy: 0.3388\n",
      "Iter-6520 train loss: 2.1354 valid loss: 2.0942, valid accuracy: 0.3384\n",
      "Iter-6530 train loss: 2.1296 valid loss: 2.0940, valid accuracy: 0.3390\n",
      "Iter-6540 train loss: 2.1198 valid loss: 2.0938, valid accuracy: 0.3390\n",
      "Iter-6550 train loss: 2.0364 valid loss: 2.0936, valid accuracy: 0.3392\n",
      "Iter-6560 train loss: 2.0713 valid loss: 2.0933, valid accuracy: 0.3392\n",
      "Iter-6570 train loss: 2.0860 valid loss: 2.0931, valid accuracy: 0.3390\n",
      "Iter-6580 train loss: 2.0765 valid loss: 2.0929, valid accuracy: 0.3390\n",
      "Iter-6590 train loss: 2.1229 valid loss: 2.0927, valid accuracy: 0.3390\n",
      "Iter-6600 train loss: 2.1469 valid loss: 2.0925, valid accuracy: 0.3390\n",
      "Iter-6610 train loss: 2.1042 valid loss: 2.0923, valid accuracy: 0.3390\n",
      "Iter-6620 train loss: 2.0800 valid loss: 2.0921, valid accuracy: 0.3390\n",
      "Iter-6630 train loss: 2.0926 valid loss: 2.0918, valid accuracy: 0.3390\n",
      "Iter-6640 train loss: 2.1009 valid loss: 2.0916, valid accuracy: 0.3394\n",
      "Iter-6650 train loss: 2.0893 valid loss: 2.0914, valid accuracy: 0.3392\n",
      "Iter-6660 train loss: 2.0639 valid loss: 2.0912, valid accuracy: 0.3392\n",
      "Iter-6670 train loss: 2.1211 valid loss: 2.0910, valid accuracy: 0.3390\n",
      "Iter-6680 train loss: 2.0938 valid loss: 2.0908, valid accuracy: 0.3390\n",
      "Iter-6690 train loss: 2.0996 valid loss: 2.0906, valid accuracy: 0.3388\n",
      "Iter-6700 train loss: 2.1588 valid loss: 2.0904, valid accuracy: 0.3388\n",
      "Iter-6710 train loss: 2.1275 valid loss: 2.0902, valid accuracy: 0.3388\n",
      "Iter-6720 train loss: 1.9985 valid loss: 2.0900, valid accuracy: 0.3390\n",
      "Iter-6730 train loss: 2.0976 valid loss: 2.0898, valid accuracy: 0.3394\n",
      "Iter-6740 train loss: 2.0582 valid loss: 2.0896, valid accuracy: 0.3390\n",
      "Iter-6750 train loss: 2.1059 valid loss: 2.0894, valid accuracy: 0.3392\n",
      "Iter-6760 train loss: 2.0478 valid loss: 2.0891, valid accuracy: 0.3388\n",
      "Iter-6770 train loss: 2.0066 valid loss: 2.0889, valid accuracy: 0.3388\n",
      "Iter-6780 train loss: 2.1316 valid loss: 2.0887, valid accuracy: 0.3390\n",
      "Iter-6790 train loss: 2.0525 valid loss: 2.0885, valid accuracy: 0.3388\n",
      "Iter-6800 train loss: 2.1025 valid loss: 2.0882, valid accuracy: 0.3394\n",
      "Iter-6810 train loss: 2.1045 valid loss: 2.0880, valid accuracy: 0.3394\n",
      "Iter-6820 train loss: 2.1899 valid loss: 2.0879, valid accuracy: 0.3396\n",
      "Iter-6830 train loss: 2.1578 valid loss: 2.0876, valid accuracy: 0.3396\n",
      "Iter-6840 train loss: 2.1139 valid loss: 2.0874, valid accuracy: 0.3400\n",
      "Iter-6850 train loss: 2.0400 valid loss: 2.0872, valid accuracy: 0.3398\n",
      "Iter-6860 train loss: 2.1450 valid loss: 2.0870, valid accuracy: 0.3398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.0919 valid loss: 2.0868, valid accuracy: 0.3400\n",
      "Iter-6880 train loss: 2.0713 valid loss: 2.0866, valid accuracy: 0.3400\n",
      "Iter-6890 train loss: 2.1311 valid loss: 2.0864, valid accuracy: 0.3402\n",
      "Iter-6900 train loss: 2.0486 valid loss: 2.0862, valid accuracy: 0.3400\n",
      "Iter-6910 train loss: 2.0796 valid loss: 2.0859, valid accuracy: 0.3396\n",
      "Iter-6920 train loss: 2.0866 valid loss: 2.0857, valid accuracy: 0.3398\n",
      "Iter-6930 train loss: 2.0881 valid loss: 2.0855, valid accuracy: 0.3398\n",
      "Iter-6940 train loss: 2.1583 valid loss: 2.0853, valid accuracy: 0.3400\n",
      "Iter-6950 train loss: 2.1652 valid loss: 2.0851, valid accuracy: 0.3402\n",
      "Iter-6960 train loss: 2.0401 valid loss: 2.0849, valid accuracy: 0.3400\n",
      "Iter-6970 train loss: 2.0065 valid loss: 2.0847, valid accuracy: 0.3400\n",
      "Iter-6980 train loss: 1.9998 valid loss: 2.0845, valid accuracy: 0.3402\n",
      "Iter-6990 train loss: 2.0881 valid loss: 2.0843, valid accuracy: 0.3398\n",
      "Iter-7000 train loss: 2.1073 valid loss: 2.0841, valid accuracy: 0.3402\n",
      "Iter-7010 train loss: 2.1569 valid loss: 2.0839, valid accuracy: 0.3398\n",
      "Iter-7020 train loss: 2.0381 valid loss: 2.0837, valid accuracy: 0.3400\n",
      "Iter-7030 train loss: 2.0745 valid loss: 2.0835, valid accuracy: 0.3400\n",
      "Iter-7040 train loss: 2.1038 valid loss: 2.0833, valid accuracy: 0.3404\n",
      "Iter-7050 train loss: 2.0944 valid loss: 2.0831, valid accuracy: 0.3406\n",
      "Iter-7060 train loss: 2.0523 valid loss: 2.0829, valid accuracy: 0.3404\n",
      "Iter-7070 train loss: 2.1238 valid loss: 2.0827, valid accuracy: 0.3402\n",
      "Iter-7080 train loss: 2.0200 valid loss: 2.0824, valid accuracy: 0.3404\n",
      "Iter-7090 train loss: 2.0922 valid loss: 2.0822, valid accuracy: 0.3402\n",
      "Iter-7100 train loss: 2.0905 valid loss: 2.0820, valid accuracy: 0.3400\n",
      "Iter-7110 train loss: 2.1802 valid loss: 2.0818, valid accuracy: 0.3402\n",
      "Iter-7120 train loss: 2.0899 valid loss: 2.0816, valid accuracy: 0.3398\n",
      "Iter-7130 train loss: 2.1248 valid loss: 2.0814, valid accuracy: 0.3402\n",
      "Iter-7140 train loss: 2.0389 valid loss: 2.0812, valid accuracy: 0.3396\n",
      "Iter-7150 train loss: 2.0638 valid loss: 2.0810, valid accuracy: 0.3400\n",
      "Iter-7160 train loss: 2.1034 valid loss: 2.0808, valid accuracy: 0.3398\n",
      "Iter-7170 train loss: 2.0257 valid loss: 2.0806, valid accuracy: 0.3400\n",
      "Iter-7180 train loss: 2.0857 valid loss: 2.0804, valid accuracy: 0.3396\n",
      "Iter-7190 train loss: 2.2075 valid loss: 2.0802, valid accuracy: 0.3400\n",
      "Iter-7200 train loss: 2.0235 valid loss: 2.0800, valid accuracy: 0.3398\n",
      "Iter-7210 train loss: 2.0206 valid loss: 2.0798, valid accuracy: 0.3392\n",
      "Iter-7220 train loss: 2.0402 valid loss: 2.0796, valid accuracy: 0.3390\n",
      "Iter-7230 train loss: 2.0298 valid loss: 2.0794, valid accuracy: 0.3396\n",
      "Iter-7240 train loss: 2.0205 valid loss: 2.0792, valid accuracy: 0.3396\n",
      "Iter-7250 train loss: 2.1266 valid loss: 2.0790, valid accuracy: 0.3394\n",
      "Iter-7260 train loss: 2.0596 valid loss: 2.0788, valid accuracy: 0.3394\n",
      "Iter-7270 train loss: 2.1187 valid loss: 2.0786, valid accuracy: 0.3400\n",
      "Iter-7280 train loss: 2.0597 valid loss: 2.0783, valid accuracy: 0.3400\n",
      "Iter-7290 train loss: 1.9981 valid loss: 2.0781, valid accuracy: 0.3396\n",
      "Iter-7300 train loss: 2.0640 valid loss: 2.0779, valid accuracy: 0.3392\n",
      "Iter-7310 train loss: 2.1566 valid loss: 2.0777, valid accuracy: 0.3390\n",
      "Iter-7320 train loss: 2.0334 valid loss: 2.0775, valid accuracy: 0.3390\n",
      "Iter-7330 train loss: 2.0458 valid loss: 2.0773, valid accuracy: 0.3386\n",
      "Iter-7340 train loss: 2.1364 valid loss: 2.0771, valid accuracy: 0.3382\n",
      "Iter-7350 train loss: 2.1056 valid loss: 2.0769, valid accuracy: 0.3384\n",
      "Iter-7360 train loss: 2.0503 valid loss: 2.0767, valid accuracy: 0.3384\n",
      "Iter-7370 train loss: 2.0634 valid loss: 2.0765, valid accuracy: 0.3384\n",
      "Iter-7380 train loss: 2.0590 valid loss: 2.0763, valid accuracy: 0.3382\n",
      "Iter-7390 train loss: 2.1329 valid loss: 2.0761, valid accuracy: 0.3380\n",
      "Iter-7400 train loss: 2.1145 valid loss: 2.0759, valid accuracy: 0.3384\n",
      "Iter-7410 train loss: 2.0486 valid loss: 2.0757, valid accuracy: 0.3378\n",
      "Iter-7420 train loss: 2.0373 valid loss: 2.0755, valid accuracy: 0.3380\n",
      "Iter-7430 train loss: 2.0286 valid loss: 2.0753, valid accuracy: 0.3374\n",
      "Iter-7440 train loss: 2.0851 valid loss: 2.0751, valid accuracy: 0.3376\n",
      "Iter-7450 train loss: 2.0359 valid loss: 2.0749, valid accuracy: 0.3384\n",
      "Iter-7460 train loss: 2.1990 valid loss: 2.0747, valid accuracy: 0.3380\n",
      "Iter-7470 train loss: 2.1336 valid loss: 2.0745, valid accuracy: 0.3378\n",
      "Iter-7480 train loss: 2.1093 valid loss: 2.0743, valid accuracy: 0.3382\n",
      "Iter-7490 train loss: 2.0516 valid loss: 2.0741, valid accuracy: 0.3384\n",
      "Iter-7500 train loss: 2.0737 valid loss: 2.0739, valid accuracy: 0.3384\n",
      "Iter-7510 train loss: 2.1613 valid loss: 2.0737, valid accuracy: 0.3384\n",
      "Iter-7520 train loss: 2.0407 valid loss: 2.0736, valid accuracy: 0.3388\n",
      "Iter-7530 train loss: 2.0660 valid loss: 2.0734, valid accuracy: 0.3382\n",
      "Iter-7540 train loss: 2.0638 valid loss: 2.0732, valid accuracy: 0.3384\n",
      "Iter-7550 train loss: 2.1065 valid loss: 2.0730, valid accuracy: 0.3386\n",
      "Iter-7560 train loss: 2.1326 valid loss: 2.0728, valid accuracy: 0.3388\n",
      "Iter-7570 train loss: 2.0870 valid loss: 2.0726, valid accuracy: 0.3386\n",
      "Iter-7580 train loss: 2.0751 valid loss: 2.0724, valid accuracy: 0.3386\n",
      "Iter-7590 train loss: 2.0411 valid loss: 2.0722, valid accuracy: 0.3386\n",
      "Iter-7600 train loss: 2.1558 valid loss: 2.0720, valid accuracy: 0.3390\n",
      "Iter-7610 train loss: 2.0854 valid loss: 2.0718, valid accuracy: 0.3390\n",
      "Iter-7620 train loss: 2.1057 valid loss: 2.0716, valid accuracy: 0.3388\n",
      "Iter-7630 train loss: 2.1201 valid loss: 2.0714, valid accuracy: 0.3388\n",
      "Iter-7640 train loss: 2.1171 valid loss: 2.0712, valid accuracy: 0.3388\n",
      "Iter-7650 train loss: 2.0334 valid loss: 2.0710, valid accuracy: 0.3388\n",
      "Iter-7660 train loss: 2.0824 valid loss: 2.0708, valid accuracy: 0.3386\n",
      "Iter-7670 train loss: 2.1137 valid loss: 2.0706, valid accuracy: 0.3386\n",
      "Iter-7680 train loss: 2.0859 valid loss: 2.0704, valid accuracy: 0.3378\n",
      "Iter-7690 train loss: 2.0799 valid loss: 2.0702, valid accuracy: 0.3378\n",
      "Iter-7700 train loss: 2.1719 valid loss: 2.0700, valid accuracy: 0.3378\n",
      "Iter-7710 train loss: 2.0647 valid loss: 2.0698, valid accuracy: 0.3378\n",
      "Iter-7720 train loss: 2.0135 valid loss: 2.0696, valid accuracy: 0.3380\n",
      "Iter-7730 train loss: 2.0575 valid loss: 2.0694, valid accuracy: 0.3378\n",
      "Iter-7740 train loss: 2.0877 valid loss: 2.0693, valid accuracy: 0.3378\n",
      "Iter-7750 train loss: 2.0758 valid loss: 2.0691, valid accuracy: 0.3378\n",
      "Iter-7760 train loss: 2.0822 valid loss: 2.0689, valid accuracy: 0.3376\n",
      "Iter-7770 train loss: 2.0960 valid loss: 2.0687, valid accuracy: 0.3376\n",
      "Iter-7780 train loss: 2.0181 valid loss: 2.0685, valid accuracy: 0.3376\n",
      "Iter-7790 train loss: 2.0740 valid loss: 2.0683, valid accuracy: 0.3380\n",
      "Iter-7800 train loss: 2.0549 valid loss: 2.0681, valid accuracy: 0.3378\n",
      "Iter-7810 train loss: 2.0687 valid loss: 2.0679, valid accuracy: 0.3380\n",
      "Iter-7820 train loss: 2.0079 valid loss: 2.0677, valid accuracy: 0.3376\n",
      "Iter-7830 train loss: 2.0228 valid loss: 2.0675, valid accuracy: 0.3370\n",
      "Iter-7840 train loss: 2.0347 valid loss: 2.0673, valid accuracy: 0.3370\n",
      "Iter-7850 train loss: 2.0296 valid loss: 2.0671, valid accuracy: 0.3368\n",
      "Iter-7860 train loss: 2.0063 valid loss: 2.0668, valid accuracy: 0.3368\n",
      "Iter-7870 train loss: 2.0712 valid loss: 2.0667, valid accuracy: 0.3372\n",
      "Iter-7880 train loss: 2.0410 valid loss: 2.0665, valid accuracy: 0.3376\n",
      "Iter-7890 train loss: 2.0871 valid loss: 2.0663, valid accuracy: 0.3376\n",
      "Iter-7900 train loss: 2.1040 valid loss: 2.0661, valid accuracy: 0.3374\n",
      "Iter-7910 train loss: 2.0388 valid loss: 2.0659, valid accuracy: 0.3374\n",
      "Iter-7920 train loss: 2.1032 valid loss: 2.0657, valid accuracy: 0.3370\n",
      "Iter-7930 train loss: 2.0468 valid loss: 2.0655, valid accuracy: 0.3378\n",
      "Iter-7940 train loss: 2.0305 valid loss: 2.0653, valid accuracy: 0.3372\n",
      "Iter-7950 train loss: 2.1373 valid loss: 2.0651, valid accuracy: 0.3370\n",
      "Iter-7960 train loss: 2.1031 valid loss: 2.0649, valid accuracy: 0.3372\n",
      "Iter-7970 train loss: 2.0894 valid loss: 2.0647, valid accuracy: 0.3370\n",
      "Iter-7980 train loss: 2.0733 valid loss: 2.0645, valid accuracy: 0.3376\n",
      "Iter-7990 train loss: 2.0160 valid loss: 2.0643, valid accuracy: 0.3378\n",
      "Iter-8000 train loss: 2.0592 valid loss: 2.0641, valid accuracy: 0.3374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.0918 valid loss: 2.0639, valid accuracy: 0.3372\n",
      "Iter-8020 train loss: 2.0419 valid loss: 2.0637, valid accuracy: 0.3380\n",
      "Iter-8030 train loss: 2.0639 valid loss: 2.0635, valid accuracy: 0.3382\n",
      "Iter-8040 train loss: 2.0924 valid loss: 2.0633, valid accuracy: 0.3378\n",
      "Iter-8050 train loss: 2.0803 valid loss: 2.0631, valid accuracy: 0.3382\n",
      "Iter-8060 train loss: 2.1026 valid loss: 2.0629, valid accuracy: 0.3378\n",
      "Iter-8070 train loss: 2.1143 valid loss: 2.0627, valid accuracy: 0.3388\n",
      "Iter-8080 train loss: 2.0526 valid loss: 2.0626, valid accuracy: 0.3388\n",
      "Iter-8090 train loss: 2.0631 valid loss: 2.0624, valid accuracy: 0.3386\n",
      "Iter-8100 train loss: 2.1137 valid loss: 2.0622, valid accuracy: 0.3386\n",
      "Iter-8110 train loss: 2.1407 valid loss: 2.0620, valid accuracy: 0.3380\n",
      "Iter-8120 train loss: 2.0505 valid loss: 2.0618, valid accuracy: 0.3380\n",
      "Iter-8130 train loss: 2.0100 valid loss: 2.0616, valid accuracy: 0.3384\n",
      "Iter-8140 train loss: 1.9960 valid loss: 2.0614, valid accuracy: 0.3384\n",
      "Iter-8150 train loss: 2.0856 valid loss: 2.0612, valid accuracy: 0.3388\n",
      "Iter-8160 train loss: 2.0461 valid loss: 2.0610, valid accuracy: 0.3388\n",
      "Iter-8170 train loss: 2.0666 valid loss: 2.0608, valid accuracy: 0.3388\n",
      "Iter-8180 train loss: 2.1232 valid loss: 2.0606, valid accuracy: 0.3390\n",
      "Iter-8190 train loss: 2.0988 valid loss: 2.0604, valid accuracy: 0.3386\n",
      "Iter-8200 train loss: 2.0594 valid loss: 2.0603, valid accuracy: 0.3390\n",
      "Iter-8210 train loss: 2.0570 valid loss: 2.0601, valid accuracy: 0.3390\n",
      "Iter-8220 train loss: 2.0287 valid loss: 2.0599, valid accuracy: 0.3390\n",
      "Iter-8230 train loss: 2.0851 valid loss: 2.0597, valid accuracy: 0.3398\n",
      "Iter-8240 train loss: 2.1113 valid loss: 2.0595, valid accuracy: 0.3396\n",
      "Iter-8250 train loss: 2.0991 valid loss: 2.0593, valid accuracy: 0.3396\n",
      "Iter-8260 train loss: 2.1336 valid loss: 2.0591, valid accuracy: 0.3396\n",
      "Iter-8270 train loss: 1.9959 valid loss: 2.0589, valid accuracy: 0.3398\n",
      "Iter-8280 train loss: 2.0779 valid loss: 2.0588, valid accuracy: 0.3404\n",
      "Iter-8290 train loss: 2.0504 valid loss: 2.0586, valid accuracy: 0.3402\n",
      "Iter-8300 train loss: 2.0295 valid loss: 2.0584, valid accuracy: 0.3400\n",
      "Iter-8310 train loss: 2.0505 valid loss: 2.0582, valid accuracy: 0.3404\n",
      "Iter-8320 train loss: 2.0783 valid loss: 2.0580, valid accuracy: 0.3404\n",
      "Iter-8330 train loss: 2.0962 valid loss: 2.0578, valid accuracy: 0.3402\n",
      "Iter-8340 train loss: 2.0415 valid loss: 2.0576, valid accuracy: 0.3404\n",
      "Iter-8350 train loss: 2.1288 valid loss: 2.0575, valid accuracy: 0.3402\n",
      "Iter-8360 train loss: 2.0606 valid loss: 2.0573, valid accuracy: 0.3402\n",
      "Iter-8370 train loss: 2.1233 valid loss: 2.0571, valid accuracy: 0.3400\n",
      "Iter-8380 train loss: 2.1167 valid loss: 2.0569, valid accuracy: 0.3404\n",
      "Iter-8390 train loss: 2.0017 valid loss: 2.0567, valid accuracy: 0.3406\n",
      "Iter-8400 train loss: 2.0700 valid loss: 2.0565, valid accuracy: 0.3400\n",
      "Iter-8410 train loss: 2.0326 valid loss: 2.0564, valid accuracy: 0.3398\n",
      "Iter-8420 train loss: 2.0556 valid loss: 2.0562, valid accuracy: 0.3400\n",
      "Iter-8430 train loss: 2.0481 valid loss: 2.0560, valid accuracy: 0.3400\n",
      "Iter-8440 train loss: 1.9728 valid loss: 2.0558, valid accuracy: 0.3400\n",
      "Iter-8450 train loss: 2.1248 valid loss: 2.0556, valid accuracy: 0.3400\n",
      "Iter-8460 train loss: 2.0440 valid loss: 2.0554, valid accuracy: 0.3394\n",
      "Iter-8470 train loss: 2.0769 valid loss: 2.0552, valid accuracy: 0.3400\n",
      "Iter-8480 train loss: 2.0508 valid loss: 2.0551, valid accuracy: 0.3400\n",
      "Iter-8490 train loss: 2.0954 valid loss: 2.0549, valid accuracy: 0.3402\n",
      "Iter-8500 train loss: 2.1288 valid loss: 2.0547, valid accuracy: 0.3404\n",
      "Iter-8510 train loss: 2.0662 valid loss: 2.0545, valid accuracy: 0.3398\n",
      "Iter-8520 train loss: 2.1143 valid loss: 2.0543, valid accuracy: 0.3396\n",
      "Iter-8530 train loss: 1.9667 valid loss: 2.0541, valid accuracy: 0.3400\n",
      "Iter-8540 train loss: 2.0474 valid loss: 2.0540, valid accuracy: 0.3400\n",
      "Iter-8550 train loss: 2.0860 valid loss: 2.0538, valid accuracy: 0.3404\n",
      "Iter-8560 train loss: 2.1386 valid loss: 2.0536, valid accuracy: 0.3404\n",
      "Iter-8570 train loss: 1.9698 valid loss: 2.0534, valid accuracy: 0.3408\n",
      "Iter-8580 train loss: 2.0591 valid loss: 2.0533, valid accuracy: 0.3408\n",
      "Iter-8590 train loss: 1.9563 valid loss: 2.0531, valid accuracy: 0.3406\n",
      "Iter-8600 train loss: 2.0915 valid loss: 2.0529, valid accuracy: 0.3402\n",
      "Iter-8610 train loss: 2.0428 valid loss: 2.0527, valid accuracy: 0.3406\n",
      "Iter-8620 train loss: 2.1632 valid loss: 2.0525, valid accuracy: 0.3404\n",
      "Iter-8630 train loss: 2.0226 valid loss: 2.0523, valid accuracy: 0.3402\n",
      "Iter-8640 train loss: 2.0720 valid loss: 2.0522, valid accuracy: 0.3402\n",
      "Iter-8650 train loss: 1.9318 valid loss: 2.0520, valid accuracy: 0.3400\n",
      "Iter-8660 train loss: 2.0019 valid loss: 2.0518, valid accuracy: 0.3400\n",
      "Iter-8670 train loss: 2.0669 valid loss: 2.0516, valid accuracy: 0.3402\n",
      "Iter-8680 train loss: 2.0837 valid loss: 2.0514, valid accuracy: 0.3400\n",
      "Iter-8690 train loss: 2.0436 valid loss: 2.0512, valid accuracy: 0.3398\n",
      "Iter-8700 train loss: 2.0671 valid loss: 2.0511, valid accuracy: 0.3402\n",
      "Iter-8710 train loss: 2.0070 valid loss: 2.0509, valid accuracy: 0.3404\n",
      "Iter-8720 train loss: 2.0392 valid loss: 2.0507, valid accuracy: 0.3406\n",
      "Iter-8730 train loss: 2.0276 valid loss: 2.0505, valid accuracy: 0.3406\n",
      "Iter-8740 train loss: 2.0685 valid loss: 2.0504, valid accuracy: 0.3406\n",
      "Iter-8750 train loss: 2.0459 valid loss: 2.0502, valid accuracy: 0.3406\n",
      "Iter-8760 train loss: 2.0505 valid loss: 2.0500, valid accuracy: 0.3408\n",
      "Iter-8770 train loss: 2.1165 valid loss: 2.0498, valid accuracy: 0.3414\n",
      "Iter-8780 train loss: 2.0222 valid loss: 2.0497, valid accuracy: 0.3408\n",
      "Iter-8790 train loss: 2.0868 valid loss: 2.0495, valid accuracy: 0.3410\n",
      "Iter-8800 train loss: 2.0377 valid loss: 2.0493, valid accuracy: 0.3414\n",
      "Iter-8810 train loss: 2.0781 valid loss: 2.0491, valid accuracy: 0.3412\n",
      "Iter-8820 train loss: 1.9834 valid loss: 2.0489, valid accuracy: 0.3412\n",
      "Iter-8830 train loss: 2.0969 valid loss: 2.0488, valid accuracy: 0.3414\n",
      "Iter-8840 train loss: 2.1234 valid loss: 2.0486, valid accuracy: 0.3418\n",
      "Iter-8850 train loss: 2.0634 valid loss: 2.0484, valid accuracy: 0.3414\n",
      "Iter-8860 train loss: 2.0311 valid loss: 2.0482, valid accuracy: 0.3410\n",
      "Iter-8870 train loss: 2.0632 valid loss: 2.0481, valid accuracy: 0.3416\n",
      "Iter-8880 train loss: 2.0218 valid loss: 2.0479, valid accuracy: 0.3410\n",
      "Iter-8890 train loss: 2.0983 valid loss: 2.0477, valid accuracy: 0.3412\n",
      "Iter-8900 train loss: 2.1622 valid loss: 2.0475, valid accuracy: 0.3410\n",
      "Iter-8910 train loss: 2.0644 valid loss: 2.0473, valid accuracy: 0.3410\n",
      "Iter-8920 train loss: 2.1096 valid loss: 2.0471, valid accuracy: 0.3412\n",
      "Iter-8930 train loss: 2.0357 valid loss: 2.0470, valid accuracy: 0.3410\n",
      "Iter-8940 train loss: 1.9942 valid loss: 2.0468, valid accuracy: 0.3410\n",
      "Iter-8950 train loss: 2.1345 valid loss: 2.0466, valid accuracy: 0.3406\n",
      "Iter-8960 train loss: 2.0722 valid loss: 2.0464, valid accuracy: 0.3406\n",
      "Iter-8970 train loss: 2.0163 valid loss: 2.0462, valid accuracy: 0.3406\n",
      "Iter-8980 train loss: 2.1017 valid loss: 2.0461, valid accuracy: 0.3406\n",
      "Iter-8990 train loss: 2.0502 valid loss: 2.0459, valid accuracy: 0.3404\n",
      "Iter-9000 train loss: 2.0136 valid loss: 2.0457, valid accuracy: 0.3414\n",
      "Iter-9010 train loss: 2.0524 valid loss: 2.0455, valid accuracy: 0.3414\n",
      "Iter-9020 train loss: 2.1280 valid loss: 2.0454, valid accuracy: 0.3418\n",
      "Iter-9030 train loss: 2.1289 valid loss: 2.0452, valid accuracy: 0.3414\n",
      "Iter-9040 train loss: 2.0097 valid loss: 2.0450, valid accuracy: 0.3420\n",
      "Iter-9050 train loss: 2.0037 valid loss: 2.0448, valid accuracy: 0.3416\n",
      "Iter-9060 train loss: 2.0496 valid loss: 2.0447, valid accuracy: 0.3416\n",
      "Iter-9070 train loss: 1.9670 valid loss: 2.0445, valid accuracy: 0.3416\n",
      "Iter-9080 train loss: 2.0849 valid loss: 2.0443, valid accuracy: 0.3418\n",
      "Iter-9090 train loss: 2.0940 valid loss: 2.0441, valid accuracy: 0.3422\n",
      "Iter-9100 train loss: 2.0552 valid loss: 2.0440, valid accuracy: 0.3418\n",
      "Iter-9110 train loss: 2.0551 valid loss: 2.0438, valid accuracy: 0.3422\n",
      "Iter-9120 train loss: 2.0161 valid loss: 2.0436, valid accuracy: 0.3418\n",
      "Iter-9130 train loss: 2.0725 valid loss: 2.0434, valid accuracy: 0.3428\n",
      "Iter-9140 train loss: 2.1228 valid loss: 2.0432, valid accuracy: 0.3420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.0252 valid loss: 2.0431, valid accuracy: 0.3422\n",
      "Iter-9160 train loss: 2.0285 valid loss: 2.0429, valid accuracy: 0.3430\n",
      "Iter-9170 train loss: 2.0112 valid loss: 2.0427, valid accuracy: 0.3434\n",
      "Iter-9180 train loss: 2.1129 valid loss: 2.0425, valid accuracy: 0.3434\n",
      "Iter-9190 train loss: 1.9939 valid loss: 2.0424, valid accuracy: 0.3434\n",
      "Iter-9200 train loss: 2.0435 valid loss: 2.0422, valid accuracy: 0.3434\n",
      "Iter-9210 train loss: 2.0563 valid loss: 2.0420, valid accuracy: 0.3428\n",
      "Iter-9220 train loss: 2.0278 valid loss: 2.0418, valid accuracy: 0.3434\n",
      "Iter-9230 train loss: 1.9883 valid loss: 2.0416, valid accuracy: 0.3434\n",
      "Iter-9240 train loss: 1.9766 valid loss: 2.0415, valid accuracy: 0.3430\n",
      "Iter-9250 train loss: 2.0158 valid loss: 2.0413, valid accuracy: 0.3434\n",
      "Iter-9260 train loss: 2.0732 valid loss: 2.0411, valid accuracy: 0.3434\n",
      "Iter-9270 train loss: 2.0778 valid loss: 2.0410, valid accuracy: 0.3436\n",
      "Iter-9280 train loss: 2.1277 valid loss: 2.0408, valid accuracy: 0.3440\n",
      "Iter-9290 train loss: 2.0251 valid loss: 2.0406, valid accuracy: 0.3438\n",
      "Iter-9300 train loss: 2.0428 valid loss: 2.0404, valid accuracy: 0.3438\n",
      "Iter-9310 train loss: 2.0632 valid loss: 2.0403, valid accuracy: 0.3436\n",
      "Iter-9320 train loss: 2.0048 valid loss: 2.0401, valid accuracy: 0.3436\n",
      "Iter-9330 train loss: 2.0413 valid loss: 2.0399, valid accuracy: 0.3436\n",
      "Iter-9340 train loss: 2.0397 valid loss: 2.0398, valid accuracy: 0.3440\n",
      "Iter-9350 train loss: 2.0289 valid loss: 2.0396, valid accuracy: 0.3438\n",
      "Iter-9360 train loss: 2.0528 valid loss: 2.0394, valid accuracy: 0.3436\n",
      "Iter-9370 train loss: 2.0633 valid loss: 2.0393, valid accuracy: 0.3434\n",
      "Iter-9380 train loss: 2.0821 valid loss: 2.0391, valid accuracy: 0.3438\n",
      "Iter-9390 train loss: 2.0610 valid loss: 2.0390, valid accuracy: 0.3440\n",
      "Iter-9400 train loss: 2.0553 valid loss: 2.0388, valid accuracy: 0.3440\n",
      "Iter-9410 train loss: 2.0307 valid loss: 2.0386, valid accuracy: 0.3444\n",
      "Iter-9420 train loss: 2.0118 valid loss: 2.0384, valid accuracy: 0.3446\n",
      "Iter-9430 train loss: 1.9978 valid loss: 2.0383, valid accuracy: 0.3442\n",
      "Iter-9440 train loss: 2.0327 valid loss: 2.0381, valid accuracy: 0.3444\n",
      "Iter-9450 train loss: 2.0695 valid loss: 2.0379, valid accuracy: 0.3444\n",
      "Iter-9460 train loss: 2.0458 valid loss: 2.0377, valid accuracy: 0.3442\n",
      "Iter-9470 train loss: 2.0836 valid loss: 2.0376, valid accuracy: 0.3440\n",
      "Iter-9480 train loss: 2.0298 valid loss: 2.0374, valid accuracy: 0.3436\n",
      "Iter-9490 train loss: 1.9794 valid loss: 2.0372, valid accuracy: 0.3438\n",
      "Iter-9500 train loss: 2.0284 valid loss: 2.0371, valid accuracy: 0.3440\n",
      "Iter-9510 train loss: 2.0732 valid loss: 2.0369, valid accuracy: 0.3434\n",
      "Iter-9520 train loss: 2.0838 valid loss: 2.0367, valid accuracy: 0.3440\n",
      "Iter-9530 train loss: 2.0570 valid loss: 2.0366, valid accuracy: 0.3438\n",
      "Iter-9540 train loss: 2.0512 valid loss: 2.0364, valid accuracy: 0.3444\n",
      "Iter-9550 train loss: 2.1258 valid loss: 2.0362, valid accuracy: 0.3444\n",
      "Iter-9560 train loss: 2.0435 valid loss: 2.0360, valid accuracy: 0.3440\n",
      "Iter-9570 train loss: 1.9686 valid loss: 2.0359, valid accuracy: 0.3440\n",
      "Iter-9580 train loss: 2.0619 valid loss: 2.0357, valid accuracy: 0.3438\n",
      "Iter-9590 train loss: 2.0267 valid loss: 2.0355, valid accuracy: 0.3432\n",
      "Iter-9600 train loss: 2.0000 valid loss: 2.0353, valid accuracy: 0.3436\n",
      "Iter-9610 train loss: 1.9868 valid loss: 2.0351, valid accuracy: 0.3434\n",
      "Iter-9620 train loss: 2.0710 valid loss: 2.0350, valid accuracy: 0.3432\n",
      "Iter-9630 train loss: 1.9870 valid loss: 2.0348, valid accuracy: 0.3434\n",
      "Iter-9640 train loss: 2.0242 valid loss: 2.0346, valid accuracy: 0.3436\n",
      "Iter-9650 train loss: 2.0437 valid loss: 2.0344, valid accuracy: 0.3434\n",
      "Iter-9660 train loss: 2.0202 valid loss: 2.0343, valid accuracy: 0.3434\n",
      "Iter-9670 train loss: 1.9222 valid loss: 2.0341, valid accuracy: 0.3430\n",
      "Iter-9680 train loss: 2.0236 valid loss: 2.0339, valid accuracy: 0.3428\n",
      "Iter-9690 train loss: 2.0288 valid loss: 2.0338, valid accuracy: 0.3428\n",
      "Iter-9700 train loss: 2.0377 valid loss: 2.0336, valid accuracy: 0.3430\n",
      "Iter-9710 train loss: 2.0462 valid loss: 2.0334, valid accuracy: 0.3428\n",
      "Iter-9720 train loss: 2.1241 valid loss: 2.0333, valid accuracy: 0.3432\n",
      "Iter-9730 train loss: 2.0556 valid loss: 2.0331, valid accuracy: 0.3430\n",
      "Iter-9740 train loss: 2.0187 valid loss: 2.0330, valid accuracy: 0.3430\n",
      "Iter-9750 train loss: 2.1271 valid loss: 2.0328, valid accuracy: 0.3430\n",
      "Iter-9760 train loss: 2.0952 valid loss: 2.0327, valid accuracy: 0.3428\n",
      "Iter-9770 train loss: 1.9668 valid loss: 2.0325, valid accuracy: 0.3430\n",
      "Iter-9780 train loss: 2.0524 valid loss: 2.0324, valid accuracy: 0.3428\n",
      "Iter-9790 train loss: 2.0929 valid loss: 2.0322, valid accuracy: 0.3430\n",
      "Iter-9800 train loss: 2.0454 valid loss: 2.0320, valid accuracy: 0.3428\n",
      "Iter-9810 train loss: 2.0402 valid loss: 2.0319, valid accuracy: 0.3424\n",
      "Iter-9820 train loss: 2.0321 valid loss: 2.0317, valid accuracy: 0.3424\n",
      "Iter-9830 train loss: 2.0189 valid loss: 2.0315, valid accuracy: 0.3424\n",
      "Iter-9840 train loss: 2.0229 valid loss: 2.0313, valid accuracy: 0.3426\n",
      "Iter-9850 train loss: 2.1088 valid loss: 2.0312, valid accuracy: 0.3426\n",
      "Iter-9860 train loss: 1.9565 valid loss: 2.0310, valid accuracy: 0.3426\n",
      "Iter-9870 train loss: 2.0374 valid loss: 2.0309, valid accuracy: 0.3424\n",
      "Iter-9880 train loss: 2.0222 valid loss: 2.0307, valid accuracy: 0.3424\n",
      "Iter-9890 train loss: 2.0711 valid loss: 2.0305, valid accuracy: 0.3424\n",
      "Iter-9900 train loss: 2.0226 valid loss: 2.0303, valid accuracy: 0.3424\n",
      "Iter-9910 train loss: 1.9721 valid loss: 2.0302, valid accuracy: 0.3424\n",
      "Iter-9920 train loss: 2.1666 valid loss: 2.0300, valid accuracy: 0.3422\n",
      "Iter-9930 train loss: 1.9981 valid loss: 2.0299, valid accuracy: 0.3418\n",
      "Iter-9940 train loss: 2.0653 valid loss: 2.0297, valid accuracy: 0.3420\n",
      "Iter-9950 train loss: 2.0316 valid loss: 2.0296, valid accuracy: 0.3420\n",
      "Iter-9960 train loss: 2.1445 valid loss: 2.0294, valid accuracy: 0.3418\n",
      "Iter-9970 train loss: 2.1249 valid loss: 2.0293, valid accuracy: 0.3418\n",
      "Iter-9980 train loss: 2.0699 valid loss: 2.0291, valid accuracy: 0.3416\n",
      "Iter-9990 train loss: 1.9728 valid loss: 2.0289, valid accuracy: 0.3420\n",
      "Iter-10000 train loss: 2.0514 valid loss: 2.0288, valid accuracy: 0.3420\n",
      "Last iteration - Test accuracy mean: 0.3570, std: 0.0000, loss: 2.0251\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCIKQxvx/y9gf5DoX0XwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11365cef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113678208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
