{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        y_prev = self.y_prev.copy() # for temporal differencing\n",
    "        self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        y =  y @ self.W_fixed[2].T # done\n",
    "        y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            y =  y @ self.W_fixed[1][layer].T # done\n",
    "            y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.2676, acc-0.2200, valid loss-2.2812, acc-0.1510, test loss-2.2825, acc-0.1593\n",
      "Iter-20, train loss-2.2766, acc-0.1200, valid loss-2.2811, acc-0.1512, test loss-2.2824, acc-0.1595\n",
      "Iter-30, train loss-2.2770, acc-0.2000, valid loss-2.2810, acc-0.1514, test loss-2.2823, acc-0.1599\n",
      "Iter-40, train loss-2.2769, acc-0.2400, valid loss-2.2809, acc-0.1514, test loss-2.2822, acc-0.1602\n",
      "Iter-50, train loss-2.2902, acc-0.2000, valid loss-2.2808, acc-0.1516, test loss-2.2821, acc-0.1602\n",
      "Iter-60, train loss-2.2712, acc-0.1600, valid loss-2.2807, acc-0.1516, test loss-2.2820, acc-0.1603\n",
      "Iter-70, train loss-2.2731, acc-0.2600, valid loss-2.2806, acc-0.1516, test loss-2.2818, acc-0.1604\n",
      "Iter-80, train loss-2.3105, acc-0.1200, valid loss-2.2805, acc-0.1522, test loss-2.2817, acc-0.1607\n",
      "Iter-90, train loss-2.2502, acc-0.3000, valid loss-2.2804, acc-0.1526, test loss-2.2816, acc-0.1609\n",
      "Iter-100, train loss-2.2824, acc-0.1400, valid loss-2.2802, acc-0.1526, test loss-2.2815, acc-0.1610\n",
      "Iter-110, train loss-2.2577, acc-0.2000, valid loss-2.2801, acc-0.1532, test loss-2.2814, acc-0.1615\n",
      "Iter-120, train loss-2.2940, acc-0.1600, valid loss-2.2800, acc-0.1536, test loss-2.2813, acc-0.1623\n",
      "Iter-130, train loss-2.2722, acc-0.2000, valid loss-2.2799, acc-0.1538, test loss-2.2812, acc-0.1625\n",
      "Iter-140, train loss-2.2728, acc-0.1400, valid loss-2.2798, acc-0.1540, test loss-2.2811, acc-0.1627\n",
      "Iter-150, train loss-2.2830, acc-0.2200, valid loss-2.2797, acc-0.1546, test loss-2.2810, acc-0.1628\n",
      "Iter-160, train loss-2.2897, acc-0.1200, valid loss-2.2796, acc-0.1546, test loss-2.2808, acc-0.1633\n",
      "Iter-170, train loss-2.2562, acc-0.2400, valid loss-2.2794, acc-0.1546, test loss-2.2807, acc-0.1639\n",
      "Iter-180, train loss-2.2808, acc-0.1200, valid loss-2.2793, acc-0.1542, test loss-2.2806, acc-0.1634\n",
      "Iter-190, train loss-2.2849, acc-0.1600, valid loss-2.2792, acc-0.1548, test loss-2.2805, acc-0.1639\n",
      "Iter-200, train loss-2.2680, acc-0.1600, valid loss-2.2791, acc-0.1550, test loss-2.2804, acc-0.1644\n",
      "Iter-210, train loss-2.2844, acc-0.2000, valid loss-2.2790, acc-0.1554, test loss-2.2803, acc-0.1643\n",
      "Iter-220, train loss-2.2891, acc-0.1400, valid loss-2.2789, acc-0.1556, test loss-2.2802, acc-0.1651\n",
      "Iter-230, train loss-2.2498, acc-0.2800, valid loss-2.2787, acc-0.1562, test loss-2.2800, acc-0.1650\n",
      "Iter-240, train loss-2.2607, acc-0.2200, valid loss-2.2786, acc-0.1560, test loss-2.2799, acc-0.1651\n",
      "Iter-250, train loss-2.2746, acc-0.1000, valid loss-2.2785, acc-0.1562, test loss-2.2798, acc-0.1653\n",
      "Iter-260, train loss-2.2715, acc-0.1600, valid loss-2.2784, acc-0.1566, test loss-2.2797, acc-0.1652\n",
      "Iter-270, train loss-2.2901, acc-0.1400, valid loss-2.2783, acc-0.1572, test loss-2.2796, acc-0.1656\n",
      "Iter-280, train loss-2.2746, acc-0.1400, valid loss-2.2782, acc-0.1574, test loss-2.2795, acc-0.1660\n",
      "Iter-290, train loss-2.2778, acc-0.1800, valid loss-2.2781, acc-0.1582, test loss-2.2794, acc-0.1662\n",
      "Iter-300, train loss-2.2994, acc-0.1200, valid loss-2.2780, acc-0.1592, test loss-2.2793, acc-0.1667\n",
      "Iter-310, train loss-2.2795, acc-0.1400, valid loss-2.2779, acc-0.1592, test loss-2.2792, acc-0.1667\n",
      "Iter-320, train loss-2.2883, acc-0.1600, valid loss-2.2778, acc-0.1596, test loss-2.2791, acc-0.1673\n",
      "Iter-330, train loss-2.2689, acc-0.2000, valid loss-2.2776, acc-0.1596, test loss-2.2789, acc-0.1675\n",
      "Iter-340, train loss-2.2837, acc-0.1200, valid loss-2.2775, acc-0.1606, test loss-2.2788, acc-0.1676\n",
      "Iter-350, train loss-2.2648, acc-0.2000, valid loss-2.2774, acc-0.1610, test loss-2.2787, acc-0.1679\n",
      "Iter-360, train loss-2.2748, acc-0.1400, valid loss-2.2773, acc-0.1612, test loss-2.2786, acc-0.1681\n",
      "Iter-370, train loss-2.2652, acc-0.2000, valid loss-2.2772, acc-0.1618, test loss-2.2785, acc-0.1684\n",
      "Iter-380, train loss-2.2666, acc-0.1600, valid loss-2.2771, acc-0.1622, test loss-2.2784, acc-0.1689\n",
      "Iter-390, train loss-2.2941, acc-0.1400, valid loss-2.2769, acc-0.1620, test loss-2.2783, acc-0.1688\n",
      "Iter-400, train loss-2.3029, acc-0.0800, valid loss-2.2768, acc-0.1620, test loss-2.2782, acc-0.1693\n",
      "Iter-410, train loss-2.2794, acc-0.1800, valid loss-2.2767, acc-0.1622, test loss-2.2780, acc-0.1696\n",
      "Iter-420, train loss-2.2988, acc-0.1000, valid loss-2.2766, acc-0.1624, test loss-2.2779, acc-0.1697\n",
      "Iter-430, train loss-2.3061, acc-0.0400, valid loss-2.2765, acc-0.1628, test loss-2.2778, acc-0.1703\n",
      "Iter-440, train loss-2.2782, acc-0.1600, valid loss-2.2764, acc-0.1632, test loss-2.2777, acc-0.1705\n",
      "Iter-450, train loss-2.2623, acc-0.1400, valid loss-2.2763, acc-0.1638, test loss-2.2776, acc-0.1707\n",
      "Iter-460, train loss-2.2838, acc-0.1600, valid loss-2.2762, acc-0.1638, test loss-2.2775, acc-0.1711\n",
      "Iter-470, train loss-2.2704, acc-0.1800, valid loss-2.2760, acc-0.1644, test loss-2.2774, acc-0.1712\n",
      "Iter-480, train loss-2.2706, acc-0.1600, valid loss-2.2759, acc-0.1648, test loss-2.2773, acc-0.1713\n",
      "Iter-490, train loss-2.2623, acc-0.1200, valid loss-2.2758, acc-0.1652, test loss-2.2772, acc-0.1719\n",
      "Iter-500, train loss-2.2920, acc-0.1800, valid loss-2.2757, acc-0.1656, test loss-2.2770, acc-0.1720\n",
      "Iter-510, train loss-2.3017, acc-0.0600, valid loss-2.2756, acc-0.1660, test loss-2.2769, acc-0.1721\n",
      "Iter-520, train loss-2.2649, acc-0.2000, valid loss-2.2755, acc-0.1664, test loss-2.2768, acc-0.1723\n",
      "Iter-530, train loss-2.2647, acc-0.2400, valid loss-2.2754, acc-0.1666, test loss-2.2767, acc-0.1726\n",
      "Iter-540, train loss-2.2679, acc-0.1400, valid loss-2.2753, acc-0.1674, test loss-2.2766, acc-0.1732\n",
      "Iter-550, train loss-2.2646, acc-0.1600, valid loss-2.2752, acc-0.1678, test loss-2.2765, acc-0.1733\n",
      "Iter-560, train loss-2.2893, acc-0.1200, valid loss-2.2751, acc-0.1682, test loss-2.2764, acc-0.1735\n",
      "Iter-570, train loss-2.2531, acc-0.2000, valid loss-2.2750, acc-0.1688, test loss-2.2763, acc-0.1743\n",
      "Iter-580, train loss-2.2967, acc-0.1000, valid loss-2.2748, acc-0.1686, test loss-2.2762, acc-0.1743\n",
      "Iter-590, train loss-2.2799, acc-0.1800, valid loss-2.2747, acc-0.1686, test loss-2.2761, acc-0.1744\n",
      "Iter-600, train loss-2.2753, acc-0.1600, valid loss-2.2746, acc-0.1688, test loss-2.2760, acc-0.1748\n",
      "Iter-610, train loss-2.2567, acc-0.2600, valid loss-2.2745, acc-0.1696, test loss-2.2759, acc-0.1750\n",
      "Iter-620, train loss-2.2739, acc-0.2000, valid loss-2.2744, acc-0.1694, test loss-2.2757, acc-0.1750\n",
      "Iter-630, train loss-2.2504, acc-0.2600, valid loss-2.2743, acc-0.1698, test loss-2.2756, acc-0.1756\n",
      "Iter-640, train loss-2.2682, acc-0.2200, valid loss-2.2742, acc-0.1706, test loss-2.2755, acc-0.1759\n",
      "Iter-650, train loss-2.2690, acc-0.1400, valid loss-2.2741, acc-0.1710, test loss-2.2754, acc-0.1762\n",
      "Iter-660, train loss-2.2575, acc-0.1400, valid loss-2.2740, acc-0.1718, test loss-2.2753, acc-0.1763\n",
      "Iter-670, train loss-2.2540, acc-0.1800, valid loss-2.2739, acc-0.1718, test loss-2.2752, acc-0.1769\n",
      "Iter-680, train loss-2.2719, acc-0.1800, valid loss-2.2737, acc-0.1720, test loss-2.2751, acc-0.1773\n",
      "Iter-690, train loss-2.2818, acc-0.1600, valid loss-2.2736, acc-0.1728, test loss-2.2750, acc-0.1772\n",
      "Iter-700, train loss-2.2687, acc-0.2400, valid loss-2.2735, acc-0.1740, test loss-2.2749, acc-0.1774\n",
      "Iter-710, train loss-2.2736, acc-0.2000, valid loss-2.2734, acc-0.1744, test loss-2.2748, acc-0.1775\n",
      "Iter-720, train loss-2.2723, acc-0.1400, valid loss-2.2733, acc-0.1754, test loss-2.2747, acc-0.1779\n",
      "Iter-730, train loss-2.2586, acc-0.1800, valid loss-2.2732, acc-0.1764, test loss-2.2745, acc-0.1783\n",
      "Iter-740, train loss-2.2798, acc-0.1200, valid loss-2.2731, acc-0.1764, test loss-2.2744, acc-0.1787\n",
      "Iter-750, train loss-2.2905, acc-0.1400, valid loss-2.2730, acc-0.1766, test loss-2.2743, acc-0.1790\n",
      "Iter-760, train loss-2.2806, acc-0.1400, valid loss-2.2729, acc-0.1768, test loss-2.2742, acc-0.1791\n",
      "Iter-770, train loss-2.2980, acc-0.1200, valid loss-2.2727, acc-0.1768, test loss-2.2741, acc-0.1790\n",
      "Iter-780, train loss-2.2891, acc-0.1400, valid loss-2.2726, acc-0.1768, test loss-2.2740, acc-0.1794\n",
      "Iter-790, train loss-2.2911, acc-0.1600, valid loss-2.2725, acc-0.1772, test loss-2.2739, acc-0.1798\n",
      "Iter-800, train loss-2.2605, acc-0.1600, valid loss-2.2724, acc-0.1776, test loss-2.2738, acc-0.1798\n",
      "Iter-810, train loss-2.2786, acc-0.1800, valid loss-2.2723, acc-0.1776, test loss-2.2737, acc-0.1799\n",
      "Iter-820, train loss-2.2888, acc-0.1400, valid loss-2.2722, acc-0.1774, test loss-2.2735, acc-0.1806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.2748, acc-0.1800, valid loss-2.2721, acc-0.1776, test loss-2.2734, acc-0.1810\n",
      "Iter-840, train loss-2.2748, acc-0.1600, valid loss-2.2719, acc-0.1778, test loss-2.2733, acc-0.1811\n",
      "Iter-850, train loss-2.2616, acc-0.2800, valid loss-2.2718, acc-0.1784, test loss-2.2732, acc-0.1814\n",
      "Iter-860, train loss-2.2841, acc-0.2000, valid loss-2.2717, acc-0.1788, test loss-2.2731, acc-0.1818\n",
      "Iter-870, train loss-2.2805, acc-0.1600, valid loss-2.2716, acc-0.1798, test loss-2.2730, acc-0.1823\n",
      "Iter-880, train loss-2.2558, acc-0.2400, valid loss-2.2715, acc-0.1802, test loss-2.2729, acc-0.1828\n",
      "Iter-890, train loss-2.2706, acc-0.1400, valid loss-2.2714, acc-0.1802, test loss-2.2728, acc-0.1833\n",
      "Iter-900, train loss-2.2833, acc-0.1400, valid loss-2.2713, acc-0.1802, test loss-2.2727, acc-0.1837\n",
      "Iter-910, train loss-2.2709, acc-0.2000, valid loss-2.2712, acc-0.1804, test loss-2.2726, acc-0.1841\n",
      "Iter-920, train loss-2.2725, acc-0.1800, valid loss-2.2711, acc-0.1802, test loss-2.2724, acc-0.1841\n",
      "Iter-930, train loss-2.2800, acc-0.1600, valid loss-2.2710, acc-0.1816, test loss-2.2723, acc-0.1849\n",
      "Iter-940, train loss-2.2721, acc-0.2000, valid loss-2.2708, acc-0.1816, test loss-2.2722, acc-0.1849\n",
      "Iter-950, train loss-2.2775, acc-0.2600, valid loss-2.2707, acc-0.1818, test loss-2.2721, acc-0.1853\n",
      "Iter-960, train loss-2.2836, acc-0.1600, valid loss-2.2706, acc-0.1820, test loss-2.2720, acc-0.1855\n",
      "Iter-970, train loss-2.2713, acc-0.2600, valid loss-2.2705, acc-0.1832, test loss-2.2719, acc-0.1856\n",
      "Iter-980, train loss-2.2778, acc-0.2000, valid loss-2.2704, acc-0.1836, test loss-2.2718, acc-0.1859\n",
      "Iter-990, train loss-2.2757, acc-0.2000, valid loss-2.2703, acc-0.1836, test loss-2.2717, acc-0.1862\n",
      "Iter-1000, train loss-2.2784, acc-0.1400, valid loss-2.2702, acc-0.1836, test loss-2.2716, acc-0.1864\n",
      "Iter-1010, train loss-2.2640, acc-0.2800, valid loss-2.2701, acc-0.1840, test loss-2.2715, acc-0.1865\n",
      "Iter-1020, train loss-2.2672, acc-0.1200, valid loss-2.2700, acc-0.1844, test loss-2.2714, acc-0.1868\n",
      "Iter-1030, train loss-2.2768, acc-0.1800, valid loss-2.2698, acc-0.1846, test loss-2.2712, acc-0.1871\n",
      "Iter-1040, train loss-2.2821, acc-0.1200, valid loss-2.2697, acc-0.1852, test loss-2.2711, acc-0.1871\n",
      "Iter-1050, train loss-2.2790, acc-0.2200, valid loss-2.2696, acc-0.1854, test loss-2.2710, acc-0.1874\n",
      "Iter-1060, train loss-2.2970, acc-0.2200, valid loss-2.2695, acc-0.1858, test loss-2.2709, acc-0.1877\n",
      "Iter-1070, train loss-2.2813, acc-0.2400, valid loss-2.2694, acc-0.1866, test loss-2.2708, acc-0.1883\n",
      "Iter-1080, train loss-2.2523, acc-0.2400, valid loss-2.2693, acc-0.1868, test loss-2.2707, acc-0.1883\n",
      "Iter-1090, train loss-2.2920, acc-0.1800, valid loss-2.2692, acc-0.1872, test loss-2.2706, acc-0.1885\n",
      "Iter-1100, train loss-2.2854, acc-0.1000, valid loss-2.2691, acc-0.1872, test loss-2.2705, acc-0.1893\n",
      "Iter-1110, train loss-2.2757, acc-0.2800, valid loss-2.2690, acc-0.1876, test loss-2.2704, acc-0.1897\n",
      "Iter-1120, train loss-2.2659, acc-0.2200, valid loss-2.2688, acc-0.1880, test loss-2.2703, acc-0.1900\n",
      "Iter-1130, train loss-2.2695, acc-0.1800, valid loss-2.2687, acc-0.1886, test loss-2.2701, acc-0.1899\n",
      "Iter-1140, train loss-2.2411, acc-0.2400, valid loss-2.2686, acc-0.1886, test loss-2.2700, acc-0.1907\n",
      "Iter-1150, train loss-2.2724, acc-0.1800, valid loss-2.2685, acc-0.1888, test loss-2.2699, acc-0.1912\n",
      "Iter-1160, train loss-2.2752, acc-0.1600, valid loss-2.2684, acc-0.1890, test loss-2.2698, acc-0.1912\n",
      "Iter-1170, train loss-2.2703, acc-0.1800, valid loss-2.2683, acc-0.1894, test loss-2.2697, acc-0.1914\n",
      "Iter-1180, train loss-2.2674, acc-0.1800, valid loss-2.2682, acc-0.1900, test loss-2.2696, acc-0.1918\n",
      "Iter-1190, train loss-2.2683, acc-0.1800, valid loss-2.2681, acc-0.1904, test loss-2.2695, acc-0.1926\n",
      "Iter-1200, train loss-2.2744, acc-0.2400, valid loss-2.2679, acc-0.1904, test loss-2.2694, acc-0.1930\n",
      "Iter-1210, train loss-2.2716, acc-0.0600, valid loss-2.2678, acc-0.1904, test loss-2.2692, acc-0.1933\n",
      "Iter-1220, train loss-2.2839, acc-0.1800, valid loss-2.2677, acc-0.1908, test loss-2.2691, acc-0.1938\n",
      "Iter-1230, train loss-2.2639, acc-0.1400, valid loss-2.2676, acc-0.1908, test loss-2.2690, acc-0.1938\n",
      "Iter-1240, train loss-2.2707, acc-0.1800, valid loss-2.2675, acc-0.1914, test loss-2.2689, acc-0.1940\n",
      "Iter-1250, train loss-2.2591, acc-0.1600, valid loss-2.2674, acc-0.1922, test loss-2.2688, acc-0.1944\n",
      "Iter-1260, train loss-2.2683, acc-0.1400, valid loss-2.2673, acc-0.1924, test loss-2.2687, acc-0.1947\n",
      "Iter-1270, train loss-2.2740, acc-0.2000, valid loss-2.2672, acc-0.1922, test loss-2.2686, acc-0.1952\n",
      "Iter-1280, train loss-2.2468, acc-0.2600, valid loss-2.2670, acc-0.1922, test loss-2.2684, acc-0.1956\n",
      "Iter-1290, train loss-2.2699, acc-0.2200, valid loss-2.2669, acc-0.1928, test loss-2.2683, acc-0.1961\n",
      "Iter-1300, train loss-2.2879, acc-0.1800, valid loss-2.2668, acc-0.1932, test loss-2.2682, acc-0.1962\n",
      "Iter-1310, train loss-2.2604, acc-0.2200, valid loss-2.2667, acc-0.1936, test loss-2.2681, acc-0.1968\n",
      "Iter-1320, train loss-2.2577, acc-0.2200, valid loss-2.2666, acc-0.1946, test loss-2.2680, acc-0.1972\n",
      "Iter-1330, train loss-2.2842, acc-0.2000, valid loss-2.2665, acc-0.1946, test loss-2.2679, acc-0.1973\n",
      "Iter-1340, train loss-2.2663, acc-0.2600, valid loss-2.2664, acc-0.1948, test loss-2.2678, acc-0.1971\n",
      "Iter-1350, train loss-2.2786, acc-0.2600, valid loss-2.2663, acc-0.1958, test loss-2.2677, acc-0.1974\n",
      "Iter-1360, train loss-2.2708, acc-0.2000, valid loss-2.2662, acc-0.1960, test loss-2.2676, acc-0.1978\n",
      "Iter-1370, train loss-2.2735, acc-0.1800, valid loss-2.2661, acc-0.1964, test loss-2.2675, acc-0.1977\n",
      "Iter-1380, train loss-2.2877, acc-0.1000, valid loss-2.2660, acc-0.1972, test loss-2.2674, acc-0.1979\n",
      "Iter-1390, train loss-2.2788, acc-0.2200, valid loss-2.2658, acc-0.1974, test loss-2.2672, acc-0.1983\n",
      "Iter-1400, train loss-2.2767, acc-0.2000, valid loss-2.2657, acc-0.1976, test loss-2.2671, acc-0.1981\n",
      "Iter-1410, train loss-2.2613, acc-0.2000, valid loss-2.2656, acc-0.1978, test loss-2.2670, acc-0.1984\n",
      "Iter-1420, train loss-2.2792, acc-0.1600, valid loss-2.2655, acc-0.1982, test loss-2.2669, acc-0.1985\n",
      "Iter-1430, train loss-2.2772, acc-0.1600, valid loss-2.2654, acc-0.1982, test loss-2.2668, acc-0.1991\n",
      "Iter-1440, train loss-2.2703, acc-0.2000, valid loss-2.2653, acc-0.1982, test loss-2.2667, acc-0.1996\n",
      "Iter-1450, train loss-2.2559, acc-0.1800, valid loss-2.2652, acc-0.1990, test loss-2.2666, acc-0.2000\n",
      "Iter-1460, train loss-2.2585, acc-0.2000, valid loss-2.2651, acc-0.1994, test loss-2.2665, acc-0.2001\n",
      "Iter-1470, train loss-2.2656, acc-0.2600, valid loss-2.2649, acc-0.1998, test loss-2.2664, acc-0.2004\n",
      "Iter-1480, train loss-2.2646, acc-0.2000, valid loss-2.2648, acc-0.2004, test loss-2.2662, acc-0.2006\n",
      "Iter-1490, train loss-2.2394, acc-0.2200, valid loss-2.2647, acc-0.2006, test loss-2.2661, acc-0.2007\n",
      "Iter-1500, train loss-2.2704, acc-0.1800, valid loss-2.2646, acc-0.2008, test loss-2.2660, acc-0.2009\n",
      "Iter-1510, train loss-2.2704, acc-0.1000, valid loss-2.2645, acc-0.2014, test loss-2.2659, acc-0.2011\n",
      "Iter-1520, train loss-2.2561, acc-0.2400, valid loss-2.2644, acc-0.2020, test loss-2.2658, acc-0.2023\n",
      "Iter-1530, train loss-2.2422, acc-0.2800, valid loss-2.2643, acc-0.2024, test loss-2.2657, acc-0.2025\n",
      "Iter-1540, train loss-2.2455, acc-0.2800, valid loss-2.2642, acc-0.2028, test loss-2.2656, acc-0.2025\n",
      "Iter-1550, train loss-2.2617, acc-0.2200, valid loss-2.2641, acc-0.2036, test loss-2.2655, acc-0.2028\n",
      "Iter-1560, train loss-2.2929, acc-0.1000, valid loss-2.2639, acc-0.2040, test loss-2.2654, acc-0.2030\n",
      "Iter-1570, train loss-2.2619, acc-0.2000, valid loss-2.2638, acc-0.2044, test loss-2.2652, acc-0.2031\n",
      "Iter-1580, train loss-2.2715, acc-0.1800, valid loss-2.2637, acc-0.2042, test loss-2.2651, acc-0.2035\n",
      "Iter-1590, train loss-2.2645, acc-0.2600, valid loss-2.2636, acc-0.2046, test loss-2.2650, acc-0.2039\n",
      "Iter-1600, train loss-2.2747, acc-0.2400, valid loss-2.2635, acc-0.2050, test loss-2.2649, acc-0.2039\n",
      "Iter-1610, train loss-2.2514, acc-0.2800, valid loss-2.2634, acc-0.2058, test loss-2.2648, acc-0.2039\n",
      "Iter-1620, train loss-2.2458, acc-0.3000, valid loss-2.2633, acc-0.2058, test loss-2.2647, acc-0.2043\n",
      "Iter-1630, train loss-2.2674, acc-0.2000, valid loss-2.2632, acc-0.2062, test loss-2.2646, acc-0.2045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.2861, acc-0.1600, valid loss-2.2631, acc-0.2064, test loss-2.2645, acc-0.2045\n",
      "Iter-1650, train loss-2.2689, acc-0.1200, valid loss-2.2629, acc-0.2070, test loss-2.2644, acc-0.2047\n",
      "Iter-1660, train loss-2.2596, acc-0.2800, valid loss-2.2628, acc-0.2076, test loss-2.2642, acc-0.2050\n",
      "Iter-1670, train loss-2.2599, acc-0.1800, valid loss-2.2627, acc-0.2076, test loss-2.2641, acc-0.2052\n",
      "Iter-1680, train loss-2.2681, acc-0.2200, valid loss-2.2626, acc-0.2082, test loss-2.2640, acc-0.2056\n",
      "Iter-1690, train loss-2.2540, acc-0.2400, valid loss-2.2625, acc-0.2084, test loss-2.2639, acc-0.2056\n",
      "Iter-1700, train loss-2.2877, acc-0.1400, valid loss-2.2624, acc-0.2084, test loss-2.2638, acc-0.2059\n",
      "Iter-1710, train loss-2.2631, acc-0.1600, valid loss-2.2623, acc-0.2086, test loss-2.2637, acc-0.2065\n",
      "Iter-1720, train loss-2.2613, acc-0.2000, valid loss-2.2622, acc-0.2090, test loss-2.2636, acc-0.2069\n",
      "Iter-1730, train loss-2.2736, acc-0.2400, valid loss-2.2621, acc-0.2094, test loss-2.2635, acc-0.2073\n",
      "Iter-1740, train loss-2.2666, acc-0.1200, valid loss-2.2619, acc-0.2092, test loss-2.2634, acc-0.2075\n",
      "Iter-1750, train loss-2.2698, acc-0.3200, valid loss-2.2618, acc-0.2100, test loss-2.2633, acc-0.2078\n",
      "Iter-1760, train loss-2.2709, acc-0.1600, valid loss-2.2617, acc-0.2104, test loss-2.2631, acc-0.2082\n",
      "Iter-1770, train loss-2.2861, acc-0.1400, valid loss-2.2616, acc-0.2110, test loss-2.2630, acc-0.2085\n",
      "Iter-1780, train loss-2.2649, acc-0.2000, valid loss-2.2615, acc-0.2116, test loss-2.2630, acc-0.2085\n",
      "Iter-1790, train loss-2.2516, acc-0.2200, valid loss-2.2614, acc-0.2118, test loss-2.2628, acc-0.2087\n",
      "Iter-1800, train loss-2.2735, acc-0.1600, valid loss-2.2613, acc-0.2124, test loss-2.2627, acc-0.2092\n",
      "Iter-1810, train loss-2.2498, acc-0.3000, valid loss-2.2612, acc-0.2130, test loss-2.2626, acc-0.2094\n",
      "Iter-1820, train loss-2.2700, acc-0.2200, valid loss-2.2611, acc-0.2132, test loss-2.2625, acc-0.2101\n",
      "Iter-1830, train loss-2.2568, acc-0.2800, valid loss-2.2610, acc-0.2130, test loss-2.2624, acc-0.2101\n",
      "Iter-1840, train loss-2.2593, acc-0.2000, valid loss-2.2609, acc-0.2130, test loss-2.2623, acc-0.2105\n",
      "Iter-1850, train loss-2.2319, acc-0.2400, valid loss-2.2608, acc-0.2132, test loss-2.2622, acc-0.2106\n",
      "Iter-1860, train loss-2.2819, acc-0.1600, valid loss-2.2606, acc-0.2138, test loss-2.2621, acc-0.2108\n",
      "Iter-1870, train loss-2.2505, acc-0.2000, valid loss-2.2605, acc-0.2142, test loss-2.2620, acc-0.2109\n",
      "Iter-1880, train loss-2.2524, acc-0.2800, valid loss-2.2604, acc-0.2148, test loss-2.2619, acc-0.2111\n",
      "Iter-1890, train loss-2.2542, acc-0.2400, valid loss-2.2603, acc-0.2148, test loss-2.2618, acc-0.2117\n",
      "Iter-1900, train loss-2.2813, acc-0.2000, valid loss-2.2602, acc-0.2152, test loss-2.2617, acc-0.2123\n",
      "Iter-1910, train loss-2.2560, acc-0.2600, valid loss-2.2601, acc-0.2154, test loss-2.2616, acc-0.2122\n",
      "Iter-1920, train loss-2.2403, acc-0.3200, valid loss-2.2600, acc-0.2154, test loss-2.2614, acc-0.2129\n",
      "Iter-1930, train loss-2.2455, acc-0.2800, valid loss-2.2599, acc-0.2156, test loss-2.2613, acc-0.2134\n",
      "Iter-1940, train loss-2.2504, acc-0.3000, valid loss-2.2598, acc-0.2154, test loss-2.2612, acc-0.2136\n",
      "Iter-1950, train loss-2.2547, acc-0.2600, valid loss-2.2597, acc-0.2156, test loss-2.2611, acc-0.2142\n",
      "Iter-1960, train loss-2.2497, acc-0.1800, valid loss-2.2595, acc-0.2156, test loss-2.2610, acc-0.2146\n",
      "Iter-1970, train loss-2.2795, acc-0.1400, valid loss-2.2594, acc-0.2164, test loss-2.2609, acc-0.2150\n",
      "Iter-1980, train loss-2.2591, acc-0.2200, valid loss-2.2593, acc-0.2170, test loss-2.2608, acc-0.2152\n",
      "Iter-1990, train loss-2.2772, acc-0.1600, valid loss-2.2592, acc-0.2172, test loss-2.2607, acc-0.2152\n",
      "Iter-2000, train loss-2.2568, acc-0.2600, valid loss-2.2591, acc-0.2178, test loss-2.2606, acc-0.2155\n",
      "Iter-2010, train loss-2.2590, acc-0.1600, valid loss-2.2590, acc-0.2186, test loss-2.2605, acc-0.2162\n",
      "Iter-2020, train loss-2.2575, acc-0.2200, valid loss-2.2589, acc-0.2188, test loss-2.2604, acc-0.2164\n",
      "Iter-2030, train loss-2.2780, acc-0.1600, valid loss-2.2588, acc-0.2192, test loss-2.2603, acc-0.2163\n",
      "Iter-2040, train loss-2.2292, acc-0.3600, valid loss-2.2587, acc-0.2198, test loss-2.2601, acc-0.2166\n",
      "Iter-2050, train loss-2.2599, acc-0.1400, valid loss-2.2586, acc-0.2200, test loss-2.2600, acc-0.2173\n",
      "Iter-2060, train loss-2.2578, acc-0.2800, valid loss-2.2584, acc-0.2204, test loss-2.2599, acc-0.2180\n",
      "Iter-2070, train loss-2.2705, acc-0.1600, valid loss-2.2583, acc-0.2208, test loss-2.2598, acc-0.2178\n",
      "Iter-2080, train loss-2.2480, acc-0.2200, valid loss-2.2582, acc-0.2208, test loss-2.2597, acc-0.2183\n",
      "Iter-2090, train loss-2.2454, acc-0.3600, valid loss-2.2581, acc-0.2212, test loss-2.2596, acc-0.2187\n",
      "Iter-2100, train loss-2.2377, acc-0.3000, valid loss-2.2580, acc-0.2210, test loss-2.2595, acc-0.2191\n",
      "Iter-2110, train loss-2.2999, acc-0.1800, valid loss-2.2579, acc-0.2210, test loss-2.2594, acc-0.2193\n",
      "Iter-2120, train loss-2.2548, acc-0.3000, valid loss-2.2578, acc-0.2212, test loss-2.2593, acc-0.2193\n",
      "Iter-2130, train loss-2.2466, acc-0.2400, valid loss-2.2577, acc-0.2224, test loss-2.2592, acc-0.2203\n",
      "Iter-2140, train loss-2.2954, acc-0.0800, valid loss-2.2576, acc-0.2232, test loss-2.2590, acc-0.2207\n",
      "Iter-2150, train loss-2.2642, acc-0.2600, valid loss-2.2574, acc-0.2236, test loss-2.2589, acc-0.2209\n",
      "Iter-2160, train loss-2.2796, acc-0.1800, valid loss-2.2573, acc-0.2234, test loss-2.2588, acc-0.2210\n",
      "Iter-2170, train loss-2.2566, acc-0.2200, valid loss-2.2572, acc-0.2244, test loss-2.2587, acc-0.2215\n",
      "Iter-2180, train loss-2.2490, acc-0.2800, valid loss-2.2571, acc-0.2244, test loss-2.2586, acc-0.2222\n",
      "Iter-2190, train loss-2.2388, acc-0.2600, valid loss-2.2570, acc-0.2252, test loss-2.2585, acc-0.2225\n",
      "Iter-2200, train loss-2.2544, acc-0.2400, valid loss-2.2569, acc-0.2254, test loss-2.2584, acc-0.2228\n",
      "Iter-2210, train loss-2.2624, acc-0.2200, valid loss-2.2568, acc-0.2254, test loss-2.2583, acc-0.2228\n",
      "Iter-2220, train loss-2.2551, acc-0.2200, valid loss-2.2567, acc-0.2254, test loss-2.2582, acc-0.2232\n",
      "Iter-2230, train loss-2.2595, acc-0.3000, valid loss-2.2565, acc-0.2256, test loss-2.2581, acc-0.2237\n",
      "Iter-2240, train loss-2.2472, acc-0.3600, valid loss-2.2564, acc-0.2268, test loss-2.2579, acc-0.2239\n",
      "Iter-2250, train loss-2.2551, acc-0.2000, valid loss-2.2563, acc-0.2278, test loss-2.2578, acc-0.2243\n",
      "Iter-2260, train loss-2.2686, acc-0.2000, valid loss-2.2562, acc-0.2282, test loss-2.2577, acc-0.2246\n",
      "Iter-2270, train loss-2.2520, acc-0.3200, valid loss-2.2561, acc-0.2284, test loss-2.2576, acc-0.2247\n",
      "Iter-2280, train loss-2.2555, acc-0.2000, valid loss-2.2560, acc-0.2284, test loss-2.2575, acc-0.2254\n",
      "Iter-2290, train loss-2.2459, acc-0.2600, valid loss-2.2559, acc-0.2290, test loss-2.2574, acc-0.2254\n",
      "Iter-2300, train loss-2.2446, acc-0.3000, valid loss-2.2558, acc-0.2282, test loss-2.2573, acc-0.2255\n",
      "Iter-2310, train loss-2.2740, acc-0.1800, valid loss-2.2557, acc-0.2284, test loss-2.2572, acc-0.2262\n",
      "Iter-2320, train loss-2.2538, acc-0.2600, valid loss-2.2556, acc-0.2288, test loss-2.2571, acc-0.2263\n",
      "Iter-2330, train loss-2.2657, acc-0.1400, valid loss-2.2555, acc-0.2288, test loss-2.2570, acc-0.2267\n",
      "Iter-2340, train loss-2.2679, acc-0.1800, valid loss-2.2553, acc-0.2292, test loss-2.2569, acc-0.2271\n",
      "Iter-2350, train loss-2.2558, acc-0.1800, valid loss-2.2553, acc-0.2296, test loss-2.2568, acc-0.2274\n",
      "Iter-2360, train loss-2.2568, acc-0.2200, valid loss-2.2551, acc-0.2298, test loss-2.2567, acc-0.2281\n",
      "Iter-2370, train loss-2.2683, acc-0.1400, valid loss-2.2550, acc-0.2300, test loss-2.2565, acc-0.2287\n",
      "Iter-2380, train loss-2.2593, acc-0.2000, valid loss-2.2549, acc-0.2302, test loss-2.2564, acc-0.2288\n",
      "Iter-2390, train loss-2.2747, acc-0.1400, valid loss-2.2548, acc-0.2304, test loss-2.2563, acc-0.2291\n",
      "Iter-2400, train loss-2.2400, acc-0.2000, valid loss-2.2547, acc-0.2310, test loss-2.2562, acc-0.2296\n",
      "Iter-2410, train loss-2.2426, acc-0.3200, valid loss-2.2546, acc-0.2316, test loss-2.2561, acc-0.2303\n",
      "Iter-2420, train loss-2.2836, acc-0.0600, valid loss-2.2545, acc-0.2316, test loss-2.2560, acc-0.2306\n",
      "Iter-2430, train loss-2.2675, acc-0.2400, valid loss-2.2544, acc-0.2318, test loss-2.2559, acc-0.2307\n",
      "Iter-2440, train loss-2.2733, acc-0.2200, valid loss-2.2543, acc-0.2316, test loss-2.2558, acc-0.2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.2790, acc-0.1200, valid loss-2.2542, acc-0.2326, test loss-2.2557, acc-0.2311\n",
      "Iter-2460, train loss-2.2518, acc-0.2800, valid loss-2.2541, acc-0.2334, test loss-2.2556, acc-0.2309\n",
      "Iter-2470, train loss-2.2496, acc-0.3200, valid loss-2.2540, acc-0.2338, test loss-2.2555, acc-0.2312\n",
      "Iter-2480, train loss-2.2356, acc-0.2600, valid loss-2.2538, acc-0.2342, test loss-2.2554, acc-0.2316\n",
      "Iter-2490, train loss-2.2743, acc-0.2200, valid loss-2.2537, acc-0.2344, test loss-2.2553, acc-0.2321\n",
      "Iter-2500, train loss-2.2477, acc-0.2400, valid loss-2.2536, acc-0.2350, test loss-2.2552, acc-0.2322\n",
      "Iter-2510, train loss-2.2434, acc-0.2800, valid loss-2.2535, acc-0.2348, test loss-2.2551, acc-0.2336\n",
      "Iter-2520, train loss-2.2347, acc-0.2800, valid loss-2.2534, acc-0.2352, test loss-2.2550, acc-0.2338\n",
      "Iter-2530, train loss-2.2566, acc-0.2600, valid loss-2.2533, acc-0.2356, test loss-2.2548, acc-0.2342\n",
      "Iter-2540, train loss-2.2564, acc-0.2600, valid loss-2.2532, acc-0.2362, test loss-2.2547, acc-0.2347\n",
      "Iter-2550, train loss-2.2482, acc-0.2400, valid loss-2.2531, acc-0.2364, test loss-2.2546, acc-0.2348\n",
      "Iter-2560, train loss-2.2563, acc-0.1800, valid loss-2.2530, acc-0.2372, test loss-2.2545, acc-0.2352\n",
      "Iter-2570, train loss-2.2684, acc-0.2200, valid loss-2.2529, acc-0.2376, test loss-2.2544, acc-0.2358\n",
      "Iter-2580, train loss-2.2348, acc-0.3000, valid loss-2.2527, acc-0.2380, test loss-2.2543, acc-0.2360\n",
      "Iter-2590, train loss-2.2455, acc-0.3000, valid loss-2.2526, acc-0.2384, test loss-2.2542, acc-0.2365\n",
      "Iter-2600, train loss-2.2540, acc-0.2800, valid loss-2.2525, acc-0.2386, test loss-2.2541, acc-0.2366\n",
      "Iter-2610, train loss-2.2723, acc-0.1800, valid loss-2.2524, acc-0.2390, test loss-2.2539, acc-0.2364\n",
      "Iter-2620, train loss-2.2677, acc-0.2400, valid loss-2.2523, acc-0.2392, test loss-2.2538, acc-0.2371\n",
      "Iter-2630, train loss-2.2477, acc-0.3400, valid loss-2.2522, acc-0.2392, test loss-2.2537, acc-0.2378\n",
      "Iter-2640, train loss-2.2454, acc-0.2400, valid loss-2.2521, acc-0.2398, test loss-2.2536, acc-0.2379\n",
      "Iter-2650, train loss-2.2605, acc-0.2000, valid loss-2.2520, acc-0.2402, test loss-2.2535, acc-0.2384\n",
      "Iter-2660, train loss-2.2572, acc-0.2200, valid loss-2.2519, acc-0.2402, test loss-2.2534, acc-0.2387\n",
      "Iter-2670, train loss-2.2674, acc-0.2000, valid loss-2.2517, acc-0.2406, test loss-2.2533, acc-0.2388\n",
      "Iter-2680, train loss-2.2512, acc-0.2200, valid loss-2.2516, acc-0.2408, test loss-2.2532, acc-0.2392\n",
      "Iter-2690, train loss-2.2658, acc-0.2200, valid loss-2.2515, acc-0.2412, test loss-2.2531, acc-0.2396\n",
      "Iter-2700, train loss-2.2470, acc-0.3000, valid loss-2.2514, acc-0.2414, test loss-2.2530, acc-0.2399\n",
      "Iter-2710, train loss-2.2296, acc-0.3200, valid loss-2.2513, acc-0.2416, test loss-2.2529, acc-0.2403\n",
      "Iter-2720, train loss-2.2580, acc-0.2200, valid loss-2.2512, acc-0.2422, test loss-2.2528, acc-0.2409\n",
      "Iter-2730, train loss-2.2663, acc-0.1800, valid loss-2.2511, acc-0.2422, test loss-2.2527, acc-0.2413\n",
      "Iter-2740, train loss-2.2515, acc-0.1800, valid loss-2.2510, acc-0.2428, test loss-2.2526, acc-0.2416\n",
      "Iter-2750, train loss-2.2540, acc-0.2800, valid loss-2.2509, acc-0.2428, test loss-2.2525, acc-0.2421\n",
      "Iter-2760, train loss-2.2307, acc-0.3400, valid loss-2.2508, acc-0.2432, test loss-2.2523, acc-0.2424\n",
      "Iter-2770, train loss-2.2710, acc-0.1200, valid loss-2.2507, acc-0.2442, test loss-2.2522, acc-0.2428\n",
      "Iter-2780, train loss-2.2358, acc-0.3200, valid loss-2.2506, acc-0.2444, test loss-2.2521, acc-0.2430\n",
      "Iter-2790, train loss-2.2237, acc-0.3000, valid loss-2.2504, acc-0.2444, test loss-2.2520, acc-0.2433\n",
      "Iter-2800, train loss-2.2658, acc-0.1000, valid loss-2.2503, acc-0.2444, test loss-2.2519, acc-0.2439\n",
      "Iter-2810, train loss-2.2371, acc-0.3600, valid loss-2.2502, acc-0.2446, test loss-2.2518, acc-0.2445\n",
      "Iter-2820, train loss-2.2774, acc-0.1600, valid loss-2.2501, acc-0.2444, test loss-2.2517, acc-0.2446\n",
      "Iter-2830, train loss-2.2445, acc-0.3000, valid loss-2.2500, acc-0.2446, test loss-2.2516, acc-0.2449\n",
      "Iter-2840, train loss-2.2754, acc-0.2000, valid loss-2.2499, acc-0.2448, test loss-2.2515, acc-0.2449\n",
      "Iter-2850, train loss-2.2276, acc-0.4600, valid loss-2.2498, acc-0.2456, test loss-2.2514, acc-0.2453\n",
      "Iter-2860, train loss-2.2567, acc-0.2600, valid loss-2.2497, acc-0.2476, test loss-2.2513, acc-0.2458\n",
      "Iter-2870, train loss-2.2372, acc-0.3800, valid loss-2.2496, acc-0.2482, test loss-2.2512, acc-0.2462\n",
      "Iter-2880, train loss-2.2681, acc-0.2200, valid loss-2.2495, acc-0.2480, test loss-2.2511, acc-0.2467\n",
      "Iter-2890, train loss-2.2445, acc-0.3000, valid loss-2.2494, acc-0.2478, test loss-2.2510, acc-0.2471\n",
      "Iter-2900, train loss-2.2444, acc-0.2800, valid loss-2.2493, acc-0.2486, test loss-2.2509, acc-0.2469\n",
      "Iter-2910, train loss-2.2202, acc-0.4000, valid loss-2.2492, acc-0.2502, test loss-2.2508, acc-0.2476\n",
      "Iter-2920, train loss-2.2653, acc-0.2000, valid loss-2.2491, acc-0.2502, test loss-2.2507, acc-0.2478\n",
      "Iter-2930, train loss-2.2314, acc-0.3200, valid loss-2.2490, acc-0.2506, test loss-2.2506, acc-0.2480\n",
      "Iter-2940, train loss-2.2464, acc-0.2400, valid loss-2.2489, acc-0.2516, test loss-2.2505, acc-0.2491\n",
      "Iter-2950, train loss-2.2361, acc-0.2600, valid loss-2.2488, acc-0.2522, test loss-2.2504, acc-0.2499\n",
      "Iter-2960, train loss-2.2455, acc-0.2000, valid loss-2.2487, acc-0.2528, test loss-2.2503, acc-0.2498\n",
      "Iter-2970, train loss-2.2386, acc-0.3000, valid loss-2.2486, acc-0.2530, test loss-2.2502, acc-0.2502\n",
      "Iter-2980, train loss-2.2325, acc-0.3600, valid loss-2.2485, acc-0.2530, test loss-2.2501, acc-0.2504\n",
      "Iter-2990, train loss-2.2409, acc-0.2600, valid loss-2.2483, acc-0.2528, test loss-2.2499, acc-0.2509\n",
      "Iter-3000, train loss-2.2324, acc-0.2600, valid loss-2.2482, acc-0.2534, test loss-2.2498, acc-0.2511\n",
      "Iter-3010, train loss-2.2596, acc-0.2600, valid loss-2.2481, acc-0.2544, test loss-2.2497, acc-0.2515\n",
      "Iter-3020, train loss-2.2551, acc-0.2200, valid loss-2.2480, acc-0.2544, test loss-2.2496, acc-0.2519\n",
      "Iter-3030, train loss-2.2985, acc-0.1600, valid loss-2.2479, acc-0.2548, test loss-2.2495, acc-0.2523\n",
      "Iter-3040, train loss-2.2424, acc-0.2400, valid loss-2.2478, acc-0.2550, test loss-2.2494, acc-0.2524\n",
      "Iter-3050, train loss-2.2552, acc-0.2000, valid loss-2.2477, acc-0.2554, test loss-2.2493, acc-0.2529\n",
      "Iter-3060, train loss-2.2721, acc-0.1400, valid loss-2.2476, acc-0.2562, test loss-2.2492, acc-0.2530\n",
      "Iter-3070, train loss-2.2666, acc-0.1600, valid loss-2.2475, acc-0.2562, test loss-2.2491, acc-0.2534\n",
      "Iter-3080, train loss-2.2422, acc-0.2600, valid loss-2.2474, acc-0.2566, test loss-2.2490, acc-0.2536\n",
      "Iter-3090, train loss-2.2411, acc-0.2800, valid loss-2.2473, acc-0.2572, test loss-2.2489, acc-0.2538\n",
      "Iter-3100, train loss-2.2284, acc-0.3200, valid loss-2.2472, acc-0.2574, test loss-2.2488, acc-0.2543\n",
      "Iter-3110, train loss-2.2638, acc-0.2600, valid loss-2.2471, acc-0.2572, test loss-2.2487, acc-0.2542\n",
      "Iter-3120, train loss-2.2647, acc-0.2400, valid loss-2.2470, acc-0.2584, test loss-2.2486, acc-0.2549\n",
      "Iter-3130, train loss-2.2347, acc-0.3600, valid loss-2.2468, acc-0.2584, test loss-2.2485, acc-0.2551\n",
      "Iter-3140, train loss-2.2496, acc-0.2600, valid loss-2.2467, acc-0.2582, test loss-2.2484, acc-0.2550\n",
      "Iter-3150, train loss-2.2503, acc-0.2800, valid loss-2.2466, acc-0.2592, test loss-2.2483, acc-0.2552\n",
      "Iter-3160, train loss-2.2634, acc-0.1600, valid loss-2.2465, acc-0.2592, test loss-2.2482, acc-0.2555\n",
      "Iter-3170, train loss-2.2274, acc-0.3600, valid loss-2.2464, acc-0.2608, test loss-2.2481, acc-0.2558\n",
      "Iter-3180, train loss-2.2469, acc-0.2800, valid loss-2.2463, acc-0.2610, test loss-2.2480, acc-0.2566\n",
      "Iter-3190, train loss-2.2626, acc-0.2600, valid loss-2.2462, acc-0.2608, test loss-2.2478, acc-0.2573\n",
      "Iter-3200, train loss-2.2456, acc-0.2600, valid loss-2.2461, acc-0.2612, test loss-2.2477, acc-0.2578\n",
      "Iter-3210, train loss-2.2595, acc-0.2000, valid loss-2.2460, acc-0.2618, test loss-2.2476, acc-0.2583\n",
      "Iter-3220, train loss-2.2460, acc-0.2600, valid loss-2.2459, acc-0.2624, test loss-2.2475, acc-0.2591\n",
      "Iter-3230, train loss-2.2578, acc-0.2200, valid loss-2.2458, acc-0.2622, test loss-2.2474, acc-0.2593\n",
      "Iter-3240, train loss-2.2497, acc-0.2600, valid loss-2.2457, acc-0.2628, test loss-2.2473, acc-0.2598\n",
      "Iter-3250, train loss-2.2517, acc-0.2800, valid loss-2.2456, acc-0.2630, test loss-2.2472, acc-0.2599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.2659, acc-0.3000, valid loss-2.2455, acc-0.2636, test loss-2.2471, acc-0.2602\n",
      "Iter-3270, train loss-2.2206, acc-0.3600, valid loss-2.2454, acc-0.2644, test loss-2.2470, acc-0.2607\n",
      "Iter-3280, train loss-2.2604, acc-0.2000, valid loss-2.2453, acc-0.2644, test loss-2.2469, acc-0.2614\n",
      "Iter-3290, train loss-2.2682, acc-0.2000, valid loss-2.2452, acc-0.2646, test loss-2.2468, acc-0.2618\n",
      "Iter-3300, train loss-2.2542, acc-0.2600, valid loss-2.2450, acc-0.2650, test loss-2.2467, acc-0.2621\n",
      "Iter-3310, train loss-2.2395, acc-0.3000, valid loss-2.2449, acc-0.2646, test loss-2.2466, acc-0.2626\n",
      "Iter-3320, train loss-2.2759, acc-0.2400, valid loss-2.2448, acc-0.2652, test loss-2.2465, acc-0.2628\n",
      "Iter-3330, train loss-2.2473, acc-0.2800, valid loss-2.2447, acc-0.2654, test loss-2.2464, acc-0.2634\n",
      "Iter-3340, train loss-2.2556, acc-0.3200, valid loss-2.2446, acc-0.2660, test loss-2.2463, acc-0.2634\n",
      "Iter-3350, train loss-2.2497, acc-0.2200, valid loss-2.2445, acc-0.2666, test loss-2.2462, acc-0.2635\n",
      "Iter-3360, train loss-2.2372, acc-0.3200, valid loss-2.2444, acc-0.2664, test loss-2.2461, acc-0.2639\n",
      "Iter-3370, train loss-2.2428, acc-0.3000, valid loss-2.2443, acc-0.2668, test loss-2.2460, acc-0.2643\n",
      "Iter-3380, train loss-2.2568, acc-0.2600, valid loss-2.2442, acc-0.2674, test loss-2.2459, acc-0.2643\n",
      "Iter-3390, train loss-2.2479, acc-0.3200, valid loss-2.2441, acc-0.2680, test loss-2.2457, acc-0.2643\n",
      "Iter-3400, train loss-2.2529, acc-0.3000, valid loss-2.2440, acc-0.2680, test loss-2.2456, acc-0.2646\n",
      "Iter-3410, train loss-2.2734, acc-0.2000, valid loss-2.2439, acc-0.2684, test loss-2.2455, acc-0.2648\n",
      "Iter-3420, train loss-2.2291, acc-0.3600, valid loss-2.2438, acc-0.2694, test loss-2.2454, acc-0.2650\n",
      "Iter-3430, train loss-2.2570, acc-0.2400, valid loss-2.2437, acc-0.2698, test loss-2.2453, acc-0.2655\n",
      "Iter-3440, train loss-2.2504, acc-0.2200, valid loss-2.2436, acc-0.2698, test loss-2.2452, acc-0.2655\n",
      "Iter-3450, train loss-2.2373, acc-0.3000, valid loss-2.2435, acc-0.2706, test loss-2.2451, acc-0.2656\n",
      "Iter-3460, train loss-2.2380, acc-0.2400, valid loss-2.2434, acc-0.2712, test loss-2.2450, acc-0.2665\n",
      "Iter-3470, train loss-2.2322, acc-0.3400, valid loss-2.2433, acc-0.2716, test loss-2.2449, acc-0.2668\n",
      "Iter-3480, train loss-2.2501, acc-0.2800, valid loss-2.2432, acc-0.2716, test loss-2.2448, acc-0.2671\n",
      "Iter-3490, train loss-2.2705, acc-0.1200, valid loss-2.2430, acc-0.2720, test loss-2.2447, acc-0.2676\n",
      "Iter-3500, train loss-2.2516, acc-0.2000, valid loss-2.2429, acc-0.2722, test loss-2.2446, acc-0.2680\n",
      "Iter-3510, train loss-2.2467, acc-0.2200, valid loss-2.2428, acc-0.2726, test loss-2.2445, acc-0.2682\n",
      "Iter-3520, train loss-2.2515, acc-0.2000, valid loss-2.2427, acc-0.2724, test loss-2.2444, acc-0.2683\n",
      "Iter-3530, train loss-2.2351, acc-0.2000, valid loss-2.2426, acc-0.2730, test loss-2.2443, acc-0.2691\n",
      "Iter-3540, train loss-2.2471, acc-0.2800, valid loss-2.2425, acc-0.2736, test loss-2.2442, acc-0.2691\n",
      "Iter-3550, train loss-2.2660, acc-0.2200, valid loss-2.2424, acc-0.2736, test loss-2.2441, acc-0.2698\n",
      "Iter-3560, train loss-2.2471, acc-0.2800, valid loss-2.2423, acc-0.2744, test loss-2.2440, acc-0.2703\n",
      "Iter-3570, train loss-2.2281, acc-0.3200, valid loss-2.2422, acc-0.2744, test loss-2.2438, acc-0.2708\n",
      "Iter-3580, train loss-2.2578, acc-0.2600, valid loss-2.2420, acc-0.2748, test loss-2.2437, acc-0.2709\n",
      "Iter-3590, train loss-2.2334, acc-0.3400, valid loss-2.2419, acc-0.2752, test loss-2.2436, acc-0.2715\n",
      "Iter-3600, train loss-2.2365, acc-0.3600, valid loss-2.2418, acc-0.2754, test loss-2.2435, acc-0.2718\n",
      "Iter-3610, train loss-2.2487, acc-0.3000, valid loss-2.2417, acc-0.2760, test loss-2.2434, acc-0.2723\n",
      "Iter-3620, train loss-2.2430, acc-0.2200, valid loss-2.2416, acc-0.2760, test loss-2.2433, acc-0.2725\n",
      "Iter-3630, train loss-2.2654, acc-0.2000, valid loss-2.2415, acc-0.2770, test loss-2.2432, acc-0.2731\n",
      "Iter-3640, train loss-2.2709, acc-0.2000, valid loss-2.2414, acc-0.2774, test loss-2.2431, acc-0.2736\n",
      "Iter-3650, train loss-2.2483, acc-0.2400, valid loss-2.2413, acc-0.2774, test loss-2.2430, acc-0.2742\n",
      "Iter-3660, train loss-2.2316, acc-0.3400, valid loss-2.2412, acc-0.2778, test loss-2.2429, acc-0.2740\n",
      "Iter-3670, train loss-2.2299, acc-0.2200, valid loss-2.2411, acc-0.2780, test loss-2.2428, acc-0.2742\n",
      "Iter-3680, train loss-2.2472, acc-0.3200, valid loss-2.2410, acc-0.2782, test loss-2.2427, acc-0.2743\n",
      "Iter-3690, train loss-2.2660, acc-0.2200, valid loss-2.2409, acc-0.2784, test loss-2.2426, acc-0.2744\n",
      "Iter-3700, train loss-2.2454, acc-0.3200, valid loss-2.2408, acc-0.2792, test loss-2.2425, acc-0.2751\n",
      "Iter-3710, train loss-2.2134, acc-0.3600, valid loss-2.2406, acc-0.2798, test loss-2.2424, acc-0.2755\n",
      "Iter-3720, train loss-2.2575, acc-0.2400, valid loss-2.2405, acc-0.2802, test loss-2.2423, acc-0.2757\n",
      "Iter-3730, train loss-2.2442, acc-0.2800, valid loss-2.2405, acc-0.2814, test loss-2.2422, acc-0.2759\n",
      "Iter-3740, train loss-2.2254, acc-0.3200, valid loss-2.2404, acc-0.2820, test loss-2.2421, acc-0.2764\n",
      "Iter-3750, train loss-2.2436, acc-0.2800, valid loss-2.2402, acc-0.2824, test loss-2.2419, acc-0.2768\n",
      "Iter-3760, train loss-2.2421, acc-0.3200, valid loss-2.2401, acc-0.2828, test loss-2.2418, acc-0.2771\n",
      "Iter-3770, train loss-2.2520, acc-0.2600, valid loss-2.2400, acc-0.2828, test loss-2.2417, acc-0.2773\n",
      "Iter-3780, train loss-2.2315, acc-0.3000, valid loss-2.2399, acc-0.2830, test loss-2.2416, acc-0.2777\n",
      "Iter-3790, train loss-2.2543, acc-0.2400, valid loss-2.2398, acc-0.2832, test loss-2.2415, acc-0.2778\n",
      "Iter-3800, train loss-2.2701, acc-0.1600, valid loss-2.2397, acc-0.2834, test loss-2.2414, acc-0.2784\n",
      "Iter-3810, train loss-2.2127, acc-0.3400, valid loss-2.2396, acc-0.2836, test loss-2.2413, acc-0.2788\n",
      "Iter-3820, train loss-2.2374, acc-0.3200, valid loss-2.2395, acc-0.2844, test loss-2.2412, acc-0.2789\n",
      "Iter-3830, train loss-2.2285, acc-0.3000, valid loss-2.2394, acc-0.2846, test loss-2.2411, acc-0.2798\n",
      "Iter-3840, train loss-2.2176, acc-0.4400, valid loss-2.2393, acc-0.2848, test loss-2.2410, acc-0.2797\n",
      "Iter-3850, train loss-2.2375, acc-0.3000, valid loss-2.2392, acc-0.2852, test loss-2.2409, acc-0.2801\n",
      "Iter-3860, train loss-2.2357, acc-0.3200, valid loss-2.2391, acc-0.2854, test loss-2.2408, acc-0.2806\n",
      "Iter-3870, train loss-2.2497, acc-0.2400, valid loss-2.2390, acc-0.2854, test loss-2.2407, acc-0.2809\n",
      "Iter-3880, train loss-2.2432, acc-0.3000, valid loss-2.2389, acc-0.2854, test loss-2.2406, acc-0.2810\n",
      "Iter-3890, train loss-2.2450, acc-0.2600, valid loss-2.2388, acc-0.2858, test loss-2.2405, acc-0.2812\n",
      "Iter-3900, train loss-2.2610, acc-0.1600, valid loss-2.2387, acc-0.2864, test loss-2.2404, acc-0.2818\n",
      "Iter-3910, train loss-2.2426, acc-0.2800, valid loss-2.2385, acc-0.2866, test loss-2.2403, acc-0.2813\n",
      "Iter-3920, train loss-2.2597, acc-0.2200, valid loss-2.2384, acc-0.2872, test loss-2.2402, acc-0.2820\n",
      "Iter-3930, train loss-2.2205, acc-0.3600, valid loss-2.2383, acc-0.2876, test loss-2.2401, acc-0.2821\n",
      "Iter-3940, train loss-2.2450, acc-0.2600, valid loss-2.2382, acc-0.2884, test loss-2.2399, acc-0.2826\n",
      "Iter-3950, train loss-2.2279, acc-0.2800, valid loss-2.2381, acc-0.2884, test loss-2.2398, acc-0.2829\n",
      "Iter-3960, train loss-2.2253, acc-0.3600, valid loss-2.2380, acc-0.2888, test loss-2.2397, acc-0.2833\n",
      "Iter-3970, train loss-2.2357, acc-0.2400, valid loss-2.2379, acc-0.2888, test loss-2.2396, acc-0.2833\n",
      "Iter-3980, train loss-2.2217, acc-0.3800, valid loss-2.2378, acc-0.2886, test loss-2.2395, acc-0.2835\n",
      "Iter-3990, train loss-2.2625, acc-0.2000, valid loss-2.2377, acc-0.2892, test loss-2.2394, acc-0.2839\n",
      "Iter-4000, train loss-2.2424, acc-0.2800, valid loss-2.2376, acc-0.2898, test loss-2.2393, acc-0.2838\n",
      "Iter-4010, train loss-2.2063, acc-0.3400, valid loss-2.2375, acc-0.2904, test loss-2.2392, acc-0.2839\n",
      "Iter-4020, train loss-2.2589, acc-0.2200, valid loss-2.2374, acc-0.2912, test loss-2.2391, acc-0.2848\n",
      "Iter-4030, train loss-2.2346, acc-0.2400, valid loss-2.2373, acc-0.2912, test loss-2.2390, acc-0.2847\n",
      "Iter-4040, train loss-2.2181, acc-0.3200, valid loss-2.2372, acc-0.2912, test loss-2.2389, acc-0.2848\n",
      "Iter-4050, train loss-2.2330, acc-0.3400, valid loss-2.2371, acc-0.2916, test loss-2.2388, acc-0.2852\n",
      "Iter-4060, train loss-2.2481, acc-0.3000, valid loss-2.2370, acc-0.2918, test loss-2.2387, acc-0.2860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.2462, acc-0.3400, valid loss-2.2368, acc-0.2924, test loss-2.2386, acc-0.2864\n",
      "Iter-4080, train loss-2.2136, acc-0.3000, valid loss-2.2367, acc-0.2926, test loss-2.2385, acc-0.2867\n",
      "Iter-4090, train loss-2.2243, acc-0.3200, valid loss-2.2366, acc-0.2926, test loss-2.2384, acc-0.2869\n",
      "Iter-4100, train loss-2.2392, acc-0.3200, valid loss-2.2365, acc-0.2930, test loss-2.2383, acc-0.2868\n",
      "Iter-4110, train loss-2.2501, acc-0.3200, valid loss-2.2364, acc-0.2928, test loss-2.2382, acc-0.2875\n",
      "Iter-4120, train loss-2.2694, acc-0.1800, valid loss-2.2363, acc-0.2934, test loss-2.2381, acc-0.2876\n",
      "Iter-4130, train loss-2.2216, acc-0.3000, valid loss-2.2362, acc-0.2936, test loss-2.2380, acc-0.2878\n",
      "Iter-4140, train loss-2.2453, acc-0.2600, valid loss-2.2361, acc-0.2938, test loss-2.2379, acc-0.2883\n",
      "Iter-4150, train loss-2.2253, acc-0.2800, valid loss-2.2360, acc-0.2940, test loss-2.2378, acc-0.2883\n",
      "Iter-4160, train loss-2.2441, acc-0.2400, valid loss-2.2359, acc-0.2948, test loss-2.2377, acc-0.2886\n",
      "Iter-4170, train loss-2.2555, acc-0.2200, valid loss-2.2358, acc-0.2950, test loss-2.2376, acc-0.2889\n",
      "Iter-4180, train loss-2.2393, acc-0.2400, valid loss-2.2357, acc-0.2952, test loss-2.2375, acc-0.2893\n",
      "Iter-4190, train loss-2.2371, acc-0.3200, valid loss-2.2356, acc-0.2968, test loss-2.2374, acc-0.2895\n",
      "Iter-4200, train loss-2.2399, acc-0.2400, valid loss-2.2355, acc-0.2962, test loss-2.2373, acc-0.2896\n",
      "Iter-4210, train loss-2.2452, acc-0.3400, valid loss-2.2354, acc-0.2970, test loss-2.2372, acc-0.2899\n",
      "Iter-4220, train loss-2.2327, acc-0.3000, valid loss-2.2353, acc-0.2974, test loss-2.2371, acc-0.2899\n",
      "Iter-4230, train loss-2.2371, acc-0.3000, valid loss-2.2352, acc-0.2970, test loss-2.2370, acc-0.2903\n",
      "Iter-4240, train loss-2.2352, acc-0.2600, valid loss-2.2351, acc-0.2974, test loss-2.2369, acc-0.2908\n",
      "Iter-4250, train loss-2.2275, acc-0.3400, valid loss-2.2350, acc-0.2976, test loss-2.2368, acc-0.2911\n",
      "Iter-4260, train loss-2.2519, acc-0.2600, valid loss-2.2349, acc-0.2982, test loss-2.2367, acc-0.2914\n",
      "Iter-4270, train loss-2.2421, acc-0.2600, valid loss-2.2348, acc-0.2984, test loss-2.2366, acc-0.2915\n",
      "Iter-4280, train loss-2.2071, acc-0.3600, valid loss-2.2347, acc-0.2992, test loss-2.2365, acc-0.2920\n",
      "Iter-4290, train loss-2.2167, acc-0.3000, valid loss-2.2346, acc-0.2996, test loss-2.2364, acc-0.2920\n",
      "Iter-4300, train loss-2.2621, acc-0.2400, valid loss-2.2345, acc-0.2996, test loss-2.2363, acc-0.2927\n",
      "Iter-4310, train loss-2.2260, acc-0.3200, valid loss-2.2344, acc-0.2998, test loss-2.2362, acc-0.2927\n",
      "Iter-4320, train loss-2.2100, acc-0.3800, valid loss-2.2343, acc-0.2996, test loss-2.2360, acc-0.2930\n",
      "Iter-4330, train loss-2.2413, acc-0.3200, valid loss-2.2341, acc-0.2996, test loss-2.2359, acc-0.2933\n",
      "Iter-4340, train loss-2.2271, acc-0.3000, valid loss-2.2340, acc-0.2998, test loss-2.2358, acc-0.2932\n",
      "Iter-4350, train loss-2.2580, acc-0.2600, valid loss-2.2339, acc-0.3004, test loss-2.2357, acc-0.2934\n",
      "Iter-4360, train loss-2.2206, acc-0.3800, valid loss-2.2338, acc-0.3000, test loss-2.2356, acc-0.2938\n",
      "Iter-4370, train loss-2.2341, acc-0.3000, valid loss-2.2337, acc-0.3004, test loss-2.2355, acc-0.2942\n",
      "Iter-4380, train loss-2.2449, acc-0.2000, valid loss-2.2336, acc-0.3008, test loss-2.2354, acc-0.2942\n",
      "Iter-4390, train loss-2.2571, acc-0.2000, valid loss-2.2335, acc-0.3008, test loss-2.2353, acc-0.2949\n",
      "Iter-4400, train loss-2.2148, acc-0.2600, valid loss-2.2334, acc-0.3012, test loss-2.2352, acc-0.2951\n",
      "Iter-4410, train loss-2.2380, acc-0.3200, valid loss-2.2333, acc-0.3014, test loss-2.2351, acc-0.2958\n",
      "Iter-4420, train loss-2.2672, acc-0.1600, valid loss-2.2332, acc-0.3014, test loss-2.2350, acc-0.2956\n",
      "Iter-4430, train loss-2.2014, acc-0.4400, valid loss-2.2331, acc-0.3018, test loss-2.2349, acc-0.2959\n",
      "Iter-4440, train loss-2.2335, acc-0.3200, valid loss-2.2330, acc-0.3018, test loss-2.2348, acc-0.2957\n",
      "Iter-4450, train loss-2.2199, acc-0.3200, valid loss-2.2329, acc-0.3022, test loss-2.2347, acc-0.2958\n",
      "Iter-4460, train loss-2.2267, acc-0.4000, valid loss-2.2328, acc-0.3026, test loss-2.2346, acc-0.2967\n",
      "Iter-4470, train loss-2.2424, acc-0.2800, valid loss-2.2327, acc-0.3026, test loss-2.2345, acc-0.2967\n",
      "Iter-4480, train loss-2.2579, acc-0.2800, valid loss-2.2326, acc-0.3030, test loss-2.2344, acc-0.2975\n",
      "Iter-4490, train loss-2.2608, acc-0.2200, valid loss-2.2325, acc-0.3034, test loss-2.2343, acc-0.2973\n",
      "Iter-4500, train loss-2.2487, acc-0.2200, valid loss-2.2324, acc-0.3036, test loss-2.2342, acc-0.2975\n",
      "Iter-4510, train loss-2.2474, acc-0.2600, valid loss-2.2323, acc-0.3038, test loss-2.2341, acc-0.2977\n",
      "Iter-4520, train loss-2.2237, acc-0.3800, valid loss-2.2322, acc-0.3038, test loss-2.2340, acc-0.2979\n",
      "Iter-4530, train loss-2.2488, acc-0.2200, valid loss-2.2321, acc-0.3038, test loss-2.2339, acc-0.2983\n",
      "Iter-4540, train loss-2.2218, acc-0.3400, valid loss-2.2320, acc-0.3036, test loss-2.2338, acc-0.2983\n",
      "Iter-4550, train loss-2.2181, acc-0.2800, valid loss-2.2319, acc-0.3040, test loss-2.2337, acc-0.2988\n",
      "Iter-4560, train loss-2.2057, acc-0.4000, valid loss-2.2318, acc-0.3042, test loss-2.2336, acc-0.2989\n",
      "Iter-4570, train loss-2.2173, acc-0.3200, valid loss-2.2317, acc-0.3046, test loss-2.2335, acc-0.2999\n",
      "Iter-4580, train loss-2.2327, acc-0.2600, valid loss-2.2316, acc-0.3050, test loss-2.2334, acc-0.2999\n",
      "Iter-4590, train loss-2.2448, acc-0.2800, valid loss-2.2315, acc-0.3054, test loss-2.2333, acc-0.3003\n",
      "Iter-4600, train loss-2.2333, acc-0.3400, valid loss-2.2314, acc-0.3056, test loss-2.2332, acc-0.3006\n",
      "Iter-4610, train loss-2.2153, acc-0.3200, valid loss-2.2313, acc-0.3054, test loss-2.2331, acc-0.3006\n",
      "Iter-4620, train loss-2.2639, acc-0.1800, valid loss-2.2312, acc-0.3062, test loss-2.2330, acc-0.3009\n",
      "Iter-4630, train loss-2.2408, acc-0.2800, valid loss-2.2311, acc-0.3064, test loss-2.2329, acc-0.3016\n",
      "Iter-4640, train loss-2.2364, acc-0.3600, valid loss-2.2310, acc-0.3064, test loss-2.2328, acc-0.3016\n",
      "Iter-4650, train loss-2.2407, acc-0.2000, valid loss-2.2309, acc-0.3072, test loss-2.2327, acc-0.3017\n",
      "Iter-4660, train loss-2.2315, acc-0.3800, valid loss-2.2308, acc-0.3076, test loss-2.2326, acc-0.3017\n",
      "Iter-4670, train loss-2.2491, acc-0.2000, valid loss-2.2307, acc-0.3080, test loss-2.2325, acc-0.3023\n",
      "Iter-4680, train loss-2.2360, acc-0.2200, valid loss-2.2305, acc-0.3084, test loss-2.2324, acc-0.3030\n",
      "Iter-4690, train loss-2.2356, acc-0.3000, valid loss-2.2304, acc-0.3084, test loss-2.2323, acc-0.3035\n",
      "Iter-4700, train loss-2.2290, acc-0.2400, valid loss-2.2303, acc-0.3084, test loss-2.2322, acc-0.3034\n",
      "Iter-4710, train loss-2.2378, acc-0.3000, valid loss-2.2302, acc-0.3092, test loss-2.2321, acc-0.3040\n",
      "Iter-4720, train loss-2.2515, acc-0.2800, valid loss-2.2302, acc-0.3094, test loss-2.2320, acc-0.3042\n",
      "Iter-4730, train loss-2.2252, acc-0.3400, valid loss-2.2300, acc-0.3092, test loss-2.2319, acc-0.3048\n",
      "Iter-4740, train loss-2.2320, acc-0.3800, valid loss-2.2299, acc-0.3088, test loss-2.2318, acc-0.3051\n",
      "Iter-4750, train loss-2.2447, acc-0.2400, valid loss-2.2298, acc-0.3096, test loss-2.2317, acc-0.3053\n",
      "Iter-4760, train loss-2.2258, acc-0.3400, valid loss-2.2297, acc-0.3094, test loss-2.2316, acc-0.3056\n",
      "Iter-4770, train loss-2.2070, acc-0.4200, valid loss-2.2296, acc-0.3098, test loss-2.2315, acc-0.3058\n",
      "Iter-4780, train loss-2.2164, acc-0.4000, valid loss-2.2295, acc-0.3100, test loss-2.2314, acc-0.3064\n",
      "Iter-4790, train loss-2.2257, acc-0.3400, valid loss-2.2294, acc-0.3100, test loss-2.2312, acc-0.3067\n",
      "Iter-4800, train loss-2.2320, acc-0.3000, valid loss-2.2293, acc-0.3102, test loss-2.2311, acc-0.3069\n",
      "Iter-4810, train loss-2.2129, acc-0.3800, valid loss-2.2292, acc-0.3110, test loss-2.2311, acc-0.3073\n",
      "Iter-4820, train loss-2.2574, acc-0.2000, valid loss-2.2291, acc-0.3106, test loss-2.2310, acc-0.3076\n",
      "Iter-4830, train loss-2.2136, acc-0.3400, valid loss-2.2290, acc-0.3114, test loss-2.2308, acc-0.3083\n",
      "Iter-4840, train loss-2.2276, acc-0.2400, valid loss-2.2289, acc-0.3116, test loss-2.2307, acc-0.3084\n",
      "Iter-4850, train loss-2.2640, acc-0.2000, valid loss-2.2288, acc-0.3122, test loss-2.2306, acc-0.3084\n",
      "Iter-4860, train loss-2.2603, acc-0.2800, valid loss-2.2287, acc-0.3118, test loss-2.2306, acc-0.3083\n",
      "Iter-4870, train loss-2.2206, acc-0.2600, valid loss-2.2286, acc-0.3124, test loss-2.2304, acc-0.3086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-2.2371, acc-0.2800, valid loss-2.2285, acc-0.3132, test loss-2.2304, acc-0.3089\n",
      "Iter-4890, train loss-2.2325, acc-0.3000, valid loss-2.2284, acc-0.3136, test loss-2.2303, acc-0.3090\n",
      "Iter-4900, train loss-2.2160, acc-0.3200, valid loss-2.2283, acc-0.3140, test loss-2.2302, acc-0.3095\n",
      "Iter-4910, train loss-2.2309, acc-0.3400, valid loss-2.2282, acc-0.3144, test loss-2.2300, acc-0.3095\n",
      "Iter-4920, train loss-2.2213, acc-0.3000, valid loss-2.2281, acc-0.3138, test loss-2.2299, acc-0.3098\n",
      "Iter-4930, train loss-2.2120, acc-0.3600, valid loss-2.2280, acc-0.3142, test loss-2.2298, acc-0.3106\n",
      "Iter-4940, train loss-2.2254, acc-0.3200, valid loss-2.2279, acc-0.3148, test loss-2.2297, acc-0.3109\n",
      "Iter-4950, train loss-2.2186, acc-0.3000, valid loss-2.2278, acc-0.3152, test loss-2.2296, acc-0.3116\n",
      "Iter-4960, train loss-2.2460, acc-0.1800, valid loss-2.2277, acc-0.3154, test loss-2.2295, acc-0.3117\n",
      "Iter-4970, train loss-2.2422, acc-0.3200, valid loss-2.2276, acc-0.3154, test loss-2.2294, acc-0.3120\n",
      "Iter-4980, train loss-2.2204, acc-0.3400, valid loss-2.2275, acc-0.3158, test loss-2.2293, acc-0.3125\n",
      "Iter-4990, train loss-2.2247, acc-0.4200, valid loss-2.2274, acc-0.3164, test loss-2.2292, acc-0.3129\n",
      "Iter-5000, train loss-2.2374, acc-0.3000, valid loss-2.2273, acc-0.3164, test loss-2.2291, acc-0.3129\n",
      "Iter-5010, train loss-2.2176, acc-0.3400, valid loss-2.2272, acc-0.3162, test loss-2.2290, acc-0.3129\n",
      "Iter-5020, train loss-2.2418, acc-0.2600, valid loss-2.2271, acc-0.3166, test loss-2.2289, acc-0.3135\n",
      "Iter-5030, train loss-2.2184, acc-0.3000, valid loss-2.2270, acc-0.3170, test loss-2.2288, acc-0.3137\n",
      "Iter-5040, train loss-2.2248, acc-0.3400, valid loss-2.2269, acc-0.3170, test loss-2.2287, acc-0.3141\n",
      "Iter-5050, train loss-2.2251, acc-0.3200, valid loss-2.2268, acc-0.3168, test loss-2.2286, acc-0.3142\n",
      "Iter-5060, train loss-2.2532, acc-0.1600, valid loss-2.2267, acc-0.3172, test loss-2.2285, acc-0.3146\n",
      "Iter-5070, train loss-2.2044, acc-0.4400, valid loss-2.2266, acc-0.3176, test loss-2.2284, acc-0.3147\n",
      "Iter-5080, train loss-2.2275, acc-0.3800, valid loss-2.2265, acc-0.3174, test loss-2.2283, acc-0.3153\n",
      "Iter-5090, train loss-2.2303, acc-0.3000, valid loss-2.2264, acc-0.3174, test loss-2.2283, acc-0.3157\n",
      "Iter-5100, train loss-2.2367, acc-0.2400, valid loss-2.2263, acc-0.3186, test loss-2.2282, acc-0.3158\n",
      "Iter-5110, train loss-2.2274, acc-0.3600, valid loss-2.2262, acc-0.3190, test loss-2.2281, acc-0.3159\n",
      "Iter-5120, train loss-2.2267, acc-0.3400, valid loss-2.2261, acc-0.3190, test loss-2.2280, acc-0.3158\n",
      "Iter-5130, train loss-2.2189, acc-0.4000, valid loss-2.2260, acc-0.3190, test loss-2.2279, acc-0.3161\n",
      "Iter-5140, train loss-2.2342, acc-0.3000, valid loss-2.2259, acc-0.3192, test loss-2.2277, acc-0.3159\n",
      "Iter-5150, train loss-2.2059, acc-0.4200, valid loss-2.2258, acc-0.3196, test loss-2.2276, acc-0.3161\n",
      "Iter-5160, train loss-2.2269, acc-0.3000, valid loss-2.2257, acc-0.3198, test loss-2.2275, acc-0.3161\n",
      "Iter-5170, train loss-2.2253, acc-0.3000, valid loss-2.2256, acc-0.3200, test loss-2.2274, acc-0.3162\n",
      "Iter-5180, train loss-2.2321, acc-0.3000, valid loss-2.2255, acc-0.3198, test loss-2.2273, acc-0.3168\n",
      "Iter-5190, train loss-2.2211, acc-0.3400, valid loss-2.2254, acc-0.3200, test loss-2.2272, acc-0.3169\n",
      "Iter-5200, train loss-2.2203, acc-0.3200, valid loss-2.2253, acc-0.3206, test loss-2.2271, acc-0.3170\n",
      "Iter-5210, train loss-2.2275, acc-0.3400, valid loss-2.2252, acc-0.3210, test loss-2.2270, acc-0.3173\n",
      "Iter-5220, train loss-2.2219, acc-0.3400, valid loss-2.2250, acc-0.3212, test loss-2.2269, acc-0.3177\n",
      "Iter-5230, train loss-2.2064, acc-0.3800, valid loss-2.2249, acc-0.3210, test loss-2.2268, acc-0.3180\n",
      "Iter-5240, train loss-2.2651, acc-0.2800, valid loss-2.2248, acc-0.3210, test loss-2.2267, acc-0.3184\n",
      "Iter-5250, train loss-2.2179, acc-0.3000, valid loss-2.2247, acc-0.3212, test loss-2.2266, acc-0.3186\n",
      "Iter-5260, train loss-2.2269, acc-0.3400, valid loss-2.2246, acc-0.3214, test loss-2.2265, acc-0.3187\n",
      "Iter-5270, train loss-2.2253, acc-0.3000, valid loss-2.2245, acc-0.3216, test loss-2.2264, acc-0.3192\n",
      "Iter-5280, train loss-2.2217, acc-0.3200, valid loss-2.2244, acc-0.3216, test loss-2.2263, acc-0.3193\n",
      "Iter-5290, train loss-2.2152, acc-0.2600, valid loss-2.2243, acc-0.3220, test loss-2.2262, acc-0.3195\n",
      "Iter-5300, train loss-2.2285, acc-0.3600, valid loss-2.2242, acc-0.3224, test loss-2.2261, acc-0.3196\n",
      "Iter-5310, train loss-2.2464, acc-0.2400, valid loss-2.2241, acc-0.3224, test loss-2.2260, acc-0.3199\n",
      "Iter-5320, train loss-2.2050, acc-0.3800, valid loss-2.2240, acc-0.3232, test loss-2.2259, acc-0.3199\n",
      "Iter-5330, train loss-2.2327, acc-0.3000, valid loss-2.2239, acc-0.3234, test loss-2.2258, acc-0.3200\n",
      "Iter-5340, train loss-2.2081, acc-0.3600, valid loss-2.2238, acc-0.3238, test loss-2.2257, acc-0.3205\n",
      "Iter-5350, train loss-2.2406, acc-0.2000, valid loss-2.2237, acc-0.3248, test loss-2.2256, acc-0.3207\n",
      "Iter-5360, train loss-2.2247, acc-0.3000, valid loss-2.2236, acc-0.3254, test loss-2.2255, acc-0.3210\n",
      "Iter-5370, train loss-2.2302, acc-0.2600, valid loss-2.2235, acc-0.3246, test loss-2.2254, acc-0.3213\n",
      "Iter-5380, train loss-2.2355, acc-0.3200, valid loss-2.2234, acc-0.3246, test loss-2.2253, acc-0.3214\n",
      "Iter-5390, train loss-2.2102, acc-0.3800, valid loss-2.2233, acc-0.3246, test loss-2.2252, acc-0.3219\n",
      "Iter-5400, train loss-2.2499, acc-0.2400, valid loss-2.2232, acc-0.3250, test loss-2.2251, acc-0.3223\n",
      "Iter-5410, train loss-2.2450, acc-0.2800, valid loss-2.2231, acc-0.3252, test loss-2.2250, acc-0.3224\n",
      "Iter-5420, train loss-2.2230, acc-0.3800, valid loss-2.2230, acc-0.3256, test loss-2.2249, acc-0.3229\n",
      "Iter-5430, train loss-2.2197, acc-0.3400, valid loss-2.2229, acc-0.3260, test loss-2.2248, acc-0.3230\n",
      "Iter-5440, train loss-2.2240, acc-0.3600, valid loss-2.2228, acc-0.3266, test loss-2.2247, acc-0.3232\n",
      "Iter-5450, train loss-2.2554, acc-0.2600, valid loss-2.2227, acc-0.3270, test loss-2.2246, acc-0.3234\n",
      "Iter-5460, train loss-2.2240, acc-0.3000, valid loss-2.2226, acc-0.3272, test loss-2.2245, acc-0.3236\n",
      "Iter-5470, train loss-2.2177, acc-0.3000, valid loss-2.2225, acc-0.3272, test loss-2.2244, acc-0.3241\n",
      "Iter-5480, train loss-2.2419, acc-0.2600, valid loss-2.2224, acc-0.3270, test loss-2.2243, acc-0.3243\n",
      "Iter-5490, train loss-2.2229, acc-0.3600, valid loss-2.2223, acc-0.3276, test loss-2.2242, acc-0.3249\n",
      "Iter-5500, train loss-2.2473, acc-0.1600, valid loss-2.2222, acc-0.3278, test loss-2.2241, acc-0.3251\n",
      "Iter-5510, train loss-2.2120, acc-0.2600, valid loss-2.2221, acc-0.3278, test loss-2.2240, acc-0.3253\n",
      "Iter-5520, train loss-2.2294, acc-0.3800, valid loss-2.2220, acc-0.3284, test loss-2.2239, acc-0.3253\n",
      "Iter-5530, train loss-2.2237, acc-0.3000, valid loss-2.2219, acc-0.3290, test loss-2.2238, acc-0.3252\n",
      "Iter-5540, train loss-2.2069, acc-0.4000, valid loss-2.2218, acc-0.3290, test loss-2.2237, acc-0.3255\n",
      "Iter-5550, train loss-2.2105, acc-0.3000, valid loss-2.2217, acc-0.3288, test loss-2.2236, acc-0.3254\n",
      "Iter-5560, train loss-2.2322, acc-0.4000, valid loss-2.2216, acc-0.3292, test loss-2.2235, acc-0.3258\n",
      "Iter-5570, train loss-2.2254, acc-0.2600, valid loss-2.2215, acc-0.3294, test loss-2.2234, acc-0.3259\n",
      "Iter-5580, train loss-2.2308, acc-0.2600, valid loss-2.2214, acc-0.3294, test loss-2.2233, acc-0.3259\n",
      "Iter-5590, train loss-2.2436, acc-0.3000, valid loss-2.2213, acc-0.3296, test loss-2.2232, acc-0.3260\n",
      "Iter-5600, train loss-2.2463, acc-0.2000, valid loss-2.2212, acc-0.3300, test loss-2.2231, acc-0.3263\n",
      "Iter-5610, train loss-2.2412, acc-0.2800, valid loss-2.2211, acc-0.3302, test loss-2.2230, acc-0.3266\n",
      "Iter-5620, train loss-2.2444, acc-0.2400, valid loss-2.2210, acc-0.3304, test loss-2.2229, acc-0.3266\n",
      "Iter-5630, train loss-2.2343, acc-0.3400, valid loss-2.2209, acc-0.3306, test loss-2.2228, acc-0.3273\n",
      "Iter-5640, train loss-2.2196, acc-0.2800, valid loss-2.2208, acc-0.3308, test loss-2.2227, acc-0.3272\n",
      "Iter-5650, train loss-2.2190, acc-0.3600, valid loss-2.2207, acc-0.3308, test loss-2.2226, acc-0.3274\n",
      "Iter-5660, train loss-2.2399, acc-0.3800, valid loss-2.2206, acc-0.3308, test loss-2.2225, acc-0.3275\n",
      "Iter-5670, train loss-2.2127, acc-0.3000, valid loss-2.2205, acc-0.3318, test loss-2.2224, acc-0.3280\n",
      "Iter-5680, train loss-2.2162, acc-0.3400, valid loss-2.2204, acc-0.3320, test loss-2.2223, acc-0.3280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-2.2083, acc-0.4800, valid loss-2.2203, acc-0.3324, test loss-2.2222, acc-0.3286\n",
      "Iter-5700, train loss-2.2093, acc-0.3600, valid loss-2.2202, acc-0.3326, test loss-2.2221, acc-0.3289\n",
      "Iter-5710, train loss-2.2519, acc-0.2600, valid loss-2.2201, acc-0.3330, test loss-2.2220, acc-0.3293\n",
      "Iter-5720, train loss-2.2463, acc-0.2600, valid loss-2.2200, acc-0.3330, test loss-2.2219, acc-0.3290\n",
      "Iter-5730, train loss-2.2725, acc-0.2000, valid loss-2.2199, acc-0.3334, test loss-2.2218, acc-0.3296\n",
      "Iter-5740, train loss-2.2416, acc-0.2400, valid loss-2.2198, acc-0.3342, test loss-2.2217, acc-0.3296\n",
      "Iter-5750, train loss-2.2276, acc-0.3000, valid loss-2.2197, acc-0.3346, test loss-2.2216, acc-0.3299\n",
      "Iter-5760, train loss-2.2168, acc-0.3200, valid loss-2.2196, acc-0.3340, test loss-2.2215, acc-0.3298\n",
      "Iter-5770, train loss-2.2188, acc-0.3400, valid loss-2.2195, acc-0.3342, test loss-2.2214, acc-0.3301\n",
      "Iter-5780, train loss-2.2288, acc-0.2400, valid loss-2.2194, acc-0.3342, test loss-2.2213, acc-0.3302\n",
      "Iter-5790, train loss-2.2422, acc-0.3000, valid loss-2.2193, acc-0.3338, test loss-2.2212, acc-0.3303\n",
      "Iter-5800, train loss-2.2151, acc-0.3200, valid loss-2.2192, acc-0.3340, test loss-2.2211, acc-0.3304\n",
      "Iter-5810, train loss-2.2274, acc-0.2800, valid loss-2.2191, acc-0.3350, test loss-2.2210, acc-0.3310\n",
      "Iter-5820, train loss-2.2062, acc-0.4400, valid loss-2.2190, acc-0.3348, test loss-2.2209, acc-0.3314\n",
      "Iter-5830, train loss-2.2437, acc-0.2400, valid loss-2.2189, acc-0.3346, test loss-2.2208, acc-0.3318\n",
      "Iter-5840, train loss-2.1998, acc-0.4400, valid loss-2.2188, acc-0.3348, test loss-2.2207, acc-0.3318\n",
      "Iter-5850, train loss-2.2121, acc-0.3600, valid loss-2.2187, acc-0.3352, test loss-2.2206, acc-0.3322\n",
      "Iter-5860, train loss-2.2171, acc-0.3200, valid loss-2.2186, acc-0.3356, test loss-2.2205, acc-0.3322\n",
      "Iter-5870, train loss-2.2321, acc-0.3600, valid loss-2.2185, acc-0.3358, test loss-2.2204, acc-0.3325\n",
      "Iter-5880, train loss-2.2570, acc-0.1600, valid loss-2.2184, acc-0.3360, test loss-2.2203, acc-0.3325\n",
      "Iter-5890, train loss-2.2192, acc-0.3200, valid loss-2.2183, acc-0.3364, test loss-2.2202, acc-0.3328\n",
      "Iter-5900, train loss-2.2117, acc-0.4000, valid loss-2.2182, acc-0.3370, test loss-2.2201, acc-0.3331\n",
      "Iter-5910, train loss-2.2224, acc-0.3000, valid loss-2.2181, acc-0.3370, test loss-2.2200, acc-0.3332\n",
      "Iter-5920, train loss-2.2097, acc-0.4000, valid loss-2.2180, acc-0.3368, test loss-2.2199, acc-0.3334\n",
      "Iter-5930, train loss-2.2168, acc-0.3400, valid loss-2.2179, acc-0.3378, test loss-2.2198, acc-0.3339\n",
      "Iter-5940, train loss-2.2341, acc-0.3800, valid loss-2.2178, acc-0.3380, test loss-2.2197, acc-0.3341\n",
      "Iter-5950, train loss-2.2245, acc-0.2400, valid loss-2.2177, acc-0.3382, test loss-2.2196, acc-0.3348\n",
      "Iter-5960, train loss-2.2297, acc-0.2800, valid loss-2.2176, acc-0.3384, test loss-2.2195, acc-0.3351\n",
      "Iter-5970, train loss-2.2055, acc-0.4200, valid loss-2.2175, acc-0.3388, test loss-2.2194, acc-0.3355\n",
      "Iter-5980, train loss-2.2119, acc-0.3200, valid loss-2.2174, acc-0.3392, test loss-2.2193, acc-0.3357\n",
      "Iter-5990, train loss-2.2237, acc-0.3200, valid loss-2.2173, acc-0.3396, test loss-2.2192, acc-0.3360\n",
      "Iter-6000, train loss-2.2204, acc-0.3600, valid loss-2.2172, acc-0.3392, test loss-2.2191, acc-0.3361\n",
      "Iter-6010, train loss-2.2188, acc-0.3200, valid loss-2.2171, acc-0.3394, test loss-2.2190, acc-0.3366\n",
      "Iter-6020, train loss-2.2027, acc-0.4200, valid loss-2.2170, acc-0.3398, test loss-2.2189, acc-0.3367\n",
      "Iter-6030, train loss-2.2278, acc-0.2600, valid loss-2.2169, acc-0.3398, test loss-2.2188, acc-0.3369\n",
      "Iter-6040, train loss-2.2468, acc-0.2400, valid loss-2.2168, acc-0.3398, test loss-2.2188, acc-0.3369\n",
      "Iter-6050, train loss-2.2336, acc-0.2600, valid loss-2.2167, acc-0.3400, test loss-2.2187, acc-0.3373\n",
      "Iter-6060, train loss-2.2106, acc-0.3200, valid loss-2.2166, acc-0.3402, test loss-2.2186, acc-0.3377\n",
      "Iter-6070, train loss-2.2372, acc-0.1800, valid loss-2.2165, acc-0.3404, test loss-2.2185, acc-0.3380\n",
      "Iter-6080, train loss-2.2287, acc-0.2800, valid loss-2.2164, acc-0.3404, test loss-2.2184, acc-0.3386\n",
      "Iter-6090, train loss-2.2169, acc-0.4000, valid loss-2.2163, acc-0.3408, test loss-2.2183, acc-0.3387\n",
      "Iter-6100, train loss-2.2279, acc-0.3400, valid loss-2.2162, acc-0.3408, test loss-2.2182, acc-0.3392\n",
      "Iter-6110, train loss-2.2232, acc-0.3800, valid loss-2.2161, acc-0.3410, test loss-2.2181, acc-0.3395\n",
      "Iter-6120, train loss-2.2157, acc-0.3600, valid loss-2.2160, acc-0.3416, test loss-2.2180, acc-0.3392\n",
      "Iter-6130, train loss-2.2274, acc-0.3000, valid loss-2.2159, acc-0.3416, test loss-2.2179, acc-0.3401\n",
      "Iter-6140, train loss-2.2068, acc-0.3400, valid loss-2.2158, acc-0.3416, test loss-2.2178, acc-0.3402\n",
      "Iter-6150, train loss-2.1941, acc-0.4400, valid loss-2.2157, acc-0.3424, test loss-2.2177, acc-0.3409\n",
      "Iter-6160, train loss-2.1873, acc-0.5000, valid loss-2.2156, acc-0.3426, test loss-2.2176, acc-0.3412\n",
      "Iter-6170, train loss-2.2020, acc-0.4000, valid loss-2.2155, acc-0.3424, test loss-2.2175, acc-0.3416\n",
      "Iter-6180, train loss-2.2157, acc-0.3800, valid loss-2.2154, acc-0.3426, test loss-2.2174, acc-0.3420\n",
      "Iter-6190, train loss-2.2425, acc-0.3200, valid loss-2.2153, acc-0.3432, test loss-2.2172, acc-0.3425\n",
      "Iter-6200, train loss-2.2425, acc-0.2400, valid loss-2.2152, acc-0.3434, test loss-2.2171, acc-0.3430\n",
      "Iter-6210, train loss-2.2227, acc-0.2800, valid loss-2.2151, acc-0.3438, test loss-2.2170, acc-0.3434\n",
      "Iter-6220, train loss-2.2192, acc-0.2600, valid loss-2.2150, acc-0.3438, test loss-2.2169, acc-0.3439\n",
      "Iter-6230, train loss-2.1933, acc-0.4200, valid loss-2.2149, acc-0.3442, test loss-2.2168, acc-0.3438\n",
      "Iter-6240, train loss-2.2162, acc-0.3400, valid loss-2.2148, acc-0.3442, test loss-2.2167, acc-0.3440\n",
      "Iter-6250, train loss-2.1978, acc-0.4200, valid loss-2.2147, acc-0.3442, test loss-2.2166, acc-0.3442\n",
      "Iter-6260, train loss-2.2076, acc-0.3600, valid loss-2.2146, acc-0.3448, test loss-2.2165, acc-0.3445\n",
      "Iter-6270, train loss-2.2023, acc-0.3600, valid loss-2.2145, acc-0.3448, test loss-2.2164, acc-0.3449\n",
      "Iter-6280, train loss-2.2329, acc-0.2800, valid loss-2.2144, acc-0.3446, test loss-2.2164, acc-0.3452\n",
      "Iter-6290, train loss-2.2265, acc-0.2600, valid loss-2.2143, acc-0.3446, test loss-2.2163, acc-0.3452\n",
      "Iter-6300, train loss-2.2251, acc-0.3400, valid loss-2.2142, acc-0.3452, test loss-2.2162, acc-0.3449\n",
      "Iter-6310, train loss-2.2098, acc-0.3600, valid loss-2.2141, acc-0.3456, test loss-2.2161, acc-0.3452\n",
      "Iter-6320, train loss-2.2171, acc-0.3200, valid loss-2.2140, acc-0.3462, test loss-2.2160, acc-0.3453\n",
      "Iter-6330, train loss-2.2106, acc-0.4000, valid loss-2.2139, acc-0.3462, test loss-2.2159, acc-0.3454\n",
      "Iter-6340, train loss-2.1989, acc-0.3600, valid loss-2.2138, acc-0.3466, test loss-2.2158, acc-0.3456\n",
      "Iter-6350, train loss-2.2128, acc-0.3000, valid loss-2.2137, acc-0.3468, test loss-2.2157, acc-0.3457\n",
      "Iter-6360, train loss-2.2072, acc-0.3800, valid loss-2.2136, acc-0.3470, test loss-2.2156, acc-0.3458\n",
      "Iter-6370, train loss-2.2312, acc-0.2400, valid loss-2.2135, acc-0.3470, test loss-2.2155, acc-0.3464\n",
      "Iter-6380, train loss-2.2140, acc-0.2600, valid loss-2.2134, acc-0.3474, test loss-2.2154, acc-0.3469\n",
      "Iter-6390, train loss-2.2225, acc-0.3400, valid loss-2.2133, acc-0.3476, test loss-2.2153, acc-0.3471\n",
      "Iter-6400, train loss-2.1949, acc-0.4000, valid loss-2.2132, acc-0.3476, test loss-2.2152, acc-0.3473\n",
      "Iter-6410, train loss-2.2107, acc-0.2800, valid loss-2.2131, acc-0.3484, test loss-2.2151, acc-0.3477\n",
      "Iter-6420, train loss-2.2034, acc-0.3600, valid loss-2.2130, acc-0.3482, test loss-2.2150, acc-0.3479\n",
      "Iter-6430, train loss-2.2102, acc-0.4200, valid loss-2.2129, acc-0.3484, test loss-2.2149, acc-0.3482\n",
      "Iter-6440, train loss-2.2077, acc-0.4600, valid loss-2.2128, acc-0.3484, test loss-2.2148, acc-0.3484\n",
      "Iter-6450, train loss-2.2269, acc-0.3800, valid loss-2.2127, acc-0.3484, test loss-2.2147, acc-0.3484\n",
      "Iter-6460, train loss-2.2298, acc-0.3000, valid loss-2.2126, acc-0.3484, test loss-2.2146, acc-0.3486\n",
      "Iter-6470, train loss-2.2304, acc-0.2400, valid loss-2.2125, acc-0.3488, test loss-2.2145, acc-0.3488\n",
      "Iter-6480, train loss-2.1963, acc-0.5000, valid loss-2.2124, acc-0.3490, test loss-2.2144, acc-0.3488\n",
      "Iter-6490, train loss-2.2057, acc-0.3200, valid loss-2.2123, acc-0.3490, test loss-2.2143, acc-0.3491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-2.2142, acc-0.3600, valid loss-2.2122, acc-0.3492, test loss-2.2142, acc-0.3490\n",
      "Iter-6510, train loss-2.2130, acc-0.3200, valid loss-2.2121, acc-0.3504, test loss-2.2141, acc-0.3492\n",
      "Iter-6520, train loss-2.2155, acc-0.3000, valid loss-2.2120, acc-0.3504, test loss-2.2140, acc-0.3495\n",
      "Iter-6530, train loss-2.2105, acc-0.3400, valid loss-2.2119, acc-0.3512, test loss-2.2139, acc-0.3495\n",
      "Iter-6540, train loss-2.2285, acc-0.3600, valid loss-2.2118, acc-0.3514, test loss-2.2138, acc-0.3496\n",
      "Iter-6550, train loss-2.1904, acc-0.5200, valid loss-2.2117, acc-0.3514, test loss-2.2137, acc-0.3497\n",
      "Iter-6560, train loss-2.2530, acc-0.2400, valid loss-2.2116, acc-0.3518, test loss-2.2136, acc-0.3500\n",
      "Iter-6570, train loss-2.2289, acc-0.2600, valid loss-2.2115, acc-0.3526, test loss-2.2135, acc-0.3501\n",
      "Iter-6580, train loss-2.2154, acc-0.2600, valid loss-2.2114, acc-0.3532, test loss-2.2134, acc-0.3502\n",
      "Iter-6590, train loss-2.2323, acc-0.2800, valid loss-2.2113, acc-0.3536, test loss-2.2133, acc-0.3504\n",
      "Iter-6600, train loss-2.2088, acc-0.4400, valid loss-2.2112, acc-0.3544, test loss-2.2132, acc-0.3505\n",
      "Iter-6610, train loss-2.2044, acc-0.3000, valid loss-2.2111, acc-0.3548, test loss-2.2131, acc-0.3508\n",
      "Iter-6620, train loss-2.2124, acc-0.3600, valid loss-2.2110, acc-0.3554, test loss-2.2130, acc-0.3512\n",
      "Iter-6630, train loss-2.2068, acc-0.3000, valid loss-2.2109, acc-0.3562, test loss-2.2129, acc-0.3511\n",
      "Iter-6640, train loss-2.1819, acc-0.4000, valid loss-2.2108, acc-0.3562, test loss-2.2128, acc-0.3514\n",
      "Iter-6650, train loss-2.1879, acc-0.5600, valid loss-2.2107, acc-0.3568, test loss-2.2127, acc-0.3517\n",
      "Iter-6660, train loss-2.1919, acc-0.3800, valid loss-2.2106, acc-0.3574, test loss-2.2126, acc-0.3522\n",
      "Iter-6670, train loss-2.2184, acc-0.2400, valid loss-2.2105, acc-0.3580, test loss-2.2125, acc-0.3525\n",
      "Iter-6680, train loss-2.1962, acc-0.4200, valid loss-2.2104, acc-0.3584, test loss-2.2124, acc-0.3524\n",
      "Iter-6690, train loss-2.2375, acc-0.3200, valid loss-2.2103, acc-0.3580, test loss-2.2123, acc-0.3527\n",
      "Iter-6700, train loss-2.2166, acc-0.2400, valid loss-2.2102, acc-0.3588, test loss-2.2122, acc-0.3527\n",
      "Iter-6710, train loss-2.2236, acc-0.3200, valid loss-2.2101, acc-0.3586, test loss-2.2121, acc-0.3532\n",
      "Iter-6720, train loss-2.2149, acc-0.3800, valid loss-2.2100, acc-0.3584, test loss-2.2120, acc-0.3535\n",
      "Iter-6730, train loss-2.2188, acc-0.3400, valid loss-2.2099, acc-0.3586, test loss-2.2119, acc-0.3538\n",
      "Iter-6740, train loss-2.2164, acc-0.3800, valid loss-2.2098, acc-0.3590, test loss-2.2118, acc-0.3539\n",
      "Iter-6750, train loss-2.2052, acc-0.4000, valid loss-2.2097, acc-0.3590, test loss-2.2117, acc-0.3544\n",
      "Iter-6760, train loss-2.2204, acc-0.3400, valid loss-2.2096, acc-0.3592, test loss-2.2116, acc-0.3546\n",
      "Iter-6770, train loss-2.1974, acc-0.4200, valid loss-2.2095, acc-0.3596, test loss-2.2115, acc-0.3545\n",
      "Iter-6780, train loss-2.1999, acc-0.4200, valid loss-2.2094, acc-0.3598, test loss-2.2114, acc-0.3545\n",
      "Iter-6790, train loss-2.1886, acc-0.3200, valid loss-2.2093, acc-0.3600, test loss-2.2113, acc-0.3548\n",
      "Iter-6800, train loss-2.2127, acc-0.4000, valid loss-2.2092, acc-0.3608, test loss-2.2112, acc-0.3551\n",
      "Iter-6810, train loss-2.2160, acc-0.3800, valid loss-2.2091, acc-0.3606, test loss-2.2111, acc-0.3552\n",
      "Iter-6820, train loss-2.2226, acc-0.3600, valid loss-2.2090, acc-0.3604, test loss-2.2110, acc-0.3556\n",
      "Iter-6830, train loss-2.2065, acc-0.3000, valid loss-2.2089, acc-0.3608, test loss-2.2109, acc-0.3558\n",
      "Iter-6840, train loss-2.2059, acc-0.3800, valid loss-2.2088, acc-0.3608, test loss-2.2108, acc-0.3561\n",
      "Iter-6850, train loss-2.2043, acc-0.4000, valid loss-2.2087, acc-0.3608, test loss-2.2107, acc-0.3564\n",
      "Iter-6860, train loss-2.2121, acc-0.4000, valid loss-2.2086, acc-0.3612, test loss-2.2106, acc-0.3564\n",
      "Iter-6870, train loss-2.2128, acc-0.3600, valid loss-2.2085, acc-0.3620, test loss-2.2105, acc-0.3567\n",
      "Iter-6880, train loss-2.2178, acc-0.4000, valid loss-2.2084, acc-0.3628, test loss-2.2104, acc-0.3572\n",
      "Iter-6890, train loss-2.2318, acc-0.2000, valid loss-2.2083, acc-0.3630, test loss-2.2103, acc-0.3576\n",
      "Iter-6900, train loss-2.2217, acc-0.3200, valid loss-2.2082, acc-0.3632, test loss-2.2102, acc-0.3574\n",
      "Iter-6910, train loss-2.2254, acc-0.3000, valid loss-2.2081, acc-0.3636, test loss-2.2101, acc-0.3575\n",
      "Iter-6920, train loss-2.2359, acc-0.2400, valid loss-2.2080, acc-0.3638, test loss-2.2101, acc-0.3577\n",
      "Iter-6930, train loss-2.2212, acc-0.3800, valid loss-2.2079, acc-0.3640, test loss-2.2100, acc-0.3576\n",
      "Iter-6940, train loss-2.2299, acc-0.3200, valid loss-2.2078, acc-0.3638, test loss-2.2099, acc-0.3578\n",
      "Iter-6950, train loss-2.2078, acc-0.3400, valid loss-2.2077, acc-0.3646, test loss-2.2098, acc-0.3578\n",
      "Iter-6960, train loss-2.1821, acc-0.3200, valid loss-2.2076, acc-0.3652, test loss-2.2097, acc-0.3579\n",
      "Iter-6970, train loss-2.2221, acc-0.3800, valid loss-2.2075, acc-0.3650, test loss-2.2096, acc-0.3586\n",
      "Iter-6980, train loss-2.2053, acc-0.4400, valid loss-2.2074, acc-0.3650, test loss-2.2095, acc-0.3586\n",
      "Iter-6990, train loss-2.2171, acc-0.3400, valid loss-2.2073, acc-0.3658, test loss-2.2094, acc-0.3590\n",
      "Iter-7000, train loss-2.2163, acc-0.3600, valid loss-2.2072, acc-0.3662, test loss-2.2093, acc-0.3593\n",
      "Iter-7010, train loss-2.2164, acc-0.3200, valid loss-2.2071, acc-0.3666, test loss-2.2092, acc-0.3597\n",
      "Iter-7020, train loss-2.2282, acc-0.2800, valid loss-2.2070, acc-0.3668, test loss-2.2091, acc-0.3599\n",
      "Iter-7030, train loss-2.2171, acc-0.2600, valid loss-2.2069, acc-0.3668, test loss-2.2090, acc-0.3604\n",
      "Iter-7040, train loss-2.2114, acc-0.3200, valid loss-2.2068, acc-0.3670, test loss-2.2089, acc-0.3606\n",
      "Iter-7050, train loss-2.2076, acc-0.3800, valid loss-2.2067, acc-0.3674, test loss-2.2088, acc-0.3608\n",
      "Iter-7060, train loss-2.2050, acc-0.3400, valid loss-2.2066, acc-0.3674, test loss-2.2087, acc-0.3608\n",
      "Iter-7070, train loss-2.1822, acc-0.4400, valid loss-2.2065, acc-0.3674, test loss-2.2086, acc-0.3612\n",
      "Iter-7080, train loss-2.2152, acc-0.3200, valid loss-2.2064, acc-0.3676, test loss-2.2085, acc-0.3612\n",
      "Iter-7090, train loss-2.2192, acc-0.3600, valid loss-2.2063, acc-0.3674, test loss-2.2084, acc-0.3619\n",
      "Iter-7100, train loss-2.2491, acc-0.2000, valid loss-2.2063, acc-0.3682, test loss-2.2083, acc-0.3619\n",
      "Iter-7110, train loss-2.2270, acc-0.2400, valid loss-2.2062, acc-0.3692, test loss-2.2082, acc-0.3619\n",
      "Iter-7120, train loss-2.2177, acc-0.3200, valid loss-2.2061, acc-0.3696, test loss-2.2081, acc-0.3623\n",
      "Iter-7130, train loss-2.2034, acc-0.3600, valid loss-2.2060, acc-0.3692, test loss-2.2080, acc-0.3622\n",
      "Iter-7140, train loss-2.2174, acc-0.3800, valid loss-2.2059, acc-0.3694, test loss-2.2079, acc-0.3626\n",
      "Iter-7150, train loss-2.2414, acc-0.2800, valid loss-2.2058, acc-0.3694, test loss-2.2078, acc-0.3628\n",
      "Iter-7160, train loss-2.2234, acc-0.3400, valid loss-2.2057, acc-0.3698, test loss-2.2077, acc-0.3630\n",
      "Iter-7170, train loss-2.2079, acc-0.4600, valid loss-2.2056, acc-0.3700, test loss-2.2076, acc-0.3630\n",
      "Iter-7180, train loss-2.1893, acc-0.3800, valid loss-2.2055, acc-0.3704, test loss-2.2075, acc-0.3635\n",
      "Iter-7190, train loss-2.1892, acc-0.5600, valid loss-2.2054, acc-0.3702, test loss-2.2074, acc-0.3636\n",
      "Iter-7200, train loss-2.1978, acc-0.3800, valid loss-2.2053, acc-0.3704, test loss-2.2073, acc-0.3638\n",
      "Iter-7210, train loss-2.2316, acc-0.3200, valid loss-2.2052, acc-0.3708, test loss-2.2072, acc-0.3640\n",
      "Iter-7220, train loss-2.2211, acc-0.2800, valid loss-2.2051, acc-0.3710, test loss-2.2071, acc-0.3646\n",
      "Iter-7230, train loss-2.2007, acc-0.3600, valid loss-2.2050, acc-0.3708, test loss-2.2070, acc-0.3645\n",
      "Iter-7240, train loss-2.2224, acc-0.2400, valid loss-2.2049, acc-0.3708, test loss-2.2069, acc-0.3646\n",
      "Iter-7250, train loss-2.2229, acc-0.3000, valid loss-2.2048, acc-0.3716, test loss-2.2069, acc-0.3647\n",
      "Iter-7260, train loss-2.1917, acc-0.5000, valid loss-2.2047, acc-0.3724, test loss-2.2067, acc-0.3651\n",
      "Iter-7270, train loss-2.2068, acc-0.3000, valid loss-2.2046, acc-0.3728, test loss-2.2066, acc-0.3652\n",
      "Iter-7280, train loss-2.2249, acc-0.3200, valid loss-2.2045, acc-0.3730, test loss-2.2066, acc-0.3655\n",
      "Iter-7290, train loss-2.2024, acc-0.4000, valid loss-2.2044, acc-0.3734, test loss-2.2065, acc-0.3656\n",
      "Iter-7300, train loss-2.2314, acc-0.3600, valid loss-2.2043, acc-0.3736, test loss-2.2064, acc-0.3660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-2.2205, acc-0.2800, valid loss-2.2042, acc-0.3742, test loss-2.2063, acc-0.3660\n",
      "Iter-7320, train loss-2.1751, acc-0.4600, valid loss-2.2041, acc-0.3740, test loss-2.2061, acc-0.3666\n",
      "Iter-7330, train loss-2.2255, acc-0.3800, valid loss-2.2040, acc-0.3742, test loss-2.2061, acc-0.3667\n",
      "Iter-7340, train loss-2.1956, acc-0.3800, valid loss-2.2039, acc-0.3740, test loss-2.2060, acc-0.3668\n",
      "Iter-7350, train loss-2.2011, acc-0.3800, valid loss-2.2038, acc-0.3744, test loss-2.2059, acc-0.3671\n",
      "Iter-7360, train loss-2.1949, acc-0.4800, valid loss-2.2037, acc-0.3744, test loss-2.2058, acc-0.3671\n",
      "Iter-7370, train loss-2.1862, acc-0.3600, valid loss-2.2036, acc-0.3754, test loss-2.2057, acc-0.3673\n",
      "Iter-7380, train loss-2.2124, acc-0.3600, valid loss-2.2035, acc-0.3752, test loss-2.2056, acc-0.3674\n",
      "Iter-7390, train loss-2.1851, acc-0.4600, valid loss-2.2034, acc-0.3756, test loss-2.2055, acc-0.3676\n",
      "Iter-7400, train loss-2.1907, acc-0.4800, valid loss-2.2033, acc-0.3758, test loss-2.2054, acc-0.3680\n",
      "Iter-7410, train loss-2.1948, acc-0.4000, valid loss-2.2032, acc-0.3764, test loss-2.2053, acc-0.3681\n",
      "Iter-7420, train loss-2.2110, acc-0.4400, valid loss-2.2031, acc-0.3768, test loss-2.2052, acc-0.3687\n",
      "Iter-7430, train loss-2.2116, acc-0.4000, valid loss-2.2030, acc-0.3770, test loss-2.2051, acc-0.3685\n",
      "Iter-7440, train loss-2.1785, acc-0.5000, valid loss-2.2029, acc-0.3768, test loss-2.2050, acc-0.3683\n",
      "Iter-7450, train loss-2.2077, acc-0.3600, valid loss-2.2028, acc-0.3770, test loss-2.2049, acc-0.3686\n",
      "Iter-7460, train loss-2.2394, acc-0.2600, valid loss-2.2027, acc-0.3770, test loss-2.2048, acc-0.3686\n",
      "Iter-7470, train loss-2.2193, acc-0.3200, valid loss-2.2026, acc-0.3770, test loss-2.2047, acc-0.3690\n",
      "Iter-7480, train loss-2.2173, acc-0.3600, valid loss-2.2025, acc-0.3768, test loss-2.2046, acc-0.3692\n",
      "Iter-7490, train loss-2.2080, acc-0.3200, valid loss-2.2024, acc-0.3772, test loss-2.2045, acc-0.3692\n",
      "Iter-7500, train loss-2.1992, acc-0.4400, valid loss-2.2023, acc-0.3776, test loss-2.2044, acc-0.3692\n",
      "Iter-7510, train loss-2.2032, acc-0.3800, valid loss-2.2022, acc-0.3778, test loss-2.2043, acc-0.3700\n",
      "Iter-7520, train loss-2.1912, acc-0.4600, valid loss-2.2021, acc-0.3776, test loss-2.2042, acc-0.3706\n",
      "Iter-7530, train loss-2.1993, acc-0.4000, valid loss-2.2020, acc-0.3776, test loss-2.2041, acc-0.3709\n",
      "Iter-7540, train loss-2.1905, acc-0.4600, valid loss-2.2019, acc-0.3778, test loss-2.2040, acc-0.3708\n",
      "Iter-7550, train loss-2.2166, acc-0.3400, valid loss-2.2018, acc-0.3774, test loss-2.2039, acc-0.3711\n",
      "Iter-7560, train loss-2.2150, acc-0.3200, valid loss-2.2017, acc-0.3776, test loss-2.2038, acc-0.3717\n",
      "Iter-7570, train loss-2.1739, acc-0.4600, valid loss-2.2016, acc-0.3778, test loss-2.2037, acc-0.3718\n",
      "Iter-7580, train loss-2.2304, acc-0.2800, valid loss-2.2015, acc-0.3782, test loss-2.2036, acc-0.3718\n",
      "Iter-7590, train loss-2.2179, acc-0.3200, valid loss-2.2014, acc-0.3784, test loss-2.2035, acc-0.3724\n",
      "Iter-7600, train loss-2.2022, acc-0.3600, valid loss-2.2013, acc-0.3786, test loss-2.2034, acc-0.3729\n",
      "Iter-7610, train loss-2.2008, acc-0.3600, valid loss-2.2012, acc-0.3786, test loss-2.2033, acc-0.3726\n",
      "Iter-7620, train loss-2.1832, acc-0.4400, valid loss-2.2012, acc-0.3784, test loss-2.2032, acc-0.3726\n",
      "Iter-7630, train loss-2.2154, acc-0.3400, valid loss-2.2011, acc-0.3790, test loss-2.2031, acc-0.3730\n",
      "Iter-7640, train loss-2.2018, acc-0.3400, valid loss-2.2010, acc-0.3788, test loss-2.2030, acc-0.3731\n",
      "Iter-7650, train loss-2.1783, acc-0.4400, valid loss-2.2009, acc-0.3794, test loss-2.2029, acc-0.3730\n",
      "Iter-7660, train loss-2.1939, acc-0.3400, valid loss-2.2008, acc-0.3796, test loss-2.2028, acc-0.3737\n",
      "Iter-7670, train loss-2.1738, acc-0.4000, valid loss-2.2007, acc-0.3802, test loss-2.2027, acc-0.3732\n",
      "Iter-7680, train loss-2.2173, acc-0.3200, valid loss-2.2006, acc-0.3806, test loss-2.2026, acc-0.3730\n",
      "Iter-7690, train loss-2.1955, acc-0.4000, valid loss-2.2005, acc-0.3814, test loss-2.2026, acc-0.3730\n",
      "Iter-7700, train loss-2.1975, acc-0.3800, valid loss-2.2004, acc-0.3810, test loss-2.2025, acc-0.3729\n",
      "Iter-7710, train loss-2.1967, acc-0.3000, valid loss-2.2003, acc-0.3820, test loss-2.2024, acc-0.3728\n",
      "Iter-7720, train loss-2.2043, acc-0.3200, valid loss-2.2002, acc-0.3822, test loss-2.2023, acc-0.3728\n",
      "Iter-7730, train loss-2.2240, acc-0.3400, valid loss-2.2001, acc-0.3822, test loss-2.2022, acc-0.3729\n",
      "Iter-7740, train loss-2.1810, acc-0.4600, valid loss-2.2000, acc-0.3826, test loss-2.2021, acc-0.3728\n",
      "Iter-7750, train loss-2.2144, acc-0.3200, valid loss-2.1999, acc-0.3830, test loss-2.2020, acc-0.3730\n",
      "Iter-7760, train loss-2.1951, acc-0.3600, valid loss-2.1998, acc-0.3834, test loss-2.2019, acc-0.3730\n",
      "Iter-7770, train loss-2.1993, acc-0.3800, valid loss-2.1997, acc-0.3836, test loss-2.2018, acc-0.3732\n",
      "Iter-7780, train loss-2.2218, acc-0.3000, valid loss-2.1996, acc-0.3840, test loss-2.2017, acc-0.3739\n",
      "Iter-7790, train loss-2.2065, acc-0.3600, valid loss-2.1995, acc-0.3840, test loss-2.2016, acc-0.3738\n",
      "Iter-7800, train loss-2.2059, acc-0.3200, valid loss-2.1994, acc-0.3844, test loss-2.2015, acc-0.3742\n",
      "Iter-7810, train loss-2.1992, acc-0.4600, valid loss-2.1993, acc-0.3854, test loss-2.2014, acc-0.3747\n",
      "Iter-7820, train loss-2.2259, acc-0.2800, valid loss-2.1992, acc-0.3854, test loss-2.2013, acc-0.3751\n",
      "Iter-7830, train loss-2.1916, acc-0.3400, valid loss-2.1991, acc-0.3852, test loss-2.2012, acc-0.3754\n",
      "Iter-7840, train loss-2.1579, acc-0.4000, valid loss-2.1990, acc-0.3852, test loss-2.2011, acc-0.3755\n",
      "Iter-7850, train loss-2.1964, acc-0.4200, valid loss-2.1989, acc-0.3852, test loss-2.2010, acc-0.3756\n",
      "Iter-7860, train loss-2.1949, acc-0.3600, valid loss-2.1988, acc-0.3858, test loss-2.2009, acc-0.3757\n",
      "Iter-7870, train loss-2.1826, acc-0.5000, valid loss-2.1987, acc-0.3862, test loss-2.2008, acc-0.3759\n",
      "Iter-7880, train loss-2.1987, acc-0.3400, valid loss-2.1986, acc-0.3860, test loss-2.2007, acc-0.3761\n",
      "Iter-7890, train loss-2.1883, acc-0.5000, valid loss-2.1985, acc-0.3862, test loss-2.2006, acc-0.3760\n",
      "Iter-7900, train loss-2.1935, acc-0.4800, valid loss-2.1984, acc-0.3862, test loss-2.2005, acc-0.3767\n",
      "Iter-7910, train loss-2.2169, acc-0.2800, valid loss-2.1983, acc-0.3870, test loss-2.2004, acc-0.3769\n",
      "Iter-7920, train loss-2.1776, acc-0.4200, valid loss-2.1982, acc-0.3864, test loss-2.2003, acc-0.3771\n",
      "Iter-7930, train loss-2.1864, acc-0.4000, valid loss-2.1981, acc-0.3872, test loss-2.2002, acc-0.3779\n",
      "Iter-7940, train loss-2.1752, acc-0.4600, valid loss-2.1980, acc-0.3874, test loss-2.2001, acc-0.3781\n",
      "Iter-7950, train loss-2.2029, acc-0.3400, valid loss-2.1979, acc-0.3872, test loss-2.2000, acc-0.3782\n",
      "Iter-7960, train loss-2.2294, acc-0.2800, valid loss-2.1978, acc-0.3870, test loss-2.1999, acc-0.3782\n",
      "Iter-7970, train loss-2.2113, acc-0.4400, valid loss-2.1977, acc-0.3874, test loss-2.1998, acc-0.3782\n",
      "Iter-7980, train loss-2.2173, acc-0.3400, valid loss-2.1977, acc-0.3874, test loss-2.1997, acc-0.3783\n",
      "Iter-7990, train loss-2.2340, acc-0.2000, valid loss-2.1976, acc-0.3876, test loss-2.1997, acc-0.3784\n",
      "Iter-8000, train loss-2.2359, acc-0.2400, valid loss-2.1975, acc-0.3882, test loss-2.1996, acc-0.3783\n",
      "Iter-8010, train loss-2.1994, acc-0.4600, valid loss-2.1974, acc-0.3880, test loss-2.1995, acc-0.3785\n",
      "Iter-8020, train loss-2.2144, acc-0.4200, valid loss-2.1973, acc-0.3878, test loss-2.1994, acc-0.3787\n",
      "Iter-8030, train loss-2.1925, acc-0.3400, valid loss-2.1972, acc-0.3880, test loss-2.1993, acc-0.3791\n",
      "Iter-8040, train loss-2.1893, acc-0.3800, valid loss-2.1971, acc-0.3888, test loss-2.1992, acc-0.3788\n",
      "Iter-8050, train loss-2.1932, acc-0.4400, valid loss-2.1970, acc-0.3888, test loss-2.1991, acc-0.3788\n",
      "Iter-8060, train loss-2.2065, acc-0.3400, valid loss-2.1969, acc-0.3888, test loss-2.1990, acc-0.3790\n",
      "Iter-8070, train loss-2.2009, acc-0.3400, valid loss-2.1968, acc-0.3890, test loss-2.1989, acc-0.3792\n",
      "Iter-8080, train loss-2.1911, acc-0.4400, valid loss-2.1967, acc-0.3894, test loss-2.1988, acc-0.3792\n",
      "Iter-8090, train loss-2.2330, acc-0.3200, valid loss-2.1966, acc-0.3890, test loss-2.1987, acc-0.3796\n",
      "Iter-8100, train loss-2.2338, acc-0.2800, valid loss-2.1965, acc-0.3894, test loss-2.1986, acc-0.3799\n",
      "Iter-8110, train loss-2.1952, acc-0.3600, valid loss-2.1964, acc-0.3892, test loss-2.1985, acc-0.3798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-2.2256, acc-0.3400, valid loss-2.1963, acc-0.3892, test loss-2.1984, acc-0.3806\n",
      "Iter-8130, train loss-2.1874, acc-0.3600, valid loss-2.1962, acc-0.3894, test loss-2.1983, acc-0.3808\n",
      "Iter-8140, train loss-2.1898, acc-0.4000, valid loss-2.1961, acc-0.3894, test loss-2.1982, acc-0.3807\n",
      "Iter-8150, train loss-2.1811, acc-0.4800, valid loss-2.1960, acc-0.3890, test loss-2.1981, acc-0.3810\n",
      "Iter-8160, train loss-2.1941, acc-0.4000, valid loss-2.1959, acc-0.3890, test loss-2.1980, acc-0.3809\n",
      "Iter-8170, train loss-2.1742, acc-0.4400, valid loss-2.1958, acc-0.3890, test loss-2.1979, acc-0.3809\n",
      "Iter-8180, train loss-2.2188, acc-0.3200, valid loss-2.1957, acc-0.3892, test loss-2.1978, acc-0.3812\n",
      "Iter-8190, train loss-2.2079, acc-0.3600, valid loss-2.1956, acc-0.3892, test loss-2.1977, acc-0.3815\n",
      "Iter-8200, train loss-2.2226, acc-0.2800, valid loss-2.1955, acc-0.3896, test loss-2.1976, acc-0.3817\n",
      "Iter-8210, train loss-2.1877, acc-0.4200, valid loss-2.1954, acc-0.3894, test loss-2.1975, acc-0.3815\n",
      "Iter-8220, train loss-2.2056, acc-0.3200, valid loss-2.1953, acc-0.3900, test loss-2.1975, acc-0.3817\n",
      "Iter-8230, train loss-2.2091, acc-0.4400, valid loss-2.1952, acc-0.3902, test loss-2.1974, acc-0.3819\n",
      "Iter-8240, train loss-2.2137, acc-0.3400, valid loss-2.1951, acc-0.3902, test loss-2.1973, acc-0.3829\n",
      "Iter-8250, train loss-2.1926, acc-0.4800, valid loss-2.1950, acc-0.3906, test loss-2.1972, acc-0.3831\n",
      "Iter-8260, train loss-2.2229, acc-0.3200, valid loss-2.1949, acc-0.3912, test loss-2.1971, acc-0.3835\n",
      "Iter-8270, train loss-2.1827, acc-0.4000, valid loss-2.1948, acc-0.3912, test loss-2.1970, acc-0.3837\n",
      "Iter-8280, train loss-2.1887, acc-0.5200, valid loss-2.1947, acc-0.3912, test loss-2.1969, acc-0.3843\n",
      "Iter-8290, train loss-2.1928, acc-0.3800, valid loss-2.1947, acc-0.3914, test loss-2.1968, acc-0.3843\n",
      "Iter-8300, train loss-2.2177, acc-0.2800, valid loss-2.1946, acc-0.3920, test loss-2.1967, acc-0.3846\n",
      "Iter-8310, train loss-2.2104, acc-0.2600, valid loss-2.1945, acc-0.3918, test loss-2.1966, acc-0.3847\n",
      "Iter-8320, train loss-2.2107, acc-0.3600, valid loss-2.1944, acc-0.3914, test loss-2.1965, acc-0.3848\n",
      "Iter-8330, train loss-2.1699, acc-0.4200, valid loss-2.1943, acc-0.3918, test loss-2.1964, acc-0.3852\n",
      "Iter-8340, train loss-2.1817, acc-0.4000, valid loss-2.1942, acc-0.3920, test loss-2.1963, acc-0.3858\n",
      "Iter-8350, train loss-2.1583, acc-0.5200, valid loss-2.1941, acc-0.3926, test loss-2.1962, acc-0.3859\n",
      "Iter-8360, train loss-2.1957, acc-0.4200, valid loss-2.1940, acc-0.3926, test loss-2.1961, acc-0.3857\n",
      "Iter-8370, train loss-2.1893, acc-0.4200, valid loss-2.1939, acc-0.3924, test loss-2.1960, acc-0.3860\n",
      "Iter-8380, train loss-2.2033, acc-0.4200, valid loss-2.1938, acc-0.3930, test loss-2.1959, acc-0.3864\n",
      "Iter-8390, train loss-2.1804, acc-0.4400, valid loss-2.1937, acc-0.3930, test loss-2.1958, acc-0.3863\n",
      "Iter-8400, train loss-2.2041, acc-0.4000, valid loss-2.1936, acc-0.3938, test loss-2.1957, acc-0.3867\n",
      "Iter-8410, train loss-2.2136, acc-0.3400, valid loss-2.1935, acc-0.3942, test loss-2.1956, acc-0.3867\n",
      "Iter-8420, train loss-2.1849, acc-0.4600, valid loss-2.1934, acc-0.3944, test loss-2.1955, acc-0.3869\n",
      "Iter-8430, train loss-2.2199, acc-0.3400, valid loss-2.1933, acc-0.3946, test loss-2.1954, acc-0.3866\n",
      "Iter-8440, train loss-2.2230, acc-0.3000, valid loss-2.1932, acc-0.3954, test loss-2.1953, acc-0.3868\n",
      "Iter-8450, train loss-2.2063, acc-0.2800, valid loss-2.1931, acc-0.3954, test loss-2.1952, acc-0.3867\n",
      "Iter-8460, train loss-2.2277, acc-0.2800, valid loss-2.1930, acc-0.3960, test loss-2.1952, acc-0.3869\n",
      "Iter-8470, train loss-2.1912, acc-0.3600, valid loss-2.1929, acc-0.3962, test loss-2.1951, acc-0.3872\n",
      "Iter-8480, train loss-2.1837, acc-0.3600, valid loss-2.1928, acc-0.3964, test loss-2.1950, acc-0.3877\n",
      "Iter-8490, train loss-2.1853, acc-0.4200, valid loss-2.1927, acc-0.3964, test loss-2.1949, acc-0.3877\n",
      "Iter-8500, train loss-2.2172, acc-0.3600, valid loss-2.1926, acc-0.3962, test loss-2.1948, acc-0.3876\n",
      "Iter-8510, train loss-2.1864, acc-0.3600, valid loss-2.1925, acc-0.3962, test loss-2.1947, acc-0.3879\n",
      "Iter-8520, train loss-2.1706, acc-0.4400, valid loss-2.1924, acc-0.3968, test loss-2.1946, acc-0.3879\n",
      "Iter-8530, train loss-2.1951, acc-0.4000, valid loss-2.1923, acc-0.3974, test loss-2.1945, acc-0.3882\n",
      "Iter-8540, train loss-2.1934, acc-0.4800, valid loss-2.1922, acc-0.3976, test loss-2.1944, acc-0.3882\n",
      "Iter-8550, train loss-2.2180, acc-0.2800, valid loss-2.1921, acc-0.3974, test loss-2.1943, acc-0.3883\n",
      "Iter-8560, train loss-2.2256, acc-0.4400, valid loss-2.1920, acc-0.3976, test loss-2.1942, acc-0.3884\n",
      "Iter-8570, train loss-2.1912, acc-0.3800, valid loss-2.1919, acc-0.3974, test loss-2.1941, acc-0.3886\n",
      "Iter-8580, train loss-2.2117, acc-0.3000, valid loss-2.1918, acc-0.3980, test loss-2.1940, acc-0.3889\n",
      "Iter-8590, train loss-2.1866, acc-0.4400, valid loss-2.1917, acc-0.3980, test loss-2.1939, acc-0.3889\n",
      "Iter-8600, train loss-2.1813, acc-0.4200, valid loss-2.1916, acc-0.3980, test loss-2.1938, acc-0.3890\n",
      "Iter-8610, train loss-2.1755, acc-0.3800, valid loss-2.1916, acc-0.3982, test loss-2.1937, acc-0.3892\n",
      "Iter-8620, train loss-2.2238, acc-0.3800, valid loss-2.1915, acc-0.3982, test loss-2.1936, acc-0.3896\n",
      "Iter-8630, train loss-2.1807, acc-0.4800, valid loss-2.1914, acc-0.3984, test loss-2.1935, acc-0.3897\n",
      "Iter-8640, train loss-2.1960, acc-0.3800, valid loss-2.1913, acc-0.3988, test loss-2.1934, acc-0.3900\n",
      "Iter-8650, train loss-2.1985, acc-0.4400, valid loss-2.1912, acc-0.3988, test loss-2.1933, acc-0.3904\n",
      "Iter-8660, train loss-2.2194, acc-0.2800, valid loss-2.1911, acc-0.3988, test loss-2.1933, acc-0.3908\n",
      "Iter-8670, train loss-2.1809, acc-0.3600, valid loss-2.1910, acc-0.3992, test loss-2.1932, acc-0.3912\n",
      "Iter-8680, train loss-2.2024, acc-0.3400, valid loss-2.1909, acc-0.3992, test loss-2.1931, acc-0.3909\n",
      "Iter-8690, train loss-2.2125, acc-0.3600, valid loss-2.1908, acc-0.3994, test loss-2.1930, acc-0.3912\n",
      "Iter-8700, train loss-2.1986, acc-0.4400, valid loss-2.1907, acc-0.4000, test loss-2.1929, acc-0.3911\n",
      "Iter-8710, train loss-2.1995, acc-0.3800, valid loss-2.1906, acc-0.3998, test loss-2.1928, acc-0.3917\n",
      "Iter-8720, train loss-2.1837, acc-0.4200, valid loss-2.1905, acc-0.3998, test loss-2.1927, acc-0.3915\n",
      "Iter-8730, train loss-2.1933, acc-0.3800, valid loss-2.1904, acc-0.3998, test loss-2.1926, acc-0.3920\n",
      "Iter-8740, train loss-2.2179, acc-0.4000, valid loss-2.1903, acc-0.4000, test loss-2.1925, acc-0.3922\n",
      "Iter-8750, train loss-2.1775, acc-0.4800, valid loss-2.1902, acc-0.4004, test loss-2.1924, acc-0.3925\n",
      "Iter-8760, train loss-2.2137, acc-0.3400, valid loss-2.1901, acc-0.4010, test loss-2.1923, acc-0.3926\n",
      "Iter-8770, train loss-2.2290, acc-0.3200, valid loss-2.1900, acc-0.4012, test loss-2.1922, acc-0.3924\n",
      "Iter-8780, train loss-2.1921, acc-0.3800, valid loss-2.1899, acc-0.4014, test loss-2.1921, acc-0.3927\n",
      "Iter-8790, train loss-2.1685, acc-0.5400, valid loss-2.1898, acc-0.4016, test loss-2.1920, acc-0.3929\n",
      "Iter-8800, train loss-2.2090, acc-0.3600, valid loss-2.1898, acc-0.4020, test loss-2.1919, acc-0.3932\n",
      "Iter-8810, train loss-2.1680, acc-0.5000, valid loss-2.1897, acc-0.4022, test loss-2.1918, acc-0.3937\n",
      "Iter-8820, train loss-2.1955, acc-0.3800, valid loss-2.1896, acc-0.4030, test loss-2.1917, acc-0.3940\n",
      "Iter-8830, train loss-2.1860, acc-0.3600, valid loss-2.1895, acc-0.4030, test loss-2.1917, acc-0.3944\n",
      "Iter-8840, train loss-2.1841, acc-0.4600, valid loss-2.1894, acc-0.4026, test loss-2.1916, acc-0.3946\n",
      "Iter-8850, train loss-2.1940, acc-0.4000, valid loss-2.1893, acc-0.4020, test loss-2.1915, acc-0.3949\n",
      "Iter-8860, train loss-2.1780, acc-0.4200, valid loss-2.1892, acc-0.4024, test loss-2.1914, acc-0.3948\n",
      "Iter-8870, train loss-2.1833, acc-0.4600, valid loss-2.1891, acc-0.4028, test loss-2.1913, acc-0.3952\n",
      "Iter-8880, train loss-2.2125, acc-0.3400, valid loss-2.1890, acc-0.4038, test loss-2.1912, acc-0.3953\n",
      "Iter-8890, train loss-2.1881, acc-0.5200, valid loss-2.1889, acc-0.4038, test loss-2.1911, acc-0.3955\n",
      "Iter-8900, train loss-2.2013, acc-0.3800, valid loss-2.1888, acc-0.4030, test loss-2.1910, acc-0.3955\n",
      "Iter-8910, train loss-2.2080, acc-0.3800, valid loss-2.1887, acc-0.4028, test loss-2.1909, acc-0.3956\n",
      "Iter-8920, train loss-2.2074, acc-0.3200, valid loss-2.1886, acc-0.4036, test loss-2.1908, acc-0.3956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-2.1916, acc-0.4000, valid loss-2.1885, acc-0.4038, test loss-2.1907, acc-0.3960\n",
      "Iter-8940, train loss-2.2156, acc-0.3200, valid loss-2.1884, acc-0.4044, test loss-2.1906, acc-0.3961\n",
      "Iter-8950, train loss-2.1800, acc-0.4400, valid loss-2.1883, acc-0.4044, test loss-2.1905, acc-0.3966\n",
      "Iter-8960, train loss-2.1975, acc-0.4200, valid loss-2.1882, acc-0.4046, test loss-2.1904, acc-0.3967\n",
      "Iter-8970, train loss-2.1897, acc-0.4000, valid loss-2.1881, acc-0.4048, test loss-2.1903, acc-0.3967\n",
      "Iter-8980, train loss-2.2224, acc-0.3200, valid loss-2.1880, acc-0.4050, test loss-2.1902, acc-0.3972\n",
      "Iter-8990, train loss-2.1797, acc-0.4400, valid loss-2.1879, acc-0.4056, test loss-2.1901, acc-0.3973\n",
      "Iter-9000, train loss-2.2044, acc-0.3200, valid loss-2.1879, acc-0.4054, test loss-2.1901, acc-0.3976\n",
      "Iter-9010, train loss-2.1987, acc-0.4000, valid loss-2.1878, acc-0.4056, test loss-2.1900, acc-0.3977\n",
      "Iter-9020, train loss-2.2165, acc-0.3000, valid loss-2.1877, acc-0.4058, test loss-2.1899, acc-0.3977\n",
      "Iter-9030, train loss-2.1925, acc-0.4200, valid loss-2.1876, acc-0.4056, test loss-2.1898, acc-0.3981\n",
      "Iter-9040, train loss-2.1772, acc-0.4600, valid loss-2.1875, acc-0.4058, test loss-2.1897, acc-0.3983\n",
      "Iter-9050, train loss-2.2080, acc-0.3600, valid loss-2.1874, acc-0.4062, test loss-2.1896, acc-0.3981\n",
      "Iter-9060, train loss-2.1802, acc-0.3600, valid loss-2.1873, acc-0.4062, test loss-2.1895, acc-0.3984\n",
      "Iter-9070, train loss-2.1850, acc-0.3200, valid loss-2.1872, acc-0.4066, test loss-2.1894, acc-0.3984\n",
      "Iter-9080, train loss-2.2049, acc-0.4200, valid loss-2.1871, acc-0.4066, test loss-2.1893, acc-0.3985\n",
      "Iter-9090, train loss-2.1872, acc-0.4400, valid loss-2.1870, acc-0.4066, test loss-2.1892, acc-0.3990\n",
      "Iter-9100, train loss-2.2148, acc-0.3200, valid loss-2.1869, acc-0.4070, test loss-2.1891, acc-0.3991\n",
      "Iter-9110, train loss-2.1922, acc-0.2800, valid loss-2.1868, acc-0.4076, test loss-2.1890, acc-0.3993\n",
      "Iter-9120, train loss-2.1811, acc-0.4000, valid loss-2.1867, acc-0.4072, test loss-2.1889, acc-0.3992\n",
      "Iter-9130, train loss-2.2006, acc-0.3400, valid loss-2.1866, acc-0.4074, test loss-2.1888, acc-0.3993\n",
      "Iter-9140, train loss-2.1968, acc-0.4200, valid loss-2.1865, acc-0.4072, test loss-2.1887, acc-0.3994\n",
      "Iter-9150, train loss-2.2036, acc-0.2400, valid loss-2.1864, acc-0.4076, test loss-2.1886, acc-0.3994\n",
      "Iter-9160, train loss-2.2081, acc-0.3800, valid loss-2.1863, acc-0.4076, test loss-2.1885, acc-0.3997\n",
      "Iter-9170, train loss-2.1755, acc-0.3800, valid loss-2.1862, acc-0.4078, test loss-2.1885, acc-0.3998\n",
      "Iter-9180, train loss-2.1866, acc-0.5200, valid loss-2.1861, acc-0.4078, test loss-2.1884, acc-0.3999\n",
      "Iter-9190, train loss-2.2082, acc-0.3000, valid loss-2.1860, acc-0.4076, test loss-2.1883, acc-0.3998\n",
      "Iter-9200, train loss-2.1989, acc-0.3400, valid loss-2.1859, acc-0.4078, test loss-2.1882, acc-0.3999\n",
      "Iter-9210, train loss-2.1915, acc-0.3400, valid loss-2.1859, acc-0.4084, test loss-2.1881, acc-0.4000\n",
      "Iter-9220, train loss-2.2000, acc-0.3800, valid loss-2.1858, acc-0.4084, test loss-2.1880, acc-0.4005\n",
      "Iter-9230, train loss-2.1725, acc-0.5600, valid loss-2.1857, acc-0.4080, test loss-2.1879, acc-0.4003\n",
      "Iter-9240, train loss-2.2240, acc-0.2000, valid loss-2.1856, acc-0.4084, test loss-2.1878, acc-0.4003\n",
      "Iter-9250, train loss-2.1929, acc-0.4400, valid loss-2.1855, acc-0.4088, test loss-2.1877, acc-0.4005\n",
      "Iter-9260, train loss-2.1641, acc-0.4600, valid loss-2.1854, acc-0.4090, test loss-2.1876, acc-0.4005\n",
      "Iter-9270, train loss-2.1959, acc-0.3600, valid loss-2.1853, acc-0.4088, test loss-2.1875, acc-0.4006\n",
      "Iter-9280, train loss-2.2122, acc-0.3000, valid loss-2.1852, acc-0.4094, test loss-2.1874, acc-0.4009\n",
      "Iter-9290, train loss-2.1801, acc-0.4600, valid loss-2.1851, acc-0.4094, test loss-2.1874, acc-0.4008\n",
      "Iter-9300, train loss-2.2091, acc-0.4600, valid loss-2.1850, acc-0.4102, test loss-2.1873, acc-0.4012\n",
      "Iter-9310, train loss-2.1748, acc-0.4600, valid loss-2.1849, acc-0.4094, test loss-2.1872, acc-0.4018\n",
      "Iter-9320, train loss-2.2220, acc-0.3800, valid loss-2.1848, acc-0.4096, test loss-2.1871, acc-0.4018\n",
      "Iter-9330, train loss-2.2040, acc-0.3600, valid loss-2.1847, acc-0.4098, test loss-2.1870, acc-0.4018\n",
      "Iter-9340, train loss-2.1676, acc-0.5200, valid loss-2.1846, acc-0.4108, test loss-2.1869, acc-0.4022\n",
      "Iter-9350, train loss-2.1895, acc-0.4200, valid loss-2.1845, acc-0.4110, test loss-2.1868, acc-0.4023\n",
      "Iter-9360, train loss-2.1806, acc-0.3600, valid loss-2.1844, acc-0.4108, test loss-2.1867, acc-0.4023\n",
      "Iter-9370, train loss-2.2132, acc-0.2400, valid loss-2.1843, acc-0.4118, test loss-2.1866, acc-0.4026\n",
      "Iter-9380, train loss-2.1878, acc-0.4200, valid loss-2.1842, acc-0.4120, test loss-2.1865, acc-0.4027\n",
      "Iter-9390, train loss-2.1876, acc-0.4000, valid loss-2.1841, acc-0.4126, test loss-2.1864, acc-0.4029\n",
      "Iter-9400, train loss-2.1786, acc-0.4800, valid loss-2.1840, acc-0.4136, test loss-2.1863, acc-0.4030\n",
      "Iter-9410, train loss-2.2159, acc-0.3600, valid loss-2.1839, acc-0.4138, test loss-2.1862, acc-0.4035\n",
      "Iter-9420, train loss-2.2132, acc-0.3800, valid loss-2.1838, acc-0.4142, test loss-2.1861, acc-0.4035\n",
      "Iter-9430, train loss-2.1708, acc-0.4600, valid loss-2.1837, acc-0.4142, test loss-2.1860, acc-0.4035\n",
      "Iter-9440, train loss-2.1752, acc-0.5000, valid loss-2.1836, acc-0.4142, test loss-2.1859, acc-0.4038\n",
      "Iter-9450, train loss-2.1939, acc-0.3400, valid loss-2.1835, acc-0.4142, test loss-2.1858, acc-0.4040\n",
      "Iter-9460, train loss-2.2157, acc-0.3400, valid loss-2.1834, acc-0.4142, test loss-2.1857, acc-0.4039\n",
      "Iter-9470, train loss-2.2145, acc-0.3000, valid loss-2.1833, acc-0.4144, test loss-2.1856, acc-0.4040\n",
      "Iter-9480, train loss-2.1880, acc-0.4200, valid loss-2.1833, acc-0.4150, test loss-2.1855, acc-0.4045\n",
      "Iter-9490, train loss-2.1710, acc-0.4800, valid loss-2.1832, acc-0.4152, test loss-2.1855, acc-0.4045\n",
      "Iter-9500, train loss-2.1865, acc-0.4200, valid loss-2.1831, acc-0.4160, test loss-2.1854, acc-0.4050\n",
      "Iter-9510, train loss-2.1861, acc-0.3800, valid loss-2.1830, acc-0.4158, test loss-2.1853, acc-0.4050\n",
      "Iter-9520, train loss-2.1741, acc-0.4400, valid loss-2.1829, acc-0.4160, test loss-2.1852, acc-0.4050\n",
      "Iter-9530, train loss-2.1980, acc-0.4000, valid loss-2.1828, acc-0.4162, test loss-2.1851, acc-0.4052\n",
      "Iter-9540, train loss-2.1853, acc-0.4200, valid loss-2.1827, acc-0.4160, test loss-2.1850, acc-0.4053\n",
      "Iter-9550, train loss-2.2008, acc-0.4800, valid loss-2.1826, acc-0.4160, test loss-2.1849, acc-0.4049\n",
      "Iter-9560, train loss-2.2150, acc-0.3400, valid loss-2.1825, acc-0.4160, test loss-2.1848, acc-0.4048\n",
      "Iter-9570, train loss-2.1954, acc-0.3400, valid loss-2.1824, acc-0.4162, test loss-2.1847, acc-0.4050\n",
      "Iter-9580, train loss-2.1950, acc-0.4000, valid loss-2.1823, acc-0.4164, test loss-2.1846, acc-0.4050\n",
      "Iter-9590, train loss-2.1580, acc-0.5200, valid loss-2.1822, acc-0.4162, test loss-2.1845, acc-0.4047\n",
      "Iter-9600, train loss-2.1528, acc-0.5400, valid loss-2.1821, acc-0.4162, test loss-2.1844, acc-0.4047\n",
      "Iter-9610, train loss-2.1882, acc-0.4200, valid loss-2.1820, acc-0.4156, test loss-2.1843, acc-0.4049\n",
      "Iter-9620, train loss-2.2078, acc-0.3800, valid loss-2.1819, acc-0.4162, test loss-2.1842, acc-0.4051\n",
      "Iter-9630, train loss-2.2008, acc-0.3800, valid loss-2.1819, acc-0.4162, test loss-2.1841, acc-0.4058\n",
      "Iter-9640, train loss-2.1625, acc-0.4800, valid loss-2.1818, acc-0.4164, test loss-2.1841, acc-0.4058\n",
      "Iter-9650, train loss-2.1167, acc-0.6600, valid loss-2.1816, acc-0.4168, test loss-2.1840, acc-0.4059\n",
      "Iter-9660, train loss-2.1738, acc-0.4400, valid loss-2.1815, acc-0.4170, test loss-2.1839, acc-0.4064\n",
      "Iter-9670, train loss-2.1364, acc-0.5000, valid loss-2.1814, acc-0.4170, test loss-2.1838, acc-0.4067\n",
      "Iter-9680, train loss-2.1423, acc-0.5800, valid loss-2.1814, acc-0.4166, test loss-2.1837, acc-0.4066\n",
      "Iter-9690, train loss-2.1310, acc-0.4800, valid loss-2.1812, acc-0.4166, test loss-2.1836, acc-0.4062\n",
      "Iter-9700, train loss-2.1868, acc-0.3800, valid loss-2.1812, acc-0.4170, test loss-2.1835, acc-0.4067\n",
      "Iter-9710, train loss-2.1755, acc-0.4000, valid loss-2.1811, acc-0.4170, test loss-2.1834, acc-0.4068\n",
      "Iter-9720, train loss-2.1874, acc-0.3200, valid loss-2.1810, acc-0.4170, test loss-2.1833, acc-0.4066\n",
      "Iter-9730, train loss-2.2158, acc-0.3000, valid loss-2.1809, acc-0.4174, test loss-2.1832, acc-0.4070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-2.1479, acc-0.5200, valid loss-2.1808, acc-0.4172, test loss-2.1831, acc-0.4073\n",
      "Iter-9750, train loss-2.1919, acc-0.4200, valid loss-2.1807, acc-0.4178, test loss-2.1830, acc-0.4072\n",
      "Iter-9760, train loss-2.2127, acc-0.3400, valid loss-2.1806, acc-0.4178, test loss-2.1829, acc-0.4076\n",
      "Iter-9770, train loss-2.1793, acc-0.4000, valid loss-2.1805, acc-0.4174, test loss-2.1828, acc-0.4080\n",
      "Iter-9780, train loss-2.2129, acc-0.2400, valid loss-2.1804, acc-0.4174, test loss-2.1827, acc-0.4080\n",
      "Iter-9790, train loss-2.1936, acc-0.4200, valid loss-2.1803, acc-0.4176, test loss-2.1826, acc-0.4079\n",
      "Iter-9800, train loss-2.1904, acc-0.3800, valid loss-2.1802, acc-0.4174, test loss-2.1825, acc-0.4081\n",
      "Iter-9810, train loss-2.1999, acc-0.2800, valid loss-2.1801, acc-0.4174, test loss-2.1824, acc-0.4082\n",
      "Iter-9820, train loss-2.2064, acc-0.3400, valid loss-2.1800, acc-0.4178, test loss-2.1823, acc-0.4087\n",
      "Iter-9830, train loss-2.1992, acc-0.3200, valid loss-2.1799, acc-0.4182, test loss-2.1822, acc-0.4088\n",
      "Iter-9840, train loss-2.1616, acc-0.4200, valid loss-2.1798, acc-0.4184, test loss-2.1821, acc-0.4092\n",
      "Iter-9850, train loss-2.1814, acc-0.4800, valid loss-2.1797, acc-0.4184, test loss-2.1820, acc-0.4090\n",
      "Iter-9860, train loss-2.2171, acc-0.3400, valid loss-2.1796, acc-0.4188, test loss-2.1819, acc-0.4094\n",
      "Iter-9870, train loss-2.1814, acc-0.4800, valid loss-2.1795, acc-0.4190, test loss-2.1819, acc-0.4095\n",
      "Iter-9880, train loss-2.1894, acc-0.3200, valid loss-2.1794, acc-0.4190, test loss-2.1818, acc-0.4094\n",
      "Iter-9890, train loss-2.1722, acc-0.4200, valid loss-2.1794, acc-0.4194, test loss-2.1817, acc-0.4095\n",
      "Iter-9900, train loss-2.1762, acc-0.4200, valid loss-2.1793, acc-0.4200, test loss-2.1816, acc-0.4092\n",
      "Iter-9910, train loss-2.1917, acc-0.3600, valid loss-2.1792, acc-0.4200, test loss-2.1815, acc-0.4097\n",
      "Iter-9920, train loss-2.1633, acc-0.4800, valid loss-2.1791, acc-0.4200, test loss-2.1814, acc-0.4097\n",
      "Iter-9930, train loss-2.1786, acc-0.4800, valid loss-2.1790, acc-0.4200, test loss-2.1813, acc-0.4099\n",
      "Iter-9940, train loss-2.2035, acc-0.3800, valid loss-2.1789, acc-0.4206, test loss-2.1812, acc-0.4104\n",
      "Iter-9950, train loss-2.1816, acc-0.4200, valid loss-2.1788, acc-0.4208, test loss-2.1811, acc-0.4103\n",
      "Iter-9960, train loss-2.1904, acc-0.3200, valid loss-2.1787, acc-0.4210, test loss-2.1810, acc-0.4108\n",
      "Iter-9970, train loss-2.1891, acc-0.4200, valid loss-2.1786, acc-0.4210, test loss-2.1809, acc-0.4108\n",
      "Iter-9980, train loss-2.1729, acc-0.4800, valid loss-2.1785, acc-0.4214, test loss-2.1809, acc-0.4110\n",
      "Iter-9990, train loss-2.1558, acc-0.4600, valid loss-2.1784, acc-0.4214, test loss-2.1808, acc-0.4110\n",
      "Iter-10000, train loss-2.1873, acc-0.3600, valid loss-2.1783, acc-0.4216, test loss-2.1807, acc-0.4111\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAgqiOQQBEEQVBDJIQiCIKjgH/TH/hg5UGs9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118201160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118c2f668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
