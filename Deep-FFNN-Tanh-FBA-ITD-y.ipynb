{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#         y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#         y *= 2.0 # uni-var/ std\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#             y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#             y *= 2.0 # uni-var/ std\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        y_prev = self.y_prev.copy() # for temporal differencing\n",
    "        self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        y =  y @ self.W_fixed[2].T # done\n",
    "        y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            y =  y @ self.W_fixed[1][layer].T # done\n",
    "            y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3043, acc-0.1200, valid loss-2.2970, acc-0.0914, test loss-2.2932, acc-0.1012\n",
      "Iter-20, train loss-2.2754, acc-0.1400, valid loss-2.2969, acc-0.0916, test loss-2.2931, acc-0.1011\n",
      "Iter-30, train loss-2.2841, acc-0.1000, valid loss-2.2968, acc-0.0918, test loss-2.2930, acc-0.1015\n",
      "Iter-40, train loss-2.2906, acc-0.1000, valid loss-2.2967, acc-0.0918, test loss-2.2929, acc-0.1014\n",
      "Iter-50, train loss-2.2893, acc-0.1000, valid loss-2.2966, acc-0.0920, test loss-2.2928, acc-0.1016\n",
      "Iter-60, train loss-2.3083, acc-0.0400, valid loss-2.2965, acc-0.0918, test loss-2.2927, acc-0.1017\n",
      "Iter-70, train loss-2.3308, acc-0.0200, valid loss-2.2964, acc-0.0920, test loss-2.2925, acc-0.1017\n",
      "Iter-80, train loss-2.3082, acc-0.1600, valid loss-2.2963, acc-0.0928, test loss-2.2924, acc-0.1024\n",
      "Iter-90, train loss-2.2969, acc-0.1200, valid loss-2.2962, acc-0.0928, test loss-2.2923, acc-0.1023\n",
      "Iter-100, train loss-2.2948, acc-0.1000, valid loss-2.2961, acc-0.0936, test loss-2.2922, acc-0.1028\n",
      "Iter-110, train loss-2.2989, acc-0.0800, valid loss-2.2960, acc-0.0942, test loss-2.2921, acc-0.1035\n",
      "Iter-120, train loss-2.2707, acc-0.2000, valid loss-2.2959, acc-0.0942, test loss-2.2920, acc-0.1036\n",
      "Iter-130, train loss-2.2889, acc-0.0600, valid loss-2.2958, acc-0.0944, test loss-2.2919, acc-0.1036\n",
      "Iter-140, train loss-2.2882, acc-0.0800, valid loss-2.2957, acc-0.0948, test loss-2.2918, acc-0.1039\n",
      "Iter-150, train loss-2.3134, acc-0.0400, valid loss-2.2956, acc-0.0952, test loss-2.2917, acc-0.1041\n",
      "Iter-160, train loss-2.3237, acc-0.0400, valid loss-2.2955, acc-0.0950, test loss-2.2916, acc-0.1044\n",
      "Iter-170, train loss-2.2935, acc-0.0800, valid loss-2.2954, acc-0.0956, test loss-2.2915, acc-0.1048\n",
      "Iter-180, train loss-2.2850, acc-0.1200, valid loss-2.2952, acc-0.0960, test loss-2.2914, acc-0.1050\n",
      "Iter-190, train loss-2.2803, acc-0.0600, valid loss-2.2951, acc-0.0962, test loss-2.2913, acc-0.1054\n",
      "Iter-200, train loss-2.2766, acc-0.1200, valid loss-2.2950, acc-0.0968, test loss-2.2912, acc-0.1054\n",
      "Iter-210, train loss-2.2746, acc-0.1400, valid loss-2.2949, acc-0.0970, test loss-2.2911, acc-0.1057\n",
      "Iter-220, train loss-2.3086, acc-0.0400, valid loss-2.2948, acc-0.0970, test loss-2.2910, acc-0.1058\n",
      "Iter-230, train loss-2.2803, acc-0.1000, valid loss-2.2947, acc-0.0972, test loss-2.2909, acc-0.1061\n",
      "Iter-240, train loss-2.3213, acc-0.0400, valid loss-2.2946, acc-0.0974, test loss-2.2908, acc-0.1066\n",
      "Iter-250, train loss-2.2745, acc-0.1200, valid loss-2.2945, acc-0.0980, test loss-2.2907, acc-0.1064\n",
      "Iter-260, train loss-2.3046, acc-0.0200, valid loss-2.2944, acc-0.0980, test loss-2.2906, acc-0.1072\n",
      "Iter-270, train loss-2.2976, acc-0.0400, valid loss-2.2943, acc-0.0986, test loss-2.2905, acc-0.1079\n",
      "Iter-280, train loss-2.2715, acc-0.1200, valid loss-2.2942, acc-0.0984, test loss-2.2904, acc-0.1078\n",
      "Iter-290, train loss-2.2745, acc-0.0800, valid loss-2.2941, acc-0.0986, test loss-2.2903, acc-0.1078\n",
      "Iter-300, train loss-2.2816, acc-0.1200, valid loss-2.2940, acc-0.0986, test loss-2.2901, acc-0.1085\n",
      "Iter-310, train loss-2.2806, acc-0.1200, valid loss-2.2939, acc-0.0982, test loss-2.2900, acc-0.1091\n",
      "Iter-320, train loss-2.2817, acc-0.1600, valid loss-2.2938, acc-0.0982, test loss-2.2899, acc-0.1096\n",
      "Iter-330, train loss-2.2902, acc-0.1200, valid loss-2.2937, acc-0.0984, test loss-2.2898, acc-0.1100\n",
      "Iter-340, train loss-2.2818, acc-0.0600, valid loss-2.2936, acc-0.0990, test loss-2.2897, acc-0.1104\n",
      "Iter-350, train loss-2.2842, acc-0.1200, valid loss-2.2935, acc-0.0992, test loss-2.2896, acc-0.1105\n",
      "Iter-360, train loss-2.2978, acc-0.1000, valid loss-2.2934, acc-0.0994, test loss-2.2895, acc-0.1107\n",
      "Iter-370, train loss-2.2443, acc-0.2000, valid loss-2.2933, acc-0.0996, test loss-2.2894, acc-0.1108\n",
      "Iter-380, train loss-2.2797, acc-0.1600, valid loss-2.2932, acc-0.1006, test loss-2.2893, acc-0.1108\n",
      "Iter-390, train loss-2.2825, acc-0.1000, valid loss-2.2931, acc-0.1006, test loss-2.2892, acc-0.1108\n",
      "Iter-400, train loss-2.2749, acc-0.1000, valid loss-2.2930, acc-0.1012, test loss-2.2891, acc-0.1110\n",
      "Iter-410, train loss-2.2985, acc-0.0600, valid loss-2.2929, acc-0.1016, test loss-2.2890, acc-0.1110\n",
      "Iter-420, train loss-2.2831, acc-0.1200, valid loss-2.2928, acc-0.1020, test loss-2.2889, acc-0.1113\n",
      "Iter-430, train loss-2.3060, acc-0.1000, valid loss-2.2927, acc-0.1024, test loss-2.2888, acc-0.1117\n",
      "Iter-440, train loss-2.3007, acc-0.1000, valid loss-2.2926, acc-0.1028, test loss-2.2887, acc-0.1118\n",
      "Iter-450, train loss-2.2905, acc-0.0600, valid loss-2.2925, acc-0.1024, test loss-2.2886, acc-0.1120\n",
      "Iter-460, train loss-2.3009, acc-0.0800, valid loss-2.2924, acc-0.1028, test loss-2.2885, acc-0.1124\n",
      "Iter-470, train loss-2.3003, acc-0.1200, valid loss-2.2923, acc-0.1030, test loss-2.2884, acc-0.1126\n",
      "Iter-480, train loss-2.3038, acc-0.1200, valid loss-2.2922, acc-0.1034, test loss-2.2883, acc-0.1129\n",
      "Iter-490, train loss-2.2897, acc-0.1400, valid loss-2.2921, acc-0.1044, test loss-2.2882, acc-0.1135\n",
      "Iter-500, train loss-2.2766, acc-0.2400, valid loss-2.2920, acc-0.1040, test loss-2.2881, acc-0.1139\n",
      "Iter-510, train loss-2.2856, acc-0.1400, valid loss-2.2919, acc-0.1046, test loss-2.2880, acc-0.1147\n",
      "Iter-520, train loss-2.2919, acc-0.0400, valid loss-2.2918, acc-0.1052, test loss-2.2879, acc-0.1148\n",
      "Iter-530, train loss-2.2898, acc-0.1000, valid loss-2.2917, acc-0.1050, test loss-2.2878, acc-0.1150\n",
      "Iter-540, train loss-2.2787, acc-0.1400, valid loss-2.2916, acc-0.1058, test loss-2.2877, acc-0.1155\n",
      "Iter-550, train loss-2.2727, acc-0.1400, valid loss-2.2915, acc-0.1066, test loss-2.2876, acc-0.1156\n",
      "Iter-560, train loss-2.2735, acc-0.2000, valid loss-2.2914, acc-0.1084, test loss-2.2875, acc-0.1160\n",
      "Iter-570, train loss-2.2840, acc-0.0800, valid loss-2.2913, acc-0.1086, test loss-2.2874, acc-0.1161\n",
      "Iter-580, train loss-2.2906, acc-0.1400, valid loss-2.2912, acc-0.1098, test loss-2.2873, acc-0.1163\n",
      "Iter-590, train loss-2.2942, acc-0.1000, valid loss-2.2911, acc-0.1104, test loss-2.2872, acc-0.1166\n",
      "Iter-600, train loss-2.2848, acc-0.2000, valid loss-2.2910, acc-0.1112, test loss-2.2871, acc-0.1174\n",
      "Iter-610, train loss-2.3044, acc-0.0800, valid loss-2.2909, acc-0.1110, test loss-2.2870, acc-0.1172\n",
      "Iter-620, train loss-2.3064, acc-0.1200, valid loss-2.2908, acc-0.1112, test loss-2.2869, acc-0.1178\n",
      "Iter-630, train loss-2.2959, acc-0.1200, valid loss-2.2907, acc-0.1114, test loss-2.2868, acc-0.1180\n",
      "Iter-640, train loss-2.2949, acc-0.1600, valid loss-2.2906, acc-0.1126, test loss-2.2867, acc-0.1186\n",
      "Iter-650, train loss-2.2625, acc-0.1200, valid loss-2.2905, acc-0.1128, test loss-2.2866, acc-0.1187\n",
      "Iter-660, train loss-2.3040, acc-0.0400, valid loss-2.2904, acc-0.1134, test loss-2.2865, acc-0.1196\n",
      "Iter-670, train loss-2.2903, acc-0.1600, valid loss-2.2903, acc-0.1138, test loss-2.2864, acc-0.1197\n",
      "Iter-680, train loss-2.3032, acc-0.1000, valid loss-2.2902, acc-0.1140, test loss-2.2863, acc-0.1197\n",
      "Iter-690, train loss-2.3032, acc-0.0600, valid loss-2.2901, acc-0.1152, test loss-2.2862, acc-0.1200\n",
      "Iter-700, train loss-2.2856, acc-0.1800, valid loss-2.2900, acc-0.1164, test loss-2.2861, acc-0.1206\n",
      "Iter-710, train loss-2.2839, acc-0.1000, valid loss-2.2899, acc-0.1162, test loss-2.2860, acc-0.1207\n",
      "Iter-720, train loss-2.2898, acc-0.1200, valid loss-2.2898, acc-0.1164, test loss-2.2859, acc-0.1216\n",
      "Iter-730, train loss-2.2794, acc-0.0800, valid loss-2.2897, acc-0.1164, test loss-2.2858, acc-0.1222\n",
      "Iter-740, train loss-2.2728, acc-0.2400, valid loss-2.2896, acc-0.1174, test loss-2.2856, acc-0.1225\n",
      "Iter-750, train loss-2.2677, acc-0.2000, valid loss-2.2895, acc-0.1180, test loss-2.2855, acc-0.1234\n",
      "Iter-760, train loss-2.2765, acc-0.1600, valid loss-2.2894, acc-0.1178, test loss-2.2854, acc-0.1238\n",
      "Iter-770, train loss-2.2846, acc-0.1200, valid loss-2.2892, acc-0.1180, test loss-2.2853, acc-0.1243\n",
      "Iter-780, train loss-2.3064, acc-0.1000, valid loss-2.2891, acc-0.1180, test loss-2.2852, acc-0.1244\n",
      "Iter-790, train loss-2.2832, acc-0.1600, valid loss-2.2890, acc-0.1182, test loss-2.2851, acc-0.1251\n",
      "Iter-800, train loss-2.2829, acc-0.1600, valid loss-2.2889, acc-0.1194, test loss-2.2850, acc-0.1258\n",
      "Iter-810, train loss-2.2778, acc-0.1200, valid loss-2.2888, acc-0.1196, test loss-2.2849, acc-0.1264\n",
      "Iter-820, train loss-2.2840, acc-0.1800, valid loss-2.2887, acc-0.1196, test loss-2.2848, acc-0.1272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.2949, acc-0.0200, valid loss-2.2886, acc-0.1204, test loss-2.2847, acc-0.1276\n",
      "Iter-840, train loss-2.2745, acc-0.2200, valid loss-2.2885, acc-0.1208, test loss-2.2846, acc-0.1284\n",
      "Iter-850, train loss-2.2687, acc-0.2200, valid loss-2.2884, acc-0.1210, test loss-2.2845, acc-0.1289\n",
      "Iter-860, train loss-2.2853, acc-0.1600, valid loss-2.2883, acc-0.1212, test loss-2.2844, acc-0.1292\n",
      "Iter-870, train loss-2.2625, acc-0.1600, valid loss-2.2882, acc-0.1212, test loss-2.2843, acc-0.1296\n",
      "Iter-880, train loss-2.2836, acc-0.0600, valid loss-2.2881, acc-0.1212, test loss-2.2842, acc-0.1298\n",
      "Iter-890, train loss-2.2937, acc-0.0600, valid loss-2.2880, acc-0.1212, test loss-2.2841, acc-0.1305\n",
      "Iter-900, train loss-2.3065, acc-0.0400, valid loss-2.2879, acc-0.1220, test loss-2.2840, acc-0.1313\n",
      "Iter-910, train loss-2.2982, acc-0.0800, valid loss-2.2878, acc-0.1222, test loss-2.2839, acc-0.1319\n",
      "Iter-920, train loss-2.3016, acc-0.1200, valid loss-2.2877, acc-0.1228, test loss-2.2838, acc-0.1319\n",
      "Iter-930, train loss-2.3091, acc-0.0400, valid loss-2.2876, acc-0.1234, test loss-2.2837, acc-0.1325\n",
      "Iter-940, train loss-2.2930, acc-0.1400, valid loss-2.2875, acc-0.1244, test loss-2.2836, acc-0.1329\n",
      "Iter-950, train loss-2.2978, acc-0.1200, valid loss-2.2874, acc-0.1246, test loss-2.2835, acc-0.1333\n",
      "Iter-960, train loss-2.2714, acc-0.1400, valid loss-2.2873, acc-0.1256, test loss-2.2834, acc-0.1337\n",
      "Iter-970, train loss-2.2548, acc-0.2400, valid loss-2.2872, acc-0.1262, test loss-2.2833, acc-0.1341\n",
      "Iter-980, train loss-2.2946, acc-0.1200, valid loss-2.2871, acc-0.1272, test loss-2.2832, acc-0.1347\n",
      "Iter-990, train loss-2.2841, acc-0.1400, valid loss-2.2870, acc-0.1276, test loss-2.2831, acc-0.1353\n",
      "Iter-1000, train loss-2.3031, acc-0.1000, valid loss-2.2869, acc-0.1274, test loss-2.2830, acc-0.1351\n",
      "Iter-1010, train loss-2.2726, acc-0.2000, valid loss-2.2868, acc-0.1276, test loss-2.2829, acc-0.1356\n",
      "Iter-1020, train loss-2.2906, acc-0.1600, valid loss-2.2867, acc-0.1286, test loss-2.2828, acc-0.1360\n",
      "Iter-1030, train loss-2.2796, acc-0.1600, valid loss-2.2866, acc-0.1296, test loss-2.2827, acc-0.1365\n",
      "Iter-1040, train loss-2.2674, acc-0.1800, valid loss-2.2865, acc-0.1300, test loss-2.2825, acc-0.1375\n",
      "Iter-1050, train loss-2.2382, acc-0.2200, valid loss-2.2864, acc-0.1302, test loss-2.2825, acc-0.1377\n",
      "Iter-1060, train loss-2.2928, acc-0.0800, valid loss-2.2863, acc-0.1304, test loss-2.2823, acc-0.1375\n",
      "Iter-1070, train loss-2.2745, acc-0.1200, valid loss-2.2862, acc-0.1310, test loss-2.2822, acc-0.1379\n",
      "Iter-1080, train loss-2.2437, acc-0.2800, valid loss-2.2861, acc-0.1308, test loss-2.2821, acc-0.1390\n",
      "Iter-1090, train loss-2.3007, acc-0.1000, valid loss-2.2860, acc-0.1312, test loss-2.2820, acc-0.1391\n",
      "Iter-1100, train loss-2.2940, acc-0.1000, valid loss-2.2859, acc-0.1314, test loss-2.2819, acc-0.1396\n",
      "Iter-1110, train loss-2.2882, acc-0.1200, valid loss-2.2858, acc-0.1318, test loss-2.2818, acc-0.1397\n",
      "Iter-1120, train loss-2.2859, acc-0.1200, valid loss-2.2857, acc-0.1318, test loss-2.2817, acc-0.1400\n",
      "Iter-1130, train loss-2.3034, acc-0.1000, valid loss-2.2855, acc-0.1326, test loss-2.2816, acc-0.1402\n",
      "Iter-1140, train loss-2.2632, acc-0.1600, valid loss-2.2855, acc-0.1330, test loss-2.2815, acc-0.1402\n",
      "Iter-1150, train loss-2.2824, acc-0.2000, valid loss-2.2854, acc-0.1334, test loss-2.2814, acc-0.1411\n",
      "Iter-1160, train loss-2.2767, acc-0.1200, valid loss-2.2852, acc-0.1336, test loss-2.2813, acc-0.1413\n",
      "Iter-1170, train loss-2.2707, acc-0.1800, valid loss-2.2851, acc-0.1338, test loss-2.2812, acc-0.1419\n",
      "Iter-1180, train loss-2.2782, acc-0.1400, valid loss-2.2850, acc-0.1346, test loss-2.2811, acc-0.1423\n",
      "Iter-1190, train loss-2.2709, acc-0.1600, valid loss-2.2850, acc-0.1344, test loss-2.2810, acc-0.1430\n",
      "Iter-1200, train loss-2.2835, acc-0.1800, valid loss-2.2849, acc-0.1348, test loss-2.2809, acc-0.1437\n",
      "Iter-1210, train loss-2.2611, acc-0.2200, valid loss-2.2848, acc-0.1356, test loss-2.2808, acc-0.1441\n",
      "Iter-1220, train loss-2.2726, acc-0.1600, valid loss-2.2847, acc-0.1362, test loss-2.2807, acc-0.1445\n",
      "Iter-1230, train loss-2.2699, acc-0.1800, valid loss-2.2846, acc-0.1364, test loss-2.2806, acc-0.1444\n",
      "Iter-1240, train loss-2.2655, acc-0.1600, valid loss-2.2844, acc-0.1366, test loss-2.2805, acc-0.1447\n",
      "Iter-1250, train loss-2.2948, acc-0.0800, valid loss-2.2843, acc-0.1372, test loss-2.2804, acc-0.1450\n",
      "Iter-1260, train loss-2.2564, acc-0.2000, valid loss-2.2842, acc-0.1380, test loss-2.2803, acc-0.1463\n",
      "Iter-1270, train loss-2.2644, acc-0.1400, valid loss-2.2841, acc-0.1380, test loss-2.2802, acc-0.1466\n",
      "Iter-1280, train loss-2.2977, acc-0.1000, valid loss-2.2840, acc-0.1388, test loss-2.2801, acc-0.1470\n",
      "Iter-1290, train loss-2.2899, acc-0.1200, valid loss-2.2839, acc-0.1392, test loss-2.2800, acc-0.1473\n",
      "Iter-1300, train loss-2.2601, acc-0.1800, valid loss-2.2838, acc-0.1404, test loss-2.2799, acc-0.1487\n",
      "Iter-1310, train loss-2.2954, acc-0.0800, valid loss-2.2837, acc-0.1404, test loss-2.2798, acc-0.1490\n",
      "Iter-1320, train loss-2.2936, acc-0.1200, valid loss-2.2836, acc-0.1412, test loss-2.2797, acc-0.1498\n",
      "Iter-1330, train loss-2.2881, acc-0.2200, valid loss-2.2835, acc-0.1414, test loss-2.2796, acc-0.1503\n",
      "Iter-1340, train loss-2.2740, acc-0.1600, valid loss-2.2834, acc-0.1414, test loss-2.2795, acc-0.1506\n",
      "Iter-1350, train loss-2.2790, acc-0.1400, valid loss-2.2833, acc-0.1414, test loss-2.2794, acc-0.1507\n",
      "Iter-1360, train loss-2.2867, acc-0.1600, valid loss-2.2832, acc-0.1418, test loss-2.2793, acc-0.1508\n",
      "Iter-1370, train loss-2.2817, acc-0.2000, valid loss-2.2831, acc-0.1420, test loss-2.2792, acc-0.1514\n",
      "Iter-1380, train loss-2.2668, acc-0.1600, valid loss-2.2830, acc-0.1422, test loss-2.2790, acc-0.1520\n",
      "Iter-1390, train loss-2.3034, acc-0.1200, valid loss-2.2829, acc-0.1428, test loss-2.2789, acc-0.1527\n",
      "Iter-1400, train loss-2.2955, acc-0.1600, valid loss-2.2828, acc-0.1430, test loss-2.2788, acc-0.1528\n",
      "Iter-1410, train loss-2.2668, acc-0.1200, valid loss-2.2827, acc-0.1434, test loss-2.2787, acc-0.1530\n",
      "Iter-1420, train loss-2.2990, acc-0.0800, valid loss-2.2826, acc-0.1430, test loss-2.2786, acc-0.1535\n",
      "Iter-1430, train loss-2.2655, acc-0.1600, valid loss-2.2825, acc-0.1440, test loss-2.2785, acc-0.1542\n",
      "Iter-1440, train loss-2.2582, acc-0.1800, valid loss-2.2824, acc-0.1450, test loss-2.2784, acc-0.1550\n",
      "Iter-1450, train loss-2.2996, acc-0.1200, valid loss-2.2823, acc-0.1450, test loss-2.2783, acc-0.1551\n",
      "Iter-1460, train loss-2.2718, acc-0.2200, valid loss-2.2822, acc-0.1450, test loss-2.2782, acc-0.1550\n",
      "Iter-1470, train loss-2.2814, acc-0.1200, valid loss-2.2821, acc-0.1454, test loss-2.2781, acc-0.1559\n",
      "Iter-1480, train loss-2.2570, acc-0.2200, valid loss-2.2820, acc-0.1452, test loss-2.2780, acc-0.1558\n",
      "Iter-1490, train loss-2.3151, acc-0.1000, valid loss-2.2819, acc-0.1458, test loss-2.2779, acc-0.1562\n",
      "Iter-1500, train loss-2.2538, acc-0.2800, valid loss-2.2818, acc-0.1464, test loss-2.2778, acc-0.1566\n",
      "Iter-1510, train loss-2.2976, acc-0.1200, valid loss-2.2817, acc-0.1468, test loss-2.2777, acc-0.1570\n",
      "Iter-1520, train loss-2.2816, acc-0.0400, valid loss-2.2816, acc-0.1470, test loss-2.2776, acc-0.1572\n",
      "Iter-1530, train loss-2.2696, acc-0.1000, valid loss-2.2815, acc-0.1478, test loss-2.2775, acc-0.1582\n",
      "Iter-1540, train loss-2.2676, acc-0.2000, valid loss-2.2814, acc-0.1480, test loss-2.2774, acc-0.1583\n",
      "Iter-1550, train loss-2.2778, acc-0.1400, valid loss-2.2813, acc-0.1478, test loss-2.2773, acc-0.1589\n",
      "Iter-1560, train loss-2.2572, acc-0.1400, valid loss-2.2812, acc-0.1480, test loss-2.2772, acc-0.1591\n",
      "Iter-1570, train loss-2.2876, acc-0.1200, valid loss-2.2811, acc-0.1480, test loss-2.2771, acc-0.1597\n",
      "Iter-1580, train loss-2.2625, acc-0.2200, valid loss-2.2810, acc-0.1490, test loss-2.2770, acc-0.1601\n",
      "Iter-1590, train loss-2.2911, acc-0.0600, valid loss-2.2809, acc-0.1494, test loss-2.2769, acc-0.1605\n",
      "Iter-1600, train loss-2.2986, acc-0.1200, valid loss-2.2808, acc-0.1494, test loss-2.2768, acc-0.1607\n",
      "Iter-1610, train loss-2.2709, acc-0.1000, valid loss-2.2807, acc-0.1500, test loss-2.2767, acc-0.1613\n",
      "Iter-1620, train loss-2.2732, acc-0.1600, valid loss-2.2806, acc-0.1510, test loss-2.2766, acc-0.1614\n",
      "Iter-1630, train loss-2.3031, acc-0.0600, valid loss-2.2805, acc-0.1516, test loss-2.2765, acc-0.1617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.2617, acc-0.1600, valid loss-2.2804, acc-0.1520, test loss-2.2764, acc-0.1621\n",
      "Iter-1650, train loss-2.2809, acc-0.1800, valid loss-2.2803, acc-0.1526, test loss-2.2763, acc-0.1630\n",
      "Iter-1660, train loss-2.2884, acc-0.0600, valid loss-2.2802, acc-0.1532, test loss-2.2762, acc-0.1632\n",
      "Iter-1670, train loss-2.2774, acc-0.1800, valid loss-2.2801, acc-0.1536, test loss-2.2761, acc-0.1637\n",
      "Iter-1680, train loss-2.2728, acc-0.1800, valid loss-2.2800, acc-0.1538, test loss-2.2760, acc-0.1639\n",
      "Iter-1690, train loss-2.2804, acc-0.1200, valid loss-2.2799, acc-0.1540, test loss-2.2759, acc-0.1643\n",
      "Iter-1700, train loss-2.3002, acc-0.1800, valid loss-2.2798, acc-0.1552, test loss-2.2758, acc-0.1649\n",
      "Iter-1710, train loss-2.2755, acc-0.1800, valid loss-2.2797, acc-0.1554, test loss-2.2757, acc-0.1649\n",
      "Iter-1720, train loss-2.2714, acc-0.1400, valid loss-2.2796, acc-0.1552, test loss-2.2756, acc-0.1651\n",
      "Iter-1730, train loss-2.3027, acc-0.1200, valid loss-2.2795, acc-0.1554, test loss-2.2755, acc-0.1652\n",
      "Iter-1740, train loss-2.2525, acc-0.2600, valid loss-2.2794, acc-0.1556, test loss-2.2754, acc-0.1657\n",
      "Iter-1750, train loss-2.2737, acc-0.1000, valid loss-2.2792, acc-0.1570, test loss-2.2752, acc-0.1665\n",
      "Iter-1760, train loss-2.2773, acc-0.1400, valid loss-2.2791, acc-0.1568, test loss-2.2752, acc-0.1667\n",
      "Iter-1770, train loss-2.2586, acc-0.2000, valid loss-2.2791, acc-0.1568, test loss-2.2751, acc-0.1668\n",
      "Iter-1780, train loss-2.2718, acc-0.2600, valid loss-2.2789, acc-0.1570, test loss-2.2750, acc-0.1680\n",
      "Iter-1790, train loss-2.2797, acc-0.0800, valid loss-2.2789, acc-0.1568, test loss-2.2749, acc-0.1682\n",
      "Iter-1800, train loss-2.2636, acc-0.1600, valid loss-2.2787, acc-0.1578, test loss-2.2747, acc-0.1684\n",
      "Iter-1810, train loss-2.2610, acc-0.3000, valid loss-2.2786, acc-0.1576, test loss-2.2746, acc-0.1686\n",
      "Iter-1820, train loss-2.2483, acc-0.2200, valid loss-2.2785, acc-0.1576, test loss-2.2745, acc-0.1689\n",
      "Iter-1830, train loss-2.2686, acc-0.1400, valid loss-2.2784, acc-0.1576, test loss-2.2744, acc-0.1691\n",
      "Iter-1840, train loss-2.2880, acc-0.1000, valid loss-2.2783, acc-0.1582, test loss-2.2743, acc-0.1696\n",
      "Iter-1850, train loss-2.2899, acc-0.1600, valid loss-2.2782, acc-0.1584, test loss-2.2742, acc-0.1697\n",
      "Iter-1860, train loss-2.2710, acc-0.1400, valid loss-2.2781, acc-0.1584, test loss-2.2741, acc-0.1699\n",
      "Iter-1870, train loss-2.2560, acc-0.2000, valid loss-2.2780, acc-0.1588, test loss-2.2740, acc-0.1703\n",
      "Iter-1880, train loss-2.2716, acc-0.1800, valid loss-2.2779, acc-0.1588, test loss-2.2739, acc-0.1710\n",
      "Iter-1890, train loss-2.2814, acc-0.1000, valid loss-2.2778, acc-0.1596, test loss-2.2738, acc-0.1711\n",
      "Iter-1900, train loss-2.2698, acc-0.0800, valid loss-2.2777, acc-0.1594, test loss-2.2737, acc-0.1717\n",
      "Iter-1910, train loss-2.2763, acc-0.1600, valid loss-2.2776, acc-0.1596, test loss-2.2736, acc-0.1722\n",
      "Iter-1920, train loss-2.2556, acc-0.2800, valid loss-2.2775, acc-0.1600, test loss-2.2735, acc-0.1723\n",
      "Iter-1930, train loss-2.2762, acc-0.1600, valid loss-2.2774, acc-0.1612, test loss-2.2734, acc-0.1728\n",
      "Iter-1940, train loss-2.2790, acc-0.1200, valid loss-2.2773, acc-0.1610, test loss-2.2733, acc-0.1728\n",
      "Iter-1950, train loss-2.2696, acc-0.2200, valid loss-2.2772, acc-0.1606, test loss-2.2732, acc-0.1733\n",
      "Iter-1960, train loss-2.2706, acc-0.2600, valid loss-2.2771, acc-0.1620, test loss-2.2731, acc-0.1735\n",
      "Iter-1970, train loss-2.2752, acc-0.1000, valid loss-2.2770, acc-0.1622, test loss-2.2730, acc-0.1732\n",
      "Iter-1980, train loss-2.2460, acc-0.2200, valid loss-2.2769, acc-0.1622, test loss-2.2729, acc-0.1738\n",
      "Iter-1990, train loss-2.2754, acc-0.1800, valid loss-2.2768, acc-0.1624, test loss-2.2728, acc-0.1742\n",
      "Iter-2000, train loss-2.2702, acc-0.2000, valid loss-2.2767, acc-0.1624, test loss-2.2727, acc-0.1743\n",
      "Iter-2010, train loss-2.2947, acc-0.2200, valid loss-2.2766, acc-0.1624, test loss-2.2726, acc-0.1747\n",
      "Iter-2020, train loss-2.2998, acc-0.1600, valid loss-2.2765, acc-0.1626, test loss-2.2725, acc-0.1750\n",
      "Iter-2030, train loss-2.2688, acc-0.2000, valid loss-2.2764, acc-0.1630, test loss-2.2724, acc-0.1755\n",
      "Iter-2040, train loss-2.2612, acc-0.2000, valid loss-2.2763, acc-0.1632, test loss-2.2723, acc-0.1754\n",
      "Iter-2050, train loss-2.2613, acc-0.2200, valid loss-2.2762, acc-0.1626, test loss-2.2722, acc-0.1755\n",
      "Iter-2060, train loss-2.2885, acc-0.1200, valid loss-2.2761, acc-0.1626, test loss-2.2721, acc-0.1757\n",
      "Iter-2070, train loss-2.2768, acc-0.1400, valid loss-2.2760, acc-0.1624, test loss-2.2720, acc-0.1759\n",
      "Iter-2080, train loss-2.2643, acc-0.1400, valid loss-2.2759, acc-0.1628, test loss-2.2719, acc-0.1766\n",
      "Iter-2090, train loss-2.2344, acc-0.2000, valid loss-2.2758, acc-0.1630, test loss-2.2718, acc-0.1774\n",
      "Iter-2100, train loss-2.2732, acc-0.1800, valid loss-2.2757, acc-0.1630, test loss-2.2717, acc-0.1778\n",
      "Iter-2110, train loss-2.2703, acc-0.1800, valid loss-2.2756, acc-0.1628, test loss-2.2716, acc-0.1780\n",
      "Iter-2120, train loss-2.2835, acc-0.1400, valid loss-2.2755, acc-0.1636, test loss-2.2715, acc-0.1782\n",
      "Iter-2130, train loss-2.2607, acc-0.2400, valid loss-2.2754, acc-0.1640, test loss-2.2714, acc-0.1789\n",
      "Iter-2140, train loss-2.2822, acc-0.2600, valid loss-2.2753, acc-0.1644, test loss-2.2713, acc-0.1789\n",
      "Iter-2150, train loss-2.2814, acc-0.1600, valid loss-2.2752, acc-0.1654, test loss-2.2712, acc-0.1790\n",
      "Iter-2160, train loss-2.2936, acc-0.1000, valid loss-2.2751, acc-0.1650, test loss-2.2711, acc-0.1797\n",
      "Iter-2170, train loss-2.2991, acc-0.1800, valid loss-2.2750, acc-0.1652, test loss-2.2710, acc-0.1797\n",
      "Iter-2180, train loss-2.2519, acc-0.1800, valid loss-2.2749, acc-0.1652, test loss-2.2709, acc-0.1799\n",
      "Iter-2190, train loss-2.2603, acc-0.2200, valid loss-2.2748, acc-0.1650, test loss-2.2708, acc-0.1799\n",
      "Iter-2200, train loss-2.2790, acc-0.1800, valid loss-2.2747, acc-0.1650, test loss-2.2707, acc-0.1804\n",
      "Iter-2210, train loss-2.2539, acc-0.2000, valid loss-2.2746, acc-0.1656, test loss-2.2706, acc-0.1809\n",
      "Iter-2220, train loss-2.2472, acc-0.2000, valid loss-2.2745, acc-0.1660, test loss-2.2705, acc-0.1808\n",
      "Iter-2230, train loss-2.2817, acc-0.2400, valid loss-2.2744, acc-0.1664, test loss-2.2704, acc-0.1809\n",
      "Iter-2240, train loss-2.2841, acc-0.1000, valid loss-2.2743, acc-0.1668, test loss-2.2703, acc-0.1812\n",
      "Iter-2250, train loss-2.2756, acc-0.1600, valid loss-2.2743, acc-0.1674, test loss-2.2702, acc-0.1816\n",
      "Iter-2260, train loss-2.2608, acc-0.2400, valid loss-2.2742, acc-0.1672, test loss-2.2701, acc-0.1820\n",
      "Iter-2270, train loss-2.2560, acc-0.2000, valid loss-2.2741, acc-0.1672, test loss-2.2700, acc-0.1828\n",
      "Iter-2280, train loss-2.2728, acc-0.2000, valid loss-2.2740, acc-0.1670, test loss-2.2699, acc-0.1829\n",
      "Iter-2290, train loss-2.2793, acc-0.1000, valid loss-2.2738, acc-0.1682, test loss-2.2698, acc-0.1832\n",
      "Iter-2300, train loss-2.2631, acc-0.1800, valid loss-2.2737, acc-0.1680, test loss-2.2697, acc-0.1829\n",
      "Iter-2310, train loss-2.2847, acc-0.1800, valid loss-2.2736, acc-0.1686, test loss-2.2696, acc-0.1831\n",
      "Iter-2320, train loss-2.2414, acc-0.2600, valid loss-2.2735, acc-0.1690, test loss-2.2695, acc-0.1837\n",
      "Iter-2330, train loss-2.2608, acc-0.1800, valid loss-2.2734, acc-0.1690, test loss-2.2694, acc-0.1838\n",
      "Iter-2340, train loss-2.2760, acc-0.1600, valid loss-2.2733, acc-0.1696, test loss-2.2693, acc-0.1846\n",
      "Iter-2350, train loss-2.2708, acc-0.2000, valid loss-2.2732, acc-0.1696, test loss-2.2692, acc-0.1847\n",
      "Iter-2360, train loss-2.2732, acc-0.1400, valid loss-2.2731, acc-0.1696, test loss-2.2691, acc-0.1850\n",
      "Iter-2370, train loss-2.2770, acc-0.2000, valid loss-2.2730, acc-0.1694, test loss-2.2690, acc-0.1849\n",
      "Iter-2380, train loss-2.3007, acc-0.1200, valid loss-2.2729, acc-0.1696, test loss-2.2689, acc-0.1848\n",
      "Iter-2390, train loss-2.2552, acc-0.1800, valid loss-2.2728, acc-0.1694, test loss-2.2688, acc-0.1849\n",
      "Iter-2400, train loss-2.2678, acc-0.2200, valid loss-2.2727, acc-0.1702, test loss-2.2687, acc-0.1854\n",
      "Iter-2410, train loss-2.2783, acc-0.1800, valid loss-2.2726, acc-0.1706, test loss-2.2686, acc-0.1855\n",
      "Iter-2420, train loss-2.2552, acc-0.2200, valid loss-2.2725, acc-0.1706, test loss-2.2685, acc-0.1863\n",
      "Iter-2430, train loss-2.2929, acc-0.1000, valid loss-2.2724, acc-0.1706, test loss-2.2684, acc-0.1866\n",
      "Iter-2440, train loss-2.2892, acc-0.1600, valid loss-2.2723, acc-0.1712, test loss-2.2683, acc-0.1871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.2418, acc-0.3000, valid loss-2.2722, acc-0.1714, test loss-2.2682, acc-0.1871\n",
      "Iter-2460, train loss-2.2758, acc-0.1400, valid loss-2.2721, acc-0.1722, test loss-2.2681, acc-0.1875\n",
      "Iter-2470, train loss-2.2384, acc-0.1600, valid loss-2.2720, acc-0.1718, test loss-2.2680, acc-0.1880\n",
      "Iter-2480, train loss-2.2528, acc-0.3200, valid loss-2.2719, acc-0.1718, test loss-2.2679, acc-0.1883\n",
      "Iter-2490, train loss-2.2556, acc-0.3000, valid loss-2.2718, acc-0.1722, test loss-2.2678, acc-0.1884\n",
      "Iter-2500, train loss-2.2877, acc-0.1400, valid loss-2.2717, acc-0.1724, test loss-2.2677, acc-0.1890\n",
      "Iter-2510, train loss-2.2711, acc-0.2000, valid loss-2.2716, acc-0.1730, test loss-2.2676, acc-0.1895\n",
      "Iter-2520, train loss-2.2327, acc-0.3000, valid loss-2.2715, acc-0.1724, test loss-2.2675, acc-0.1893\n",
      "Iter-2530, train loss-2.2800, acc-0.1800, valid loss-2.2714, acc-0.1728, test loss-2.2674, acc-0.1901\n",
      "Iter-2540, train loss-2.2864, acc-0.1200, valid loss-2.2713, acc-0.1728, test loss-2.2673, acc-0.1904\n",
      "Iter-2550, train loss-2.2404, acc-0.2600, valid loss-2.2712, acc-0.1724, test loss-2.2672, acc-0.1909\n",
      "Iter-2560, train loss-2.2527, acc-0.2200, valid loss-2.2711, acc-0.1728, test loss-2.2671, acc-0.1913\n",
      "Iter-2570, train loss-2.2456, acc-0.2400, valid loss-2.2710, acc-0.1730, test loss-2.2670, acc-0.1916\n",
      "Iter-2580, train loss-2.2836, acc-0.1000, valid loss-2.2709, acc-0.1736, test loss-2.2669, acc-0.1919\n",
      "Iter-2590, train loss-2.2274, acc-0.3000, valid loss-2.2708, acc-0.1740, test loss-2.2668, acc-0.1921\n",
      "Iter-2600, train loss-2.2506, acc-0.3000, valid loss-2.2707, acc-0.1744, test loss-2.2667, acc-0.1921\n",
      "Iter-2610, train loss-2.2574, acc-0.1400, valid loss-2.2706, acc-0.1742, test loss-2.2666, acc-0.1922\n",
      "Iter-2620, train loss-2.3062, acc-0.1000, valid loss-2.2705, acc-0.1740, test loss-2.2665, acc-0.1919\n",
      "Iter-2630, train loss-2.2727, acc-0.1800, valid loss-2.2704, acc-0.1740, test loss-2.2664, acc-0.1921\n",
      "Iter-2640, train loss-2.2734, acc-0.1800, valid loss-2.2703, acc-0.1748, test loss-2.2663, acc-0.1919\n",
      "Iter-2650, train loss-2.2832, acc-0.2200, valid loss-2.2702, acc-0.1748, test loss-2.2662, acc-0.1921\n",
      "Iter-2660, train loss-2.2496, acc-0.2600, valid loss-2.2701, acc-0.1750, test loss-2.2661, acc-0.1921\n",
      "Iter-2670, train loss-2.2533, acc-0.1600, valid loss-2.2700, acc-0.1752, test loss-2.2660, acc-0.1925\n",
      "Iter-2680, train loss-2.2426, acc-0.2600, valid loss-2.2699, acc-0.1750, test loss-2.2659, acc-0.1925\n",
      "Iter-2690, train loss-2.2471, acc-0.1400, valid loss-2.2698, acc-0.1754, test loss-2.2658, acc-0.1930\n",
      "Iter-2700, train loss-2.2725, acc-0.1600, valid loss-2.2697, acc-0.1756, test loss-2.2657, acc-0.1930\n",
      "Iter-2710, train loss-2.2486, acc-0.2200, valid loss-2.2696, acc-0.1756, test loss-2.2656, acc-0.1930\n",
      "Iter-2720, train loss-2.2878, acc-0.1800, valid loss-2.2695, acc-0.1756, test loss-2.2655, acc-0.1936\n",
      "Iter-2730, train loss-2.2862, acc-0.1200, valid loss-2.2694, acc-0.1760, test loss-2.2654, acc-0.1936\n",
      "Iter-2740, train loss-2.2789, acc-0.2400, valid loss-2.2693, acc-0.1764, test loss-2.2653, acc-0.1938\n",
      "Iter-2750, train loss-2.2887, acc-0.1400, valid loss-2.2692, acc-0.1770, test loss-2.2652, acc-0.1938\n",
      "Iter-2760, train loss-2.2535, acc-0.2200, valid loss-2.2691, acc-0.1770, test loss-2.2651, acc-0.1941\n",
      "Iter-2770, train loss-2.2591, acc-0.2200, valid loss-2.2690, acc-0.1780, test loss-2.2650, acc-0.1940\n",
      "Iter-2780, train loss-2.2591, acc-0.1600, valid loss-2.2689, acc-0.1784, test loss-2.2649, acc-0.1940\n",
      "Iter-2790, train loss-2.2821, acc-0.1200, valid loss-2.2688, acc-0.1786, test loss-2.2648, acc-0.1939\n",
      "Iter-2800, train loss-2.2471, acc-0.2200, valid loss-2.2687, acc-0.1786, test loss-2.2647, acc-0.1940\n",
      "Iter-2810, train loss-2.2556, acc-0.2200, valid loss-2.2686, acc-0.1784, test loss-2.2646, acc-0.1939\n",
      "Iter-2820, train loss-2.2419, acc-0.1400, valid loss-2.2685, acc-0.1786, test loss-2.2645, acc-0.1940\n",
      "Iter-2830, train loss-2.2691, acc-0.1600, valid loss-2.2684, acc-0.1788, test loss-2.2644, acc-0.1944\n",
      "Iter-2840, train loss-2.2861, acc-0.1200, valid loss-2.2683, acc-0.1792, test loss-2.2643, acc-0.1950\n",
      "Iter-2850, train loss-2.2687, acc-0.2200, valid loss-2.2682, acc-0.1792, test loss-2.2642, acc-0.1952\n",
      "Iter-2860, train loss-2.2469, acc-0.2200, valid loss-2.2681, acc-0.1794, test loss-2.2641, acc-0.1954\n",
      "Iter-2870, train loss-2.2684, acc-0.2200, valid loss-2.2681, acc-0.1792, test loss-2.2640, acc-0.1954\n",
      "Iter-2880, train loss-2.2662, acc-0.2200, valid loss-2.2680, acc-0.1794, test loss-2.2639, acc-0.1959\n",
      "Iter-2890, train loss-2.2581, acc-0.1800, valid loss-2.2678, acc-0.1794, test loss-2.2638, acc-0.1958\n",
      "Iter-2900, train loss-2.2770, acc-0.2000, valid loss-2.2678, acc-0.1800, test loss-2.2637, acc-0.1961\n",
      "Iter-2910, train loss-2.2764, acc-0.1600, valid loss-2.2677, acc-0.1796, test loss-2.2636, acc-0.1962\n",
      "Iter-2920, train loss-2.2663, acc-0.2400, valid loss-2.2676, acc-0.1798, test loss-2.2635, acc-0.1960\n",
      "Iter-2930, train loss-2.2339, acc-0.2400, valid loss-2.2675, acc-0.1798, test loss-2.2634, acc-0.1963\n",
      "Iter-2940, train loss-2.2836, acc-0.1200, valid loss-2.2674, acc-0.1800, test loss-2.2633, acc-0.1963\n",
      "Iter-2950, train loss-2.2606, acc-0.2400, valid loss-2.2673, acc-0.1802, test loss-2.2632, acc-0.1966\n",
      "Iter-2960, train loss-2.2606, acc-0.1800, valid loss-2.2672, acc-0.1804, test loss-2.2631, acc-0.1967\n",
      "Iter-2970, train loss-2.2530, acc-0.2600, valid loss-2.2671, acc-0.1800, test loss-2.2630, acc-0.1971\n",
      "Iter-2980, train loss-2.2817, acc-0.2000, valid loss-2.2670, acc-0.1802, test loss-2.2629, acc-0.1975\n",
      "Iter-2990, train loss-2.2600, acc-0.1600, valid loss-2.2669, acc-0.1802, test loss-2.2629, acc-0.1975\n",
      "Iter-3000, train loss-2.2479, acc-0.1800, valid loss-2.2668, acc-0.1804, test loss-2.2628, acc-0.1982\n",
      "Iter-3010, train loss-2.2643, acc-0.2200, valid loss-2.2667, acc-0.1810, test loss-2.2627, acc-0.1982\n",
      "Iter-3020, train loss-2.2499, acc-0.2400, valid loss-2.2666, acc-0.1808, test loss-2.2626, acc-0.1982\n",
      "Iter-3030, train loss-2.2625, acc-0.1600, valid loss-2.2665, acc-0.1814, test loss-2.2625, acc-0.1987\n",
      "Iter-3040, train loss-2.2594, acc-0.2000, valid loss-2.2664, acc-0.1814, test loss-2.2624, acc-0.1986\n",
      "Iter-3050, train loss-2.2634, acc-0.2200, valid loss-2.2663, acc-0.1816, test loss-2.2623, acc-0.1987\n",
      "Iter-3060, train loss-2.2480, acc-0.2000, valid loss-2.2662, acc-0.1818, test loss-2.2622, acc-0.1987\n",
      "Iter-3070, train loss-2.2475, acc-0.2600, valid loss-2.2661, acc-0.1816, test loss-2.2621, acc-0.1992\n",
      "Iter-3080, train loss-2.2456, acc-0.1800, valid loss-2.2660, acc-0.1818, test loss-2.2620, acc-0.1991\n",
      "Iter-3090, train loss-2.2722, acc-0.2200, valid loss-2.2659, acc-0.1822, test loss-2.2619, acc-0.1991\n",
      "Iter-3100, train loss-2.2948, acc-0.1600, valid loss-2.2658, acc-0.1820, test loss-2.2618, acc-0.1991\n",
      "Iter-3110, train loss-2.2674, acc-0.2800, valid loss-2.2657, acc-0.1818, test loss-2.2617, acc-0.1991\n",
      "Iter-3120, train loss-2.2770, acc-0.2200, valid loss-2.2656, acc-0.1818, test loss-2.2616, acc-0.1993\n",
      "Iter-3130, train loss-2.2776, acc-0.1800, valid loss-2.2655, acc-0.1816, test loss-2.2615, acc-0.1995\n",
      "Iter-3140, train loss-2.2559, acc-0.2200, valid loss-2.2654, acc-0.1820, test loss-2.2614, acc-0.1999\n",
      "Iter-3150, train loss-2.2669, acc-0.1800, valid loss-2.2653, acc-0.1826, test loss-2.2613, acc-0.2001\n",
      "Iter-3160, train loss-2.2621, acc-0.2200, valid loss-2.2652, acc-0.1826, test loss-2.2612, acc-0.1999\n",
      "Iter-3170, train loss-2.2524, acc-0.2000, valid loss-2.2651, acc-0.1822, test loss-2.2611, acc-0.1999\n",
      "Iter-3180, train loss-2.2476, acc-0.2200, valid loss-2.2650, acc-0.1822, test loss-2.2610, acc-0.2001\n",
      "Iter-3190, train loss-2.2655, acc-0.2000, valid loss-2.2649, acc-0.1820, test loss-2.2609, acc-0.2004\n",
      "Iter-3200, train loss-2.2947, acc-0.1800, valid loss-2.2648, acc-0.1824, test loss-2.2608, acc-0.2003\n",
      "Iter-3210, train loss-2.2625, acc-0.1600, valid loss-2.2647, acc-0.1824, test loss-2.2607, acc-0.2002\n",
      "Iter-3220, train loss-2.2757, acc-0.2400, valid loss-2.2646, acc-0.1828, test loss-2.2606, acc-0.2005\n",
      "Iter-3230, train loss-2.2484, acc-0.2600, valid loss-2.2645, acc-0.1830, test loss-2.2605, acc-0.2004\n",
      "Iter-3240, train loss-2.2675, acc-0.2200, valid loss-2.2644, acc-0.1830, test loss-2.2604, acc-0.2005\n",
      "Iter-3250, train loss-2.2483, acc-0.1400, valid loss-2.2643, acc-0.1832, test loss-2.2603, acc-0.2005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.2434, acc-0.2200, valid loss-2.2643, acc-0.1832, test loss-2.2602, acc-0.2004\n",
      "Iter-3270, train loss-2.2597, acc-0.2200, valid loss-2.2642, acc-0.1834, test loss-2.2601, acc-0.2007\n",
      "Iter-3280, train loss-2.2748, acc-0.2200, valid loss-2.2641, acc-0.1832, test loss-2.2600, acc-0.2008\n",
      "Iter-3290, train loss-2.2645, acc-0.2800, valid loss-2.2640, acc-0.1832, test loss-2.2599, acc-0.2008\n",
      "Iter-3300, train loss-2.2600, acc-0.2400, valid loss-2.2639, acc-0.1828, test loss-2.2598, acc-0.2009\n",
      "Iter-3310, train loss-2.2299, acc-0.2800, valid loss-2.2638, acc-0.1828, test loss-2.2597, acc-0.2013\n",
      "Iter-3320, train loss-2.2537, acc-0.1800, valid loss-2.2637, acc-0.1834, test loss-2.2596, acc-0.2015\n",
      "Iter-3330, train loss-2.2772, acc-0.1200, valid loss-2.2636, acc-0.1834, test loss-2.2595, acc-0.2012\n",
      "Iter-3340, train loss-2.3140, acc-0.1400, valid loss-2.2635, acc-0.1830, test loss-2.2594, acc-0.2015\n",
      "Iter-3350, train loss-2.2421, acc-0.2200, valid loss-2.2634, acc-0.1830, test loss-2.2593, acc-0.2016\n",
      "Iter-3360, train loss-2.2582, acc-0.2000, valid loss-2.2633, acc-0.1828, test loss-2.2592, acc-0.2012\n",
      "Iter-3370, train loss-2.2622, acc-0.1600, valid loss-2.2632, acc-0.1832, test loss-2.2591, acc-0.2012\n",
      "Iter-3380, train loss-2.2520, acc-0.1600, valid loss-2.2631, acc-0.1832, test loss-2.2590, acc-0.2012\n",
      "Iter-3390, train loss-2.2538, acc-0.2200, valid loss-2.2630, acc-0.1834, test loss-2.2589, acc-0.2014\n",
      "Iter-3400, train loss-2.2654, acc-0.2600, valid loss-2.2629, acc-0.1836, test loss-2.2588, acc-0.2016\n",
      "Iter-3410, train loss-2.2431, acc-0.2800, valid loss-2.2628, acc-0.1834, test loss-2.2587, acc-0.2014\n",
      "Iter-3420, train loss-2.2354, acc-0.2400, valid loss-2.2627, acc-0.1834, test loss-2.2586, acc-0.2015\n",
      "Iter-3430, train loss-2.2588, acc-0.2800, valid loss-2.2626, acc-0.1840, test loss-2.2585, acc-0.2015\n",
      "Iter-3440, train loss-2.2258, acc-0.2400, valid loss-2.2625, acc-0.1840, test loss-2.2584, acc-0.2016\n",
      "Iter-3450, train loss-2.2408, acc-0.2200, valid loss-2.2624, acc-0.1840, test loss-2.2583, acc-0.2019\n",
      "Iter-3460, train loss-2.2621, acc-0.2400, valid loss-2.2623, acc-0.1846, test loss-2.2582, acc-0.2023\n",
      "Iter-3470, train loss-2.2366, acc-0.2400, valid loss-2.2622, acc-0.1846, test loss-2.2581, acc-0.2023\n",
      "Iter-3480, train loss-2.2490, acc-0.2000, valid loss-2.2621, acc-0.1848, test loss-2.2580, acc-0.2025\n",
      "Iter-3490, train loss-2.2659, acc-0.1400, valid loss-2.2620, acc-0.1852, test loss-2.2579, acc-0.2026\n",
      "Iter-3500, train loss-2.2231, acc-0.2800, valid loss-2.2619, acc-0.1848, test loss-2.2578, acc-0.2028\n",
      "Iter-3510, train loss-2.2898, acc-0.0800, valid loss-2.2618, acc-0.1852, test loss-2.2577, acc-0.2028\n",
      "Iter-3520, train loss-2.2715, acc-0.2000, valid loss-2.2617, acc-0.1854, test loss-2.2576, acc-0.2033\n",
      "Iter-3530, train loss-2.2593, acc-0.1000, valid loss-2.2616, acc-0.1854, test loss-2.2575, acc-0.2034\n",
      "Iter-3540, train loss-2.2972, acc-0.1200, valid loss-2.2615, acc-0.1852, test loss-2.2574, acc-0.2035\n",
      "Iter-3550, train loss-2.2307, acc-0.2600, valid loss-2.2614, acc-0.1854, test loss-2.2573, acc-0.2034\n",
      "Iter-3560, train loss-2.2474, acc-0.1400, valid loss-2.2613, acc-0.1854, test loss-2.2572, acc-0.2032\n",
      "Iter-3570, train loss-2.2771, acc-0.2400, valid loss-2.2612, acc-0.1854, test loss-2.2571, acc-0.2031\n",
      "Iter-3580, train loss-2.2448, acc-0.2200, valid loss-2.2611, acc-0.1856, test loss-2.2570, acc-0.2033\n",
      "Iter-3590, train loss-2.2655, acc-0.1600, valid loss-2.2610, acc-0.1856, test loss-2.2569, acc-0.2034\n",
      "Iter-3600, train loss-2.2442, acc-0.2000, valid loss-2.2609, acc-0.1858, test loss-2.2568, acc-0.2033\n",
      "Iter-3610, train loss-2.2499, acc-0.2800, valid loss-2.2608, acc-0.1858, test loss-2.2567, acc-0.2033\n",
      "Iter-3620, train loss-2.2629, acc-0.1800, valid loss-2.2607, acc-0.1860, test loss-2.2566, acc-0.2033\n",
      "Iter-3630, train loss-2.2613, acc-0.1800, valid loss-2.2606, acc-0.1858, test loss-2.2565, acc-0.2033\n",
      "Iter-3640, train loss-2.2654, acc-0.1600, valid loss-2.2605, acc-0.1860, test loss-2.2564, acc-0.2032\n",
      "Iter-3650, train loss-2.2591, acc-0.2200, valid loss-2.2604, acc-0.1862, test loss-2.2563, acc-0.2039\n",
      "Iter-3660, train loss-2.2520, acc-0.1600, valid loss-2.2603, acc-0.1860, test loss-2.2563, acc-0.2036\n",
      "Iter-3670, train loss-2.2626, acc-0.1800, valid loss-2.2602, acc-0.1860, test loss-2.2562, acc-0.2039\n",
      "Iter-3680, train loss-2.2774, acc-0.1200, valid loss-2.2601, acc-0.1860, test loss-2.2561, acc-0.2043\n",
      "Iter-3690, train loss-2.2655, acc-0.1000, valid loss-2.2600, acc-0.1860, test loss-2.2560, acc-0.2041\n",
      "Iter-3700, train loss-2.2450, acc-0.2200, valid loss-2.2599, acc-0.1860, test loss-2.2559, acc-0.2044\n",
      "Iter-3710, train loss-2.2349, acc-0.2600, valid loss-2.2598, acc-0.1858, test loss-2.2558, acc-0.2046\n",
      "Iter-3720, train loss-2.2728, acc-0.1600, valid loss-2.2597, acc-0.1864, test loss-2.2557, acc-0.2045\n",
      "Iter-3730, train loss-2.2462, acc-0.2000, valid loss-2.2596, acc-0.1864, test loss-2.2556, acc-0.2046\n",
      "Iter-3740, train loss-2.2264, acc-0.3400, valid loss-2.2595, acc-0.1864, test loss-2.2555, acc-0.2047\n",
      "Iter-3750, train loss-2.2512, acc-0.2200, valid loss-2.2594, acc-0.1864, test loss-2.2554, acc-0.2047\n",
      "Iter-3760, train loss-2.2811, acc-0.1400, valid loss-2.2593, acc-0.1868, test loss-2.2553, acc-0.2049\n",
      "Iter-3770, train loss-2.2753, acc-0.1800, valid loss-2.2593, acc-0.1870, test loss-2.2552, acc-0.2047\n",
      "Iter-3780, train loss-2.2663, acc-0.1600, valid loss-2.2592, acc-0.1870, test loss-2.2551, acc-0.2049\n",
      "Iter-3790, train loss-2.2639, acc-0.1400, valid loss-2.2591, acc-0.1870, test loss-2.2550, acc-0.2052\n",
      "Iter-3800, train loss-2.2567, acc-0.2000, valid loss-2.2590, acc-0.1872, test loss-2.2549, acc-0.2054\n",
      "Iter-3810, train loss-2.2597, acc-0.1600, valid loss-2.2589, acc-0.1872, test loss-2.2548, acc-0.2055\n",
      "Iter-3820, train loss-2.2470, acc-0.2200, valid loss-2.2588, acc-0.1872, test loss-2.2547, acc-0.2055\n",
      "Iter-3830, train loss-2.2709, acc-0.2000, valid loss-2.2587, acc-0.1872, test loss-2.2546, acc-0.2057\n",
      "Iter-3840, train loss-2.2842, acc-0.2200, valid loss-2.2586, acc-0.1876, test loss-2.2545, acc-0.2059\n",
      "Iter-3850, train loss-2.2362, acc-0.2000, valid loss-2.2585, acc-0.1880, test loss-2.2544, acc-0.2056\n",
      "Iter-3860, train loss-2.2629, acc-0.1200, valid loss-2.2584, acc-0.1882, test loss-2.2543, acc-0.2058\n",
      "Iter-3870, train loss-2.2829, acc-0.1000, valid loss-2.2583, acc-0.1882, test loss-2.2542, acc-0.2057\n",
      "Iter-3880, train loss-2.2275, acc-0.2400, valid loss-2.2582, acc-0.1880, test loss-2.2541, acc-0.2059\n",
      "Iter-3890, train loss-2.2460, acc-0.1800, valid loss-2.2581, acc-0.1882, test loss-2.2540, acc-0.2061\n",
      "Iter-3900, train loss-2.2899, acc-0.1200, valid loss-2.2580, acc-0.1882, test loss-2.2539, acc-0.2063\n",
      "Iter-3910, train loss-2.2588, acc-0.2000, valid loss-2.2579, acc-0.1886, test loss-2.2538, acc-0.2066\n",
      "Iter-3920, train loss-2.2617, acc-0.1400, valid loss-2.2578, acc-0.1886, test loss-2.2537, acc-0.2069\n",
      "Iter-3930, train loss-2.2705, acc-0.1000, valid loss-2.2577, acc-0.1886, test loss-2.2536, acc-0.2069\n",
      "Iter-3940, train loss-2.2754, acc-0.1800, valid loss-2.2576, acc-0.1886, test loss-2.2536, acc-0.2070\n",
      "Iter-3950, train loss-2.2833, acc-0.1400, valid loss-2.2576, acc-0.1890, test loss-2.2535, acc-0.2070\n",
      "Iter-3960, train loss-2.2586, acc-0.2600, valid loss-2.2575, acc-0.1890, test loss-2.2534, acc-0.2071\n",
      "Iter-3970, train loss-2.2498, acc-0.2000, valid loss-2.2574, acc-0.1890, test loss-2.2533, acc-0.2073\n",
      "Iter-3980, train loss-2.2556, acc-0.1400, valid loss-2.2573, acc-0.1890, test loss-2.2532, acc-0.2071\n",
      "Iter-3990, train loss-2.2653, acc-0.2000, valid loss-2.2572, acc-0.1888, test loss-2.2531, acc-0.2070\n",
      "Iter-4000, train loss-2.2539, acc-0.1800, valid loss-2.2571, acc-0.1888, test loss-2.2530, acc-0.2071\n",
      "Iter-4010, train loss-2.2299, acc-0.2600, valid loss-2.2570, acc-0.1888, test loss-2.2529, acc-0.2069\n",
      "Iter-4020, train loss-2.2632, acc-0.1800, valid loss-2.2569, acc-0.1892, test loss-2.2528, acc-0.2070\n",
      "Iter-4030, train loss-2.2524, acc-0.1600, valid loss-2.2568, acc-0.1892, test loss-2.2527, acc-0.2072\n",
      "Iter-4040, train loss-2.2597, acc-0.1200, valid loss-2.2567, acc-0.1888, test loss-2.2526, acc-0.2073\n",
      "Iter-4050, train loss-2.2418, acc-0.3000, valid loss-2.2566, acc-0.1886, test loss-2.2525, acc-0.2072\n",
      "Iter-4060, train loss-2.2792, acc-0.1600, valid loss-2.2565, acc-0.1890, test loss-2.2524, acc-0.2071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.2081, acc-0.3600, valid loss-2.2564, acc-0.1890, test loss-2.2523, acc-0.2071\n",
      "Iter-4080, train loss-2.2504, acc-0.2000, valid loss-2.2563, acc-0.1890, test loss-2.2522, acc-0.2072\n",
      "Iter-4090, train loss-2.2690, acc-0.2800, valid loss-2.2562, acc-0.1896, test loss-2.2521, acc-0.2072\n",
      "Iter-4100, train loss-2.2569, acc-0.1600, valid loss-2.2561, acc-0.1898, test loss-2.2520, acc-0.2074\n",
      "Iter-4110, train loss-2.2458, acc-0.2000, valid loss-2.2560, acc-0.1896, test loss-2.2519, acc-0.2077\n",
      "Iter-4120, train loss-2.2454, acc-0.2000, valid loss-2.2559, acc-0.1896, test loss-2.2518, acc-0.2077\n",
      "Iter-4130, train loss-2.2683, acc-0.2600, valid loss-2.2558, acc-0.1900, test loss-2.2517, acc-0.2077\n",
      "Iter-4140, train loss-2.2447, acc-0.2200, valid loss-2.2557, acc-0.1900, test loss-2.2516, acc-0.2078\n",
      "Iter-4150, train loss-2.2740, acc-0.2600, valid loss-2.2556, acc-0.1898, test loss-2.2515, acc-0.2079\n",
      "Iter-4160, train loss-2.2348, acc-0.2400, valid loss-2.2555, acc-0.1898, test loss-2.2514, acc-0.2078\n",
      "Iter-4170, train loss-2.2438, acc-0.2000, valid loss-2.2554, acc-0.1900, test loss-2.2513, acc-0.2078\n",
      "Iter-4180, train loss-2.2547, acc-0.1800, valid loss-2.2553, acc-0.1896, test loss-2.2512, acc-0.2080\n",
      "Iter-4190, train loss-2.2349, acc-0.2600, valid loss-2.2552, acc-0.1892, test loss-2.2511, acc-0.2081\n",
      "Iter-4200, train loss-2.2559, acc-0.1600, valid loss-2.2551, acc-0.1892, test loss-2.2510, acc-0.2084\n",
      "Iter-4210, train loss-2.2677, acc-0.2000, valid loss-2.2550, acc-0.1898, test loss-2.2509, acc-0.2085\n",
      "Iter-4220, train loss-2.2482, acc-0.1600, valid loss-2.2549, acc-0.1900, test loss-2.2508, acc-0.2086\n",
      "Iter-4230, train loss-2.2795, acc-0.1000, valid loss-2.2549, acc-0.1898, test loss-2.2507, acc-0.2087\n",
      "Iter-4240, train loss-2.2400, acc-0.2400, valid loss-2.2548, acc-0.1902, test loss-2.2506, acc-0.2086\n",
      "Iter-4250, train loss-2.2605, acc-0.2600, valid loss-2.2546, acc-0.1900, test loss-2.2505, acc-0.2087\n",
      "Iter-4260, train loss-2.2828, acc-0.2200, valid loss-2.2546, acc-0.1904, test loss-2.2504, acc-0.2085\n",
      "Iter-4270, train loss-2.2609, acc-0.1800, valid loss-2.2545, acc-0.1910, test loss-2.2503, acc-0.2085\n",
      "Iter-4280, train loss-2.2482, acc-0.2400, valid loss-2.2544, acc-0.1912, test loss-2.2502, acc-0.2089\n",
      "Iter-4290, train loss-2.2779, acc-0.0800, valid loss-2.2543, acc-0.1910, test loss-2.2501, acc-0.2087\n",
      "Iter-4300, train loss-2.2562, acc-0.1800, valid loss-2.2542, acc-0.1910, test loss-2.2501, acc-0.2085\n",
      "Iter-4310, train loss-2.2492, acc-0.1400, valid loss-2.2541, acc-0.1912, test loss-2.2500, acc-0.2089\n",
      "Iter-4320, train loss-2.2382, acc-0.2200, valid loss-2.2540, acc-0.1914, test loss-2.2499, acc-0.2087\n",
      "Iter-4330, train loss-2.2407, acc-0.2000, valid loss-2.2539, acc-0.1914, test loss-2.2498, acc-0.2088\n",
      "Iter-4340, train loss-2.2447, acc-0.2200, valid loss-2.2538, acc-0.1914, test loss-2.2497, acc-0.2089\n",
      "Iter-4350, train loss-2.2364, acc-0.2000, valid loss-2.2537, acc-0.1916, test loss-2.2496, acc-0.2087\n",
      "Iter-4360, train loss-2.2400, acc-0.2800, valid loss-2.2536, acc-0.1914, test loss-2.2495, acc-0.2090\n",
      "Iter-4370, train loss-2.2376, acc-0.2000, valid loss-2.2535, acc-0.1918, test loss-2.2494, acc-0.2093\n",
      "Iter-4380, train loss-2.2454, acc-0.1800, valid loss-2.2534, acc-0.1918, test loss-2.2493, acc-0.2094\n",
      "Iter-4390, train loss-2.2682, acc-0.1600, valid loss-2.2533, acc-0.1918, test loss-2.2492, acc-0.2092\n",
      "Iter-4400, train loss-2.2531, acc-0.2000, valid loss-2.2532, acc-0.1924, test loss-2.2491, acc-0.2094\n",
      "Iter-4410, train loss-2.2363, acc-0.2400, valid loss-2.2531, acc-0.1926, test loss-2.2490, acc-0.2097\n",
      "Iter-4420, train loss-2.2455, acc-0.2200, valid loss-2.2530, acc-0.1922, test loss-2.2489, acc-0.2095\n",
      "Iter-4430, train loss-2.2864, acc-0.1400, valid loss-2.2529, acc-0.1922, test loss-2.2488, acc-0.2095\n",
      "Iter-4440, train loss-2.2706, acc-0.2200, valid loss-2.2528, acc-0.1926, test loss-2.2487, acc-0.2095\n",
      "Iter-4450, train loss-2.2203, acc-0.2400, valid loss-2.2527, acc-0.1926, test loss-2.2486, acc-0.2095\n",
      "Iter-4460, train loss-2.2386, acc-0.2200, valid loss-2.2526, acc-0.1926, test loss-2.2485, acc-0.2094\n",
      "Iter-4470, train loss-2.2610, acc-0.1400, valid loss-2.2525, acc-0.1926, test loss-2.2484, acc-0.2095\n",
      "Iter-4480, train loss-2.2508, acc-0.1600, valid loss-2.2524, acc-0.1930, test loss-2.2483, acc-0.2095\n",
      "Iter-4490, train loss-2.2767, acc-0.1200, valid loss-2.2523, acc-0.1928, test loss-2.2482, acc-0.2094\n",
      "Iter-4500, train loss-2.2669, acc-0.1800, valid loss-2.2522, acc-0.1928, test loss-2.2481, acc-0.2095\n",
      "Iter-4510, train loss-2.2376, acc-0.2600, valid loss-2.2522, acc-0.1932, test loss-2.2480, acc-0.2096\n",
      "Iter-4520, train loss-2.2434, acc-0.2800, valid loss-2.2521, acc-0.1930, test loss-2.2479, acc-0.2099\n",
      "Iter-4530, train loss-2.2835, acc-0.1400, valid loss-2.2520, acc-0.1932, test loss-2.2478, acc-0.2098\n",
      "Iter-4540, train loss-2.2363, acc-0.2800, valid loss-2.2519, acc-0.1932, test loss-2.2477, acc-0.2097\n",
      "Iter-4550, train loss-2.2556, acc-0.1400, valid loss-2.2518, acc-0.1934, test loss-2.2476, acc-0.2098\n",
      "Iter-4560, train loss-2.2456, acc-0.2600, valid loss-2.2517, acc-0.1936, test loss-2.2475, acc-0.2098\n",
      "Iter-4570, train loss-2.2453, acc-0.1800, valid loss-2.2516, acc-0.1936, test loss-2.2474, acc-0.2101\n",
      "Iter-4580, train loss-2.2349, acc-0.2800, valid loss-2.2515, acc-0.1936, test loss-2.2474, acc-0.2100\n",
      "Iter-4590, train loss-2.2265, acc-0.2200, valid loss-2.2514, acc-0.1936, test loss-2.2473, acc-0.2101\n",
      "Iter-4600, train loss-2.2244, acc-0.2800, valid loss-2.2513, acc-0.1938, test loss-2.2471, acc-0.2103\n",
      "Iter-4610, train loss-2.2517, acc-0.2200, valid loss-2.2512, acc-0.1936, test loss-2.2471, acc-0.2106\n",
      "Iter-4620, train loss-2.2663, acc-0.2000, valid loss-2.2511, acc-0.1936, test loss-2.2470, acc-0.2102\n",
      "Iter-4630, train loss-2.2146, acc-0.2600, valid loss-2.2510, acc-0.1934, test loss-2.2469, acc-0.2103\n",
      "Iter-4640, train loss-2.2494, acc-0.1400, valid loss-2.2509, acc-0.1932, test loss-2.2468, acc-0.2105\n",
      "Iter-4650, train loss-2.2465, acc-0.2600, valid loss-2.2508, acc-0.1930, test loss-2.2467, acc-0.2106\n",
      "Iter-4660, train loss-2.2408, acc-0.2000, valid loss-2.2507, acc-0.1934, test loss-2.2466, acc-0.2109\n",
      "Iter-4670, train loss-2.2377, acc-0.2000, valid loss-2.2506, acc-0.1948, test loss-2.2465, acc-0.2108\n",
      "Iter-4680, train loss-2.2630, acc-0.1800, valid loss-2.2505, acc-0.1946, test loss-2.2464, acc-0.2109\n",
      "Iter-4690, train loss-2.2467, acc-0.2000, valid loss-2.2504, acc-0.1946, test loss-2.2463, acc-0.2107\n",
      "Iter-4700, train loss-2.2459, acc-0.2000, valid loss-2.2503, acc-0.1946, test loss-2.2462, acc-0.2108\n",
      "Iter-4710, train loss-2.2386, acc-0.2200, valid loss-2.2502, acc-0.1944, test loss-2.2461, acc-0.2107\n",
      "Iter-4720, train loss-2.2328, acc-0.1800, valid loss-2.2502, acc-0.1944, test loss-2.2460, acc-0.2107\n",
      "Iter-4730, train loss-2.2403, acc-0.1600, valid loss-2.2501, acc-0.1958, test loss-2.2459, acc-0.2109\n",
      "Iter-4740, train loss-2.2522, acc-0.1800, valid loss-2.2500, acc-0.1956, test loss-2.2458, acc-0.2110\n",
      "Iter-4750, train loss-2.2415, acc-0.1800, valid loss-2.2499, acc-0.1958, test loss-2.2457, acc-0.2111\n",
      "Iter-4760, train loss-2.2429, acc-0.1800, valid loss-2.2498, acc-0.1958, test loss-2.2456, acc-0.2111\n",
      "Iter-4770, train loss-2.2245, acc-0.2200, valid loss-2.2497, acc-0.1958, test loss-2.2455, acc-0.2109\n",
      "Iter-4780, train loss-2.2226, acc-0.2200, valid loss-2.2496, acc-0.1958, test loss-2.2454, acc-0.2112\n",
      "Iter-4790, train loss-2.2544, acc-0.1000, valid loss-2.2495, acc-0.1962, test loss-2.2453, acc-0.2113\n",
      "Iter-4800, train loss-2.2473, acc-0.2200, valid loss-2.2494, acc-0.1966, test loss-2.2452, acc-0.2115\n",
      "Iter-4810, train loss-2.2294, acc-0.2800, valid loss-2.2493, acc-0.1968, test loss-2.2451, acc-0.2112\n",
      "Iter-4820, train loss-2.2559, acc-0.2400, valid loss-2.2492, acc-0.1966, test loss-2.2451, acc-0.2115\n",
      "Iter-4830, train loss-2.2522, acc-0.1800, valid loss-2.2491, acc-0.1968, test loss-2.2450, acc-0.2117\n",
      "Iter-4840, train loss-2.2617, acc-0.1400, valid loss-2.2490, acc-0.1966, test loss-2.2449, acc-0.2119\n",
      "Iter-4850, train loss-2.2438, acc-0.2000, valid loss-2.2489, acc-0.1970, test loss-2.2448, acc-0.2120\n",
      "Iter-4860, train loss-2.2600, acc-0.1600, valid loss-2.2488, acc-0.1970, test loss-2.2447, acc-0.2119\n",
      "Iter-4870, train loss-2.2710, acc-0.2000, valid loss-2.2488, acc-0.1972, test loss-2.2446, acc-0.2120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-2.2658, acc-0.1800, valid loss-2.2487, acc-0.1976, test loss-2.2445, acc-0.2121\n",
      "Iter-4890, train loss-2.2490, acc-0.2000, valid loss-2.2485, acc-0.1974, test loss-2.2444, acc-0.2122\n",
      "Iter-4900, train loss-2.2486, acc-0.1400, valid loss-2.2485, acc-0.1974, test loss-2.2443, acc-0.2122\n",
      "Iter-4910, train loss-2.2426, acc-0.3000, valid loss-2.2484, acc-0.1974, test loss-2.2442, acc-0.2122\n",
      "Iter-4920, train loss-2.2405, acc-0.2000, valid loss-2.2483, acc-0.1974, test loss-2.2441, acc-0.2120\n",
      "Iter-4930, train loss-2.2479, acc-0.2400, valid loss-2.2482, acc-0.1976, test loss-2.2440, acc-0.2121\n",
      "Iter-4940, train loss-2.2251, acc-0.3200, valid loss-2.2481, acc-0.1976, test loss-2.2439, acc-0.2122\n",
      "Iter-4950, train loss-2.2680, acc-0.1400, valid loss-2.2480, acc-0.1976, test loss-2.2438, acc-0.2120\n",
      "Iter-4960, train loss-2.2378, acc-0.2400, valid loss-2.2479, acc-0.1982, test loss-2.2437, acc-0.2124\n",
      "Iter-4970, train loss-2.2272, acc-0.3000, valid loss-2.2478, acc-0.1982, test loss-2.2436, acc-0.2123\n",
      "Iter-4980, train loss-2.2534, acc-0.1400, valid loss-2.2477, acc-0.1986, test loss-2.2435, acc-0.2124\n",
      "Iter-4990, train loss-2.2454, acc-0.2000, valid loss-2.2476, acc-0.1986, test loss-2.2434, acc-0.2125\n",
      "Iter-5000, train loss-2.2214, acc-0.3200, valid loss-2.2475, acc-0.1990, test loss-2.2433, acc-0.2126\n",
      "Iter-5010, train loss-2.2441, acc-0.2200, valid loss-2.2474, acc-0.1986, test loss-2.2432, acc-0.2125\n",
      "Iter-5020, train loss-2.2402, acc-0.2200, valid loss-2.2473, acc-0.1988, test loss-2.2431, acc-0.2124\n",
      "Iter-5030, train loss-2.2152, acc-0.2200, valid loss-2.2472, acc-0.1988, test loss-2.2430, acc-0.2124\n",
      "Iter-5040, train loss-2.2475, acc-0.2000, valid loss-2.2471, acc-0.1986, test loss-2.2430, acc-0.2126\n",
      "Iter-5050, train loss-2.2380, acc-0.1600, valid loss-2.2470, acc-0.1990, test loss-2.2429, acc-0.2126\n",
      "Iter-5060, train loss-2.2375, acc-0.3200, valid loss-2.2469, acc-0.1992, test loss-2.2428, acc-0.2126\n",
      "Iter-5070, train loss-2.2367, acc-0.2200, valid loss-2.2468, acc-0.1992, test loss-2.2426, acc-0.2121\n",
      "Iter-5080, train loss-2.2392, acc-0.1600, valid loss-2.2467, acc-0.1994, test loss-2.2426, acc-0.2120\n",
      "Iter-5090, train loss-2.2520, acc-0.1800, valid loss-2.2466, acc-0.1994, test loss-2.2425, acc-0.2119\n",
      "Iter-5100, train loss-2.2835, acc-0.1600, valid loss-2.2465, acc-0.1996, test loss-2.2424, acc-0.2119\n",
      "Iter-5110, train loss-2.2278, acc-0.2200, valid loss-2.2464, acc-0.1998, test loss-2.2423, acc-0.2120\n",
      "Iter-5120, train loss-2.2407, acc-0.1800, valid loss-2.2463, acc-0.2000, test loss-2.2422, acc-0.2120\n",
      "Iter-5130, train loss-2.2314, acc-0.3200, valid loss-2.2462, acc-0.2010, test loss-2.2421, acc-0.2121\n",
      "Iter-5140, train loss-2.2370, acc-0.2400, valid loss-2.2461, acc-0.2016, test loss-2.2420, acc-0.2121\n",
      "Iter-5150, train loss-2.2342, acc-0.2800, valid loss-2.2460, acc-0.2016, test loss-2.2419, acc-0.2123\n",
      "Iter-5160, train loss-2.2742, acc-0.0800, valid loss-2.2459, acc-0.2022, test loss-2.2418, acc-0.2122\n",
      "Iter-5170, train loss-2.2601, acc-0.1600, valid loss-2.2458, acc-0.2020, test loss-2.2417, acc-0.2121\n",
      "Iter-5180, train loss-2.2213, acc-0.2600, valid loss-2.2457, acc-0.2026, test loss-2.2416, acc-0.2123\n",
      "Iter-5190, train loss-2.2679, acc-0.1800, valid loss-2.2456, acc-0.2026, test loss-2.2415, acc-0.2123\n",
      "Iter-5200, train loss-2.2408, acc-0.2400, valid loss-2.2455, acc-0.2024, test loss-2.2414, acc-0.2124\n",
      "Iter-5210, train loss-2.2414, acc-0.2000, valid loss-2.2455, acc-0.2024, test loss-2.2413, acc-0.2125\n",
      "Iter-5220, train loss-2.2440, acc-0.1400, valid loss-2.2454, acc-0.2026, test loss-2.2412, acc-0.2129\n",
      "Iter-5230, train loss-2.2387, acc-0.2800, valid loss-2.2453, acc-0.2024, test loss-2.2411, acc-0.2125\n",
      "Iter-5240, train loss-2.2507, acc-0.2200, valid loss-2.2452, acc-0.2024, test loss-2.2410, acc-0.2128\n",
      "Iter-5250, train loss-2.2243, acc-0.2800, valid loss-2.2451, acc-0.2022, test loss-2.2409, acc-0.2127\n",
      "Iter-5260, train loss-2.2411, acc-0.2000, valid loss-2.2450, acc-0.2024, test loss-2.2408, acc-0.2127\n",
      "Iter-5270, train loss-2.2480, acc-0.1400, valid loss-2.2449, acc-0.2026, test loss-2.2407, acc-0.2128\n",
      "Iter-5280, train loss-2.2043, acc-0.3600, valid loss-2.2448, acc-0.2028, test loss-2.2406, acc-0.2127\n",
      "Iter-5290, train loss-2.2611, acc-0.1400, valid loss-2.2447, acc-0.2028, test loss-2.2405, acc-0.2128\n",
      "Iter-5300, train loss-2.2398, acc-0.1800, valid loss-2.2446, acc-0.2028, test loss-2.2404, acc-0.2129\n",
      "Iter-5310, train loss-2.2203, acc-0.3000, valid loss-2.2445, acc-0.2028, test loss-2.2403, acc-0.2130\n",
      "Iter-5320, train loss-2.2373, acc-0.2800, valid loss-2.2444, acc-0.2028, test loss-2.2402, acc-0.2130\n",
      "Iter-5330, train loss-2.2057, acc-0.2400, valid loss-2.2443, acc-0.2030, test loss-2.2401, acc-0.2132\n",
      "Iter-5340, train loss-2.2092, acc-0.3400, valid loss-2.2442, acc-0.2030, test loss-2.2400, acc-0.2131\n",
      "Iter-5350, train loss-2.2471, acc-0.2000, valid loss-2.2441, acc-0.2032, test loss-2.2399, acc-0.2133\n",
      "Iter-5360, train loss-2.2311, acc-0.2000, valid loss-2.2440, acc-0.2032, test loss-2.2398, acc-0.2132\n",
      "Iter-5370, train loss-2.2373, acc-0.2400, valid loss-2.2439, acc-0.2032, test loss-2.2397, acc-0.2136\n",
      "Iter-5380, train loss-2.2546, acc-0.1800, valid loss-2.2438, acc-0.2036, test loss-2.2397, acc-0.2135\n",
      "Iter-5390, train loss-2.2227, acc-0.2400, valid loss-2.2437, acc-0.2032, test loss-2.2396, acc-0.2134\n",
      "Iter-5400, train loss-2.2367, acc-0.2800, valid loss-2.2436, acc-0.2032, test loss-2.2395, acc-0.2133\n",
      "Iter-5410, train loss-2.2654, acc-0.2400, valid loss-2.2435, acc-0.2034, test loss-2.2394, acc-0.2135\n",
      "Iter-5420, train loss-2.2096, acc-0.2600, valid loss-2.2434, acc-0.2032, test loss-2.2393, acc-0.2137\n",
      "Iter-5430, train loss-2.2096, acc-0.2600, valid loss-2.2433, acc-0.2032, test loss-2.2392, acc-0.2135\n",
      "Iter-5440, train loss-2.2532, acc-0.2200, valid loss-2.2432, acc-0.2032, test loss-2.2391, acc-0.2136\n",
      "Iter-5450, train loss-2.2533, acc-0.1400, valid loss-2.2431, acc-0.2032, test loss-2.2390, acc-0.2136\n",
      "Iter-5460, train loss-2.2524, acc-0.1400, valid loss-2.2430, acc-0.2032, test loss-2.2389, acc-0.2136\n",
      "Iter-5470, train loss-2.2385, acc-0.1800, valid loss-2.2429, acc-0.2032, test loss-2.2388, acc-0.2135\n",
      "Iter-5480, train loss-2.2229, acc-0.2000, valid loss-2.2429, acc-0.2032, test loss-2.2387, acc-0.2137\n",
      "Iter-5490, train loss-2.2730, acc-0.1200, valid loss-2.2428, acc-0.2032, test loss-2.2386, acc-0.2139\n",
      "Iter-5500, train loss-2.2499, acc-0.1600, valid loss-2.2427, acc-0.2034, test loss-2.2385, acc-0.2140\n",
      "Iter-5510, train loss-2.2352, acc-0.2200, valid loss-2.2426, acc-0.2036, test loss-2.2384, acc-0.2140\n",
      "Iter-5520, train loss-2.2467, acc-0.1800, valid loss-2.2425, acc-0.2036, test loss-2.2383, acc-0.2142\n",
      "Iter-5530, train loss-2.2740, acc-0.1800, valid loss-2.2424, acc-0.2032, test loss-2.2382, acc-0.2143\n",
      "Iter-5540, train loss-2.2500, acc-0.1400, valid loss-2.2423, acc-0.2036, test loss-2.2381, acc-0.2144\n",
      "Iter-5550, train loss-2.2150, acc-0.2800, valid loss-2.2422, acc-0.2034, test loss-2.2380, acc-0.2147\n",
      "Iter-5560, train loss-2.2606, acc-0.2000, valid loss-2.2421, acc-0.2034, test loss-2.2379, acc-0.2148\n",
      "Iter-5570, train loss-2.2429, acc-0.2600, valid loss-2.2420, acc-0.2040, test loss-2.2378, acc-0.2150\n",
      "Iter-5580, train loss-2.2258, acc-0.2000, valid loss-2.2419, acc-0.2040, test loss-2.2377, acc-0.2151\n",
      "Iter-5590, train loss-2.2320, acc-0.2600, valid loss-2.2418, acc-0.2042, test loss-2.2377, acc-0.2153\n",
      "Iter-5600, train loss-2.2085, acc-0.2800, valid loss-2.2417, acc-0.2040, test loss-2.2376, acc-0.2156\n",
      "Iter-5610, train loss-2.2074, acc-0.3200, valid loss-2.2417, acc-0.2048, test loss-2.2375, acc-0.2159\n",
      "Iter-5620, train loss-2.2308, acc-0.2400, valid loss-2.2416, acc-0.2048, test loss-2.2374, acc-0.2159\n",
      "Iter-5630, train loss-2.2405, acc-0.1400, valid loss-2.2415, acc-0.2054, test loss-2.2373, acc-0.2161\n",
      "Iter-5640, train loss-2.2277, acc-0.2200, valid loss-2.2414, acc-0.2054, test loss-2.2372, acc-0.2161\n",
      "Iter-5650, train loss-2.2570, acc-0.1600, valid loss-2.2413, acc-0.2056, test loss-2.2371, acc-0.2164\n",
      "Iter-5660, train loss-2.2380, acc-0.2200, valid loss-2.2412, acc-0.2058, test loss-2.2370, acc-0.2163\n",
      "Iter-5670, train loss-2.2278, acc-0.2000, valid loss-2.2411, acc-0.2058, test loss-2.2369, acc-0.2164\n",
      "Iter-5680, train loss-2.2309, acc-0.2400, valid loss-2.2410, acc-0.2056, test loss-2.2368, acc-0.2165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-2.2570, acc-0.2400, valid loss-2.2409, acc-0.2056, test loss-2.2367, acc-0.2163\n",
      "Iter-5700, train loss-2.2167, acc-0.2400, valid loss-2.2408, acc-0.2060, test loss-2.2366, acc-0.2165\n",
      "Iter-5710, train loss-2.2627, acc-0.1800, valid loss-2.2407, acc-0.2062, test loss-2.2365, acc-0.2169\n",
      "Iter-5720, train loss-2.2509, acc-0.2400, valid loss-2.2406, acc-0.2064, test loss-2.2364, acc-0.2168\n",
      "Iter-5730, train loss-2.2522, acc-0.1800, valid loss-2.2405, acc-0.2064, test loss-2.2363, acc-0.2172\n",
      "Iter-5740, train loss-2.1973, acc-0.2800, valid loss-2.2404, acc-0.2062, test loss-2.2362, acc-0.2172\n",
      "Iter-5750, train loss-2.2324, acc-0.2600, valid loss-2.2403, acc-0.2060, test loss-2.2361, acc-0.2172\n",
      "Iter-5760, train loss-2.2369, acc-0.2000, valid loss-2.2402, acc-0.2058, test loss-2.2360, acc-0.2173\n",
      "Iter-5770, train loss-2.2564, acc-0.1800, valid loss-2.2402, acc-0.2058, test loss-2.2359, acc-0.2176\n",
      "Iter-5780, train loss-2.2093, acc-0.2600, valid loss-2.2401, acc-0.2058, test loss-2.2358, acc-0.2175\n",
      "Iter-5790, train loss-2.2578, acc-0.2000, valid loss-2.2400, acc-0.2062, test loss-2.2358, acc-0.2174\n",
      "Iter-5800, train loss-2.2333, acc-0.2400, valid loss-2.2399, acc-0.2066, test loss-2.2357, acc-0.2176\n",
      "Iter-5810, train loss-2.2349, acc-0.2200, valid loss-2.2398, acc-0.2062, test loss-2.2356, acc-0.2177\n",
      "Iter-5820, train loss-2.2419, acc-0.1600, valid loss-2.2397, acc-0.2062, test loss-2.2355, acc-0.2177\n",
      "Iter-5830, train loss-2.2253, acc-0.2200, valid loss-2.2396, acc-0.2062, test loss-2.2354, acc-0.2180\n",
      "Iter-5840, train loss-2.2375, acc-0.1800, valid loss-2.2395, acc-0.2070, test loss-2.2353, acc-0.2182\n",
      "Iter-5850, train loss-2.2065, acc-0.3200, valid loss-2.2394, acc-0.2072, test loss-2.2352, acc-0.2179\n",
      "Iter-5860, train loss-2.2697, acc-0.1600, valid loss-2.2393, acc-0.2068, test loss-2.2351, acc-0.2179\n",
      "Iter-5870, train loss-2.2238, acc-0.1800, valid loss-2.2392, acc-0.2070, test loss-2.2350, acc-0.2184\n",
      "Iter-5880, train loss-2.2404, acc-0.2400, valid loss-2.2391, acc-0.2070, test loss-2.2349, acc-0.2184\n",
      "Iter-5890, train loss-2.2489, acc-0.1400, valid loss-2.2391, acc-0.2074, test loss-2.2348, acc-0.2184\n",
      "Iter-5900, train loss-2.2642, acc-0.2000, valid loss-2.2390, acc-0.2072, test loss-2.2347, acc-0.2186\n",
      "Iter-5910, train loss-2.2551, acc-0.2400, valid loss-2.2389, acc-0.2078, test loss-2.2346, acc-0.2186\n",
      "Iter-5920, train loss-2.2403, acc-0.2200, valid loss-2.2388, acc-0.2078, test loss-2.2345, acc-0.2187\n",
      "Iter-5930, train loss-2.2562, acc-0.1400, valid loss-2.2387, acc-0.2078, test loss-2.2344, acc-0.2188\n",
      "Iter-5940, train loss-2.2282, acc-0.1200, valid loss-2.2386, acc-0.2072, test loss-2.2344, acc-0.2188\n",
      "Iter-5950, train loss-2.2126, acc-0.2400, valid loss-2.2385, acc-0.2072, test loss-2.2342, acc-0.2188\n",
      "Iter-5960, train loss-2.2135, acc-0.3000, valid loss-2.2384, acc-0.2070, test loss-2.2341, acc-0.2187\n",
      "Iter-5970, train loss-2.2447, acc-0.2400, valid loss-2.2383, acc-0.2072, test loss-2.2341, acc-0.2189\n",
      "Iter-5980, train loss-2.2444, acc-0.2200, valid loss-2.2382, acc-0.2068, test loss-2.2340, acc-0.2190\n",
      "Iter-5990, train loss-2.2505, acc-0.2600, valid loss-2.2381, acc-0.2066, test loss-2.2339, acc-0.2192\n",
      "Iter-6000, train loss-2.2524, acc-0.2800, valid loss-2.2380, acc-0.2074, test loss-2.2338, acc-0.2195\n",
      "Iter-6010, train loss-2.2147, acc-0.1400, valid loss-2.2379, acc-0.2072, test loss-2.2337, acc-0.2194\n",
      "Iter-6020, train loss-2.2373, acc-0.1600, valid loss-2.2378, acc-0.2074, test loss-2.2336, acc-0.2196\n",
      "Iter-6030, train loss-2.2264, acc-0.2400, valid loss-2.2377, acc-0.2076, test loss-2.2335, acc-0.2195\n",
      "Iter-6040, train loss-2.2542, acc-0.1600, valid loss-2.2376, acc-0.2078, test loss-2.2334, acc-0.2196\n",
      "Iter-6050, train loss-2.2363, acc-0.1600, valid loss-2.2376, acc-0.2078, test loss-2.2333, acc-0.2195\n",
      "Iter-6060, train loss-2.2044, acc-0.2800, valid loss-2.2375, acc-0.2074, test loss-2.2332, acc-0.2198\n",
      "Iter-6070, train loss-2.2105, acc-0.2800, valid loss-2.2374, acc-0.2074, test loss-2.2331, acc-0.2199\n",
      "Iter-6080, train loss-2.2245, acc-0.1800, valid loss-2.2373, acc-0.2076, test loss-2.2330, acc-0.2199\n",
      "Iter-6090, train loss-2.2287, acc-0.3600, valid loss-2.2372, acc-0.2076, test loss-2.2329, acc-0.2203\n",
      "Iter-6100, train loss-2.2270, acc-0.1600, valid loss-2.2371, acc-0.2072, test loss-2.2328, acc-0.2206\n",
      "Iter-6110, train loss-2.2193, acc-0.3200, valid loss-2.2370, acc-0.2070, test loss-2.2328, acc-0.2202\n",
      "Iter-6120, train loss-2.2175, acc-0.2800, valid loss-2.2369, acc-0.2072, test loss-2.2327, acc-0.2203\n",
      "Iter-6130, train loss-2.2665, acc-0.1600, valid loss-2.2368, acc-0.2072, test loss-2.2326, acc-0.2203\n",
      "Iter-6140, train loss-2.2253, acc-0.3600, valid loss-2.2367, acc-0.2076, test loss-2.2325, acc-0.2205\n",
      "Iter-6150, train loss-2.2156, acc-0.2000, valid loss-2.2366, acc-0.2078, test loss-2.2324, acc-0.2204\n",
      "Iter-6160, train loss-2.2248, acc-0.2200, valid loss-2.2365, acc-0.2082, test loss-2.2323, acc-0.2203\n",
      "Iter-6170, train loss-2.2492, acc-0.1800, valid loss-2.2364, acc-0.2086, test loss-2.2322, acc-0.2207\n",
      "Iter-6180, train loss-2.2132, acc-0.2800, valid loss-2.2363, acc-0.2092, test loss-2.2321, acc-0.2206\n",
      "Iter-6190, train loss-2.2393, acc-0.2000, valid loss-2.2362, acc-0.2090, test loss-2.2320, acc-0.2206\n",
      "Iter-6200, train loss-2.2528, acc-0.1400, valid loss-2.2361, acc-0.2092, test loss-2.2319, acc-0.2211\n",
      "Iter-6210, train loss-2.2089, acc-0.2000, valid loss-2.2360, acc-0.2094, test loss-2.2318, acc-0.2208\n",
      "Iter-6220, train loss-2.2360, acc-0.2400, valid loss-2.2359, acc-0.2096, test loss-2.2317, acc-0.2209\n",
      "Iter-6230, train loss-2.2143, acc-0.2800, valid loss-2.2358, acc-0.2096, test loss-2.2316, acc-0.2206\n",
      "Iter-6240, train loss-2.2292, acc-0.2400, valid loss-2.2357, acc-0.2092, test loss-2.2315, acc-0.2209\n",
      "Iter-6250, train loss-2.2355, acc-0.1600, valid loss-2.2356, acc-0.2096, test loss-2.2314, acc-0.2210\n",
      "Iter-6260, train loss-2.2417, acc-0.1800, valid loss-2.2356, acc-0.2094, test loss-2.2313, acc-0.2212\n",
      "Iter-6270, train loss-2.2036, acc-0.2800, valid loss-2.2355, acc-0.2096, test loss-2.2312, acc-0.2215\n",
      "Iter-6280, train loss-2.2481, acc-0.2000, valid loss-2.2354, acc-0.2096, test loss-2.2311, acc-0.2215\n",
      "Iter-6290, train loss-2.2381, acc-0.1600, valid loss-2.2353, acc-0.2100, test loss-2.2310, acc-0.2220\n",
      "Iter-6300, train loss-2.1948, acc-0.2400, valid loss-2.2352, acc-0.2106, test loss-2.2309, acc-0.2220\n",
      "Iter-6310, train loss-2.2262, acc-0.2000, valid loss-2.2351, acc-0.2104, test loss-2.2309, acc-0.2224\n",
      "Iter-6320, train loss-2.2245, acc-0.2600, valid loss-2.2350, acc-0.2110, test loss-2.2307, acc-0.2223\n",
      "Iter-6330, train loss-2.2281, acc-0.2000, valid loss-2.2349, acc-0.2110, test loss-2.2307, acc-0.2224\n",
      "Iter-6340, train loss-2.2633, acc-0.1600, valid loss-2.2348, acc-0.2108, test loss-2.2306, acc-0.2229\n",
      "Iter-6350, train loss-2.2277, acc-0.2400, valid loss-2.2347, acc-0.2110, test loss-2.2305, acc-0.2228\n",
      "Iter-6360, train loss-2.2513, acc-0.1400, valid loss-2.2346, acc-0.2112, test loss-2.2304, acc-0.2228\n",
      "Iter-6370, train loss-2.2338, acc-0.2000, valid loss-2.2345, acc-0.2114, test loss-2.2303, acc-0.2229\n",
      "Iter-6380, train loss-2.2482, acc-0.2000, valid loss-2.2344, acc-0.2118, test loss-2.2302, acc-0.2231\n",
      "Iter-6390, train loss-2.2336, acc-0.2400, valid loss-2.2344, acc-0.2124, test loss-2.2301, acc-0.2232\n",
      "Iter-6400, train loss-2.2396, acc-0.2200, valid loss-2.2343, acc-0.2128, test loss-2.2300, acc-0.2233\n",
      "Iter-6410, train loss-2.2380, acc-0.2200, valid loss-2.2342, acc-0.2128, test loss-2.2299, acc-0.2240\n",
      "Iter-6420, train loss-2.2180, acc-0.3600, valid loss-2.2341, acc-0.2128, test loss-2.2299, acc-0.2242\n",
      "Iter-6430, train loss-2.2460, acc-0.1400, valid loss-2.2340, acc-0.2128, test loss-2.2298, acc-0.2239\n",
      "Iter-6440, train loss-2.2406, acc-0.1200, valid loss-2.2339, acc-0.2128, test loss-2.2297, acc-0.2244\n",
      "Iter-6450, train loss-2.2321, acc-0.1600, valid loss-2.2338, acc-0.2128, test loss-2.2296, acc-0.2246\n",
      "Iter-6460, train loss-2.2421, acc-0.2600, valid loss-2.2337, acc-0.2128, test loss-2.2295, acc-0.2246\n",
      "Iter-6470, train loss-2.2500, acc-0.2600, valid loss-2.2337, acc-0.2126, test loss-2.2294, acc-0.2247\n",
      "Iter-6480, train loss-2.2502, acc-0.1800, valid loss-2.2336, acc-0.2130, test loss-2.2293, acc-0.2248\n",
      "Iter-6490, train loss-2.2403, acc-0.1600, valid loss-2.2335, acc-0.2132, test loss-2.2292, acc-0.2247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-2.2326, acc-0.1400, valid loss-2.2334, acc-0.2132, test loss-2.2291, acc-0.2250\n",
      "Iter-6510, train loss-2.2363, acc-0.1000, valid loss-2.2333, acc-0.2132, test loss-2.2290, acc-0.2251\n",
      "Iter-6520, train loss-2.2440, acc-0.1800, valid loss-2.2332, acc-0.2134, test loss-2.2289, acc-0.2252\n",
      "Iter-6530, train loss-2.1943, acc-0.2400, valid loss-2.2331, acc-0.2136, test loss-2.2288, acc-0.2254\n",
      "Iter-6540, train loss-2.2278, acc-0.2000, valid loss-2.2330, acc-0.2136, test loss-2.2287, acc-0.2255\n",
      "Iter-6550, train loss-2.2344, acc-0.2800, valid loss-2.2329, acc-0.2136, test loss-2.2287, acc-0.2259\n",
      "Iter-6560, train loss-2.2183, acc-0.1800, valid loss-2.2328, acc-0.2144, test loss-2.2286, acc-0.2260\n",
      "Iter-6570, train loss-2.2449, acc-0.2400, valid loss-2.2327, acc-0.2146, test loss-2.2285, acc-0.2262\n",
      "Iter-6580, train loss-2.2270, acc-0.2600, valid loss-2.2326, acc-0.2144, test loss-2.2284, acc-0.2261\n",
      "Iter-6590, train loss-2.2366, acc-0.1800, valid loss-2.2325, acc-0.2146, test loss-2.2283, acc-0.2265\n",
      "Iter-6600, train loss-2.2100, acc-0.3200, valid loss-2.2325, acc-0.2144, test loss-2.2282, acc-0.2262\n",
      "Iter-6610, train loss-2.2277, acc-0.1600, valid loss-2.2324, acc-0.2150, test loss-2.2281, acc-0.2267\n",
      "Iter-6620, train loss-2.2363, acc-0.1200, valid loss-2.2323, acc-0.2146, test loss-2.2280, acc-0.2269\n",
      "Iter-6630, train loss-2.2183, acc-0.2200, valid loss-2.2322, acc-0.2148, test loss-2.2279, acc-0.2270\n",
      "Iter-6640, train loss-2.2114, acc-0.2800, valid loss-2.2321, acc-0.2152, test loss-2.2278, acc-0.2267\n",
      "Iter-6650, train loss-2.2386, acc-0.2400, valid loss-2.2320, acc-0.2152, test loss-2.2277, acc-0.2268\n",
      "Iter-6660, train loss-2.2246, acc-0.2600, valid loss-2.2319, acc-0.2154, test loss-2.2276, acc-0.2271\n",
      "Iter-6670, train loss-2.2525, acc-0.1400, valid loss-2.2318, acc-0.2154, test loss-2.2276, acc-0.2274\n",
      "Iter-6680, train loss-2.2701, acc-0.1400, valid loss-2.2317, acc-0.2158, test loss-2.2275, acc-0.2273\n",
      "Iter-6690, train loss-2.2206, acc-0.2800, valid loss-2.2316, acc-0.2158, test loss-2.2274, acc-0.2275\n",
      "Iter-6700, train loss-2.1992, acc-0.2400, valid loss-2.2315, acc-0.2160, test loss-2.2273, acc-0.2275\n",
      "Iter-6710, train loss-2.2073, acc-0.2600, valid loss-2.2315, acc-0.2158, test loss-2.2272, acc-0.2277\n",
      "Iter-6720, train loss-2.2412, acc-0.1400, valid loss-2.2314, acc-0.2156, test loss-2.2271, acc-0.2276\n",
      "Iter-6730, train loss-2.2048, acc-0.3000, valid loss-2.2313, acc-0.2156, test loss-2.2270, acc-0.2277\n",
      "Iter-6740, train loss-2.1899, acc-0.3600, valid loss-2.2312, acc-0.2158, test loss-2.2269, acc-0.2277\n",
      "Iter-6750, train loss-2.2547, acc-0.2000, valid loss-2.2311, acc-0.2158, test loss-2.2268, acc-0.2277\n",
      "Iter-6760, train loss-2.2225, acc-0.2800, valid loss-2.2310, acc-0.2160, test loss-2.2267, acc-0.2278\n",
      "Iter-6770, train loss-2.2681, acc-0.1800, valid loss-2.2309, acc-0.2160, test loss-2.2266, acc-0.2277\n",
      "Iter-6780, train loss-2.2319, acc-0.3000, valid loss-2.2308, acc-0.2160, test loss-2.2265, acc-0.2279\n",
      "Iter-6790, train loss-2.2325, acc-0.2600, valid loss-2.2307, acc-0.2162, test loss-2.2264, acc-0.2279\n",
      "Iter-6800, train loss-2.1991, acc-0.2000, valid loss-2.2306, acc-0.2164, test loss-2.2263, acc-0.2278\n",
      "Iter-6810, train loss-2.2163, acc-0.2400, valid loss-2.2305, acc-0.2164, test loss-2.2262, acc-0.2279\n",
      "Iter-6820, train loss-2.2118, acc-0.2400, valid loss-2.2304, acc-0.2168, test loss-2.2262, acc-0.2277\n",
      "Iter-6830, train loss-2.2234, acc-0.2200, valid loss-2.2304, acc-0.2168, test loss-2.2261, acc-0.2278\n",
      "Iter-6840, train loss-2.2262, acc-0.2000, valid loss-2.2303, acc-0.2170, test loss-2.2260, acc-0.2280\n",
      "Iter-6850, train loss-2.2133, acc-0.2200, valid loss-2.2302, acc-0.2170, test loss-2.2259, acc-0.2282\n",
      "Iter-6860, train loss-2.1994, acc-0.2800, valid loss-2.2301, acc-0.2166, test loss-2.2258, acc-0.2281\n",
      "Iter-6870, train loss-2.2287, acc-0.2600, valid loss-2.2300, acc-0.2170, test loss-2.2257, acc-0.2283\n",
      "Iter-6880, train loss-2.2142, acc-0.3000, valid loss-2.2299, acc-0.2170, test loss-2.2256, acc-0.2284\n",
      "Iter-6890, train loss-2.2318, acc-0.2600, valid loss-2.2298, acc-0.2174, test loss-2.2255, acc-0.2285\n",
      "Iter-6900, train loss-2.2518, acc-0.2000, valid loss-2.2297, acc-0.2176, test loss-2.2254, acc-0.2286\n",
      "Iter-6910, train loss-2.2376, acc-0.2200, valid loss-2.2296, acc-0.2180, test loss-2.2253, acc-0.2288\n",
      "Iter-6920, train loss-2.2320, acc-0.2000, valid loss-2.2295, acc-0.2178, test loss-2.2252, acc-0.2289\n",
      "Iter-6930, train loss-2.2185, acc-0.2400, valid loss-2.2295, acc-0.2180, test loss-2.2252, acc-0.2289\n",
      "Iter-6940, train loss-2.2331, acc-0.2200, valid loss-2.2294, acc-0.2182, test loss-2.2251, acc-0.2290\n",
      "Iter-6950, train loss-2.2393, acc-0.1800, valid loss-2.2293, acc-0.2186, test loss-2.2250, acc-0.2287\n",
      "Iter-6960, train loss-2.2474, acc-0.2000, valid loss-2.2292, acc-0.2186, test loss-2.2249, acc-0.2287\n",
      "Iter-6970, train loss-2.2286, acc-0.2200, valid loss-2.2291, acc-0.2184, test loss-2.2248, acc-0.2290\n",
      "Iter-6980, train loss-2.2373, acc-0.2000, valid loss-2.2290, acc-0.2186, test loss-2.2247, acc-0.2290\n",
      "Iter-6990, train loss-2.2160, acc-0.2800, valid loss-2.2289, acc-0.2186, test loss-2.2246, acc-0.2292\n",
      "Iter-7000, train loss-2.2309, acc-0.2400, valid loss-2.2288, acc-0.2182, test loss-2.2245, acc-0.2292\n",
      "Iter-7010, train loss-2.1941, acc-0.3200, valid loss-2.2287, acc-0.2186, test loss-2.2244, acc-0.2290\n",
      "Iter-7020, train loss-2.2085, acc-0.3000, valid loss-2.2286, acc-0.2186, test loss-2.2243, acc-0.2292\n",
      "Iter-7030, train loss-2.2404, acc-0.2600, valid loss-2.2285, acc-0.2190, test loss-2.2242, acc-0.2292\n",
      "Iter-7040, train loss-2.2068, acc-0.2200, valid loss-2.2284, acc-0.2188, test loss-2.2241, acc-0.2293\n",
      "Iter-7050, train loss-2.2329, acc-0.2000, valid loss-2.2283, acc-0.2192, test loss-2.2240, acc-0.2292\n",
      "Iter-7060, train loss-2.1860, acc-0.3000, valid loss-2.2282, acc-0.2192, test loss-2.2239, acc-0.2291\n",
      "Iter-7070, train loss-2.2382, acc-0.1400, valid loss-2.2282, acc-0.2192, test loss-2.2239, acc-0.2293\n",
      "Iter-7080, train loss-2.2223, acc-0.1800, valid loss-2.2281, acc-0.2192, test loss-2.2238, acc-0.2291\n",
      "Iter-7090, train loss-2.2151, acc-0.2800, valid loss-2.2280, acc-0.2192, test loss-2.2237, acc-0.2292\n",
      "Iter-7100, train loss-2.2171, acc-0.2600, valid loss-2.2279, acc-0.2194, test loss-2.2236, acc-0.2294\n",
      "Iter-7110, train loss-2.2614, acc-0.1600, valid loss-2.2278, acc-0.2194, test loss-2.2235, acc-0.2293\n",
      "Iter-7120, train loss-2.2175, acc-0.2800, valid loss-2.2277, acc-0.2190, test loss-2.2234, acc-0.2293\n",
      "Iter-7130, train loss-2.2368, acc-0.1400, valid loss-2.2276, acc-0.2196, test loss-2.2233, acc-0.2298\n",
      "Iter-7140, train loss-2.2232, acc-0.2400, valid loss-2.2275, acc-0.2196, test loss-2.2232, acc-0.2302\n",
      "Iter-7150, train loss-2.2203, acc-0.2400, valid loss-2.2274, acc-0.2194, test loss-2.2231, acc-0.2304\n",
      "Iter-7160, train loss-2.2384, acc-0.2000, valid loss-2.2273, acc-0.2196, test loss-2.2230, acc-0.2305\n",
      "Iter-7170, train loss-2.2236, acc-0.2400, valid loss-2.2272, acc-0.2196, test loss-2.2229, acc-0.2306\n",
      "Iter-7180, train loss-2.2001, acc-0.2800, valid loss-2.2271, acc-0.2198, test loss-2.2228, acc-0.2305\n",
      "Iter-7190, train loss-2.2365, acc-0.2200, valid loss-2.2270, acc-0.2198, test loss-2.2228, acc-0.2306\n",
      "Iter-7200, train loss-2.2565, acc-0.1200, valid loss-2.2270, acc-0.2202, test loss-2.2227, acc-0.2310\n",
      "Iter-7210, train loss-2.1935, acc-0.2400, valid loss-2.2269, acc-0.2206, test loss-2.2226, acc-0.2312\n",
      "Iter-7220, train loss-2.2120, acc-0.2800, valid loss-2.2268, acc-0.2206, test loss-2.2225, acc-0.2314\n",
      "Iter-7230, train loss-2.1983, acc-0.3200, valid loss-2.2267, acc-0.2204, test loss-2.2224, acc-0.2316\n",
      "Iter-7240, train loss-2.2256, acc-0.1600, valid loss-2.2266, acc-0.2204, test loss-2.2223, acc-0.2316\n",
      "Iter-7250, train loss-2.2169, acc-0.1200, valid loss-2.2265, acc-0.2206, test loss-2.2222, acc-0.2316\n",
      "Iter-7260, train loss-2.1980, acc-0.3000, valid loss-2.2264, acc-0.2210, test loss-2.2221, acc-0.2317\n",
      "Iter-7270, train loss-2.2292, acc-0.1800, valid loss-2.2263, acc-0.2216, test loss-2.2220, acc-0.2318\n",
      "Iter-7280, train loss-2.2072, acc-0.3000, valid loss-2.2262, acc-0.2214, test loss-2.2219, acc-0.2322\n",
      "Iter-7290, train loss-2.2507, acc-0.1600, valid loss-2.2262, acc-0.2216, test loss-2.2219, acc-0.2322\n",
      "Iter-7300, train loss-2.2134, acc-0.2200, valid loss-2.2261, acc-0.2218, test loss-2.2218, acc-0.2323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-2.2238, acc-0.1200, valid loss-2.2260, acc-0.2218, test loss-2.2217, acc-0.2324\n",
      "Iter-7320, train loss-2.2197, acc-0.1400, valid loss-2.2259, acc-0.2218, test loss-2.2216, acc-0.2325\n",
      "Iter-7330, train loss-2.2366, acc-0.1400, valid loss-2.2258, acc-0.2218, test loss-2.2215, acc-0.2324\n",
      "Iter-7340, train loss-2.2179, acc-0.1800, valid loss-2.2257, acc-0.2220, test loss-2.2214, acc-0.2326\n",
      "Iter-7350, train loss-2.2014, acc-0.3400, valid loss-2.2256, acc-0.2222, test loss-2.2213, acc-0.2328\n",
      "Iter-7360, train loss-2.2048, acc-0.2000, valid loss-2.2255, acc-0.2224, test loss-2.2212, acc-0.2329\n",
      "Iter-7370, train loss-2.2309, acc-0.2400, valid loss-2.2254, acc-0.2224, test loss-2.2211, acc-0.2330\n",
      "Iter-7380, train loss-2.2210, acc-0.2600, valid loss-2.2254, acc-0.2222, test loss-2.2210, acc-0.2331\n",
      "Iter-7390, train loss-2.2307, acc-0.1600, valid loss-2.2253, acc-0.2218, test loss-2.2209, acc-0.2328\n",
      "Iter-7400, train loss-2.2504, acc-0.2400, valid loss-2.2252, acc-0.2216, test loss-2.2208, acc-0.2330\n",
      "Iter-7410, train loss-2.2237, acc-0.2800, valid loss-2.2251, acc-0.2218, test loss-2.2207, acc-0.2329\n",
      "Iter-7420, train loss-2.2184, acc-0.2800, valid loss-2.2250, acc-0.2218, test loss-2.2207, acc-0.2338\n",
      "Iter-7430, train loss-2.2244, acc-0.2000, valid loss-2.2249, acc-0.2218, test loss-2.2206, acc-0.2339\n",
      "Iter-7440, train loss-2.2295, acc-0.2000, valid loss-2.2248, acc-0.2222, test loss-2.2205, acc-0.2340\n",
      "Iter-7450, train loss-2.2507, acc-0.1800, valid loss-2.2247, acc-0.2222, test loss-2.2204, acc-0.2342\n",
      "Iter-7460, train loss-2.2323, acc-0.2200, valid loss-2.2246, acc-0.2224, test loss-2.2203, acc-0.2342\n",
      "Iter-7470, train loss-2.2312, acc-0.1800, valid loss-2.2245, acc-0.2228, test loss-2.2202, acc-0.2343\n",
      "Iter-7480, train loss-2.2098, acc-0.2600, valid loss-2.2244, acc-0.2228, test loss-2.2201, acc-0.2343\n",
      "Iter-7490, train loss-2.2506, acc-0.2400, valid loss-2.2244, acc-0.2228, test loss-2.2200, acc-0.2345\n",
      "Iter-7500, train loss-2.2481, acc-0.2000, valid loss-2.2243, acc-0.2230, test loss-2.2199, acc-0.2346\n",
      "Iter-7510, train loss-2.1968, acc-0.2600, valid loss-2.2242, acc-0.2230, test loss-2.2199, acc-0.2344\n",
      "Iter-7520, train loss-2.2489, acc-0.1600, valid loss-2.2241, acc-0.2234, test loss-2.2198, acc-0.2346\n",
      "Iter-7530, train loss-2.2413, acc-0.1600, valid loss-2.2240, acc-0.2234, test loss-2.2197, acc-0.2348\n",
      "Iter-7540, train loss-2.2280, acc-0.2000, valid loss-2.2239, acc-0.2238, test loss-2.2196, acc-0.2350\n",
      "Iter-7550, train loss-2.2181, acc-0.2600, valid loss-2.2238, acc-0.2238, test loss-2.2195, acc-0.2349\n",
      "Iter-7560, train loss-2.2111, acc-0.2400, valid loss-2.2237, acc-0.2240, test loss-2.2194, acc-0.2352\n",
      "Iter-7570, train loss-2.2362, acc-0.2200, valid loss-2.2236, acc-0.2234, test loss-2.2193, acc-0.2350\n",
      "Iter-7580, train loss-2.2314, acc-0.2400, valid loss-2.2235, acc-0.2238, test loss-2.2192, acc-0.2352\n",
      "Iter-7590, train loss-2.2400, acc-0.1600, valid loss-2.2234, acc-0.2236, test loss-2.2191, acc-0.2349\n",
      "Iter-7600, train loss-2.2289, acc-0.2200, valid loss-2.2233, acc-0.2236, test loss-2.2190, acc-0.2351\n",
      "Iter-7610, train loss-2.2281, acc-0.1800, valid loss-2.2233, acc-0.2240, test loss-2.2189, acc-0.2354\n",
      "Iter-7620, train loss-2.2296, acc-0.2000, valid loss-2.2232, acc-0.2238, test loss-2.2188, acc-0.2354\n",
      "Iter-7630, train loss-2.2128, acc-0.2800, valid loss-2.2231, acc-0.2240, test loss-2.2188, acc-0.2353\n",
      "Iter-7640, train loss-2.2119, acc-0.2200, valid loss-2.2230, acc-0.2242, test loss-2.2187, acc-0.2354\n",
      "Iter-7650, train loss-2.2210, acc-0.2400, valid loss-2.2229, acc-0.2242, test loss-2.2186, acc-0.2354\n",
      "Iter-7660, train loss-2.2152, acc-0.2800, valid loss-2.2228, acc-0.2240, test loss-2.2185, acc-0.2353\n",
      "Iter-7670, train loss-2.2262, acc-0.2800, valid loss-2.2227, acc-0.2244, test loss-2.2184, acc-0.2356\n",
      "Iter-7680, train loss-2.1748, acc-0.3200, valid loss-2.2226, acc-0.2246, test loss-2.2183, acc-0.2357\n",
      "Iter-7690, train loss-2.1738, acc-0.2600, valid loss-2.2225, acc-0.2244, test loss-2.2182, acc-0.2359\n",
      "Iter-7700, train loss-2.1944, acc-0.2600, valid loss-2.2224, acc-0.2252, test loss-2.2181, acc-0.2359\n",
      "Iter-7710, train loss-2.2045, acc-0.2400, valid loss-2.2223, acc-0.2248, test loss-2.2180, acc-0.2358\n",
      "Iter-7720, train loss-2.1851, acc-0.3200, valid loss-2.2223, acc-0.2248, test loss-2.2179, acc-0.2359\n",
      "Iter-7730, train loss-2.2011, acc-0.3400, valid loss-2.2222, acc-0.2246, test loss-2.2178, acc-0.2359\n",
      "Iter-7740, train loss-2.1989, acc-0.3400, valid loss-2.2221, acc-0.2250, test loss-2.2177, acc-0.2361\n",
      "Iter-7750, train loss-2.2206, acc-0.2200, valid loss-2.2220, acc-0.2244, test loss-2.2177, acc-0.2361\n",
      "Iter-7760, train loss-2.1999, acc-0.2800, valid loss-2.2219, acc-0.2244, test loss-2.2176, acc-0.2362\n",
      "Iter-7770, train loss-2.2296, acc-0.2400, valid loss-2.2218, acc-0.2246, test loss-2.2175, acc-0.2364\n",
      "Iter-7780, train loss-2.2166, acc-0.2000, valid loss-2.2217, acc-0.2248, test loss-2.2174, acc-0.2364\n",
      "Iter-7790, train loss-2.2132, acc-0.2600, valid loss-2.2216, acc-0.2252, test loss-2.2173, acc-0.2363\n",
      "Iter-7800, train loss-2.1899, acc-0.2600, valid loss-2.2215, acc-0.2250, test loss-2.2172, acc-0.2366\n",
      "Iter-7810, train loss-2.2708, acc-0.1600, valid loss-2.2214, acc-0.2250, test loss-2.2171, acc-0.2367\n",
      "Iter-7820, train loss-2.2222, acc-0.2600, valid loss-2.2213, acc-0.2250, test loss-2.2170, acc-0.2370\n",
      "Iter-7830, train loss-2.2284, acc-0.1600, valid loss-2.2213, acc-0.2250, test loss-2.2169, acc-0.2369\n",
      "Iter-7840, train loss-2.2236, acc-0.2000, valid loss-2.2212, acc-0.2250, test loss-2.2168, acc-0.2371\n",
      "Iter-7850, train loss-2.2197, acc-0.1800, valid loss-2.2211, acc-0.2254, test loss-2.2168, acc-0.2372\n",
      "Iter-7860, train loss-2.2240, acc-0.2200, valid loss-2.2210, acc-0.2256, test loss-2.2167, acc-0.2372\n",
      "Iter-7870, train loss-2.1782, acc-0.3600, valid loss-2.2209, acc-0.2256, test loss-2.2166, acc-0.2370\n",
      "Iter-7880, train loss-2.1936, acc-0.2400, valid loss-2.2208, acc-0.2258, test loss-2.2165, acc-0.2371\n",
      "Iter-7890, train loss-2.1867, acc-0.2600, valid loss-2.2207, acc-0.2260, test loss-2.2164, acc-0.2375\n",
      "Iter-7900, train loss-2.2262, acc-0.1800, valid loss-2.2206, acc-0.2260, test loss-2.2163, acc-0.2374\n",
      "Iter-7910, train loss-2.2567, acc-0.1600, valid loss-2.2205, acc-0.2264, test loss-2.2162, acc-0.2376\n",
      "Iter-7920, train loss-2.2142, acc-0.2200, valid loss-2.2204, acc-0.2262, test loss-2.2161, acc-0.2377\n",
      "Iter-7930, train loss-2.2036, acc-0.2000, valid loss-2.2203, acc-0.2264, test loss-2.2160, acc-0.2379\n",
      "Iter-7940, train loss-2.2019, acc-0.2800, valid loss-2.2203, acc-0.2266, test loss-2.2159, acc-0.2378\n",
      "Iter-7950, train loss-2.2034, acc-0.2000, valid loss-2.2202, acc-0.2262, test loss-2.2158, acc-0.2380\n",
      "Iter-7960, train loss-2.2146, acc-0.1600, valid loss-2.2201, acc-0.2264, test loss-2.2157, acc-0.2382\n",
      "Iter-7970, train loss-2.2370, acc-0.2000, valid loss-2.2200, acc-0.2260, test loss-2.2157, acc-0.2384\n",
      "Iter-7980, train loss-2.2471, acc-0.1400, valid loss-2.2199, acc-0.2262, test loss-2.2156, acc-0.2388\n",
      "Iter-7990, train loss-2.2184, acc-0.2800, valid loss-2.2198, acc-0.2268, test loss-2.2155, acc-0.2389\n",
      "Iter-8000, train loss-2.2264, acc-0.2200, valid loss-2.2197, acc-0.2272, test loss-2.2154, acc-0.2391\n",
      "Iter-8010, train loss-2.2448, acc-0.2400, valid loss-2.2197, acc-0.2272, test loss-2.2153, acc-0.2392\n",
      "Iter-8020, train loss-2.2242, acc-0.2000, valid loss-2.2196, acc-0.2274, test loss-2.2152, acc-0.2392\n",
      "Iter-8030, train loss-2.2447, acc-0.1600, valid loss-2.2195, acc-0.2278, test loss-2.2151, acc-0.2394\n",
      "Iter-8040, train loss-2.2026, acc-0.2400, valid loss-2.2194, acc-0.2278, test loss-2.2150, acc-0.2393\n",
      "Iter-8050, train loss-2.2243, acc-0.2000, valid loss-2.2193, acc-0.2278, test loss-2.2149, acc-0.2395\n",
      "Iter-8060, train loss-2.2218, acc-0.2600, valid loss-2.2192, acc-0.2278, test loss-2.2148, acc-0.2397\n",
      "Iter-8070, train loss-2.2114, acc-0.1600, valid loss-2.2191, acc-0.2276, test loss-2.2147, acc-0.2403\n",
      "Iter-8080, train loss-2.1900, acc-0.3400, valid loss-2.2190, acc-0.2280, test loss-2.2147, acc-0.2407\n",
      "Iter-8090, train loss-2.1829, acc-0.3400, valid loss-2.2189, acc-0.2278, test loss-2.2146, acc-0.2407\n",
      "Iter-8100, train loss-2.1993, acc-0.2400, valid loss-2.2188, acc-0.2276, test loss-2.2145, acc-0.2407\n",
      "Iter-8110, train loss-2.2116, acc-0.2600, valid loss-2.2187, acc-0.2274, test loss-2.2144, acc-0.2407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-2.1852, acc-0.2800, valid loss-2.2186, acc-0.2274, test loss-2.2143, acc-0.2410\n",
      "Iter-8130, train loss-2.1805, acc-0.3000, valid loss-2.2186, acc-0.2280, test loss-2.2142, acc-0.2412\n",
      "Iter-8140, train loss-2.2066, acc-0.3400, valid loss-2.2185, acc-0.2282, test loss-2.2141, acc-0.2412\n",
      "Iter-8150, train loss-2.2314, acc-0.2400, valid loss-2.2184, acc-0.2278, test loss-2.2140, acc-0.2415\n",
      "Iter-8160, train loss-2.1795, acc-0.3000, valid loss-2.2183, acc-0.2278, test loss-2.2139, acc-0.2415\n",
      "Iter-8170, train loss-2.2118, acc-0.2400, valid loss-2.2182, acc-0.2272, test loss-2.2139, acc-0.2414\n",
      "Iter-8180, train loss-2.1974, acc-0.2400, valid loss-2.2181, acc-0.2272, test loss-2.2138, acc-0.2414\n",
      "Iter-8190, train loss-2.1965, acc-0.3200, valid loss-2.2180, acc-0.2278, test loss-2.2137, acc-0.2421\n",
      "Iter-8200, train loss-2.1870, acc-0.2800, valid loss-2.2179, acc-0.2282, test loss-2.2136, acc-0.2425\n",
      "Iter-8210, train loss-2.2017, acc-0.1800, valid loss-2.2178, acc-0.2288, test loss-2.2135, acc-0.2428\n",
      "Iter-8220, train loss-2.1973, acc-0.2800, valid loss-2.2177, acc-0.2288, test loss-2.2134, acc-0.2427\n",
      "Iter-8230, train loss-2.2580, acc-0.1000, valid loss-2.2177, acc-0.2288, test loss-2.2133, acc-0.2430\n",
      "Iter-8240, train loss-2.1945, acc-0.2600, valid loss-2.2176, acc-0.2288, test loss-2.2132, acc-0.2430\n",
      "Iter-8250, train loss-2.2068, acc-0.2000, valid loss-2.2175, acc-0.2288, test loss-2.2131, acc-0.2434\n",
      "Iter-8260, train loss-2.2542, acc-0.2200, valid loss-2.2174, acc-0.2284, test loss-2.2130, acc-0.2430\n",
      "Iter-8270, train loss-2.1913, acc-0.2600, valid loss-2.2173, acc-0.2282, test loss-2.2130, acc-0.2436\n",
      "Iter-8280, train loss-2.2555, acc-0.1800, valid loss-2.2172, acc-0.2288, test loss-2.2129, acc-0.2435\n",
      "Iter-8290, train loss-2.1943, acc-0.2800, valid loss-2.2171, acc-0.2288, test loss-2.2128, acc-0.2434\n",
      "Iter-8300, train loss-2.2326, acc-0.2000, valid loss-2.2170, acc-0.2290, test loss-2.2127, acc-0.2433\n",
      "Iter-8310, train loss-2.2030, acc-0.2200, valid loss-2.2169, acc-0.2290, test loss-2.2126, acc-0.2434\n",
      "Iter-8320, train loss-2.1822, acc-0.2400, valid loss-2.2169, acc-0.2284, test loss-2.2125, acc-0.2437\n",
      "Iter-8330, train loss-2.1742, acc-0.2800, valid loss-2.2168, acc-0.2290, test loss-2.2124, acc-0.2437\n",
      "Iter-8340, train loss-2.2161, acc-0.2200, valid loss-2.2167, acc-0.2292, test loss-2.2123, acc-0.2436\n",
      "Iter-8350, train loss-2.1822, acc-0.3000, valid loss-2.2166, acc-0.2292, test loss-2.2122, acc-0.2439\n",
      "Iter-8360, train loss-2.2056, acc-0.2200, valid loss-2.2165, acc-0.2290, test loss-2.2121, acc-0.2439\n",
      "Iter-8370, train loss-2.1956, acc-0.2800, valid loss-2.2164, acc-0.2290, test loss-2.2120, acc-0.2439\n",
      "Iter-8380, train loss-2.2053, acc-0.2000, valid loss-2.2163, acc-0.2290, test loss-2.2120, acc-0.2439\n",
      "Iter-8390, train loss-2.2110, acc-0.2400, valid loss-2.2162, acc-0.2290, test loss-2.2119, acc-0.2439\n",
      "Iter-8400, train loss-2.2179, acc-0.2400, valid loss-2.2161, acc-0.2290, test loss-2.2118, acc-0.2438\n",
      "Iter-8410, train loss-2.2456, acc-0.2000, valid loss-2.2161, acc-0.2292, test loss-2.2117, acc-0.2439\n",
      "Iter-8420, train loss-2.2361, acc-0.2200, valid loss-2.2160, acc-0.2292, test loss-2.2116, acc-0.2439\n",
      "Iter-8430, train loss-2.2039, acc-0.1800, valid loss-2.2159, acc-0.2294, test loss-2.2115, acc-0.2439\n",
      "Iter-8440, train loss-2.2228, acc-0.2000, valid loss-2.2158, acc-0.2292, test loss-2.2114, acc-0.2442\n",
      "Iter-8450, train loss-2.2324, acc-0.2000, valid loss-2.2157, acc-0.2292, test loss-2.2113, acc-0.2445\n",
      "Iter-8460, train loss-2.2138, acc-0.2200, valid loss-2.2156, acc-0.2290, test loss-2.2112, acc-0.2443\n",
      "Iter-8470, train loss-2.1936, acc-0.2600, valid loss-2.2155, acc-0.2290, test loss-2.2111, acc-0.2444\n",
      "Iter-8480, train loss-2.1771, acc-0.3200, valid loss-2.2154, acc-0.2288, test loss-2.2111, acc-0.2445\n",
      "Iter-8490, train loss-2.2035, acc-0.2200, valid loss-2.2153, acc-0.2284, test loss-2.2110, acc-0.2443\n",
      "Iter-8500, train loss-2.2025, acc-0.2000, valid loss-2.2152, acc-0.2288, test loss-2.2109, acc-0.2444\n",
      "Iter-8510, train loss-2.2093, acc-0.2000, valid loss-2.2152, acc-0.2286, test loss-2.2108, acc-0.2443\n",
      "Iter-8520, train loss-2.1765, acc-0.3600, valid loss-2.2151, acc-0.2286, test loss-2.2107, acc-0.2444\n",
      "Iter-8530, train loss-2.2299, acc-0.2000, valid loss-2.2150, acc-0.2288, test loss-2.2106, acc-0.2443\n",
      "Iter-8540, train loss-2.2321, acc-0.2000, valid loss-2.2149, acc-0.2288, test loss-2.2105, acc-0.2442\n",
      "Iter-8550, train loss-2.2504, acc-0.1800, valid loss-2.2148, acc-0.2288, test loss-2.2104, acc-0.2443\n",
      "Iter-8560, train loss-2.2325, acc-0.1600, valid loss-2.2147, acc-0.2290, test loss-2.2103, acc-0.2446\n",
      "Iter-8570, train loss-2.2061, acc-0.2400, valid loss-2.2146, acc-0.2290, test loss-2.2103, acc-0.2445\n",
      "Iter-8580, train loss-2.2268, acc-0.2200, valid loss-2.2145, acc-0.2290, test loss-2.2102, acc-0.2444\n",
      "Iter-8590, train loss-2.1839, acc-0.3000, valid loss-2.2145, acc-0.2292, test loss-2.2101, acc-0.2447\n",
      "Iter-8600, train loss-2.2040, acc-0.2400, valid loss-2.2144, acc-0.2292, test loss-2.2100, acc-0.2449\n",
      "Iter-8610, train loss-2.1895, acc-0.3200, valid loss-2.2143, acc-0.2294, test loss-2.2099, acc-0.2446\n",
      "Iter-8620, train loss-2.2046, acc-0.3200, valid loss-2.2142, acc-0.2298, test loss-2.2098, acc-0.2452\n",
      "Iter-8630, train loss-2.1874, acc-0.3000, valid loss-2.2141, acc-0.2298, test loss-2.2097, acc-0.2451\n",
      "Iter-8640, train loss-2.2328, acc-0.2400, valid loss-2.2140, acc-0.2298, test loss-2.2096, acc-0.2454\n",
      "Iter-8650, train loss-2.2267, acc-0.2000, valid loss-2.2139, acc-0.2304, test loss-2.2095, acc-0.2453\n",
      "Iter-8660, train loss-2.1713, acc-0.2800, valid loss-2.2138, acc-0.2304, test loss-2.2094, acc-0.2453\n",
      "Iter-8670, train loss-2.1916, acc-0.2400, valid loss-2.2137, acc-0.2302, test loss-2.2093, acc-0.2454\n",
      "Iter-8680, train loss-2.2075, acc-0.2200, valid loss-2.2137, acc-0.2304, test loss-2.2093, acc-0.2457\n",
      "Iter-8690, train loss-2.1927, acc-0.3600, valid loss-2.2136, acc-0.2302, test loss-2.2092, acc-0.2458\n",
      "Iter-8700, train loss-2.1992, acc-0.2600, valid loss-2.2135, acc-0.2302, test loss-2.2091, acc-0.2457\n",
      "Iter-8710, train loss-2.1668, acc-0.3400, valid loss-2.2134, acc-0.2308, test loss-2.2090, acc-0.2458\n",
      "Iter-8720, train loss-2.2344, acc-0.2000, valid loss-2.2133, acc-0.2310, test loss-2.2089, acc-0.2456\n",
      "Iter-8730, train loss-2.2076, acc-0.2000, valid loss-2.2132, acc-0.2310, test loss-2.2088, acc-0.2460\n",
      "Iter-8740, train loss-2.2274, acc-0.2200, valid loss-2.2131, acc-0.2316, test loss-2.2087, acc-0.2462\n",
      "Iter-8750, train loss-2.2227, acc-0.2400, valid loss-2.2130, acc-0.2316, test loss-2.2086, acc-0.2465\n",
      "Iter-8760, train loss-2.2287, acc-0.2000, valid loss-2.2129, acc-0.2320, test loss-2.2085, acc-0.2467\n",
      "Iter-8770, train loss-2.1785, acc-0.3600, valid loss-2.2128, acc-0.2318, test loss-2.2084, acc-0.2468\n",
      "Iter-8780, train loss-2.2121, acc-0.1800, valid loss-2.2128, acc-0.2316, test loss-2.2084, acc-0.2468\n",
      "Iter-8790, train loss-2.2152, acc-0.2200, valid loss-2.2127, acc-0.2314, test loss-2.2083, acc-0.2471\n",
      "Iter-8800, train loss-2.2190, acc-0.2400, valid loss-2.2126, acc-0.2314, test loss-2.2082, acc-0.2471\n",
      "Iter-8810, train loss-2.2151, acc-0.2200, valid loss-2.2125, acc-0.2314, test loss-2.2081, acc-0.2473\n",
      "Iter-8820, train loss-2.2020, acc-0.3400, valid loss-2.2124, acc-0.2322, test loss-2.2080, acc-0.2475\n",
      "Iter-8830, train loss-2.2339, acc-0.2000, valid loss-2.2123, acc-0.2322, test loss-2.2079, acc-0.2476\n",
      "Iter-8840, train loss-2.1643, acc-0.3000, valid loss-2.2122, acc-0.2322, test loss-2.2078, acc-0.2476\n",
      "Iter-8850, train loss-2.1914, acc-0.3400, valid loss-2.2121, acc-0.2318, test loss-2.2077, acc-0.2475\n",
      "Iter-8860, train loss-2.2220, acc-0.2800, valid loss-2.2121, acc-0.2320, test loss-2.2076, acc-0.2476\n",
      "Iter-8870, train loss-2.2180, acc-0.2800, valid loss-2.2120, acc-0.2320, test loss-2.2075, acc-0.2475\n",
      "Iter-8880, train loss-2.2246, acc-0.2000, valid loss-2.2119, acc-0.2322, test loss-2.2075, acc-0.2477\n",
      "Iter-8890, train loss-2.2190, acc-0.1800, valid loss-2.2118, acc-0.2326, test loss-2.2074, acc-0.2475\n",
      "Iter-8900, train loss-2.2118, acc-0.2400, valid loss-2.2117, acc-0.2324, test loss-2.2073, acc-0.2475\n",
      "Iter-8910, train loss-2.1699, acc-0.3200, valid loss-2.2116, acc-0.2324, test loss-2.2072, acc-0.2477\n",
      "Iter-8920, train loss-2.2111, acc-0.2400, valid loss-2.2115, acc-0.2320, test loss-2.2071, acc-0.2477\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-2.2153, acc-0.1800, valid loss-2.2114, acc-0.2322, test loss-2.2070, acc-0.2480\n",
      "Iter-8940, train loss-2.2078, acc-0.2400, valid loss-2.2114, acc-0.2322, test loss-2.2069, acc-0.2479\n",
      "Iter-8950, train loss-2.2220, acc-0.3200, valid loss-2.2113, acc-0.2320, test loss-2.2068, acc-0.2478\n",
      "Iter-8960, train loss-2.2106, acc-0.2200, valid loss-2.2112, acc-0.2320, test loss-2.2067, acc-0.2481\n",
      "Iter-8970, train loss-2.2014, acc-0.3200, valid loss-2.2111, acc-0.2320, test loss-2.2067, acc-0.2481\n",
      "Iter-8980, train loss-2.2413, acc-0.2200, valid loss-2.2110, acc-0.2318, test loss-2.2066, acc-0.2482\n",
      "Iter-8990, train loss-2.2154, acc-0.2000, valid loss-2.2109, acc-0.2320, test loss-2.2065, acc-0.2482\n",
      "Iter-9000, train loss-2.2206, acc-0.2000, valid loss-2.2108, acc-0.2326, test loss-2.2064, acc-0.2483\n",
      "Iter-9010, train loss-2.2307, acc-0.1200, valid loss-2.2107, acc-0.2328, test loss-2.2063, acc-0.2486\n",
      "Iter-9020, train loss-2.2299, acc-0.1800, valid loss-2.2106, acc-0.2330, test loss-2.2062, acc-0.2489\n",
      "Iter-9030, train loss-2.1784, acc-0.3000, valid loss-2.2106, acc-0.2334, test loss-2.2061, acc-0.2493\n",
      "Iter-9040, train loss-2.2307, acc-0.2400, valid loss-2.2105, acc-0.2334, test loss-2.2061, acc-0.2494\n",
      "Iter-9050, train loss-2.2036, acc-0.2800, valid loss-2.2104, acc-0.2336, test loss-2.2060, acc-0.2495\n",
      "Iter-9060, train loss-2.1923, acc-0.3000, valid loss-2.2103, acc-0.2340, test loss-2.2059, acc-0.2498\n",
      "Iter-9070, train loss-2.2419, acc-0.2400, valid loss-2.2102, acc-0.2342, test loss-2.2058, acc-0.2497\n",
      "Iter-9080, train loss-2.2025, acc-0.3400, valid loss-2.2101, acc-0.2342, test loss-2.2057, acc-0.2496\n",
      "Iter-9090, train loss-2.1840, acc-0.1600, valid loss-2.2100, acc-0.2340, test loss-2.2056, acc-0.2498\n",
      "Iter-9100, train loss-2.2293, acc-0.2400, valid loss-2.2099, acc-0.2342, test loss-2.2055, acc-0.2501\n",
      "Iter-9110, train loss-2.2260, acc-0.2000, valid loss-2.2099, acc-0.2346, test loss-2.2054, acc-0.2502\n",
      "Iter-9120, train loss-2.2266, acc-0.2600, valid loss-2.2098, acc-0.2346, test loss-2.2053, acc-0.2501\n",
      "Iter-9130, train loss-2.2088, acc-0.1600, valid loss-2.2097, acc-0.2350, test loss-2.2053, acc-0.2504\n",
      "Iter-9140, train loss-2.1554, acc-0.3000, valid loss-2.2096, acc-0.2350, test loss-2.2052, acc-0.2504\n",
      "Iter-9150, train loss-2.2302, acc-0.1400, valid loss-2.2095, acc-0.2352, test loss-2.2051, acc-0.2506\n",
      "Iter-9160, train loss-2.1852, acc-0.2800, valid loss-2.2094, acc-0.2356, test loss-2.2050, acc-0.2506\n",
      "Iter-9170, train loss-2.2123, acc-0.1600, valid loss-2.2093, acc-0.2356, test loss-2.2049, acc-0.2507\n",
      "Iter-9180, train loss-2.2323, acc-0.1400, valid loss-2.2092, acc-0.2356, test loss-2.2048, acc-0.2506\n",
      "Iter-9190, train loss-2.2634, acc-0.1000, valid loss-2.2092, acc-0.2360, test loss-2.2047, acc-0.2508\n",
      "Iter-9200, train loss-2.2046, acc-0.3000, valid loss-2.2091, acc-0.2360, test loss-2.2046, acc-0.2506\n",
      "Iter-9210, train loss-2.2197, acc-0.2200, valid loss-2.2090, acc-0.2362, test loss-2.2045, acc-0.2506\n",
      "Iter-9220, train loss-2.1639, acc-0.3200, valid loss-2.2089, acc-0.2364, test loss-2.2044, acc-0.2507\n",
      "Iter-9230, train loss-2.1936, acc-0.2400, valid loss-2.2088, acc-0.2368, test loss-2.2044, acc-0.2507\n",
      "Iter-9240, train loss-2.2083, acc-0.2600, valid loss-2.2087, acc-0.2368, test loss-2.2043, acc-0.2508\n",
      "Iter-9250, train loss-2.1693, acc-0.3600, valid loss-2.2086, acc-0.2364, test loss-2.2042, acc-0.2509\n",
      "Iter-9260, train loss-2.2001, acc-0.2200, valid loss-2.2085, acc-0.2366, test loss-2.2041, acc-0.2514\n",
      "Iter-9270, train loss-2.1950, acc-0.2200, valid loss-2.2084, acc-0.2366, test loss-2.2040, acc-0.2514\n",
      "Iter-9280, train loss-2.1981, acc-0.3400, valid loss-2.2083, acc-0.2370, test loss-2.2039, acc-0.2514\n",
      "Iter-9290, train loss-2.1957, acc-0.3000, valid loss-2.2083, acc-0.2370, test loss-2.2038, acc-0.2515\n",
      "Iter-9300, train loss-2.2357, acc-0.1800, valid loss-2.2082, acc-0.2372, test loss-2.2037, acc-0.2515\n",
      "Iter-9310, train loss-2.2119, acc-0.2000, valid loss-2.2081, acc-0.2374, test loss-2.2036, acc-0.2516\n",
      "Iter-9320, train loss-2.2024, acc-0.2600, valid loss-2.2080, acc-0.2372, test loss-2.2036, acc-0.2518\n",
      "Iter-9330, train loss-2.2058, acc-0.3000, valid loss-2.2079, acc-0.2372, test loss-2.2035, acc-0.2518\n",
      "Iter-9340, train loss-2.2014, acc-0.2400, valid loss-2.2078, acc-0.2374, test loss-2.2034, acc-0.2516\n",
      "Iter-9350, train loss-2.2274, acc-0.2400, valid loss-2.2077, acc-0.2374, test loss-2.2033, acc-0.2518\n",
      "Iter-9360, train loss-2.2340, acc-0.1600, valid loss-2.2076, acc-0.2376, test loss-2.2032, acc-0.2520\n",
      "Iter-9370, train loss-2.2269, acc-0.1800, valid loss-2.2075, acc-0.2378, test loss-2.2031, acc-0.2519\n",
      "Iter-9380, train loss-2.1825, acc-0.3200, valid loss-2.2074, acc-0.2386, test loss-2.2030, acc-0.2522\n",
      "Iter-9390, train loss-2.1885, acc-0.2600, valid loss-2.2074, acc-0.2384, test loss-2.2029, acc-0.2523\n",
      "Iter-9400, train loss-2.1874, acc-0.2200, valid loss-2.2073, acc-0.2386, test loss-2.2028, acc-0.2523\n",
      "Iter-9410, train loss-2.2054, acc-0.2600, valid loss-2.2072, acc-0.2384, test loss-2.2027, acc-0.2526\n",
      "Iter-9420, train loss-2.1982, acc-0.2800, valid loss-2.2071, acc-0.2384, test loss-2.2026, acc-0.2526\n",
      "Iter-9430, train loss-2.2140, acc-0.3000, valid loss-2.2070, acc-0.2386, test loss-2.2026, acc-0.2527\n",
      "Iter-9440, train loss-2.1704, acc-0.2800, valid loss-2.2069, acc-0.2392, test loss-2.2025, acc-0.2527\n",
      "Iter-9450, train loss-2.1885, acc-0.3600, valid loss-2.2068, acc-0.2388, test loss-2.2024, acc-0.2529\n",
      "Iter-9460, train loss-2.2041, acc-0.2800, valid loss-2.2068, acc-0.2392, test loss-2.2023, acc-0.2530\n",
      "Iter-9470, train loss-2.1722, acc-0.3600, valid loss-2.2067, acc-0.2394, test loss-2.2022, acc-0.2529\n",
      "Iter-9480, train loss-2.2224, acc-0.3400, valid loss-2.2066, acc-0.2392, test loss-2.2021, acc-0.2530\n",
      "Iter-9490, train loss-2.2169, acc-0.2400, valid loss-2.2065, acc-0.2394, test loss-2.2020, acc-0.2530\n",
      "Iter-9500, train loss-2.2140, acc-0.2000, valid loss-2.2064, acc-0.2396, test loss-2.2019, acc-0.2532\n",
      "Iter-9510, train loss-2.2241, acc-0.2200, valid loss-2.2063, acc-0.2400, test loss-2.2018, acc-0.2534\n",
      "Iter-9520, train loss-2.2172, acc-0.1400, valid loss-2.2062, acc-0.2398, test loss-2.2018, acc-0.2536\n",
      "Iter-9530, train loss-2.2298, acc-0.2400, valid loss-2.2061, acc-0.2400, test loss-2.2017, acc-0.2535\n",
      "Iter-9540, train loss-2.1883, acc-0.2400, valid loss-2.2060, acc-0.2402, test loss-2.2016, acc-0.2539\n",
      "Iter-9550, train loss-2.1999, acc-0.2400, valid loss-2.2060, acc-0.2402, test loss-2.2015, acc-0.2540\n",
      "Iter-9560, train loss-2.2035, acc-0.2400, valid loss-2.2059, acc-0.2404, test loss-2.2014, acc-0.2541\n",
      "Iter-9570, train loss-2.2192, acc-0.2600, valid loss-2.2058, acc-0.2400, test loss-2.2013, acc-0.2545\n",
      "Iter-9580, train loss-2.2327, acc-0.1800, valid loss-2.2057, acc-0.2402, test loss-2.2012, acc-0.2543\n",
      "Iter-9590, train loss-2.2071, acc-0.1800, valid loss-2.2056, acc-0.2402, test loss-2.2011, acc-0.2542\n",
      "Iter-9600, train loss-2.1944, acc-0.2200, valid loss-2.2055, acc-0.2402, test loss-2.2010, acc-0.2545\n",
      "Iter-9610, train loss-2.2224, acc-0.1800, valid loss-2.2054, acc-0.2402, test loss-2.2010, acc-0.2545\n",
      "Iter-9620, train loss-2.1835, acc-0.2800, valid loss-2.2053, acc-0.2400, test loss-2.2009, acc-0.2545\n",
      "Iter-9630, train loss-2.2224, acc-0.2400, valid loss-2.2052, acc-0.2400, test loss-2.2008, acc-0.2546\n",
      "Iter-9640, train loss-2.2286, acc-0.1600, valid loss-2.2052, acc-0.2398, test loss-2.2007, acc-0.2546\n",
      "Iter-9650, train loss-2.2221, acc-0.3400, valid loss-2.2051, acc-0.2400, test loss-2.2006, acc-0.2546\n",
      "Iter-9660, train loss-2.1710, acc-0.2800, valid loss-2.2050, acc-0.2402, test loss-2.2005, acc-0.2550\n",
      "Iter-9670, train loss-2.2091, acc-0.2400, valid loss-2.2049, acc-0.2400, test loss-2.2004, acc-0.2550\n",
      "Iter-9680, train loss-2.2310, acc-0.2200, valid loss-2.2048, acc-0.2398, test loss-2.2003, acc-0.2550\n",
      "Iter-9690, train loss-2.2218, acc-0.2000, valid loss-2.2047, acc-0.2398, test loss-2.2003, acc-0.2550\n",
      "Iter-9700, train loss-2.2254, acc-0.2400, valid loss-2.2046, acc-0.2398, test loss-2.2002, acc-0.2550\n",
      "Iter-9710, train loss-2.2074, acc-0.2200, valid loss-2.2046, acc-0.2402, test loss-2.2001, acc-0.2550\n",
      "Iter-9720, train loss-2.1889, acc-0.3600, valid loss-2.2045, acc-0.2398, test loss-2.2000, acc-0.2551\n",
      "Iter-9730, train loss-2.1661, acc-0.3600, valid loss-2.2044, acc-0.2402, test loss-2.1999, acc-0.2551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-2.1867, acc-0.2800, valid loss-2.2043, acc-0.2404, test loss-2.1998, acc-0.2551\n",
      "Iter-9750, train loss-2.2209, acc-0.2200, valid loss-2.2042, acc-0.2408, test loss-2.1997, acc-0.2552\n",
      "Iter-9760, train loss-2.1731, acc-0.3000, valid loss-2.2041, acc-0.2410, test loss-2.1996, acc-0.2552\n",
      "Iter-9770, train loss-2.2023, acc-0.2800, valid loss-2.2040, acc-0.2410, test loss-2.1995, acc-0.2553\n",
      "Iter-9780, train loss-2.1618, acc-0.2600, valid loss-2.2039, acc-0.2414, test loss-2.1995, acc-0.2554\n",
      "Iter-9790, train loss-2.2122, acc-0.2000, valid loss-2.2038, acc-0.2418, test loss-2.1994, acc-0.2557\n",
      "Iter-9800, train loss-2.2049, acc-0.2200, valid loss-2.2037, acc-0.2416, test loss-2.1993, acc-0.2556\n",
      "Iter-9810, train loss-2.2172, acc-0.2600, valid loss-2.2037, acc-0.2416, test loss-2.1992, acc-0.2556\n",
      "Iter-9820, train loss-2.2044, acc-0.2200, valid loss-2.2036, acc-0.2420, test loss-2.1991, acc-0.2558\n",
      "Iter-9830, train loss-2.1961, acc-0.2600, valid loss-2.2035, acc-0.2422, test loss-2.1990, acc-0.2558\n",
      "Iter-9840, train loss-2.2134, acc-0.2200, valid loss-2.2034, acc-0.2420, test loss-2.1989, acc-0.2560\n",
      "Iter-9850, train loss-2.2271, acc-0.2200, valid loss-2.2033, acc-0.2418, test loss-2.1988, acc-0.2559\n",
      "Iter-9860, train loss-2.1569, acc-0.3000, valid loss-2.2032, acc-0.2418, test loss-2.1987, acc-0.2557\n",
      "Iter-9870, train loss-2.2299, acc-0.1800, valid loss-2.2031, acc-0.2422, test loss-2.1987, acc-0.2560\n",
      "Iter-9880, train loss-2.1893, acc-0.3000, valid loss-2.2030, acc-0.2416, test loss-2.1986, acc-0.2560\n",
      "Iter-9890, train loss-2.1533, acc-0.3600, valid loss-2.2030, acc-0.2418, test loss-2.1985, acc-0.2561\n",
      "Iter-9900, train loss-2.1960, acc-0.2200, valid loss-2.2029, acc-0.2420, test loss-2.1984, acc-0.2561\n",
      "Iter-9910, train loss-2.2233, acc-0.1400, valid loss-2.2028, acc-0.2420, test loss-2.1983, acc-0.2561\n",
      "Iter-9920, train loss-2.2186, acc-0.2000, valid loss-2.2027, acc-0.2418, test loss-2.1982, acc-0.2564\n",
      "Iter-9930, train loss-2.2219, acc-0.1200, valid loss-2.2026, acc-0.2424, test loss-2.1981, acc-0.2563\n",
      "Iter-9940, train loss-2.2118, acc-0.2800, valid loss-2.2025, acc-0.2428, test loss-2.1981, acc-0.2565\n",
      "Iter-9950, train loss-2.2091, acc-0.1600, valid loss-2.2024, acc-0.2430, test loss-2.1980, acc-0.2566\n",
      "Iter-9960, train loss-2.1763, acc-0.3000, valid loss-2.2024, acc-0.2430, test loss-2.1979, acc-0.2566\n",
      "Iter-9970, train loss-2.1868, acc-0.3200, valid loss-2.2023, acc-0.2432, test loss-2.1978, acc-0.2568\n",
      "Iter-9980, train loss-2.2269, acc-0.1800, valid loss-2.2022, acc-0.2434, test loss-2.1977, acc-0.2567\n",
      "Iter-9990, train loss-2.1858, acc-0.3000, valid loss-2.2021, acc-0.2440, test loss-2.1976, acc-0.2567\n",
      "Iter-10000, train loss-2.1965, acc-0.2200, valid loss-2.2020, acc-0.2440, test loss-2.1975, acc-0.2571\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOQRBEAR1RHIIgiAI6rgKwJtFpsvEpUwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11379bf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nALnACQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11379b5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
