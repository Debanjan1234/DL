{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "#         y, _ = l.relu_forward(X=y)\n",
    "#         y, _ = l.lrelu_forward(X=y)\n",
    "#         y = l.elu_fwd(X=y)\n",
    "#         y, _ = l.softplus_forward(X=y)\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = np.exp(y)/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        y, _ = l.integ_tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "#             y, _ = l.relu_forward(X=y)\n",
    "#             y, _ = l.lrelu_forward(X=y)\n",
    "#             y = l.elu_fwd(X=y)\n",
    "#             y, _ = l.softplus_forward(X=y)\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = np.exp(y)/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            y, _ = l.integ_tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.0924\n",
      "Iter-20 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.0924\n",
      "Iter-30 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1100\n",
      "Iter-40 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1002\n",
      "Iter-50 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1002\n",
      "Iter-60 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1070\n",
      "Iter-70 train loss: 2.3027 valid loss: 2.3026, valid accuracy: 0.1070\n",
      "Iter-80 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1070\n",
      "Iter-90 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-100 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1126\n",
      "Iter-110 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-120 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-130 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-140 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-150 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-160 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-170 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-180 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-190 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-200 train loss: 2.3022 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-210 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-220 train loss: 2.3028 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-230 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-240 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-250 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-260 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-270 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-280 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-290 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-300 train loss: 2.3028 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-310 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-320 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-330 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-340 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-350 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-360 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-370 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-380 train loss: 2.3019 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-390 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-400 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-410 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-420 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-430 train loss: 2.3030 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-440 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-450 train loss: 2.3022 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-460 train loss: 2.3020 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-470 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-480 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-490 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-500 train loss: 2.3022 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-510 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-520 train loss: 2.3029 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-530 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-540 train loss: 2.3015 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-550 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-560 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-570 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-580 train loss: 2.3033 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-590 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-600 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-610 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-620 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-630 train loss: 2.3028 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-640 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-650 train loss: 2.3021 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-660 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-670 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-680 train loss: 2.3035 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-690 train loss: 2.3015 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-700 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-710 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-720 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-730 train loss: 2.3035 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-740 train loss: 2.3034 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-750 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-760 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-770 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-780 train loss: 2.3014 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-790 train loss: 2.3012 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-800 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-810 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-820 train loss: 2.3020 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-830 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-840 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-850 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-860 train loss: 2.3028 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-870 train loss: 2.3017 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-880 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-890 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-900 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-910 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-920 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-930 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-940 train loss: 2.3018 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-950 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-960 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-970 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-980 train loss: 2.3016 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-990 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-1000 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-1010 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-1020 train loss: 2.3032 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-1030 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-1040 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-1050 train loss: 2.3029 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1060 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1070 train loss: 2.3034 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1080 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1090 train loss: 2.3014 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1100 train loss: 2.3030 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1110 train loss: 2.3040 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1120 train loss: 2.3025 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1130 train loss: 2.3030 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1140 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1150 train loss: 2.3017 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1160 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1180 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1190 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.3025 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1210 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1220 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1230 train loss: 2.3034 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1240 train loss: 2.3016 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1250 train loss: 2.3031 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1260 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1270 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1280 train loss: 2.3012 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1290 train loss: 2.3031 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.3017 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1310 train loss: 2.3029 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1320 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1330 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1340 train loss: 2.3013 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1350 train loss: 2.2998 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1360 train loss: 2.3016 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1370 train loss: 2.3036 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1380 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1390 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3015 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1410 train loss: 2.3031 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1420 train loss: 2.3022 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1430 train loss: 2.3037 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1440 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1450 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1460 train loss: 2.3012 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1470 train loss: 2.3009 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1480 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1490 train loss: 2.3040 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1510 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1520 train loss: 2.3015 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1530 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1540 train loss: 2.3031 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1550 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1560 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1570 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1580 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1590 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3025 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1610 train loss: 2.3033 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1620 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1630 train loss: 2.3033 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1640 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1650 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1660 train loss: 2.3034 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1670 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1680 train loss: 2.3019 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1690 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3027 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1710 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1720 train loss: 2.3033 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1730 train loss: 2.3027 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1740 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1750 train loss: 2.3045 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1760 train loss: 2.3019 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1770 train loss: 2.3001 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1780 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1790 train loss: 2.3016 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3003 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1810 train loss: 2.3034 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1820 train loss: 2.3011 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1830 train loss: 2.3018 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1840 train loss: 2.3014 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1850 train loss: 2.3014 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1860 train loss: 2.3027 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1870 train loss: 2.3009 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1880 train loss: 2.3014 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1890 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3024 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1910 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1920 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1930 train loss: 2.3019 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1940 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1950 train loss: 2.3014 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1960 train loss: 2.3011 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1970 train loss: 2.3018 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1980 train loss: 2.3044 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1990 train loss: 2.3016 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3033 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2010 train loss: 2.3043 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2020 train loss: 2.3031 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2030 train loss: 2.3031 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2040 train loss: 2.3017 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2050 train loss: 2.3028 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2060 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2070 train loss: 2.3034 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2080 train loss: 2.3042 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2090 train loss: 2.2999 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-2110 train loss: 2.3007 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2120 train loss: 2.3029 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2130 train loss: 2.3032 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2140 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2150 train loss: 2.2993 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2160 train loss: 2.3012 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2170 train loss: 2.3029 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2180 train loss: 2.3016 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2190 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.2997 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2210 train loss: 2.3035 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2220 train loss: 2.3012 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2230 train loss: 2.3025 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2240 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2250 train loss: 2.2998 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2260 train loss: 2.3029 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2270 train loss: 2.3028 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2280 train loss: 2.3034 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2290 train loss: 2.3027 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3004 valid loss: 2.3021, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3008 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2320 train loss: 2.3017 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2330 train loss: 2.2990 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2340 train loss: 2.3026 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2350 train loss: 2.3024 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2360 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2370 train loss: 2.3031 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2380 train loss: 2.3016 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2390 train loss: 2.3009 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.3022 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2410 train loss: 2.3020 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2420 train loss: 2.3039 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2430 train loss: 2.3012 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2440 train loss: 2.3026 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2450 train loss: 2.3025 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2460 train loss: 2.2994 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2470 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2480 train loss: 2.3016 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2490 train loss: 2.3028 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2510 train loss: 2.3016 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2520 train loss: 2.3015 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2530 train loss: 2.3031 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2540 train loss: 2.3036 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2550 train loss: 2.3024 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2560 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2570 train loss: 2.3004 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2580 train loss: 2.3055 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2590 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.2992 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2610 train loss: 2.3041 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2620 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2630 train loss: 2.3046 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2640 train loss: 2.3005 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2650 train loss: 2.2999 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2660 train loss: 2.3022 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2670 train loss: 2.3015 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2680 train loss: 2.3022 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2690 train loss: 2.3039 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.3012 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2710 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2720 train loss: 2.3031 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2730 train loss: 2.3036 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2740 train loss: 2.3028 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2750 train loss: 2.3006 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2760 train loss: 2.3012 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2770 train loss: 2.3044 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2780 train loss: 2.3022 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2790 train loss: 2.3030 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.3024 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2810 train loss: 2.3027 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2820 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2830 train loss: 2.3000 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2840 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2850 train loss: 2.3001 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2860 train loss: 2.3032 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2870 train loss: 2.3066 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2880 train loss: 2.3048 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2890 train loss: 2.3015 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2910 train loss: 2.3042 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2920 train loss: 2.3052 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2930 train loss: 2.2985 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2940 train loss: 2.3030 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2950 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2960 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2970 train loss: 2.2969 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2980 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2990 train loss: 2.3040 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3010 train loss: 2.3039 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3020 train loss: 2.3023 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3030 train loss: 2.3004 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3040 train loss: 2.2990 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3050 train loss: 2.3029 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3060 train loss: 2.3042 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3070 train loss: 2.3024 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3080 train loss: 2.3028 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3090 train loss: 2.2986 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3110 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-3120 train loss: 2.3022 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3130 train loss: 2.3036 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3140 train loss: 2.3032 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3150 train loss: 2.2992 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3160 train loss: 2.3008 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3170 train loss: 2.3033 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3180 train loss: 2.3025 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3190 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3009 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3210 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3220 train loss: 2.3023 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3230 train loss: 2.3024 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3240 train loss: 2.3014 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3250 train loss: 2.3045 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3260 train loss: 2.3045 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3270 train loss: 2.3029 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3280 train loss: 2.3036 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3290 train loss: 2.2987 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.2989 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3310 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3320 train loss: 2.3015 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3330 train loss: 2.2990 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3340 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3350 train loss: 2.3019 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3360 train loss: 2.3006 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3370 train loss: 2.3021 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3380 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3390 train loss: 2.3013 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.2984 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3410 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3420 train loss: 2.3054 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3430 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3440 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3019 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3460 train loss: 2.3029 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3470 train loss: 2.2989 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3480 train loss: 2.3023 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3490 train loss: 2.2974 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.2985 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3510 train loss: 2.3024 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3520 train loss: 2.3012 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3530 train loss: 2.3028 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3540 train loss: 2.3031 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3550 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3560 train loss: 2.2995 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3570 train loss: 2.3043 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3580 train loss: 2.3004 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3590 train loss: 2.2965 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.3024 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3610 train loss: 2.2977 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3620 train loss: 2.3016 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3630 train loss: 2.3000 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3640 train loss: 2.3041 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3650 train loss: 2.3014 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3660 train loss: 2.2996 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3670 train loss: 2.3055 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3680 train loss: 2.3044 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3690 train loss: 2.3039 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3710 train loss: 2.2976 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3720 train loss: 2.3047 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3730 train loss: 2.2991 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3740 train loss: 2.2971 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3750 train loss: 2.3036 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3760 train loss: 2.2984 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3770 train loss: 2.2975 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3780 train loss: 2.3045 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3790 train loss: 2.2989 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3036 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3810 train loss: 2.3064 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3820 train loss: 2.3022 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3830 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3840 train loss: 2.3028 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3850 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3860 train loss: 2.3059 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3870 train loss: 2.3046 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3880 train loss: 2.3026 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3890 train loss: 2.2994 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3034 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3910 train loss: 2.3011 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3920 train loss: 2.3003 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3930 train loss: 2.3036 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3940 train loss: 2.3023 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3950 train loss: 2.3042 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3960 train loss: 2.2993 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3970 train loss: 2.3045 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3980 train loss: 2.2972 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3990 train loss: 2.3027 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3016 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4010 train loss: 2.3032 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4020 train loss: 2.3049 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4030 train loss: 2.3019 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4040 train loss: 2.3053 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4050 train loss: 2.3005 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4060 train loss: 2.3030 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4070 train loss: 2.3002 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4080 train loss: 2.3028 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4090 train loss: 2.3023 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4110 train loss: 2.3048 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4120 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4130 train loss: 2.3009 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-4140 train loss: 2.3014 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4150 train loss: 2.3015 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4160 train loss: 2.3028 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4170 train loss: 2.3039 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4180 train loss: 2.3065 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4190 train loss: 2.3048 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3035 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4210 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4220 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4230 train loss: 2.3012 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4240 train loss: 2.3006 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4250 train loss: 2.3045 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4260 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4270 train loss: 2.3052 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4280 train loss: 2.2999 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4290 train loss: 2.3040 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.3082 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4310 train loss: 2.3044 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4320 train loss: 2.3018 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4330 train loss: 2.3039 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4340 train loss: 2.3040 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4350 train loss: 2.3034 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4360 train loss: 2.3004 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4370 train loss: 2.3032 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4380 train loss: 2.3045 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4390 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.3002 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4410 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4420 train loss: 2.2985 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4430 train loss: 2.3043 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4440 train loss: 2.3067 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4450 train loss: 2.3028 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4460 train loss: 2.3039 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4470 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4480 train loss: 2.2976 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4490 train loss: 2.3022 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4510 train loss: 2.3023 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4520 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4530 train loss: 2.2991 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4540 train loss: 2.3037 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4550 train loss: 2.3089 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4560 train loss: 2.3038 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4570 train loss: 2.3066 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4580 train loss: 2.2958 valid loss: 2.3017, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3014 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.3035 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4610 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4620 train loss: 2.3054 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4630 train loss: 2.3049 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4640 train loss: 2.3006 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4650 train loss: 2.2988 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4660 train loss: 2.2979 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4670 train loss: 2.2989 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4680 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4690 train loss: 2.3039 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.2973 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4710 train loss: 2.3018 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4720 train loss: 2.3004 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4730 train loss: 2.3069 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4740 train loss: 2.3048 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4750 train loss: 2.3057 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4760 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4770 train loss: 2.3008 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4780 train loss: 2.3007 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4790 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.3002 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4810 train loss: 2.3028 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4820 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4830 train loss: 2.2920 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4840 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4850 train loss: 2.3027 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4860 train loss: 2.3034 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4870 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4880 train loss: 2.3015 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4890 train loss: 2.3016 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4910 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4920 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4930 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4940 train loss: 2.2975 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4950 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4960 train loss: 2.3017 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4970 train loss: 2.3042 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4980 train loss: 2.3058 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4990 train loss: 2.2988 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3020 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5010 train loss: 2.3036 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5020 train loss: 2.3001 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5030 train loss: 2.2988 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5040 train loss: 2.3029 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5050 train loss: 2.3017 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5060 train loss: 2.2953 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5070 train loss: 2.2997 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5080 train loss: 2.3011 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5090 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3052 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5110 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5120 train loss: 2.3066 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5130 train loss: 2.3052 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5140 train loss: 2.3081 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5150 train loss: 2.3023 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5160 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5170 train loss: 2.3067 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5180 train loss: 2.3051 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5190 train loss: 2.2964 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.3033 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5210 train loss: 2.3038 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5220 train loss: 2.2985 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5230 train loss: 2.3032 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5240 train loss: 2.3037 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5250 train loss: 2.3022 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5260 train loss: 2.3032 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5270 train loss: 2.3023 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5280 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5290 train loss: 2.2987 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5310 train loss: 2.2960 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5320 train loss: 2.3014 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5330 train loss: 2.3017 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5340 train loss: 2.3057 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5350 train loss: 2.3047 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5360 train loss: 2.3011 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5370 train loss: 2.3006 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5380 train loss: 2.3015 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5390 train loss: 2.3011 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.2989 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5410 train loss: 2.3007 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5420 train loss: 2.3023 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5430 train loss: 2.3032 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5440 train loss: 2.3025 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5450 train loss: 2.3032 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5460 train loss: 2.3015 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5470 train loss: 2.2989 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5480 train loss: 2.3050 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5490 train loss: 2.2994 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3012 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5510 train loss: 2.3078 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5520 train loss: 2.2979 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5530 train loss: 2.3045 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5540 train loss: 2.2948 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5550 train loss: 2.3040 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5560 train loss: 2.2997 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5570 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5580 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5590 train loss: 2.2987 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.3019 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5610 train loss: 2.2999 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5620 train loss: 2.3034 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5630 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5640 train loss: 2.3066 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5650 train loss: 2.2984 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5660 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5670 train loss: 2.2992 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5680 train loss: 2.3035 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5690 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.2979 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5710 train loss: 2.3014 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5720 train loss: 2.3023 valid loss: 2.3015, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5740 train loss: 2.2961 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5750 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5760 train loss: 2.3039 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5770 train loss: 2.2994 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5780 train loss: 2.2983 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5790 train loss: 2.3021 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3088 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5810 train loss: 2.2995 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5820 train loss: 2.2961 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5830 train loss: 2.3049 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5840 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5850 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5860 train loss: 2.3015 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5870 train loss: 2.3051 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5880 train loss: 2.2978 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5890 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.3001 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5910 train loss: 2.3007 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5920 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5930 train loss: 2.2987 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5940 train loss: 2.2996 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5950 train loss: 2.2948 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5960 train loss: 2.2970 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5970 train loss: 2.3017 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5980 train loss: 2.3043 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5990 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6010 train loss: 2.3024 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6020 train loss: 2.3024 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6030 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6040 train loss: 2.3045 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6050 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6060 train loss: 2.3033 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6070 train loss: 2.3049 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6080 train loss: 2.3005 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6090 train loss: 2.2997 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.2950 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6110 train loss: 2.2966 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6120 train loss: 2.3085 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6130 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6140 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6150 train loss: 2.3048 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6160 train loss: 2.2983 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6170 train loss: 2.3036 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6180 train loss: 2.3000 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-6190 train loss: 2.2988 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6210 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6220 train loss: 2.2915 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6230 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6240 train loss: 2.2980 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6250 train loss: 2.3085 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6260 train loss: 2.3001 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6270 train loss: 2.2963 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6280 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6290 train loss: 2.3024 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6310 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6320 train loss: 2.2985 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6330 train loss: 2.3054 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6340 train loss: 2.2975 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6350 train loss: 2.3058 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6360 train loss: 2.3052 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6370 train loss: 2.3089 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6380 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6390 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.2965 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6410 train loss: 2.3054 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6420 train loss: 2.3046 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6430 train loss: 2.2979 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6440 train loss: 2.2972 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6450 train loss: 2.3041 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6460 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6470 train loss: 2.2963 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6480 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6490 train loss: 2.2975 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.2968 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6510 train loss: 2.3030 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6520 train loss: 2.2933 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6530 train loss: 2.3007 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6540 train loss: 2.2984 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6550 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6560 train loss: 2.2982 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6570 train loss: 2.3010 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6580 train loss: 2.2984 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6590 train loss: 2.3006 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.3067 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6610 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6620 train loss: 2.3085 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6630 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6640 train loss: 2.2937 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6650 train loss: 2.2976 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6660 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6670 train loss: 2.3008 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6680 train loss: 2.3060 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6690 train loss: 2.3015 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.3005 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6710 train loss: 2.3066 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6720 train loss: 2.3024 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6730 train loss: 2.3024 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6740 train loss: 2.3031 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6750 train loss: 2.3002 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6760 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6770 train loss: 2.2880 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6780 train loss: 2.2977 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6790 train loss: 2.3068 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.2980 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6810 train loss: 2.2995 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6820 train loss: 2.2932 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6830 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6840 train loss: 2.2965 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6850 train loss: 2.3023 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6860 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.3040 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6880 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6890 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6910 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6920 train loss: 2.3087 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6930 train loss: 2.3023 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6940 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6950 train loss: 2.2967 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6960 train loss: 2.2990 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6970 train loss: 2.3072 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6980 train loss: 2.2972 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6990 train loss: 2.2979 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7010 train loss: 2.3068 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7020 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7030 train loss: 2.3054 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7040 train loss: 2.3009 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7050 train loss: 2.2942 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7060 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7070 train loss: 2.3012 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7080 train loss: 2.3089 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7090 train loss: 2.2997 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7110 train loss: 2.3016 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7120 train loss: 2.3058 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7130 train loss: 2.2955 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7140 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7150 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7160 train loss: 2.3054 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7170 train loss: 2.2980 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7180 train loss: 2.2994 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7190 train loss: 2.2967 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.3031 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7210 train loss: 2.3035 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7220 train loss: 2.2988 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7230 train loss: 2.3082 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7240 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7250 train loss: 2.2963 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7260 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7270 train loss: 2.2965 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7280 train loss: 2.2976 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7290 train loss: 2.2964 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.2977 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7310 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7320 train loss: 2.2965 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7330 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7340 train loss: 2.2993 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7350 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7360 train loss: 2.3023 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7370 train loss: 2.3017 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7380 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7390 train loss: 2.2939 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.2993 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7410 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7420 train loss: 2.3009 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7430 train loss: 2.3002 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7440 train loss: 2.2974 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7450 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7460 train loss: 2.2964 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7470 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7480 train loss: 2.2969 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7490 train loss: 2.3061 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7510 train loss: 2.3043 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7520 train loss: 2.2999 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7530 train loss: 2.3035 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7540 train loss: 2.2987 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7550 train loss: 2.3067 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7560 train loss: 2.2964 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7570 train loss: 2.3037 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7580 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7590 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.2931 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7610 train loss: 2.3058 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7620 train loss: 2.2954 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7630 train loss: 2.3064 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7640 train loss: 2.3048 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7650 train loss: 2.3036 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7660 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7670 train loss: 2.2999 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7680 train loss: 2.3036 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7690 train loss: 2.3001 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.3054 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7710 train loss: 2.3038 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7720 train loss: 2.2973 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7730 train loss: 2.3049 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7740 train loss: 2.3046 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7750 train loss: 2.3049 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7760 train loss: 2.3050 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7770 train loss: 2.2974 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7780 train loss: 2.3040 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7790 train loss: 2.3038 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7810 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7820 train loss: 2.3064 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7830 train loss: 2.2943 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7840 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7850 train loss: 2.3020 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7860 train loss: 2.2947 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7870 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7880 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7890 train loss: 2.2967 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.3030 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7910 train loss: 2.2998 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7920 train loss: 2.2980 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7930 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7940 train loss: 2.2997 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7950 train loss: 2.3008 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7960 train loss: 2.3015 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7970 train loss: 2.3019 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7980 train loss: 2.2947 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-7990 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.3018 valid loss: 2.3012, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.3036 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8020 train loss: 2.3004 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8030 train loss: 2.3082 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8040 train loss: 2.3080 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8050 train loss: 2.3014 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8060 train loss: 2.3034 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8070 train loss: 2.3061 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8080 train loss: 2.2945 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8090 train loss: 2.3025 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3055 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8110 train loss: 2.2999 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8120 train loss: 2.3108 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8130 train loss: 2.3043 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8140 train loss: 2.2994 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8150 train loss: 2.2966 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8160 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8170 train loss: 2.2950 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8180 train loss: 2.3026 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8190 train loss: 2.2984 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.2999 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8210 train loss: 2.3043 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8220 train loss: 2.2985 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8230 train loss: 2.2988 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8240 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8250 train loss: 2.3005 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8260 train loss: 2.3104 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8270 train loss: 2.2983 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8280 train loss: 2.2946 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8290 train loss: 2.3023 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.3018 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8310 train loss: 2.3040 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8320 train loss: 2.2998 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8330 train loss: 2.3067 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8340 train loss: 2.2961 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8350 train loss: 2.3010 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8360 train loss: 2.3054 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8370 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8380 train loss: 2.2983 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8390 train loss: 2.2985 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.3041 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8410 train loss: 2.2945 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8420 train loss: 2.2993 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8430 train loss: 2.2953 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8440 train loss: 2.3014 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8450 train loss: 2.3056 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8460 train loss: 2.2966 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8470 train loss: 2.3043 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8480 train loss: 2.3026 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8490 train loss: 2.2978 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8510 train loss: 2.3066 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8520 train loss: 2.3114 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8530 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8540 train loss: 2.3092 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8550 train loss: 2.3039 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8560 train loss: 2.3074 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8570 train loss: 2.2950 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8580 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8590 train loss: 2.3009 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.3063 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8610 train loss: 2.3064 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8620 train loss: 2.3095 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8630 train loss: 2.3071 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8640 train loss: 2.2927 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8650 train loss: 2.2962 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8660 train loss: 2.2905 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8670 train loss: 2.2990 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8680 train loss: 2.2971 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8690 train loss: 2.3070 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.3033 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8710 train loss: 2.3073 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8720 train loss: 2.3074 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8730 train loss: 2.2990 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8740 train loss: 2.3157 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8750 train loss: 2.3036 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8760 train loss: 2.3051 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8770 train loss: 2.2991 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8780 train loss: 2.3037 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8790 train loss: 2.2987 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.2998 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8810 train loss: 2.3038 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8820 train loss: 2.2982 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8830 train loss: 2.2962 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8840 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8850 train loss: 2.2987 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8860 train loss: 2.3109 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8870 train loss: 2.3018 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8880 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8890 train loss: 2.2935 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.3066 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8910 train loss: 2.3011 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8920 train loss: 2.3076 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8930 train loss: 2.3039 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8940 train loss: 2.3033 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8950 train loss: 2.3002 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8960 train loss: 2.3055 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8970 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8980 train loss: 2.2999 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8990 train loss: 2.2889 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.3010 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9010 train loss: 2.2907 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9020 train loss: 2.2988 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9030 train loss: 2.3029 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9040 train loss: 2.3029 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9050 train loss: 2.2967 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9060 train loss: 2.3058 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9070 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9080 train loss: 2.2970 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9090 train loss: 2.3063 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.2934 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9110 train loss: 2.3013 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9120 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9130 train loss: 2.2895 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9140 train loss: 2.3004 valid loss: 2.3012, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.2997 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9160 train loss: 2.3042 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9170 train loss: 2.3055 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9180 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9190 train loss: 2.2973 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.2980 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9210 train loss: 2.2982 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9220 train loss: 2.3060 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9230 train loss: 2.3095 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9240 train loss: 2.3047 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9250 train loss: 2.3088 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9260 train loss: 2.3012 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9270 train loss: 2.3017 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9280 train loss: 2.2991 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9290 train loss: 2.2996 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.3027 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9310 train loss: 2.3001 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9320 train loss: 2.3075 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9330 train loss: 2.3033 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9340 train loss: 2.3043 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9350 train loss: 2.2996 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9360 train loss: 2.2971 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9370 train loss: 2.2916 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9380 train loss: 2.2957 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9390 train loss: 2.3090 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2987 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9410 train loss: 2.3093 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9420 train loss: 2.2972 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9430 train loss: 2.2996 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9440 train loss: 2.2961 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9450 train loss: 2.3040 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9460 train loss: 2.3047 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9470 train loss: 2.3142 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9480 train loss: 2.2992 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9490 train loss: 2.3083 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.3042 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9510 train loss: 2.3080 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9520 train loss: 2.3028 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9530 train loss: 2.2901 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9540 train loss: 2.3039 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9550 train loss: 2.2974 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9560 train loss: 2.2964 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9570 train loss: 2.3009 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9580 train loss: 2.3079 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9590 train loss: 2.3039 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.3098 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9610 train loss: 2.3027 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9620 train loss: 2.3013 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9630 train loss: 2.2983 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9640 train loss: 2.2996 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9650 train loss: 2.2980 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9660 train loss: 2.2974 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9670 train loss: 2.2993 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9680 train loss: 2.3064 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9690 train loss: 2.2953 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.3043 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9710 train loss: 2.2960 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9720 train loss: 2.2940 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9730 train loss: 2.2960 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9740 train loss: 2.3043 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9750 train loss: 2.2973 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9760 train loss: 2.2981 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9770 train loss: 2.3043 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9780 train loss: 2.3031 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9790 train loss: 2.2919 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.2954 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9810 train loss: 2.2998 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9820 train loss: 2.3055 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9830 train loss: 2.3077 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9840 train loss: 2.3042 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9850 train loss: 2.3038 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9860 train loss: 2.3019 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9870 train loss: 2.2931 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9880 train loss: 2.2960 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9890 train loss: 2.2962 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3087 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9910 train loss: 2.2935 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9920 train loss: 2.3066 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9930 train loss: 2.2981 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9940 train loss: 2.3079 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9950 train loss: 2.2966 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9960 train loss: 2.2998 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9970 train loss: 2.2992 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9980 train loss: 2.2986 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9990 train loss: 2.3036 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.3031 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3012\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAqNCQ1EURQnM/wOx9CO2sAJrmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122096e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122096a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
