{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        self.W_fixed = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y = np.tanh(y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = np.tanh(y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def onehot(self, labels):\n",
    "        # y = np.zeros([labels.size, np.max(labels) + 1])\n",
    "        y = np.zeros([labels.size, self.C])\n",
    "        y[range(labels.size), labels] = 1.\n",
    "        return y\n",
    "\n",
    "    def squared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        data_loss = 0.5 * np.sum((y_pred - self.onehot(y_train))**2)/ m\n",
    "        return data_loss\n",
    "\n",
    "    def dsquared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        grad_y = (y_pred - self.onehot(y_train))/ m\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.squared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        dy = self.dsquared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        \n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-0.4511, acc-0.0900, valid loss-0.4504, acc-0.1082, test loss-0.4507, acc-0.1053\n",
      "Iter-20, train loss-0.4509, acc-0.1200, valid loss-0.4502, acc-0.1156, test loss-0.4505, acc-0.1118\n",
      "Iter-30, train loss-0.4514, acc-0.1000, valid loss-0.4500, acc-0.1232, test loss-0.4503, acc-0.1167\n",
      "Iter-40, train loss-0.4496, acc-0.1400, valid loss-0.4498, acc-0.1318, test loss-0.4501, acc-0.1257\n",
      "Iter-50, train loss-0.4493, acc-0.1300, valid loss-0.4495, acc-0.1466, test loss-0.4498, acc-0.1394\n",
      "Iter-60, train loss-0.4481, acc-0.1700, valid loss-0.4491, acc-0.1582, test loss-0.4494, acc-0.1498\n",
      "Iter-70, train loss-0.4501, acc-0.1500, valid loss-0.4488, acc-0.1666, test loss-0.4491, acc-0.1574\n",
      "Iter-80, train loss-0.4489, acc-0.1500, valid loss-0.4484, acc-0.1756, test loss-0.4487, acc-0.1673\n",
      "Iter-90, train loss-0.4459, acc-0.1900, valid loss-0.4480, acc-0.1858, test loss-0.4482, acc-0.1760\n",
      "Iter-100, train loss-0.4467, acc-0.1400, valid loss-0.4477, acc-0.1906, test loss-0.4479, acc-0.1826\n",
      "Iter-110, train loss-0.4479, acc-0.1700, valid loss-0.4473, acc-0.1946, test loss-0.4475, acc-0.1853\n",
      "Iter-120, train loss-0.4470, acc-0.1800, valid loss-0.4468, acc-0.2004, test loss-0.4471, acc-0.1922\n",
      "Iter-130, train loss-0.4455, acc-0.2600, valid loss-0.4463, acc-0.2034, test loss-0.4465, acc-0.1988\n",
      "Iter-140, train loss-0.4493, acc-0.1300, valid loss-0.4457, acc-0.2076, test loss-0.4460, acc-0.2053\n",
      "Iter-150, train loss-0.4464, acc-0.2200, valid loss-0.4453, acc-0.2116, test loss-0.4456, acc-0.2088\n",
      "Iter-160, train loss-0.4436, acc-0.2400, valid loss-0.4445, acc-0.2164, test loss-0.4448, acc-0.2161\n",
      "Iter-170, train loss-0.4437, acc-0.2600, valid loss-0.4439, acc-0.2186, test loss-0.4442, acc-0.2205\n",
      "Iter-180, train loss-0.4419, acc-0.2100, valid loss-0.4431, acc-0.2216, test loss-0.4434, acc-0.2249\n",
      "Iter-190, train loss-0.4455, acc-0.2100, valid loss-0.4423, acc-0.2254, test loss-0.4426, acc-0.2293\n",
      "Iter-200, train loss-0.4405, acc-0.2200, valid loss-0.4415, acc-0.2274, test loss-0.4418, acc-0.2312\n",
      "Iter-210, train loss-0.4418, acc-0.2100, valid loss-0.4405, acc-0.2352, test loss-0.4409, acc-0.2383\n",
      "Iter-220, train loss-0.4404, acc-0.2500, valid loss-0.4396, acc-0.2414, test loss-0.4399, acc-0.2472\n",
      "Iter-230, train loss-0.4389, acc-0.3000, valid loss-0.4387, acc-0.2512, test loss-0.4390, acc-0.2555\n",
      "Iter-240, train loss-0.4365, acc-0.3400, valid loss-0.4375, acc-0.2590, test loss-0.4379, acc-0.2647\n",
      "Iter-250, train loss-0.4386, acc-0.2300, valid loss-0.4363, acc-0.2664, test loss-0.4367, acc-0.2738\n",
      "Iter-260, train loss-0.4336, acc-0.2400, valid loss-0.4351, acc-0.2738, test loss-0.4355, acc-0.2813\n",
      "Iter-270, train loss-0.4381, acc-0.2100, valid loss-0.4339, acc-0.2846, test loss-0.4344, acc-0.2886\n",
      "Iter-280, train loss-0.4297, acc-0.2900, valid loss-0.4325, acc-0.2950, test loss-0.4330, acc-0.2953\n",
      "Iter-290, train loss-0.4362, acc-0.2900, valid loss-0.4312, acc-0.2990, test loss-0.4317, acc-0.3053\n",
      "Iter-300, train loss-0.4325, acc-0.2600, valid loss-0.4295, acc-0.3108, test loss-0.4300, acc-0.3178\n",
      "Iter-310, train loss-0.4280, acc-0.3800, valid loss-0.4279, acc-0.3288, test loss-0.4284, acc-0.3321\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
