{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane 1\n",
    "* automobile 2\n",
    "* bird 3\n",
    "* cat 4\n",
    "* deer 5\n",
    "* dog 6\n",
    "* frog 7\n",
    "* horse 8\n",
    "* ship 9\n",
    "* truck 10\n",
    "\n",
    "* Total 10 classes (Aras changed above/this section a bit)\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAHF9JREFUeJzt3UmPZOl1HuAvxsyMrKzKqsqau6rYA5vNbropkjJJmYIs\nUIBXWtn+BV7YO/8Yr73wymtDNAwIggwSMEmBNMeW2Wz2VOzumquyco6M2QttzI2Bc5gChYPn2Z88\nEd+9cd+8q7ezWq0aAFBT9w/9AQCAfzyCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/T/0B/jH8l/+w79fZebGx9PwTK+f\n+3+pc/tGeGZvtJHa9faFYWruk1/+LDzznR/+PLVrbzILz/R6ybPvdFJzg7X18MylKzupXec34t/t\n83eupHb9+be+Hp6Zz+LXq7XWnu0fpeYGWxfDM+9+8NvUrr/97g/jQ8nnwNogN3dhMAjPDPuL1K5p\n4lrPZ7nfWFstU2NrvbXwzMkq/rxvrbUXp/F46eZ+Lu073/+75EH+P7t/3z8AAPzTJegBoDBBDwCF\nCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+te3P84NddfxJuT\nBv1UUV67v5qEZ94f5yqQ3v7iK6m55TT+Ga/t5NraNlLfLXf22fa6k0n8PPZ3X6R2HXXiTWOT03Fq\n15e/+o3wzOzkNLXr2fPceVxbjzc3LqcHqV0ba/H7atlyrWtXt86l5r70ymvhmadP7qd2jceH4Zmj\no1xLYevGW/laa22tPw/P3Lx+IbVrNrwanvngV/dSu86CN3oAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZUpuPT9dScyfj/fDMsJMr92iLeKFCtzNMrXr2\n28epuZ88+Cw88+snudKS1SReSpEtp1lfX0/NzebxopnWzf0/vb4Rv4f3xrlilR+983545sblXCHI\nZJ67ZpkCo7XkE24wSHzG3NG3L7z6amruc3fuhme2t0apXY8e3gvPLGe55+K5izdSc4tBvPRotJYr\n3rm5Ey8i+rSXO/uz4I0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLLtdeNeriFrtxtvJ+ssJqldl/vx4z93/mJq1+lxvJWvtdb2DuPf7eB0ltq1\nSpz9YpFok2ut9ZKfsZ/533gWb11rrbXjafzsz61yu370i1+GZ15/7bXUrjdevZOa6w/j7V+f+1yu\nGe54OQjPPH74NLXr4HCcmmvrm+GRP/6zt1Orfv7j74VnxvN4G2VrrR3Oci1vz4/jz8ZL41zD3q3e\nYXjm9Cjb2vj780YPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANA\nYYIeAAorW2qz1tlNzd0YxYsYtlu8AKO11i5d3AjPfLyKlym01trmxjI1t9aJl6SMOrnbara5Fp+Z\n58ppTie5IqJF4n/jjVGupGO4Fr+vrt++kdp186Xb4ZlnR7lCkEcHuRKXb3zj6+GZ3cePUrv+9b/5\nVnjmf/z3v07t+uEP/i41d+dLXw3PfPvtr6V2fXj/o/DMx9//cWrX/nQrNXc0jz/jvvjP42fYWmvj\n2YvwzM7OemrXWfBGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNAD\nQGGCHgAKE/QAUFjZ9rrhZu6rvbJ1NTzz8iq368Iw0Wa0/1lq12g73gzXWmvHw5PwzHKwSO364z+K\nN0lduxq/Xq219tEHH6TmPv3kfnim28u1G67m8Xa49W7u7P/kG/Gzfxq/NVprrf3oe99Nzb333p3w\nzGKc/JCbF8Mje8e5RsSjWe5964OHz8Mzx8teatfxPP4Zn+zlzmOyfi419/m7r4Rntq/dTO16+jx+\n9t/+9lupXWfBGz0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCF\nCXoAKEzQA0BhZdvrjqa5xrALvc3wzOzZi9SuT/fiTWh/+uU3UrvG0+PU3K1lfGZ9tErt+uZ2/Ozf\nvLKT2nWyzH3GZ2vxFsCT/dz9sZjGZ/rTw9Suu598HJ7Z2Jundl26sp2am/39z8Iz2ebAH/7q3fDM\new8epHadznMtb/c/iTdZPnn+NLXr61/5Znjm7vbt1K7/9F//W2puOn4UnvnJj5+ldj1+/GF45qt/\nkXt2nwVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nsLKlNld666m5W60Xnjl/fiu16+cv4qUULyb7qV13r99Izf3bJy+HZwYHuQKdy+/Hz2Ptw4epXYvl\nLDX3uU58ZrBIDLXWuv34Pbzo5EpcJj/6aXjmQrKMZbkTLy9qrbXFPNGwdLBI7TrfOxeemRzn7vtL\n8UdOa6210Wocnjl49NvUrltffD08s7WZewZ//dVbqbkn+/EWqEdHJ6ldJye74ZmP3n8/tesseKMH\ngMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGx7\n3Rtbo9Tc5vNn4ZleN9Gq1Vp7/aWXwjOHj5+mdrVVrkHtVmcVnhkNc7t6iUaozjL++VprLd5z9Q8m\n3cT/xsO11K7BKv7d+pmGt9baoBtv85tt5WrXVie51rv5JH4ei5a7F69143fItzdyrXzTzjA1t7h5\nLTyzfu9eatdJ5iMmWz3feuO11NyNk/g1uzGbp3a9/urN8MxrO/FGxLPijR4AChP0AFCYoAeAwgQ9\nABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21Gb3wUepuck8XoIx7uWKRE4u\nxEsONk7i5SOttXb67oepuUVvEZ6Zb+Zuq24vXkqxlixx6bT11Nw8UQ60WOY+42owiM+kNuXm+ldf\nSe3a2su9X5wmLtn07sXUrovzo/DM5mmuKmm+lytWOXqyH545efD91K6H//sX4Znzb72e2vX8Ua64\nazq6FJ6Zj1Or2snzF+GZg0G2Suv3540eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdc+P9lJznx6fhmfmy1z71LBzPTwzuriT2vV8fJiau95b\nC89snOb+f1wcxJv5JtNcm1/byZ3j5uuvhWdOE01orbV29OwgPLO2jLfrtdZabzIJz0ye5u6ptpZr\nlOtsx9se+51cn9/yIP4c2Hgr1+bXhvHv1Vproyfx6rXj+/dTu/Z+/UF4ZvnJ49SurUtbqbnd7XhL\n5PNHud/mwyefhWdeHt5I7ToL3ugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGg\nMEEPAIUJegAoTNADQGGCHgAKK9te9+I03j7VWmuPTuJtRrOD49SunWtXwjOr21dTu9Yu5hqh1g7i\nzXz9B09Tu6ZHJ+GZoxZvrGqttcW5jdTc4O6d8Ey/s0jt2tyOn8fsN5+kds0SLYCn3Vxz4NafvZma\nO9l7Fh9679epXW2eeAd6mPh8rbXJMte0Obh+Mzxz/V9+M7VrbaMXntn9zYepXdsn8V2ttXbhbrxp\n85NHuYa9jV68FXEwGKZ2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgsLKlNrdvv5Sa6358PzyzMU6taotpvBhhrTNI7XpxfJCa+8Gnn4Vnbp4epna9\n0eIHOUmUsbTW2vh+/Dq31tr0p7+K72rx69xaa51bt8Izp69fT+06mY/CM2+/miunOe6eS82NH9wL\nzwz3c+VW8/PxApLpJ8lCoce5UqzB1SfhmZNruVKswaUL4ZmLf/HV1K69Tx+m5rZ34mU4Xz13N7Xr\nb/7Xi/DM2na8xOyseKMHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAorGx73fWb11Jzh/efhWdGFzupXa2zFh4ZdHO7Hj57npr7z7/4P+GZL1zOtZP9\nx/XN8Mwo+a/q6vgoNbf7Try9bvdKvPmrtdY+msRbzabJprybr98Mz9y5mPte04ePU3PnEq1mneU0\ntasdxn9na92N1KqD8UlqbvHRR+GZ1YNHqV0vtuLPqs0v5BpEb778amru9FH8vroyij9zWmvtK196\nLTxz++XceZwFb/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAU\nJugBoLCypTb7ixepuf5qPzwz6OeOcdqLF5DszcepXbvjXNnJfBX/bgeDXLnH/cEoPLO9mqd2Tbu5\nudVqEp7ZX+ZKSz57Ei+1Od9dT+16kbhkf3X/r1K7vnDrVmru1Uvx73Z57Xpq1/G9++GZxTh+vVpr\nbbXI3YsvXjxN7Mo9B6br8VKb2X68IKy11qa/fD81N0oUOk3WB6ldd998Kzwze/Db1K6z4I0eAAoT\n9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtdcPV\nMjXXX87CMzvdXAPStBdvrerPpqldJ6e587h15Up45qWXb6d23T9KNPOtcm1cw2RrVWce/8lMl/HG\nu9Zau3F5JzzTzxWhtYOnj8Izq91cK9+D57mWt/3RMDxzZxL/PbfWWvdZvL2ujXOH353n3rfG8/g5\nnixyz49VohVxNO6kdj28/1lqbtSJ7zue567Z9iQ+t/P266ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQ\nA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAorGypzcZ4lJp7ML8QnrnaPU3tujjeC8/0\nnzxM7ZofvkjNffHNl8Mzd77w+dSu3V+8F5650emldrVBrgxnsIr/b7xxlCtx6bf4ZxyNNlK7fvPh\nvfDMznHuPeGVz11KzX02jBfUPP4g93vZONwNz3TmuXuqs8jdw6eJUqxpN3fNpsfxXbuLw9Su0eh8\nau5wGi+POp7krtnu/cfhmf6d66ldZ8EbPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeA\nwgQ9ABQm6AGgMEEPAIUJegAoTNADQGFl2+v2j+NNV6219t39eEvT/HJqVfvWchqe2XjyKLVrfXaS\nmvvK174dnrl5+7XUru/86J3wzP4k1xy46Ofuj1miLW9j1UntOv0sfq17l3LNcK9c3AnPnC72U7v6\nm8PU3Nt/+vXwzG680Owf5n7yJDwzWeaa0Jb9tdTcOHFfbW4mH1Ybm+GR8TDXyre8fDE1d9ri+x49\njbcUttba/t6z8MyLX7+f2vWXqanf5Y0eAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh\ngh4AChP0AFCYoAeAwgQ9ABQm6AGgsLLtddODB6m5D54/Ds+MZ7k2ru2X4o1hXx7kWte2+vFWvtZa\ne/n27fDM+XO5BrXJIt7mNzmJz7TW2nCwSM2druL7ht3c/TGcxq/ZeDfXxtXtxx8Fy16ure3x81wD\n44t3fxWeGa3nGtQO18/FZzZGqV2Tc1upuePj4/DMaCf329ydxlsiD+e531h3Nk7NPXx0FN+1Hm/l\na621g1n8ObB5kGt7PAve6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhQl6AChM0ANAYWVLbf7V3VxZwdPdeJnFjz8+Se36m3vxkoONV3Lfa3RuLTW31YsXdcwO4wUYrbW2\n6MRLMI4nuV3rvdytv+gl/jfu5P6fXnbjc7vH8WKP1lpbncYLdIbHubOf7eWKiFYffhKeGSXfZaaj\n8+GZd+aT1K57z56k5taX8ZnhMlcYM1iP/146s05q1+lerpjpeBUvB+qfG6R2LQbx73b34nZq11nw\nRg8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFBY\n2fa612/mvtq/G90Jz9xeu5/a9T/fizeN/e29WWrXH929mZo7+vDj8Mxe8v/H3jJex7U3zTUHXhnF\nm65aa22x6oVnZsvcNXu6ip/Hs1G8fbG11k778fa6rU7uN7Z5IXf2y2n8M7bnB6lda2vxlsjPTnPN\ncM8Xq9Tc9UG8eW20mbs/tjbj57Ea59oNn01z59jvxZ8Fvd3c8+NLq2F45txh7jlwFrzRA0Bhgh4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCypbaTJJlJ5fWO+GZ\nP3l9J7Xr2XG8tOQn9/dTu959/CI19/lEUcd0mLutVsv4/52Hp5Pcrkm8lKK11gbr8e+2WuZKS1pi\nbmNtPbXqcBUvIDm4cy216/Jbb6TmevGfS3vnr7+X2nU7cV+9dPFKalebTFNj6/34gezPcoUxx8/j\nz9PryYKlmzuXU3PDbvy3OdjNPU/vHsYLyW5vb6d2nQVv9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoA\nKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIWVba/r9HJfrTOPt1bd2M41hv2Lly+EZw6m\n8Zax1lq7t5dr8zvpxdv8rt6+ndrVG47CM6fzXDPc6eFhaq4/W4RnhoON1K743dHa/PHT1K7zi3l4\nZnKQu6d2Z4kautba9sWL8ZlO7l1mcBr/brc2N1O7hsn3rc7mWnxmkPuM3aN4w961fvz33FpriQLR\n1lpr3Un8t3mSfA5c6MXvj1fv5HLiLHijB4DCBD0AFCboAaAwQQ8AhQl6AChM0ANAYYIeAAoT9ABQ\nmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91qlatAWi0T7WTLeONda629eSl+/E9vnEvtOp7kPuN8\nHG/L27l8JbVr/Vy8r21vmWuvm01nqbl5Ym7SyzUOdju98Mz55L/umV6t6cF+btlp7jxWj56EZ15q\nuefAoBdv89sa587jai/Xbvgi0Ui5thVvAGytteUsfmPNT/ZSuw4muVbERHldW06OU7tuvHk1PPPy\nndxz8Sx4oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8A\nhZUttVl2cv/DLFq8SKTNcwUpF/rxwo2v3N5J7Xp+uJuamz5+GJ6ZHeeKIoab8XKP0+R1nq1yc91l\n/FovZom2jdZaZxG/P+bJ85gOMuUv8eKX1lrrzHPnsegN40PdXKnNYh7/bqtkWc/6YpCaW82m4ZlH\n67mimdla/OyXa6lVbbCZO4+Tk/h5DFfL1K4rd66HZ9b7ifv3jHijB4DCBD0AFCboAaAwQQ8AhQl6\nAChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKKxse91wYzM111sfhWeme0epXZlW\ns5vb8c/XWmv/bD/XrPXu3uPwzKMHn6R2HYwPwjNHy1z71Gk39z/uYLkKz8xXuba27ir+8zzu5Nra\nTlbxuX7yPWE5yV2z5SR+D3eS7XUtcZ1P+7nrvEw05bXW2nHmM65NUrtaN/7d1ge5+rrlIt5C11pr\nm8v4d3vt2lZq18Vh/OxPnueaA3Of8Hd5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QA\nUJigB4DCBD0AFCboAaAwQQ8AhZUttWndXmqs0xmEZ/obqVXttDsLzwwSZQqttXbnRq4M5+PP4gUT\n08lxatdiGd+1N88VYDzr5G79rV78vuqscteskyio2c/1xbRH03hpSbeTe0/oJQp0srJvMoMWv86P\nl/Hfc2ut7bdcGc5R4lrfSpb8bCcKuHq7h6ld1/rrqbmv3b4ennn1du7hPRrHi8wmybIepTYAwP+X\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhdVtr1vm\n/oeZjE/CM9k2rk6iSWo1zTVkndvcTM3tnI83Lu0+fZLadfgoPrffy13nHySbxi4miujOJxoRW2tt\nM9FeN+vmmvIO5vG502TrWra7rteNX+thom2wtdZGqU+Z29Xv5CoHR4lrvZzNU7umi/h5bCTvjwvn\ncp+xzQ7CI0cvcmd/cD7+m+7Mc8+cndTU7/JGDwCFCXoAKEzQA0Bhgh4AChP0AFCYoAeAwgQ9ABQm\n6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUFjZ9rrFMtfitUrMdZINasP+MDyzGucakFruONrVzfhn\n/Ok7f5/a9fzB0/DMvJO7hZ8mO9QO5vE2v9Ei2U6W+IhryXtxNYxf526iTa611jqJVr7WWuv3441h\ni1WynWwR/53N57m2tlXyMw4zx59sr1sm7qtuP/fQWbbcM27vaC8801vlzmOtuxWe6Sz/cHHrjR4A\nChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFFa21KY7iBdg\ntNbaINHD0EkWxnR6ieNf5IozFsdHqbkbW6PwzOVB7jMOTsfhmfPLXEHKaSf3P243MTfv50pLjpfx\nuXHyXmyJEpfePLeskywU6iYKhVarZLlVJ372uW/V2qDTy80lnh8byfv+XGJss5N8DuTGWmvxwcn4\nOLUp8zgddePP0rPijR4AChP0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJig\nB4DCBD0AFCboAaCwuu11/dxX660S//uscu1kLdVel2vl63dz3VrnOvHGsD9762Zq1/5JfNfPPnmW\n2vVsMk/NnS7jbWiTZK/ZMnF/LJP/uy8S36ubrG3sJGveut1sNV9cL9Hy1k9+vI1u7lk16safBVv9\n3OFvdePPuMvJdBklb5BBi/+mh8l7arWI7zpNtHOeFW/0AFCYoAeAwgQ9ABQm6AGgMEEPAIUJegAo\nTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaCwsqU2bbieHIyXFXRWyTaLRPHOfD5LrVomL3WmvOHG\nKLWq/eWXb4Vnrg1yhUIfPD5IzT0+jp//i3mupON02QvPTJK34rwTv86rRPFLa611e/Hv1VprvcRc\nsj+nDRIlP/1kt9VmptyqtbaWOP+1Tu5Dnu8twjMXkwU6m73cfbU+iJ9jP3crttks/hw46cTP8Kx4\noweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6ACis\ns8o2rwEA/+R5oweAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAw\nQQ8AhQl6AChM0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bhgh4AChP0AFCY\noAeAwgQ9ABQm6AGgMEEPAIUJegAoTNADQGGCHgAKE/QAUJigB4DCBD0AFCboAaAwQQ8AhQl6AChM\n0ANAYYIeAAoT9ABQmKAHgMIEPQAUJugBoDBBDwCFCXoAKEzQA0Bh/xfkBwlHN40TWAAAAABJRU5E\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11db29dd8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ## image data shape = [t, i,j,k], t= num_img_per_batch (basically the list of images), i,j,k=height,width, and depth/channel\n",
    "    return x/255\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "# import helper ## I did this because sklearn.preprocessing was defined in there\n",
    "from sklearn import preprocessing  ## from sklearn lib import preprocessing lib/sublib/functionality/class\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    ## This was in the helper.py which belongs to the generic helper functions\n",
    "    #     def display_image_predictions(features, labels, predictions):\n",
    "    #     n_classes = 10\n",
    "    #     label_names = _load_label_names()\n",
    "    #     label_binarizer = LabelBinarizer()\n",
    "    #     label_binarizer.fit(range(n_classes))\n",
    "    #     label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "    label_binarizer = preprocessing.LabelBinarizer() ## instantiate and initialized the one-hot encoder from class to one-hot\n",
    "    n_class = 10 ## total num_classes\n",
    "    label_binarizer.fit(range(n_class)) ## fit the one-vec to the range of number of classes, 10 in this case (dataset)\n",
    "    return label_binarizer.transform(x) ## transform the class labels to one-hot vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ## name the placeholder?? why should I do that? I can return it directly like below\n",
    "    ## It is also worth mentioning that the overal image_shape = [i,j,k] meaning row, col, channels/depth or \n",
    "    ## i, j, k\n",
    "    ## h, w, depth (deep-wide learning)\n",
    "    ## r, c, channels\n",
    "    ## y, x, z\n",
    "    ## Data_structure AKA data_shape are usually defined dshape = [i, j, k] as a tensor/Mat/Vec or even a scalar\n",
    "    ## This is kind of tricky: image_shape is probablly pointing at the img_hight, img_width, and image_depth as well\n",
    "    ## x_tensor is probably the input image or images or input batch\n",
    "    return tf.placeholder(tf.float32, \n",
    "                          [None, image_shape[0], image_shape[1], image_shape[2]], \n",
    "                          name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ## tf.placehoolder(data_type, data_structure (data_shape))\n",
    "    return tf.placeholder(tf.int32, [None, n_classes], name='y') ## the t/batch_size/num_img_per_batch = None & n/num_dimension = n_classes\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ## Basically the main command should be like tf.placeholder(tf.float32, [None, 1]) since it is a scalar but I can also use a variable for it as well.\n",
    "    return tf.placeholder(dtype=tf.float32, name='keep_prob') ## this is basically a scalar but it is data_type/dtype is not INT but float since it is a probability value 0-1 (is it really??).\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer ## Aras: This might be probablly 3-D Tuple???\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_tensor_shape = x_tensor.get_shape() ## x_tensor is 4D tensor [t/0, i/1, j/2, k/3] => x_tensor 4D tensor\n",
    "    k = x_tensor.get_shape()[3:4].num_elements() ## there shoudl be a simpler way!! num_elements/length\n",
    "    w_shape = [conv_ksize[0], conv_ksize[1], k, conv_num_outputs] # conv_num_output 1D tuple or a scalar\n",
    "    w = tf.Variable(tf.truncated_normal(w_shape, stddev=0.05))\n",
    "    \n",
    "    ## conv_kernel_strides\n",
    "    stride = [1, conv_strides[0], conv_strides[1], 1] # t == batch/num_data-img-images, i=hight/row/y, j=width/x/col, k=depth is usually 1\n",
    "    y_tensor = tf.nn.conv2d(x_tensor, w, stride, padding='VALID') ## apply convolution\n",
    "\n",
    "    ## Let's create biases\n",
    "    #     b = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    b = tf.zeros(conv_num_outputs)\n",
    "    #conv_layer = tf.nn.bias_add(conv_layer, bias) ## add biases\n",
    "    y_tensor += b\n",
    "    \n",
    "    ## adding relu function/activate function and the output is the h_tensor hidden layer output\n",
    "    h_tensor = tf.nn.relu(y_tensor) ## apply non-linearity, i.e. ReLU function\n",
    "    \n",
    "    ## pooling: can be max pooling or can be average pooling. Do not know any other kind.\n",
    "    ## pool_ksize is a 2D tuple = [i, j]\n",
    "    kernel = [1, pool_ksize[0], pool_ksize[1], 1] # the same as stride, it is 4D tuple [batch=1, ksize, kchannels=1]\n",
    "    ## pool_strides is a 2D tuple [i, j]\n",
    "    stride = [1, pool_strides[0], pool_ksize[1], 1] # [batch=1 (usually one input tensor per batch), ksize (2d tuple), k=1 (depth/num of channels)]\n",
    "    return tf.nn.max_pool(h_tensor, kernel, stride, padding='VALID')\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # x_tensor shape = [batch, img_w, img_height, img_depth]\n",
    "    # img_flat = img_w * img_h * img_d\n",
    "    n_size = x_tensor.get_shape()[1:4].num_elements() #1,2,3\n",
    "    ## The final output should be x_tensor_flat = [t_size, n_size] in which t_size is totally unchanged\n",
    "    #t_size = x_tensor.get_shape()[0:1].num_elements() # only [0] which ic sthe first dimension/num_elements in 1st dimension\n",
    "    # now we should do reshape \n",
    "    return tf.reshape(x_tensor, [-1, n_size])\n",
    "\n",
    "# def conv_net(x, weights, biases, dropout):\n",
    "#     # Layer 1 - 28*28*1 to 14*14*32\n",
    "#     conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "#     conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "#     # Layer 2 - 14*14*32 to 7*7*64\n",
    "#     conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "#     conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "#     # Fully connected layer - 7*7*64 to 1024\n",
    "#     fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "#     fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "#     fc1 = tf.nn.relu(fc1)\n",
    "#     fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "#     # Output Layer - class prediction - 1024 to 10\n",
    "#     out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "#     return out\n",
    "\n",
    "\n",
    "########## This is how Siraj implemented this layer.\n",
    "# def flatten_layer(layer):\n",
    "#     # Get the shape of the input layer.\n",
    "#     layer_shape = layer.get_shape()\n",
    "\n",
    "#     # The shape of the input layer is assumed to be:\n",
    "#     # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "#     # The number of features is: img_height * img_width * num_channels\n",
    "#     # We can use a function from TensorFlow to calculate this.\n",
    "#     num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "#     # Reshape the layer to [num_images, num_features].\n",
    "#     # Note that we just set the size of the second dimension\n",
    "#     # to num_features and the size of the first dimension to -1\n",
    "#     # which means the size in that dimension is calculated\n",
    "#     # so the total size of the tensor is unchanged from the reshaping.\n",
    "#     layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "#     # The shape of the flattened layer is now:\n",
    "#     # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "#     # Return both the flattened layer and the number of features.\n",
    "#     return layer_flat, num_features\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # This is basically a typical MLP input units and hidden units/ neurons with input data dimensions/variables\n",
    "    # w*x+b\n",
    "    ## X_tensor is two dimensional tensor [t, n]. That is why w shape is [n, num_outputs]\n",
    "    ## Since this is a FC (fully connected layer), n is 1 since the size of input units are equal to 1.\n",
    "    ### In this layer, wx+b and then we apply ReLU/Sigmoid, and ..\n",
    "    ## Let's define out w with variables\n",
    "    n_size = x_tensor.get_shape()[1:2].num_elements() # [t, n]\n",
    "    w = tf.Variable(tf.truncated_normal(shape=[n_size, num_outputs], stddev=0.05)) # normal dist function has mean and stddev\n",
    "    b = tf.zeros(num_outputs)\n",
    "    y_tensor = tf.matmul(x_tensor, w) + b\n",
    "    ### Apply ReLU activatinon/non-linearity\n",
    "    return tf.nn.relu(y_tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    ## The only differemce between this layer and the previous fully connected layer is the softmax for classification.\n",
    "    ## Instead of ReLU, softmax should be used.\n",
    "    n_size = x_tensor.get_shape()[1:2].num_elements() ## to get the num_features\n",
    "    ## Let's assign w and b for wx+b\n",
    "    w = tf.Variable(tf.truncated_normal(shape=[n_size, num_outputs], stddev=0.05))\n",
    "    b = tf.zeros(num_outputs)\n",
    "    return tf.matmul(x_tensor, w) + b\n",
    "    #return tf.nn.softmax(y_tensor) ## this should NOT be applied because in error with cross entropy softmax will be applied.\n",
    "    ## That is why only logits wx+b is needed for this layer.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "\n",
    "    # The 1st convolutional layer\n",
    "    h_tensor = conv2d_maxpool(x_tensor=x, conv_num_outputs=20, conv_ksize=[3, 3], conv_strides=[1, 1], \n",
    "                              pool_ksize=[2, 2], pool_strides=[1, 1])\n",
    "    \n",
    "    ## The 2nd convolutional layer is added to increase the validation accuracy above 50%\n",
    "    h_tensor = conv2d_maxpool(x_tensor=h_tensor, conv_num_outputs=20, conv_ksize=[3, 3], conv_strides=[1, 1], \n",
    "                              pool_ksize=[2, 2], pool_strides=[1, 1])\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    x_tensor_flattened = flatten(x_tensor=h_tensor)\n",
    "    \n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    h_tensor = fully_conn(x_tensor=x_tensor_flattened, num_outputs=20)\n",
    "\n",
    "    ## Where should Dropout be applied?\n",
    "    ## tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)\n",
    "    h_tensor = tf.nn.dropout(x=h_tensor, keep_prob=keep_prob, noise_shape=None, seed=None, name=None)\n",
    "  \n",
    "    # TODO: Apply an Output Layer\n",
    "    # TODO: return output\n",
    "    num_classes = 10  ## This is the toal number of classes for the clssification task\n",
    "    return output(x_tensor=h_tensor, num_outputs=num_classes)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    # Feed the dictionary (x, y, dropout_prob) from Numpy (Python) into TensorFlow (Tensors)\n",
    "    feed_dict_train = {x:feature_batch, y:label_batch, keep_prob:keep_probability}\n",
    "\n",
    "    # Run the optimizer on the fed training dict (TF training data).\n",
    "    session.run(optimizer, feed_dict=feed_dict_train)\n",
    "    \n",
    "    pass\n",
    "        \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    ## Placeholders for input/feature and output/labels -> I am NOT sure if redefining the TFPlaceholders/tf.placeholders are neccessary again for this part.\n",
    "    ## Feed_dict_train has to be redefined again I guess.\n",
    "    feed_dict_train = {x:feature_batch, y:label_batch, keep_prob:1.0}\n",
    "    ## Print out loss using TF cost function in a session\n",
    "    cost_train = session.run(cost, feed_dict=feed_dict_train)\n",
    "    accuracy_train = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "    print(\"trainging cost: \", cost_train, \"accuracy: \", accuracy_train)\n",
    "\n",
    "    ## Using the placeholders declared globally before and at the top\n",
    "    feed_dict_valid = {x:valid_features, y:valid_labels, keep_prob:1.0}\n",
    "    ## Print out validation accuracy using TF accuracy function with valid_features and valid_labels\n",
    "    cost_valid = session.run(cost, feed_dict=feed_dict_valid)\n",
    "    accuracy_valid = session.run(accuracy, feed_dict=feed_dict_valid)\n",
    "    print(\"validation cost: \", cost_valid, \"accuracy: \", accuracy_valid)\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100 #None\n",
    "batch_size = 64 # recommanded as min #32 #L1d Cache from lsppu command #None\n",
    "keep_probability = 0.5 #None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  trainging cost:  2.20174 accuracy:  0.15\n",
      "validation cost:  2.13537 accuracy:  0.1758\n",
      "Epoch  2, CIFAR-10 Batch 1:  trainging cost:  2.17666 accuracy:  0.275\n",
      "validation cost:  2.02108 accuracy:  0.243\n",
      "Epoch  3, CIFAR-10 Batch 1:  trainging cost:  2.09295 accuracy:  0.3\n",
      "validation cost:  1.88334 accuracy:  0.3128\n",
      "Epoch  4, CIFAR-10 Batch 1:  trainging cost:  2.08236 accuracy:  0.275\n",
      "validation cost:  1.80959 accuracy:  0.3402\n",
      "Epoch  5, CIFAR-10 Batch 1:  trainging cost:  2.05333 accuracy:  0.45\n",
      "validation cost:  1.75386 accuracy:  0.3608\n",
      "Epoch  6, CIFAR-10 Batch 1:  trainging cost:  2.02974 accuracy:  0.25\n",
      "validation cost:  1.70122 accuracy:  0.3838\n",
      "Epoch  7, CIFAR-10 Batch 1:  trainging cost:  2.00613 accuracy:  0.325\n",
      "validation cost:  1.71892 accuracy:  0.3846\n",
      "Epoch  8, CIFAR-10 Batch 1:  trainging cost:  1.81111 accuracy:  0.425\n",
      "validation cost:  1.62851 accuracy:  0.4318\n",
      "Epoch  9, CIFAR-10 Batch 1:  trainging cost:  1.78442 accuracy:  0.35\n",
      "validation cost:  1.58589 accuracy:  0.4372\n",
      "Epoch 10, CIFAR-10 Batch 1:  trainging cost:  1.77105 accuracy:  0.425\n",
      "validation cost:  1.57224 accuracy:  0.4406\n",
      "Epoch 11, CIFAR-10 Batch 1:  trainging cost:  1.76993 accuracy:  0.45\n",
      "validation cost:  1.57003 accuracy:  0.4294\n",
      "Epoch 12, CIFAR-10 Batch 1:  trainging cost:  1.72282 accuracy:  0.425\n",
      "validation cost:  1.53703 accuracy:  0.446\n",
      "Epoch 13, CIFAR-10 Batch 1:  trainging cost:  1.65401 accuracy:  0.4\n",
      "validation cost:  1.49834 accuracy:  0.4684\n",
      "Epoch 14, CIFAR-10 Batch 1:  trainging cost:  1.61322 accuracy:  0.425\n",
      "validation cost:  1.50801 accuracy:  0.4684\n",
      "Epoch 15, CIFAR-10 Batch 1:  trainging cost:  1.57755 accuracy:  0.425\n",
      "validation cost:  1.50121 accuracy:  0.4618\n",
      "Epoch 16, CIFAR-10 Batch 1:  trainging cost:  1.55769 accuracy:  0.4\n",
      "validation cost:  1.4751 accuracy:  0.4802\n",
      "Epoch 17, CIFAR-10 Batch 1:  trainging cost:  1.60191 accuracy:  0.425\n",
      "validation cost:  1.48322 accuracy:  0.4726\n",
      "Epoch 18, CIFAR-10 Batch 1:  trainging cost:  1.58241 accuracy:  0.5\n",
      "validation cost:  1.49168 accuracy:  0.4638\n",
      "Epoch 19, CIFAR-10 Batch 1:  trainging cost:  1.50081 accuracy:  0.425\n",
      "validation cost:  1.46349 accuracy:  0.4722\n",
      "Epoch 20, CIFAR-10 Batch 1:  trainging cost:  1.49232 accuracy:  0.45\n",
      "validation cost:  1.45729 accuracy:  0.4806\n",
      "Epoch 21, CIFAR-10 Batch 1:  trainging cost:  1.53268 accuracy:  0.4\n",
      "validation cost:  1.46573 accuracy:  0.4796\n",
      "Epoch 22, CIFAR-10 Batch 1:  trainging cost:  1.50489 accuracy:  0.45\n",
      "validation cost:  1.47224 accuracy:  0.4754\n",
      "Epoch 23, CIFAR-10 Batch 1:  trainging cost:  1.44074 accuracy:  0.5\n",
      "validation cost:  1.45907 accuracy:  0.4802\n",
      "Epoch 24, CIFAR-10 Batch 1:  trainging cost:  1.44442 accuracy:  0.45\n",
      "validation cost:  1.4686 accuracy:  0.4746\n",
      "Epoch 25, CIFAR-10 Batch 1:  trainging cost:  1.3952 accuracy:  0.5\n",
      "validation cost:  1.42829 accuracy:  0.4892\n",
      "Epoch 26, CIFAR-10 Batch 1:  trainging cost:  1.40077 accuracy:  0.45\n",
      "validation cost:  1.43788 accuracy:  0.4902\n",
      "Epoch 27, CIFAR-10 Batch 1:  trainging cost:  1.3777 accuracy:  0.4\n",
      "validation cost:  1.3948 accuracy:  0.5032\n",
      "Epoch 28, CIFAR-10 Batch 1:  trainging cost:  1.2995 accuracy:  0.5\n",
      "validation cost:  1.3934 accuracy:  0.5056\n",
      "Epoch 29, CIFAR-10 Batch 1:  trainging cost:  1.33372 accuracy:  0.5\n",
      "validation cost:  1.43761 accuracy:  0.4914\n",
      "Epoch 30, CIFAR-10 Batch 1:  trainging cost:  1.27572 accuracy:  0.525\n",
      "validation cost:  1.3999 accuracy:  0.502\n",
      "Epoch 31, CIFAR-10 Batch 1:  trainging cost:  1.28804 accuracy:  0.525\n",
      "validation cost:  1.42287 accuracy:  0.4952\n",
      "Epoch 32, CIFAR-10 Batch 1:  trainging cost:  1.26566 accuracy:  0.55\n",
      "validation cost:  1.41951 accuracy:  0.4958\n",
      "Epoch 33, CIFAR-10 Batch 1:  trainging cost:  1.21211 accuracy:  0.55\n",
      "validation cost:  1.40718 accuracy:  0.4968\n",
      "Epoch 34, CIFAR-10 Batch 1:  trainging cost:  1.22901 accuracy:  0.5\n",
      "validation cost:  1.37328 accuracy:  0.5104\n",
      "Epoch 35, CIFAR-10 Batch 1:  trainging cost:  1.17884 accuracy:  0.575\n",
      "validation cost:  1.41704 accuracy:  0.503\n",
      "Epoch 36, CIFAR-10 Batch 1:  trainging cost:  1.16773 accuracy:  0.575\n",
      "validation cost:  1.38475 accuracy:  0.5138\n",
      "Epoch 37, CIFAR-10 Batch 1:  trainging cost:  1.14558 accuracy:  0.55\n",
      "validation cost:  1.37611 accuracy:  0.5132\n",
      "Epoch 38, CIFAR-10 Batch 1:  trainging cost:  1.18348 accuracy:  0.6\n",
      "validation cost:  1.43404 accuracy:  0.5012\n",
      "Epoch 39, CIFAR-10 Batch 1:  trainging cost:  1.12787 accuracy:  0.625\n",
      "validation cost:  1.37944 accuracy:  0.5172\n",
      "Epoch 40, CIFAR-10 Batch 1:  trainging cost:  1.09127 accuracy:  0.625\n",
      "validation cost:  1.38837 accuracy:  0.5224\n",
      "Epoch 41, CIFAR-10 Batch 1:  trainging cost:  1.07844 accuracy:  0.675\n",
      "validation cost:  1.44533 accuracy:  0.4876\n",
      "Epoch 42, CIFAR-10 Batch 1:  trainging cost:  1.0501 accuracy:  0.575\n",
      "validation cost:  1.38627 accuracy:  0.5218\n",
      "Epoch 43, CIFAR-10 Batch 1:  trainging cost:  1.05601 accuracy:  0.6\n",
      "validation cost:  1.39706 accuracy:  0.5114\n",
      "Epoch 44, CIFAR-10 Batch 1:  trainging cost:  0.999393 accuracy:  0.675\n",
      "validation cost:  1.38801 accuracy:  0.5234\n",
      "Epoch 45, CIFAR-10 Batch 1:  trainging cost:  0.950627 accuracy:  0.675\n",
      "validation cost:  1.38025 accuracy:  0.5208\n",
      "Epoch 46, CIFAR-10 Batch 1:  trainging cost:  0.981807 accuracy:  0.675\n",
      "validation cost:  1.42468 accuracy:  0.5114\n",
      "Epoch 47, CIFAR-10 Batch 1:  trainging cost:  0.942426 accuracy:  0.725\n",
      "validation cost:  1.3931 accuracy:  0.5204\n",
      "Epoch 48, CIFAR-10 Batch 1:  trainging cost:  0.971446 accuracy:  0.7\n",
      "validation cost:  1.40836 accuracy:  0.5182\n",
      "Epoch 49, CIFAR-10 Batch 1:  trainging cost:  0.944241 accuracy:  0.675\n",
      "validation cost:  1.40713 accuracy:  0.5216\n",
      "Epoch 50, CIFAR-10 Batch 1:  trainging cost:  0.912532 accuracy:  0.65\n",
      "validation cost:  1.41401 accuracy:  0.5182\n",
      "Epoch 51, CIFAR-10 Batch 1:  trainging cost:  0.879146 accuracy:  0.8\n",
      "validation cost:  1.42428 accuracy:  0.514\n",
      "Epoch 52, CIFAR-10 Batch 1:  trainging cost:  0.817731 accuracy:  0.75\n",
      "validation cost:  1.41843 accuracy:  0.5152\n",
      "Epoch 53, CIFAR-10 Batch 1:  trainging cost:  0.902369 accuracy:  0.8\n",
      "validation cost:  1.42894 accuracy:  0.5182\n",
      "Epoch 54, CIFAR-10 Batch 1:  trainging cost:  0.865044 accuracy:  0.7\n",
      "validation cost:  1.42864 accuracy:  0.517\n",
      "Epoch 55, CIFAR-10 Batch 1:  trainging cost:  0.854032 accuracy:  0.675\n",
      "validation cost:  1.43666 accuracy:  0.5196\n",
      "Epoch 56, CIFAR-10 Batch 1:  trainging cost:  0.81716 accuracy:  0.775\n",
      "validation cost:  1.44778 accuracy:  0.5116\n",
      "Epoch 57, CIFAR-10 Batch 1:  trainging cost:  0.803679 accuracy:  0.75\n",
      "validation cost:  1.4326 accuracy:  0.521\n",
      "Epoch 58, CIFAR-10 Batch 1:  trainging cost:  0.827864 accuracy:  0.775\n",
      "validation cost:  1.45062 accuracy:  0.5244\n",
      "Epoch 59, CIFAR-10 Batch 1:  trainging cost:  0.853289 accuracy:  0.775\n",
      "validation cost:  1.44281 accuracy:  0.5236\n",
      "Epoch 60, CIFAR-10 Batch 1:  trainging cost:  0.818087 accuracy:  0.725\n",
      "validation cost:  1.4316 accuracy:  0.5238\n",
      "Epoch 61, CIFAR-10 Batch 1:  trainging cost:  0.78061 accuracy:  0.8\n",
      "validation cost:  1.4658 accuracy:  0.5234\n",
      "Epoch 62, CIFAR-10 Batch 1:  trainging cost:  0.800008 accuracy:  0.725\n",
      "validation cost:  1.48945 accuracy:  0.5102\n",
      "Epoch 63, CIFAR-10 Batch 1:  trainging cost:  0.800268 accuracy:  0.75\n",
      "validation cost:  1.47029 accuracy:  0.5176\n",
      "Epoch 64, CIFAR-10 Batch 1:  trainging cost:  0.82055 accuracy:  0.725\n",
      "validation cost:  1.53647 accuracy:  0.512\n",
      "Epoch 65, CIFAR-10 Batch 1:  trainging cost:  0.797202 accuracy:  0.75\n",
      "validation cost:  1.54935 accuracy:  0.5094\n",
      "Epoch 66, CIFAR-10 Batch 1:  trainging cost:  0.758709 accuracy:  0.8\n",
      "validation cost:  1.50623 accuracy:  0.5158\n",
      "Epoch 67, CIFAR-10 Batch 1:  trainging cost:  0.806434 accuracy:  0.775\n",
      "validation cost:  1.50965 accuracy:  0.5148\n",
      "Epoch 68, CIFAR-10 Batch 1:  trainging cost:  0.758577 accuracy:  0.8\n",
      "validation cost:  1.51077 accuracy:  0.5162\n",
      "Epoch 69, CIFAR-10 Batch 1:  trainging cost:  0.755038 accuracy:  0.725\n",
      "validation cost:  1.55356 accuracy:  0.5118\n",
      "Epoch 70, CIFAR-10 Batch 1:  trainging cost:  0.729448 accuracy:  0.775\n",
      "validation cost:  1.54655 accuracy:  0.5086\n",
      "Epoch 71, CIFAR-10 Batch 1:  trainging cost:  0.748678 accuracy:  0.75\n",
      "validation cost:  1.61496 accuracy:  0.4976\n",
      "Epoch 72, CIFAR-10 Batch 1:  trainging cost:  0.763392 accuracy:  0.75\n",
      "validation cost:  1.59771 accuracy:  0.4992\n",
      "Epoch 73, CIFAR-10 Batch 1:  trainging cost:  0.729511 accuracy:  0.85\n",
      "validation cost:  1.58077 accuracy:  0.5102\n",
      "Epoch 74, CIFAR-10 Batch 1:  trainging cost:  0.693553 accuracy:  0.8\n",
      "validation cost:  1.58153 accuracy:  0.5066\n",
      "Epoch 75, CIFAR-10 Batch 1:  trainging cost:  0.699177 accuracy:  0.85\n",
      "validation cost:  1.58631 accuracy:  0.504\n",
      "Epoch 76, CIFAR-10 Batch 1:  trainging cost:  0.697579 accuracy:  0.825\n",
      "validation cost:  1.57627 accuracy:  0.5044\n",
      "Epoch 77, CIFAR-10 Batch 1:  trainging cost:  0.704571 accuracy:  0.825\n",
      "validation cost:  1.54793 accuracy:  0.5186\n",
      "Epoch 78, CIFAR-10 Batch 1:  trainging cost:  0.721335 accuracy:  0.75\n",
      "validation cost:  1.54057 accuracy:  0.5168\n",
      "Epoch 79, CIFAR-10 Batch 1:  trainging cost:  0.733687 accuracy:  0.725\n",
      "validation cost:  1.56555 accuracy:  0.5116\n",
      "Epoch 80, CIFAR-10 Batch 1:  trainging cost:  0.660894 accuracy:  0.85\n",
      "validation cost:  1.60025 accuracy:  0.5102\n",
      "Epoch 81, CIFAR-10 Batch 1:  trainging cost:  0.678982 accuracy:  0.775\n",
      "validation cost:  1.6336 accuracy:  0.511\n",
      "Epoch 82, CIFAR-10 Batch 1:  trainging cost:  0.673537 accuracy:  0.775\n",
      "validation cost:  1.68833 accuracy:  0.4974\n",
      "Epoch 83, CIFAR-10 Batch 1:  trainging cost:  0.615333 accuracy:  0.875\n",
      "validation cost:  1.6029 accuracy:  0.5104\n",
      "Epoch 84, CIFAR-10 Batch 1:  trainging cost:  0.687795 accuracy:  0.75\n",
      "validation cost:  1.67305 accuracy:  0.5028\n",
      "Epoch 85, CIFAR-10 Batch 1:  trainging cost:  0.643063 accuracy:  0.85\n",
      "validation cost:  1.61753 accuracy:  0.5074\n",
      "Epoch 86, CIFAR-10 Batch 1:  trainging cost:  0.647623 accuracy:  0.85\n",
      "validation cost:  1.61956 accuracy:  0.5072\n",
      "Epoch 87, CIFAR-10 Batch 1:  trainging cost:  0.684475 accuracy:  0.825\n",
      "validation cost:  1.63534 accuracy:  0.5164\n",
      "Epoch 88, CIFAR-10 Batch 1:  trainging cost:  0.625418 accuracy:  0.825\n",
      "validation cost:  1.67263 accuracy:  0.5158\n",
      "Epoch 89, CIFAR-10 Batch 1:  trainging cost:  0.595158 accuracy:  0.85\n",
      "validation cost:  1.67431 accuracy:  0.5076\n",
      "Epoch 90, CIFAR-10 Batch 1:  trainging cost:  0.67471 accuracy:  0.75\n",
      "validation cost:  1.74082 accuracy:  0.5036\n",
      "Epoch 91, CIFAR-10 Batch 1:  trainging cost:  0.638926 accuracy:  0.775\n",
      "validation cost:  1.73234 accuracy:  0.5128\n",
      "Epoch 92, CIFAR-10 Batch 1:  trainging cost:  0.60095 accuracy:  0.8\n",
      "validation cost:  1.6953 accuracy:  0.5138\n",
      "Epoch 93, CIFAR-10 Batch 1:  trainging cost:  0.587371 accuracy:  0.875\n",
      "validation cost:  1.69647 accuracy:  0.5066\n",
      "Epoch 94, CIFAR-10 Batch 1:  trainging cost:  0.639469 accuracy:  0.825\n",
      "validation cost:  1.80899 accuracy:  0.51\n",
      "Epoch 95, CIFAR-10 Batch 1:  trainging cost:  0.571246 accuracy:  0.825\n",
      "validation cost:  1.75647 accuracy:  0.5166\n",
      "Epoch 96, CIFAR-10 Batch 1:  trainging cost:  0.620057 accuracy:  0.85\n",
      "validation cost:  1.72298 accuracy:  0.5114\n",
      "Epoch 97, CIFAR-10 Batch 1:  trainging cost:  0.582675 accuracy:  0.85\n",
      "validation cost:  1.71275 accuracy:  0.5058\n",
      "Epoch 98, CIFAR-10 Batch 1:  trainging cost:  0.559631 accuracy:  0.85\n",
      "validation cost:  1.7708 accuracy:  0.5066\n",
      "Epoch 99, CIFAR-10 Batch 1:  trainging cost:  0.592813 accuracy:  0.8\n",
      "validation cost:  1.76457 accuracy:  0.5058\n",
      "Epoch 100, CIFAR-10 Batch 1:  trainging cost:  0.565094 accuracy:  0.85\n",
      "validation cost:  1.77141 accuracy:  0.512\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  trainging cost:  2.20062 accuracy:  0.15\n",
      "validation cost:  2.09171 accuracy:  0.262\n",
      "Epoch  1, CIFAR-10 Batch 2:  trainging cost:  1.91578 accuracy:  0.325\n",
      "validation cost:  1.83999 accuracy:  0.345\n",
      "Epoch  1, CIFAR-10 Batch 3:  trainging cost:  1.76105 accuracy:  0.375\n",
      "validation cost:  1.78518 accuracy:  0.331\n",
      "Epoch  1, CIFAR-10 Batch 4:  trainging cost:  1.78036 accuracy:  0.4\n",
      "validation cost:  1.728 accuracy:  0.3846\n",
      "Epoch  1, CIFAR-10 Batch 5:  trainging cost:  1.75093 accuracy:  0.35\n",
      "validation cost:  1.76081 accuracy:  0.3732\n",
      "Epoch  2, CIFAR-10 Batch 1:  trainging cost:  1.95584 accuracy:  0.3\n",
      "validation cost:  1.67616 accuracy:  0.3938\n",
      "Epoch  2, CIFAR-10 Batch 2:  trainging cost:  1.81444 accuracy:  0.3\n",
      "validation cost:  1.67495 accuracy:  0.3996\n",
      "Epoch  2, CIFAR-10 Batch 3:  trainging cost:  1.55641 accuracy:  0.45\n",
      "validation cost:  1.63523 accuracy:  0.4032\n",
      "Epoch  2, CIFAR-10 Batch 4:  trainging cost:  1.7886 accuracy:  0.3\n",
      "validation cost:  1.66407 accuracy:  0.3984\n",
      "Epoch  2, CIFAR-10 Batch 5:  trainging cost:  1.65143 accuracy:  0.525\n",
      "validation cost:  1.63268 accuracy:  0.4132\n",
      "Epoch  3, CIFAR-10 Batch 1:  trainging cost:  1.88886 accuracy:  0.35\n",
      "validation cost:  1.59901 accuracy:  0.4306\n",
      "Epoch  3, CIFAR-10 Batch 2:  trainging cost:  1.63162 accuracy:  0.525\n",
      "validation cost:  1.63169 accuracy:  0.4232\n",
      "Epoch  3, CIFAR-10 Batch 3:  trainging cost:  1.45511 accuracy:  0.475\n",
      "validation cost:  1.56003 accuracy:  0.4354\n",
      "Epoch  3, CIFAR-10 Batch 4:  trainging cost:  1.64286 accuracy:  0.35\n",
      "validation cost:  1.55514 accuracy:  0.4428\n",
      "Epoch  3, CIFAR-10 Batch 5:  trainging cost:  1.51757 accuracy:  0.525\n",
      "validation cost:  1.54419 accuracy:  0.4568\n",
      "Epoch  4, CIFAR-10 Batch 1:  trainging cost:  1.75524 accuracy:  0.45\n",
      "validation cost:  1.52279 accuracy:  0.4586\n",
      "Epoch  4, CIFAR-10 Batch 2:  trainging cost:  1.55745 accuracy:  0.475\n",
      "validation cost:  1.51542 accuracy:  0.4694\n",
      "Epoch  4, CIFAR-10 Batch 3:  trainging cost:  1.37575 accuracy:  0.525\n",
      "validation cost:  1.49842 accuracy:  0.48\n",
      "Epoch  4, CIFAR-10 Batch 4:  trainging cost:  1.51341 accuracy:  0.45\n",
      "validation cost:  1.47415 accuracy:  0.4876\n",
      "Epoch  4, CIFAR-10 Batch 5:  trainging cost:  1.49015 accuracy:  0.5\n",
      "validation cost:  1.5045 accuracy:  0.4834\n",
      "Epoch  5, CIFAR-10 Batch 1:  trainging cost:  1.70276 accuracy:  0.425\n",
      "validation cost:  1.43932 accuracy:  0.494\n",
      "Epoch  5, CIFAR-10 Batch 2:  trainging cost:  1.45754 accuracy:  0.55\n",
      "validation cost:  1.5022 accuracy:  0.4738\n",
      "Epoch  5, CIFAR-10 Batch 3:  trainging cost:  1.33473 accuracy:  0.625\n",
      "validation cost:  1.4541 accuracy:  0.4908\n",
      "Epoch  5, CIFAR-10 Batch 4:  trainging cost:  1.47915 accuracy:  0.45\n",
      "validation cost:  1.44931 accuracy:  0.4938\n",
      "Epoch  5, CIFAR-10 Batch 5:  trainging cost:  1.41989 accuracy:  0.6\n",
      "validation cost:  1.45999 accuracy:  0.5004\n",
      "Epoch  6, CIFAR-10 Batch 1:  trainging cost:  1.71819 accuracy:  0.45\n",
      "validation cost:  1.42541 accuracy:  0.5074\n",
      "Epoch  6, CIFAR-10 Batch 2:  trainging cost:  1.41527 accuracy:  0.525\n",
      "validation cost:  1.42743 accuracy:  0.5056\n",
      "Epoch  6, CIFAR-10 Batch 3:  trainging cost:  1.26378 accuracy:  0.675\n",
      "validation cost:  1.40836 accuracy:  0.5152\n",
      "Epoch  6, CIFAR-10 Batch 4:  trainging cost:  1.40183 accuracy:  0.5\n",
      "validation cost:  1.44365 accuracy:  0.4902\n",
      "Epoch  6, CIFAR-10 Batch 5:  trainging cost:  1.32533 accuracy:  0.625\n",
      "validation cost:  1.39809 accuracy:  0.5242\n",
      "Epoch  7, CIFAR-10 Batch 1:  trainging cost:  1.66094 accuracy:  0.45\n",
      "validation cost:  1.35693 accuracy:  0.5322\n",
      "Epoch  7, CIFAR-10 Batch 2:  trainging cost:  1.35376 accuracy:  0.5\n",
      "validation cost:  1.43627 accuracy:  0.4996\n",
      "Epoch  7, CIFAR-10 Batch 3:  trainging cost:  1.23874 accuracy:  0.625\n",
      "validation cost:  1.39682 accuracy:  0.523\n",
      "Epoch  7, CIFAR-10 Batch 4:  trainging cost:  1.40132 accuracy:  0.525\n",
      "validation cost:  1.38237 accuracy:  0.5192\n",
      "Epoch  7, CIFAR-10 Batch 5:  trainging cost:  1.29303 accuracy:  0.625\n",
      "validation cost:  1.39051 accuracy:  0.5216\n",
      "Epoch  8, CIFAR-10 Batch 1:  trainging cost:  1.49754 accuracy:  0.525\n",
      "validation cost:  1.37229 accuracy:  0.527\n",
      "Epoch  8, CIFAR-10 Batch 2:  trainging cost:  1.30017 accuracy:  0.6\n",
      "validation cost:  1.41542 accuracy:  0.5198\n",
      "Epoch  8, CIFAR-10 Batch 3:  trainging cost:  1.18343 accuracy:  0.55\n",
      "validation cost:  1.34971 accuracy:  0.5348\n",
      "Epoch  8, CIFAR-10 Batch 4:  trainging cost:  1.34021 accuracy:  0.475\n",
      "validation cost:  1.32746 accuracy:  0.5386\n",
      "Epoch  8, CIFAR-10 Batch 5:  trainging cost:  1.27602 accuracy:  0.55\n",
      "validation cost:  1.36319 accuracy:  0.5366\n",
      "Epoch  9, CIFAR-10 Batch 1:  trainging cost:  1.49057 accuracy:  0.575\n",
      "validation cost:  1.35837 accuracy:  0.5186\n",
      "Epoch  9, CIFAR-10 Batch 2:  trainging cost:  1.31108 accuracy:  0.575\n",
      "validation cost:  1.37304 accuracy:  0.5194\n",
      "Epoch  9, CIFAR-10 Batch 3:  trainging cost:  1.20756 accuracy:  0.625\n",
      "validation cost:  1.32707 accuracy:  0.5422\n",
      "Epoch  9, CIFAR-10 Batch 4:  trainging cost:  1.32737 accuracy:  0.475\n",
      "validation cost:  1.32425 accuracy:  0.541\n",
      "Epoch  9, CIFAR-10 Batch 5:  trainging cost:  1.23931 accuracy:  0.625\n",
      "validation cost:  1.33871 accuracy:  0.5372\n",
      "Epoch 10, CIFAR-10 Batch 1:  trainging cost:  1.4937 accuracy:  0.475\n",
      "validation cost:  1.30398 accuracy:  0.5438\n",
      "Epoch 10, CIFAR-10 Batch 2:  trainging cost:  1.22068 accuracy:  0.625\n",
      "validation cost:  1.34145 accuracy:  0.5466\n",
      "Epoch 10, CIFAR-10 Batch 3:  trainging cost:  1.11184 accuracy:  0.625\n",
      "validation cost:  1.30235 accuracy:  0.5462\n",
      "Epoch 10, CIFAR-10 Batch 4:  trainging cost:  1.3024 accuracy:  0.525\n",
      "validation cost:  1.31085 accuracy:  0.538\n",
      "Epoch 10, CIFAR-10 Batch 5:  trainging cost:  1.18534 accuracy:  0.675\n",
      "validation cost:  1.31223 accuracy:  0.5368\n",
      "Epoch 11, CIFAR-10 Batch 1:  trainging cost:  1.39748 accuracy:  0.475\n",
      "validation cost:  1.28509 accuracy:  0.5532\n",
      "Epoch 11, CIFAR-10 Batch 2:  trainging cost:  1.18325 accuracy:  0.6\n",
      "validation cost:  1.31315 accuracy:  0.5504\n",
      "Epoch 11, CIFAR-10 Batch 3:  trainging cost:  1.1386 accuracy:  0.6\n",
      "validation cost:  1.28657 accuracy:  0.5406\n",
      "Epoch 11, CIFAR-10 Batch 4:  trainging cost:  1.30694 accuracy:  0.6\n",
      "validation cost:  1.28797 accuracy:  0.5404\n",
      "Epoch 11, CIFAR-10 Batch 5:  trainging cost:  1.08362 accuracy:  0.7\n",
      "validation cost:  1.26978 accuracy:  0.5564\n",
      "Epoch 12, CIFAR-10 Batch 1:  trainging cost:  1.44198 accuracy:  0.55\n",
      "validation cost:  1.25469 accuracy:  0.5584\n",
      "Epoch 12, CIFAR-10 Batch 2:  trainging cost:  1.17908 accuracy:  0.6\n",
      "validation cost:  1.29362 accuracy:  0.5484\n",
      "Epoch 12, CIFAR-10 Batch 3:  trainging cost:  1.10155 accuracy:  0.625\n",
      "validation cost:  1.26712 accuracy:  0.552\n",
      "Epoch 12, CIFAR-10 Batch 4:  trainging cost:  1.26283 accuracy:  0.55\n",
      "validation cost:  1.27067 accuracy:  0.5516\n",
      "Epoch 12, CIFAR-10 Batch 5:  trainging cost:  1.08314 accuracy:  0.675\n",
      "validation cost:  1.25006 accuracy:  0.5614\n",
      "Epoch 13, CIFAR-10 Batch 1:  trainging cost:  1.33772 accuracy:  0.55\n",
      "validation cost:  1.26061 accuracy:  0.56\n",
      "Epoch 13, CIFAR-10 Batch 2:  trainging cost:  1.18409 accuracy:  0.675\n",
      "validation cost:  1.29728 accuracy:  0.5522\n",
      "Epoch 13, CIFAR-10 Batch 3:  trainging cost:  1.07419 accuracy:  0.675\n",
      "validation cost:  1.25533 accuracy:  0.5534\n",
      "Epoch 13, CIFAR-10 Batch 4:  trainging cost:  1.26266 accuracy:  0.525\n",
      "validation cost:  1.25204 accuracy:  0.5568\n",
      "Epoch 13, CIFAR-10 Batch 5:  trainging cost:  1.11974 accuracy:  0.7\n",
      "validation cost:  1.26781 accuracy:  0.556\n",
      "Epoch 14, CIFAR-10 Batch 1:  trainging cost:  1.36578 accuracy:  0.6\n",
      "validation cost:  1.2413 accuracy:  0.561\n",
      "Epoch 14, CIFAR-10 Batch 2:  trainging cost:  1.13818 accuracy:  0.625\n",
      "validation cost:  1.28031 accuracy:  0.5662\n",
      "Epoch 14, CIFAR-10 Batch 3:  trainging cost:  1.03506 accuracy:  0.625\n",
      "validation cost:  1.24237 accuracy:  0.557\n",
      "Epoch 14, CIFAR-10 Batch 4:  trainging cost:  1.28709 accuracy:  0.475\n",
      "validation cost:  1.24686 accuracy:  0.5572\n",
      "Epoch 14, CIFAR-10 Batch 5:  trainging cost:  1.04416 accuracy:  0.7\n",
      "validation cost:  1.22797 accuracy:  0.5628\n",
      "Epoch 15, CIFAR-10 Batch 1:  trainging cost:  1.30702 accuracy:  0.525\n",
      "validation cost:  1.24514 accuracy:  0.5608\n",
      "Epoch 15, CIFAR-10 Batch 2:  trainging cost:  1.15587 accuracy:  0.6\n",
      "validation cost:  1.27953 accuracy:  0.5616\n",
      "Epoch 15, CIFAR-10 Batch 3:  trainging cost:  1.06196 accuracy:  0.625\n",
      "validation cost:  1.23857 accuracy:  0.5576\n",
      "Epoch 15, CIFAR-10 Batch 4:  trainging cost:  1.26089 accuracy:  0.575\n",
      "validation cost:  1.24074 accuracy:  0.5624\n",
      "Epoch 15, CIFAR-10 Batch 5:  trainging cost:  1.06098 accuracy:  0.725\n",
      "validation cost:  1.24782 accuracy:  0.5586\n",
      "Epoch 16, CIFAR-10 Batch 1:  trainging cost:  1.27237 accuracy:  0.55\n",
      "validation cost:  1.22025 accuracy:  0.5648\n",
      "Epoch 16, CIFAR-10 Batch 2:  trainging cost:  1.15816 accuracy:  0.675\n",
      "validation cost:  1.26943 accuracy:  0.5642\n",
      "Epoch 16, CIFAR-10 Batch 3:  trainging cost:  1.06476 accuracy:  0.6\n",
      "validation cost:  1.23368 accuracy:  0.5646\n",
      "Epoch 16, CIFAR-10 Batch 4:  trainging cost:  1.21987 accuracy:  0.525\n",
      "validation cost:  1.22442 accuracy:  0.5742\n",
      "Epoch 16, CIFAR-10 Batch 5:  trainging cost:  1.01805 accuracy:  0.725\n",
      "validation cost:  1.19247 accuracy:  0.5716\n",
      "Epoch 17, CIFAR-10 Batch 1:  trainging cost:  1.26265 accuracy:  0.525\n",
      "validation cost:  1.20933 accuracy:  0.5774\n",
      "Epoch 17, CIFAR-10 Batch 2:  trainging cost:  1.0582 accuracy:  0.65\n",
      "validation cost:  1.24548 accuracy:  0.5628\n",
      "Epoch 17, CIFAR-10 Batch 3:  trainging cost:  1.06932 accuracy:  0.7\n",
      "validation cost:  1.23531 accuracy:  0.5604\n",
      "Epoch 17, CIFAR-10 Batch 4:  trainging cost:  1.19635 accuracy:  0.575\n",
      "validation cost:  1.23426 accuracy:  0.5632\n",
      "Epoch 17, CIFAR-10 Batch 5:  trainging cost:  1.01097 accuracy:  0.7\n",
      "validation cost:  1.21511 accuracy:  0.565\n",
      "Epoch 18, CIFAR-10 Batch 1:  trainging cost:  1.27824 accuracy:  0.5\n",
      "validation cost:  1.20808 accuracy:  0.576\n",
      "Epoch 18, CIFAR-10 Batch 2:  trainging cost:  1.09504 accuracy:  0.65\n",
      "validation cost:  1.27371 accuracy:  0.5648\n",
      "Epoch 18, CIFAR-10 Batch 3:  trainging cost:  1.0041 accuracy:  0.65\n",
      "validation cost:  1.21038 accuracy:  0.5738\n",
      "Epoch 18, CIFAR-10 Batch 4:  trainging cost:  1.23513 accuracy:  0.55\n",
      "validation cost:  1.23499 accuracy:  0.558\n",
      "Epoch 18, CIFAR-10 Batch 5:  trainging cost:  1.08347 accuracy:  0.7\n",
      "validation cost:  1.21961 accuracy:  0.5638\n",
      "Epoch 19, CIFAR-10 Batch 1:  trainging cost:  1.25524 accuracy:  0.575\n",
      "validation cost:  1.19356 accuracy:  0.5784\n",
      "Epoch 19, CIFAR-10 Batch 2:  trainging cost:  1.07335 accuracy:  0.675\n",
      "validation cost:  1.25786 accuracy:  0.552\n",
      "Epoch 19, CIFAR-10 Batch 3:  trainging cost:  1.06283 accuracy:  0.65\n",
      "validation cost:  1.20737 accuracy:  0.5728\n",
      "Epoch 19, CIFAR-10 Batch 4:  trainging cost:  1.22066 accuracy:  0.6\n",
      "validation cost:  1.21539 accuracy:  0.5666\n",
      "Epoch 19, CIFAR-10 Batch 5:  trainging cost:  1.05042 accuracy:  0.65\n",
      "validation cost:  1.22369 accuracy:  0.5648\n",
      "Epoch 20, CIFAR-10 Batch 1:  trainging cost:  1.20688 accuracy:  0.55\n",
      "validation cost:  1.1748 accuracy:  0.585\n",
      "Epoch 20, CIFAR-10 Batch 2:  trainging cost:  1.05561 accuracy:  0.675\n",
      "validation cost:  1.22124 accuracy:  0.5656\n",
      "Epoch 20, CIFAR-10 Batch 3:  trainging cost:  0.998243 accuracy:  0.625\n",
      "validation cost:  1.18408 accuracy:  0.5792\n",
      "Epoch 20, CIFAR-10 Batch 4:  trainging cost:  1.23862 accuracy:  0.6\n",
      "validation cost:  1.20254 accuracy:  0.5728\n",
      "Epoch 20, CIFAR-10 Batch 5:  trainging cost:  1.0524 accuracy:  0.75\n",
      "validation cost:  1.20062 accuracy:  0.5752\n",
      "Epoch 21, CIFAR-10 Batch 1:  trainging cost:  1.19502 accuracy:  0.575\n",
      "validation cost:  1.19397 accuracy:  0.575\n",
      "Epoch 21, CIFAR-10 Batch 2:  trainging cost:  1.04355 accuracy:  0.725\n",
      "validation cost:  1.22985 accuracy:  0.5734\n",
      "Epoch 21, CIFAR-10 Batch 3:  trainging cost:  1.07286 accuracy:  0.675\n",
      "validation cost:  1.22047 accuracy:  0.5782\n",
      "Epoch 21, CIFAR-10 Batch 4:  trainging cost:  1.19573 accuracy:  0.525\n",
      "validation cost:  1.1981 accuracy:  0.5716\n",
      "Epoch 21, CIFAR-10 Batch 5:  trainging cost:  1.0484 accuracy:  0.7\n",
      "validation cost:  1.20062 accuracy:  0.5702\n",
      "Epoch 22, CIFAR-10 Batch 1:  trainging cost:  1.24227 accuracy:  0.525\n",
      "validation cost:  1.19521 accuracy:  0.5766\n",
      "Epoch 22, CIFAR-10 Batch 2:  trainging cost:  1.03958 accuracy:  0.625\n",
      "validation cost:  1.23015 accuracy:  0.5762\n",
      "Epoch 22, CIFAR-10 Batch 3:  trainging cost:  1.05536 accuracy:  0.7\n",
      "validation cost:  1.20739 accuracy:  0.5772\n",
      "Epoch 22, CIFAR-10 Batch 4:  trainging cost:  1.14827 accuracy:  0.55\n",
      "validation cost:  1.17314 accuracy:  0.5814\n",
      "Epoch 22, CIFAR-10 Batch 5:  trainging cost:  1.13123 accuracy:  0.625\n",
      "validation cost:  1.21261 accuracy:  0.5754\n",
      "Epoch 23, CIFAR-10 Batch 1:  trainging cost:  1.15843 accuracy:  0.525\n",
      "validation cost:  1.19594 accuracy:  0.582\n",
      "Epoch 23, CIFAR-10 Batch 2:  trainging cost:  1.03639 accuracy:  0.65\n",
      "validation cost:  1.24354 accuracy:  0.5676\n",
      "Epoch 23, CIFAR-10 Batch 3:  trainging cost:  0.992285 accuracy:  0.7\n",
      "validation cost:  1.20362 accuracy:  0.5762\n",
      "Epoch 23, CIFAR-10 Batch 4:  trainging cost:  1.17252 accuracy:  0.5\n",
      "validation cost:  1.2046 accuracy:  0.5778\n",
      "Epoch 23, CIFAR-10 Batch 5:  trainging cost:  1.03268 accuracy:  0.65\n",
      "validation cost:  1.18037 accuracy:  0.5874\n",
      "Epoch 24, CIFAR-10 Batch 1:  trainging cost:  1.2219 accuracy:  0.55\n",
      "validation cost:  1.204 accuracy:  0.576\n",
      "Epoch 24, CIFAR-10 Batch 2:  trainging cost:  0.998918 accuracy:  0.7\n",
      "validation cost:  1.18658 accuracy:  0.5852\n",
      "Epoch 24, CIFAR-10 Batch 3:  trainging cost:  0.991029 accuracy:  0.775\n",
      "validation cost:  1.18086 accuracy:  0.585\n",
      "Epoch 24, CIFAR-10 Batch 4:  trainging cost:  1.18719 accuracy:  0.6\n",
      "validation cost:  1.19593 accuracy:  0.5782\n",
      "Epoch 24, CIFAR-10 Batch 5:  trainging cost:  0.999917 accuracy:  0.675\n",
      "validation cost:  1.19124 accuracy:  0.578\n",
      "Epoch 25, CIFAR-10 Batch 1:  trainging cost:  1.27982 accuracy:  0.525\n",
      "validation cost:  1.22129 accuracy:  0.5766\n",
      "Epoch 25, CIFAR-10 Batch 2:  trainging cost:  0.960748 accuracy:  0.725\n",
      "validation cost:  1.19741 accuracy:  0.5772\n",
      "Epoch 25, CIFAR-10 Batch 3:  trainging cost:  0.973065 accuracy:  0.725\n",
      "validation cost:  1.18049 accuracy:  0.5838\n",
      "Epoch 25, CIFAR-10 Batch 4:  trainging cost:  1.1374 accuracy:  0.625\n",
      "validation cost:  1.18297 accuracy:  0.5844\n",
      "Epoch 25, CIFAR-10 Batch 5:  trainging cost:  1.01002 accuracy:  0.675\n",
      "validation cost:  1.17756 accuracy:  0.5816\n",
      "Epoch 26, CIFAR-10 Batch 1:  trainging cost:  1.22175 accuracy:  0.525\n",
      "validation cost:  1.17798 accuracy:  0.587\n",
      "Epoch 26, CIFAR-10 Batch 2:  trainging cost:  1.06533 accuracy:  0.7\n",
      "validation cost:  1.21864 accuracy:  0.5808\n",
      "Epoch 26, CIFAR-10 Batch 3:  trainging cost:  0.993286 accuracy:  0.675\n",
      "validation cost:  1.19118 accuracy:  0.5754\n",
      "Epoch 26, CIFAR-10 Batch 4:  trainging cost:  1.11828 accuracy:  0.6\n",
      "validation cost:  1.17846 accuracy:  0.5834\n",
      "Epoch 26, CIFAR-10 Batch 5:  trainging cost:  1.00784 accuracy:  0.65\n",
      "validation cost:  1.18763 accuracy:  0.5788\n",
      "Epoch 27, CIFAR-10 Batch 1:  trainging cost:  1.14477 accuracy:  0.6\n",
      "validation cost:  1.17212 accuracy:  0.585\n",
      "Epoch 27, CIFAR-10 Batch 2:  trainging cost:  0.997079 accuracy:  0.725\n",
      "validation cost:  1.24262 accuracy:  0.5656\n",
      "Epoch 27, CIFAR-10 Batch 3:  trainging cost:  0.952808 accuracy:  0.725\n",
      "validation cost:  1.16211 accuracy:  0.5814\n",
      "Epoch 27, CIFAR-10 Batch 4:  trainging cost:  1.13986 accuracy:  0.625\n",
      "validation cost:  1.17533 accuracy:  0.5834\n",
      "Epoch 27, CIFAR-10 Batch 5:  trainging cost:  0.988662 accuracy:  0.625\n",
      "validation cost:  1.17496 accuracy:  0.5814\n",
      "Epoch 28, CIFAR-10 Batch 1:  trainging cost:  1.13146 accuracy:  0.575\n",
      "validation cost:  1.20099 accuracy:  0.5818\n",
      "Epoch 28, CIFAR-10 Batch 2:  trainging cost:  0.98605 accuracy:  0.675\n",
      "validation cost:  1.18408 accuracy:  0.5902\n",
      "Epoch 28, CIFAR-10 Batch 3:  trainging cost:  0.94841 accuracy:  0.675\n",
      "validation cost:  1.16948 accuracy:  0.5824\n",
      "Epoch 28, CIFAR-10 Batch 4:  trainging cost:  1.0967 accuracy:  0.575\n",
      "validation cost:  1.19072 accuracy:  0.5822\n",
      "Epoch 28, CIFAR-10 Batch 5:  trainging cost:  0.949832 accuracy:  0.65\n",
      "validation cost:  1.16115 accuracy:  0.5972\n",
      "Epoch 29, CIFAR-10 Batch 1:  trainging cost:  1.16259 accuracy:  0.525\n",
      "validation cost:  1.19187 accuracy:  0.587\n",
      "Epoch 29, CIFAR-10 Batch 2:  trainging cost:  0.949085 accuracy:  0.725\n",
      "validation cost:  1.2083 accuracy:  0.5816\n",
      "Epoch 29, CIFAR-10 Batch 3:  trainging cost:  0.934797 accuracy:  0.725\n",
      "validation cost:  1.19049 accuracy:  0.5834\n",
      "Epoch 29, CIFAR-10 Batch 4:  trainging cost:  1.07196 accuracy:  0.55\n",
      "validation cost:  1.16147 accuracy:  0.582\n",
      "Epoch 29, CIFAR-10 Batch 5:  trainging cost:  0.988906 accuracy:  0.65\n",
      "validation cost:  1.18642 accuracy:  0.5926\n",
      "Epoch 30, CIFAR-10 Batch 1:  trainging cost:  1.1016 accuracy:  0.525\n",
      "validation cost:  1.1712 accuracy:  0.5886\n",
      "Epoch 30, CIFAR-10 Batch 2:  trainging cost:  0.944719 accuracy:  0.65\n",
      "validation cost:  1.1954 accuracy:  0.5838\n",
      "Epoch 30, CIFAR-10 Batch 3:  trainging cost:  0.971996 accuracy:  0.7\n",
      "validation cost:  1.20514 accuracy:  0.5734\n",
      "Epoch 30, CIFAR-10 Batch 4:  trainging cost:  1.10662 accuracy:  0.6\n",
      "validation cost:  1.16062 accuracy:  0.5896\n",
      "Epoch 30, CIFAR-10 Batch 5:  trainging cost:  0.944233 accuracy:  0.675\n",
      "validation cost:  1.17867 accuracy:  0.5842\n",
      "Epoch 31, CIFAR-10 Batch 1:  trainging cost:  1.08381 accuracy:  0.6\n",
      "validation cost:  1.14962 accuracy:  0.6014\n",
      "Epoch 31, CIFAR-10 Batch 2:  trainging cost:  0.881839 accuracy:  0.7\n",
      "validation cost:  1.16899 accuracy:  0.5892\n",
      "Epoch 31, CIFAR-10 Batch 3:  trainging cost:  0.939308 accuracy:  0.675\n",
      "validation cost:  1.16683 accuracy:  0.5882\n",
      "Epoch 31, CIFAR-10 Batch 4:  trainging cost:  1.06917 accuracy:  0.625\n",
      "validation cost:  1.16243 accuracy:  0.5902\n",
      "Epoch 31, CIFAR-10 Batch 5:  trainging cost:  0.969348 accuracy:  0.675\n",
      "validation cost:  1.17514 accuracy:  0.5856\n",
      "Epoch 32, CIFAR-10 Batch 1:  trainging cost:  1.06338 accuracy:  0.6\n",
      "validation cost:  1.16478 accuracy:  0.5928\n",
      "Epoch 32, CIFAR-10 Batch 2:  trainging cost:  0.92578 accuracy:  0.75\n",
      "validation cost:  1.19627 accuracy:  0.5848\n",
      "Epoch 32, CIFAR-10 Batch 3:  trainging cost:  0.943527 accuracy:  0.675\n",
      "validation cost:  1.19496 accuracy:  0.5766\n",
      "Epoch 32, CIFAR-10 Batch 4:  trainging cost:  1.06974 accuracy:  0.625\n",
      "validation cost:  1.16571 accuracy:  0.5872\n",
      "Epoch 32, CIFAR-10 Batch 5:  trainging cost:  0.968704 accuracy:  0.725\n",
      "validation cost:  1.1633 accuracy:  0.5894\n",
      "Epoch 33, CIFAR-10 Batch 1:  trainging cost:  1.09008 accuracy:  0.55\n",
      "validation cost:  1.16404 accuracy:  0.5962\n",
      "Epoch 33, CIFAR-10 Batch 2:  trainging cost:  0.877634 accuracy:  0.8\n",
      "validation cost:  1.18429 accuracy:  0.5816\n",
      "Epoch 33, CIFAR-10 Batch 3:  trainging cost:  0.940408 accuracy:  0.65\n",
      "validation cost:  1.15673 accuracy:  0.5964\n",
      "Epoch 33, CIFAR-10 Batch 4:  trainging cost:  1.01924 accuracy:  0.65\n",
      "validation cost:  1.1683 accuracy:  0.5932\n",
      "Epoch 33, CIFAR-10 Batch 5:  trainging cost:  0.940549 accuracy:  0.625\n",
      "validation cost:  1.15757 accuracy:  0.595\n",
      "Epoch 34, CIFAR-10 Batch 1:  trainging cost:  1.05036 accuracy:  0.6\n",
      "validation cost:  1.15354 accuracy:  0.5982\n",
      "Epoch 34, CIFAR-10 Batch 2:  trainging cost:  0.857226 accuracy:  0.775\n",
      "validation cost:  1.18205 accuracy:  0.5934\n",
      "Epoch 34, CIFAR-10 Batch 3:  trainging cost:  0.930548 accuracy:  0.65\n",
      "validation cost:  1.16827 accuracy:  0.5896\n",
      "Epoch 34, CIFAR-10 Batch 4:  trainging cost:  0.989771 accuracy:  0.65\n",
      "validation cost:  1.15902 accuracy:  0.5944\n",
      "Epoch 34, CIFAR-10 Batch 5:  trainging cost:  0.974317 accuracy:  0.675\n",
      "validation cost:  1.1819 accuracy:  0.5804\n",
      "Epoch 35, CIFAR-10 Batch 1:  trainging cost:  1.01652 accuracy:  0.55\n",
      "validation cost:  1.17101 accuracy:  0.593\n",
      "Epoch 35, CIFAR-10 Batch 2:  trainging cost:  0.91488 accuracy:  0.75\n",
      "validation cost:  1.19243 accuracy:  0.5888\n",
      "Epoch 35, CIFAR-10 Batch 3:  trainging cost:  0.914844 accuracy:  0.7\n",
      "validation cost:  1.17761 accuracy:  0.5846\n",
      "Epoch 35, CIFAR-10 Batch 4:  trainging cost:  1.0419 accuracy:  0.6\n",
      "validation cost:  1.17139 accuracy:  0.5904\n",
      "Epoch 35, CIFAR-10 Batch 5:  trainging cost:  0.969667 accuracy:  0.675\n",
      "validation cost:  1.16021 accuracy:  0.6014\n",
      "Epoch 36, CIFAR-10 Batch 1:  trainging cost:  1.02699 accuracy:  0.625\n",
      "validation cost:  1.18038 accuracy:  0.5914\n",
      "Epoch 36, CIFAR-10 Batch 2:  trainging cost:  0.93506 accuracy:  0.75\n",
      "validation cost:  1.21204 accuracy:  0.5822\n",
      "Epoch 36, CIFAR-10 Batch 3:  trainging cost:  0.908628 accuracy:  0.725\n",
      "validation cost:  1.16657 accuracy:  0.5882\n",
      "Epoch 36, CIFAR-10 Batch 4:  trainging cost:  1.03223 accuracy:  0.65\n",
      "validation cost:  1.17648 accuracy:  0.583\n",
      "Epoch 36, CIFAR-10 Batch 5:  trainging cost:  0.96962 accuracy:  0.7\n",
      "validation cost:  1.1782 accuracy:  0.589\n",
      "Epoch 37, CIFAR-10 Batch 1:  trainging cost:  0.953643 accuracy:  0.7\n",
      "validation cost:  1.1633 accuracy:  0.5926\n",
      "Epoch 37, CIFAR-10 Batch 2:  trainging cost:  0.88824 accuracy:  0.775\n",
      "validation cost:  1.19035 accuracy:  0.588\n",
      "Epoch 37, CIFAR-10 Batch 3:  trainging cost:  0.89914 accuracy:  0.7\n",
      "validation cost:  1.17207 accuracy:  0.5946\n",
      "Epoch 37, CIFAR-10 Batch 4:  trainging cost:  1.04582 accuracy:  0.575\n",
      "validation cost:  1.17069 accuracy:  0.5906\n",
      "Epoch 37, CIFAR-10 Batch 5:  trainging cost:  0.937667 accuracy:  0.675\n",
      "validation cost:  1.15954 accuracy:  0.5924\n",
      "Epoch 38, CIFAR-10 Batch 1:  trainging cost:  1.0022 accuracy:  0.6\n",
      "validation cost:  1.16802 accuracy:  0.592\n",
      "Epoch 38, CIFAR-10 Batch 2:  trainging cost:  1.03002 accuracy:  0.65\n",
      "validation cost:  1.25767 accuracy:  0.564\n",
      "Epoch 38, CIFAR-10 Batch 3:  trainging cost:  0.896626 accuracy:  0.75\n",
      "validation cost:  1.16342 accuracy:  0.5906\n",
      "Epoch 38, CIFAR-10 Batch 4:  trainging cost:  1.05158 accuracy:  0.625\n",
      "validation cost:  1.17586 accuracy:  0.5892\n",
      "Epoch 38, CIFAR-10 Batch 5:  trainging cost:  0.906778 accuracy:  0.7\n",
      "validation cost:  1.16569 accuracy:  0.5922\n",
      "Epoch 39, CIFAR-10 Batch 1:  trainging cost:  0.997067 accuracy:  0.625\n",
      "validation cost:  1.16602 accuracy:  0.5898\n",
      "Epoch 39, CIFAR-10 Batch 2:  trainging cost:  0.868464 accuracy:  0.725\n",
      "validation cost:  1.20094 accuracy:  0.581\n",
      "Epoch 39, CIFAR-10 Batch 3:  trainging cost:  0.919016 accuracy:  0.775\n",
      "validation cost:  1.16578 accuracy:  0.5908\n",
      "Epoch 39, CIFAR-10 Batch 4:  trainging cost:  1.03007 accuracy:  0.65\n",
      "validation cost:  1.17669 accuracy:  0.5806\n",
      "Epoch 39, CIFAR-10 Batch 5:  trainging cost:  0.940187 accuracy:  0.625\n",
      "validation cost:  1.16812 accuracy:  0.5942\n",
      "Epoch 40, CIFAR-10 Batch 1:  trainging cost:  0.983735 accuracy:  0.6\n",
      "validation cost:  1.17808 accuracy:  0.5902\n",
      "Epoch 40, CIFAR-10 Batch 2:  trainging cost:  0.932041 accuracy:  0.7\n",
      "validation cost:  1.21819 accuracy:  0.5736\n",
      "Epoch 40, CIFAR-10 Batch 3:  trainging cost:  0.944271 accuracy:  0.725\n",
      "validation cost:  1.16435 accuracy:  0.5932\n",
      "Epoch 40, CIFAR-10 Batch 4:  trainging cost:  1.01566 accuracy:  0.625\n",
      "validation cost:  1.16455 accuracy:  0.5922\n",
      "Epoch 40, CIFAR-10 Batch 5:  trainging cost:  0.922548 accuracy:  0.675\n",
      "validation cost:  1.17161 accuracy:  0.5926\n",
      "Epoch 41, CIFAR-10 Batch 1:  trainging cost:  0.989363 accuracy:  0.675\n",
      "validation cost:  1.18396 accuracy:  0.5834\n",
      "Epoch 41, CIFAR-10 Batch 2:  trainging cost:  0.859862 accuracy:  0.75\n",
      "validation cost:  1.19617 accuracy:  0.5824\n",
      "Epoch 41, CIFAR-10 Batch 3:  trainging cost:  0.933264 accuracy:  0.65\n",
      "validation cost:  1.18633 accuracy:  0.5788\n",
      "Epoch 41, CIFAR-10 Batch 4:  trainging cost:  0.962401 accuracy:  0.625\n",
      "validation cost:  1.15158 accuracy:  0.598\n",
      "Epoch 41, CIFAR-10 Batch 5:  trainging cost:  0.909635 accuracy:  0.675\n",
      "validation cost:  1.16698 accuracy:  0.5936\n",
      "Epoch 42, CIFAR-10 Batch 1:  trainging cost:  0.992727 accuracy:  0.625\n",
      "validation cost:  1.16839 accuracy:  0.5942\n",
      "Epoch 42, CIFAR-10 Batch 2:  trainging cost:  0.801123 accuracy:  0.725\n",
      "validation cost:  1.15831 accuracy:  0.597\n",
      "Epoch 42, CIFAR-10 Batch 3:  trainging cost:  0.90498 accuracy:  0.725\n",
      "validation cost:  1.16995 accuracy:  0.59\n",
      "Epoch 42, CIFAR-10 Batch 4:  trainging cost:  0.975615 accuracy:  0.65\n",
      "validation cost:  1.16394 accuracy:  0.5852\n",
      "Epoch 42, CIFAR-10 Batch 5:  trainging cost:  0.964404 accuracy:  0.625\n",
      "validation cost:  1.17563 accuracy:  0.5856\n",
      "Epoch 43, CIFAR-10 Batch 1:  trainging cost:  0.913282 accuracy:  0.7\n",
      "validation cost:  1.16698 accuracy:  0.5906\n",
      "Epoch 43, CIFAR-10 Batch 2:  trainging cost:  0.82455 accuracy:  0.75\n",
      "validation cost:  1.17446 accuracy:  0.5888\n",
      "Epoch 43, CIFAR-10 Batch 3:  trainging cost:  0.87968 accuracy:  0.725\n",
      "validation cost:  1.16 accuracy:  0.5884\n",
      "Epoch 43, CIFAR-10 Batch 4:  trainging cost:  1.04839 accuracy:  0.625\n",
      "validation cost:  1.17173 accuracy:  0.591\n",
      "Epoch 43, CIFAR-10 Batch 5:  trainging cost:  1.00437 accuracy:  0.625\n",
      "validation cost:  1.17402 accuracy:  0.5966\n",
      "Epoch 44, CIFAR-10 Batch 1:  trainging cost:  0.997857 accuracy:  0.6\n",
      "validation cost:  1.19028 accuracy:  0.5908\n",
      "Epoch 44, CIFAR-10 Batch 2:  trainging cost:  0.865632 accuracy:  0.725\n",
      "validation cost:  1.19455 accuracy:  0.5798\n",
      "Epoch 44, CIFAR-10 Batch 3:  trainging cost:  0.899796 accuracy:  0.7\n",
      "validation cost:  1.18267 accuracy:  0.5768\n",
      "Epoch 44, CIFAR-10 Batch 4:  trainging cost:  0.979488 accuracy:  0.625\n",
      "validation cost:  1.16245 accuracy:  0.5892\n",
      "Epoch 44, CIFAR-10 Batch 5:  trainging cost:  0.883988 accuracy:  0.65\n",
      "validation cost:  1.13851 accuracy:  0.6054\n",
      "Epoch 45, CIFAR-10 Batch 1:  trainging cost:  0.991175 accuracy:  0.625\n",
      "validation cost:  1.17988 accuracy:  0.5844\n",
      "Epoch 45, CIFAR-10 Batch 2:  trainging cost:  0.82016 accuracy:  0.675\n",
      "validation cost:  1.18986 accuracy:  0.5796\n",
      "Epoch 45, CIFAR-10 Batch 3:  trainging cost:  0.964467 accuracy:  0.65\n",
      "validation cost:  1.20752 accuracy:  0.573\n",
      "Epoch 45, CIFAR-10 Batch 4:  trainging cost:  0.953729 accuracy:  0.65\n",
      "validation cost:  1.16014 accuracy:  0.5924\n",
      "Epoch 45, CIFAR-10 Batch 5:  trainging cost:  0.918953 accuracy:  0.65\n",
      "validation cost:  1.16866 accuracy:  0.5952\n",
      "Epoch 46, CIFAR-10 Batch 1:  trainging cost:  0.941284 accuracy:  0.65\n",
      "validation cost:  1.15672 accuracy:  0.5978\n",
      "Epoch 46, CIFAR-10 Batch 2:  trainging cost:  0.815853 accuracy:  0.75\n",
      "validation cost:  1.18005 accuracy:  0.5884\n",
      "Epoch 46, CIFAR-10 Batch 3:  trainging cost:  0.890045 accuracy:  0.725\n",
      "validation cost:  1.18584 accuracy:  0.5788\n",
      "Epoch 46, CIFAR-10 Batch 4:  trainging cost:  0.953288 accuracy:  0.65\n",
      "validation cost:  1.1822 accuracy:  0.5938\n",
      "Epoch 46, CIFAR-10 Batch 5:  trainging cost:  0.898765 accuracy:  0.7\n",
      "validation cost:  1.16968 accuracy:  0.5912\n",
      "Epoch 47, CIFAR-10 Batch 1:  trainging cost:  0.97302 accuracy:  0.625\n",
      "validation cost:  1.15943 accuracy:  0.5978\n",
      "Epoch 47, CIFAR-10 Batch 2:  trainging cost:  0.854615 accuracy:  0.725\n",
      "validation cost:  1.20282 accuracy:  0.574\n",
      "Epoch 47, CIFAR-10 Batch 3:  trainging cost:  0.915522 accuracy:  0.725\n",
      "validation cost:  1.18003 accuracy:  0.593\n",
      "Epoch 47, CIFAR-10 Batch 4:  trainging cost:  0.968262 accuracy:  0.675\n",
      "validation cost:  1.15897 accuracy:  0.5942\n",
      "Epoch 47, CIFAR-10 Batch 5:  trainging cost:  0.902701 accuracy:  0.675\n",
      "validation cost:  1.16743 accuracy:  0.5926\n",
      "Epoch 48, CIFAR-10 Batch 1:  trainging cost:  0.951127 accuracy:  0.6\n",
      "validation cost:  1.19485 accuracy:  0.588\n",
      "Epoch 48, CIFAR-10 Batch 2:  trainging cost:  0.818092 accuracy:  0.7\n",
      "validation cost:  1.17058 accuracy:  0.5896\n",
      "Epoch 48, CIFAR-10 Batch 3:  trainging cost:  0.958786 accuracy:  0.675\n",
      "validation cost:  1.20273 accuracy:  0.5748\n",
      "Epoch 48, CIFAR-10 Batch 4:  trainging cost:  0.996656 accuracy:  0.6\n",
      "validation cost:  1.17914 accuracy:  0.5918\n",
      "Epoch 48, CIFAR-10 Batch 5:  trainging cost:  0.887386 accuracy:  0.7\n",
      "validation cost:  1.16174 accuracy:  0.6004\n",
      "Epoch 49, CIFAR-10 Batch 1:  trainging cost:  0.93274 accuracy:  0.7\n",
      "validation cost:  1.15513 accuracy:  0.5994\n",
      "Epoch 49, CIFAR-10 Batch 2:  trainging cost:  0.838415 accuracy:  0.675\n",
      "validation cost:  1.18269 accuracy:  0.5806\n",
      "Epoch 49, CIFAR-10 Batch 3:  trainging cost:  0.853926 accuracy:  0.775\n",
      "validation cost:  1.18465 accuracy:  0.5848\n",
      "Epoch 49, CIFAR-10 Batch 4:  trainging cost:  0.993959 accuracy:  0.6\n",
      "validation cost:  1.16819 accuracy:  0.5952\n",
      "Epoch 49, CIFAR-10 Batch 5:  trainging cost:  0.939237 accuracy:  0.65\n",
      "validation cost:  1.15352 accuracy:  0.599\n",
      "Epoch 50, CIFAR-10 Batch 1:  trainging cost:  0.947734 accuracy:  0.65\n",
      "validation cost:  1.15885 accuracy:  0.6038\n",
      "Epoch 50, CIFAR-10 Batch 2:  trainging cost:  0.837372 accuracy:  0.7\n",
      "validation cost:  1.21079 accuracy:  0.5804\n",
      "Epoch 50, CIFAR-10 Batch 3:  trainging cost:  0.872769 accuracy:  0.7\n",
      "validation cost:  1.16725 accuracy:  0.5904\n",
      "Epoch 50, CIFAR-10 Batch 4:  trainging cost:  0.93263 accuracy:  0.6\n",
      "validation cost:  1.16363 accuracy:  0.5928\n",
      "Epoch 50, CIFAR-10 Batch 5:  trainging cost:  0.915095 accuracy:  0.575\n",
      "validation cost:  1.15496 accuracy:  0.5956\n",
      "Epoch 51, CIFAR-10 Batch 1:  trainging cost:  0.914136 accuracy:  0.65\n",
      "validation cost:  1.15747 accuracy:  0.5994\n",
      "Epoch 51, CIFAR-10 Batch 2:  trainging cost:  0.789071 accuracy:  0.75\n",
      "validation cost:  1.15297 accuracy:  0.5996\n",
      "Epoch 51, CIFAR-10 Batch 3:  trainging cost:  0.904468 accuracy:  0.7\n",
      "validation cost:  1.19306 accuracy:  0.5866\n",
      "Epoch 51, CIFAR-10 Batch 4:  trainging cost:  0.989159 accuracy:  0.6\n",
      "validation cost:  1.18468 accuracy:  0.5864\n",
      "Epoch 51, CIFAR-10 Batch 5:  trainging cost:  0.950112 accuracy:  0.65\n",
      "validation cost:  1.15532 accuracy:  0.5958\n",
      "Epoch 52, CIFAR-10 Batch 1:  trainging cost:  0.938521 accuracy:  0.65\n",
      "validation cost:  1.18597 accuracy:  0.5918\n",
      "Epoch 52, CIFAR-10 Batch 2:  trainging cost:  0.759396 accuracy:  0.7\n",
      "validation cost:  1.16085 accuracy:  0.5964\n",
      "Epoch 52, CIFAR-10 Batch 3:  trainging cost:  0.857894 accuracy:  0.75\n",
      "validation cost:  1.16467 accuracy:  0.5948\n",
      "Epoch 52, CIFAR-10 Batch 4:  trainging cost:  0.948862 accuracy:  0.65\n",
      "validation cost:  1.17476 accuracy:  0.5888\n",
      "Epoch 52, CIFAR-10 Batch 5:  trainging cost:  0.922489 accuracy:  0.675\n",
      "validation cost:  1.1597 accuracy:  0.597\n",
      "Epoch 53, CIFAR-10 Batch 1:  trainging cost:  0.992347 accuracy:  0.6\n",
      "validation cost:  1.18185 accuracy:  0.5984\n",
      "Epoch 53, CIFAR-10 Batch 2:  trainging cost:  0.767821 accuracy:  0.775\n",
      "validation cost:  1.15743 accuracy:  0.599\n",
      "Epoch 53, CIFAR-10 Batch 3:  trainging cost:  0.88192 accuracy:  0.675\n",
      "validation cost:  1.15587 accuracy:  0.5968\n",
      "Epoch 53, CIFAR-10 Batch 4:  trainging cost:  0.922779 accuracy:  0.625\n",
      "validation cost:  1.15765 accuracy:  0.5942\n",
      "Epoch 53, CIFAR-10 Batch 5:  trainging cost:  0.880115 accuracy:  0.675\n",
      "validation cost:  1.16335 accuracy:  0.5996\n",
      "Epoch 54, CIFAR-10 Batch 1:  trainging cost:  0.914401 accuracy:  0.65\n",
      "validation cost:  1.16483 accuracy:  0.5974\n",
      "Epoch 54, CIFAR-10 Batch 2:  trainging cost:  0.815857 accuracy:  0.725\n",
      "validation cost:  1.20949 accuracy:  0.5836\n",
      "Epoch 54, CIFAR-10 Batch 3:  trainging cost:  0.875948 accuracy:  0.75\n",
      "validation cost:  1.16734 accuracy:  0.5942\n",
      "Epoch 54, CIFAR-10 Batch 4:  trainging cost:  0.956482 accuracy:  0.575\n",
      "validation cost:  1.17916 accuracy:  0.5936\n",
      "Epoch 54, CIFAR-10 Batch 5:  trainging cost:  0.891232 accuracy:  0.65\n",
      "validation cost:  1.16373 accuracy:  0.5988\n",
      "Epoch 55, CIFAR-10 Batch 1:  trainging cost:  0.915249 accuracy:  0.675\n",
      "validation cost:  1.18934 accuracy:  0.5896\n",
      "Epoch 55, CIFAR-10 Batch 2:  trainging cost:  0.763112 accuracy:  0.725\n",
      "validation cost:  1.18153 accuracy:  0.5986\n",
      "Epoch 55, CIFAR-10 Batch 3:  trainging cost:  0.867236 accuracy:  0.725\n",
      "validation cost:  1.16499 accuracy:  0.597\n",
      "Epoch 55, CIFAR-10 Batch 4:  trainging cost:  0.922387 accuracy:  0.675\n",
      "validation cost:  1.17898 accuracy:  0.5922\n",
      "Epoch 55, CIFAR-10 Batch 5:  trainging cost:  0.885466 accuracy:  0.65\n",
      "validation cost:  1.15669 accuracy:  0.6016\n",
      "Epoch 56, CIFAR-10 Batch 1:  trainging cost:  0.93594 accuracy:  0.6\n",
      "validation cost:  1.20628 accuracy:  0.5856\n",
      "Epoch 56, CIFAR-10 Batch 2:  trainging cost:  0.783416 accuracy:  0.75\n",
      "validation cost:  1.16569 accuracy:  0.5984\n",
      "Epoch 56, CIFAR-10 Batch 3:  trainging cost:  0.827221 accuracy:  0.725\n",
      "validation cost:  1.14907 accuracy:  0.6084\n",
      "Epoch 56, CIFAR-10 Batch 4:  trainging cost:  0.976832 accuracy:  0.65\n",
      "validation cost:  1.17712 accuracy:  0.592\n",
      "Epoch 56, CIFAR-10 Batch 5:  trainging cost:  0.910854 accuracy:  0.675\n",
      "validation cost:  1.1617 accuracy:  0.598\n",
      "Epoch 57, CIFAR-10 Batch 1:  trainging cost:  0.825796 accuracy:  0.775\n",
      "validation cost:  1.16572 accuracy:  0.6024\n",
      "Epoch 57, CIFAR-10 Batch 2:  trainging cost:  0.790074 accuracy:  0.7\n",
      "validation cost:  1.18483 accuracy:  0.5884\n",
      "Epoch 57, CIFAR-10 Batch 3:  trainging cost:  0.83043 accuracy:  0.775\n",
      "validation cost:  1.15259 accuracy:  0.5996\n",
      "Epoch 57, CIFAR-10 Batch 4:  trainging cost:  0.9701 accuracy:  0.625\n",
      "validation cost:  1.19331 accuracy:  0.59\n",
      "Epoch 57, CIFAR-10 Batch 5:  trainging cost:  0.883221 accuracy:  0.675\n",
      "validation cost:  1.16555 accuracy:  0.6018\n",
      "Epoch 58, CIFAR-10 Batch 1:  trainging cost:  0.892055 accuracy:  0.7\n",
      "validation cost:  1.21112 accuracy:  0.582\n",
      "Epoch 58, CIFAR-10 Batch 2:  trainging cost:  0.802828 accuracy:  0.825\n",
      "validation cost:  1.20417 accuracy:  0.583\n",
      "Epoch 58, CIFAR-10 Batch 3:  trainging cost:  0.90564 accuracy:  0.65\n",
      "validation cost:  1.16879 accuracy:  0.5904\n",
      "Epoch 58, CIFAR-10 Batch 4:  trainging cost:  0.911768 accuracy:  0.65\n",
      "validation cost:  1.1673 accuracy:  0.5994\n",
      "Epoch 58, CIFAR-10 Batch 5:  trainging cost:  0.85368 accuracy:  0.675\n",
      "validation cost:  1.15654 accuracy:  0.5998\n",
      "Epoch 59, CIFAR-10 Batch 1:  trainging cost:  0.905021 accuracy:  0.65\n",
      "validation cost:  1.18865 accuracy:  0.5994\n",
      "Epoch 59, CIFAR-10 Batch 2:  trainging cost:  0.820244 accuracy:  0.65\n",
      "validation cost:  1.1912 accuracy:  0.5768\n",
      "Epoch 59, CIFAR-10 Batch 3:  trainging cost:  0.839164 accuracy:  0.775\n",
      "validation cost:  1.17826 accuracy:  0.5968\n",
      "Epoch 59, CIFAR-10 Batch 4:  trainging cost:  0.907262 accuracy:  0.625\n",
      "validation cost:  1.16397 accuracy:  0.5916\n",
      "Epoch 59, CIFAR-10 Batch 5:  trainging cost:  0.887393 accuracy:  0.7\n",
      "validation cost:  1.18061 accuracy:  0.5942\n",
      "Epoch 60, CIFAR-10 Batch 1:  trainging cost:  0.83507 accuracy:  0.75\n",
      "validation cost:  1.18351 accuracy:  0.5954\n",
      "Epoch 60, CIFAR-10 Batch 2:  trainging cost:  0.726681 accuracy:  0.7\n",
      "validation cost:  1.15745 accuracy:  0.5998\n",
      "Epoch 60, CIFAR-10 Batch 3:  trainging cost:  0.851772 accuracy:  0.675\n",
      "validation cost:  1.17516 accuracy:  0.5922\n",
      "Epoch 60, CIFAR-10 Batch 4:  trainging cost:  0.868263 accuracy:  0.65\n",
      "validation cost:  1.18045 accuracy:  0.598\n",
      "Epoch 60, CIFAR-10 Batch 5:  trainging cost:  0.868054 accuracy:  0.675\n",
      "validation cost:  1.16614 accuracy:  0.5996\n",
      "Epoch 61, CIFAR-10 Batch 1:  trainging cost:  0.847963 accuracy:  0.75\n",
      "validation cost:  1.20432 accuracy:  0.5972\n",
      "Epoch 61, CIFAR-10 Batch 2:  trainging cost:  0.768656 accuracy:  0.75\n",
      "validation cost:  1.1807 accuracy:  0.5886\n",
      "Epoch 61, CIFAR-10 Batch 3:  trainging cost:  0.818415 accuracy:  0.75\n",
      "validation cost:  1.17352 accuracy:  0.5986\n",
      "Epoch 61, CIFAR-10 Batch 4:  trainging cost:  0.880829 accuracy:  0.7\n",
      "validation cost:  1.17559 accuracy:  0.5858\n",
      "Epoch 61, CIFAR-10 Batch 5:  trainging cost:  0.863129 accuracy:  0.725\n",
      "validation cost:  1.17388 accuracy:  0.5942\n",
      "Epoch 62, CIFAR-10 Batch 1:  trainging cost:  0.852394 accuracy:  0.75\n",
      "validation cost:  1.19513 accuracy:  0.6002\n",
      "Epoch 62, CIFAR-10 Batch 2:  trainging cost:  0.785941 accuracy:  0.7\n",
      "validation cost:  1.20156 accuracy:  0.584\n",
      "Epoch 62, CIFAR-10 Batch 3:  trainging cost:  0.835001 accuracy:  0.725\n",
      "validation cost:  1.1761 accuracy:  0.5966\n",
      "Epoch 62, CIFAR-10 Batch 4:  trainging cost:  0.937829 accuracy:  0.575\n",
      "validation cost:  1.17748 accuracy:  0.5906\n",
      "Epoch 62, CIFAR-10 Batch 5:  trainging cost:  0.844772 accuracy:  0.7\n",
      "validation cost:  1.1685 accuracy:  0.6022\n",
      "Epoch 63, CIFAR-10 Batch 1:  trainging cost:  0.840097 accuracy:  0.775\n",
      "validation cost:  1.17883 accuracy:  0.5916\n",
      "Epoch 63, CIFAR-10 Batch 2:  trainging cost:  0.79197 accuracy:  0.75\n",
      "validation cost:  1.19001 accuracy:  0.5888\n",
      "Epoch 63, CIFAR-10 Batch 3:  trainging cost:  0.798362 accuracy:  0.75\n",
      "validation cost:  1.16519 accuracy:  0.6036\n",
      "Epoch 63, CIFAR-10 Batch 4:  trainging cost:  0.857297 accuracy:  0.65\n",
      "validation cost:  1.17682 accuracy:  0.594\n",
      "Epoch 63, CIFAR-10 Batch 5:  trainging cost:  0.819609 accuracy:  0.7\n",
      "validation cost:  1.18696 accuracy:  0.598\n",
      "Epoch 64, CIFAR-10 Batch 1:  trainging cost:  0.831135 accuracy:  0.75\n",
      "validation cost:  1.18324 accuracy:  0.6006\n",
      "Epoch 64, CIFAR-10 Batch 2:  trainging cost:  0.72201 accuracy:  0.75\n",
      "validation cost:  1.18092 accuracy:  0.5946\n",
      "Epoch 64, CIFAR-10 Batch 3:  trainging cost:  0.85893 accuracy:  0.675\n",
      "validation cost:  1.17916 accuracy:  0.5898\n",
      "Epoch 64, CIFAR-10 Batch 4:  trainging cost:  0.886604 accuracy:  0.625\n",
      "validation cost:  1.17757 accuracy:  0.5992\n",
      "Epoch 64, CIFAR-10 Batch 5:  trainging cost:  0.885195 accuracy:  0.625\n",
      "validation cost:  1.18259 accuracy:  0.5996\n",
      "Epoch 65, CIFAR-10 Batch 1:  trainging cost:  0.855543 accuracy:  0.575\n",
      "validation cost:  1.20323 accuracy:  0.5828\n",
      "Epoch 65, CIFAR-10 Batch 2:  trainging cost:  0.757143 accuracy:  0.825\n",
      "validation cost:  1.20408 accuracy:  0.5866\n",
      "Epoch 65, CIFAR-10 Batch 3:  trainging cost:  0.811471 accuracy:  0.725\n",
      "validation cost:  1.16417 accuracy:  0.5992\n",
      "Epoch 65, CIFAR-10 Batch 4:  trainging cost:  0.911937 accuracy:  0.625\n",
      "validation cost:  1.1885 accuracy:  0.5978\n",
      "Epoch 65, CIFAR-10 Batch 5:  trainging cost:  0.824646 accuracy:  0.7\n",
      "validation cost:  1.16146 accuracy:  0.6058\n",
      "Epoch 66, CIFAR-10 Batch 1:  trainging cost:  0.822473 accuracy:  0.75\n",
      "validation cost:  1.19071 accuracy:  0.6022\n",
      "Epoch 66, CIFAR-10 Batch 2:  trainging cost:  0.750831 accuracy:  0.725\n",
      "validation cost:  1.18875 accuracy:  0.5912\n",
      "Epoch 66, CIFAR-10 Batch 3:  trainging cost:  0.788353 accuracy:  0.775\n",
      "validation cost:  1.177 accuracy:  0.5984\n",
      "Epoch 66, CIFAR-10 Batch 4:  trainging cost:  0.921114 accuracy:  0.65\n",
      "validation cost:  1.19116 accuracy:  0.5882\n",
      "Epoch 66, CIFAR-10 Batch 5:  trainging cost:  0.857716 accuracy:  0.75\n",
      "validation cost:  1.17459 accuracy:  0.5972\n",
      "Epoch 67, CIFAR-10 Batch 1:  trainging cost:  0.864869 accuracy:  0.725\n",
      "validation cost:  1.20342 accuracy:  0.593\n",
      "Epoch 67, CIFAR-10 Batch 2:  trainging cost:  0.7535 accuracy:  0.725\n",
      "validation cost:  1.19248 accuracy:  0.5862\n",
      "Epoch 67, CIFAR-10 Batch 3:  trainging cost:  0.784988 accuracy:  0.75\n",
      "validation cost:  1.16714 accuracy:  0.6006\n",
      "Epoch 67, CIFAR-10 Batch 4:  trainging cost:  0.861832 accuracy:  0.65\n",
      "validation cost:  1.18162 accuracy:  0.5934\n",
      "Epoch 67, CIFAR-10 Batch 5:  trainging cost:  0.82908 accuracy:  0.725\n",
      "validation cost:  1.19156 accuracy:  0.5986\n",
      "Epoch 68, CIFAR-10 Batch 1:  trainging cost:  0.830107 accuracy:  0.75\n",
      "validation cost:  1.20217 accuracy:  0.593\n",
      "Epoch 68, CIFAR-10 Batch 2:  trainging cost:  0.758121 accuracy:  0.725\n",
      "validation cost:  1.21567 accuracy:  0.5782\n",
      "Epoch 68, CIFAR-10 Batch 3:  trainging cost:  0.782363 accuracy:  0.775\n",
      "validation cost:  1.17348 accuracy:  0.598\n",
      "Epoch 68, CIFAR-10 Batch 4:  trainging cost:  0.866735 accuracy:  0.675\n",
      "validation cost:  1.196 accuracy:  0.5928\n",
      "Epoch 68, CIFAR-10 Batch 5:  trainging cost:  0.840881 accuracy:  0.7\n",
      "validation cost:  1.17539 accuracy:  0.5956\n",
      "Epoch 69, CIFAR-10 Batch 1:  trainging cost:  0.890544 accuracy:  0.675\n",
      "validation cost:  1.16951 accuracy:  0.602\n",
      "Epoch 69, CIFAR-10 Batch 2:  trainging cost:  0.763382 accuracy:  0.8\n",
      "validation cost:  1.22394 accuracy:  0.5798\n",
      "Epoch 69, CIFAR-10 Batch 3:  trainging cost:  0.787777 accuracy:  0.8\n",
      "validation cost:  1.17297 accuracy:  0.5982\n",
      "Epoch 69, CIFAR-10 Batch 4:  trainging cost:  0.8302 accuracy:  0.675\n",
      "validation cost:  1.17202 accuracy:  0.5986\n",
      "Epoch 69, CIFAR-10 Batch 5:  trainging cost:  0.866441 accuracy:  0.725\n",
      "validation cost:  1.1818 accuracy:  0.5988\n",
      "Epoch 70, CIFAR-10 Batch 1:  trainging cost:  0.842382 accuracy:  0.675\n",
      "validation cost:  1.19498 accuracy:  0.5932\n",
      "Epoch 70, CIFAR-10 Batch 2:  trainging cost:  0.719803 accuracy:  0.8\n",
      "validation cost:  1.18501 accuracy:  0.593\n",
      "Epoch 70, CIFAR-10 Batch 3:  trainging cost:  0.803505 accuracy:  0.7\n",
      "validation cost:  1.1765 accuracy:  0.6026\n",
      "Epoch 70, CIFAR-10 Batch 4:  trainging cost:  0.851313 accuracy:  0.65\n",
      "validation cost:  1.181 accuracy:  0.5938\n",
      "Epoch 70, CIFAR-10 Batch 5:  trainging cost:  0.809912 accuracy:  0.725\n",
      "validation cost:  1.18057 accuracy:  0.5984\n",
      "Epoch 71, CIFAR-10 Batch 1:  trainging cost:  0.829813 accuracy:  0.7\n",
      "validation cost:  1.22274 accuracy:  0.5834\n",
      "Epoch 71, CIFAR-10 Batch 2:  trainging cost:  0.740505 accuracy:  0.8\n",
      "validation cost:  1.21051 accuracy:  0.5842\n",
      "Epoch 71, CIFAR-10 Batch 3:  trainging cost:  0.779731 accuracy:  0.75\n",
      "validation cost:  1.19398 accuracy:  0.5916\n",
      "Epoch 71, CIFAR-10 Batch 4:  trainging cost:  0.789966 accuracy:  0.675\n",
      "validation cost:  1.17443 accuracy:  0.6014\n",
      "Epoch 71, CIFAR-10 Batch 5:  trainging cost:  0.818712 accuracy:  0.75\n",
      "validation cost:  1.1794 accuracy:  0.596\n",
      "Epoch 72, CIFAR-10 Batch 1:  trainging cost:  0.809901 accuracy:  0.65\n",
      "validation cost:  1.20525 accuracy:  0.59\n",
      "Epoch 72, CIFAR-10 Batch 2:  trainging cost:  0.707904 accuracy:  0.8\n",
      "validation cost:  1.19049 accuracy:  0.595\n",
      "Epoch 72, CIFAR-10 Batch 3:  trainging cost:  0.8299 accuracy:  0.725\n",
      "validation cost:  1.17447 accuracy:  0.607\n",
      "Epoch 72, CIFAR-10 Batch 4:  trainging cost:  0.823652 accuracy:  0.675\n",
      "validation cost:  1.17747 accuracy:  0.599\n",
      "Epoch 72, CIFAR-10 Batch 5:  trainging cost:  0.861238 accuracy:  0.725\n",
      "validation cost:  1.18655 accuracy:  0.6004\n",
      "Epoch 73, CIFAR-10 Batch 1:  trainging cost:  0.825482 accuracy:  0.75\n",
      "validation cost:  1.20346 accuracy:  0.5994\n",
      "Epoch 73, CIFAR-10 Batch 2:  trainging cost:  0.735462 accuracy:  0.775\n",
      "validation cost:  1.22196 accuracy:  0.5802\n",
      "Epoch 73, CIFAR-10 Batch 3:  trainging cost:  0.805169 accuracy:  0.725\n",
      "validation cost:  1.19538 accuracy:  0.5908\n",
      "Epoch 73, CIFAR-10 Batch 4:  trainging cost:  0.854952 accuracy:  0.675\n",
      "validation cost:  1.2038 accuracy:  0.5836\n",
      "Epoch 73, CIFAR-10 Batch 5:  trainging cost:  0.749169 accuracy:  0.75\n",
      "validation cost:  1.17734 accuracy:  0.6038\n",
      "Epoch 74, CIFAR-10 Batch 1:  trainging cost:  0.789542 accuracy:  0.725\n",
      "validation cost:  1.20142 accuracy:  0.5904\n",
      "Epoch 74, CIFAR-10 Batch 2:  trainging cost:  0.700098 accuracy:  0.8\n",
      "validation cost:  1.19359 accuracy:  0.5904\n",
      "Epoch 74, CIFAR-10 Batch 3:  trainging cost:  0.825897 accuracy:  0.75\n",
      "validation cost:  1.19596 accuracy:  0.5902\n",
      "Epoch 74, CIFAR-10 Batch 4:  trainging cost:  0.869966 accuracy:  0.675\n",
      "validation cost:  1.20772 accuracy:  0.59\n",
      "Epoch 74, CIFAR-10 Batch 5:  trainging cost:  0.81088 accuracy:  0.675\n",
      "validation cost:  1.18462 accuracy:  0.604\n",
      "Epoch 75, CIFAR-10 Batch 1:  trainging cost:  0.800623 accuracy:  0.75\n",
      "validation cost:  1.18749 accuracy:  0.6066\n",
      "Epoch 75, CIFAR-10 Batch 2:  trainging cost:  0.730084 accuracy:  0.8\n",
      "validation cost:  1.19779 accuracy:  0.5924\n",
      "Epoch 75, CIFAR-10 Batch 3:  trainging cost:  0.800733 accuracy:  0.8\n",
      "validation cost:  1.18771 accuracy:  0.602\n",
      "Epoch 75, CIFAR-10 Batch 4:  trainging cost:  0.861843 accuracy:  0.675\n",
      "validation cost:  1.20295 accuracy:  0.5958\n",
      "Epoch 75, CIFAR-10 Batch 5:  trainging cost:  0.877807 accuracy:  0.65\n",
      "validation cost:  1.18943 accuracy:  0.5936\n",
      "Epoch 76, CIFAR-10 Batch 1:  trainging cost:  0.843864 accuracy:  0.675\n",
      "validation cost:  1.21455 accuracy:  0.5904\n",
      "Epoch 76, CIFAR-10 Batch 2:  trainging cost:  0.717008 accuracy:  0.7\n",
      "validation cost:  1.22995 accuracy:  0.5908\n",
      "Epoch 76, CIFAR-10 Batch 3:  trainging cost:  0.755925 accuracy:  0.75\n",
      "validation cost:  1.18569 accuracy:  0.5974\n",
      "Epoch 76, CIFAR-10 Batch 4:  trainging cost:  0.842582 accuracy:  0.675\n",
      "validation cost:  1.20892 accuracy:  0.5964\n",
      "Epoch 76, CIFAR-10 Batch 5:  trainging cost:  0.79927 accuracy:  0.725\n",
      "validation cost:  1.1804 accuracy:  0.5974\n",
      "Epoch 77, CIFAR-10 Batch 1:  trainging cost:  0.792724 accuracy:  0.7\n",
      "validation cost:  1.20082 accuracy:  0.5946\n",
      "Epoch 77, CIFAR-10 Batch 2:  trainging cost:  0.721858 accuracy:  0.775\n",
      "validation cost:  1.23717 accuracy:  0.581\n",
      "Epoch 77, CIFAR-10 Batch 3:  trainging cost:  0.754518 accuracy:  0.775\n",
      "validation cost:  1.18273 accuracy:  0.606\n",
      "Epoch 77, CIFAR-10 Batch 4:  trainging cost:  0.813889 accuracy:  0.7\n",
      "validation cost:  1.19325 accuracy:  0.5988\n",
      "Epoch 77, CIFAR-10 Batch 5:  trainging cost:  0.859254 accuracy:  0.675\n",
      "validation cost:  1.17847 accuracy:  0.6056\n",
      "Epoch 78, CIFAR-10 Batch 1:  trainging cost:  0.809906 accuracy:  0.725\n",
      "validation cost:  1.23262 accuracy:  0.5902\n",
      "Epoch 78, CIFAR-10 Batch 2:  trainging cost:  0.68826 accuracy:  0.8\n",
      "validation cost:  1.20634 accuracy:  0.594\n",
      "Epoch 78, CIFAR-10 Batch 3:  trainging cost:  0.774711 accuracy:  0.8\n",
      "validation cost:  1.21032 accuracy:  0.59\n",
      "Epoch 78, CIFAR-10 Batch 4:  trainging cost:  0.871078 accuracy:  0.65\n",
      "validation cost:  1.22472 accuracy:  0.589\n",
      "Epoch 78, CIFAR-10 Batch 5:  trainging cost:  0.797607 accuracy:  0.7\n",
      "validation cost:  1.18543 accuracy:  0.5996\n",
      "Epoch 79, CIFAR-10 Batch 1:  trainging cost:  0.764494 accuracy:  0.775\n",
      "validation cost:  1.1914 accuracy:  0.5988\n",
      "Epoch 79, CIFAR-10 Batch 2:  trainging cost:  0.713039 accuracy:  0.8\n",
      "validation cost:  1.2199 accuracy:  0.595\n",
      "Epoch 79, CIFAR-10 Batch 3:  trainging cost:  0.731444 accuracy:  0.8\n",
      "validation cost:  1.18513 accuracy:  0.5998\n",
      "Epoch 79, CIFAR-10 Batch 4:  trainging cost:  0.814317 accuracy:  0.675\n",
      "validation cost:  1.20938 accuracy:  0.5898\n",
      "Epoch 79, CIFAR-10 Batch 5:  trainging cost:  0.800465 accuracy:  0.675\n",
      "validation cost:  1.19683 accuracy:  0.594\n",
      "Epoch 80, CIFAR-10 Batch 1:  trainging cost:  0.793772 accuracy:  0.725\n",
      "validation cost:  1.21954 accuracy:  0.5878\n",
      "Epoch 80, CIFAR-10 Batch 2:  trainging cost:  0.760217 accuracy:  0.775\n",
      "validation cost:  1.21969 accuracy:  0.5872\n",
      "Epoch 80, CIFAR-10 Batch 3:  trainging cost:  0.74511 accuracy:  0.8\n",
      "validation cost:  1.18876 accuracy:  0.6022\n",
      "Epoch 80, CIFAR-10 Batch 4:  trainging cost:  0.875216 accuracy:  0.6\n",
      "validation cost:  1.18431 accuracy:  0.6008\n",
      "Epoch 80, CIFAR-10 Batch 5:  trainging cost:  0.751545 accuracy:  0.725\n",
      "validation cost:  1.18065 accuracy:  0.6032\n",
      "Epoch 81, CIFAR-10 Batch 1:  trainging cost:  0.774185 accuracy:  0.75\n",
      "validation cost:  1.20498 accuracy:  0.6022\n",
      "Epoch 81, CIFAR-10 Batch 2:  trainging cost:  0.691417 accuracy:  0.8\n",
      "validation cost:  1.22389 accuracy:  0.591\n",
      "Epoch 81, CIFAR-10 Batch 3:  trainging cost:  0.757333 accuracy:  0.775\n",
      "validation cost:  1.18214 accuracy:  0.6034\n",
      "Epoch 81, CIFAR-10 Batch 4:  trainging cost:  0.801431 accuracy:  0.675\n",
      "validation cost:  1.20382 accuracy:  0.5968\n",
      "Epoch 81, CIFAR-10 Batch 5:  trainging cost:  0.758822 accuracy:  0.725\n",
      "validation cost:  1.18036 accuracy:  0.6006\n",
      "Epoch 82, CIFAR-10 Batch 1:  trainging cost:  0.872035 accuracy:  0.725\n",
      "validation cost:  1.23568 accuracy:  0.5926\n",
      "Epoch 82, CIFAR-10 Batch 2:  trainging cost:  0.679328 accuracy:  0.675\n",
      "validation cost:  1.18869 accuracy:  0.6\n",
      "Epoch 82, CIFAR-10 Batch 3:  trainging cost:  0.749477 accuracy:  0.75\n",
      "validation cost:  1.19896 accuracy:  0.5988\n",
      "Epoch 82, CIFAR-10 Batch 4:  trainging cost:  0.802965 accuracy:  0.65\n",
      "validation cost:  1.22027 accuracy:  0.5904\n",
      "Epoch 82, CIFAR-10 Batch 5:  trainging cost:  0.790082 accuracy:  0.725\n",
      "validation cost:  1.20312 accuracy:  0.5984\n",
      "Epoch 83, CIFAR-10 Batch 1:  trainging cost:  0.830499 accuracy:  0.7\n",
      "validation cost:  1.21144 accuracy:  0.5952\n",
      "Epoch 83, CIFAR-10 Batch 2:  trainging cost:  0.638325 accuracy:  0.775\n",
      "validation cost:  1.20706 accuracy:  0.5856\n",
      "Epoch 83, CIFAR-10 Batch 3:  trainging cost:  0.747093 accuracy:  0.8\n",
      "validation cost:  1.20249 accuracy:  0.595\n",
      "Epoch 83, CIFAR-10 Batch 4:  trainging cost:  0.872674 accuracy:  0.625\n",
      "validation cost:  1.20084 accuracy:  0.593\n",
      "Epoch 83, CIFAR-10 Batch 5:  trainging cost:  0.779763 accuracy:  0.725\n",
      "validation cost:  1.20817 accuracy:  0.5936\n",
      "Epoch 84, CIFAR-10 Batch 1:  trainging cost:  0.796702 accuracy:  0.8\n",
      "validation cost:  1.21684 accuracy:  0.5988\n",
      "Epoch 84, CIFAR-10 Batch 2:  trainging cost:  0.710889 accuracy:  0.775\n",
      "validation cost:  1.24397 accuracy:  0.5836\n",
      "Epoch 84, CIFAR-10 Batch 3:  trainging cost:  0.795395 accuracy:  0.75\n",
      "validation cost:  1.22675 accuracy:  0.5916\n",
      "Epoch 84, CIFAR-10 Batch 4:  trainging cost:  0.821883 accuracy:  0.7\n",
      "validation cost:  1.20358 accuracy:  0.5904\n",
      "Epoch 84, CIFAR-10 Batch 5:  trainging cost:  0.781374 accuracy:  0.725\n",
      "validation cost:  1.20494 accuracy:  0.5974\n",
      "Epoch 85, CIFAR-10 Batch 1:  trainging cost:  0.807062 accuracy:  0.825\n",
      "validation cost:  1.22215 accuracy:  0.5988\n",
      "Epoch 85, CIFAR-10 Batch 2:  trainging cost:  0.705175 accuracy:  0.7\n",
      "validation cost:  1.21172 accuracy:  0.5912\n",
      "Epoch 85, CIFAR-10 Batch 3:  trainging cost:  0.753523 accuracy:  0.775\n",
      "validation cost:  1.19801 accuracy:  0.5884\n",
      "Epoch 85, CIFAR-10 Batch 4:  trainging cost:  0.850025 accuracy:  0.65\n",
      "validation cost:  1.26292 accuracy:  0.5858\n",
      "Epoch 85, CIFAR-10 Batch 5:  trainging cost:  0.799482 accuracy:  0.7\n",
      "validation cost:  1.20763 accuracy:  0.6006\n",
      "Epoch 86, CIFAR-10 Batch 1:  trainging cost:  0.771333 accuracy:  0.75\n",
      "validation cost:  1.20455 accuracy:  0.5988\n",
      "Epoch 86, CIFAR-10 Batch 2:  trainging cost:  0.722286 accuracy:  0.7\n",
      "validation cost:  1.20488 accuracy:  0.5888\n",
      "Epoch 86, CIFAR-10 Batch 3:  trainging cost:  0.78714 accuracy:  0.8\n",
      "validation cost:  1.2313 accuracy:  0.5818\n",
      "Epoch 86, CIFAR-10 Batch 4:  trainging cost:  0.876509 accuracy:  0.625\n",
      "validation cost:  1.21557 accuracy:  0.5922\n",
      "Epoch 86, CIFAR-10 Batch 5:  trainging cost:  0.757765 accuracy:  0.725\n",
      "validation cost:  1.19308 accuracy:  0.6044\n",
      "Epoch 87, CIFAR-10 Batch 1:  trainging cost:  0.737563 accuracy:  0.825\n",
      "validation cost:  1.24543 accuracy:  0.5942\n",
      "Epoch 87, CIFAR-10 Batch 2:  trainging cost:  0.638671 accuracy:  0.775\n",
      "validation cost:  1.18764 accuracy:  0.601\n",
      "Epoch 87, CIFAR-10 Batch 3:  trainging cost:  0.787188 accuracy:  0.775\n",
      "validation cost:  1.19661 accuracy:  0.6046\n",
      "Epoch 87, CIFAR-10 Batch 4:  trainging cost:  0.827062 accuracy:  0.7\n",
      "validation cost:  1.23786 accuracy:  0.5938\n",
      "Epoch 87, CIFAR-10 Batch 5:  trainging cost:  0.789304 accuracy:  0.725\n",
      "validation cost:  1.21319 accuracy:  0.5954\n",
      "Epoch 88, CIFAR-10 Batch 1:  trainging cost:  0.749302 accuracy:  0.775\n",
      "validation cost:  1.20512 accuracy:  0.6044\n",
      "Epoch 88, CIFAR-10 Batch 2:  trainging cost:  0.721912 accuracy:  0.825\n",
      "validation cost:  1.21719 accuracy:  0.5866\n",
      "Epoch 88, CIFAR-10 Batch 3:  trainging cost:  0.739587 accuracy:  0.825\n",
      "validation cost:  1.22347 accuracy:  0.5942\n",
      "Epoch 88, CIFAR-10 Batch 4:  trainging cost:  0.796264 accuracy:  0.65\n",
      "validation cost:  1.22075 accuracy:  0.6038\n",
      "Epoch 88, CIFAR-10 Batch 5:  trainging cost:  0.745579 accuracy:  0.725\n",
      "validation cost:  1.21653 accuracy:  0.592\n",
      "Epoch 89, CIFAR-10 Batch 1:  trainging cost:  0.766632 accuracy:  0.725\n",
      "validation cost:  1.23753 accuracy:  0.5902\n",
      "Epoch 89, CIFAR-10 Batch 2:  trainging cost:  0.66539 accuracy:  0.775\n",
      "validation cost:  1.21811 accuracy:  0.5912\n",
      "Epoch 89, CIFAR-10 Batch 3:  trainging cost:  0.734746 accuracy:  0.85\n",
      "validation cost:  1.22952 accuracy:  0.5956\n",
      "Epoch 89, CIFAR-10 Batch 4:  trainging cost:  0.813129 accuracy:  0.725\n",
      "validation cost:  1.26757 accuracy:  0.5854\n",
      "Epoch 89, CIFAR-10 Batch 5:  trainging cost:  0.786432 accuracy:  0.675\n",
      "validation cost:  1.24454 accuracy:  0.5928\n",
      "Epoch 90, CIFAR-10 Batch 1:  trainging cost:  0.766832 accuracy:  0.75\n",
      "validation cost:  1.25426 accuracy:  0.5938\n",
      "Epoch 90, CIFAR-10 Batch 2:  trainging cost:  0.664523 accuracy:  0.725\n",
      "validation cost:  1.21142 accuracy:  0.5946\n",
      "Epoch 90, CIFAR-10 Batch 3:  trainging cost:  0.745714 accuracy:  0.75\n",
      "validation cost:  1.2177 accuracy:  0.5964\n",
      "Epoch 90, CIFAR-10 Batch 4:  trainging cost:  0.79467 accuracy:  0.675\n",
      "validation cost:  1.21446 accuracy:  0.5928\n",
      "Epoch 90, CIFAR-10 Batch 5:  trainging cost:  0.755031 accuracy:  0.775\n",
      "validation cost:  1.20507 accuracy:  0.6014\n",
      "Epoch 91, CIFAR-10 Batch 1:  trainging cost:  0.816505 accuracy:  0.7\n",
      "validation cost:  1.254 accuracy:  0.586\n",
      "Epoch 91, CIFAR-10 Batch 2:  trainging cost:  0.688065 accuracy:  0.8\n",
      "validation cost:  1.21135 accuracy:  0.5892\n",
      "Epoch 91, CIFAR-10 Batch 3:  trainging cost:  0.683636 accuracy:  0.825\n",
      "validation cost:  1.22033 accuracy:  0.6016\n",
      "Epoch 91, CIFAR-10 Batch 4:  trainging cost:  0.821004 accuracy:  0.675\n",
      "validation cost:  1.23817 accuracy:  0.5904\n",
      "Epoch 91, CIFAR-10 Batch 5:  trainging cost:  0.767853 accuracy:  0.7\n",
      "validation cost:  1.23111 accuracy:  0.591\n",
      "Epoch 92, CIFAR-10 Batch 1:  trainging cost:  0.758212 accuracy:  0.85\n",
      "validation cost:  1.24012 accuracy:  0.5942\n",
      "Epoch 92, CIFAR-10 Batch 2:  trainging cost:  0.704154 accuracy:  0.8\n",
      "validation cost:  1.22616 accuracy:  0.5826\n",
      "Epoch 92, CIFAR-10 Batch 3:  trainging cost:  0.754073 accuracy:  0.825\n",
      "validation cost:  1.21879 accuracy:  0.5988\n",
      "Epoch 92, CIFAR-10 Batch 4:  trainging cost:  0.814414 accuracy:  0.7\n",
      "validation cost:  1.24562 accuracy:  0.585\n",
      "Epoch 92, CIFAR-10 Batch 5:  trainging cost:  0.841646 accuracy:  0.675\n",
      "validation cost:  1.251 accuracy:  0.586\n",
      "Epoch 93, CIFAR-10 Batch 1:  trainging cost:  0.789497 accuracy:  0.775\n",
      "validation cost:  1.27405 accuracy:  0.5842\n",
      "Epoch 93, CIFAR-10 Batch 2:  trainging cost:  0.737972 accuracy:  0.775\n",
      "validation cost:  1.22739 accuracy:  0.5818\n",
      "Epoch 93, CIFAR-10 Batch 3:  trainging cost:  0.723164 accuracy:  0.8\n",
      "validation cost:  1.22252 accuracy:  0.5966\n",
      "Epoch 93, CIFAR-10 Batch 4:  trainging cost:  0.866315 accuracy:  0.7\n",
      "validation cost:  1.23951 accuracy:  0.5934\n",
      "Epoch 93, CIFAR-10 Batch 5:  trainging cost:  0.729657 accuracy:  0.725\n",
      "validation cost:  1.21575 accuracy:  0.596\n",
      "Epoch 94, CIFAR-10 Batch 1:  trainging cost:  0.799149 accuracy:  0.775\n",
      "validation cost:  1.27008 accuracy:  0.6008\n",
      "Epoch 94, CIFAR-10 Batch 2:  trainging cost:  0.746743 accuracy:  0.8\n",
      "validation cost:  1.23453 accuracy:  0.5844\n",
      "Epoch 94, CIFAR-10 Batch 3:  trainging cost:  0.728649 accuracy:  0.8\n",
      "validation cost:  1.19557 accuracy:  0.6066\n",
      "Epoch 94, CIFAR-10 Batch 4:  trainging cost:  0.782496 accuracy:  0.7\n",
      "validation cost:  1.22125 accuracy:  0.6014\n",
      "Epoch 94, CIFAR-10 Batch 5:  trainging cost:  0.773681 accuracy:  0.7\n",
      "validation cost:  1.21288 accuracy:  0.5896\n",
      "Epoch 95, CIFAR-10 Batch 1:  trainging cost:  0.763081 accuracy:  0.725\n",
      "validation cost:  1.23792 accuracy:  0.599\n",
      "Epoch 95, CIFAR-10 Batch 2:  trainging cost:  0.674773 accuracy:  0.775\n",
      "validation cost:  1.20857 accuracy:  0.5982\n",
      "Epoch 95, CIFAR-10 Batch 3:  trainging cost:  0.707894 accuracy:  0.8\n",
      "validation cost:  1.22282 accuracy:  0.601\n",
      "Epoch 95, CIFAR-10 Batch 4:  trainging cost:  0.809844 accuracy:  0.65\n",
      "validation cost:  1.21869 accuracy:  0.6022\n",
      "Epoch 95, CIFAR-10 Batch 5:  trainging cost:  0.761903 accuracy:  0.725\n",
      "validation cost:  1.20913 accuracy:  0.598\n",
      "Epoch 96, CIFAR-10 Batch 1:  trainging cost:  0.781684 accuracy:  0.725\n",
      "validation cost:  1.25546 accuracy:  0.5808\n",
      "Epoch 96, CIFAR-10 Batch 2:  trainging cost:  0.673242 accuracy:  0.775\n",
      "validation cost:  1.23114 accuracy:  0.5886\n",
      "Epoch 96, CIFAR-10 Batch 3:  trainging cost:  0.747541 accuracy:  0.725\n",
      "validation cost:  1.22328 accuracy:  0.5998\n",
      "Epoch 96, CIFAR-10 Batch 4:  trainging cost:  0.773547 accuracy:  0.65\n",
      "validation cost:  1.24209 accuracy:  0.5858\n",
      "Epoch 96, CIFAR-10 Batch 5:  trainging cost:  0.748602 accuracy:  0.75\n",
      "validation cost:  1.21818 accuracy:  0.602\n",
      "Epoch 97, CIFAR-10 Batch 1:  trainging cost:  0.832877 accuracy:  0.75\n",
      "validation cost:  1.26324 accuracy:  0.5802\n",
      "Epoch 97, CIFAR-10 Batch 2:  trainging cost:  0.683202 accuracy:  0.775\n",
      "validation cost:  1.21049 accuracy:  0.5908\n",
      "Epoch 97, CIFAR-10 Batch 3:  trainging cost:  0.734482 accuracy:  0.75\n",
      "validation cost:  1.2346 accuracy:  0.6042\n",
      "Epoch 97, CIFAR-10 Batch 4:  trainging cost:  0.815612 accuracy:  0.675\n",
      "validation cost:  1.25928 accuracy:  0.5866\n",
      "Epoch 97, CIFAR-10 Batch 5:  trainging cost:  0.732404 accuracy:  0.725\n",
      "validation cost:  1.21315 accuracy:  0.595\n",
      "Epoch 98, CIFAR-10 Batch 1:  trainging cost:  0.720119 accuracy:  0.8\n",
      "validation cost:  1.28035 accuracy:  0.589\n",
      "Epoch 98, CIFAR-10 Batch 2:  trainging cost:  0.682177 accuracy:  0.75\n",
      "validation cost:  1.22244 accuracy:  0.5856\n",
      "Epoch 98, CIFAR-10 Batch 3:  trainging cost:  0.695109 accuracy:  0.85\n",
      "validation cost:  1.22653 accuracy:  0.595\n",
      "Epoch 98, CIFAR-10 Batch 4:  trainging cost:  0.792917 accuracy:  0.625\n",
      "validation cost:  1.22991 accuracy:  0.5894\n",
      "Epoch 98, CIFAR-10 Batch 5:  trainging cost:  0.746741 accuracy:  0.75\n",
      "validation cost:  1.21264 accuracy:  0.5892\n",
      "Epoch 99, CIFAR-10 Batch 1:  trainging cost:  0.753435 accuracy:  0.75\n",
      "validation cost:  1.23215 accuracy:  0.5968\n",
      "Epoch 99, CIFAR-10 Batch 2:  trainging cost:  0.612695 accuracy:  0.725\n",
      "validation cost:  1.2221 accuracy:  0.5898\n",
      "Epoch 99, CIFAR-10 Batch 3:  trainging cost:  0.732597 accuracy:  0.8\n",
      "validation cost:  1.22356 accuracy:  0.5916\n",
      "Epoch 99, CIFAR-10 Batch 4:  trainging cost:  0.780251 accuracy:  0.65\n",
      "validation cost:  1.23318 accuracy:  0.5958\n",
      "Epoch 99, CIFAR-10 Batch 5:  trainging cost:  0.793001 accuracy:  0.7\n",
      "validation cost:  1.24133 accuracy:  0.578\n",
      "Epoch 100, CIFAR-10 Batch 1:  trainging cost:  0.775081 accuracy:  0.775\n",
      "validation cost:  1.26092 accuracy:  0.5892\n",
      "Epoch 100, CIFAR-10 Batch 2:  trainging cost:  0.636532 accuracy:  0.775\n",
      "validation cost:  1.21455 accuracy:  0.5894\n",
      "Epoch 100, CIFAR-10 Batch 3:  trainging cost:  0.715839 accuracy:  0.8\n",
      "validation cost:  1.24248 accuracy:  0.5908\n",
      "Epoch 100, CIFAR-10 Batch 4:  trainging cost:  0.833197 accuracy:  0.675\n",
      "validation cost:  1.24972 accuracy:  0.5972\n",
      "Epoch 100, CIFAR-10 Batch 5:  trainging cost:  0.729741 accuracy:  0.7\n",
      "validation cost:  1.21786 accuracy:  0.5984\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5022963258785943\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1177231d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
