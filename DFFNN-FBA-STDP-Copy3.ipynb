{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.relu_forward(X=y)\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.relu_forward(X=y)\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1086\n",
      "Iter-20 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1150\n",
      "Iter-30 train loss: 2.3026 valid loss: 2.3026, valid accuracy: 0.1048\n",
      "Iter-40 train loss: 2.3027 valid loss: 2.3026, valid accuracy: 0.1158\n",
      "Iter-50 train loss: 2.3025 valid loss: 2.3026, valid accuracy: 0.1194\n",
      "Iter-60 train loss: 2.3024 valid loss: 2.3026, valid accuracy: 0.1166\n",
      "Iter-70 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1154\n",
      "Iter-80 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1144\n",
      "Iter-90 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1138\n",
      "Iter-100 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1130\n",
      "Iter-110 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1128\n",
      "Iter-120 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-130 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-140 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-150 train loss: 2.3023 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-160 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-170 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-180 train loss: 2.3025 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-190 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-200 train loss: 2.3024 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-210 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-220 train loss: 2.3027 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-230 train loss: 2.3022 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-240 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-250 train loss: 2.3020 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-260 train loss: 2.3022 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-270 train loss: 2.3029 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-280 train loss: 2.3028 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-290 train loss: 2.3026 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-300 train loss: 2.3021 valid loss: 2.3025, valid accuracy: 0.1126\n",
      "Iter-310 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-320 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-330 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-340 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-350 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-360 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-370 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-380 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-390 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-400 train loss: 2.3018 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-410 train loss: 2.3032 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-420 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-430 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-440 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-450 train loss: 2.3020 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-460 train loss: 2.3031 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-470 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-480 train loss: 2.3018 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-490 train loss: 2.3014 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-500 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-510 train loss: 2.3023 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-520 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-530 train loss: 2.3026 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-540 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-550 train loss: 2.3030 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-560 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-570 train loss: 2.3027 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-580 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-590 train loss: 2.3013 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-600 train loss: 2.3028 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-610 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-620 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-630 train loss: 2.3024 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-640 train loss: 2.3028 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-650 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-660 train loss: 2.3020 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-670 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-680 train loss: 2.3025 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-690 train loss: 2.3015 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-700 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-710 train loss: 2.3022 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-720 train loss: 2.3019 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-730 train loss: 2.3029 valid loss: 2.3024, valid accuracy: 0.1126\n",
      "Iter-740 train loss: 2.3015 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-750 train loss: 2.3025 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-760 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-770 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-780 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-790 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-800 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-810 train loss: 2.3024 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-820 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-830 train loss: 2.3022 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-840 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-850 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-860 train loss: 2.3034 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-870 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-880 train loss: 2.3029 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-890 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-900 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-910 train loss: 2.3008 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-920 train loss: 2.3030 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-930 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-940 train loss: 2.3030 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-950 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-960 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-970 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-980 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-990 train loss: 2.3010 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1000 train loss: 2.3029 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1010 train loss: 2.3018 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1020 train loss: 2.3014 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1030 train loss: 2.3007 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1040 train loss: 2.3041 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1050 train loss: 2.3010 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1060 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1070 train loss: 2.3016 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1080 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1090 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1100 train loss: 2.3015 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1110 train loss: 2.3022 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1120 train loss: 2.3028 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1130 train loss: 2.3027 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1140 train loss: 2.3015 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1150 train loss: 2.3026 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1160 train loss: 2.3032 valid loss: 2.3023, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.3021 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1180 train loss: 2.3031 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1190 train loss: 2.3012 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.3019 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1210 train loss: 2.3022 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1220 train loss: 2.3009 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1230 train loss: 2.3023 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1240 train loss: 2.3020 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1250 train loss: 2.3012 valid loss: 2.3023, valid accuracy: 0.1126\n",
      "Iter-1260 train loss: 2.3013 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1270 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1280 train loss: 2.3033 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1290 train loss: 2.3013 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.3025 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1310 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1320 train loss: 2.3024 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1330 train loss: 2.3022 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1340 train loss: 2.3012 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1350 train loss: 2.3028 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1360 train loss: 2.3012 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1370 train loss: 2.3018 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1380 train loss: 2.3009 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1390 train loss: 2.3019 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1410 train loss: 2.3038 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1420 train loss: 2.3013 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1430 train loss: 2.3030 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1440 train loss: 2.3028 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1450 train loss: 2.3009 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1460 train loss: 2.3030 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1470 train loss: 2.3000 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1480 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1490 train loss: 2.3010 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.3020 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1510 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1520 train loss: 2.3032 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1530 train loss: 2.3029 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1540 train loss: 2.3028 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1550 train loss: 2.3023 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1560 train loss: 2.3026 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1570 train loss: 2.3031 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1580 train loss: 2.3021 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1590 train loss: 2.3031 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3015 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1610 train loss: 2.3013 valid loss: 2.3022, valid accuracy: 0.1126\n",
      "Iter-1620 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1630 train loss: 2.3021 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1640 train loss: 2.3027 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1650 train loss: 2.3017 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1660 train loss: 2.3004 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1670 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1680 train loss: 2.3005 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1690 train loss: 2.3036 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1710 train loss: 2.3006 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1720 train loss: 2.3013 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1730 train loss: 2.3031 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1740 train loss: 2.3038 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1750 train loss: 2.2993 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1760 train loss: 2.3009 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1770 train loss: 2.3031 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1780 train loss: 2.3005 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1790 train loss: 2.3004 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3039 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1810 train loss: 2.3018 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1820 train loss: 2.3026 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1830 train loss: 2.3027 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1840 train loss: 2.3002 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1850 train loss: 2.2998 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1860 train loss: 2.3016 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1870 train loss: 2.3017 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1880 train loss: 2.3019 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1890 train loss: 2.3008 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3003 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1910 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1920 train loss: 2.3009 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1930 train loss: 2.3036 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1940 train loss: 2.3014 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1950 train loss: 2.3025 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1960 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1970 train loss: 2.3038 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1980 train loss: 2.3045 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-1990 train loss: 2.3014 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3027 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2010 train loss: 2.3011 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2020 train loss: 2.3006 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2030 train loss: 2.3019 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2040 train loss: 2.3015 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2050 train loss: 2.3031 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2060 train loss: 2.3024 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2070 train loss: 2.3026 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2080 train loss: 2.3046 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2090 train loss: 2.3031 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3026 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2110 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2120 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2130 train loss: 2.3030 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2140 train loss: 2.3006 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2150 train loss: 2.3023 valid loss: 2.3021, valid accuracy: 0.1126\n",
      "Iter-2160 train loss: 2.3018 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2170 train loss: 2.3035 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2180 train loss: 2.3029 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2190 train loss: 2.3011 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3003 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2210 train loss: 2.3026 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2220 train loss: 2.3013 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2230 train loss: 2.3042 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2240 train loss: 2.3038 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2250 train loss: 2.3034 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2260 train loss: 2.3014 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2270 train loss: 2.3004 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2280 train loss: 2.3037 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2290 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3010 valid loss: 2.3020, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3046 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2320 train loss: 2.3002 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2330 train loss: 2.3037 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2340 train loss: 2.3025 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2350 train loss: 2.3058 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2360 train loss: 2.3022 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2370 train loss: 2.3020 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2380 train loss: 2.3013 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2390 train loss: 2.3030 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.3047 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2410 train loss: 2.3028 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2420 train loss: 2.3046 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2430 train loss: 2.3045 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2440 train loss: 2.3052 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2450 train loss: 2.3003 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2460 train loss: 2.3003 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2470 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2480 train loss: 2.2999 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2490 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3022 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2510 train loss: 2.3013 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2520 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2530 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2540 train loss: 2.3024 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2550 train loss: 2.3048 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2560 train loss: 2.3028 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2570 train loss: 2.3033 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2580 train loss: 2.3015 valid loss: 2.3020, valid accuracy: 0.1126\n",
      "Iter-2590 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.2994 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2610 train loss: 2.3056 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2620 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2630 train loss: 2.3050 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2640 train loss: 2.3004 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2650 train loss: 2.3042 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2660 train loss: 2.3027 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2670 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2680 train loss: 2.3019 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2690 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.3044 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2710 train loss: 2.3016 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2720 train loss: 2.3012 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2730 train loss: 2.3009 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2740 train loss: 2.3039 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2750 train loss: 2.3023 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2760 train loss: 2.3007 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2770 train loss: 2.3021 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2780 train loss: 2.3001 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2790 train loss: 2.2995 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.3011 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2810 train loss: 2.3041 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2820 train loss: 2.3031 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2830 train loss: 2.3018 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2840 train loss: 2.3013 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2850 train loss: 2.3050 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2860 train loss: 2.3064 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2870 train loss: 2.3008 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2880 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2890 train loss: 2.3018 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.3039 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2910 train loss: 2.3021 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2920 train loss: 2.3034 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2930 train loss: 2.3021 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2940 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2950 train loss: 2.3036 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2960 train loss: 2.3013 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2970 train loss: 2.3047 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2980 train loss: 2.3044 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-2990 train loss: 2.3012 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.2995 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3010 train loss: 2.2979 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3020 train loss: 2.3006 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3030 train loss: 2.3025 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3040 train loss: 2.3014 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3050 train loss: 2.3048 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3060 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3070 train loss: 2.2989 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3080 train loss: 2.3050 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3090 train loss: 2.3028 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3005 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3110 train loss: 2.3008 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3120 train loss: 2.3032 valid loss: 2.3019, valid accuracy: 0.1126\n",
      "Iter-3130 train loss: 2.3056 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3140 train loss: 2.3022 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3150 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3160 train loss: 2.3057 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3170 train loss: 2.3011 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3180 train loss: 2.3032 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3190 train loss: 2.3034 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3210 train loss: 2.3039 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3220 train loss: 2.2981 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3230 train loss: 2.3060 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3240 train loss: 2.3020 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3250 train loss: 2.3032 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3260 train loss: 2.3061 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3270 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3280 train loss: 2.3034 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3290 train loss: 2.2978 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.2995 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3310 train loss: 2.3028 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3320 train loss: 2.2986 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3330 train loss: 2.3013 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3340 train loss: 2.2991 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3350 train loss: 2.3050 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3360 train loss: 2.3008 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3370 train loss: 2.3015 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3380 train loss: 2.3040 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3390 train loss: 2.2996 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.3043 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3410 train loss: 2.3015 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3420 train loss: 2.2997 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3430 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3440 train loss: 2.3069 valid loss: 2.3018, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3037 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3460 train loss: 2.3042 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3470 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3480 train loss: 2.3027 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3490 train loss: 2.3040 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3510 train loss: 2.3020 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3520 train loss: 2.3014 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3530 train loss: 2.3016 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3540 train loss: 2.2989 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3550 train loss: 2.3012 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3560 train loss: 2.3022 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3570 train loss: 2.3023 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3580 train loss: 2.3012 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3590 train loss: 2.3057 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3610 train loss: 2.2993 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3620 train loss: 2.3012 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3630 train loss: 2.3005 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3640 train loss: 2.3001 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3650 train loss: 2.3012 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3660 train loss: 2.3020 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3670 train loss: 2.3003 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3680 train loss: 2.3018 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3690 train loss: 2.3007 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3049 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3710 train loss: 2.3033 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3720 train loss: 2.3031 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3730 train loss: 2.3001 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3740 train loss: 2.3015 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3750 train loss: 2.2996 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3760 train loss: 2.3022 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-3770 train loss: 2.3001 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3780 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3790 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3810 train loss: 2.3067 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3820 train loss: 2.3021 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3830 train loss: 2.3021 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3840 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3850 train loss: 2.3001 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3860 train loss: 2.3012 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3870 train loss: 2.3037 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3880 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3890 train loss: 2.3006 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3065 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3910 train loss: 2.3035 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3920 train loss: 2.3062 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3930 train loss: 2.3011 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3940 train loss: 2.3014 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3950 train loss: 2.3003 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3960 train loss: 2.2942 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3970 train loss: 2.2991 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3980 train loss: 2.3000 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-3990 train loss: 2.2996 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3051 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4010 train loss: 2.2999 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4020 train loss: 2.3004 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4030 train loss: 2.3008 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4040 train loss: 2.2987 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4050 train loss: 2.3024 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4060 train loss: 2.3013 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4070 train loss: 2.2975 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4080 train loss: 2.3004 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4090 train loss: 2.3014 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4110 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4120 train loss: 2.3043 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4130 train loss: 2.2971 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4140 train loss: 2.3001 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4150 train loss: 2.3029 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4160 train loss: 2.3032 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4170 train loss: 2.3031 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4180 train loss: 2.3013 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4190 train loss: 2.3011 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.2983 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4210 train loss: 2.3048 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4220 train loss: 2.3011 valid loss: 2.3017, valid accuracy: 0.1126\n",
      "Iter-4230 train loss: 2.3015 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4240 train loss: 2.3023 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4250 train loss: 2.2980 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4260 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4270 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4280 train loss: 2.3002 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4290 train loss: 2.3021 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.3100 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4310 train loss: 2.2986 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4320 train loss: 2.3056 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4330 train loss: 2.3000 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4340 train loss: 2.3009 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4350 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4360 train loss: 2.2969 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4370 train loss: 2.3024 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4380 train loss: 2.2973 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4390 train loss: 2.3027 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.2955 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4410 train loss: 2.3045 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4420 train loss: 2.3053 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4430 train loss: 2.3033 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4440 train loss: 2.3078 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4450 train loss: 2.3024 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4460 train loss: 2.3006 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4470 train loss: 2.3008 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4480 train loss: 2.3019 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4490 train loss: 2.2995 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.3011 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4510 train loss: 2.3055 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4520 train loss: 2.3022 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4530 train loss: 2.3031 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4540 train loss: 2.3018 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4550 train loss: 2.2991 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4560 train loss: 2.3060 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4570 train loss: 2.3065 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4580 train loss: 2.3047 valid loss: 2.3016, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.3068 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.2960 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4610 train loss: 2.3038 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4620 train loss: 2.3100 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4630 train loss: 2.3028 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4640 train loss: 2.3020 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4650 train loss: 2.3013 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4660 train loss: 2.3065 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4670 train loss: 2.2994 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4680 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4690 train loss: 2.2988 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.3027 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4710 train loss: 2.3034 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4720 train loss: 2.3074 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4730 train loss: 2.3064 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4740 train loss: 2.2994 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4750 train loss: 2.3063 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4760 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4770 train loss: 2.2989 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4780 train loss: 2.3001 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4790 train loss: 2.3014 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.3043 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4810 train loss: 2.3025 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4820 train loss: 2.3066 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4830 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4840 train loss: 2.2998 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4850 train loss: 2.3015 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4860 train loss: 2.3050 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4870 train loss: 2.2961 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4880 train loss: 2.3038 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4890 train loss: 2.2993 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.2996 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4910 train loss: 2.2952 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4920 train loss: 2.3050 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4930 train loss: 2.3026 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4940 train loss: 2.2995 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4950 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4960 train loss: 2.3014 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4970 train loss: 2.3024 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4980 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-4990 train loss: 2.3003 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3007 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5010 train loss: 2.2978 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5020 train loss: 2.2968 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5030 train loss: 2.3010 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5040 train loss: 2.3027 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5050 train loss: 2.2994 valid loss: 2.3016, valid accuracy: 0.1126\n",
      "Iter-5060 train loss: 2.2985 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5070 train loss: 2.3020 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5080 train loss: 2.3032 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5090 train loss: 2.3068 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3032 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5110 train loss: 2.3035 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5120 train loss: 2.2999 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5130 train loss: 2.3031 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5140 train loss: 2.3093 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5150 train loss: 2.3005 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5160 train loss: 2.3032 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5170 train loss: 2.2996 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5180 train loss: 2.3021 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5190 train loss: 2.2993 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.2971 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5210 train loss: 2.3047 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5220 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5230 train loss: 2.2981 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5240 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5250 train loss: 2.3018 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5260 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5270 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5280 train loss: 2.2996 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5290 train loss: 2.3033 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.2979 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5310 train loss: 2.3010 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5320 train loss: 2.2995 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5330 train loss: 2.2960 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5340 train loss: 2.2987 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5350 train loss: 2.3023 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5360 train loss: 2.2950 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5370 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5380 train loss: 2.3016 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5390 train loss: 2.3021 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3027 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5410 train loss: 2.3021 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5420 train loss: 2.3052 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5430 train loss: 2.3017 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5440 train loss: 2.2976 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5450 train loss: 2.3005 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5460 train loss: 2.3038 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5470 train loss: 2.3019 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5480 train loss: 2.3063 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5490 train loss: 2.3026 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3074 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5510 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5520 train loss: 2.3013 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5530 train loss: 2.2999 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5540 train loss: 2.3009 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5550 train loss: 2.2977 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5560 train loss: 2.2989 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5570 train loss: 2.2975 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5580 train loss: 2.3059 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5590 train loss: 2.3004 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5610 train loss: 2.2962 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5620 train loss: 2.3000 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5630 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5640 train loss: 2.3061 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5650 train loss: 2.3029 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5660 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5670 train loss: 2.2940 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5680 train loss: 2.3091 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5690 train loss: 2.3046 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.3052 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5710 train loss: 2.3030 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5720 train loss: 2.3075 valid loss: 2.3015, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.2957 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5740 train loss: 2.3041 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5750 train loss: 2.2979 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5760 train loss: 2.2969 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5770 train loss: 2.2930 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5780 train loss: 2.3003 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5790 train loss: 2.3002 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3067 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5810 train loss: 2.3044 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5820 train loss: 2.2984 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5830 train loss: 2.2999 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5840 train loss: 2.3004 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5850 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5860 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5870 train loss: 2.3013 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5880 train loss: 2.3008 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5890 train loss: 2.3066 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.3033 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5910 train loss: 2.3034 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5920 train loss: 2.2975 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5930 train loss: 2.3035 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5940 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5950 train loss: 2.3096 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5960 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5970 train loss: 2.2965 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5980 train loss: 2.2984 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-5990 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6010 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6020 train loss: 2.2998 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6030 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6040 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6050 train loss: 2.3025 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6060 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6070 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6080 train loss: 2.3024 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6090 train loss: 2.3018 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3079 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6110 train loss: 2.2944 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6120 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6130 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6140 train loss: 2.2960 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6150 train loss: 2.2995 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6160 train loss: 2.3061 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6170 train loss: 2.2994 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6180 train loss: 2.3072 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6190 train loss: 2.2980 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6210 train loss: 2.2976 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6220 train loss: 2.3036 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6230 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6240 train loss: 2.3022 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6250 train loss: 2.3029 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6260 train loss: 2.3016 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6270 train loss: 2.2974 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6280 train loss: 2.2930 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6290 train loss: 2.3019 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6310 train loss: 2.2991 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6320 train loss: 2.3039 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6330 train loss: 2.2964 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6340 train loss: 2.3068 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6350 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6360 train loss: 2.3027 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6370 train loss: 2.2991 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6380 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6390 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.3069 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6410 train loss: 2.2966 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6420 train loss: 2.2966 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6430 train loss: 2.3021 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6440 train loss: 2.2980 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6450 train loss: 2.2992 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6460 train loss: 2.2989 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6470 train loss: 2.2967 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6480 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6490 train loss: 2.3001 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6510 train loss: 2.2979 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6520 train loss: 2.2978 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6530 train loss: 2.2975 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6540 train loss: 2.3031 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6550 train loss: 2.3064 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6560 train loss: 2.3070 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6570 train loss: 2.3082 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6580 train loss: 2.3028 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6590 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.3030 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6610 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6620 train loss: 2.2982 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6630 train loss: 2.3056 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6640 train loss: 2.3038 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6650 train loss: 2.2998 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6660 train loss: 2.2965 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6670 train loss: 2.2962 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6680 train loss: 2.3025 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6690 train loss: 2.3001 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.3018 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6710 train loss: 2.3033 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6720 train loss: 2.2958 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6730 train loss: 2.3003 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6740 train loss: 2.3015 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6750 train loss: 2.2977 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6760 train loss: 2.3067 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6770 train loss: 2.2986 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6780 train loss: 2.2979 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6790 train loss: 2.3048 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.3046 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6810 train loss: 2.3047 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6820 train loss: 2.3018 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6830 train loss: 2.3025 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6840 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6850 train loss: 2.2992 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6860 train loss: 2.3030 valid loss: 2.3014, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.3045 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6880 train loss: 2.3000 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6890 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.3081 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6910 train loss: 2.3055 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6920 train loss: 2.2995 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6930 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6940 train loss: 2.3019 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-6950 train loss: 2.2969 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6960 train loss: 2.3034 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6970 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6980 train loss: 2.2943 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-6990 train loss: 2.3027 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7010 train loss: 2.3019 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7020 train loss: 2.3044 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7030 train loss: 2.2990 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7040 train loss: 2.3131 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7050 train loss: 2.3041 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7060 train loss: 2.3022 valid loss: 2.3014, valid accuracy: 0.1126\n",
      "Iter-7070 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7080 train loss: 2.2970 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7090 train loss: 2.3012 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.2937 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7110 train loss: 2.3051 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7120 train loss: 2.2967 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7130 train loss: 2.3026 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7140 train loss: 2.3008 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7150 train loss: 2.3000 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7160 train loss: 2.2970 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7170 train loss: 2.3012 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7180 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7190 train loss: 2.2956 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7210 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7220 train loss: 2.2991 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7230 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7240 train loss: 2.3061 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7250 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7260 train loss: 2.2999 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7270 train loss: 2.3064 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7280 train loss: 2.2949 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7290 train loss: 2.3006 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.3038 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7310 train loss: 2.3013 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7320 train loss: 2.3046 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7330 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7340 train loss: 2.3034 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7350 train loss: 2.3015 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7360 train loss: 2.3043 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7370 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7380 train loss: 2.3041 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7390 train loss: 2.3066 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.2999 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7410 train loss: 2.3003 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7420 train loss: 2.3073 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7430 train loss: 2.2963 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7440 train loss: 2.3033 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7450 train loss: 2.3022 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7460 train loss: 2.2960 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7470 train loss: 2.3030 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7480 train loss: 2.3056 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7490 train loss: 2.3049 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.3069 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7510 train loss: 2.2938 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7520 train loss: 2.2979 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7530 train loss: 2.3082 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7540 train loss: 2.2937 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7550 train loss: 2.2985 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7560 train loss: 2.3069 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7570 train loss: 2.2995 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7580 train loss: 2.3028 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7590 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.3039 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7610 train loss: 2.2997 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7620 train loss: 2.3036 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7630 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7640 train loss: 2.3042 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7650 train loss: 2.3050 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7660 train loss: 2.3059 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7670 train loss: 2.3071 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7680 train loss: 2.2949 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7690 train loss: 2.3020 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.3062 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7710 train loss: 2.3080 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7720 train loss: 2.3053 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7730 train loss: 2.3089 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7740 train loss: 2.3099 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7750 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7760 train loss: 2.2983 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7770 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7780 train loss: 2.3048 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7790 train loss: 2.3048 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3029 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7810 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7820 train loss: 2.3007 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7830 train loss: 2.3057 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7840 train loss: 2.3012 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7850 train loss: 2.3005 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7860 train loss: 2.2990 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7870 train loss: 2.3034 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7880 train loss: 2.3043 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7890 train loss: 2.2986 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.2966 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7910 train loss: 2.2968 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7920 train loss: 2.3013 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7930 train loss: 2.2980 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7940 train loss: 2.3021 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7950 train loss: 2.3060 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7960 train loss: 2.2998 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7970 train loss: 2.3076 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7980 train loss: 2.2962 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-7990 train loss: 2.2947 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.3010 valid loss: 2.3013, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.2996 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8020 train loss: 2.2990 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8030 train loss: 2.3022 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8040 train loss: 2.2990 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8050 train loss: 2.2919 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8060 train loss: 2.3023 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8070 train loss: 2.2967 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8080 train loss: 2.3083 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8090 train loss: 2.3047 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3005 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8110 train loss: 2.3064 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8120 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8130 train loss: 2.3008 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8140 train loss: 2.2927 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-8150 train loss: 2.2978 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8160 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8170 train loss: 2.3051 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8180 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8190 train loss: 2.3043 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.2932 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8210 train loss: 2.2982 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8220 train loss: 2.2963 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8230 train loss: 2.3036 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8240 train loss: 2.3004 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8250 train loss: 2.3029 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8260 train loss: 2.2983 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8270 train loss: 2.3007 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8280 train loss: 2.2959 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8290 train loss: 2.3058 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.2997 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8310 train loss: 2.3008 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8320 train loss: 2.2993 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8330 train loss: 2.3015 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8340 train loss: 2.3005 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8350 train loss: 2.2975 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8360 train loss: 2.2972 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8370 train loss: 2.3013 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8380 train loss: 2.3008 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8390 train loss: 2.2971 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.2977 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8410 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8420 train loss: 2.3067 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8430 train loss: 2.3027 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8440 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8450 train loss: 2.2948 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8460 train loss: 2.3058 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8470 train loss: 2.3015 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8480 train loss: 2.3049 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8490 train loss: 2.3056 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8510 train loss: 2.2976 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8520 train loss: 2.2957 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8530 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8540 train loss: 2.3080 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8550 train loss: 2.3044 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8560 train loss: 2.2958 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8570 train loss: 2.3030 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8580 train loss: 2.2976 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8590 train loss: 2.3013 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.2990 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8610 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8620 train loss: 2.3026 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8630 train loss: 2.2955 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8640 train loss: 2.2963 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8650 train loss: 2.3042 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8660 train loss: 2.2984 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8670 train loss: 2.2997 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8680 train loss: 2.3044 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8690 train loss: 2.3037 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.2988 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8710 train loss: 2.2910 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8720 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8730 train loss: 2.2926 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8740 train loss: 2.3072 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8750 train loss: 2.2953 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8760 train loss: 2.3019 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8770 train loss: 2.3099 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8780 train loss: 2.2962 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8790 train loss: 2.3009 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.3007 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8810 train loss: 2.3075 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8820 train loss: 2.2952 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8830 train loss: 2.3033 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8840 train loss: 2.3000 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8850 train loss: 2.2988 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8860 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8870 train loss: 2.2962 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8880 train loss: 2.3007 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8890 train loss: 2.2983 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.3053 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8910 train loss: 2.3037 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8920 train loss: 2.3053 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8930 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8940 train loss: 2.2995 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8950 train loss: 2.2973 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8960 train loss: 2.3057 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8970 train loss: 2.2999 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8980 train loss: 2.3057 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-8990 train loss: 2.2994 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.3019 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9010 train loss: 2.3076 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9020 train loss: 2.3061 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9030 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9040 train loss: 2.3069 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9050 train loss: 2.2987 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9060 train loss: 2.2978 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9070 train loss: 2.3063 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9080 train loss: 2.3050 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9090 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.3019 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9110 train loss: 2.3017 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9120 train loss: 2.2964 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9130 train loss: 2.3050 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9140 train loss: 2.3068 valid loss: 2.3012, valid accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.3007 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9160 train loss: 2.3011 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9170 train loss: 2.3022 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9180 train loss: 2.2976 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9190 train loss: 2.3026 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.3047 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9210 train loss: 2.2970 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9220 train loss: 2.3096 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9230 train loss: 2.3037 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9240 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9250 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9260 train loss: 2.2994 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9270 train loss: 2.2998 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9280 train loss: 2.2966 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9290 train loss: 2.3019 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.2951 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9310 train loss: 2.3016 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9320 train loss: 2.3031 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9330 train loss: 2.3042 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9340 train loss: 2.3090 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9350 train loss: 2.3073 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9360 train loss: 2.2997 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9370 train loss: 2.2976 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9380 train loss: 2.2978 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9390 train loss: 2.3025 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2957 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9410 train loss: 2.3023 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9420 train loss: 2.3002 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9430 train loss: 2.2906 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9440 train loss: 2.2965 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9450 train loss: 2.3021 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9460 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9470 train loss: 2.3034 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9480 train loss: 2.3042 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9490 train loss: 2.2887 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.2989 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9510 train loss: 2.3078 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9520 train loss: 2.2981 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9530 train loss: 2.3089 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9540 train loss: 2.3077 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9550 train loss: 2.2986 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9560 train loss: 2.3035 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9570 train loss: 2.3003 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9580 train loss: 2.2963 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9590 train loss: 2.3015 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.2990 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9610 train loss: 2.3061 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9620 train loss: 2.2968 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9630 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9640 train loss: 2.2968 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9650 train loss: 2.3053 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9660 train loss: 2.3093 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9670 train loss: 2.2977 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9680 train loss: 2.3026 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9690 train loss: 2.3018 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.2888 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9710 train loss: 2.3006 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9720 train loss: 2.3018 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9730 train loss: 2.2961 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9740 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9750 train loss: 2.3022 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9760 train loss: 2.3062 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9770 train loss: 2.3017 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9780 train loss: 2.2992 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9790 train loss: 2.2957 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.3069 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9810 train loss: 2.2951 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9820 train loss: 2.2987 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9830 train loss: 2.2916 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9840 train loss: 2.2963 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9850 train loss: 2.3086 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9860 train loss: 2.2968 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9870 train loss: 2.2972 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9880 train loss: 2.2970 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9890 train loss: 2.3097 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3049 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9910 train loss: 2.3059 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9920 train loss: 2.3091 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9930 train loss: 2.3028 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9940 train loss: 2.2969 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9950 train loss: 2.3012 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9960 train loss: 2.3030 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9970 train loss: 2.3052 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9980 train loss: 2.3055 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-9990 train loss: 2.3050 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.3051 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3013\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXYFugQEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120e2af60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120e260f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
