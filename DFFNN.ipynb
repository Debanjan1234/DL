{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "#         self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "#         self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m = dict(W=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "                 b=np.zeros((1, H)))\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, h = cache\n",
    "\n",
    "        dW = h.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "#         print('db.shape', db.shape)\n",
    "        dX = dout @ W.T # Backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = selu_forward(X=y)\n",
    "        X = y.copy() # pass the previous output to the next layer\n",
    "        caches.append((fc_cache, nl_cache)) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = selu_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            nl_caches.append(nl_cache)\n",
    "        caches.append((fc_caches, nl_caches)) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "#         grads = self.model.copy()\n",
    "#         print('dy.shape', dy.shape)\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Input layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dX.copy() # pass it to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "#         print('''grads[2]['W'].shape, grads[2]['b'].shape''', grads[2]['W'].shape, grads[2]['b'].shape)\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = selu_backward(dout=dy, cache=nl_caches[layer])\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "#             print('''grads[1][]layer['W'].shape, grads[1][layer]['b'].shape''', grads[1][layer]['W'].shape, \n",
    "#                   grads[1][layer]['b'].shape)\n",
    "\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = selu_backward(dout=dy, cache=nl_cache)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "#         print('''grads[0]['W'].shape, grads[0]['b'].shape''', grads[0]['W'].shape, grads[0]['b'].shape)\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Momentums\n",
    "        M, R = [], []\n",
    "\n",
    "        # Input layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers momentum\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        # Output layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    " \n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "#                 M[0][key] = l.exp_running_avg(M[0][key], grads[0][key], beta1)\n",
    "#                 R[0][key] = l.exp_running_avg(R[0][key], grads[0][key]**2, beta2)\n",
    "#                 m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "#                 r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "#                 self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "#                     M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grads[1][layer][key], beta1)\n",
    "#                     R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grads[1][layer][key]**2, beta2)\n",
    "#                     m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "#                     r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "#                     self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "#                 M[2][key] = l.exp_running_avg(M[2][key], grads[2][key], beta1)\n",
    "#                 R[2][key] = l.exp_running_avg(R[2][key], grads[2][key]**2, beta2)\n",
    "#                 m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "#                 r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "#                 self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.5032 valid loss: 2.5223, valid accuracy: 0.1218\n",
      "Iter-20 train loss: 2.5962 valid loss: 2.4542, valid accuracy: 0.1256\n",
      "Iter-30 train loss: 2.3379 valid loss: 2.3940, valid accuracy: 0.1378\n",
      "Iter-40 train loss: 2.4953 valid loss: 2.3422, valid accuracy: 0.1484\n",
      "Iter-50 train loss: 2.4086 valid loss: 2.2920, valid accuracy: 0.1588\n",
      "Iter-60 train loss: 2.2327 valid loss: 2.2482, valid accuracy: 0.1724\n",
      "Iter-70 train loss: 2.2144 valid loss: 2.2045, valid accuracy: 0.1872\n",
      "Iter-80 train loss: 2.2398 valid loss: 2.1645, valid accuracy: 0.1982\n",
      "Iter-90 train loss: 2.0804 valid loss: 2.1245, valid accuracy: 0.2180\n",
      "Iter-100 train loss: 2.0258 valid loss: 2.0839, valid accuracy: 0.2326\n",
      "Iter-110 train loss: 2.0534 valid loss: 2.0442, valid accuracy: 0.2528\n",
      "Iter-120 train loss: 1.9820 valid loss: 2.0092, valid accuracy: 0.2712\n",
      "Iter-130 train loss: 1.9396 valid loss: 1.9767, valid accuracy: 0.2870\n",
      "Iter-140 train loss: 1.9860 valid loss: 1.9411, valid accuracy: 0.3106\n",
      "Iter-150 train loss: 1.9088 valid loss: 1.9083, valid accuracy: 0.3312\n",
      "Iter-160 train loss: 1.8213 valid loss: 1.8763, valid accuracy: 0.3462\n",
      "Iter-170 train loss: 1.8490 valid loss: 1.8478, valid accuracy: 0.3626\n",
      "Iter-180 train loss: 2.0177 valid loss: 1.8197, valid accuracy: 0.3772\n",
      "Iter-190 train loss: 1.6515 valid loss: 1.7907, valid accuracy: 0.3978\n",
      "Iter-200 train loss: 1.7686 valid loss: 1.7641, valid accuracy: 0.4130\n",
      "Iter-210 train loss: 1.9047 valid loss: 1.7393, valid accuracy: 0.4260\n",
      "Iter-220 train loss: 1.8509 valid loss: 1.7150, valid accuracy: 0.4386\n",
      "Iter-230 train loss: 1.7182 valid loss: 1.6925, valid accuracy: 0.4534\n",
      "Iter-240 train loss: 1.7032 valid loss: 1.6700, valid accuracy: 0.4672\n",
      "Iter-250 train loss: 1.6100 valid loss: 1.6465, valid accuracy: 0.4812\n",
      "Iter-260 train loss: 1.6369 valid loss: 1.6259, valid accuracy: 0.4926\n",
      "Iter-270 train loss: 1.4515 valid loss: 1.6057, valid accuracy: 0.5018\n",
      "Iter-280 train loss: 1.6577 valid loss: 1.5857, valid accuracy: 0.5126\n",
      "Iter-290 train loss: 1.6574 valid loss: 1.5657, valid accuracy: 0.5242\n",
      "Iter-300 train loss: 1.5196 valid loss: 1.5477, valid accuracy: 0.5336\n",
      "Iter-310 train loss: 1.5646 valid loss: 1.5283, valid accuracy: 0.5396\n",
      "Iter-320 train loss: 1.4953 valid loss: 1.5110, valid accuracy: 0.5500\n",
      "Iter-330 train loss: 1.5931 valid loss: 1.4928, valid accuracy: 0.5572\n",
      "Iter-340 train loss: 1.5654 valid loss: 1.4771, valid accuracy: 0.5642\n",
      "Iter-350 train loss: 1.3625 valid loss: 1.4601, valid accuracy: 0.5730\n",
      "Iter-360 train loss: 1.4798 valid loss: 1.4448, valid accuracy: 0.5792\n",
      "Iter-370 train loss: 1.5432 valid loss: 1.4293, valid accuracy: 0.5860\n",
      "Iter-380 train loss: 1.3513 valid loss: 1.4141, valid accuracy: 0.5910\n",
      "Iter-390 train loss: 1.3501 valid loss: 1.3998, valid accuracy: 0.5982\n",
      "Iter-400 train loss: 1.5316 valid loss: 1.3857, valid accuracy: 0.6050\n",
      "Iter-410 train loss: 1.3141 valid loss: 1.3714, valid accuracy: 0.6116\n",
      "Iter-420 train loss: 1.3310 valid loss: 1.3581, valid accuracy: 0.6182\n",
      "Iter-430 train loss: 1.3537 valid loss: 1.3444, valid accuracy: 0.6246\n",
      "Iter-440 train loss: 1.3355 valid loss: 1.3323, valid accuracy: 0.6280\n",
      "Iter-450 train loss: 1.3766 valid loss: 1.3201, valid accuracy: 0.6330\n",
      "Iter-460 train loss: 1.2810 valid loss: 1.3077, valid accuracy: 0.6380\n",
      "Iter-470 train loss: 1.2268 valid loss: 1.2964, valid accuracy: 0.6414\n",
      "Iter-480 train loss: 1.3578 valid loss: 1.2851, valid accuracy: 0.6436\n",
      "Iter-490 train loss: 1.3995 valid loss: 1.2734, valid accuracy: 0.6512\n",
      "Iter-500 train loss: 1.3768 valid loss: 1.2628, valid accuracy: 0.6528\n",
      "Iter-510 train loss: 1.2469 valid loss: 1.2525, valid accuracy: 0.6582\n",
      "Iter-520 train loss: 1.2784 valid loss: 1.2407, valid accuracy: 0.6616\n",
      "Iter-530 train loss: 1.1539 valid loss: 1.2300, valid accuracy: 0.6642\n",
      "Iter-540 train loss: 1.3066 valid loss: 1.2202, valid accuracy: 0.6670\n",
      "Iter-550 train loss: 1.0904 valid loss: 1.2102, valid accuracy: 0.6708\n",
      "Iter-560 train loss: 1.2281 valid loss: 1.2011, valid accuracy: 0.6746\n",
      "Iter-570 train loss: 1.0739 valid loss: 1.1917, valid accuracy: 0.6780\n",
      "Iter-580 train loss: 1.3483 valid loss: 1.1821, valid accuracy: 0.6826\n",
      "Iter-590 train loss: 1.1905 valid loss: 1.1732, valid accuracy: 0.6854\n",
      "Iter-600 train loss: 1.1971 valid loss: 1.1644, valid accuracy: 0.6890\n",
      "Iter-610 train loss: 1.3098 valid loss: 1.1559, valid accuracy: 0.6920\n",
      "Iter-620 train loss: 1.0149 valid loss: 1.1474, valid accuracy: 0.6948\n",
      "Iter-630 train loss: 1.1783 valid loss: 1.1390, valid accuracy: 0.6968\n",
      "Iter-640 train loss: 1.1079 valid loss: 1.1310, valid accuracy: 0.7000\n",
      "Iter-650 train loss: 1.1875 valid loss: 1.1226, valid accuracy: 0.7020\n",
      "Iter-660 train loss: 1.3160 valid loss: 1.1149, valid accuracy: 0.7024\n",
      "Iter-670 train loss: 1.1149 valid loss: 1.1066, valid accuracy: 0.7060\n",
      "Iter-680 train loss: 1.0864 valid loss: 1.0984, valid accuracy: 0.7074\n",
      "Iter-690 train loss: 1.1455 valid loss: 1.0911, valid accuracy: 0.7116\n",
      "Iter-700 train loss: 1.0396 valid loss: 1.0838, valid accuracy: 0.7126\n",
      "Iter-710 train loss: 1.1530 valid loss: 1.0769, valid accuracy: 0.7150\n",
      "Iter-720 train loss: 1.0840 valid loss: 1.0703, valid accuracy: 0.7176\n",
      "Iter-730 train loss: 1.0299 valid loss: 1.0632, valid accuracy: 0.7192\n",
      "Iter-740 train loss: 0.8805 valid loss: 1.0562, valid accuracy: 0.7226\n",
      "Iter-750 train loss: 1.2100 valid loss: 1.0499, valid accuracy: 0.7250\n",
      "Iter-760 train loss: 0.9684 valid loss: 1.0435, valid accuracy: 0.7262\n",
      "Iter-770 train loss: 1.2263 valid loss: 1.0371, valid accuracy: 0.7274\n",
      "Iter-780 train loss: 0.9317 valid loss: 1.0305, valid accuracy: 0.7296\n",
      "Iter-790 train loss: 1.0731 valid loss: 1.0243, valid accuracy: 0.7328\n",
      "Iter-800 train loss: 0.9572 valid loss: 1.0180, valid accuracy: 0.7346\n",
      "Iter-810 train loss: 1.0431 valid loss: 1.0123, valid accuracy: 0.7342\n",
      "Iter-820 train loss: 0.9464 valid loss: 1.0066, valid accuracy: 0.7374\n",
      "Iter-830 train loss: 1.0890 valid loss: 1.0007, valid accuracy: 0.7378\n",
      "Iter-840 train loss: 1.0183 valid loss: 0.9954, valid accuracy: 0.7382\n",
      "Iter-850 train loss: 0.9601 valid loss: 0.9899, valid accuracy: 0.7394\n",
      "Iter-860 train loss: 0.8771 valid loss: 0.9844, valid accuracy: 0.7410\n",
      "Iter-870 train loss: 0.9751 valid loss: 0.9787, valid accuracy: 0.7418\n",
      "Iter-880 train loss: 1.0664 valid loss: 0.9742, valid accuracy: 0.7434\n",
      "Iter-890 train loss: 0.9402 valid loss: 0.9690, valid accuracy: 0.7438\n",
      "Iter-900 train loss: 0.9648 valid loss: 0.9635, valid accuracy: 0.7466\n",
      "Iter-910 train loss: 0.9982 valid loss: 0.9586, valid accuracy: 0.7486\n",
      "Iter-920 train loss: 0.8451 valid loss: 0.9536, valid accuracy: 0.7496\n",
      "Iter-930 train loss: 1.1708 valid loss: 0.9484, valid accuracy: 0.7514\n",
      "Iter-940 train loss: 1.1019 valid loss: 0.9435, valid accuracy: 0.7530\n",
      "Iter-950 train loss: 0.9004 valid loss: 0.9385, valid accuracy: 0.7554\n",
      "Iter-960 train loss: 0.8708 valid loss: 0.9342, valid accuracy: 0.7574\n",
      "Iter-970 train loss: 1.0568 valid loss: 0.9294, valid accuracy: 0.7576\n",
      "Iter-980 train loss: 0.7516 valid loss: 0.9245, valid accuracy: 0.7582\n",
      "Iter-990 train loss: 0.9270 valid loss: 0.9201, valid accuracy: 0.7600\n",
      "Iter-1000 train loss: 0.9471 valid loss: 0.9158, valid accuracy: 0.7606\n",
      "Iter-1010 train loss: 0.9554 valid loss: 0.9110, valid accuracy: 0.7630\n",
      "Iter-1020 train loss: 0.8189 valid loss: 0.9066, valid accuracy: 0.7638\n",
      "Iter-1030 train loss: 0.8648 valid loss: 0.9024, valid accuracy: 0.7666\n",
      "Iter-1040 train loss: 0.9647 valid loss: 0.8977, valid accuracy: 0.7676\n",
      "Iter-1050 train loss: 0.7704 valid loss: 0.8933, valid accuracy: 0.7678\n",
      "Iter-1060 train loss: 0.8623 valid loss: 0.8892, valid accuracy: 0.7698\n",
      "Iter-1070 train loss: 0.8646 valid loss: 0.8852, valid accuracy: 0.7714\n",
      "Iter-1080 train loss: 0.9519 valid loss: 0.8810, valid accuracy: 0.7716\n",
      "Iter-1090 train loss: 0.7438 valid loss: 0.8771, valid accuracy: 0.7724\n",
      "Iter-1100 train loss: 0.7589 valid loss: 0.8732, valid accuracy: 0.7722\n",
      "Iter-1110 train loss: 1.0089 valid loss: 0.8691, valid accuracy: 0.7730\n",
      "Iter-1120 train loss: 0.8309 valid loss: 0.8650, valid accuracy: 0.7748\n",
      "Iter-1130 train loss: 0.8497 valid loss: 0.8613, valid accuracy: 0.7760\n",
      "Iter-1140 train loss: 0.8085 valid loss: 0.8577, valid accuracy: 0.7756\n",
      "Iter-1150 train loss: 0.7500 valid loss: 0.8540, valid accuracy: 0.7768\n",
      "Iter-1160 train loss: 0.7589 valid loss: 0.8507, valid accuracy: 0.7776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 0.8112 valid loss: 0.8474, valid accuracy: 0.7788\n",
      "Iter-1180 train loss: 0.8282 valid loss: 0.8439, valid accuracy: 0.7798\n",
      "Iter-1190 train loss: 0.9695 valid loss: 0.8405, valid accuracy: 0.7822\n",
      "Iter-1200 train loss: 0.8464 valid loss: 0.8373, valid accuracy: 0.7836\n",
      "Iter-1210 train loss: 0.7477 valid loss: 0.8338, valid accuracy: 0.7826\n",
      "Iter-1220 train loss: 0.9184 valid loss: 0.8304, valid accuracy: 0.7830\n",
      "Iter-1230 train loss: 0.8695 valid loss: 0.8268, valid accuracy: 0.7852\n",
      "Iter-1240 train loss: 0.7705 valid loss: 0.8231, valid accuracy: 0.7860\n",
      "Iter-1250 train loss: 1.1348 valid loss: 0.8200, valid accuracy: 0.7874\n",
      "Iter-1260 train loss: 0.7927 valid loss: 0.8171, valid accuracy: 0.7856\n",
      "Iter-1270 train loss: 0.8614 valid loss: 0.8137, valid accuracy: 0.7886\n",
      "Iter-1280 train loss: 0.8899 valid loss: 0.8106, valid accuracy: 0.7900\n",
      "Iter-1290 train loss: 0.6292 valid loss: 0.8074, valid accuracy: 0.7896\n",
      "Iter-1300 train loss: 0.8841 valid loss: 0.8044, valid accuracy: 0.7918\n",
      "Iter-1310 train loss: 0.9054 valid loss: 0.8015, valid accuracy: 0.7922\n",
      "Iter-1320 train loss: 0.9642 valid loss: 0.7986, valid accuracy: 0.7928\n",
      "Iter-1330 train loss: 0.8081 valid loss: 0.7954, valid accuracy: 0.7930\n",
      "Iter-1340 train loss: 0.5863 valid loss: 0.7927, valid accuracy: 0.7930\n",
      "Iter-1350 train loss: 0.8831 valid loss: 0.7900, valid accuracy: 0.7944\n",
      "Iter-1360 train loss: 0.7464 valid loss: 0.7870, valid accuracy: 0.7938\n",
      "Iter-1370 train loss: 0.7816 valid loss: 0.7839, valid accuracy: 0.7944\n",
      "Iter-1380 train loss: 0.8897 valid loss: 0.7811, valid accuracy: 0.7938\n",
      "Iter-1390 train loss: 0.7474 valid loss: 0.7783, valid accuracy: 0.7960\n",
      "Iter-1400 train loss: 0.9379 valid loss: 0.7757, valid accuracy: 0.7970\n",
      "Iter-1410 train loss: 0.7840 valid loss: 0.7731, valid accuracy: 0.7980\n",
      "Iter-1420 train loss: 0.8222 valid loss: 0.7706, valid accuracy: 0.7982\n",
      "Iter-1430 train loss: 0.8025 valid loss: 0.7681, valid accuracy: 0.7980\n",
      "Iter-1440 train loss: 0.8242 valid loss: 0.7658, valid accuracy: 0.7992\n",
      "Iter-1450 train loss: 0.7141 valid loss: 0.7631, valid accuracy: 0.8002\n",
      "Iter-1460 train loss: 0.8875 valid loss: 0.7605, valid accuracy: 0.8004\n",
      "Iter-1470 train loss: 0.8742 valid loss: 0.7581, valid accuracy: 0.8010\n",
      "Iter-1480 train loss: 0.7740 valid loss: 0.7555, valid accuracy: 0.8014\n",
      "Iter-1490 train loss: 0.6877 valid loss: 0.7528, valid accuracy: 0.8018\n",
      "Iter-1500 train loss: 0.8503 valid loss: 0.7507, valid accuracy: 0.8014\n",
      "Iter-1510 train loss: 0.8789 valid loss: 0.7482, valid accuracy: 0.8026\n",
      "Iter-1520 train loss: 0.6751 valid loss: 0.7458, valid accuracy: 0.8030\n",
      "Iter-1530 train loss: 0.7519 valid loss: 0.7435, valid accuracy: 0.8034\n",
      "Iter-1540 train loss: 0.9255 valid loss: 0.7409, valid accuracy: 0.8036\n",
      "Iter-1550 train loss: 0.7771 valid loss: 0.7384, valid accuracy: 0.8034\n",
      "Iter-1560 train loss: 0.7533 valid loss: 0.7361, valid accuracy: 0.8048\n",
      "Iter-1570 train loss: 0.6011 valid loss: 0.7341, valid accuracy: 0.8052\n",
      "Iter-1580 train loss: 0.7036 valid loss: 0.7319, valid accuracy: 0.8060\n",
      "Iter-1590 train loss: 0.8045 valid loss: 0.7300, valid accuracy: 0.8058\n",
      "Iter-1600 train loss: 0.6543 valid loss: 0.7277, valid accuracy: 0.8066\n",
      "Iter-1610 train loss: 0.7035 valid loss: 0.7255, valid accuracy: 0.8092\n",
      "Iter-1620 train loss: 0.7877 valid loss: 0.7233, valid accuracy: 0.8108\n",
      "Iter-1630 train loss: 0.7601 valid loss: 0.7208, valid accuracy: 0.8110\n",
      "Iter-1640 train loss: 0.7002 valid loss: 0.7186, valid accuracy: 0.8106\n",
      "Iter-1650 train loss: 0.6139 valid loss: 0.7165, valid accuracy: 0.8116\n",
      "Iter-1660 train loss: 0.7799 valid loss: 0.7146, valid accuracy: 0.8110\n",
      "Iter-1670 train loss: 0.6728 valid loss: 0.7122, valid accuracy: 0.8114\n",
      "Iter-1680 train loss: 0.6784 valid loss: 0.7103, valid accuracy: 0.8116\n",
      "Iter-1690 train loss: 0.6208 valid loss: 0.7080, valid accuracy: 0.8120\n",
      "Iter-1700 train loss: 0.6493 valid loss: 0.7063, valid accuracy: 0.8126\n",
      "Iter-1710 train loss: 0.8637 valid loss: 0.7040, valid accuracy: 0.8130\n",
      "Iter-1720 train loss: 0.6917 valid loss: 0.7021, valid accuracy: 0.8140\n",
      "Iter-1730 train loss: 0.6951 valid loss: 0.7002, valid accuracy: 0.8146\n",
      "Iter-1740 train loss: 0.6741 valid loss: 0.6983, valid accuracy: 0.8146\n",
      "Iter-1750 train loss: 0.6366 valid loss: 0.6961, valid accuracy: 0.8150\n",
      "Iter-1760 train loss: 0.6080 valid loss: 0.6941, valid accuracy: 0.8160\n",
      "Iter-1770 train loss: 0.6336 valid loss: 0.6925, valid accuracy: 0.8156\n",
      "Iter-1780 train loss: 0.7621 valid loss: 0.6906, valid accuracy: 0.8160\n",
      "Iter-1790 train loss: 0.6272 valid loss: 0.6887, valid accuracy: 0.8166\n",
      "Iter-1800 train loss: 0.5369 valid loss: 0.6869, valid accuracy: 0.8178\n",
      "Iter-1810 train loss: 0.7662 valid loss: 0.6850, valid accuracy: 0.8182\n",
      "Iter-1820 train loss: 0.7624 valid loss: 0.6833, valid accuracy: 0.8182\n",
      "Iter-1830 train loss: 0.7963 valid loss: 0.6815, valid accuracy: 0.8176\n",
      "Iter-1840 train loss: 0.5844 valid loss: 0.6796, valid accuracy: 0.8180\n",
      "Iter-1850 train loss: 0.8149 valid loss: 0.6780, valid accuracy: 0.8194\n",
      "Iter-1860 train loss: 0.8536 valid loss: 0.6764, valid accuracy: 0.8190\n",
      "Iter-1870 train loss: 0.6264 valid loss: 0.6746, valid accuracy: 0.8198\n",
      "Iter-1880 train loss: 0.5807 valid loss: 0.6728, valid accuracy: 0.8196\n",
      "Iter-1890 train loss: 0.5287 valid loss: 0.6709, valid accuracy: 0.8200\n",
      "Iter-1900 train loss: 0.7752 valid loss: 0.6694, valid accuracy: 0.8198\n",
      "Iter-1910 train loss: 0.8200 valid loss: 0.6676, valid accuracy: 0.8208\n",
      "Iter-1920 train loss: 0.7870 valid loss: 0.6658, valid accuracy: 0.8216\n",
      "Iter-1930 train loss: 1.0371 valid loss: 0.6640, valid accuracy: 0.8218\n",
      "Iter-1940 train loss: 0.8528 valid loss: 0.6624, valid accuracy: 0.8226\n",
      "Iter-1950 train loss: 0.6408 valid loss: 0.6607, valid accuracy: 0.8228\n",
      "Iter-1960 train loss: 0.7003 valid loss: 0.6591, valid accuracy: 0.8236\n",
      "Iter-1970 train loss: 0.6005 valid loss: 0.6574, valid accuracy: 0.8248\n",
      "Iter-1980 train loss: 0.6357 valid loss: 0.6560, valid accuracy: 0.8242\n",
      "Iter-1990 train loss: 0.7449 valid loss: 0.6545, valid accuracy: 0.8248\n",
      "Iter-2000 train loss: 0.7641 valid loss: 0.6528, valid accuracy: 0.8248\n",
      "Iter-2010 train loss: 0.6098 valid loss: 0.6512, valid accuracy: 0.8266\n",
      "Iter-2020 train loss: 0.7808 valid loss: 0.6496, valid accuracy: 0.8274\n",
      "Iter-2030 train loss: 0.7901 valid loss: 0.6481, valid accuracy: 0.8272\n",
      "Iter-2040 train loss: 0.5804 valid loss: 0.6466, valid accuracy: 0.8282\n",
      "Iter-2050 train loss: 0.5801 valid loss: 0.6452, valid accuracy: 0.8294\n",
      "Iter-2060 train loss: 0.5073 valid loss: 0.6439, valid accuracy: 0.8300\n",
      "Iter-2070 train loss: 0.7083 valid loss: 0.6424, valid accuracy: 0.8296\n",
      "Iter-2080 train loss: 0.7023 valid loss: 0.6409, valid accuracy: 0.8294\n",
      "Iter-2090 train loss: 0.8002 valid loss: 0.6395, valid accuracy: 0.8284\n",
      "Iter-2100 train loss: 0.7092 valid loss: 0.6380, valid accuracy: 0.8272\n",
      "Iter-2110 train loss: 0.6096 valid loss: 0.6364, valid accuracy: 0.8296\n",
      "Iter-2120 train loss: 0.7757 valid loss: 0.6351, valid accuracy: 0.8296\n",
      "Iter-2130 train loss: 0.6356 valid loss: 0.6338, valid accuracy: 0.8284\n",
      "Iter-2140 train loss: 0.4665 valid loss: 0.6324, valid accuracy: 0.8296\n",
      "Iter-2150 train loss: 0.6174 valid loss: 0.6314, valid accuracy: 0.8312\n",
      "Iter-2160 train loss: 0.7911 valid loss: 0.6297, valid accuracy: 0.8324\n",
      "Iter-2170 train loss: 0.7689 valid loss: 0.6283, valid accuracy: 0.8328\n",
      "Iter-2180 train loss: 0.7014 valid loss: 0.6270, valid accuracy: 0.8322\n",
      "Iter-2190 train loss: 0.7434 valid loss: 0.6259, valid accuracy: 0.8322\n",
      "Iter-2200 train loss: 0.5809 valid loss: 0.6244, valid accuracy: 0.8322\n",
      "Iter-2210 train loss: 0.8247 valid loss: 0.6235, valid accuracy: 0.8324\n",
      "Iter-2220 train loss: 0.5899 valid loss: 0.6219, valid accuracy: 0.8328\n",
      "Iter-2230 train loss: 0.7054 valid loss: 0.6205, valid accuracy: 0.8328\n",
      "Iter-2240 train loss: 0.5173 valid loss: 0.6193, valid accuracy: 0.8340\n",
      "Iter-2250 train loss: 0.6997 valid loss: 0.6179, valid accuracy: 0.8336\n",
      "Iter-2260 train loss: 0.7114 valid loss: 0.6167, valid accuracy: 0.8336\n",
      "Iter-2270 train loss: 0.5661 valid loss: 0.6153, valid accuracy: 0.8344\n",
      "Iter-2280 train loss: 0.7960 valid loss: 0.6140, valid accuracy: 0.8330\n",
      "Iter-2290 train loss: 0.5888 valid loss: 0.6127, valid accuracy: 0.8332\n",
      "Iter-2300 train loss: 0.7026 valid loss: 0.6115, valid accuracy: 0.8344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 0.4915 valid loss: 0.6103, valid accuracy: 0.8350\n",
      "Iter-2320 train loss: 0.8936 valid loss: 0.6090, valid accuracy: 0.8344\n",
      "Iter-2330 train loss: 0.5644 valid loss: 0.6079, valid accuracy: 0.8344\n",
      "Iter-2340 train loss: 0.7758 valid loss: 0.6068, valid accuracy: 0.8342\n",
      "Iter-2350 train loss: 0.6675 valid loss: 0.6056, valid accuracy: 0.8348\n",
      "Iter-2360 train loss: 0.7106 valid loss: 0.6044, valid accuracy: 0.8356\n",
      "Iter-2370 train loss: 0.8975 valid loss: 0.6033, valid accuracy: 0.8356\n",
      "Iter-2380 train loss: 0.7189 valid loss: 0.6021, valid accuracy: 0.8362\n",
      "Iter-2390 train loss: 0.6375 valid loss: 0.6010, valid accuracy: 0.8360\n",
      "Iter-2400 train loss: 0.6479 valid loss: 0.5999, valid accuracy: 0.8364\n",
      "Iter-2410 train loss: 0.5400 valid loss: 0.5986, valid accuracy: 0.8370\n",
      "Iter-2420 train loss: 0.5539 valid loss: 0.5975, valid accuracy: 0.8366\n",
      "Iter-2430 train loss: 0.5490 valid loss: 0.5962, valid accuracy: 0.8360\n",
      "Iter-2440 train loss: 0.5780 valid loss: 0.5951, valid accuracy: 0.8376\n",
      "Iter-2450 train loss: 0.5571 valid loss: 0.5939, valid accuracy: 0.8384\n",
      "Iter-2460 train loss: 0.7371 valid loss: 0.5927, valid accuracy: 0.8392\n",
      "Iter-2470 train loss: 0.5381 valid loss: 0.5916, valid accuracy: 0.8388\n",
      "Iter-2480 train loss: 0.4850 valid loss: 0.5904, valid accuracy: 0.8388\n",
      "Iter-2490 train loss: 0.7407 valid loss: 0.5892, valid accuracy: 0.8394\n",
      "Iter-2500 train loss: 0.6623 valid loss: 0.5883, valid accuracy: 0.8408\n",
      "Iter-2510 train loss: 0.6276 valid loss: 0.5871, valid accuracy: 0.8416\n",
      "Iter-2520 train loss: 0.7097 valid loss: 0.5860, valid accuracy: 0.8418\n",
      "Iter-2530 train loss: 0.6246 valid loss: 0.5850, valid accuracy: 0.8414\n",
      "Iter-2540 train loss: 0.5253 valid loss: 0.5839, valid accuracy: 0.8410\n",
      "Iter-2550 train loss: 0.5882 valid loss: 0.5828, valid accuracy: 0.8410\n",
      "Iter-2560 train loss: 0.7201 valid loss: 0.5817, valid accuracy: 0.8424\n",
      "Iter-2570 train loss: 0.6182 valid loss: 0.5807, valid accuracy: 0.8426\n",
      "Iter-2580 train loss: 0.5351 valid loss: 0.5795, valid accuracy: 0.8416\n",
      "Iter-2590 train loss: 0.6035 valid loss: 0.5786, valid accuracy: 0.8426\n",
      "Iter-2600 train loss: 0.4505 valid loss: 0.5776, valid accuracy: 0.8424\n",
      "Iter-2610 train loss: 0.4585 valid loss: 0.5766, valid accuracy: 0.8422\n",
      "Iter-2620 train loss: 0.7067 valid loss: 0.5756, valid accuracy: 0.8420\n",
      "Iter-2630 train loss: 0.6343 valid loss: 0.5746, valid accuracy: 0.8424\n",
      "Iter-2640 train loss: 0.6596 valid loss: 0.5739, valid accuracy: 0.8426\n",
      "Iter-2650 train loss: 0.7381 valid loss: 0.5728, valid accuracy: 0.8418\n",
      "Iter-2660 train loss: 0.6785 valid loss: 0.5718, valid accuracy: 0.8430\n",
      "Iter-2670 train loss: 0.7629 valid loss: 0.5708, valid accuracy: 0.8430\n",
      "Iter-2680 train loss: 0.6591 valid loss: 0.5699, valid accuracy: 0.8432\n",
      "Iter-2690 train loss: 0.5647 valid loss: 0.5690, valid accuracy: 0.8446\n",
      "Iter-2700 train loss: 0.5096 valid loss: 0.5680, valid accuracy: 0.8450\n",
      "Iter-2710 train loss: 0.7946 valid loss: 0.5669, valid accuracy: 0.8456\n",
      "Iter-2720 train loss: 0.5056 valid loss: 0.5660, valid accuracy: 0.8456\n",
      "Iter-2730 train loss: 0.6251 valid loss: 0.5652, valid accuracy: 0.8458\n",
      "Iter-2740 train loss: 0.7011 valid loss: 0.5643, valid accuracy: 0.8456\n",
      "Iter-2750 train loss: 0.5825 valid loss: 0.5632, valid accuracy: 0.8456\n",
      "Iter-2760 train loss: 0.5452 valid loss: 0.5623, valid accuracy: 0.8444\n",
      "Iter-2770 train loss: 0.3978 valid loss: 0.5614, valid accuracy: 0.8462\n",
      "Iter-2780 train loss: 0.6039 valid loss: 0.5604, valid accuracy: 0.8464\n",
      "Iter-2790 train loss: 0.7012 valid loss: 0.5597, valid accuracy: 0.8458\n",
      "Iter-2800 train loss: 0.6184 valid loss: 0.5589, valid accuracy: 0.8472\n",
      "Iter-2810 train loss: 0.7814 valid loss: 0.5579, valid accuracy: 0.8472\n",
      "Iter-2820 train loss: 0.6479 valid loss: 0.5568, valid accuracy: 0.8478\n",
      "Iter-2830 train loss: 0.4122 valid loss: 0.5560, valid accuracy: 0.8476\n",
      "Iter-2840 train loss: 0.4873 valid loss: 0.5551, valid accuracy: 0.8484\n",
      "Iter-2850 train loss: 0.6944 valid loss: 0.5542, valid accuracy: 0.8480\n",
      "Iter-2860 train loss: 0.6087 valid loss: 0.5533, valid accuracy: 0.8488\n",
      "Iter-2870 train loss: 0.7299 valid loss: 0.5524, valid accuracy: 0.8486\n",
      "Iter-2880 train loss: 0.6241 valid loss: 0.5515, valid accuracy: 0.8492\n",
      "Iter-2890 train loss: 0.6684 valid loss: 0.5509, valid accuracy: 0.8490\n",
      "Iter-2900 train loss: 0.5755 valid loss: 0.5499, valid accuracy: 0.8500\n",
      "Iter-2910 train loss: 0.6238 valid loss: 0.5491, valid accuracy: 0.8498\n",
      "Iter-2920 train loss: 0.6417 valid loss: 0.5485, valid accuracy: 0.8500\n",
      "Iter-2930 train loss: 0.5919 valid loss: 0.5477, valid accuracy: 0.8512\n",
      "Iter-2940 train loss: 0.5395 valid loss: 0.5466, valid accuracy: 0.8522\n",
      "Iter-2950 train loss: 0.7778 valid loss: 0.5457, valid accuracy: 0.8512\n",
      "Iter-2960 train loss: 0.5969 valid loss: 0.5449, valid accuracy: 0.8520\n",
      "Iter-2970 train loss: 0.5615 valid loss: 0.5443, valid accuracy: 0.8518\n",
      "Iter-2980 train loss: 0.4851 valid loss: 0.5439, valid accuracy: 0.8524\n",
      "Iter-2990 train loss: 0.5470 valid loss: 0.5426, valid accuracy: 0.8530\n",
      "Iter-3000 train loss: 0.4640 valid loss: 0.5420, valid accuracy: 0.8532\n",
      "Iter-3010 train loss: 0.5245 valid loss: 0.5415, valid accuracy: 0.8532\n",
      "Iter-3020 train loss: 0.6639 valid loss: 0.5405, valid accuracy: 0.8528\n",
      "Iter-3030 train loss: 0.6187 valid loss: 0.5398, valid accuracy: 0.8532\n",
      "Iter-3040 train loss: 0.3546 valid loss: 0.5392, valid accuracy: 0.8530\n",
      "Iter-3050 train loss: 0.5640 valid loss: 0.5382, valid accuracy: 0.8524\n",
      "Iter-3060 train loss: 0.6004 valid loss: 0.5374, valid accuracy: 0.8522\n",
      "Iter-3070 train loss: 0.4084 valid loss: 0.5367, valid accuracy: 0.8526\n",
      "Iter-3080 train loss: 0.4370 valid loss: 0.5358, valid accuracy: 0.8532\n",
      "Iter-3090 train loss: 0.5233 valid loss: 0.5348, valid accuracy: 0.8530\n",
      "Iter-3100 train loss: 0.4286 valid loss: 0.5342, valid accuracy: 0.8532\n",
      "Iter-3110 train loss: 0.8002 valid loss: 0.5333, valid accuracy: 0.8540\n",
      "Iter-3120 train loss: 0.5620 valid loss: 0.5325, valid accuracy: 0.8546\n",
      "Iter-3130 train loss: 0.4767 valid loss: 0.5318, valid accuracy: 0.8546\n",
      "Iter-3140 train loss: 0.5601 valid loss: 0.5309, valid accuracy: 0.8548\n",
      "Iter-3150 train loss: 0.4851 valid loss: 0.5301, valid accuracy: 0.8550\n",
      "Iter-3160 train loss: 0.6075 valid loss: 0.5292, valid accuracy: 0.8552\n",
      "Iter-3170 train loss: 0.4872 valid loss: 0.5284, valid accuracy: 0.8546\n",
      "Iter-3180 train loss: 0.5987 valid loss: 0.5280, valid accuracy: 0.8550\n",
      "Iter-3190 train loss: 0.4895 valid loss: 0.5271, valid accuracy: 0.8554\n",
      "Iter-3200 train loss: 0.6452 valid loss: 0.5263, valid accuracy: 0.8558\n",
      "Iter-3210 train loss: 0.4587 valid loss: 0.5256, valid accuracy: 0.8562\n",
      "Iter-3220 train loss: 0.7418 valid loss: 0.5248, valid accuracy: 0.8566\n",
      "Iter-3230 train loss: 0.5856 valid loss: 0.5240, valid accuracy: 0.8562\n",
      "Iter-3240 train loss: 0.4319 valid loss: 0.5233, valid accuracy: 0.8568\n",
      "Iter-3250 train loss: 0.4948 valid loss: 0.5227, valid accuracy: 0.8562\n",
      "Iter-3260 train loss: 0.6390 valid loss: 0.5219, valid accuracy: 0.8572\n",
      "Iter-3270 train loss: 0.5075 valid loss: 0.5212, valid accuracy: 0.8568\n",
      "Iter-3280 train loss: 0.4457 valid loss: 0.5205, valid accuracy: 0.8570\n",
      "Iter-3290 train loss: 0.6290 valid loss: 0.5200, valid accuracy: 0.8574\n",
      "Iter-3300 train loss: 0.5345 valid loss: 0.5194, valid accuracy: 0.8574\n",
      "Iter-3310 train loss: 0.5816 valid loss: 0.5188, valid accuracy: 0.8580\n",
      "Iter-3320 train loss: 0.4790 valid loss: 0.5182, valid accuracy: 0.8582\n",
      "Iter-3330 train loss: 0.8255 valid loss: 0.5174, valid accuracy: 0.8594\n",
      "Iter-3340 train loss: 0.5853 valid loss: 0.5167, valid accuracy: 0.8592\n",
      "Iter-3350 train loss: 0.6821 valid loss: 0.5161, valid accuracy: 0.8602\n",
      "Iter-3360 train loss: 0.7376 valid loss: 0.5153, valid accuracy: 0.8588\n",
      "Iter-3370 train loss: 0.6758 valid loss: 0.5145, valid accuracy: 0.8588\n",
      "Iter-3380 train loss: 0.6144 valid loss: 0.5138, valid accuracy: 0.8584\n",
      "Iter-3390 train loss: 0.4317 valid loss: 0.5133, valid accuracy: 0.8592\n",
      "Iter-3400 train loss: 0.4900 valid loss: 0.5126, valid accuracy: 0.8590\n",
      "Iter-3410 train loss: 0.5491 valid loss: 0.5120, valid accuracy: 0.8592\n",
      "Iter-3420 train loss: 0.4590 valid loss: 0.5115, valid accuracy: 0.8588\n",
      "Iter-3430 train loss: 0.4771 valid loss: 0.5109, valid accuracy: 0.8596\n",
      "Iter-3440 train loss: 0.5325 valid loss: 0.5103, valid accuracy: 0.8594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 0.7826 valid loss: 0.5096, valid accuracy: 0.8594\n",
      "Iter-3460 train loss: 0.5905 valid loss: 0.5090, valid accuracy: 0.8600\n",
      "Iter-3470 train loss: 0.4373 valid loss: 0.5084, valid accuracy: 0.8600\n",
      "Iter-3480 train loss: 0.7528 valid loss: 0.5077, valid accuracy: 0.8600\n",
      "Iter-3490 train loss: 0.4836 valid loss: 0.5071, valid accuracy: 0.8600\n",
      "Iter-3500 train loss: 0.3619 valid loss: 0.5066, valid accuracy: 0.8596\n",
      "Iter-3510 train loss: 0.6947 valid loss: 0.5059, valid accuracy: 0.8600\n",
      "Iter-3520 train loss: 0.6698 valid loss: 0.5059, valid accuracy: 0.8620\n",
      "Iter-3530 train loss: 0.5298 valid loss: 0.5051, valid accuracy: 0.8614\n",
      "Iter-3540 train loss: 0.5018 valid loss: 0.5045, valid accuracy: 0.8612\n",
      "Iter-3550 train loss: 0.6266 valid loss: 0.5037, valid accuracy: 0.8608\n",
      "Iter-3560 train loss: 0.4900 valid loss: 0.5032, valid accuracy: 0.8618\n",
      "Iter-3570 train loss: 0.6083 valid loss: 0.5026, valid accuracy: 0.8624\n",
      "Iter-3580 train loss: 0.5724 valid loss: 0.5019, valid accuracy: 0.8624\n",
      "Iter-3590 train loss: 0.4193 valid loss: 0.5014, valid accuracy: 0.8638\n",
      "Iter-3600 train loss: 0.6468 valid loss: 0.5009, valid accuracy: 0.8640\n",
      "Iter-3610 train loss: 0.3942 valid loss: 0.5002, valid accuracy: 0.8640\n",
      "Iter-3620 train loss: 0.4438 valid loss: 0.4996, valid accuracy: 0.8634\n",
      "Iter-3630 train loss: 0.5397 valid loss: 0.4990, valid accuracy: 0.8646\n",
      "Iter-3640 train loss: 0.4322 valid loss: 0.4982, valid accuracy: 0.8642\n",
      "Iter-3650 train loss: 0.3700 valid loss: 0.4977, valid accuracy: 0.8642\n",
      "Iter-3660 train loss: 0.5273 valid loss: 0.4973, valid accuracy: 0.8640\n",
      "Iter-3670 train loss: 0.5436 valid loss: 0.4966, valid accuracy: 0.8646\n",
      "Iter-3680 train loss: 0.6387 valid loss: 0.4961, valid accuracy: 0.8644\n",
      "Iter-3690 train loss: 0.5606 valid loss: 0.4954, valid accuracy: 0.8642\n",
      "Iter-3700 train loss: 0.5513 valid loss: 0.4947, valid accuracy: 0.8638\n",
      "Iter-3710 train loss: 0.6154 valid loss: 0.4941, valid accuracy: 0.8652\n",
      "Iter-3720 train loss: 0.7643 valid loss: 0.4936, valid accuracy: 0.8648\n",
      "Iter-3730 train loss: 0.4287 valid loss: 0.4930, valid accuracy: 0.8662\n",
      "Iter-3740 train loss: 0.4832 valid loss: 0.4925, valid accuracy: 0.8662\n",
      "Iter-3750 train loss: 0.4429 valid loss: 0.4919, valid accuracy: 0.8654\n",
      "Iter-3760 train loss: 0.3742 valid loss: 0.4914, valid accuracy: 0.8652\n",
      "Iter-3770 train loss: 0.5581 valid loss: 0.4910, valid accuracy: 0.8654\n",
      "Iter-3780 train loss: 0.5402 valid loss: 0.4905, valid accuracy: 0.8654\n",
      "Iter-3790 train loss: 0.4763 valid loss: 0.4902, valid accuracy: 0.8662\n",
      "Iter-3800 train loss: 0.4930 valid loss: 0.4898, valid accuracy: 0.8670\n",
      "Iter-3810 train loss: 0.4617 valid loss: 0.4894, valid accuracy: 0.8668\n",
      "Iter-3820 train loss: 0.5265 valid loss: 0.4888, valid accuracy: 0.8672\n",
      "Iter-3830 train loss: 0.4050 valid loss: 0.4882, valid accuracy: 0.8678\n",
      "Iter-3840 train loss: 0.5102 valid loss: 0.4876, valid accuracy: 0.8674\n",
      "Iter-3850 train loss: 0.7649 valid loss: 0.4868, valid accuracy: 0.8680\n",
      "Iter-3860 train loss: 0.7388 valid loss: 0.4863, valid accuracy: 0.8672\n",
      "Iter-3870 train loss: 0.5655 valid loss: 0.4857, valid accuracy: 0.8684\n",
      "Iter-3880 train loss: 0.4282 valid loss: 0.4852, valid accuracy: 0.8678\n",
      "Iter-3890 train loss: 0.4497 valid loss: 0.4846, valid accuracy: 0.8684\n",
      "Iter-3900 train loss: 0.5908 valid loss: 0.4843, valid accuracy: 0.8682\n",
      "Iter-3910 train loss: 0.5480 valid loss: 0.4837, valid accuracy: 0.8686\n",
      "Iter-3920 train loss: 0.4824 valid loss: 0.4831, valid accuracy: 0.8682\n",
      "Iter-3930 train loss: 0.5389 valid loss: 0.4826, valid accuracy: 0.8688\n",
      "Iter-3940 train loss: 0.5455 valid loss: 0.4822, valid accuracy: 0.8692\n",
      "Iter-3950 train loss: 0.4728 valid loss: 0.4818, valid accuracy: 0.8690\n",
      "Iter-3960 train loss: 0.4752 valid loss: 0.4815, valid accuracy: 0.8698\n",
      "Iter-3970 train loss: 0.3072 valid loss: 0.4811, valid accuracy: 0.8694\n",
      "Iter-3980 train loss: 0.5603 valid loss: 0.4805, valid accuracy: 0.8694\n",
      "Iter-3990 train loss: 0.4169 valid loss: 0.4800, valid accuracy: 0.8696\n",
      "Iter-4000 train loss: 0.3532 valid loss: 0.4796, valid accuracy: 0.8692\n",
      "Iter-4010 train loss: 0.5333 valid loss: 0.4789, valid accuracy: 0.8696\n",
      "Iter-4020 train loss: 0.4540 valid loss: 0.4783, valid accuracy: 0.8696\n",
      "Iter-4030 train loss: 0.5266 valid loss: 0.4777, valid accuracy: 0.8704\n",
      "Iter-4040 train loss: 0.5551 valid loss: 0.4773, valid accuracy: 0.8698\n",
      "Iter-4050 train loss: 0.3729 valid loss: 0.4768, valid accuracy: 0.8694\n",
      "Iter-4060 train loss: 0.6010 valid loss: 0.4763, valid accuracy: 0.8698\n",
      "Iter-4070 train loss: 0.6022 valid loss: 0.4759, valid accuracy: 0.8702\n",
      "Iter-4080 train loss: 0.3924 valid loss: 0.4756, valid accuracy: 0.8704\n",
      "Iter-4090 train loss: 0.6428 valid loss: 0.4753, valid accuracy: 0.8696\n",
      "Iter-4100 train loss: 0.5343 valid loss: 0.4748, valid accuracy: 0.8698\n",
      "Iter-4110 train loss: 0.4405 valid loss: 0.4742, valid accuracy: 0.8706\n",
      "Iter-4120 train loss: 0.4381 valid loss: 0.4737, valid accuracy: 0.8696\n",
      "Iter-4130 train loss: 0.4172 valid loss: 0.4731, valid accuracy: 0.8698\n",
      "Iter-4140 train loss: 0.6505 valid loss: 0.4726, valid accuracy: 0.8702\n",
      "Iter-4150 train loss: 0.4206 valid loss: 0.4721, valid accuracy: 0.8698\n",
      "Iter-4160 train loss: 0.5619 valid loss: 0.4716, valid accuracy: 0.8700\n",
      "Iter-4170 train loss: 0.6091 valid loss: 0.4712, valid accuracy: 0.8692\n",
      "Iter-4180 train loss: 0.3079 valid loss: 0.4707, valid accuracy: 0.8700\n",
      "Iter-4190 train loss: 0.3814 valid loss: 0.4702, valid accuracy: 0.8700\n",
      "Iter-4200 train loss: 0.5292 valid loss: 0.4698, valid accuracy: 0.8700\n",
      "Iter-4210 train loss: 0.4499 valid loss: 0.4693, valid accuracy: 0.8704\n",
      "Iter-4220 train loss: 0.4819 valid loss: 0.4688, valid accuracy: 0.8700\n",
      "Iter-4230 train loss: 0.5546 valid loss: 0.4683, valid accuracy: 0.8700\n",
      "Iter-4240 train loss: 0.5173 valid loss: 0.4680, valid accuracy: 0.8700\n",
      "Iter-4250 train loss: 0.4013 valid loss: 0.4674, valid accuracy: 0.8706\n",
      "Iter-4260 train loss: 0.3824 valid loss: 0.4671, valid accuracy: 0.8702\n",
      "Iter-4270 train loss: 0.6915 valid loss: 0.4665, valid accuracy: 0.8706\n",
      "Iter-4280 train loss: 0.4958 valid loss: 0.4661, valid accuracy: 0.8706\n",
      "Iter-4290 train loss: 0.3160 valid loss: 0.4656, valid accuracy: 0.8704\n",
      "Iter-4300 train loss: 0.3643 valid loss: 0.4653, valid accuracy: 0.8710\n",
      "Iter-4310 train loss: 0.6957 valid loss: 0.4648, valid accuracy: 0.8716\n",
      "Iter-4320 train loss: 0.6787 valid loss: 0.4644, valid accuracy: 0.8716\n",
      "Iter-4330 train loss: 0.5411 valid loss: 0.4641, valid accuracy: 0.8704\n",
      "Iter-4340 train loss: 0.3912 valid loss: 0.4636, valid accuracy: 0.8708\n",
      "Iter-4350 train loss: 0.5207 valid loss: 0.4633, valid accuracy: 0.8704\n",
      "Iter-4360 train loss: 0.3872 valid loss: 0.4628, valid accuracy: 0.8706\n",
      "Iter-4370 train loss: 0.4230 valid loss: 0.4626, valid accuracy: 0.8702\n",
      "Iter-4380 train loss: 0.4703 valid loss: 0.4620, valid accuracy: 0.8708\n",
      "Iter-4390 train loss: 0.4775 valid loss: 0.4614, valid accuracy: 0.8714\n",
      "Iter-4400 train loss: 0.5141 valid loss: 0.4610, valid accuracy: 0.8710\n",
      "Iter-4410 train loss: 0.4925 valid loss: 0.4608, valid accuracy: 0.8708\n",
      "Iter-4420 train loss: 0.4289 valid loss: 0.4604, valid accuracy: 0.8716\n",
      "Iter-4430 train loss: 0.5069 valid loss: 0.4598, valid accuracy: 0.8720\n",
      "Iter-4440 train loss: 0.5448 valid loss: 0.4594, valid accuracy: 0.8712\n",
      "Iter-4450 train loss: 0.4300 valid loss: 0.4589, valid accuracy: 0.8712\n",
      "Iter-4460 train loss: 0.3084 valid loss: 0.4586, valid accuracy: 0.8714\n",
      "Iter-4470 train loss: 0.3468 valid loss: 0.4582, valid accuracy: 0.8718\n",
      "Iter-4480 train loss: 0.4986 valid loss: 0.4579, valid accuracy: 0.8716\n",
      "Iter-4490 train loss: 0.5298 valid loss: 0.4574, valid accuracy: 0.8714\n",
      "Iter-4500 train loss: 0.5196 valid loss: 0.4569, valid accuracy: 0.8716\n",
      "Iter-4510 train loss: 0.4322 valid loss: 0.4565, valid accuracy: 0.8722\n",
      "Iter-4520 train loss: 0.5506 valid loss: 0.4560, valid accuracy: 0.8730\n",
      "Iter-4530 train loss: 0.3747 valid loss: 0.4557, valid accuracy: 0.8724\n",
      "Iter-4540 train loss: 0.5162 valid loss: 0.4554, valid accuracy: 0.8726\n",
      "Iter-4550 train loss: 0.3495 valid loss: 0.4550, valid accuracy: 0.8726\n",
      "Iter-4560 train loss: 0.5721 valid loss: 0.4547, valid accuracy: 0.8730\n",
      "Iter-4570 train loss: 0.6763 valid loss: 0.4543, valid accuracy: 0.8736\n",
      "Iter-4580 train loss: 0.5312 valid loss: 0.4539, valid accuracy: 0.8738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 0.3770 valid loss: 0.4536, valid accuracy: 0.8732\n",
      "Iter-4600 train loss: 0.3704 valid loss: 0.4533, valid accuracy: 0.8730\n",
      "Iter-4610 train loss: 0.5869 valid loss: 0.4527, valid accuracy: 0.8728\n",
      "Iter-4620 train loss: 0.5228 valid loss: 0.4523, valid accuracy: 0.8732\n",
      "Iter-4630 train loss: 0.5058 valid loss: 0.4517, valid accuracy: 0.8730\n",
      "Iter-4640 train loss: 0.4050 valid loss: 0.4513, valid accuracy: 0.8728\n",
      "Iter-4650 train loss: 0.3949 valid loss: 0.4509, valid accuracy: 0.8730\n",
      "Iter-4660 train loss: 0.2986 valid loss: 0.4504, valid accuracy: 0.8730\n",
      "Iter-4670 train loss: 0.3895 valid loss: 0.4500, valid accuracy: 0.8734\n",
      "Iter-4680 train loss: 0.4508 valid loss: 0.4496, valid accuracy: 0.8730\n",
      "Iter-4690 train loss: 0.4544 valid loss: 0.4492, valid accuracy: 0.8734\n",
      "Iter-4700 train loss: 0.5522 valid loss: 0.4489, valid accuracy: 0.8730\n",
      "Iter-4710 train loss: 0.3857 valid loss: 0.4486, valid accuracy: 0.8732\n",
      "Iter-4720 train loss: 0.6644 valid loss: 0.4482, valid accuracy: 0.8728\n",
      "Iter-4730 train loss: 0.4873 valid loss: 0.4477, valid accuracy: 0.8738\n",
      "Iter-4740 train loss: 0.4447 valid loss: 0.4474, valid accuracy: 0.8742\n",
      "Iter-4750 train loss: 0.4947 valid loss: 0.4470, valid accuracy: 0.8746\n",
      "Iter-4760 train loss: 0.5948 valid loss: 0.4471, valid accuracy: 0.8756\n",
      "Iter-4770 train loss: 0.6039 valid loss: 0.4467, valid accuracy: 0.8760\n",
      "Iter-4780 train loss: 0.5121 valid loss: 0.4461, valid accuracy: 0.8752\n",
      "Iter-4790 train loss: 0.3687 valid loss: 0.4456, valid accuracy: 0.8750\n",
      "Iter-4800 train loss: 0.5418 valid loss: 0.4452, valid accuracy: 0.8750\n",
      "Iter-4810 train loss: 0.5276 valid loss: 0.4449, valid accuracy: 0.8746\n",
      "Iter-4820 train loss: 0.3899 valid loss: 0.4446, valid accuracy: 0.8752\n",
      "Iter-4830 train loss: 0.4304 valid loss: 0.4443, valid accuracy: 0.8756\n",
      "Iter-4840 train loss: 0.4642 valid loss: 0.4440, valid accuracy: 0.8758\n",
      "Iter-4850 train loss: 0.3186 valid loss: 0.4436, valid accuracy: 0.8752\n",
      "Iter-4860 train loss: 0.5102 valid loss: 0.4433, valid accuracy: 0.8758\n",
      "Iter-4870 train loss: 0.5117 valid loss: 0.4429, valid accuracy: 0.8754\n",
      "Iter-4880 train loss: 0.3731 valid loss: 0.4426, valid accuracy: 0.8762\n",
      "Iter-4890 train loss: 0.4670 valid loss: 0.4423, valid accuracy: 0.8760\n",
      "Iter-4900 train loss: 0.3386 valid loss: 0.4421, valid accuracy: 0.8758\n",
      "Iter-4910 train loss: 0.3583 valid loss: 0.4416, valid accuracy: 0.8764\n",
      "Iter-4920 train loss: 0.3644 valid loss: 0.4411, valid accuracy: 0.8756\n",
      "Iter-4930 train loss: 0.4711 valid loss: 0.4408, valid accuracy: 0.8758\n",
      "Iter-4940 train loss: 0.5422 valid loss: 0.4405, valid accuracy: 0.8762\n",
      "Iter-4950 train loss: 0.4421 valid loss: 0.4403, valid accuracy: 0.8764\n",
      "Iter-4960 train loss: 0.4010 valid loss: 0.4400, valid accuracy: 0.8760\n",
      "Iter-4970 train loss: 0.4603 valid loss: 0.4398, valid accuracy: 0.8760\n",
      "Iter-4980 train loss: 0.6453 valid loss: 0.4395, valid accuracy: 0.8768\n",
      "Iter-4990 train loss: 0.3738 valid loss: 0.4391, valid accuracy: 0.8766\n",
      "Iter-5000 train loss: 0.5930 valid loss: 0.4386, valid accuracy: 0.8762\n",
      "Iter-5010 train loss: 0.6569 valid loss: 0.4382, valid accuracy: 0.8768\n",
      "Iter-5020 train loss: 0.4203 valid loss: 0.4380, valid accuracy: 0.8764\n",
      "Iter-5030 train loss: 0.4554 valid loss: 0.4375, valid accuracy: 0.8766\n",
      "Iter-5040 train loss: 0.5093 valid loss: 0.4372, valid accuracy: 0.8762\n",
      "Iter-5050 train loss: 0.3778 valid loss: 0.4369, valid accuracy: 0.8762\n",
      "Iter-5060 train loss: 0.4808 valid loss: 0.4365, valid accuracy: 0.8768\n",
      "Iter-5070 train loss: 0.5710 valid loss: 0.4362, valid accuracy: 0.8768\n",
      "Iter-5080 train loss: 0.2575 valid loss: 0.4360, valid accuracy: 0.8772\n",
      "Iter-5090 train loss: 0.4453 valid loss: 0.4357, valid accuracy: 0.8774\n",
      "Iter-5100 train loss: 0.3880 valid loss: 0.4353, valid accuracy: 0.8768\n",
      "Iter-5110 train loss: 0.5413 valid loss: 0.4351, valid accuracy: 0.8766\n",
      "Iter-5120 train loss: 0.4662 valid loss: 0.4346, valid accuracy: 0.8770\n",
      "Iter-5130 train loss: 0.4635 valid loss: 0.4343, valid accuracy: 0.8768\n",
      "Iter-5140 train loss: 0.3359 valid loss: 0.4339, valid accuracy: 0.8762\n",
      "Iter-5150 train loss: 0.3562 valid loss: 0.4335, valid accuracy: 0.8762\n",
      "Iter-5160 train loss: 0.4407 valid loss: 0.4332, valid accuracy: 0.8768\n",
      "Iter-5170 train loss: 0.5785 valid loss: 0.4328, valid accuracy: 0.8772\n",
      "Iter-5180 train loss: 0.3077 valid loss: 0.4325, valid accuracy: 0.8772\n",
      "Iter-5190 train loss: 0.3786 valid loss: 0.4322, valid accuracy: 0.8780\n",
      "Iter-5200 train loss: 0.4893 valid loss: 0.4318, valid accuracy: 0.8786\n",
      "Iter-5210 train loss: 0.3502 valid loss: 0.4316, valid accuracy: 0.8790\n",
      "Iter-5220 train loss: 0.4720 valid loss: 0.4314, valid accuracy: 0.8786\n",
      "Iter-5230 train loss: 0.3931 valid loss: 0.4311, valid accuracy: 0.8786\n",
      "Iter-5240 train loss: 0.3410 valid loss: 0.4307, valid accuracy: 0.8784\n",
      "Iter-5250 train loss: 0.5464 valid loss: 0.4304, valid accuracy: 0.8794\n",
      "Iter-5260 train loss: 0.3343 valid loss: 0.4300, valid accuracy: 0.8806\n",
      "Iter-5270 train loss: 0.4657 valid loss: 0.4297, valid accuracy: 0.8802\n",
      "Iter-5280 train loss: 0.3086 valid loss: 0.4294, valid accuracy: 0.8802\n",
      "Iter-5290 train loss: 0.3496 valid loss: 0.4291, valid accuracy: 0.8796\n",
      "Iter-5300 train loss: 0.4317 valid loss: 0.4287, valid accuracy: 0.8800\n",
      "Iter-5310 train loss: 0.7534 valid loss: 0.4283, valid accuracy: 0.8800\n",
      "Iter-5320 train loss: 0.4283 valid loss: 0.4280, valid accuracy: 0.8790\n",
      "Iter-5330 train loss: 0.6894 valid loss: 0.4277, valid accuracy: 0.8796\n",
      "Iter-5340 train loss: 0.4439 valid loss: 0.4273, valid accuracy: 0.8800\n",
      "Iter-5350 train loss: 0.6347 valid loss: 0.4271, valid accuracy: 0.8800\n",
      "Iter-5360 train loss: 0.4653 valid loss: 0.4269, valid accuracy: 0.8798\n",
      "Iter-5370 train loss: 0.3773 valid loss: 0.4266, valid accuracy: 0.8794\n",
      "Iter-5380 train loss: 0.4848 valid loss: 0.4265, valid accuracy: 0.8796\n",
      "Iter-5390 train loss: 0.4240 valid loss: 0.4261, valid accuracy: 0.8796\n",
      "Iter-5400 train loss: 0.3357 valid loss: 0.4259, valid accuracy: 0.8796\n",
      "Iter-5410 train loss: 0.4278 valid loss: 0.4258, valid accuracy: 0.8800\n",
      "Iter-5420 train loss: 0.3716 valid loss: 0.4255, valid accuracy: 0.8796\n",
      "Iter-5430 train loss: 0.4564 valid loss: 0.4250, valid accuracy: 0.8794\n",
      "Iter-5440 train loss: 0.4808 valid loss: 0.4248, valid accuracy: 0.8792\n",
      "Iter-5450 train loss: 0.3600 valid loss: 0.4245, valid accuracy: 0.8804\n",
      "Iter-5460 train loss: 0.3390 valid loss: 0.4241, valid accuracy: 0.8806\n",
      "Iter-5470 train loss: 0.4657 valid loss: 0.4239, valid accuracy: 0.8800\n",
      "Iter-5480 train loss: 0.4142 valid loss: 0.4237, valid accuracy: 0.8806\n",
      "Iter-5490 train loss: 0.4600 valid loss: 0.4234, valid accuracy: 0.8798\n",
      "Iter-5500 train loss: 0.5106 valid loss: 0.4229, valid accuracy: 0.8802\n",
      "Iter-5510 train loss: 0.2816 valid loss: 0.4226, valid accuracy: 0.8804\n",
      "Iter-5520 train loss: 0.3581 valid loss: 0.4223, valid accuracy: 0.8800\n",
      "Iter-5530 train loss: 0.3891 valid loss: 0.4220, valid accuracy: 0.8804\n",
      "Iter-5540 train loss: 0.4291 valid loss: 0.4217, valid accuracy: 0.8808\n",
      "Iter-5550 train loss: 0.2589 valid loss: 0.4213, valid accuracy: 0.8806\n",
      "Iter-5560 train loss: 0.4868 valid loss: 0.4211, valid accuracy: 0.8806\n",
      "Iter-5570 train loss: 0.4209 valid loss: 0.4207, valid accuracy: 0.8810\n",
      "Iter-5580 train loss: 0.3552 valid loss: 0.4204, valid accuracy: 0.8808\n",
      "Iter-5590 train loss: 0.4459 valid loss: 0.4201, valid accuracy: 0.8810\n",
      "Iter-5600 train loss: 0.4107 valid loss: 0.4199, valid accuracy: 0.8806\n",
      "Iter-5610 train loss: 0.5138 valid loss: 0.4196, valid accuracy: 0.8808\n",
      "Iter-5620 train loss: 0.3539 valid loss: 0.4194, valid accuracy: 0.8808\n",
      "Iter-5630 train loss: 0.4419 valid loss: 0.4192, valid accuracy: 0.8810\n",
      "Iter-5640 train loss: 0.3479 valid loss: 0.4190, valid accuracy: 0.8812\n",
      "Iter-5650 train loss: 0.4878 valid loss: 0.4189, valid accuracy: 0.8816\n",
      "Iter-5660 train loss: 0.4421 valid loss: 0.4186, valid accuracy: 0.8812\n",
      "Iter-5670 train loss: 0.3597 valid loss: 0.4183, valid accuracy: 0.8818\n",
      "Iter-5680 train loss: 0.5550 valid loss: 0.4179, valid accuracy: 0.8812\n",
      "Iter-5690 train loss: 0.4156 valid loss: 0.4174, valid accuracy: 0.8820\n",
      "Iter-5700 train loss: 0.3646 valid loss: 0.4172, valid accuracy: 0.8818\n",
      "Iter-5710 train loss: 0.3785 valid loss: 0.4168, valid accuracy: 0.8814\n",
      "Iter-5720 train loss: 0.4680 valid loss: 0.4165, valid accuracy: 0.8818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 0.5561 valid loss: 0.4165, valid accuracy: 0.8816\n",
      "Iter-5740 train loss: 0.4709 valid loss: 0.4163, valid accuracy: 0.8814\n",
      "Iter-5750 train loss: 0.3521 valid loss: 0.4161, valid accuracy: 0.8810\n",
      "Iter-5760 train loss: 0.4629 valid loss: 0.4158, valid accuracy: 0.8812\n",
      "Iter-5770 train loss: 0.5420 valid loss: 0.4154, valid accuracy: 0.8820\n",
      "Iter-5780 train loss: 0.4285 valid loss: 0.4153, valid accuracy: 0.8826\n",
      "Iter-5790 train loss: 0.4117 valid loss: 0.4149, valid accuracy: 0.8820\n",
      "Iter-5800 train loss: 0.4488 valid loss: 0.4147, valid accuracy: 0.8818\n",
      "Iter-5810 train loss: 0.5024 valid loss: 0.4143, valid accuracy: 0.8824\n",
      "Iter-5820 train loss: 0.3740 valid loss: 0.4142, valid accuracy: 0.8824\n",
      "Iter-5830 train loss: 0.4294 valid loss: 0.4138, valid accuracy: 0.8824\n",
      "Iter-5840 train loss: 0.3523 valid loss: 0.4135, valid accuracy: 0.8822\n",
      "Iter-5850 train loss: 0.5115 valid loss: 0.4132, valid accuracy: 0.8824\n",
      "Iter-5860 train loss: 0.2434 valid loss: 0.4129, valid accuracy: 0.8830\n",
      "Iter-5870 train loss: 0.3599 valid loss: 0.4129, valid accuracy: 0.8824\n",
      "Iter-5880 train loss: 0.4470 valid loss: 0.4127, valid accuracy: 0.8826\n",
      "Iter-5890 train loss: 0.3113 valid loss: 0.4127, valid accuracy: 0.8826\n",
      "Iter-5900 train loss: 0.3350 valid loss: 0.4124, valid accuracy: 0.8824\n",
      "Iter-5910 train loss: 0.4287 valid loss: 0.4120, valid accuracy: 0.8822\n",
      "Iter-5920 train loss: 0.4524 valid loss: 0.4116, valid accuracy: 0.8834\n",
      "Iter-5930 train loss: 0.4525 valid loss: 0.4111, valid accuracy: 0.8832\n",
      "Iter-5940 train loss: 0.3280 valid loss: 0.4107, valid accuracy: 0.8834\n",
      "Iter-5950 train loss: 0.3958 valid loss: 0.4105, valid accuracy: 0.8830\n",
      "Iter-5960 train loss: 0.7449 valid loss: 0.4102, valid accuracy: 0.8828\n",
      "Iter-5970 train loss: 0.3829 valid loss: 0.4101, valid accuracy: 0.8832\n",
      "Iter-5980 train loss: 0.3605 valid loss: 0.4097, valid accuracy: 0.8834\n",
      "Iter-5990 train loss: 0.4520 valid loss: 0.4094, valid accuracy: 0.8820\n",
      "Iter-6000 train loss: 0.5585 valid loss: 0.4089, valid accuracy: 0.8834\n",
      "Iter-6010 train loss: 0.4670 valid loss: 0.4088, valid accuracy: 0.8830\n",
      "Iter-6020 train loss: 0.3856 valid loss: 0.4085, valid accuracy: 0.8830\n",
      "Iter-6030 train loss: 0.2812 valid loss: 0.4083, valid accuracy: 0.8832\n",
      "Iter-6040 train loss: 0.4360 valid loss: 0.4081, valid accuracy: 0.8832\n",
      "Iter-6050 train loss: 0.5125 valid loss: 0.4078, valid accuracy: 0.8828\n",
      "Iter-6060 train loss: 0.5199 valid loss: 0.4076, valid accuracy: 0.8832\n",
      "Iter-6070 train loss: 0.4194 valid loss: 0.4075, valid accuracy: 0.8834\n",
      "Iter-6080 train loss: 0.4545 valid loss: 0.4072, valid accuracy: 0.8832\n",
      "Iter-6090 train loss: 0.4220 valid loss: 0.4071, valid accuracy: 0.8830\n",
      "Iter-6100 train loss: 0.3648 valid loss: 0.4067, valid accuracy: 0.8832\n",
      "Iter-6110 train loss: 0.3171 valid loss: 0.4065, valid accuracy: 0.8832\n",
      "Iter-6120 train loss: 0.5836 valid loss: 0.4063, valid accuracy: 0.8838\n",
      "Iter-6130 train loss: 0.4134 valid loss: 0.4061, valid accuracy: 0.8840\n",
      "Iter-6140 train loss: 0.3934 valid loss: 0.4058, valid accuracy: 0.8840\n",
      "Iter-6150 train loss: 0.5359 valid loss: 0.4057, valid accuracy: 0.8842\n",
      "Iter-6160 train loss: 0.3127 valid loss: 0.4054, valid accuracy: 0.8848\n",
      "Iter-6170 train loss: 0.5316 valid loss: 0.4052, valid accuracy: 0.8838\n",
      "Iter-6180 train loss: 0.5045 valid loss: 0.4049, valid accuracy: 0.8834\n",
      "Iter-6190 train loss: 0.4601 valid loss: 0.4047, valid accuracy: 0.8840\n",
      "Iter-6200 train loss: 0.4454 valid loss: 0.4044, valid accuracy: 0.8842\n",
      "Iter-6210 train loss: 0.5583 valid loss: 0.4041, valid accuracy: 0.8840\n",
      "Iter-6220 train loss: 0.5063 valid loss: 0.4039, valid accuracy: 0.8842\n",
      "Iter-6230 train loss: 0.3476 valid loss: 0.4037, valid accuracy: 0.8836\n",
      "Iter-6240 train loss: 0.5173 valid loss: 0.4033, valid accuracy: 0.8838\n",
      "Iter-6250 train loss: 0.2961 valid loss: 0.4032, valid accuracy: 0.8844\n",
      "Iter-6260 train loss: 0.4507 valid loss: 0.4029, valid accuracy: 0.8846\n",
      "Iter-6270 train loss: 0.4048 valid loss: 0.4027, valid accuracy: 0.8840\n",
      "Iter-6280 train loss: 0.3843 valid loss: 0.4024, valid accuracy: 0.8848\n",
      "Iter-6290 train loss: 0.3583 valid loss: 0.4021, valid accuracy: 0.8834\n",
      "Iter-6300 train loss: 0.4406 valid loss: 0.4018, valid accuracy: 0.8840\n",
      "Iter-6310 train loss: 0.4545 valid loss: 0.4015, valid accuracy: 0.8842\n",
      "Iter-6320 train loss: 0.3582 valid loss: 0.4013, valid accuracy: 0.8842\n",
      "Iter-6330 train loss: 0.3609 valid loss: 0.4010, valid accuracy: 0.8846\n",
      "Iter-6340 train loss: 0.4432 valid loss: 0.4007, valid accuracy: 0.8844\n",
      "Iter-6350 train loss: 0.3179 valid loss: 0.4006, valid accuracy: 0.8850\n",
      "Iter-6360 train loss: 0.6029 valid loss: 0.4004, valid accuracy: 0.8850\n",
      "Iter-6370 train loss: 0.3145 valid loss: 0.4001, valid accuracy: 0.8848\n",
      "Iter-6380 train loss: 0.4998 valid loss: 0.3998, valid accuracy: 0.8850\n",
      "Iter-6390 train loss: 0.3593 valid loss: 0.3996, valid accuracy: 0.8850\n",
      "Iter-6400 train loss: 0.3358 valid loss: 0.3993, valid accuracy: 0.8856\n",
      "Iter-6410 train loss: 0.4407 valid loss: 0.3991, valid accuracy: 0.8852\n",
      "Iter-6420 train loss: 0.5496 valid loss: 0.3989, valid accuracy: 0.8846\n",
      "Iter-6430 train loss: 0.5011 valid loss: 0.3988, valid accuracy: 0.8856\n",
      "Iter-6440 train loss: 0.4383 valid loss: 0.3987, valid accuracy: 0.8852\n",
      "Iter-6450 train loss: 0.3427 valid loss: 0.3984, valid accuracy: 0.8854\n",
      "Iter-6460 train loss: 0.5480 valid loss: 0.3981, valid accuracy: 0.8850\n",
      "Iter-6470 train loss: 0.5083 valid loss: 0.3978, valid accuracy: 0.8848\n",
      "Iter-6480 train loss: 0.4046 valid loss: 0.3977, valid accuracy: 0.8856\n",
      "Iter-6490 train loss: 0.3721 valid loss: 0.3975, valid accuracy: 0.8854\n",
      "Iter-6500 train loss: 0.6283 valid loss: 0.3972, valid accuracy: 0.8858\n",
      "Iter-6510 train loss: 0.4880 valid loss: 0.3970, valid accuracy: 0.8854\n",
      "Iter-6520 train loss: 0.5603 valid loss: 0.3967, valid accuracy: 0.8858\n",
      "Iter-6530 train loss: 0.4394 valid loss: 0.3965, valid accuracy: 0.8858\n",
      "Iter-6540 train loss: 0.6146 valid loss: 0.3964, valid accuracy: 0.8858\n",
      "Iter-6550 train loss: 0.4148 valid loss: 0.3961, valid accuracy: 0.8862\n",
      "Iter-6560 train loss: 0.3358 valid loss: 0.3959, valid accuracy: 0.8860\n",
      "Iter-6570 train loss: 0.3454 valid loss: 0.3957, valid accuracy: 0.8852\n",
      "Iter-6580 train loss: 0.4105 valid loss: 0.3953, valid accuracy: 0.8868\n",
      "Iter-6590 train loss: 0.5966 valid loss: 0.3952, valid accuracy: 0.8860\n",
      "Iter-6600 train loss: 0.3123 valid loss: 0.3949, valid accuracy: 0.8870\n",
      "Iter-6610 train loss: 0.3916 valid loss: 0.3947, valid accuracy: 0.8866\n",
      "Iter-6620 train loss: 0.3073 valid loss: 0.3945, valid accuracy: 0.8874\n",
      "Iter-6630 train loss: 0.4411 valid loss: 0.3943, valid accuracy: 0.8868\n",
      "Iter-6640 train loss: 0.3772 valid loss: 0.3940, valid accuracy: 0.8864\n",
      "Iter-6650 train loss: 0.6063 valid loss: 0.3937, valid accuracy: 0.8868\n",
      "Iter-6660 train loss: 0.4713 valid loss: 0.3936, valid accuracy: 0.8868\n",
      "Iter-6670 train loss: 0.4875 valid loss: 0.3936, valid accuracy: 0.8866\n",
      "Iter-6680 train loss: 0.5329 valid loss: 0.3934, valid accuracy: 0.8866\n",
      "Iter-6690 train loss: 0.3050 valid loss: 0.3932, valid accuracy: 0.8864\n",
      "Iter-6700 train loss: 0.3633 valid loss: 0.3931, valid accuracy: 0.8866\n",
      "Iter-6710 train loss: 0.3816 valid loss: 0.3929, valid accuracy: 0.8872\n",
      "Iter-6720 train loss: 0.3128 valid loss: 0.3927, valid accuracy: 0.8874\n",
      "Iter-6730 train loss: 0.3940 valid loss: 0.3926, valid accuracy: 0.8876\n",
      "Iter-6740 train loss: 0.5785 valid loss: 0.3923, valid accuracy: 0.8876\n",
      "Iter-6750 train loss: 0.4725 valid loss: 0.3921, valid accuracy: 0.8878\n",
      "Iter-6760 train loss: 0.3396 valid loss: 0.3917, valid accuracy: 0.8876\n",
      "Iter-6770 train loss: 0.3380 valid loss: 0.3916, valid accuracy: 0.8878\n",
      "Iter-6780 train loss: 0.3278 valid loss: 0.3913, valid accuracy: 0.8874\n",
      "Iter-6790 train loss: 0.4698 valid loss: 0.3912, valid accuracy: 0.8872\n",
      "Iter-6800 train loss: 0.2457 valid loss: 0.3909, valid accuracy: 0.8870\n",
      "Iter-6810 train loss: 0.4550 valid loss: 0.3908, valid accuracy: 0.8872\n",
      "Iter-6820 train loss: 0.3250 valid loss: 0.3906, valid accuracy: 0.8874\n",
      "Iter-6830 train loss: 0.3866 valid loss: 0.3905, valid accuracy: 0.8872\n",
      "Iter-6840 train loss: 0.5320 valid loss: 0.3903, valid accuracy: 0.8874\n",
      "Iter-6850 train loss: 0.3568 valid loss: 0.3901, valid accuracy: 0.8872\n",
      "Iter-6860 train loss: 0.5291 valid loss: 0.3900, valid accuracy: 0.8868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 0.5013 valid loss: 0.3898, valid accuracy: 0.8876\n",
      "Iter-6880 train loss: 0.4082 valid loss: 0.3895, valid accuracy: 0.8878\n",
      "Iter-6890 train loss: 0.4167 valid loss: 0.3892, valid accuracy: 0.8878\n",
      "Iter-6900 train loss: 0.2891 valid loss: 0.3891, valid accuracy: 0.8882\n",
      "Iter-6910 train loss: 0.3556 valid loss: 0.3889, valid accuracy: 0.8880\n",
      "Iter-6920 train loss: 0.4019 valid loss: 0.3887, valid accuracy: 0.8882\n",
      "Iter-6930 train loss: 0.6046 valid loss: 0.3885, valid accuracy: 0.8882\n",
      "Iter-6940 train loss: 0.3395 valid loss: 0.3883, valid accuracy: 0.8882\n",
      "Iter-6950 train loss: 0.3955 valid loss: 0.3880, valid accuracy: 0.8882\n",
      "Iter-6960 train loss: 0.5226 valid loss: 0.3878, valid accuracy: 0.8878\n",
      "Iter-6970 train loss: 0.3854 valid loss: 0.3877, valid accuracy: 0.8886\n",
      "Iter-6980 train loss: 0.4216 valid loss: 0.3875, valid accuracy: 0.8882\n",
      "Iter-6990 train loss: 0.3596 valid loss: 0.3873, valid accuracy: 0.8882\n",
      "Iter-7000 train loss: 0.4081 valid loss: 0.3871, valid accuracy: 0.8882\n",
      "Iter-7010 train loss: 0.4216 valid loss: 0.3869, valid accuracy: 0.8884\n",
      "Iter-7020 train loss: 0.4401 valid loss: 0.3867, valid accuracy: 0.8884\n",
      "Iter-7030 train loss: 0.4784 valid loss: 0.3865, valid accuracy: 0.8884\n",
      "Iter-7040 train loss: 0.5301 valid loss: 0.3864, valid accuracy: 0.8886\n",
      "Iter-7050 train loss: 0.4931 valid loss: 0.3862, valid accuracy: 0.8886\n",
      "Iter-7060 train loss: 0.3938 valid loss: 0.3861, valid accuracy: 0.8882\n",
      "Iter-7070 train loss: 0.6289 valid loss: 0.3859, valid accuracy: 0.8888\n",
      "Iter-7080 train loss: 0.3837 valid loss: 0.3857, valid accuracy: 0.8894\n",
      "Iter-7090 train loss: 0.4004 valid loss: 0.3854, valid accuracy: 0.8884\n",
      "Iter-7100 train loss: 0.3315 valid loss: 0.3854, valid accuracy: 0.8886\n",
      "Iter-7110 train loss: 0.3009 valid loss: 0.3853, valid accuracy: 0.8884\n",
      "Iter-7120 train loss: 0.3166 valid loss: 0.3853, valid accuracy: 0.8884\n",
      "Iter-7130 train loss: 0.4138 valid loss: 0.3852, valid accuracy: 0.8882\n",
      "Iter-7140 train loss: 0.5164 valid loss: 0.3849, valid accuracy: 0.8884\n",
      "Iter-7150 train loss: 0.3747 valid loss: 0.3847, valid accuracy: 0.8888\n",
      "Iter-7160 train loss: 0.5282 valid loss: 0.3844, valid accuracy: 0.8890\n",
      "Iter-7170 train loss: 0.4930 valid loss: 0.3841, valid accuracy: 0.8886\n",
      "Iter-7180 train loss: 0.3015 valid loss: 0.3840, valid accuracy: 0.8882\n",
      "Iter-7190 train loss: 0.3181 valid loss: 0.3838, valid accuracy: 0.8884\n",
      "Iter-7200 train loss: 0.5388 valid loss: 0.3836, valid accuracy: 0.8884\n",
      "Iter-7210 train loss: 0.4472 valid loss: 0.3835, valid accuracy: 0.8888\n",
      "Iter-7220 train loss: 0.2694 valid loss: 0.3833, valid accuracy: 0.8880\n",
      "Iter-7230 train loss: 0.5145 valid loss: 0.3831, valid accuracy: 0.8888\n",
      "Iter-7240 train loss: 0.4714 valid loss: 0.3829, valid accuracy: 0.8886\n",
      "Iter-7250 train loss: 0.4133 valid loss: 0.3829, valid accuracy: 0.8888\n",
      "Iter-7260 train loss: 0.4116 valid loss: 0.3827, valid accuracy: 0.8892\n",
      "Iter-7270 train loss: 0.4413 valid loss: 0.3823, valid accuracy: 0.8894\n",
      "Iter-7280 train loss: 0.3810 valid loss: 0.3821, valid accuracy: 0.8898\n",
      "Iter-7290 train loss: 0.4238 valid loss: 0.3819, valid accuracy: 0.8896\n",
      "Iter-7300 train loss: 0.4366 valid loss: 0.3817, valid accuracy: 0.8902\n",
      "Iter-7310 train loss: 0.4495 valid loss: 0.3817, valid accuracy: 0.8894\n",
      "Iter-7320 train loss: 0.2184 valid loss: 0.3816, valid accuracy: 0.8896\n",
      "Iter-7330 train loss: 0.3265 valid loss: 0.3815, valid accuracy: 0.8896\n",
      "Iter-7340 train loss: 0.2937 valid loss: 0.3811, valid accuracy: 0.8894\n",
      "Iter-7350 train loss: 0.2540 valid loss: 0.3810, valid accuracy: 0.8902\n",
      "Iter-7360 train loss: 0.3119 valid loss: 0.3809, valid accuracy: 0.8898\n",
      "Iter-7370 train loss: 0.2264 valid loss: 0.3808, valid accuracy: 0.8896\n",
      "Iter-7380 train loss: 0.3294 valid loss: 0.3807, valid accuracy: 0.8898\n",
      "Iter-7390 train loss: 0.4912 valid loss: 0.3804, valid accuracy: 0.8898\n",
      "Iter-7400 train loss: 0.4568 valid loss: 0.3802, valid accuracy: 0.8900\n",
      "Iter-7410 train loss: 0.5639 valid loss: 0.3800, valid accuracy: 0.8910\n",
      "Iter-7420 train loss: 0.5314 valid loss: 0.3799, valid accuracy: 0.8904\n",
      "Iter-7430 train loss: 0.4301 valid loss: 0.3798, valid accuracy: 0.8910\n",
      "Iter-7440 train loss: 0.4012 valid loss: 0.3795, valid accuracy: 0.8900\n",
      "Iter-7450 train loss: 0.1875 valid loss: 0.3793, valid accuracy: 0.8912\n",
      "Iter-7460 train loss: 0.2266 valid loss: 0.3790, valid accuracy: 0.8912\n",
      "Iter-7470 train loss: 0.2079 valid loss: 0.3790, valid accuracy: 0.8906\n",
      "Iter-7480 train loss: 0.4153 valid loss: 0.3788, valid accuracy: 0.8908\n",
      "Iter-7490 train loss: 0.3562 valid loss: 0.3787, valid accuracy: 0.8908\n",
      "Iter-7500 train loss: 0.4214 valid loss: 0.3785, valid accuracy: 0.8906\n",
      "Iter-7510 train loss: 0.3579 valid loss: 0.3783, valid accuracy: 0.8904\n",
      "Iter-7520 train loss: 0.3399 valid loss: 0.3781, valid accuracy: 0.8906\n",
      "Iter-7530 train loss: 0.3704 valid loss: 0.3780, valid accuracy: 0.8906\n",
      "Iter-7540 train loss: 0.3040 valid loss: 0.3778, valid accuracy: 0.8908\n",
      "Iter-7550 train loss: 0.5410 valid loss: 0.3777, valid accuracy: 0.8906\n",
      "Iter-7560 train loss: 0.4149 valid loss: 0.3775, valid accuracy: 0.8912\n",
      "Iter-7570 train loss: 0.4149 valid loss: 0.3773, valid accuracy: 0.8904\n",
      "Iter-7580 train loss: 0.2531 valid loss: 0.3772, valid accuracy: 0.8908\n",
      "Iter-7590 train loss: 0.3112 valid loss: 0.3771, valid accuracy: 0.8908\n",
      "Iter-7600 train loss: 0.2692 valid loss: 0.3769, valid accuracy: 0.8916\n",
      "Iter-7610 train loss: 0.4052 valid loss: 0.3768, valid accuracy: 0.8916\n",
      "Iter-7620 train loss: 0.4086 valid loss: 0.3767, valid accuracy: 0.8910\n",
      "Iter-7630 train loss: 0.3735 valid loss: 0.3762, valid accuracy: 0.8916\n",
      "Iter-7640 train loss: 0.3161 valid loss: 0.3760, valid accuracy: 0.8914\n",
      "Iter-7650 train loss: 0.4943 valid loss: 0.3757, valid accuracy: 0.8912\n",
      "Iter-7660 train loss: 0.4570 valid loss: 0.3756, valid accuracy: 0.8914\n",
      "Iter-7670 train loss: 0.5554 valid loss: 0.3755, valid accuracy: 0.8912\n",
      "Iter-7680 train loss: 0.2697 valid loss: 0.3752, valid accuracy: 0.8912\n",
      "Iter-7690 train loss: 0.3853 valid loss: 0.3750, valid accuracy: 0.8910\n",
      "Iter-7700 train loss: 0.3807 valid loss: 0.3750, valid accuracy: 0.8914\n",
      "Iter-7710 train loss: 0.5488 valid loss: 0.3746, valid accuracy: 0.8916\n",
      "Iter-7720 train loss: 0.3248 valid loss: 0.3745, valid accuracy: 0.8920\n",
      "Iter-7730 train loss: 0.2275 valid loss: 0.3745, valid accuracy: 0.8924\n",
      "Iter-7740 train loss: 0.4800 valid loss: 0.3744, valid accuracy: 0.8920\n",
      "Iter-7750 train loss: 0.2966 valid loss: 0.3741, valid accuracy: 0.8922\n",
      "Iter-7760 train loss: 0.3485 valid loss: 0.3740, valid accuracy: 0.8920\n",
      "Iter-7770 train loss: 0.4385 valid loss: 0.3739, valid accuracy: 0.8920\n",
      "Iter-7780 train loss: 0.2858 valid loss: 0.3737, valid accuracy: 0.8920\n",
      "Iter-7790 train loss: 0.2830 valid loss: 0.3736, valid accuracy: 0.8924\n",
      "Iter-7800 train loss: 0.4630 valid loss: 0.3734, valid accuracy: 0.8924\n",
      "Iter-7810 train loss: 0.3554 valid loss: 0.3733, valid accuracy: 0.8924\n",
      "Iter-7820 train loss: 0.3074 valid loss: 0.3733, valid accuracy: 0.8924\n",
      "Iter-7830 train loss: 0.4264 valid loss: 0.3730, valid accuracy: 0.8930\n",
      "Iter-7840 train loss: 0.3620 valid loss: 0.3729, valid accuracy: 0.8926\n",
      "Iter-7850 train loss: 0.4404 valid loss: 0.3728, valid accuracy: 0.8930\n",
      "Iter-7860 train loss: 0.2018 valid loss: 0.3727, valid accuracy: 0.8930\n",
      "Iter-7870 train loss: 0.4529 valid loss: 0.3727, valid accuracy: 0.8940\n",
      "Iter-7880 train loss: 0.3255 valid loss: 0.3727, valid accuracy: 0.8936\n",
      "Iter-7890 train loss: 0.4931 valid loss: 0.3724, valid accuracy: 0.8940\n",
      "Iter-7900 train loss: 0.4857 valid loss: 0.3719, valid accuracy: 0.8932\n",
      "Iter-7910 train loss: 0.4348 valid loss: 0.3719, valid accuracy: 0.8938\n",
      "Iter-7920 train loss: 0.3239 valid loss: 0.3717, valid accuracy: 0.8938\n",
      "Iter-7930 train loss: 0.4603 valid loss: 0.3714, valid accuracy: 0.8938\n",
      "Iter-7940 train loss: 0.4420 valid loss: 0.3712, valid accuracy: 0.8934\n",
      "Iter-7950 train loss: 0.5630 valid loss: 0.3711, valid accuracy: 0.8928\n",
      "Iter-7960 train loss: 0.3933 valid loss: 0.3707, valid accuracy: 0.8926\n",
      "Iter-7970 train loss: 0.3397 valid loss: 0.3706, valid accuracy: 0.8924\n",
      "Iter-7980 train loss: 0.5212 valid loss: 0.3704, valid accuracy: 0.8926\n",
      "Iter-7990 train loss: 0.3613 valid loss: 0.3704, valid accuracy: 0.8928\n",
      "Iter-8000 train loss: 0.4219 valid loss: 0.3703, valid accuracy: 0.8924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 0.3479 valid loss: 0.3704, valid accuracy: 0.8926\n",
      "Iter-8020 train loss: 0.3914 valid loss: 0.3700, valid accuracy: 0.8926\n",
      "Iter-8030 train loss: 0.4018 valid loss: 0.3698, valid accuracy: 0.8924\n",
      "Iter-8040 train loss: 0.3501 valid loss: 0.3695, valid accuracy: 0.8928\n",
      "Iter-8050 train loss: 0.6034 valid loss: 0.3693, valid accuracy: 0.8924\n",
      "Iter-8060 train loss: 0.1787 valid loss: 0.3692, valid accuracy: 0.8924\n",
      "Iter-8070 train loss: 0.3149 valid loss: 0.3691, valid accuracy: 0.8930\n",
      "Iter-8080 train loss: 0.3874 valid loss: 0.3687, valid accuracy: 0.8922\n",
      "Iter-8090 train loss: 0.4706 valid loss: 0.3687, valid accuracy: 0.8932\n",
      "Iter-8100 train loss: 0.2462 valid loss: 0.3685, valid accuracy: 0.8930\n",
      "Iter-8110 train loss: 0.3032 valid loss: 0.3682, valid accuracy: 0.8930\n",
      "Iter-8120 train loss: 0.3950 valid loss: 0.3679, valid accuracy: 0.8930\n",
      "Iter-8130 train loss: 0.4295 valid loss: 0.3679, valid accuracy: 0.8932\n",
      "Iter-8140 train loss: 0.6209 valid loss: 0.3677, valid accuracy: 0.8930\n",
      "Iter-8150 train loss: 0.2813 valid loss: 0.3675, valid accuracy: 0.8932\n",
      "Iter-8160 train loss: 0.3135 valid loss: 0.3675, valid accuracy: 0.8928\n",
      "Iter-8170 train loss: 0.4226 valid loss: 0.3674, valid accuracy: 0.8934\n",
      "Iter-8180 train loss: 0.3564 valid loss: 0.3673, valid accuracy: 0.8934\n",
      "Iter-8190 train loss: 0.2636 valid loss: 0.3671, valid accuracy: 0.8934\n",
      "Iter-8200 train loss: 0.3456 valid loss: 0.3668, valid accuracy: 0.8930\n",
      "Iter-8210 train loss: 0.4778 valid loss: 0.3669, valid accuracy: 0.8930\n",
      "Iter-8220 train loss: 0.4514 valid loss: 0.3668, valid accuracy: 0.8934\n",
      "Iter-8230 train loss: 0.4525 valid loss: 0.3667, valid accuracy: 0.8938\n",
      "Iter-8240 train loss: 0.2202 valid loss: 0.3667, valid accuracy: 0.8940\n",
      "Iter-8250 train loss: 0.4787 valid loss: 0.3666, valid accuracy: 0.8936\n",
      "Iter-8260 train loss: 0.4058 valid loss: 0.3664, valid accuracy: 0.8938\n",
      "Iter-8270 train loss: 0.4932 valid loss: 0.3664, valid accuracy: 0.8946\n",
      "Iter-8280 train loss: 0.4909 valid loss: 0.3663, valid accuracy: 0.8940\n",
      "Iter-8290 train loss: 0.2417 valid loss: 0.3660, valid accuracy: 0.8934\n",
      "Iter-8300 train loss: 0.4278 valid loss: 0.3657, valid accuracy: 0.8942\n",
      "Iter-8310 train loss: 0.4217 valid loss: 0.3656, valid accuracy: 0.8938\n",
      "Iter-8320 train loss: 0.3787 valid loss: 0.3656, valid accuracy: 0.8932\n",
      "Iter-8330 train loss: 0.2789 valid loss: 0.3654, valid accuracy: 0.8936\n",
      "Iter-8340 train loss: 0.4493 valid loss: 0.3652, valid accuracy: 0.8928\n",
      "Iter-8350 train loss: 0.3839 valid loss: 0.3651, valid accuracy: 0.8928\n",
      "Iter-8360 train loss: 0.4621 valid loss: 0.3649, valid accuracy: 0.8932\n",
      "Iter-8370 train loss: 0.4120 valid loss: 0.3648, valid accuracy: 0.8932\n",
      "Iter-8380 train loss: 0.5271 valid loss: 0.3645, valid accuracy: 0.8926\n",
      "Iter-8390 train loss: 0.2285 valid loss: 0.3644, valid accuracy: 0.8930\n",
      "Iter-8400 train loss: 0.4340 valid loss: 0.3642, valid accuracy: 0.8932\n",
      "Iter-8410 train loss: 0.2536 valid loss: 0.3642, valid accuracy: 0.8930\n",
      "Iter-8420 train loss: 0.2474 valid loss: 0.3639, valid accuracy: 0.8926\n",
      "Iter-8430 train loss: 0.5592 valid loss: 0.3638, valid accuracy: 0.8936\n",
      "Iter-8440 train loss: 0.4371 valid loss: 0.3640, valid accuracy: 0.8944\n",
      "Iter-8450 train loss: 0.3712 valid loss: 0.3637, valid accuracy: 0.8940\n",
      "Iter-8460 train loss: 0.2647 valid loss: 0.3634, valid accuracy: 0.8934\n",
      "Iter-8470 train loss: 0.3821 valid loss: 0.3633, valid accuracy: 0.8942\n",
      "Iter-8480 train loss: 0.5258 valid loss: 0.3634, valid accuracy: 0.8948\n",
      "Iter-8490 train loss: 0.3355 valid loss: 0.3632, valid accuracy: 0.8942\n",
      "Iter-8500 train loss: 0.4413 valid loss: 0.3632, valid accuracy: 0.8948\n",
      "Iter-8510 train loss: 0.3669 valid loss: 0.3628, valid accuracy: 0.8946\n",
      "Iter-8520 train loss: 0.2985 valid loss: 0.3626, valid accuracy: 0.8942\n",
      "Iter-8530 train loss: 0.5435 valid loss: 0.3625, valid accuracy: 0.8946\n",
      "Iter-8540 train loss: 0.4906 valid loss: 0.3621, valid accuracy: 0.8934\n",
      "Iter-8550 train loss: 0.3375 valid loss: 0.3621, valid accuracy: 0.8940\n",
      "Iter-8560 train loss: 0.2191 valid loss: 0.3620, valid accuracy: 0.8930\n",
      "Iter-8570 train loss: 0.3830 valid loss: 0.3618, valid accuracy: 0.8930\n",
      "Iter-8580 train loss: 0.2821 valid loss: 0.3616, valid accuracy: 0.8928\n",
      "Iter-8590 train loss: 0.2817 valid loss: 0.3614, valid accuracy: 0.8934\n",
      "Iter-8600 train loss: 0.2527 valid loss: 0.3612, valid accuracy: 0.8930\n",
      "Iter-8610 train loss: 0.5361 valid loss: 0.3611, valid accuracy: 0.8930\n",
      "Iter-8620 train loss: 0.3695 valid loss: 0.3610, valid accuracy: 0.8936\n",
      "Iter-8630 train loss: 0.3228 valid loss: 0.3609, valid accuracy: 0.8946\n",
      "Iter-8640 train loss: 0.3993 valid loss: 0.3608, valid accuracy: 0.8944\n",
      "Iter-8650 train loss: 0.3563 valid loss: 0.3607, valid accuracy: 0.8946\n",
      "Iter-8660 train loss: 0.2942 valid loss: 0.3607, valid accuracy: 0.8948\n",
      "Iter-8670 train loss: 0.4418 valid loss: 0.3603, valid accuracy: 0.8946\n",
      "Iter-8680 train loss: 0.6366 valid loss: 0.3600, valid accuracy: 0.8946\n",
      "Iter-8690 train loss: 0.4117 valid loss: 0.3600, valid accuracy: 0.8948\n",
      "Iter-8700 train loss: 0.2926 valid loss: 0.3599, valid accuracy: 0.8942\n",
      "Iter-8710 train loss: 0.2433 valid loss: 0.3597, valid accuracy: 0.8952\n",
      "Iter-8720 train loss: 0.5813 valid loss: 0.3596, valid accuracy: 0.8954\n",
      "Iter-8730 train loss: 0.4381 valid loss: 0.3595, valid accuracy: 0.8948\n",
      "Iter-8740 train loss: 0.4182 valid loss: 0.3594, valid accuracy: 0.8950\n",
      "Iter-8750 train loss: 0.3652 valid loss: 0.3592, valid accuracy: 0.8948\n",
      "Iter-8760 train loss: 0.4664 valid loss: 0.3591, valid accuracy: 0.8940\n",
      "Iter-8770 train loss: 0.3854 valid loss: 0.3590, valid accuracy: 0.8948\n",
      "Iter-8780 train loss: 0.4485 valid loss: 0.3589, valid accuracy: 0.8946\n",
      "Iter-8790 train loss: 0.3455 valid loss: 0.3587, valid accuracy: 0.8950\n",
      "Iter-8800 train loss: 0.4079 valid loss: 0.3587, valid accuracy: 0.8952\n",
      "Iter-8810 train loss: 0.2856 valid loss: 0.3586, valid accuracy: 0.8946\n",
      "Iter-8820 train loss: 0.4172 valid loss: 0.3585, valid accuracy: 0.8948\n",
      "Iter-8830 train loss: 0.4060 valid loss: 0.3582, valid accuracy: 0.8946\n",
      "Iter-8840 train loss: 0.3316 valid loss: 0.3580, valid accuracy: 0.8944\n",
      "Iter-8850 train loss: 0.3320 valid loss: 0.3580, valid accuracy: 0.8944\n",
      "Iter-8860 train loss: 0.5484 valid loss: 0.3579, valid accuracy: 0.8946\n",
      "Iter-8870 train loss: 0.4111 valid loss: 0.3577, valid accuracy: 0.8950\n",
      "Iter-8880 train loss: 0.2964 valid loss: 0.3576, valid accuracy: 0.8950\n",
      "Iter-8890 train loss: 0.4099 valid loss: 0.3575, valid accuracy: 0.8956\n",
      "Iter-8900 train loss: 0.3406 valid loss: 0.3571, valid accuracy: 0.8952\n",
      "Iter-8910 train loss: 0.4781 valid loss: 0.3571, valid accuracy: 0.8950\n",
      "Iter-8920 train loss: 0.3607 valid loss: 0.3570, valid accuracy: 0.8950\n",
      "Iter-8930 train loss: 0.2965 valid loss: 0.3570, valid accuracy: 0.8954\n",
      "Iter-8940 train loss: 0.2173 valid loss: 0.3568, valid accuracy: 0.8954\n",
      "Iter-8950 train loss: 0.6199 valid loss: 0.3566, valid accuracy: 0.8954\n",
      "Iter-8960 train loss: 0.3494 valid loss: 0.3563, valid accuracy: 0.8954\n",
      "Iter-8970 train loss: 0.3625 valid loss: 0.3561, valid accuracy: 0.8956\n",
      "Iter-8980 train loss: 0.3365 valid loss: 0.3560, valid accuracy: 0.8956\n",
      "Iter-8990 train loss: 0.5051 valid loss: 0.3560, valid accuracy: 0.8954\n",
      "Iter-9000 train loss: 0.3715 valid loss: 0.3560, valid accuracy: 0.8956\n",
      "Iter-9010 train loss: 0.4263 valid loss: 0.3558, valid accuracy: 0.8956\n",
      "Iter-9020 train loss: 0.2357 valid loss: 0.3557, valid accuracy: 0.8952\n",
      "Iter-9030 train loss: 0.4484 valid loss: 0.3556, valid accuracy: 0.8956\n",
      "Iter-9040 train loss: 0.3428 valid loss: 0.3554, valid accuracy: 0.8954\n",
      "Iter-9050 train loss: 0.4405 valid loss: 0.3553, valid accuracy: 0.8954\n",
      "Iter-9060 train loss: 0.5143 valid loss: 0.3553, valid accuracy: 0.8954\n",
      "Iter-9070 train loss: 0.4123 valid loss: 0.3551, valid accuracy: 0.8956\n",
      "Iter-9080 train loss: 0.3576 valid loss: 0.3549, valid accuracy: 0.8952\n",
      "Iter-9090 train loss: 0.3624 valid loss: 0.3549, valid accuracy: 0.8956\n",
      "Iter-9100 train loss: 0.3619 valid loss: 0.3547, valid accuracy: 0.8952\n",
      "Iter-9110 train loss: 0.4905 valid loss: 0.3546, valid accuracy: 0.8958\n",
      "Iter-9120 train loss: 0.4639 valid loss: 0.3544, valid accuracy: 0.8958\n",
      "Iter-9130 train loss: 0.2699 valid loss: 0.3542, valid accuracy: 0.8964\n",
      "Iter-9140 train loss: 0.2897 valid loss: 0.3542, valid accuracy: 0.8954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 0.2753 valid loss: 0.3543, valid accuracy: 0.8950\n",
      "Iter-9160 train loss: 0.4371 valid loss: 0.3542, valid accuracy: 0.8950\n",
      "Iter-9170 train loss: 0.2767 valid loss: 0.3542, valid accuracy: 0.8948\n",
      "Iter-9180 train loss: 0.3693 valid loss: 0.3539, valid accuracy: 0.8954\n",
      "Iter-9190 train loss: 0.5232 valid loss: 0.3538, valid accuracy: 0.8948\n",
      "Iter-9200 train loss: 0.3563 valid loss: 0.3537, valid accuracy: 0.8958\n",
      "Iter-9210 train loss: 0.2023 valid loss: 0.3537, valid accuracy: 0.8956\n",
      "Iter-9220 train loss: 0.2396 valid loss: 0.3535, valid accuracy: 0.8960\n",
      "Iter-9230 train loss: 0.2816 valid loss: 0.3534, valid accuracy: 0.8954\n",
      "Iter-9240 train loss: 0.3500 valid loss: 0.3532, valid accuracy: 0.8954\n",
      "Iter-9250 train loss: 0.2735 valid loss: 0.3531, valid accuracy: 0.8952\n",
      "Iter-9260 train loss: 0.5177 valid loss: 0.3532, valid accuracy: 0.8956\n",
      "Iter-9270 train loss: 0.3022 valid loss: 0.3532, valid accuracy: 0.8954\n",
      "Iter-9280 train loss: 0.4968 valid loss: 0.3531, valid accuracy: 0.8954\n",
      "Iter-9290 train loss: 0.3222 valid loss: 0.3532, valid accuracy: 0.8954\n",
      "Iter-9300 train loss: 0.4182 valid loss: 0.3531, valid accuracy: 0.8956\n",
      "Iter-9310 train loss: 0.4278 valid loss: 0.3528, valid accuracy: 0.8958\n",
      "Iter-9320 train loss: 0.3831 valid loss: 0.3524, valid accuracy: 0.8960\n",
      "Iter-9330 train loss: 0.3926 valid loss: 0.3524, valid accuracy: 0.8962\n",
      "Iter-9340 train loss: 0.4107 valid loss: 0.3522, valid accuracy: 0.8964\n",
      "Iter-9350 train loss: 0.3936 valid loss: 0.3523, valid accuracy: 0.8960\n",
      "Iter-9360 train loss: 0.4862 valid loss: 0.3519, valid accuracy: 0.8960\n",
      "Iter-9370 train loss: 0.3081 valid loss: 0.3520, valid accuracy: 0.8964\n",
      "Iter-9380 train loss: 0.2814 valid loss: 0.3518, valid accuracy: 0.8964\n",
      "Iter-9390 train loss: 0.3272 valid loss: 0.3517, valid accuracy: 0.8968\n",
      "Iter-9400 train loss: 0.4572 valid loss: 0.3514, valid accuracy: 0.8966\n",
      "Iter-9410 train loss: 0.1918 valid loss: 0.3515, valid accuracy: 0.8966\n",
      "Iter-9420 train loss: 0.3974 valid loss: 0.3513, valid accuracy: 0.8970\n",
      "Iter-9430 train loss: 0.4904 valid loss: 0.3512, valid accuracy: 0.8966\n",
      "Iter-9440 train loss: 0.3169 valid loss: 0.3510, valid accuracy: 0.8964\n",
      "Iter-9450 train loss: 0.4361 valid loss: 0.3508, valid accuracy: 0.8966\n",
      "Iter-9460 train loss: 0.3272 valid loss: 0.3507, valid accuracy: 0.8968\n",
      "Iter-9470 train loss: 0.4988 valid loss: 0.3504, valid accuracy: 0.8966\n",
      "Iter-9480 train loss: 0.3727 valid loss: 0.3502, valid accuracy: 0.8964\n",
      "Iter-9490 train loss: 0.2768 valid loss: 0.3502, valid accuracy: 0.8964\n",
      "Iter-9500 train loss: 0.1752 valid loss: 0.3500, valid accuracy: 0.8962\n",
      "Iter-9510 train loss: 0.3665 valid loss: 0.3501, valid accuracy: 0.8960\n",
      "Iter-9520 train loss: 0.3567 valid loss: 0.3501, valid accuracy: 0.8958\n",
      "Iter-9530 train loss: 0.3619 valid loss: 0.3499, valid accuracy: 0.8962\n",
      "Iter-9540 train loss: 0.3508 valid loss: 0.3497, valid accuracy: 0.8964\n",
      "Iter-9550 train loss: 0.3718 valid loss: 0.3496, valid accuracy: 0.8956\n",
      "Iter-9560 train loss: 0.2284 valid loss: 0.3496, valid accuracy: 0.8964\n",
      "Iter-9570 train loss: 0.4984 valid loss: 0.3494, valid accuracy: 0.8964\n",
      "Iter-9580 train loss: 0.2405 valid loss: 0.3493, valid accuracy: 0.8970\n",
      "Iter-9590 train loss: 0.2604 valid loss: 0.3491, valid accuracy: 0.8968\n",
      "Iter-9600 train loss: 0.2797 valid loss: 0.3491, valid accuracy: 0.8964\n",
      "Iter-9610 train loss: 0.3117 valid loss: 0.3491, valid accuracy: 0.8968\n",
      "Iter-9620 train loss: 0.3815 valid loss: 0.3489, valid accuracy: 0.8968\n",
      "Iter-9630 train loss: 0.5888 valid loss: 0.3488, valid accuracy: 0.8964\n",
      "Iter-9640 train loss: 0.4663 valid loss: 0.3488, valid accuracy: 0.8962\n",
      "Iter-9650 train loss: 0.2763 valid loss: 0.3487, valid accuracy: 0.8960\n",
      "Iter-9660 train loss: 0.2460 valid loss: 0.3486, valid accuracy: 0.8964\n",
      "Iter-9670 train loss: 0.4763 valid loss: 0.3483, valid accuracy: 0.8964\n",
      "Iter-9680 train loss: 0.5029 valid loss: 0.3484, valid accuracy: 0.8958\n",
      "Iter-9690 train loss: 0.4913 valid loss: 0.3481, valid accuracy: 0.8962\n",
      "Iter-9700 train loss: 0.5484 valid loss: 0.3480, valid accuracy: 0.8968\n",
      "Iter-9710 train loss: 0.3265 valid loss: 0.3478, valid accuracy: 0.8966\n",
      "Iter-9720 train loss: 0.3135 valid loss: 0.3477, valid accuracy: 0.8970\n",
      "Iter-9730 train loss: 0.4892 valid loss: 0.3477, valid accuracy: 0.8972\n",
      "Iter-9740 train loss: 0.4394 valid loss: 0.3475, valid accuracy: 0.8968\n",
      "Iter-9750 train loss: 0.2709 valid loss: 0.3474, valid accuracy: 0.8966\n",
      "Iter-9760 train loss: 0.2767 valid loss: 0.3473, valid accuracy: 0.8968\n",
      "Iter-9770 train loss: 0.4654 valid loss: 0.3472, valid accuracy: 0.8968\n",
      "Iter-9780 train loss: 0.3075 valid loss: 0.3471, valid accuracy: 0.8972\n",
      "Iter-9790 train loss: 0.3086 valid loss: 0.3472, valid accuracy: 0.8986\n",
      "Iter-9800 train loss: 0.3075 valid loss: 0.3469, valid accuracy: 0.8972\n",
      "Iter-9810 train loss: 0.4192 valid loss: 0.3468, valid accuracy: 0.8980\n",
      "Iter-9820 train loss: 0.4979 valid loss: 0.3467, valid accuracy: 0.8978\n",
      "Iter-9830 train loss: 0.2727 valid loss: 0.3466, valid accuracy: 0.8974\n",
      "Iter-9840 train loss: 0.4081 valid loss: 0.3464, valid accuracy: 0.8978\n",
      "Iter-9850 train loss: 0.4991 valid loss: 0.3463, valid accuracy: 0.8984\n",
      "Iter-9860 train loss: 0.3227 valid loss: 0.3463, valid accuracy: 0.8986\n",
      "Iter-9870 train loss: 0.3842 valid loss: 0.3463, valid accuracy: 0.8992\n",
      "Iter-9880 train loss: 0.2807 valid loss: 0.3462, valid accuracy: 0.8994\n",
      "Iter-9890 train loss: 0.3852 valid loss: 0.3460, valid accuracy: 0.8994\n",
      "Iter-9900 train loss: 0.4068 valid loss: 0.3459, valid accuracy: 0.8998\n",
      "Iter-9910 train loss: 0.4401 valid loss: 0.3458, valid accuracy: 0.8992\n",
      "Iter-9920 train loss: 0.4082 valid loss: 0.3456, valid accuracy: 0.8990\n",
      "Iter-9930 train loss: 0.2708 valid loss: 0.3455, valid accuracy: 0.8998\n",
      "Iter-9940 train loss: 0.4560 valid loss: 0.3455, valid accuracy: 0.8996\n",
      "Iter-9950 train loss: 0.2947 valid loss: 0.3453, valid accuracy: 0.8994\n",
      "Iter-9960 train loss: 0.5650 valid loss: 0.3453, valid accuracy: 0.8990\n",
      "Iter-9970 train loss: 0.5311 valid loss: 0.3451, valid accuracy: 0.8988\n",
      "Iter-9980 train loss: 0.2584 valid loss: 0.3449, valid accuracy: 0.8994\n",
      "Iter-9990 train loss: 0.2937 valid loss: 0.3446, valid accuracy: 0.8988\n",
      "Iter-10000 train loss: 0.3986 valid loss: 0.3446, valid accuracy: 0.8986\n",
      "Last iteration - Test accuracy mean: 0.9014, std: 0.0000, loss: 0.3454\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# NOT used now\n",
    "p_dropout = 0.95 #  layer & unit noise: keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEClMgoAQQqQwCQJCCJHC/h92ohlnVD64SAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117f5a978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGQwJQERkQymJCAiksGUBEREMtj/BxQjM7sQ9ZV+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117f5aeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
