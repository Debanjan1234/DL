{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2674 valid loss: 2.2775, valid accuracy: 0.1692\n",
      "Iter-200 train loss: 2.2481 valid loss: 2.2430, valid accuracy: 0.2560\n",
      "Iter-300 train loss: 2.2605 valid loss: 2.2090, valid accuracy: 0.3276\n",
      "Iter-400 train loss: 2.1635 valid loss: 2.1769, valid accuracy: 0.3754\n",
      "Iter-500 train loss: 2.1474 valid loss: 2.1446, valid accuracy: 0.4078\n",
      "Iter-600 train loss: 2.1095 valid loss: 2.1130, valid accuracy: 0.4358\n",
      "Iter-700 train loss: 2.1137 valid loss: 2.0819, valid accuracy: 0.4554\n",
      "Iter-800 train loss: 2.0167 valid loss: 2.0513, valid accuracy: 0.4728\n",
      "Iter-900 train loss: 2.0292 valid loss: 2.0211, valid accuracy: 0.4842\n",
      "Iter-1000 train loss: 1.9572 valid loss: 1.9906, valid accuracy: 0.4976\n",
      "Iter-1100 train loss: 1.9891 valid loss: 1.9607, valid accuracy: 0.5074\n",
      "Iter-1200 train loss: 1.9874 valid loss: 1.9307, valid accuracy: 0.5138\n",
      "Iter-1300 train loss: 1.8727 valid loss: 1.9015, valid accuracy: 0.5214\n",
      "Iter-1400 train loss: 1.9988 valid loss: 1.8722, valid accuracy: 0.5288\n",
      "Iter-1500 train loss: 1.7704 valid loss: 1.8438, valid accuracy: 0.5360\n",
      "Iter-1600 train loss: 1.7934 valid loss: 1.8153, valid accuracy: 0.5426\n",
      "Iter-1700 train loss: 1.8031 valid loss: 1.7874, valid accuracy: 0.5466\n",
      "Iter-1800 train loss: 1.7801 valid loss: 1.7602, valid accuracy: 0.5502\n",
      "Iter-1900 train loss: 1.7185 valid loss: 1.7328, valid accuracy: 0.5546\n",
      "Iter-2000 train loss: 1.7255 valid loss: 1.7067, valid accuracy: 0.5582\n",
      "Iter-2100 train loss: 1.6931 valid loss: 1.6812, valid accuracy: 0.5644\n",
      "Iter-2200 train loss: 1.6420 valid loss: 1.6561, valid accuracy: 0.5682\n",
      "Iter-2300 train loss: 1.7211 valid loss: 1.6316, valid accuracy: 0.5720\n",
      "Iter-2400 train loss: 1.6359 valid loss: 1.6078, valid accuracy: 0.5774\n",
      "Iter-2500 train loss: 1.5827 valid loss: 1.5845, valid accuracy: 0.5804\n",
      "Iter-2600 train loss: 1.6458 valid loss: 1.5615, valid accuracy: 0.5836\n",
      "Iter-2700 train loss: 1.4246 valid loss: 1.5395, valid accuracy: 0.5884\n",
      "Iter-2800 train loss: 1.5210 valid loss: 1.5178, valid accuracy: 0.5932\n",
      "Iter-2900 train loss: 1.4756 valid loss: 1.4960, valid accuracy: 0.5988\n",
      "Iter-3000 train loss: 1.4976 valid loss: 1.4750, valid accuracy: 0.6040\n",
      "Iter-3100 train loss: 1.4840 valid loss: 1.4545, valid accuracy: 0.6106\n",
      "Iter-3200 train loss: 1.4600 valid loss: 1.4346, valid accuracy: 0.6166\n",
      "Iter-3300 train loss: 1.3879 valid loss: 1.4148, valid accuracy: 0.6206\n",
      "Iter-3400 train loss: 1.4748 valid loss: 1.3954, valid accuracy: 0.6266\n",
      "Iter-3500 train loss: 1.2901 valid loss: 1.3763, valid accuracy: 0.6312\n",
      "Iter-3600 train loss: 1.3825 valid loss: 1.3572, valid accuracy: 0.6366\n",
      "Iter-3700 train loss: 1.2766 valid loss: 1.3386, valid accuracy: 0.6398\n",
      "Iter-3800 train loss: 1.2955 valid loss: 1.3206, valid accuracy: 0.6458\n",
      "Iter-3900 train loss: 1.2889 valid loss: 1.3026, valid accuracy: 0.6500\n",
      "Iter-4000 train loss: 1.3196 valid loss: 1.2849, valid accuracy: 0.6548\n",
      "Iter-4100 train loss: 1.3378 valid loss: 1.2674, valid accuracy: 0.6592\n",
      "Iter-4200 train loss: 1.3549 valid loss: 1.2506, valid accuracy: 0.6642\n",
      "Iter-4300 train loss: 1.4074 valid loss: 1.2340, valid accuracy: 0.6704\n",
      "Iter-4400 train loss: 1.1168 valid loss: 1.2172, valid accuracy: 0.6762\n",
      "Iter-4500 train loss: 1.2826 valid loss: 1.2009, valid accuracy: 0.6798\n",
      "Iter-4600 train loss: 1.0621 valid loss: 1.1848, valid accuracy: 0.6862\n",
      "Iter-4700 train loss: 1.2659 valid loss: 1.1690, valid accuracy: 0.6952\n",
      "Iter-4800 train loss: 1.1563 valid loss: 1.1533, valid accuracy: 0.7016\n",
      "Iter-4900 train loss: 1.0773 valid loss: 1.1381, valid accuracy: 0.7056\n",
      "Iter-5000 train loss: 1.1390 valid loss: 1.1233, valid accuracy: 0.7098\n",
      "Iter-5100 train loss: 1.0957 valid loss: 1.1084, valid accuracy: 0.7162\n",
      "Iter-5200 train loss: 1.0252 valid loss: 1.0938, valid accuracy: 0.7216\n",
      "Iter-5300 train loss: 1.0649 valid loss: 1.0798, valid accuracy: 0.7276\n",
      "Iter-5400 train loss: 1.2287 valid loss: 1.0659, valid accuracy: 0.7314\n",
      "Iter-5500 train loss: 1.2972 valid loss: 1.0520, valid accuracy: 0.7388\n",
      "Iter-5600 train loss: 1.1694 valid loss: 1.0385, valid accuracy: 0.7448\n",
      "Iter-5700 train loss: 1.1116 valid loss: 1.0253, valid accuracy: 0.7504\n",
      "Iter-5800 train loss: 1.0925 valid loss: 1.0128, valid accuracy: 0.7570\n",
      "Iter-5900 train loss: 1.1312 valid loss: 0.9999, valid accuracy: 0.7626\n",
      "Iter-6000 train loss: 0.9606 valid loss: 0.9874, valid accuracy: 0.7686\n",
      "Iter-6100 train loss: 1.0863 valid loss: 0.9752, valid accuracy: 0.7716\n",
      "Iter-6200 train loss: 0.9623 valid loss: 0.9633, valid accuracy: 0.7766\n",
      "Iter-6300 train loss: 0.9722 valid loss: 0.9518, valid accuracy: 0.7786\n",
      "Iter-6400 train loss: 0.8436 valid loss: 0.9404, valid accuracy: 0.7830\n",
      "Iter-6500 train loss: 0.8565 valid loss: 0.9293, valid accuracy: 0.7876\n",
      "Iter-6600 train loss: 0.9123 valid loss: 0.9179, valid accuracy: 0.7914\n",
      "Iter-6700 train loss: 0.8335 valid loss: 0.9072, valid accuracy: 0.7934\n",
      "Iter-6800 train loss: 1.0356 valid loss: 0.8969, valid accuracy: 0.7954\n",
      "Iter-6900 train loss: 0.8814 valid loss: 0.8867, valid accuracy: 0.7986\n",
      "Iter-7000 train loss: 0.8962 valid loss: 0.8767, valid accuracy: 0.8012\n",
      "Iter-7100 train loss: 0.8664 valid loss: 0.8666, valid accuracy: 0.8052\n",
      "Iter-7200 train loss: 0.7420 valid loss: 0.8569, valid accuracy: 0.8098\n",
      "Iter-7300 train loss: 0.7790 valid loss: 0.8474, valid accuracy: 0.8114\n",
      "Iter-7400 train loss: 0.8044 valid loss: 0.8383, valid accuracy: 0.8136\n",
      "Iter-7500 train loss: 0.8734 valid loss: 0.8292, valid accuracy: 0.8168\n",
      "Iter-7600 train loss: 0.8327 valid loss: 0.8203, valid accuracy: 0.8200\n",
      "Iter-7700 train loss: 0.8835 valid loss: 0.8115, valid accuracy: 0.8230\n",
      "Iter-7800 train loss: 0.8451 valid loss: 0.8030, valid accuracy: 0.8266\n",
      "Iter-7900 train loss: 0.8611 valid loss: 0.7947, valid accuracy: 0.8282\n",
      "Iter-8000 train loss: 0.7854 valid loss: 0.7865, valid accuracy: 0.8318\n",
      "Iter-8100 train loss: 0.8432 valid loss: 0.7787, valid accuracy: 0.8324\n",
      "Iter-8200 train loss: 0.9977 valid loss: 0.7708, valid accuracy: 0.8350\n",
      "Iter-8300 train loss: 0.8181 valid loss: 0.7632, valid accuracy: 0.8370\n",
      "Iter-8400 train loss: 0.9678 valid loss: 0.7557, valid accuracy: 0.8392\n",
      "Iter-8500 train loss: 0.7735 valid loss: 0.7483, valid accuracy: 0.8416\n",
      "Iter-8600 train loss: 0.8167 valid loss: 0.7412, valid accuracy: 0.8426\n",
      "Iter-8700 train loss: 0.8597 valid loss: 0.7344, valid accuracy: 0.8434\n",
      "Iter-8800 train loss: 0.8433 valid loss: 0.7274, valid accuracy: 0.8464\n",
      "Iter-8900 train loss: 0.7394 valid loss: 0.7206, valid accuracy: 0.8486\n",
      "Iter-9000 train loss: 0.6922 valid loss: 0.7140, valid accuracy: 0.8502\n",
      "Iter-9100 train loss: 0.7550 valid loss: 0.7075, valid accuracy: 0.8514\n",
      "Iter-9200 train loss: 0.7490 valid loss: 0.7012, valid accuracy: 0.8512\n",
      "Iter-9300 train loss: 0.7520 valid loss: 0.6949, valid accuracy: 0.8518\n",
      "Iter-9400 train loss: 0.5943 valid loss: 0.6889, valid accuracy: 0.8530\n",
      "Iter-9500 train loss: 0.7928 valid loss: 0.6832, valid accuracy: 0.8544\n",
      "Iter-9600 train loss: 0.6995 valid loss: 0.6774, valid accuracy: 0.8560\n",
      "Iter-9700 train loss: 0.6533 valid loss: 0.6718, valid accuracy: 0.8568\n",
      "Iter-9800 train loss: 0.7120 valid loss: 0.6662, valid accuracy: 0.8572\n",
      "Iter-9900 train loss: 0.6258 valid loss: 0.6608, valid accuracy: 0.8588\n",
      "Iter-10000 train loss: 0.9319 valid loss: 0.6554, valid accuracy: 0.8606\n",
      "Iter-10100 train loss: 0.6575 valid loss: 0.6502, valid accuracy: 0.8610\n",
      "Iter-10200 train loss: 0.6573 valid loss: 0.6450, valid accuracy: 0.8614\n",
      "Iter-10300 train loss: 0.6585 valid loss: 0.6401, valid accuracy: 0.8616\n",
      "Iter-10400 train loss: 0.5470 valid loss: 0.6351, valid accuracy: 0.8626\n",
      "Iter-10500 train loss: 0.5382 valid loss: 0.6303, valid accuracy: 0.8642\n",
      "Iter-10600 train loss: 0.6691 valid loss: 0.6254, valid accuracy: 0.8658\n",
      "Iter-10700 train loss: 0.6941 valid loss: 0.6207, valid accuracy: 0.8658\n",
      "Iter-10800 train loss: 0.5193 valid loss: 0.6160, valid accuracy: 0.8666\n",
      "Iter-10900 train loss: 0.7599 valid loss: 0.6116, valid accuracy: 0.8676\n",
      "Iter-11000 train loss: 0.4931 valid loss: 0.6072, valid accuracy: 0.8684\n",
      "Iter-11100 train loss: 0.5548 valid loss: 0.6031, valid accuracy: 0.8688\n",
      "Iter-11200 train loss: 0.7303 valid loss: 0.5990, valid accuracy: 0.8696\n",
      "Iter-11300 train loss: 0.7871 valid loss: 0.5948, valid accuracy: 0.8708\n",
      "Iter-11400 train loss: 0.7733 valid loss: 0.5906, valid accuracy: 0.8710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 0.5169 valid loss: 0.5867, valid accuracy: 0.8712\n",
      "Iter-11600 train loss: 0.5712 valid loss: 0.5829, valid accuracy: 0.8720\n",
      "Iter-11700 train loss: 0.6542 valid loss: 0.5790, valid accuracy: 0.8722\n",
      "Iter-11800 train loss: 0.6261 valid loss: 0.5754, valid accuracy: 0.8724\n",
      "Iter-11900 train loss: 0.4155 valid loss: 0.5718, valid accuracy: 0.8728\n",
      "Iter-12000 train loss: 0.6471 valid loss: 0.5682, valid accuracy: 0.8740\n",
      "Iter-12100 train loss: 0.4579 valid loss: 0.5645, valid accuracy: 0.8744\n",
      "Iter-12200 train loss: 0.6255 valid loss: 0.5610, valid accuracy: 0.8744\n",
      "Iter-12300 train loss: 0.6339 valid loss: 0.5575, valid accuracy: 0.8752\n",
      "Iter-12400 train loss: 0.5578 valid loss: 0.5542, valid accuracy: 0.8760\n",
      "Iter-12500 train loss: 0.6614 valid loss: 0.5506, valid accuracy: 0.8764\n",
      "Iter-12600 train loss: 0.9034 valid loss: 0.5473, valid accuracy: 0.8768\n",
      "Iter-12700 train loss: 0.4852 valid loss: 0.5442, valid accuracy: 0.8782\n",
      "Iter-12800 train loss: 0.3894 valid loss: 0.5411, valid accuracy: 0.8782\n",
      "Iter-12900 train loss: 0.4632 valid loss: 0.5381, valid accuracy: 0.8790\n",
      "Iter-13000 train loss: 0.6705 valid loss: 0.5350, valid accuracy: 0.8794\n",
      "Iter-13100 train loss: 0.6239 valid loss: 0.5320, valid accuracy: 0.8792\n",
      "Iter-13200 train loss: 0.5771 valid loss: 0.5291, valid accuracy: 0.8800\n",
      "Iter-13300 train loss: 0.5304 valid loss: 0.5262, valid accuracy: 0.8802\n",
      "Iter-13400 train loss: 0.7117 valid loss: 0.5233, valid accuracy: 0.8812\n",
      "Iter-13500 train loss: 0.4952 valid loss: 0.5205, valid accuracy: 0.8812\n",
      "Iter-13600 train loss: 0.5092 valid loss: 0.5177, valid accuracy: 0.8816\n",
      "Iter-13700 train loss: 0.4664 valid loss: 0.5149, valid accuracy: 0.8822\n",
      "Iter-13800 train loss: 0.6013 valid loss: 0.5123, valid accuracy: 0.8822\n",
      "Iter-13900 train loss: 0.4650 valid loss: 0.5096, valid accuracy: 0.8834\n",
      "Iter-14000 train loss: 0.5750 valid loss: 0.5070, valid accuracy: 0.8836\n",
      "Iter-14100 train loss: 0.4566 valid loss: 0.5044, valid accuracy: 0.8834\n",
      "Iter-14200 train loss: 0.5154 valid loss: 0.5020, valid accuracy: 0.8836\n",
      "Iter-14300 train loss: 0.5567 valid loss: 0.4996, valid accuracy: 0.8842\n",
      "Iter-14400 train loss: 0.3988 valid loss: 0.4973, valid accuracy: 0.8836\n",
      "Iter-14500 train loss: 0.5408 valid loss: 0.4948, valid accuracy: 0.8844\n",
      "Iter-14600 train loss: 0.5301 valid loss: 0.4922, valid accuracy: 0.8852\n",
      "Iter-14700 train loss: 0.6365 valid loss: 0.4901, valid accuracy: 0.8850\n",
      "Iter-14800 train loss: 0.4361 valid loss: 0.4877, valid accuracy: 0.8854\n",
      "Iter-14900 train loss: 0.5224 valid loss: 0.4855, valid accuracy: 0.8860\n",
      "Iter-15000 train loss: 0.3778 valid loss: 0.4833, valid accuracy: 0.8862\n",
      "Iter-15100 train loss: 0.6866 valid loss: 0.4812, valid accuracy: 0.8868\n",
      "Iter-15200 train loss: 0.5510 valid loss: 0.4792, valid accuracy: 0.8864\n",
      "Iter-15300 train loss: 0.6289 valid loss: 0.4770, valid accuracy: 0.8860\n",
      "Iter-15400 train loss: 0.5001 valid loss: 0.4749, valid accuracy: 0.8864\n",
      "Iter-15500 train loss: 0.4143 valid loss: 0.4727, valid accuracy: 0.8870\n",
      "Iter-15600 train loss: 0.5447 valid loss: 0.4707, valid accuracy: 0.8870\n",
      "Iter-15700 train loss: 0.4736 valid loss: 0.4688, valid accuracy: 0.8876\n",
      "Iter-15800 train loss: 0.5088 valid loss: 0.4668, valid accuracy: 0.8878\n",
      "Iter-15900 train loss: 0.4127 valid loss: 0.4649, valid accuracy: 0.8878\n",
      "Iter-16000 train loss: 0.6433 valid loss: 0.4629, valid accuracy: 0.8878\n",
      "Iter-16100 train loss: 0.6295 valid loss: 0.4611, valid accuracy: 0.8878\n",
      "Iter-16200 train loss: 0.3232 valid loss: 0.4593, valid accuracy: 0.8886\n",
      "Iter-16300 train loss: 0.5065 valid loss: 0.4576, valid accuracy: 0.8892\n",
      "Iter-16400 train loss: 0.5988 valid loss: 0.4557, valid accuracy: 0.8896\n",
      "Iter-16500 train loss: 0.4288 valid loss: 0.4541, valid accuracy: 0.8900\n",
      "Iter-16600 train loss: 0.5011 valid loss: 0.4525, valid accuracy: 0.8906\n",
      "Iter-16700 train loss: 0.3897 valid loss: 0.4507, valid accuracy: 0.8908\n",
      "Iter-16800 train loss: 0.4604 valid loss: 0.4489, valid accuracy: 0.8916\n",
      "Iter-16900 train loss: 0.3810 valid loss: 0.4473, valid accuracy: 0.8916\n",
      "Iter-17000 train loss: 0.4246 valid loss: 0.4457, valid accuracy: 0.8916\n",
      "Iter-17100 train loss: 0.5655 valid loss: 0.4439, valid accuracy: 0.8922\n",
      "Iter-17200 train loss: 0.3597 valid loss: 0.4424, valid accuracy: 0.8922\n",
      "Iter-17300 train loss: 0.5962 valid loss: 0.4409, valid accuracy: 0.8924\n",
      "Iter-17400 train loss: 0.4980 valid loss: 0.4394, valid accuracy: 0.8932\n",
      "Iter-17500 train loss: 0.5902 valid loss: 0.4378, valid accuracy: 0.8932\n",
      "Iter-17600 train loss: 0.4796 valid loss: 0.4363, valid accuracy: 0.8932\n",
      "Iter-17700 train loss: 0.4111 valid loss: 0.4348, valid accuracy: 0.8934\n",
      "Iter-17800 train loss: 0.6137 valid loss: 0.4334, valid accuracy: 0.8936\n",
      "Iter-17900 train loss: 0.4957 valid loss: 0.4321, valid accuracy: 0.8940\n",
      "Iter-18000 train loss: 0.4928 valid loss: 0.4306, valid accuracy: 0.8942\n",
      "Iter-18100 train loss: 0.6223 valid loss: 0.4293, valid accuracy: 0.8946\n",
      "Iter-18200 train loss: 0.4696 valid loss: 0.4278, valid accuracy: 0.8956\n",
      "Iter-18300 train loss: 0.4111 valid loss: 0.4263, valid accuracy: 0.8954\n",
      "Iter-18400 train loss: 0.2961 valid loss: 0.4249, valid accuracy: 0.8956\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
