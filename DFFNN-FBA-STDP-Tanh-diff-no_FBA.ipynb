{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2674 valid loss: 2.2775, valid accuracy: 0.1692\n",
      "Iter-200 train loss: 2.2481 valid loss: 2.2430, valid accuracy: 0.2560\n",
      "Iter-300 train loss: 2.2605 valid loss: 2.2090, valid accuracy: 0.3276\n",
      "Iter-400 train loss: 2.1635 valid loss: 2.1769, valid accuracy: 0.3754\n",
      "Iter-500 train loss: 2.1474 valid loss: 2.1446, valid accuracy: 0.4078\n",
      "Iter-600 train loss: 2.1095 valid loss: 2.1130, valid accuracy: 0.4358\n",
      "Iter-700 train loss: 2.1137 valid loss: 2.0819, valid accuracy: 0.4554\n",
      "Iter-800 train loss: 2.0167 valid loss: 2.0513, valid accuracy: 0.4728\n",
      "Iter-900 train loss: 2.0292 valid loss: 2.0211, valid accuracy: 0.4842\n",
      "Iter-1000 train loss: 1.9572 valid loss: 1.9906, valid accuracy: 0.4976\n",
      "Iter-1100 train loss: 1.9891 valid loss: 1.9607, valid accuracy: 0.5074\n",
      "Iter-1200 train loss: 1.9874 valid loss: 1.9307, valid accuracy: 0.5138\n",
      "Iter-1300 train loss: 1.8727 valid loss: 1.9015, valid accuracy: 0.5214\n",
      "Iter-1400 train loss: 1.9988 valid loss: 1.8722, valid accuracy: 0.5288\n",
      "Iter-1500 train loss: 1.7704 valid loss: 1.8438, valid accuracy: 0.5360\n",
      "Iter-1600 train loss: 1.7934 valid loss: 1.8153, valid accuracy: 0.5426\n",
      "Iter-1700 train loss: 1.8031 valid loss: 1.7874, valid accuracy: 0.5466\n",
      "Iter-1800 train loss: 1.7801 valid loss: 1.7602, valid accuracy: 0.5502\n",
      "Iter-1900 train loss: 1.7185 valid loss: 1.7328, valid accuracy: 0.5546\n",
      "Iter-2000 train loss: 1.7255 valid loss: 1.7067, valid accuracy: 0.5582\n",
      "Iter-2100 train loss: 1.6931 valid loss: 1.6812, valid accuracy: 0.5644\n",
      "Iter-2200 train loss: 1.6420 valid loss: 1.6561, valid accuracy: 0.5682\n",
      "Iter-2300 train loss: 1.7211 valid loss: 1.6316, valid accuracy: 0.5720\n",
      "Iter-2400 train loss: 1.6359 valid loss: 1.6078, valid accuracy: 0.5774\n",
      "Iter-2500 train loss: 1.5827 valid loss: 1.5845, valid accuracy: 0.5804\n",
      "Iter-2600 train loss: 1.6458 valid loss: 1.5615, valid accuracy: 0.5836\n",
      "Iter-2700 train loss: 1.4246 valid loss: 1.5395, valid accuracy: 0.5884\n",
      "Iter-2800 train loss: 1.5210 valid loss: 1.5178, valid accuracy: 0.5932\n",
      "Iter-2900 train loss: 1.4756 valid loss: 1.4960, valid accuracy: 0.5988\n",
      "Iter-3000 train loss: 1.4976 valid loss: 1.4750, valid accuracy: 0.6040\n",
      "Iter-3100 train loss: 1.4840 valid loss: 1.4545, valid accuracy: 0.6106\n",
      "Iter-3200 train loss: 1.4600 valid loss: 1.4346, valid accuracy: 0.6166\n",
      "Iter-3300 train loss: 1.3879 valid loss: 1.4148, valid accuracy: 0.6206\n",
      "Iter-3400 train loss: 1.4748 valid loss: 1.3954, valid accuracy: 0.6266\n",
      "Iter-3500 train loss: 1.2901 valid loss: 1.3763, valid accuracy: 0.6312\n",
      "Iter-3600 train loss: 1.3825 valid loss: 1.3572, valid accuracy: 0.6366\n",
      "Iter-3700 train loss: 1.2766 valid loss: 1.3386, valid accuracy: 0.6398\n",
      "Iter-3800 train loss: 1.2955 valid loss: 1.3206, valid accuracy: 0.6458\n",
      "Iter-3900 train loss: 1.2889 valid loss: 1.3026, valid accuracy: 0.6500\n",
      "Iter-4000 train loss: 1.3196 valid loss: 1.2849, valid accuracy: 0.6548\n",
      "Iter-4100 train loss: 1.3378 valid loss: 1.2674, valid accuracy: 0.6592\n",
      "Iter-4200 train loss: 1.3549 valid loss: 1.2506, valid accuracy: 0.6642\n",
      "Iter-4300 train loss: 1.4074 valid loss: 1.2340, valid accuracy: 0.6704\n",
      "Iter-4400 train loss: 1.1168 valid loss: 1.2172, valid accuracy: 0.6762\n",
      "Iter-4500 train loss: 1.2826 valid loss: 1.2009, valid accuracy: 0.6798\n",
      "Iter-4600 train loss: 1.0621 valid loss: 1.1848, valid accuracy: 0.6862\n",
      "Iter-4700 train loss: 1.2659 valid loss: 1.1690, valid accuracy: 0.6952\n",
      "Iter-4800 train loss: 1.1563 valid loss: 1.1533, valid accuracy: 0.7016\n",
      "Iter-4900 train loss: 1.0773 valid loss: 1.1381, valid accuracy: 0.7056\n",
      "Iter-5000 train loss: 1.1390 valid loss: 1.1233, valid accuracy: 0.7098\n",
      "Iter-5100 train loss: 1.0957 valid loss: 1.1084, valid accuracy: 0.7162\n",
      "Iter-5200 train loss: 1.0252 valid loss: 1.0938, valid accuracy: 0.7216\n",
      "Iter-5300 train loss: 1.0649 valid loss: 1.0798, valid accuracy: 0.7276\n",
      "Iter-5400 train loss: 1.2287 valid loss: 1.0659, valid accuracy: 0.7314\n",
      "Iter-5500 train loss: 1.2972 valid loss: 1.0520, valid accuracy: 0.7388\n",
      "Iter-5600 train loss: 1.1694 valid loss: 1.0385, valid accuracy: 0.7448\n",
      "Iter-5700 train loss: 1.1116 valid loss: 1.0253, valid accuracy: 0.7504\n",
      "Iter-5800 train loss: 1.0925 valid loss: 1.0128, valid accuracy: 0.7570\n",
      "Iter-5900 train loss: 1.1312 valid loss: 0.9999, valid accuracy: 0.7626\n",
      "Iter-6000 train loss: 0.9606 valid loss: 0.9874, valid accuracy: 0.7686\n",
      "Iter-6100 train loss: 1.0863 valid loss: 0.9752, valid accuracy: 0.7716\n",
      "Iter-6200 train loss: 0.9623 valid loss: 0.9633, valid accuracy: 0.7766\n",
      "Iter-6300 train loss: 0.9722 valid loss: 0.9518, valid accuracy: 0.7786\n",
      "Iter-6400 train loss: 0.8436 valid loss: 0.9404, valid accuracy: 0.7830\n",
      "Iter-6500 train loss: 0.8565 valid loss: 0.9293, valid accuracy: 0.7876\n",
      "Iter-6600 train loss: 0.9123 valid loss: 0.9179, valid accuracy: 0.7914\n",
      "Iter-6700 train loss: 0.8335 valid loss: 0.9072, valid accuracy: 0.7934\n",
      "Iter-6800 train loss: 1.0356 valid loss: 0.8969, valid accuracy: 0.7954\n",
      "Iter-6900 train loss: 0.8814 valid loss: 0.8867, valid accuracy: 0.7986\n",
      "Iter-7000 train loss: 0.8962 valid loss: 0.8767, valid accuracy: 0.8012\n",
      "Iter-7100 train loss: 0.8664 valid loss: 0.8666, valid accuracy: 0.8052\n",
      "Iter-7200 train loss: 0.7420 valid loss: 0.8569, valid accuracy: 0.8098\n",
      "Iter-7300 train loss: 0.7790 valid loss: 0.8474, valid accuracy: 0.8114\n",
      "Iter-7400 train loss: 0.8044 valid loss: 0.8383, valid accuracy: 0.8136\n",
      "Iter-7500 train loss: 0.8734 valid loss: 0.8292, valid accuracy: 0.8168\n",
      "Iter-7600 train loss: 0.8327 valid loss: 0.8203, valid accuracy: 0.8200\n",
      "Iter-7700 train loss: 0.8835 valid loss: 0.8115, valid accuracy: 0.8230\n",
      "Iter-7800 train loss: 0.8451 valid loss: 0.8030, valid accuracy: 0.8266\n",
      "Iter-7900 train loss: 0.8611 valid loss: 0.7947, valid accuracy: 0.8282\n",
      "Iter-8000 train loss: 0.7854 valid loss: 0.7865, valid accuracy: 0.8318\n",
      "Iter-8100 train loss: 0.8432 valid loss: 0.7787, valid accuracy: 0.8324\n",
      "Iter-8200 train loss: 0.9977 valid loss: 0.7708, valid accuracy: 0.8350\n",
      "Iter-8300 train loss: 0.8181 valid loss: 0.7632, valid accuracy: 0.8370\n",
      "Iter-8400 train loss: 0.9678 valid loss: 0.7557, valid accuracy: 0.8392\n",
      "Iter-8500 train loss: 0.7735 valid loss: 0.7483, valid accuracy: 0.8416\n",
      "Iter-8600 train loss: 0.8167 valid loss: 0.7412, valid accuracy: 0.8426\n",
      "Iter-8700 train loss: 0.8597 valid loss: 0.7344, valid accuracy: 0.8434\n",
      "Iter-8800 train loss: 0.8433 valid loss: 0.7274, valid accuracy: 0.8464\n",
      "Iter-8900 train loss: 0.7394 valid loss: 0.7206, valid accuracy: 0.8486\n",
      "Iter-9000 train loss: 0.6922 valid loss: 0.7140, valid accuracy: 0.8502\n",
      "Iter-9100 train loss: 0.7550 valid loss: 0.7075, valid accuracy: 0.8514\n",
      "Iter-9200 train loss: 0.7490 valid loss: 0.7012, valid accuracy: 0.8512\n",
      "Iter-9300 train loss: 0.7520 valid loss: 0.6949, valid accuracy: 0.8518\n",
      "Iter-9400 train loss: 0.5943 valid loss: 0.6889, valid accuracy: 0.8530\n",
      "Iter-9500 train loss: 0.7928 valid loss: 0.6832, valid accuracy: 0.8544\n",
      "Iter-9600 train loss: 0.6995 valid loss: 0.6774, valid accuracy: 0.8560\n",
      "Iter-9700 train loss: 0.6533 valid loss: 0.6718, valid accuracy: 0.8568\n",
      "Iter-9800 train loss: 0.7120 valid loss: 0.6662, valid accuracy: 0.8572\n",
      "Iter-9900 train loss: 0.6258 valid loss: 0.6608, valid accuracy: 0.8588\n",
      "Iter-10000 train loss: 0.9319 valid loss: 0.6554, valid accuracy: 0.8606\n",
      "Iter-10100 train loss: 0.6575 valid loss: 0.6502, valid accuracy: 0.8610\n",
      "Iter-10200 train loss: 0.6573 valid loss: 0.6450, valid accuracy: 0.8614\n",
      "Iter-10300 train loss: 0.6585 valid loss: 0.6401, valid accuracy: 0.8616\n",
      "Iter-10400 train loss: 0.5470 valid loss: 0.6351, valid accuracy: 0.8626\n",
      "Iter-10500 train loss: 0.5382 valid loss: 0.6303, valid accuracy: 0.8642\n",
      "Iter-10600 train loss: 0.6691 valid loss: 0.6254, valid accuracy: 0.8658\n",
      "Iter-10700 train loss: 0.6941 valid loss: 0.6207, valid accuracy: 0.8658\n",
      "Iter-10800 train loss: 0.5193 valid loss: 0.6160, valid accuracy: 0.8666\n",
      "Iter-10900 train loss: 0.7599 valid loss: 0.6116, valid accuracy: 0.8676\n",
      "Iter-11000 train loss: 0.4931 valid loss: 0.6072, valid accuracy: 0.8684\n",
      "Iter-11100 train loss: 0.5548 valid loss: 0.6031, valid accuracy: 0.8688\n",
      "Iter-11200 train loss: 0.7303 valid loss: 0.5990, valid accuracy: 0.8696\n",
      "Iter-11300 train loss: 0.7871 valid loss: 0.5948, valid accuracy: 0.8708\n",
      "Iter-11400 train loss: 0.7733 valid loss: 0.5906, valid accuracy: 0.8710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 0.5169 valid loss: 0.5867, valid accuracy: 0.8712\n",
      "Iter-11600 train loss: 0.5712 valid loss: 0.5829, valid accuracy: 0.8720\n",
      "Iter-11700 train loss: 0.6542 valid loss: 0.5790, valid accuracy: 0.8722\n",
      "Iter-11800 train loss: 0.6261 valid loss: 0.5754, valid accuracy: 0.8724\n",
      "Iter-11900 train loss: 0.4155 valid loss: 0.5718, valid accuracy: 0.8728\n",
      "Iter-12000 train loss: 0.6471 valid loss: 0.5682, valid accuracy: 0.8740\n",
      "Iter-12100 train loss: 0.4579 valid loss: 0.5645, valid accuracy: 0.8744\n",
      "Iter-12200 train loss: 0.6255 valid loss: 0.5610, valid accuracy: 0.8744\n",
      "Iter-12300 train loss: 0.6339 valid loss: 0.5575, valid accuracy: 0.8752\n",
      "Iter-12400 train loss: 0.5578 valid loss: 0.5542, valid accuracy: 0.8760\n",
      "Iter-12500 train loss: 0.6614 valid loss: 0.5506, valid accuracy: 0.8764\n",
      "Iter-12600 train loss: 0.9034 valid loss: 0.5473, valid accuracy: 0.8768\n",
      "Iter-12700 train loss: 0.4852 valid loss: 0.5442, valid accuracy: 0.8782\n",
      "Iter-12800 train loss: 0.3894 valid loss: 0.5411, valid accuracy: 0.8782\n",
      "Iter-12900 train loss: 0.4632 valid loss: 0.5381, valid accuracy: 0.8790\n",
      "Iter-13000 train loss: 0.6705 valid loss: 0.5350, valid accuracy: 0.8794\n",
      "Iter-13100 train loss: 0.6239 valid loss: 0.5320, valid accuracy: 0.8792\n",
      "Iter-13200 train loss: 0.5771 valid loss: 0.5291, valid accuracy: 0.8800\n",
      "Iter-13300 train loss: 0.5304 valid loss: 0.5262, valid accuracy: 0.8802\n",
      "Iter-13400 train loss: 0.7117 valid loss: 0.5233, valid accuracy: 0.8812\n",
      "Iter-13500 train loss: 0.4952 valid loss: 0.5205, valid accuracy: 0.8812\n",
      "Iter-13600 train loss: 0.5092 valid loss: 0.5177, valid accuracy: 0.8816\n",
      "Iter-13700 train loss: 0.4664 valid loss: 0.5149, valid accuracy: 0.8822\n",
      "Iter-13800 train loss: 0.6013 valid loss: 0.5123, valid accuracy: 0.8822\n",
      "Iter-13900 train loss: 0.4650 valid loss: 0.5096, valid accuracy: 0.8834\n",
      "Iter-14000 train loss: 0.5750 valid loss: 0.5070, valid accuracy: 0.8836\n",
      "Iter-14100 train loss: 0.4566 valid loss: 0.5044, valid accuracy: 0.8834\n",
      "Iter-14200 train loss: 0.5154 valid loss: 0.5020, valid accuracy: 0.8836\n",
      "Iter-14300 train loss: 0.5567 valid loss: 0.4996, valid accuracy: 0.8842\n",
      "Iter-14400 train loss: 0.3988 valid loss: 0.4973, valid accuracy: 0.8836\n",
      "Iter-14500 train loss: 0.5408 valid loss: 0.4948, valid accuracy: 0.8844\n",
      "Iter-14600 train loss: 0.5301 valid loss: 0.4922, valid accuracy: 0.8852\n",
      "Iter-14700 train loss: 0.6365 valid loss: 0.4901, valid accuracy: 0.8850\n",
      "Iter-14800 train loss: 0.4361 valid loss: 0.4877, valid accuracy: 0.8854\n",
      "Iter-14900 train loss: 0.5224 valid loss: 0.4855, valid accuracy: 0.8860\n",
      "Iter-15000 train loss: 0.3778 valid loss: 0.4833, valid accuracy: 0.8862\n",
      "Iter-15100 train loss: 0.6866 valid loss: 0.4812, valid accuracy: 0.8868\n",
      "Iter-15200 train loss: 0.5510 valid loss: 0.4792, valid accuracy: 0.8864\n",
      "Iter-15300 train loss: 0.6289 valid loss: 0.4770, valid accuracy: 0.8860\n",
      "Iter-15400 train loss: 0.5001 valid loss: 0.4749, valid accuracy: 0.8864\n",
      "Iter-15500 train loss: 0.4143 valid loss: 0.4727, valid accuracy: 0.8870\n",
      "Iter-15600 train loss: 0.5447 valid loss: 0.4707, valid accuracy: 0.8870\n",
      "Iter-15700 train loss: 0.4736 valid loss: 0.4688, valid accuracy: 0.8876\n",
      "Iter-15800 train loss: 0.5088 valid loss: 0.4668, valid accuracy: 0.8878\n",
      "Iter-15900 train loss: 0.4127 valid loss: 0.4649, valid accuracy: 0.8878\n",
      "Iter-16000 train loss: 0.6433 valid loss: 0.4629, valid accuracy: 0.8878\n",
      "Iter-16100 train loss: 0.6295 valid loss: 0.4611, valid accuracy: 0.8878\n",
      "Iter-16200 train loss: 0.3232 valid loss: 0.4593, valid accuracy: 0.8886\n",
      "Iter-16300 train loss: 0.5065 valid loss: 0.4576, valid accuracy: 0.8892\n",
      "Iter-16400 train loss: 0.5988 valid loss: 0.4557, valid accuracy: 0.8896\n",
      "Iter-16500 train loss: 0.4288 valid loss: 0.4541, valid accuracy: 0.8900\n",
      "Iter-16600 train loss: 0.5011 valid loss: 0.4525, valid accuracy: 0.8906\n",
      "Iter-16700 train loss: 0.3897 valid loss: 0.4507, valid accuracy: 0.8908\n",
      "Iter-16800 train loss: 0.4604 valid loss: 0.4489, valid accuracy: 0.8916\n",
      "Iter-16900 train loss: 0.3810 valid loss: 0.4473, valid accuracy: 0.8916\n",
      "Iter-17000 train loss: 0.4246 valid loss: 0.4457, valid accuracy: 0.8916\n",
      "Iter-17100 train loss: 0.5655 valid loss: 0.4439, valid accuracy: 0.8922\n",
      "Iter-17200 train loss: 0.3597 valid loss: 0.4424, valid accuracy: 0.8922\n",
      "Iter-17300 train loss: 0.5962 valid loss: 0.4409, valid accuracy: 0.8924\n",
      "Iter-17400 train loss: 0.4980 valid loss: 0.4394, valid accuracy: 0.8932\n",
      "Iter-17500 train loss: 0.5902 valid loss: 0.4378, valid accuracy: 0.8932\n",
      "Iter-17600 train loss: 0.4796 valid loss: 0.4363, valid accuracy: 0.8932\n",
      "Iter-17700 train loss: 0.4111 valid loss: 0.4348, valid accuracy: 0.8934\n",
      "Iter-17800 train loss: 0.6137 valid loss: 0.4334, valid accuracy: 0.8936\n",
      "Iter-17900 train loss: 0.4957 valid loss: 0.4321, valid accuracy: 0.8940\n",
      "Iter-18000 train loss: 0.4928 valid loss: 0.4306, valid accuracy: 0.8942\n",
      "Iter-18100 train loss: 0.6223 valid loss: 0.4293, valid accuracy: 0.8946\n",
      "Iter-18200 train loss: 0.4696 valid loss: 0.4278, valid accuracy: 0.8956\n",
      "Iter-18300 train loss: 0.4111 valid loss: 0.4263, valid accuracy: 0.8954\n",
      "Iter-18400 train loss: 0.2961 valid loss: 0.4249, valid accuracy: 0.8956\n",
      "Iter-18500 train loss: 0.5247 valid loss: 0.4235, valid accuracy: 0.8960\n",
      "Iter-18600 train loss: 0.4472 valid loss: 0.4223, valid accuracy: 0.8950\n",
      "Iter-18700 train loss: 0.7982 valid loss: 0.4209, valid accuracy: 0.8958\n",
      "Iter-18800 train loss: 0.5184 valid loss: 0.4195, valid accuracy: 0.8966\n",
      "Iter-18900 train loss: 0.2787 valid loss: 0.4182, valid accuracy: 0.8970\n",
      "Iter-19000 train loss: 0.6348 valid loss: 0.4170, valid accuracy: 0.8964\n",
      "Iter-19100 train loss: 0.4258 valid loss: 0.4157, valid accuracy: 0.8966\n",
      "Iter-19200 train loss: 0.5493 valid loss: 0.4145, valid accuracy: 0.8962\n",
      "Iter-19300 train loss: 0.4081 valid loss: 0.4132, valid accuracy: 0.8966\n",
      "Iter-19400 train loss: 0.4139 valid loss: 0.4121, valid accuracy: 0.8966\n",
      "Iter-19500 train loss: 0.4358 valid loss: 0.4109, valid accuracy: 0.8970\n",
      "Iter-19600 train loss: 0.3751 valid loss: 0.4096, valid accuracy: 0.8970\n",
      "Iter-19700 train loss: 0.3976 valid loss: 0.4085, valid accuracy: 0.8972\n",
      "Iter-19800 train loss: 0.5197 valid loss: 0.4074, valid accuracy: 0.8970\n",
      "Iter-19900 train loss: 0.4259 valid loss: 0.4063, valid accuracy: 0.8972\n",
      "Iter-20000 train loss: 0.3591 valid loss: 0.4050, valid accuracy: 0.8980\n",
      "Iter-20100 train loss: 0.3611 valid loss: 0.4040, valid accuracy: 0.8976\n",
      "Iter-20200 train loss: 0.5371 valid loss: 0.4028, valid accuracy: 0.8976\n",
      "Iter-20300 train loss: 0.4626 valid loss: 0.4020, valid accuracy: 0.8980\n",
      "Iter-20400 train loss: 0.4548 valid loss: 0.4011, valid accuracy: 0.8982\n",
      "Iter-20500 train loss: 0.4685 valid loss: 0.3999, valid accuracy: 0.8982\n",
      "Iter-20600 train loss: 0.4067 valid loss: 0.3988, valid accuracy: 0.8986\n",
      "Iter-20700 train loss: 0.4306 valid loss: 0.3978, valid accuracy: 0.8984\n",
      "Iter-20800 train loss: 0.3450 valid loss: 0.3971, valid accuracy: 0.8984\n",
      "Iter-20900 train loss: 0.6325 valid loss: 0.3961, valid accuracy: 0.8992\n",
      "Iter-21000 train loss: 0.3292 valid loss: 0.3948, valid accuracy: 0.8988\n",
      "Iter-21100 train loss: 0.3492 valid loss: 0.3938, valid accuracy: 0.8998\n",
      "Iter-21200 train loss: 0.5381 valid loss: 0.3927, valid accuracy: 0.9002\n",
      "Iter-21300 train loss: 0.3822 valid loss: 0.3917, valid accuracy: 0.9000\n",
      "Iter-21400 train loss: 0.5009 valid loss: 0.3908, valid accuracy: 0.9002\n",
      "Iter-21500 train loss: 0.2879 valid loss: 0.3896, valid accuracy: 0.9006\n",
      "Iter-21600 train loss: 0.2193 valid loss: 0.3886, valid accuracy: 0.9008\n",
      "Iter-21700 train loss: 0.5183 valid loss: 0.3876, valid accuracy: 0.9004\n",
      "Iter-21800 train loss: 0.3559 valid loss: 0.3867, valid accuracy: 0.9006\n",
      "Iter-21900 train loss: 0.2935 valid loss: 0.3858, valid accuracy: 0.9008\n",
      "Iter-22000 train loss: 0.3306 valid loss: 0.3847, valid accuracy: 0.9016\n",
      "Iter-22100 train loss: 0.4141 valid loss: 0.3838, valid accuracy: 0.9020\n",
      "Iter-22200 train loss: 0.4153 valid loss: 0.3828, valid accuracy: 0.9018\n",
      "Iter-22300 train loss: 0.4229 valid loss: 0.3819, valid accuracy: 0.9016\n",
      "Iter-22400 train loss: 0.5316 valid loss: 0.3812, valid accuracy: 0.9016\n",
      "Iter-22500 train loss: 0.4801 valid loss: 0.3802, valid accuracy: 0.9024\n",
      "Iter-22600 train loss: 0.3971 valid loss: 0.3794, valid accuracy: 0.9022\n",
      "Iter-22700 train loss: 0.4766 valid loss: 0.3785, valid accuracy: 0.9022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 0.2948 valid loss: 0.3778, valid accuracy: 0.9026\n",
      "Iter-22900 train loss: 0.2804 valid loss: 0.3769, valid accuracy: 0.9020\n",
      "Iter-23000 train loss: 0.3858 valid loss: 0.3762, valid accuracy: 0.9020\n",
      "Iter-23100 train loss: 0.5381 valid loss: 0.3753, valid accuracy: 0.9020\n",
      "Iter-23200 train loss: 0.4316 valid loss: 0.3744, valid accuracy: 0.9022\n",
      "Iter-23300 train loss: 0.2531 valid loss: 0.3737, valid accuracy: 0.9026\n",
      "Iter-23400 train loss: 0.4643 valid loss: 0.3731, valid accuracy: 0.9026\n",
      "Iter-23500 train loss: 0.4012 valid loss: 0.3723, valid accuracy: 0.9024\n",
      "Iter-23600 train loss: 0.2693 valid loss: 0.3718, valid accuracy: 0.9022\n",
      "Iter-23700 train loss: 0.2254 valid loss: 0.3708, valid accuracy: 0.9022\n",
      "Iter-23800 train loss: 0.4064 valid loss: 0.3700, valid accuracy: 0.9024\n",
      "Iter-23900 train loss: 0.3118 valid loss: 0.3694, valid accuracy: 0.9024\n",
      "Iter-24000 train loss: 0.4918 valid loss: 0.3687, valid accuracy: 0.9020\n",
      "Iter-24100 train loss: 0.3042 valid loss: 0.3678, valid accuracy: 0.9026\n",
      "Iter-24200 train loss: 0.4115 valid loss: 0.3670, valid accuracy: 0.9020\n",
      "Iter-24300 train loss: 0.4274 valid loss: 0.3663, valid accuracy: 0.9028\n",
      "Iter-24400 train loss: 0.4507 valid loss: 0.3655, valid accuracy: 0.9030\n",
      "Iter-24500 train loss: 0.4174 valid loss: 0.3647, valid accuracy: 0.9028\n",
      "Iter-24600 train loss: 0.3832 valid loss: 0.3640, valid accuracy: 0.9034\n",
      "Iter-24700 train loss: 0.5294 valid loss: 0.3631, valid accuracy: 0.9030\n",
      "Iter-24800 train loss: 0.4025 valid loss: 0.3624, valid accuracy: 0.9030\n",
      "Iter-24900 train loss: 0.3625 valid loss: 0.3617, valid accuracy: 0.9032\n",
      "Iter-25000 train loss: 0.4996 valid loss: 0.3612, valid accuracy: 0.9030\n",
      "Iter-25100 train loss: 0.4192 valid loss: 0.3603, valid accuracy: 0.9034\n",
      "Iter-25200 train loss: 0.5804 valid loss: 0.3597, valid accuracy: 0.9036\n",
      "Iter-25300 train loss: 0.3134 valid loss: 0.3589, valid accuracy: 0.9038\n",
      "Iter-25400 train loss: 0.2151 valid loss: 0.3584, valid accuracy: 0.9034\n",
      "Iter-25500 train loss: 0.3344 valid loss: 0.3578, valid accuracy: 0.9038\n",
      "Iter-25600 train loss: 0.3987 valid loss: 0.3571, valid accuracy: 0.9038\n",
      "Iter-25700 train loss: 0.4453 valid loss: 0.3564, valid accuracy: 0.9040\n",
      "Iter-25800 train loss: 0.3966 valid loss: 0.3557, valid accuracy: 0.9042\n",
      "Iter-25900 train loss: 0.2827 valid loss: 0.3551, valid accuracy: 0.9044\n",
      "Iter-26000 train loss: 0.5198 valid loss: 0.3545, valid accuracy: 0.9042\n",
      "Iter-26100 train loss: 0.3991 valid loss: 0.3537, valid accuracy: 0.9046\n",
      "Iter-26200 train loss: 0.3054 valid loss: 0.3529, valid accuracy: 0.9054\n",
      "Iter-26300 train loss: 0.2688 valid loss: 0.3521, valid accuracy: 0.9058\n",
      "Iter-26400 train loss: 0.4716 valid loss: 0.3516, valid accuracy: 0.9062\n",
      "Iter-26500 train loss: 0.3778 valid loss: 0.3511, valid accuracy: 0.9074\n",
      "Iter-26600 train loss: 0.3335 valid loss: 0.3506, valid accuracy: 0.9070\n",
      "Iter-26700 train loss: 0.2930 valid loss: 0.3499, valid accuracy: 0.9070\n",
      "Iter-26800 train loss: 0.3558 valid loss: 0.3495, valid accuracy: 0.9070\n",
      "Iter-26900 train loss: 0.2949 valid loss: 0.3489, valid accuracy: 0.9074\n",
      "Iter-27000 train loss: 0.4491 valid loss: 0.3483, valid accuracy: 0.9072\n",
      "Iter-27100 train loss: 0.2486 valid loss: 0.3479, valid accuracy: 0.9070\n",
      "Iter-27200 train loss: 0.2410 valid loss: 0.3474, valid accuracy: 0.9078\n",
      "Iter-27300 train loss: 0.4807 valid loss: 0.3469, valid accuracy: 0.9082\n",
      "Iter-27400 train loss: 0.1984 valid loss: 0.3463, valid accuracy: 0.9082\n",
      "Iter-27500 train loss: 0.5334 valid loss: 0.3458, valid accuracy: 0.9084\n",
      "Iter-27600 train loss: 0.2233 valid loss: 0.3453, valid accuracy: 0.9084\n",
      "Iter-27700 train loss: 0.3823 valid loss: 0.3445, valid accuracy: 0.9086\n",
      "Iter-27800 train loss: 0.4185 valid loss: 0.3441, valid accuracy: 0.9082\n",
      "Iter-27900 train loss: 0.2454 valid loss: 0.3434, valid accuracy: 0.9080\n",
      "Iter-28000 train loss: 0.2699 valid loss: 0.3429, valid accuracy: 0.9076\n",
      "Iter-28100 train loss: 0.3784 valid loss: 0.3425, valid accuracy: 0.9082\n",
      "Iter-28200 train loss: 0.3716 valid loss: 0.3419, valid accuracy: 0.9088\n",
      "Iter-28300 train loss: 0.4033 valid loss: 0.3413, valid accuracy: 0.9086\n",
      "Iter-28400 train loss: 0.2224 valid loss: 0.3407, valid accuracy: 0.9090\n",
      "Iter-28500 train loss: 0.2593 valid loss: 0.3401, valid accuracy: 0.9092\n",
      "Iter-28600 train loss: 0.5746 valid loss: 0.3396, valid accuracy: 0.9098\n",
      "Iter-28700 train loss: 0.2319 valid loss: 0.3391, valid accuracy: 0.9100\n",
      "Iter-28800 train loss: 0.3915 valid loss: 0.3386, valid accuracy: 0.9094\n",
      "Iter-28900 train loss: 0.5261 valid loss: 0.3382, valid accuracy: 0.9094\n",
      "Iter-29000 train loss: 0.4572 valid loss: 0.3378, valid accuracy: 0.9092\n",
      "Iter-29100 train loss: 0.3234 valid loss: 0.3372, valid accuracy: 0.9092\n",
      "Iter-29200 train loss: 0.1918 valid loss: 0.3366, valid accuracy: 0.9098\n",
      "Iter-29300 train loss: 0.4001 valid loss: 0.3361, valid accuracy: 0.9098\n",
      "Iter-29400 train loss: 0.2960 valid loss: 0.3358, valid accuracy: 0.9096\n",
      "Iter-29500 train loss: 0.1740 valid loss: 0.3353, valid accuracy: 0.9102\n",
      "Iter-29600 train loss: 0.3958 valid loss: 0.3349, valid accuracy: 0.9096\n",
      "Iter-29700 train loss: 0.5189 valid loss: 0.3344, valid accuracy: 0.9094\n",
      "Iter-29800 train loss: 0.2566 valid loss: 0.3337, valid accuracy: 0.9096\n",
      "Iter-29900 train loss: 0.3637 valid loss: 0.3331, valid accuracy: 0.9100\n",
      "Iter-30000 train loss: 0.4995 valid loss: 0.3325, valid accuracy: 0.9100\n",
      "Iter-30100 train loss: 0.3742 valid loss: 0.3320, valid accuracy: 0.9104\n",
      "Iter-30200 train loss: 0.4941 valid loss: 0.3316, valid accuracy: 0.9106\n",
      "Iter-30300 train loss: 0.5269 valid loss: 0.3312, valid accuracy: 0.9104\n",
      "Iter-30400 train loss: 0.3914 valid loss: 0.3306, valid accuracy: 0.9110\n",
      "Iter-30500 train loss: 0.4099 valid loss: 0.3302, valid accuracy: 0.9110\n",
      "Iter-30600 train loss: 0.3502 valid loss: 0.3298, valid accuracy: 0.9112\n",
      "Iter-30700 train loss: 0.3118 valid loss: 0.3294, valid accuracy: 0.9108\n",
      "Iter-30800 train loss: 0.5705 valid loss: 0.3291, valid accuracy: 0.9116\n",
      "Iter-30900 train loss: 0.4513 valid loss: 0.3287, valid accuracy: 0.9118\n",
      "Iter-31000 train loss: 0.5787 valid loss: 0.3283, valid accuracy: 0.9112\n",
      "Iter-31100 train loss: 0.2321 valid loss: 0.3278, valid accuracy: 0.9116\n",
      "Iter-31200 train loss: 0.1706 valid loss: 0.3274, valid accuracy: 0.9120\n",
      "Iter-31300 train loss: 0.2764 valid loss: 0.3270, valid accuracy: 0.9118\n",
      "Iter-31400 train loss: 0.2352 valid loss: 0.3266, valid accuracy: 0.9118\n",
      "Iter-31500 train loss: 0.4266 valid loss: 0.3260, valid accuracy: 0.9118\n",
      "Iter-31600 train loss: 0.3511 valid loss: 0.3256, valid accuracy: 0.9112\n",
      "Iter-31700 train loss: 0.3712 valid loss: 0.3252, valid accuracy: 0.9114\n",
      "Iter-31800 train loss: 0.3225 valid loss: 0.3247, valid accuracy: 0.9120\n",
      "Iter-31900 train loss: 0.4774 valid loss: 0.3241, valid accuracy: 0.9114\n",
      "Iter-32000 train loss: 0.2558 valid loss: 0.3238, valid accuracy: 0.9118\n",
      "Iter-32100 train loss: 0.3518 valid loss: 0.3233, valid accuracy: 0.9112\n",
      "Iter-32200 train loss: 0.2331 valid loss: 0.3229, valid accuracy: 0.9114\n",
      "Iter-32300 train loss: 0.3093 valid loss: 0.3226, valid accuracy: 0.9112\n",
      "Iter-32400 train loss: 0.3457 valid loss: 0.3220, valid accuracy: 0.9112\n",
      "Iter-32500 train loss: 0.2828 valid loss: 0.3214, valid accuracy: 0.9118\n",
      "Iter-32600 train loss: 0.2389 valid loss: 0.3211, valid accuracy: 0.9124\n",
      "Iter-32700 train loss: 0.2817 valid loss: 0.3205, valid accuracy: 0.9124\n",
      "Iter-32800 train loss: 0.3721 valid loss: 0.3203, valid accuracy: 0.9126\n",
      "Iter-32900 train loss: 0.4004 valid loss: 0.3200, valid accuracy: 0.9126\n",
      "Iter-33000 train loss: 0.2360 valid loss: 0.3194, valid accuracy: 0.9124\n",
      "Iter-33100 train loss: 0.3898 valid loss: 0.3189, valid accuracy: 0.9128\n",
      "Iter-33200 train loss: 0.3978 valid loss: 0.3185, valid accuracy: 0.9132\n",
      "Iter-33300 train loss: 0.3216 valid loss: 0.3183, valid accuracy: 0.9128\n",
      "Iter-33400 train loss: 0.3562 valid loss: 0.3178, valid accuracy: 0.9128\n",
      "Iter-33500 train loss: 0.2496 valid loss: 0.3174, valid accuracy: 0.9126\n",
      "Iter-33600 train loss: 0.3842 valid loss: 0.3172, valid accuracy: 0.9132\n",
      "Iter-33700 train loss: 0.3736 valid loss: 0.3170, valid accuracy: 0.9132\n",
      "Iter-33800 train loss: 0.2172 valid loss: 0.3167, valid accuracy: 0.9130\n",
      "Iter-33900 train loss: 0.4182 valid loss: 0.3162, valid accuracy: 0.9130\n",
      "Iter-34000 train loss: 0.4219 valid loss: 0.3159, valid accuracy: 0.9124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 0.3666 valid loss: 0.3154, valid accuracy: 0.9130\n",
      "Iter-34200 train loss: 0.3991 valid loss: 0.3150, valid accuracy: 0.9128\n",
      "Iter-34300 train loss: 0.2050 valid loss: 0.3145, valid accuracy: 0.9132\n",
      "Iter-34400 train loss: 0.6333 valid loss: 0.3141, valid accuracy: 0.9128\n",
      "Iter-34500 train loss: 0.5686 valid loss: 0.3139, valid accuracy: 0.9126\n",
      "Iter-34600 train loss: 0.3746 valid loss: 0.3135, valid accuracy: 0.9130\n",
      "Iter-34700 train loss: 0.3010 valid loss: 0.3131, valid accuracy: 0.9124\n",
      "Iter-34800 train loss: 0.4007 valid loss: 0.3126, valid accuracy: 0.9132\n",
      "Iter-34900 train loss: 0.2138 valid loss: 0.3123, valid accuracy: 0.9134\n",
      "Iter-35000 train loss: 0.6684 valid loss: 0.3118, valid accuracy: 0.9142\n",
      "Iter-35100 train loss: 0.2966 valid loss: 0.3112, valid accuracy: 0.9142\n",
      "Iter-35200 train loss: 0.3290 valid loss: 0.3109, valid accuracy: 0.9142\n",
      "Iter-35300 train loss: 0.2106 valid loss: 0.3105, valid accuracy: 0.9144\n",
      "Iter-35400 train loss: 0.3616 valid loss: 0.3100, valid accuracy: 0.9140\n",
      "Iter-35500 train loss: 0.3016 valid loss: 0.3097, valid accuracy: 0.9140\n",
      "Iter-35600 train loss: 0.2900 valid loss: 0.3094, valid accuracy: 0.9144\n",
      "Iter-35700 train loss: 0.4374 valid loss: 0.3092, valid accuracy: 0.9142\n",
      "Iter-35800 train loss: 0.4246 valid loss: 0.3089, valid accuracy: 0.9140\n",
      "Iter-35900 train loss: 0.3945 valid loss: 0.3086, valid accuracy: 0.9140\n",
      "Iter-36000 train loss: 0.2688 valid loss: 0.3085, valid accuracy: 0.9144\n",
      "Iter-36100 train loss: 0.2173 valid loss: 0.3081, valid accuracy: 0.9144\n",
      "Iter-36200 train loss: 0.2151 valid loss: 0.3079, valid accuracy: 0.9144\n",
      "Iter-36300 train loss: 0.3791 valid loss: 0.3077, valid accuracy: 0.9146\n",
      "Iter-36400 train loss: 0.1884 valid loss: 0.3074, valid accuracy: 0.9146\n",
      "Iter-36500 train loss: 0.4895 valid loss: 0.3071, valid accuracy: 0.9148\n",
      "Iter-36600 train loss: 0.2243 valid loss: 0.3067, valid accuracy: 0.9148\n",
      "Iter-36700 train loss: 0.3857 valid loss: 0.3064, valid accuracy: 0.9152\n",
      "Iter-36800 train loss: 0.3649 valid loss: 0.3061, valid accuracy: 0.9152\n",
      "Iter-36900 train loss: 0.2726 valid loss: 0.3059, valid accuracy: 0.9146\n",
      "Iter-37000 train loss: 0.3411 valid loss: 0.3055, valid accuracy: 0.9150\n",
      "Iter-37100 train loss: 0.1995 valid loss: 0.3052, valid accuracy: 0.9150\n",
      "Iter-37200 train loss: 0.2894 valid loss: 0.3049, valid accuracy: 0.9150\n",
      "Iter-37300 train loss: 0.2119 valid loss: 0.3045, valid accuracy: 0.9144\n",
      "Iter-37400 train loss: 0.2109 valid loss: 0.3042, valid accuracy: 0.9142\n",
      "Iter-37500 train loss: 0.2526 valid loss: 0.3040, valid accuracy: 0.9148\n",
      "Iter-37600 train loss: 0.3671 valid loss: 0.3036, valid accuracy: 0.9152\n",
      "Iter-37700 train loss: 0.2479 valid loss: 0.3032, valid accuracy: 0.9152\n",
      "Iter-37800 train loss: 0.2968 valid loss: 0.3026, valid accuracy: 0.9158\n",
      "Iter-37900 train loss: 0.1430 valid loss: 0.3022, valid accuracy: 0.9158\n",
      "Iter-38000 train loss: 0.3969 valid loss: 0.3019, valid accuracy: 0.9162\n",
      "Iter-38100 train loss: 0.3169 valid loss: 0.3015, valid accuracy: 0.9160\n",
      "Iter-38200 train loss: 0.3137 valid loss: 0.3013, valid accuracy: 0.9160\n",
      "Iter-38300 train loss: 0.3112 valid loss: 0.3011, valid accuracy: 0.9158\n",
      "Iter-38400 train loss: 0.4104 valid loss: 0.3007, valid accuracy: 0.9158\n",
      "Iter-38500 train loss: 0.1778 valid loss: 0.3003, valid accuracy: 0.9164\n",
      "Iter-38600 train loss: 0.2551 valid loss: 0.3001, valid accuracy: 0.9162\n",
      "Iter-38700 train loss: 0.2241 valid loss: 0.2999, valid accuracy: 0.9158\n",
      "Iter-38800 train loss: 0.4446 valid loss: 0.2996, valid accuracy: 0.9162\n",
      "Iter-38900 train loss: 0.3335 valid loss: 0.2994, valid accuracy: 0.9170\n",
      "Iter-39000 train loss: 0.1900 valid loss: 0.2989, valid accuracy: 0.9168\n",
      "Iter-39100 train loss: 0.2206 valid loss: 0.2988, valid accuracy: 0.9166\n",
      "Iter-39200 train loss: 0.2001 valid loss: 0.2984, valid accuracy: 0.9164\n",
      "Iter-39300 train loss: 0.3561 valid loss: 0.2981, valid accuracy: 0.9162\n",
      "Iter-39400 train loss: 0.5365 valid loss: 0.2978, valid accuracy: 0.9166\n",
      "Iter-39500 train loss: 0.2180 valid loss: 0.2976, valid accuracy: 0.9168\n",
      "Iter-39600 train loss: 0.1922 valid loss: 0.2977, valid accuracy: 0.9166\n",
      "Iter-39700 train loss: 0.3330 valid loss: 0.2973, valid accuracy: 0.9168\n",
      "Iter-39800 train loss: 0.2708 valid loss: 0.2970, valid accuracy: 0.9168\n",
      "Iter-39900 train loss: 0.3562 valid loss: 0.2967, valid accuracy: 0.9178\n",
      "Iter-40000 train loss: 0.5227 valid loss: 0.2964, valid accuracy: 0.9176\n",
      "Iter-40100 train loss: 0.3572 valid loss: 0.2961, valid accuracy: 0.9176\n",
      "Iter-40200 train loss: 0.4241 valid loss: 0.2956, valid accuracy: 0.9180\n",
      "Iter-40300 train loss: 0.2794 valid loss: 0.2953, valid accuracy: 0.9180\n",
      "Iter-40400 train loss: 0.3325 valid loss: 0.2951, valid accuracy: 0.9172\n",
      "Iter-40500 train loss: 0.2246 valid loss: 0.2949, valid accuracy: 0.9174\n",
      "Iter-40600 train loss: 0.3059 valid loss: 0.2947, valid accuracy: 0.9178\n",
      "Iter-40700 train loss: 0.2341 valid loss: 0.2945, valid accuracy: 0.9168\n",
      "Iter-40800 train loss: 0.2725 valid loss: 0.2942, valid accuracy: 0.9178\n",
      "Iter-40900 train loss: 0.4835 valid loss: 0.2940, valid accuracy: 0.9176\n",
      "Iter-41000 train loss: 0.3343 valid loss: 0.2938, valid accuracy: 0.9174\n",
      "Iter-41100 train loss: 0.1466 valid loss: 0.2935, valid accuracy: 0.9168\n",
      "Iter-41200 train loss: 0.3133 valid loss: 0.2931, valid accuracy: 0.9166\n",
      "Iter-41300 train loss: 0.4345 valid loss: 0.2928, valid accuracy: 0.9168\n",
      "Iter-41400 train loss: 0.2500 valid loss: 0.2925, valid accuracy: 0.9176\n",
      "Iter-41500 train loss: 0.2303 valid loss: 0.2922, valid accuracy: 0.9182\n",
      "Iter-41600 train loss: 0.3391 valid loss: 0.2919, valid accuracy: 0.9186\n",
      "Iter-41700 train loss: 0.3350 valid loss: 0.2917, valid accuracy: 0.9180\n",
      "Iter-41800 train loss: 0.2358 valid loss: 0.2914, valid accuracy: 0.9184\n",
      "Iter-41900 train loss: 0.2049 valid loss: 0.2910, valid accuracy: 0.9186\n",
      "Iter-42000 train loss: 0.2270 valid loss: 0.2908, valid accuracy: 0.9190\n",
      "Iter-42100 train loss: 0.4442 valid loss: 0.2906, valid accuracy: 0.9186\n",
      "Iter-42200 train loss: 0.1879 valid loss: 0.2904, valid accuracy: 0.9186\n",
      "Iter-42300 train loss: 0.2378 valid loss: 0.2903, valid accuracy: 0.9186\n",
      "Iter-42400 train loss: 0.3204 valid loss: 0.2899, valid accuracy: 0.9192\n",
      "Iter-42500 train loss: 0.4027 valid loss: 0.2898, valid accuracy: 0.9196\n",
      "Iter-42600 train loss: 0.1632 valid loss: 0.2895, valid accuracy: 0.9196\n",
      "Iter-42700 train loss: 0.1435 valid loss: 0.2893, valid accuracy: 0.9192\n",
      "Iter-42800 train loss: 0.1969 valid loss: 0.2892, valid accuracy: 0.9192\n",
      "Iter-42900 train loss: 0.3383 valid loss: 0.2889, valid accuracy: 0.9190\n",
      "Iter-43000 train loss: 0.3981 valid loss: 0.2886, valid accuracy: 0.9184\n",
      "Iter-43100 train loss: 0.2698 valid loss: 0.2884, valid accuracy: 0.9180\n",
      "Iter-43200 train loss: 0.2013 valid loss: 0.2880, valid accuracy: 0.9186\n",
      "Iter-43300 train loss: 0.3816 valid loss: 0.2878, valid accuracy: 0.9186\n",
      "Iter-43400 train loss: 0.3487 valid loss: 0.2877, valid accuracy: 0.9196\n",
      "Iter-43500 train loss: 0.3429 valid loss: 0.2874, valid accuracy: 0.9196\n",
      "Iter-43600 train loss: 0.2162 valid loss: 0.2872, valid accuracy: 0.9196\n",
      "Iter-43700 train loss: 0.3078 valid loss: 0.2871, valid accuracy: 0.9188\n",
      "Iter-43800 train loss: 0.3337 valid loss: 0.2867, valid accuracy: 0.9186\n",
      "Iter-43900 train loss: 0.1643 valid loss: 0.2865, valid accuracy: 0.9188\n",
      "Iter-44000 train loss: 0.3126 valid loss: 0.2861, valid accuracy: 0.9196\n",
      "Iter-44100 train loss: 0.3045 valid loss: 0.2859, valid accuracy: 0.9196\n",
      "Iter-44200 train loss: 0.1859 valid loss: 0.2856, valid accuracy: 0.9196\n",
      "Iter-44300 train loss: 0.2434 valid loss: 0.2853, valid accuracy: 0.9200\n",
      "Iter-44400 train loss: 0.2622 valid loss: 0.2852, valid accuracy: 0.9194\n",
      "Iter-44500 train loss: 0.3151 valid loss: 0.2850, valid accuracy: 0.9194\n",
      "Iter-44600 train loss: 0.3020 valid loss: 0.2848, valid accuracy: 0.9192\n",
      "Iter-44700 train loss: 0.1629 valid loss: 0.2847, valid accuracy: 0.9196\n",
      "Iter-44800 train loss: 0.2483 valid loss: 0.2847, valid accuracy: 0.9202\n",
      "Iter-44900 train loss: 0.4520 valid loss: 0.2843, valid accuracy: 0.9200\n",
      "Iter-45000 train loss: 0.3092 valid loss: 0.2841, valid accuracy: 0.9200\n",
      "Iter-45100 train loss: 0.2294 valid loss: 0.2836, valid accuracy: 0.9198\n",
      "Iter-45200 train loss: 0.1943 valid loss: 0.2834, valid accuracy: 0.9202\n",
      "Iter-45300 train loss: 0.3108 valid loss: 0.2830, valid accuracy: 0.9198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 0.3624 valid loss: 0.2827, valid accuracy: 0.9202\n",
      "Iter-45500 train loss: 0.2520 valid loss: 0.2827, valid accuracy: 0.9200\n",
      "Iter-45600 train loss: 0.2351 valid loss: 0.2822, valid accuracy: 0.9200\n",
      "Iter-45700 train loss: 0.1338 valid loss: 0.2820, valid accuracy: 0.9194\n",
      "Iter-45800 train loss: 0.3376 valid loss: 0.2819, valid accuracy: 0.9192\n",
      "Iter-45900 train loss: 0.1467 valid loss: 0.2818, valid accuracy: 0.9198\n",
      "Iter-46000 train loss: 0.2095 valid loss: 0.2815, valid accuracy: 0.9202\n",
      "Iter-46100 train loss: 0.2189 valid loss: 0.2814, valid accuracy: 0.9200\n",
      "Iter-46200 train loss: 0.2221 valid loss: 0.2813, valid accuracy: 0.9200\n",
      "Iter-46300 train loss: 0.3051 valid loss: 0.2811, valid accuracy: 0.9202\n",
      "Iter-46400 train loss: 0.1813 valid loss: 0.2810, valid accuracy: 0.9202\n",
      "Iter-46500 train loss: 0.4076 valid loss: 0.2808, valid accuracy: 0.9206\n",
      "Iter-46600 train loss: 0.2940 valid loss: 0.2806, valid accuracy: 0.9208\n",
      "Iter-46700 train loss: 0.2404 valid loss: 0.2804, valid accuracy: 0.9212\n",
      "Iter-46800 train loss: 0.2221 valid loss: 0.2800, valid accuracy: 0.9214\n",
      "Iter-46900 train loss: 0.3213 valid loss: 0.2796, valid accuracy: 0.9214\n",
      "Iter-47000 train loss: 0.1801 valid loss: 0.2795, valid accuracy: 0.9216\n",
      "Iter-47100 train loss: 0.2156 valid loss: 0.2794, valid accuracy: 0.9214\n",
      "Iter-47200 train loss: 0.4287 valid loss: 0.2793, valid accuracy: 0.9212\n",
      "Iter-47300 train loss: 0.3721 valid loss: 0.2789, valid accuracy: 0.9210\n",
      "Iter-47400 train loss: 0.3471 valid loss: 0.2786, valid accuracy: 0.9208\n",
      "Iter-47500 train loss: 0.2224 valid loss: 0.2783, valid accuracy: 0.9212\n",
      "Iter-47600 train loss: 0.3297 valid loss: 0.2780, valid accuracy: 0.9208\n",
      "Iter-47700 train loss: 0.2136 valid loss: 0.2778, valid accuracy: 0.9218\n",
      "Iter-47800 train loss: 0.2948 valid loss: 0.2776, valid accuracy: 0.9218\n",
      "Iter-47900 train loss: 0.5285 valid loss: 0.2776, valid accuracy: 0.9214\n",
      "Iter-48000 train loss: 0.2710 valid loss: 0.2775, valid accuracy: 0.9216\n",
      "Iter-48100 train loss: 0.2421 valid loss: 0.2774, valid accuracy: 0.9212\n",
      "Iter-48200 train loss: 0.3435 valid loss: 0.2773, valid accuracy: 0.9218\n",
      "Iter-48300 train loss: 0.3691 valid loss: 0.2771, valid accuracy: 0.9212\n",
      "Iter-48400 train loss: 0.2288 valid loss: 0.2771, valid accuracy: 0.9214\n",
      "Iter-48500 train loss: 0.3210 valid loss: 0.2769, valid accuracy: 0.9222\n",
      "Iter-48600 train loss: 0.0999 valid loss: 0.2766, valid accuracy: 0.9224\n",
      "Iter-48700 train loss: 0.1395 valid loss: 0.2765, valid accuracy: 0.9216\n",
      "Iter-48800 train loss: 0.2613 valid loss: 0.2763, valid accuracy: 0.9218\n",
      "Iter-48900 train loss: 0.3813 valid loss: 0.2760, valid accuracy: 0.9222\n",
      "Iter-49000 train loss: 0.3250 valid loss: 0.2758, valid accuracy: 0.9228\n",
      "Iter-49100 train loss: 0.1809 valid loss: 0.2756, valid accuracy: 0.9228\n",
      "Iter-49200 train loss: 0.3676 valid loss: 0.2754, valid accuracy: 0.9222\n",
      "Iter-49300 train loss: 0.3349 valid loss: 0.2751, valid accuracy: 0.9222\n",
      "Iter-49400 train loss: 0.1775 valid loss: 0.2750, valid accuracy: 0.9220\n",
      "Iter-49500 train loss: 0.3284 valid loss: 0.2746, valid accuracy: 0.9216\n",
      "Iter-49600 train loss: 0.4900 valid loss: 0.2743, valid accuracy: 0.9220\n",
      "Iter-49700 train loss: 0.5133 valid loss: 0.2741, valid accuracy: 0.9224\n",
      "Iter-49800 train loss: 0.2582 valid loss: 0.2741, valid accuracy: 0.9226\n",
      "Iter-49900 train loss: 0.4092 valid loss: 0.2736, valid accuracy: 0.9232\n",
      "Iter-50000 train loss: 0.3426 valid loss: 0.2733, valid accuracy: 0.9230\n",
      "Iter-50100 train loss: 0.1905 valid loss: 0.2732, valid accuracy: 0.9230\n",
      "Iter-50200 train loss: 0.5004 valid loss: 0.2730, valid accuracy: 0.9234\n",
      "Iter-50300 train loss: 0.3821 valid loss: 0.2729, valid accuracy: 0.9228\n",
      "Iter-50400 train loss: 0.3943 valid loss: 0.2727, valid accuracy: 0.9226\n",
      "Iter-50500 train loss: 0.4118 valid loss: 0.2725, valid accuracy: 0.9228\n",
      "Iter-50600 train loss: 0.3211 valid loss: 0.2725, valid accuracy: 0.9226\n",
      "Iter-50700 train loss: 0.1705 valid loss: 0.2722, valid accuracy: 0.9220\n",
      "Iter-50800 train loss: 0.2916 valid loss: 0.2720, valid accuracy: 0.9228\n",
      "Iter-50900 train loss: 0.2504 valid loss: 0.2719, valid accuracy: 0.9226\n",
      "Iter-51000 train loss: 0.2606 valid loss: 0.2717, valid accuracy: 0.9228\n",
      "Iter-51100 train loss: 0.3983 valid loss: 0.2715, valid accuracy: 0.9228\n",
      "Iter-51200 train loss: 0.3780 valid loss: 0.2714, valid accuracy: 0.9222\n",
      "Iter-51300 train loss: 0.3759 valid loss: 0.2712, valid accuracy: 0.9228\n",
      "Iter-51400 train loss: 0.3816 valid loss: 0.2709, valid accuracy: 0.9228\n",
      "Iter-51500 train loss: 0.3581 valid loss: 0.2707, valid accuracy: 0.9232\n",
      "Iter-51600 train loss: 0.1333 valid loss: 0.2704, valid accuracy: 0.9230\n",
      "Iter-51700 train loss: 0.2598 valid loss: 0.2700, valid accuracy: 0.9240\n",
      "Iter-51800 train loss: 0.1479 valid loss: 0.2698, valid accuracy: 0.9236\n",
      "Iter-51900 train loss: 0.1664 valid loss: 0.2695, valid accuracy: 0.9242\n",
      "Iter-52000 train loss: 0.4216 valid loss: 0.2693, valid accuracy: 0.9242\n",
      "Iter-52100 train loss: 0.3296 valid loss: 0.2692, valid accuracy: 0.9244\n",
      "Iter-52200 train loss: 0.3034 valid loss: 0.2691, valid accuracy: 0.9238\n",
      "Iter-52300 train loss: 0.1978 valid loss: 0.2690, valid accuracy: 0.9240\n",
      "Iter-52400 train loss: 0.1464 valid loss: 0.2689, valid accuracy: 0.9244\n",
      "Iter-52500 train loss: 0.2813 valid loss: 0.2687, valid accuracy: 0.9242\n",
      "Iter-52600 train loss: 0.3695 valid loss: 0.2688, valid accuracy: 0.9240\n",
      "Iter-52700 train loss: 0.2183 valid loss: 0.2685, valid accuracy: 0.9246\n",
      "Iter-52800 train loss: 0.3260 valid loss: 0.2684, valid accuracy: 0.9244\n",
      "Iter-52900 train loss: 0.1944 valid loss: 0.2683, valid accuracy: 0.9246\n",
      "Iter-53000 train loss: 0.2320 valid loss: 0.2680, valid accuracy: 0.9240\n",
      "Iter-53100 train loss: 0.2744 valid loss: 0.2678, valid accuracy: 0.9248\n",
      "Iter-53200 train loss: 0.2589 valid loss: 0.2676, valid accuracy: 0.9244\n",
      "Iter-53300 train loss: 0.1824 valid loss: 0.2674, valid accuracy: 0.9244\n",
      "Iter-53400 train loss: 0.3223 valid loss: 0.2670, valid accuracy: 0.9246\n",
      "Iter-53500 train loss: 0.1926 valid loss: 0.2670, valid accuracy: 0.9244\n",
      "Iter-53600 train loss: 0.2878 valid loss: 0.2666, valid accuracy: 0.9244\n",
      "Iter-53700 train loss: 0.2794 valid loss: 0.2664, valid accuracy: 0.9244\n",
      "Iter-53800 train loss: 0.3448 valid loss: 0.2663, valid accuracy: 0.9246\n",
      "Iter-53900 train loss: 0.2814 valid loss: 0.2662, valid accuracy: 0.9242\n",
      "Iter-54000 train loss: 0.1272 valid loss: 0.2661, valid accuracy: 0.9242\n",
      "Iter-54100 train loss: 0.3239 valid loss: 0.2661, valid accuracy: 0.9244\n",
      "Iter-54200 train loss: 0.2411 valid loss: 0.2660, valid accuracy: 0.9254\n",
      "Iter-54300 train loss: 0.4170 valid loss: 0.2658, valid accuracy: 0.9244\n",
      "Iter-54400 train loss: 0.2239 valid loss: 0.2656, valid accuracy: 0.9244\n",
      "Iter-54500 train loss: 0.3603 valid loss: 0.2654, valid accuracy: 0.9250\n",
      "Iter-54600 train loss: 0.3115 valid loss: 0.2653, valid accuracy: 0.9248\n",
      "Iter-54700 train loss: 0.2124 valid loss: 0.2653, valid accuracy: 0.9250\n",
      "Iter-54800 train loss: 0.3230 valid loss: 0.2651, valid accuracy: 0.9250\n",
      "Iter-54900 train loss: 0.1722 valid loss: 0.2650, valid accuracy: 0.9250\n",
      "Iter-55000 train loss: 0.1621 valid loss: 0.2649, valid accuracy: 0.9248\n",
      "Iter-55100 train loss: 0.4040 valid loss: 0.2646, valid accuracy: 0.9248\n",
      "Iter-55200 train loss: 0.4598 valid loss: 0.2644, valid accuracy: 0.9250\n",
      "Iter-55300 train loss: 0.4756 valid loss: 0.2641, valid accuracy: 0.9252\n",
      "Iter-55400 train loss: 0.3354 valid loss: 0.2640, valid accuracy: 0.9250\n",
      "Iter-55500 train loss: 0.2738 valid loss: 0.2639, valid accuracy: 0.9248\n",
      "Iter-55600 train loss: 0.2530 valid loss: 0.2640, valid accuracy: 0.9248\n",
      "Iter-55700 train loss: 0.3036 valid loss: 0.2640, valid accuracy: 0.9250\n",
      "Iter-55800 train loss: 0.2960 valid loss: 0.2637, valid accuracy: 0.9254\n",
      "Iter-55900 train loss: 0.1815 valid loss: 0.2634, valid accuracy: 0.9256\n",
      "Iter-56000 train loss: 0.3422 valid loss: 0.2633, valid accuracy: 0.9264\n",
      "Iter-56100 train loss: 0.6758 valid loss: 0.2630, valid accuracy: 0.9258\n",
      "Iter-56200 train loss: 0.2488 valid loss: 0.2627, valid accuracy: 0.9258\n",
      "Iter-56300 train loss: 0.2860 valid loss: 0.2627, valid accuracy: 0.9262\n",
      "Iter-56400 train loss: 0.1741 valid loss: 0.2624, valid accuracy: 0.9260\n",
      "Iter-56500 train loss: 0.3742 valid loss: 0.2621, valid accuracy: 0.9260\n",
      "Iter-56600 train loss: 0.2529 valid loss: 0.2620, valid accuracy: 0.9256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 0.2413 valid loss: 0.2618, valid accuracy: 0.9258\n",
      "Iter-56800 train loss: 0.3071 valid loss: 0.2616, valid accuracy: 0.9262\n",
      "Iter-56900 train loss: 0.3254 valid loss: 0.2613, valid accuracy: 0.9260\n",
      "Iter-57000 train loss: 0.2210 valid loss: 0.2611, valid accuracy: 0.9266\n",
      "Iter-57100 train loss: 0.1897 valid loss: 0.2610, valid accuracy: 0.9270\n",
      "Iter-57200 train loss: 0.3724 valid loss: 0.2610, valid accuracy: 0.9264\n",
      "Iter-57300 train loss: 0.1617 valid loss: 0.2608, valid accuracy: 0.9260\n",
      "Iter-57400 train loss: 0.1520 valid loss: 0.2608, valid accuracy: 0.9260\n",
      "Iter-57500 train loss: 0.2804 valid loss: 0.2610, valid accuracy: 0.9262\n",
      "Iter-57600 train loss: 0.4790 valid loss: 0.2608, valid accuracy: 0.9264\n",
      "Iter-57700 train loss: 0.2890 valid loss: 0.2606, valid accuracy: 0.9266\n",
      "Iter-57800 train loss: 0.2351 valid loss: 0.2605, valid accuracy: 0.9266\n",
      "Iter-57900 train loss: 0.2157 valid loss: 0.2603, valid accuracy: 0.9262\n",
      "Iter-58000 train loss: 0.3032 valid loss: 0.2600, valid accuracy: 0.9268\n",
      "Iter-58100 train loss: 0.2161 valid loss: 0.2597, valid accuracy: 0.9268\n",
      "Iter-58200 train loss: 0.2863 valid loss: 0.2597, valid accuracy: 0.9264\n",
      "Iter-58300 train loss: 0.2168 valid loss: 0.2595, valid accuracy: 0.9262\n",
      "Iter-58400 train loss: 0.1326 valid loss: 0.2592, valid accuracy: 0.9272\n",
      "Iter-58500 train loss: 0.2461 valid loss: 0.2590, valid accuracy: 0.9276\n",
      "Iter-58600 train loss: 0.2997 valid loss: 0.2590, valid accuracy: 0.9276\n",
      "Iter-58700 train loss: 0.1386 valid loss: 0.2589, valid accuracy: 0.9268\n",
      "Iter-58800 train loss: 0.2371 valid loss: 0.2586, valid accuracy: 0.9276\n",
      "Iter-58900 train loss: 0.2238 valid loss: 0.2587, valid accuracy: 0.9274\n",
      "Iter-59000 train loss: 0.3285 valid loss: 0.2585, valid accuracy: 0.9272\n",
      "Iter-59100 train loss: 0.1575 valid loss: 0.2583, valid accuracy: 0.9278\n",
      "Iter-59200 train loss: 0.3059 valid loss: 0.2583, valid accuracy: 0.9272\n",
      "Iter-59300 train loss: 0.2124 valid loss: 0.2581, valid accuracy: 0.9274\n",
      "Iter-59400 train loss: 0.2042 valid loss: 0.2580, valid accuracy: 0.9268\n",
      "Iter-59500 train loss: 0.1293 valid loss: 0.2578, valid accuracy: 0.9266\n",
      "Iter-59600 train loss: 0.1502 valid loss: 0.2578, valid accuracy: 0.9262\n",
      "Iter-59700 train loss: 0.4598 valid loss: 0.2576, valid accuracy: 0.9262\n",
      "Iter-59800 train loss: 0.3301 valid loss: 0.2574, valid accuracy: 0.9272\n",
      "Iter-59900 train loss: 0.2486 valid loss: 0.2570, valid accuracy: 0.9266\n",
      "Iter-60000 train loss: 0.1281 valid loss: 0.2568, valid accuracy: 0.9270\n",
      "Iter-60100 train loss: 0.2653 valid loss: 0.2565, valid accuracy: 0.9272\n",
      "Iter-60200 train loss: 0.1271 valid loss: 0.2564, valid accuracy: 0.9270\n",
      "Iter-60300 train loss: 0.3099 valid loss: 0.2560, valid accuracy: 0.9270\n",
      "Iter-60400 train loss: 0.3864 valid loss: 0.2559, valid accuracy: 0.9268\n",
      "Iter-60500 train loss: 0.1979 valid loss: 0.2560, valid accuracy: 0.9262\n",
      "Iter-60600 train loss: 0.4313 valid loss: 0.2559, valid accuracy: 0.9260\n",
      "Iter-60700 train loss: 0.2083 valid loss: 0.2560, valid accuracy: 0.9258\n",
      "Iter-60800 train loss: 0.2209 valid loss: 0.2559, valid accuracy: 0.9262\n",
      "Iter-60900 train loss: 0.2257 valid loss: 0.2557, valid accuracy: 0.9268\n",
      "Iter-61000 train loss: 0.2357 valid loss: 0.2553, valid accuracy: 0.9272\n",
      "Iter-61100 train loss: 0.1267 valid loss: 0.2551, valid accuracy: 0.9266\n",
      "Iter-61200 train loss: 0.4698 valid loss: 0.2551, valid accuracy: 0.9270\n",
      "Iter-61300 train loss: 0.2095 valid loss: 0.2550, valid accuracy: 0.9274\n",
      "Iter-61400 train loss: 0.3833 valid loss: 0.2548, valid accuracy: 0.9274\n",
      "Iter-61500 train loss: 0.1984 valid loss: 0.2546, valid accuracy: 0.9274\n",
      "Iter-61600 train loss: 0.1864 valid loss: 0.2543, valid accuracy: 0.9274\n",
      "Iter-61700 train loss: 0.1917 valid loss: 0.2541, valid accuracy: 0.9274\n",
      "Iter-61800 train loss: 0.1617 valid loss: 0.2539, valid accuracy: 0.9274\n",
      "Iter-61900 train loss: 0.2748 valid loss: 0.2539, valid accuracy: 0.9272\n",
      "Iter-62000 train loss: 0.3331 valid loss: 0.2536, valid accuracy: 0.9274\n",
      "Iter-62100 train loss: 0.2347 valid loss: 0.2534, valid accuracy: 0.9272\n",
      "Iter-62200 train loss: 0.4874 valid loss: 0.2533, valid accuracy: 0.9274\n",
      "Iter-62300 train loss: 0.2759 valid loss: 0.2532, valid accuracy: 0.9280\n",
      "Iter-62400 train loss: 0.4082 valid loss: 0.2531, valid accuracy: 0.9280\n",
      "Iter-62500 train loss: 0.1096 valid loss: 0.2529, valid accuracy: 0.9282\n",
      "Iter-62600 train loss: 0.1811 valid loss: 0.2527, valid accuracy: 0.9278\n",
      "Iter-62700 train loss: 0.5597 valid loss: 0.2526, valid accuracy: 0.9278\n",
      "Iter-62800 train loss: 0.2793 valid loss: 0.2524, valid accuracy: 0.9278\n",
      "Iter-62900 train loss: 0.3045 valid loss: 0.2523, valid accuracy: 0.9276\n",
      "Iter-63000 train loss: 0.3047 valid loss: 0.2522, valid accuracy: 0.9276\n",
      "Iter-63100 train loss: 0.1439 valid loss: 0.2521, valid accuracy: 0.9280\n",
      "Iter-63200 train loss: 0.2483 valid loss: 0.2521, valid accuracy: 0.9276\n",
      "Iter-63300 train loss: 0.3198 valid loss: 0.2520, valid accuracy: 0.9276\n",
      "Iter-63400 train loss: 0.1314 valid loss: 0.2520, valid accuracy: 0.9276\n",
      "Iter-63500 train loss: 0.4143 valid loss: 0.2519, valid accuracy: 0.9278\n",
      "Iter-63600 train loss: 0.1937 valid loss: 0.2516, valid accuracy: 0.9278\n",
      "Iter-63700 train loss: 0.2851 valid loss: 0.2513, valid accuracy: 0.9282\n",
      "Iter-63800 train loss: 0.3351 valid loss: 0.2511, valid accuracy: 0.9280\n",
      "Iter-63900 train loss: 0.2320 valid loss: 0.2509, valid accuracy: 0.9282\n",
      "Iter-64000 train loss: 0.3357 valid loss: 0.2508, valid accuracy: 0.9280\n",
      "Iter-64100 train loss: 0.1510 valid loss: 0.2504, valid accuracy: 0.9284\n",
      "Iter-64200 train loss: 0.3527 valid loss: 0.2504, valid accuracy: 0.9286\n",
      "Iter-64300 train loss: 0.3123 valid loss: 0.2502, valid accuracy: 0.9284\n",
      "Iter-64400 train loss: 0.1985 valid loss: 0.2501, valid accuracy: 0.9284\n",
      "Iter-64500 train loss: 0.1959 valid loss: 0.2501, valid accuracy: 0.9280\n",
      "Iter-64600 train loss: 0.2421 valid loss: 0.2501, valid accuracy: 0.9280\n",
      "Iter-64700 train loss: 0.2761 valid loss: 0.2500, valid accuracy: 0.9280\n",
      "Iter-64800 train loss: 0.3096 valid loss: 0.2499, valid accuracy: 0.9286\n",
      "Iter-64900 train loss: 0.3401 valid loss: 0.2497, valid accuracy: 0.9290\n",
      "Iter-65000 train loss: 0.1350 valid loss: 0.2494, valid accuracy: 0.9288\n",
      "Iter-65100 train loss: 0.2740 valid loss: 0.2492, valid accuracy: 0.9282\n",
      "Iter-65200 train loss: 0.1571 valid loss: 0.2489, valid accuracy: 0.9282\n",
      "Iter-65300 train loss: 0.1761 valid loss: 0.2486, valid accuracy: 0.9282\n",
      "Iter-65400 train loss: 0.1990 valid loss: 0.2486, valid accuracy: 0.9284\n",
      "Iter-65500 train loss: 0.1612 valid loss: 0.2485, valid accuracy: 0.9286\n",
      "Iter-65600 train loss: 0.2277 valid loss: 0.2482, valid accuracy: 0.9282\n",
      "Iter-65700 train loss: 0.3136 valid loss: 0.2481, valid accuracy: 0.9280\n",
      "Iter-65800 train loss: 0.2589 valid loss: 0.2480, valid accuracy: 0.9284\n",
      "Iter-65900 train loss: 0.3047 valid loss: 0.2478, valid accuracy: 0.9282\n",
      "Iter-66000 train loss: 0.3820 valid loss: 0.2478, valid accuracy: 0.9282\n",
      "Iter-66100 train loss: 0.3943 valid loss: 0.2475, valid accuracy: 0.9286\n",
      "Iter-66200 train loss: 0.6002 valid loss: 0.2475, valid accuracy: 0.9284\n",
      "Iter-66300 train loss: 0.4065 valid loss: 0.2474, valid accuracy: 0.9286\n",
      "Iter-66400 train loss: 0.3683 valid loss: 0.2471, valid accuracy: 0.9286\n",
      "Iter-66500 train loss: 0.1394 valid loss: 0.2471, valid accuracy: 0.9286\n",
      "Iter-66600 train loss: 0.3000 valid loss: 0.2471, valid accuracy: 0.9284\n",
      "Iter-66700 train loss: 0.5232 valid loss: 0.2468, valid accuracy: 0.9288\n",
      "Iter-66800 train loss: 0.1959 valid loss: 0.2466, valid accuracy: 0.9286\n",
      "Iter-66900 train loss: 0.2436 valid loss: 0.2464, valid accuracy: 0.9288\n",
      "Iter-67000 train loss: 0.2872 valid loss: 0.2464, valid accuracy: 0.9288\n",
      "Iter-67100 train loss: 0.1540 valid loss: 0.2461, valid accuracy: 0.9292\n",
      "Iter-67200 train loss: 0.1947 valid loss: 0.2461, valid accuracy: 0.9292\n",
      "Iter-67300 train loss: 0.5155 valid loss: 0.2461, valid accuracy: 0.9282\n",
      "Iter-67400 train loss: 0.4360 valid loss: 0.2459, valid accuracy: 0.9288\n",
      "Iter-67500 train loss: 0.2471 valid loss: 0.2459, valid accuracy: 0.9284\n",
      "Iter-67600 train loss: 0.2845 valid loss: 0.2458, valid accuracy: 0.9284\n",
      "Iter-67700 train loss: 0.2763 valid loss: 0.2455, valid accuracy: 0.9292\n",
      "Iter-67800 train loss: 0.2379 valid loss: 0.2454, valid accuracy: 0.9288\n",
      "Iter-67900 train loss: 0.1004 valid loss: 0.2453, valid accuracy: 0.9294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 0.3365 valid loss: 0.2452, valid accuracy: 0.9302\n",
      "Iter-68100 train loss: 0.2091 valid loss: 0.2452, valid accuracy: 0.9298\n",
      "Iter-68200 train loss: 0.1815 valid loss: 0.2453, valid accuracy: 0.9294\n",
      "Iter-68300 train loss: 0.3544 valid loss: 0.2449, valid accuracy: 0.9296\n",
      "Iter-68400 train loss: 0.3415 valid loss: 0.2447, valid accuracy: 0.9298\n",
      "Iter-68500 train loss: 0.2254 valid loss: 0.2447, valid accuracy: 0.9298\n",
      "Iter-68600 train loss: 0.4278 valid loss: 0.2447, valid accuracy: 0.9300\n",
      "Iter-68700 train loss: 0.2742 valid loss: 0.2445, valid accuracy: 0.9300\n",
      "Iter-68800 train loss: 0.3473 valid loss: 0.2443, valid accuracy: 0.9302\n",
      "Iter-68900 train loss: 0.1573 valid loss: 0.2441, valid accuracy: 0.9304\n",
      "Iter-69000 train loss: 0.1319 valid loss: 0.2440, valid accuracy: 0.9304\n",
      "Iter-69100 train loss: 0.3436 valid loss: 0.2440, valid accuracy: 0.9306\n",
      "Iter-69200 train loss: 0.1660 valid loss: 0.2439, valid accuracy: 0.9308\n",
      "Iter-69300 train loss: 0.0934 valid loss: 0.2437, valid accuracy: 0.9308\n",
      "Iter-69400 train loss: 0.3518 valid loss: 0.2436, valid accuracy: 0.9304\n",
      "Iter-69500 train loss: 0.2758 valid loss: 0.2434, valid accuracy: 0.9306\n",
      "Iter-69600 train loss: 0.5153 valid loss: 0.2431, valid accuracy: 0.9312\n",
      "Iter-69700 train loss: 0.1658 valid loss: 0.2428, valid accuracy: 0.9306\n",
      "Iter-69800 train loss: 0.1848 valid loss: 0.2428, valid accuracy: 0.9316\n",
      "Iter-69900 train loss: 0.2329 valid loss: 0.2428, valid accuracy: 0.9312\n",
      "Iter-70000 train loss: 0.2295 valid loss: 0.2426, valid accuracy: 0.9308\n",
      "Iter-70100 train loss: 0.3422 valid loss: 0.2424, valid accuracy: 0.9312\n",
      "Iter-70200 train loss: 0.2072 valid loss: 0.2423, valid accuracy: 0.9312\n",
      "Iter-70300 train loss: 0.3300 valid loss: 0.2422, valid accuracy: 0.9312\n",
      "Iter-70400 train loss: 0.1111 valid loss: 0.2422, valid accuracy: 0.9308\n",
      "Iter-70500 train loss: 0.1726 valid loss: 0.2420, valid accuracy: 0.9302\n",
      "Iter-70600 train loss: 0.3042 valid loss: 0.2418, valid accuracy: 0.9304\n",
      "Iter-70700 train loss: 0.2088 valid loss: 0.2417, valid accuracy: 0.9304\n",
      "Iter-70800 train loss: 0.2730 valid loss: 0.2416, valid accuracy: 0.9302\n",
      "Iter-70900 train loss: 0.1302 valid loss: 0.2415, valid accuracy: 0.9296\n",
      "Iter-71000 train loss: 0.3743 valid loss: 0.2415, valid accuracy: 0.9300\n",
      "Iter-71100 train loss: 0.2916 valid loss: 0.2411, valid accuracy: 0.9300\n",
      "Iter-71200 train loss: 0.2137 valid loss: 0.2410, valid accuracy: 0.9300\n",
      "Iter-71300 train loss: 0.1848 valid loss: 0.2409, valid accuracy: 0.9290\n",
      "Iter-71400 train loss: 0.1942 valid loss: 0.2407, valid accuracy: 0.9294\n",
      "Iter-71500 train loss: 0.1328 valid loss: 0.2405, valid accuracy: 0.9300\n",
      "Iter-71600 train loss: 0.0994 valid loss: 0.2404, valid accuracy: 0.9294\n",
      "Iter-71700 train loss: 0.2023 valid loss: 0.2403, valid accuracy: 0.9296\n",
      "Iter-71800 train loss: 0.3108 valid loss: 0.2403, valid accuracy: 0.9292\n",
      "Iter-71900 train loss: 0.3595 valid loss: 0.2404, valid accuracy: 0.9292\n",
      "Iter-72000 train loss: 0.2872 valid loss: 0.2401, valid accuracy: 0.9294\n",
      "Iter-72100 train loss: 0.1926 valid loss: 0.2400, valid accuracy: 0.9296\n",
      "Iter-72200 train loss: 0.3502 valid loss: 0.2400, valid accuracy: 0.9300\n",
      "Iter-72300 train loss: 0.2385 valid loss: 0.2398, valid accuracy: 0.9308\n",
      "Iter-72400 train loss: 0.3726 valid loss: 0.2396, valid accuracy: 0.9306\n",
      "Iter-72500 train loss: 0.1635 valid loss: 0.2395, valid accuracy: 0.9308\n",
      "Iter-72600 train loss: 0.2144 valid loss: 0.2395, valid accuracy: 0.9302\n",
      "Iter-72700 train loss: 0.2759 valid loss: 0.2394, valid accuracy: 0.9308\n",
      "Iter-72800 train loss: 0.2732 valid loss: 0.2395, valid accuracy: 0.9314\n",
      "Iter-72900 train loss: 0.2764 valid loss: 0.2392, valid accuracy: 0.9310\n",
      "Iter-73000 train loss: 0.2454 valid loss: 0.2390, valid accuracy: 0.9310\n",
      "Iter-73100 train loss: 0.1341 valid loss: 0.2389, valid accuracy: 0.9318\n",
      "Iter-73200 train loss: 0.1577 valid loss: 0.2388, valid accuracy: 0.9320\n",
      "Iter-73300 train loss: 0.2364 valid loss: 0.2387, valid accuracy: 0.9318\n",
      "Iter-73400 train loss: 0.1093 valid loss: 0.2385, valid accuracy: 0.9314\n",
      "Iter-73500 train loss: 0.1554 valid loss: 0.2383, valid accuracy: 0.9316\n",
      "Iter-73600 train loss: 0.2410 valid loss: 0.2381, valid accuracy: 0.9312\n",
      "Iter-73700 train loss: 0.1734 valid loss: 0.2382, valid accuracy: 0.9316\n",
      "Iter-73800 train loss: 0.1823 valid loss: 0.2381, valid accuracy: 0.9318\n",
      "Iter-73900 train loss: 0.2673 valid loss: 0.2379, valid accuracy: 0.9322\n",
      "Iter-74000 train loss: 0.1854 valid loss: 0.2376, valid accuracy: 0.9318\n",
      "Iter-74100 train loss: 0.2161 valid loss: 0.2376, valid accuracy: 0.9310\n",
      "Iter-74200 train loss: 0.1094 valid loss: 0.2375, valid accuracy: 0.9314\n",
      "Iter-74300 train loss: 0.3067 valid loss: 0.2374, valid accuracy: 0.9314\n",
      "Iter-74400 train loss: 0.2452 valid loss: 0.2374, valid accuracy: 0.9316\n",
      "Iter-74500 train loss: 0.2754 valid loss: 0.2374, valid accuracy: 0.9312\n",
      "Iter-74600 train loss: 0.3046 valid loss: 0.2372, valid accuracy: 0.9312\n",
      "Iter-74700 train loss: 0.1788 valid loss: 0.2368, valid accuracy: 0.9314\n",
      "Iter-74800 train loss: 0.2485 valid loss: 0.2367, valid accuracy: 0.9318\n",
      "Iter-74900 train loss: 0.1603 valid loss: 0.2368, valid accuracy: 0.9322\n",
      "Iter-75000 train loss: 0.3330 valid loss: 0.2365, valid accuracy: 0.9318\n",
      "Iter-75100 train loss: 0.2265 valid loss: 0.2366, valid accuracy: 0.9310\n",
      "Iter-75200 train loss: 0.3557 valid loss: 0.2363, valid accuracy: 0.9318\n",
      "Iter-75300 train loss: 0.1827 valid loss: 0.2362, valid accuracy: 0.9318\n",
      "Iter-75400 train loss: 0.1137 valid loss: 0.2361, valid accuracy: 0.9316\n",
      "Iter-75500 train loss: 0.1654 valid loss: 0.2361, valid accuracy: 0.9318\n",
      "Iter-75600 train loss: 0.1869 valid loss: 0.2360, valid accuracy: 0.9314\n",
      "Iter-75700 train loss: 0.1766 valid loss: 0.2359, valid accuracy: 0.9316\n",
      "Iter-75800 train loss: 0.1691 valid loss: 0.2358, valid accuracy: 0.9316\n",
      "Iter-75900 train loss: 0.0894 valid loss: 0.2358, valid accuracy: 0.9318\n",
      "Iter-76000 train loss: 0.2387 valid loss: 0.2357, valid accuracy: 0.9318\n",
      "Iter-76100 train loss: 0.1848 valid loss: 0.2356, valid accuracy: 0.9326\n",
      "Iter-76200 train loss: 0.1971 valid loss: 0.2353, valid accuracy: 0.9324\n",
      "Iter-76300 train loss: 0.1971 valid loss: 0.2354, valid accuracy: 0.9326\n",
      "Iter-76400 train loss: 0.3594 valid loss: 0.2352, valid accuracy: 0.9322\n",
      "Iter-76500 train loss: 0.3041 valid loss: 0.2349, valid accuracy: 0.9324\n",
      "Iter-76600 train loss: 0.3825 valid loss: 0.2349, valid accuracy: 0.9326\n",
      "Iter-76700 train loss: 0.4482 valid loss: 0.2349, valid accuracy: 0.9324\n",
      "Iter-76800 train loss: 0.5197 valid loss: 0.2349, valid accuracy: 0.9320\n",
      "Iter-76900 train loss: 0.1861 valid loss: 0.2349, valid accuracy: 0.9324\n",
      "Iter-77000 train loss: 0.4299 valid loss: 0.2346, valid accuracy: 0.9330\n",
      "Iter-77100 train loss: 0.2402 valid loss: 0.2345, valid accuracy: 0.9328\n",
      "Iter-77200 train loss: 0.3350 valid loss: 0.2345, valid accuracy: 0.9322\n",
      "Iter-77300 train loss: 0.2041 valid loss: 0.2343, valid accuracy: 0.9314\n",
      "Iter-77400 train loss: 0.1692 valid loss: 0.2341, valid accuracy: 0.9318\n",
      "Iter-77500 train loss: 0.3083 valid loss: 0.2340, valid accuracy: 0.9316\n",
      "Iter-77600 train loss: 0.3624 valid loss: 0.2341, valid accuracy: 0.9318\n",
      "Iter-77700 train loss: 0.2995 valid loss: 0.2339, valid accuracy: 0.9316\n",
      "Iter-77800 train loss: 0.1847 valid loss: 0.2340, valid accuracy: 0.9320\n",
      "Iter-77900 train loss: 0.1469 valid loss: 0.2339, valid accuracy: 0.9324\n",
      "Iter-78000 train loss: 0.4912 valid loss: 0.2339, valid accuracy: 0.9334\n",
      "Iter-78100 train loss: 0.2147 valid loss: 0.2336, valid accuracy: 0.9336\n",
      "Iter-78200 train loss: 0.2231 valid loss: 0.2336, valid accuracy: 0.9338\n",
      "Iter-78300 train loss: 0.2416 valid loss: 0.2333, valid accuracy: 0.9332\n",
      "Iter-78400 train loss: 0.1338 valid loss: 0.2331, valid accuracy: 0.9332\n",
      "Iter-78500 train loss: 0.2125 valid loss: 0.2330, valid accuracy: 0.9332\n",
      "Iter-78600 train loss: 0.3029 valid loss: 0.2329, valid accuracy: 0.9332\n",
      "Iter-78700 train loss: 0.3090 valid loss: 0.2328, valid accuracy: 0.9332\n",
      "Iter-78800 train loss: 0.1581 valid loss: 0.2325, valid accuracy: 0.9334\n",
      "Iter-78900 train loss: 0.1540 valid loss: 0.2323, valid accuracy: 0.9336\n",
      "Iter-79000 train loss: 0.2015 valid loss: 0.2320, valid accuracy: 0.9334\n",
      "Iter-79100 train loss: 0.3572 valid loss: 0.2319, valid accuracy: 0.9338\n",
      "Iter-79200 train loss: 0.1778 valid loss: 0.2318, valid accuracy: 0.9332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 0.1687 valid loss: 0.2317, valid accuracy: 0.9328\n",
      "Iter-79400 train loss: 0.2984 valid loss: 0.2315, valid accuracy: 0.9326\n",
      "Iter-79500 train loss: 0.1208 valid loss: 0.2313, valid accuracy: 0.9334\n",
      "Iter-79600 train loss: 0.2383 valid loss: 0.2311, valid accuracy: 0.9338\n",
      "Iter-79700 train loss: 0.1928 valid loss: 0.2311, valid accuracy: 0.9334\n",
      "Iter-79800 train loss: 0.2310 valid loss: 0.2307, valid accuracy: 0.9338\n",
      "Iter-79900 train loss: 0.2269 valid loss: 0.2305, valid accuracy: 0.9338\n",
      "Iter-80000 train loss: 0.2629 valid loss: 0.2304, valid accuracy: 0.9338\n",
      "Iter-80100 train loss: 0.3131 valid loss: 0.2304, valid accuracy: 0.9340\n",
      "Iter-80200 train loss: 0.2844 valid loss: 0.2302, valid accuracy: 0.9340\n",
      "Iter-80300 train loss: 0.2273 valid loss: 0.2302, valid accuracy: 0.9336\n",
      "Iter-80400 train loss: 0.1698 valid loss: 0.2301, valid accuracy: 0.9338\n",
      "Iter-80500 train loss: 0.3594 valid loss: 0.2302, valid accuracy: 0.9338\n",
      "Iter-80600 train loss: 0.5105 valid loss: 0.2302, valid accuracy: 0.9334\n",
      "Iter-80700 train loss: 0.3292 valid loss: 0.2302, valid accuracy: 0.9334\n",
      "Iter-80800 train loss: 0.0848 valid loss: 0.2301, valid accuracy: 0.9334\n",
      "Iter-80900 train loss: 0.2872 valid loss: 0.2301, valid accuracy: 0.9326\n",
      "Iter-81000 train loss: 0.3073 valid loss: 0.2298, valid accuracy: 0.9332\n",
      "Iter-81100 train loss: 0.2654 valid loss: 0.2297, valid accuracy: 0.9332\n",
      "Iter-81200 train loss: 0.2313 valid loss: 0.2298, valid accuracy: 0.9324\n",
      "Iter-81300 train loss: 0.3411 valid loss: 0.2297, valid accuracy: 0.9332\n",
      "Iter-81400 train loss: 0.1710 valid loss: 0.2296, valid accuracy: 0.9328\n",
      "Iter-81500 train loss: 0.1163 valid loss: 0.2294, valid accuracy: 0.9330\n",
      "Iter-81600 train loss: 0.3594 valid loss: 0.2293, valid accuracy: 0.9332\n",
      "Iter-81700 train loss: 0.2953 valid loss: 0.2290, valid accuracy: 0.9334\n",
      "Iter-81800 train loss: 0.1249 valid loss: 0.2289, valid accuracy: 0.9332\n",
      "Iter-81900 train loss: 0.1910 valid loss: 0.2287, valid accuracy: 0.9328\n",
      "Iter-82000 train loss: 0.2863 valid loss: 0.2285, valid accuracy: 0.9326\n",
      "Iter-82100 train loss: 0.1869 valid loss: 0.2286, valid accuracy: 0.9326\n",
      "Iter-82200 train loss: 0.3507 valid loss: 0.2287, valid accuracy: 0.9322\n",
      "Iter-82300 train loss: 0.2445 valid loss: 0.2288, valid accuracy: 0.9326\n",
      "Iter-82400 train loss: 0.1547 valid loss: 0.2287, valid accuracy: 0.9330\n",
      "Iter-82500 train loss: 0.2748 valid loss: 0.2286, valid accuracy: 0.9338\n",
      "Iter-82600 train loss: 0.3551 valid loss: 0.2286, valid accuracy: 0.9338\n",
      "Iter-82700 train loss: 0.2185 valid loss: 0.2284, valid accuracy: 0.9344\n",
      "Iter-82800 train loss: 0.1574 valid loss: 0.2283, valid accuracy: 0.9338\n",
      "Iter-82900 train loss: 0.2783 valid loss: 0.2280, valid accuracy: 0.9342\n",
      "Iter-83000 train loss: 0.2650 valid loss: 0.2278, valid accuracy: 0.9338\n",
      "Iter-83100 train loss: 0.1777 valid loss: 0.2277, valid accuracy: 0.9334\n",
      "Iter-83200 train loss: 0.0908 valid loss: 0.2275, valid accuracy: 0.9340\n",
      "Iter-83300 train loss: 0.2255 valid loss: 0.2275, valid accuracy: 0.9346\n",
      "Iter-83400 train loss: 0.3698 valid loss: 0.2273, valid accuracy: 0.9340\n",
      "Iter-83500 train loss: 0.1912 valid loss: 0.2272, valid accuracy: 0.9342\n",
      "Iter-83600 train loss: 0.5406 valid loss: 0.2270, valid accuracy: 0.9346\n",
      "Iter-83700 train loss: 0.1087 valid loss: 0.2269, valid accuracy: 0.9338\n",
      "Iter-83800 train loss: 0.2364 valid loss: 0.2268, valid accuracy: 0.9336\n",
      "Iter-83900 train loss: 0.1391 valid loss: 0.2269, valid accuracy: 0.9342\n",
      "Iter-84000 train loss: 0.2017 valid loss: 0.2266, valid accuracy: 0.9342\n",
      "Iter-84100 train loss: 0.3396 valid loss: 0.2267, valid accuracy: 0.9346\n",
      "Iter-84200 train loss: 0.1267 valid loss: 0.2269, valid accuracy: 0.9346\n",
      "Iter-84300 train loss: 0.1987 valid loss: 0.2270, valid accuracy: 0.9350\n",
      "Iter-84400 train loss: 0.1279 valid loss: 0.2269, valid accuracy: 0.9348\n",
      "Iter-84500 train loss: 0.2547 valid loss: 0.2266, valid accuracy: 0.9350\n",
      "Iter-84600 train loss: 0.5330 valid loss: 0.2265, valid accuracy: 0.9356\n",
      "Iter-84700 train loss: 0.4783 valid loss: 0.2262, valid accuracy: 0.9364\n",
      "Iter-84800 train loss: 0.1890 valid loss: 0.2260, valid accuracy: 0.9364\n",
      "Iter-84900 train loss: 0.0961 valid loss: 0.2259, valid accuracy: 0.9352\n",
      "Iter-85000 train loss: 0.1383 valid loss: 0.2257, valid accuracy: 0.9350\n",
      "Iter-85100 train loss: 0.4052 valid loss: 0.2256, valid accuracy: 0.9350\n",
      "Iter-85200 train loss: 0.2591 valid loss: 0.2255, valid accuracy: 0.9350\n",
      "Iter-85300 train loss: 0.2431 valid loss: 0.2255, valid accuracy: 0.9354\n",
      "Iter-85400 train loss: 0.3034 valid loss: 0.2253, valid accuracy: 0.9358\n",
      "Iter-85500 train loss: 0.1745 valid loss: 0.2253, valid accuracy: 0.9360\n",
      "Iter-85600 train loss: 0.1335 valid loss: 0.2249, valid accuracy: 0.9362\n",
      "Iter-85700 train loss: 0.1887 valid loss: 0.2246, valid accuracy: 0.9364\n",
      "Iter-85800 train loss: 0.4024 valid loss: 0.2247, valid accuracy: 0.9356\n",
      "Iter-85900 train loss: 0.1565 valid loss: 0.2247, valid accuracy: 0.9358\n",
      "Iter-86000 train loss: 0.2534 valid loss: 0.2247, valid accuracy: 0.9362\n",
      "Iter-86100 train loss: 0.1262 valid loss: 0.2245, valid accuracy: 0.9356\n",
      "Iter-86200 train loss: 0.1958 valid loss: 0.2241, valid accuracy: 0.9362\n",
      "Iter-86300 train loss: 0.3182 valid loss: 0.2239, valid accuracy: 0.9364\n",
      "Iter-86400 train loss: 0.2930 valid loss: 0.2239, valid accuracy: 0.9362\n",
      "Iter-86500 train loss: 0.2823 valid loss: 0.2238, valid accuracy: 0.9362\n",
      "Iter-86600 train loss: 0.1352 valid loss: 0.2238, valid accuracy: 0.9358\n",
      "Iter-86700 train loss: 0.1625 valid loss: 0.2237, valid accuracy: 0.9360\n",
      "Iter-86800 train loss: 0.3162 valid loss: 0.2235, valid accuracy: 0.9360\n",
      "Iter-86900 train loss: 0.1996 valid loss: 0.2236, valid accuracy: 0.9356\n",
      "Iter-87000 train loss: 0.2335 valid loss: 0.2234, valid accuracy: 0.9360\n",
      "Iter-87100 train loss: 0.3015 valid loss: 0.2234, valid accuracy: 0.9360\n",
      "Iter-87200 train loss: 0.1585 valid loss: 0.2233, valid accuracy: 0.9360\n",
      "Iter-87300 train loss: 0.3200 valid loss: 0.2230, valid accuracy: 0.9364\n",
      "Iter-87400 train loss: 0.2751 valid loss: 0.2229, valid accuracy: 0.9358\n",
      "Iter-87500 train loss: 0.1098 valid loss: 0.2228, valid accuracy: 0.9362\n",
      "Iter-87600 train loss: 0.1200 valid loss: 0.2225, valid accuracy: 0.9358\n",
      "Iter-87700 train loss: 0.2132 valid loss: 0.2225, valid accuracy: 0.9362\n",
      "Iter-87800 train loss: 0.2817 valid loss: 0.2222, valid accuracy: 0.9362\n",
      "Iter-87900 train loss: 0.2409 valid loss: 0.2222, valid accuracy: 0.9358\n",
      "Iter-88000 train loss: 0.1072 valid loss: 0.2221, valid accuracy: 0.9364\n",
      "Iter-88100 train loss: 0.3135 valid loss: 0.2219, valid accuracy: 0.9362\n",
      "Iter-88200 train loss: 0.4519 valid loss: 0.2220, valid accuracy: 0.9362\n",
      "Iter-88300 train loss: 0.2822 valid loss: 0.2219, valid accuracy: 0.9358\n",
      "Iter-88400 train loss: 0.1604 valid loss: 0.2218, valid accuracy: 0.9360\n",
      "Iter-88500 train loss: 0.2023 valid loss: 0.2217, valid accuracy: 0.9362\n",
      "Iter-88600 train loss: 0.1758 valid loss: 0.2215, valid accuracy: 0.9362\n",
      "Iter-88700 train loss: 0.2485 valid loss: 0.2213, valid accuracy: 0.9360\n",
      "Iter-88800 train loss: 0.0674 valid loss: 0.2213, valid accuracy: 0.9362\n",
      "Iter-88900 train loss: 0.1398 valid loss: 0.2212, valid accuracy: 0.9372\n",
      "Iter-89000 train loss: 0.2225 valid loss: 0.2213, valid accuracy: 0.9362\n",
      "Iter-89100 train loss: 0.2204 valid loss: 0.2213, valid accuracy: 0.9366\n",
      "Iter-89200 train loss: 0.3144 valid loss: 0.2211, valid accuracy: 0.9366\n",
      "Iter-89300 train loss: 0.4129 valid loss: 0.2211, valid accuracy: 0.9370\n",
      "Iter-89400 train loss: 0.3090 valid loss: 0.2209, valid accuracy: 0.9372\n",
      "Iter-89500 train loss: 0.2611 valid loss: 0.2208, valid accuracy: 0.9368\n",
      "Iter-89600 train loss: 0.3586 valid loss: 0.2205, valid accuracy: 0.9362\n",
      "Iter-89700 train loss: 0.3163 valid loss: 0.2205, valid accuracy: 0.9366\n",
      "Iter-89800 train loss: 0.1575 valid loss: 0.2206, valid accuracy: 0.9364\n",
      "Iter-89900 train loss: 0.1229 valid loss: 0.2207, valid accuracy: 0.9364\n",
      "Iter-90000 train loss: 0.3062 valid loss: 0.2206, valid accuracy: 0.9370\n",
      "Iter-90100 train loss: 0.2977 valid loss: 0.2205, valid accuracy: 0.9370\n",
      "Iter-90200 train loss: 0.1101 valid loss: 0.2204, valid accuracy: 0.9372\n",
      "Iter-90300 train loss: 0.1869 valid loss: 0.2204, valid accuracy: 0.9370\n",
      "Iter-90400 train loss: 0.2486 valid loss: 0.2204, valid accuracy: 0.9368\n",
      "Iter-90500 train loss: 0.2961 valid loss: 0.2202, valid accuracy: 0.9370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 0.1811 valid loss: 0.2201, valid accuracy: 0.9374\n",
      "Iter-90700 train loss: 0.3121 valid loss: 0.2201, valid accuracy: 0.9366\n",
      "Iter-90800 train loss: 0.0992 valid loss: 0.2199, valid accuracy: 0.9368\n",
      "Iter-90900 train loss: 0.1964 valid loss: 0.2198, valid accuracy: 0.9374\n",
      "Iter-91000 train loss: 0.3304 valid loss: 0.2197, valid accuracy: 0.9370\n",
      "Iter-91100 train loss: 0.2286 valid loss: 0.2196, valid accuracy: 0.9376\n",
      "Iter-91200 train loss: 0.3053 valid loss: 0.2195, valid accuracy: 0.9372\n",
      "Iter-91300 train loss: 0.1651 valid loss: 0.2193, valid accuracy: 0.9372\n",
      "Iter-91400 train loss: 0.0877 valid loss: 0.2192, valid accuracy: 0.9374\n",
      "Iter-91500 train loss: 0.1954 valid loss: 0.2192, valid accuracy: 0.9376\n",
      "Iter-91600 train loss: 0.1390 valid loss: 0.2191, valid accuracy: 0.9368\n",
      "Iter-91700 train loss: 0.3620 valid loss: 0.2191, valid accuracy: 0.9366\n",
      "Iter-91800 train loss: 0.2553 valid loss: 0.2191, valid accuracy: 0.9370\n",
      "Iter-91900 train loss: 0.2430 valid loss: 0.2192, valid accuracy: 0.9368\n",
      "Iter-92000 train loss: 0.3342 valid loss: 0.2190, valid accuracy: 0.9360\n",
      "Iter-92100 train loss: 0.1577 valid loss: 0.2190, valid accuracy: 0.9364\n",
      "Iter-92200 train loss: 0.3118 valid loss: 0.2188, valid accuracy: 0.9362\n",
      "Iter-92300 train loss: 0.3734 valid loss: 0.2188, valid accuracy: 0.9362\n",
      "Iter-92400 train loss: 0.2362 valid loss: 0.2186, valid accuracy: 0.9368\n",
      "Iter-92500 train loss: 0.2283 valid loss: 0.2187, valid accuracy: 0.9368\n",
      "Iter-92600 train loss: 0.3310 valid loss: 0.2185, valid accuracy: 0.9368\n",
      "Iter-92700 train loss: 0.2788 valid loss: 0.2185, valid accuracy: 0.9366\n",
      "Iter-92800 train loss: 0.1594 valid loss: 0.2183, valid accuracy: 0.9376\n",
      "Iter-92900 train loss: 0.1320 valid loss: 0.2183, valid accuracy: 0.9372\n",
      "Iter-93000 train loss: 0.0972 valid loss: 0.2182, valid accuracy: 0.9374\n",
      "Iter-93100 train loss: 0.2498 valid loss: 0.2181, valid accuracy: 0.9372\n",
      "Iter-93200 train loss: 0.2854 valid loss: 0.2182, valid accuracy: 0.9366\n",
      "Iter-93300 train loss: 0.2443 valid loss: 0.2181, valid accuracy: 0.9368\n",
      "Iter-93400 train loss: 0.1555 valid loss: 0.2181, valid accuracy: 0.9370\n",
      "Iter-93500 train loss: 0.3115 valid loss: 0.2179, valid accuracy: 0.9372\n",
      "Iter-93600 train loss: 0.1984 valid loss: 0.2176, valid accuracy: 0.9368\n",
      "Iter-93700 train loss: 0.1667 valid loss: 0.2174, valid accuracy: 0.9366\n",
      "Iter-93800 train loss: 0.3630 valid loss: 0.2173, valid accuracy: 0.9368\n",
      "Iter-93900 train loss: 0.2116 valid loss: 0.2170, valid accuracy: 0.9376\n",
      "Iter-94000 train loss: 0.2874 valid loss: 0.2170, valid accuracy: 0.9380\n",
      "Iter-94100 train loss: 0.1730 valid loss: 0.2170, valid accuracy: 0.9380\n",
      "Iter-94200 train loss: 0.2553 valid loss: 0.2170, valid accuracy: 0.9382\n",
      "Iter-94300 train loss: 0.1946 valid loss: 0.2169, valid accuracy: 0.9376\n",
      "Iter-94400 train loss: 0.3381 valid loss: 0.2167, valid accuracy: 0.9374\n",
      "Iter-94500 train loss: 0.1338 valid loss: 0.2167, valid accuracy: 0.9370\n",
      "Iter-94600 train loss: 0.1596 valid loss: 0.2166, valid accuracy: 0.9374\n",
      "Iter-94700 train loss: 0.1884 valid loss: 0.2167, valid accuracy: 0.9374\n",
      "Iter-94800 train loss: 0.2470 valid loss: 0.2165, valid accuracy: 0.9370\n",
      "Iter-94900 train loss: 0.2068 valid loss: 0.2164, valid accuracy: 0.9380\n",
      "Iter-95000 train loss: 0.2095 valid loss: 0.2162, valid accuracy: 0.9372\n",
      "Iter-95100 train loss: 0.2077 valid loss: 0.2162, valid accuracy: 0.9374\n",
      "Iter-95200 train loss: 0.5464 valid loss: 0.2159, valid accuracy: 0.9382\n",
      "Iter-95300 train loss: 0.1245 valid loss: 0.2159, valid accuracy: 0.9378\n",
      "Iter-95400 train loss: 0.1414 valid loss: 0.2158, valid accuracy: 0.9376\n",
      "Iter-95500 train loss: 0.2192 valid loss: 0.2159, valid accuracy: 0.9378\n",
      "Iter-95600 train loss: 0.2497 valid loss: 0.2156, valid accuracy: 0.9378\n",
      "Iter-95700 train loss: 0.2265 valid loss: 0.2154, valid accuracy: 0.9384\n",
      "Iter-95800 train loss: 0.1318 valid loss: 0.2152, valid accuracy: 0.9382\n",
      "Iter-95900 train loss: 0.3106 valid loss: 0.2153, valid accuracy: 0.9382\n",
      "Iter-96000 train loss: 0.3551 valid loss: 0.2152, valid accuracy: 0.9380\n",
      "Iter-96100 train loss: 0.2053 valid loss: 0.2151, valid accuracy: 0.9380\n",
      "Iter-96200 train loss: 0.1323 valid loss: 0.2151, valid accuracy: 0.9378\n",
      "Iter-96300 train loss: 0.1927 valid loss: 0.2149, valid accuracy: 0.9380\n",
      "Iter-96400 train loss: 0.1122 valid loss: 0.2147, valid accuracy: 0.9378\n",
      "Iter-96500 train loss: 0.3517 valid loss: 0.2147, valid accuracy: 0.9376\n",
      "Iter-96600 train loss: 0.1778 valid loss: 0.2146, valid accuracy: 0.9382\n",
      "Iter-96700 train loss: 0.1450 valid loss: 0.2144, valid accuracy: 0.9384\n",
      "Iter-96800 train loss: 0.2612 valid loss: 0.2143, valid accuracy: 0.9386\n",
      "Iter-96900 train loss: 0.3919 valid loss: 0.2143, valid accuracy: 0.9390\n",
      "Iter-97000 train loss: 0.1897 valid loss: 0.2141, valid accuracy: 0.9384\n",
      "Iter-97100 train loss: 0.1327 valid loss: 0.2140, valid accuracy: 0.9388\n",
      "Iter-97200 train loss: 0.1402 valid loss: 0.2138, valid accuracy: 0.9384\n",
      "Iter-97300 train loss: 0.2582 valid loss: 0.2138, valid accuracy: 0.9386\n",
      "Iter-97400 train loss: 0.2758 valid loss: 0.2138, valid accuracy: 0.9384\n",
      "Iter-97500 train loss: 0.4340 valid loss: 0.2136, valid accuracy: 0.9384\n",
      "Iter-97600 train loss: 0.1491 valid loss: 0.2133, valid accuracy: 0.9386\n",
      "Iter-97700 train loss: 0.2587 valid loss: 0.2132, valid accuracy: 0.9390\n",
      "Iter-97800 train loss: 0.2759 valid loss: 0.2131, valid accuracy: 0.9386\n",
      "Iter-97900 train loss: 0.1890 valid loss: 0.2130, valid accuracy: 0.9386\n",
      "Iter-98000 train loss: 0.2802 valid loss: 0.2130, valid accuracy: 0.9388\n",
      "Iter-98100 train loss: 0.1986 valid loss: 0.2130, valid accuracy: 0.9388\n",
      "Iter-98200 train loss: 0.0940 valid loss: 0.2128, valid accuracy: 0.9388\n",
      "Iter-98300 train loss: 0.2663 valid loss: 0.2127, valid accuracy: 0.9386\n",
      "Iter-98400 train loss: 0.4036 valid loss: 0.2126, valid accuracy: 0.9388\n",
      "Iter-98500 train loss: 0.2343 valid loss: 0.2126, valid accuracy: 0.9388\n",
      "Iter-98600 train loss: 0.2086 valid loss: 0.2126, valid accuracy: 0.9386\n",
      "Iter-98700 train loss: 0.1495 valid loss: 0.2125, valid accuracy: 0.9388\n",
      "Iter-98800 train loss: 0.1296 valid loss: 0.2125, valid accuracy: 0.9388\n",
      "Iter-98900 train loss: 0.0801 valid loss: 0.2124, valid accuracy: 0.9384\n",
      "Iter-99000 train loss: 0.2923 valid loss: 0.2121, valid accuracy: 0.9388\n",
      "Iter-99100 train loss: 0.2724 valid loss: 0.2120, valid accuracy: 0.9386\n",
      "Iter-99200 train loss: 0.2733 valid loss: 0.2119, valid accuracy: 0.9386\n",
      "Iter-99300 train loss: 0.1560 valid loss: 0.2119, valid accuracy: 0.9388\n",
      "Iter-99400 train loss: 0.2952 valid loss: 0.2119, valid accuracy: 0.9388\n",
      "Iter-99500 train loss: 0.1836 valid loss: 0.2119, valid accuracy: 0.9390\n",
      "Iter-99600 train loss: 0.1486 valid loss: 0.2120, valid accuracy: 0.9386\n",
      "Iter-99700 train loss: 0.1647 valid loss: 0.2117, valid accuracy: 0.9390\n",
      "Iter-99800 train loss: 0.2187 valid loss: 0.2116, valid accuracy: 0.9390\n",
      "Iter-99900 train loss: 0.3153 valid loss: 0.2113, valid accuracy: 0.9392\n",
      "Iter-100000 train loss: 0.1045 valid loss: 0.2113, valid accuracy: 0.9396\n",
      "Last iteration - Test accuracy mean: 0.9356, std: 0.0000, loss: 0.2240\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAPj/7w5nzHqjCicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3e8cf7be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf6dVMDhPmTF/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3e8cf7ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
