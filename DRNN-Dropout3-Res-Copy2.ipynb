{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "#             Wxh_res=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "#             Whh_res=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "#             bh_res=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "#         Wxh, Whh, Wxh_res, Whh_res, Why = m['Wxh'], m['Whh'], m['Wxh_res'], m['Whh_res'], m['Why']\n",
    "#         bh, bh_res, by = m['bh'], m['bh_res'], m['by']\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "#         h, h_cache = self.selu_forward(h)\n",
    "\n",
    "#         # h_res for residual connection or skip connection for gradients\n",
    "#         # Residual connection to avoid vanishing gradients\n",
    "#         # SELU act_function to avoid exploding gradients\n",
    "#         # x+ f(x)\n",
    "#         h_res = (X @ Wxh_res) + (hprev @ Whh_res) + bh_res\n",
    "#         h += h_res\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "#         y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "        y, nl_cache = self.selu_forward(y)\n",
    "        y, do_cache = self.alpha_dropout_fwd(h=y, q=1.0-self.p_dropout) # q=1-p, 1=keep_prob\n",
    "\n",
    "#         cache = (X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, do_cache)\n",
    "        cache = (X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache)\n",
    "#         cache = (X, hprev, Wxh, Whh, h_cache, y_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "#         X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, do_cache = cache\n",
    "        X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache = cache\n",
    "#         X, hprev, Wxh, Whh, h_cache, y_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "#         dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dy = self.alpha_dropout_bwd(dout=dy, cache=do_cache)\n",
    "        dy = self.selu_backward(dy, nl_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "#         dh_res = dh.copy()\n",
    "#         dbh_res = dh_res * 1.0\n",
    "#         dWhh_res = hprev.T @ dh_res\n",
    "#         dWxh_res = X.T @ dh_res\n",
    "#         dX_res = dh_res @ Wxh_res.T\n",
    "#         dh_res = dh_res @ Whh_res.T\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "#         dh = self.selu_backward(dh, h_cache)\n",
    "        dbh = dh * 1.0\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "#         dX += dX_res\n",
    "#         dh += dh_res\n",
    "\n",
    "#         grad = dict(Wxh=dWxh, Whh=dWhh, Wxh_res=dWxh_res, Whh_res=dWhh_res, Why=dWhy, bh=dbh, bh_res=dbh_res, \n",
    "#                     by=dby)\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 training loss: 3.5475\n",
      "erant D pi 2tast: RsHt 8ka)ei%he tiOt 8site\"I19Ite s;K)L')iiCaHg Sa\n",
      "mRHUs \n",
      "Us\n",
      "s esin; aInS'; ;ges dan\n",
      "Iter-26 training loss: 3.4652\n",
      "ed ODbven06–PaULoD Fs Ttho5tikihR4本u((n: (IryveiKH 79veaf gah teaioIIBTejeten inaiB;h  sMin87Utone4b \n",
      "Iter-39 training loss: 3.5690\n",
      "eret x Dias9whe -::piaFshinK-R, , ;e(jPisas iMea;oRt is s pix3as 6unOnih,7opeje oBwNbd\"in(n fe: agitg\n",
      "Iter-52 training loss: 3.4258\n",
      "er;eDiso1ri7FE PijW–anBDe the eo,oPharie U'\n",
      "CiDe seith3s Mtot,\n",
      ":i4)ahans)intie gisC Ko)iya6t 4ata 1z:\n",
      "Iter-65 training loss: 3.5593\n",
      "erezot i,hInat 8 ve \"J , ransohatha\n",
      "-pe2(7 本 ve HoMa Sit raAnAC;B,o.\n",
      "TiRiiA 1kIr i3st S g a–mOMi,ania\n",
      "Iter-78 training loss: 3.4820\n",
      "erFas0LoP he8\"\"nwonaBa2iO,t a日1t iOBsi1nH1naJIn本hosRet uInanntheFRa 日osthinanit t -3in本 A-Ethi–MajeiK\n",
      "Iter-91 training loss: 3.3271\n",
      "er.coO) ni ve i2here-HSeis. anb  e–t 77e(9WiNanIs6t PanoHo9t HaJo ot ch. aMino) 21t if t (2EtisJoRce \n",
      "Iter-104 training loss: 3.3471\n",
      "ein,e weeWo; thachat   tete0S 'ans .hagtt ivaKaniSveHn-jo6sos Wone 7 Se DiLas ixLonthaT a8ete a.senst\n",
      "Iter-117 training loss: 3.4447\n",
      "ereoos vi%wesj%e 0t 本 Earbete.aJ\"e o itetarL ahant zan e BanoT tanva int %aWFoet aFPa7nKT thhiWe , iK\n",
      "Iter-130 training loss: 3.6575\n",
      "ed milween–thelamaba zalvaROPsa8onet NeCe -I6Eet iEshi日E.hhe gina I os \"n \"vnin A o thaun KShis ans0h\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f79c10e7a20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 130 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.05 # keep_prob=1.0-p_dropout, q=1-p\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD2nWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79c10d5400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
