{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        self.W_fixed = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y, _ = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train]) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3087, acc-0.1100, valid loss-2.3165, acc-0.0904, test loss-2.3212, acc-0.0782\n",
      "Iter-20, train loss-2.3131, acc-0.0900, valid loss-2.3123, acc-0.0944, test loss-2.3173, acc-0.0810\n",
      "Iter-30, train loss-2.3156, acc-0.1000, valid loss-2.3091, acc-0.0984, test loss-2.3142, acc-0.0823\n",
      "Iter-40, train loss-2.3062, acc-0.1200, valid loss-2.3037, acc-0.1090, test loss-2.3088, acc-0.0934\n",
      "Iter-50, train loss-2.3060, acc-0.1000, valid loss-2.2982, acc-0.1204, test loss-2.3034, acc-0.1052\n",
      "Iter-60, train loss-2.3041, acc-0.0800, valid loss-2.2925, acc-0.1362, test loss-2.2977, acc-0.1198\n",
      "Iter-70, train loss-2.2926, acc-0.1100, valid loss-2.2863, acc-0.1450, test loss-2.2914, acc-0.1261\n",
      "Iter-80, train loss-2.2872, acc-0.1500, valid loss-2.2792, acc-0.1596, test loss-2.2842, acc-0.1393\n",
      "Iter-90, train loss-2.2717, acc-0.2700, valid loss-2.2716, acc-0.1682, test loss-2.2768, acc-0.1468\n",
      "Iter-100, train loss-2.2874, acc-0.1800, valid loss-2.2642, acc-0.1792, test loss-2.2692, acc-0.1540\n",
      "Iter-110, train loss-2.2547, acc-0.1800, valid loss-2.2556, acc-0.1938, test loss-2.2602, acc-0.1682\n",
      "Iter-120, train loss-2.2491, acc-0.1600, valid loss-2.2478, acc-0.2044, test loss-2.2523, acc-0.1825\n",
      "Iter-130, train loss-2.2459, acc-0.2700, valid loss-2.2393, acc-0.2148, test loss-2.2436, acc-0.1945\n",
      "Iter-140, train loss-2.2627, acc-0.1300, valid loss-2.2317, acc-0.2176, test loss-2.2357, acc-0.1995\n",
      "Iter-150, train loss-2.2466, acc-0.2000, valid loss-2.2217, acc-0.2254, test loss-2.2255, acc-0.2089\n",
      "Iter-160, train loss-2.2290, acc-0.2300, valid loss-2.2120, acc-0.2350, test loss-2.2154, acc-0.2183\n",
      "Iter-170, train loss-2.2323, acc-0.1700, valid loss-2.2013, acc-0.2344, test loss-2.2042, acc-0.2220\n",
      "Iter-180, train loss-2.2013, acc-0.2400, valid loss-2.1899, acc-0.2416, test loss-2.1923, acc-0.2275\n",
      "Iter-190, train loss-2.1974, acc-0.2000, valid loss-2.1783, acc-0.2492, test loss-2.1804, acc-0.2307\n",
      "Iter-200, train loss-2.1643, acc-0.3300, valid loss-2.1664, acc-0.2574, test loss-2.1682, acc-0.2383\n",
      "Iter-210, train loss-2.1402, acc-0.3200, valid loss-2.1536, acc-0.2690, test loss-2.1552, acc-0.2488\n",
      "Iter-220, train loss-2.1421, acc-0.2700, valid loss-2.1404, acc-0.2802, test loss-2.1417, acc-0.2603\n",
      "Iter-230, train loss-2.1359, acc-0.3100, valid loss-2.1252, acc-0.3058, test loss-2.1264, acc-0.2898\n",
      "Iter-240, train loss-2.0935, acc-0.3000, valid loss-2.1088, acc-0.3292, test loss-2.1099, acc-0.3189\n",
      "Iter-250, train loss-2.1277, acc-0.2700, valid loss-2.0911, acc-0.3618, test loss-2.0925, acc-0.3497\n",
      "Iter-260, train loss-2.0834, acc-0.2900, valid loss-2.0746, acc-0.3822, test loss-2.0759, acc-0.3731\n",
      "Iter-270, train loss-2.0531, acc-0.4000, valid loss-2.0568, acc-0.3998, test loss-2.0581, acc-0.3909\n",
      "Iter-280, train loss-2.0698, acc-0.3900, valid loss-2.0382, acc-0.4150, test loss-2.0397, acc-0.4120\n",
      "Iter-290, train loss-2.0242, acc-0.4200, valid loss-2.0196, acc-0.4240, test loss-2.0213, acc-0.4219\n",
      "Iter-300, train loss-1.9966, acc-0.4300, valid loss-2.0003, acc-0.4264, test loss-2.0022, acc-0.4250\n",
      "Iter-310, train loss-1.9927, acc-0.4200, valid loss-1.9820, acc-0.4308, test loss-1.9841, acc-0.4287\n",
      "Iter-320, train loss-1.9811, acc-0.3800, valid loss-1.9614, acc-0.4364, test loss-1.9638, acc-0.4353\n",
      "Iter-330, train loss-2.0031, acc-0.3400, valid loss-1.9414, acc-0.4384, test loss-1.9440, acc-0.4399\n",
      "Iter-340, train loss-1.9349, acc-0.3900, valid loss-1.9227, acc-0.4400, test loss-1.9256, acc-0.4406\n",
      "Iter-350, train loss-1.9201, acc-0.4700, valid loss-1.9039, acc-0.4400, test loss-1.9074, acc-0.4376\n",
      "Iter-360, train loss-1.8914, acc-0.3700, valid loss-1.8856, acc-0.4404, test loss-1.8893, acc-0.4395\n",
      "Iter-370, train loss-1.9131, acc-0.4200, valid loss-1.8682, acc-0.4400, test loss-1.8718, acc-0.4373\n",
      "Iter-380, train loss-1.8914, acc-0.3800, valid loss-1.8498, acc-0.4420, test loss-1.8537, acc-0.4356\n",
      "Iter-390, train loss-1.8481, acc-0.4200, valid loss-1.8335, acc-0.4410, test loss-1.8377, acc-0.4320\n",
      "Iter-400, train loss-1.7603, acc-0.5000, valid loss-1.8166, acc-0.4396, test loss-1.8210, acc-0.4299\n",
      "Iter-410, train loss-1.8010, acc-0.4600, valid loss-1.8004, acc-0.4388, test loss-1.8051, acc-0.4303\n",
      "Iter-420, train loss-1.8431, acc-0.3800, valid loss-1.7855, acc-0.4410, test loss-1.7903, acc-0.4316\n",
      "Iter-430, train loss-1.7829, acc-0.4300, valid loss-1.7717, acc-0.4394, test loss-1.7766, acc-0.4300\n",
      "Iter-440, train loss-1.7560, acc-0.4000, valid loss-1.7589, acc-0.4362, test loss-1.7643, acc-0.4273\n",
      "Iter-450, train loss-1.7707, acc-0.4200, valid loss-1.7464, acc-0.4350, test loss-1.7518, acc-0.4269\n",
      "Iter-460, train loss-1.7051, acc-0.4500, valid loss-1.7337, acc-0.4362, test loss-1.7393, acc-0.4258\n",
      "Iter-470, train loss-1.7558, acc-0.4300, valid loss-1.7223, acc-0.4326, test loss-1.7280, acc-0.4227\n",
      "Iter-480, train loss-1.7243, acc-0.4300, valid loss-1.7121, acc-0.4326, test loss-1.7179, acc-0.4239\n",
      "Iter-490, train loss-1.7049, acc-0.4200, valid loss-1.7020, acc-0.4350, test loss-1.7075, acc-0.4273\n",
      "Iter-500, train loss-1.7697, acc-0.4200, valid loss-1.6934, acc-0.4362, test loss-1.6989, acc-0.4276\n",
      "Iter-510, train loss-1.6502, acc-0.3900, valid loss-1.6848, acc-0.4378, test loss-1.6904, acc-0.4282\n",
      "Iter-520, train loss-1.7706, acc-0.3700, valid loss-1.6770, acc-0.4408, test loss-1.6820, acc-0.4319\n",
      "Iter-530, train loss-1.6602, acc-0.4200, valid loss-1.6669, acc-0.4472, test loss-1.6726, acc-0.4370\n",
      "Iter-540, train loss-1.6683, acc-0.4800, valid loss-1.6578, acc-0.4494, test loss-1.6635, acc-0.4405\n",
      "Iter-550, train loss-1.5535, acc-0.4900, valid loss-1.6496, acc-0.4524, test loss-1.6559, acc-0.4472\n",
      "Iter-560, train loss-1.6358, acc-0.5300, valid loss-1.6425, acc-0.4574, test loss-1.6485, acc-0.4500\n",
      "Iter-570, train loss-1.7142, acc-0.4300, valid loss-1.6358, acc-0.4570, test loss-1.6417, acc-0.4507\n",
      "Iter-580, train loss-1.6960, acc-0.4100, valid loss-1.6293, acc-0.4582, test loss-1.6350, acc-0.4520\n",
      "Iter-590, train loss-1.6223, acc-0.4400, valid loss-1.6223, acc-0.4650, test loss-1.6280, acc-0.4537\n",
      "Iter-600, train loss-1.6950, acc-0.4500, valid loss-1.6170, acc-0.4644, test loss-1.6218, acc-0.4553\n",
      "Iter-610, train loss-1.6502, acc-0.4600, valid loss-1.6108, acc-0.4672, test loss-1.6148, acc-0.4586\n",
      "Iter-620, train loss-1.6524, acc-0.3900, valid loss-1.6047, acc-0.4710, test loss-1.6086, acc-0.4607\n",
      "Iter-630, train loss-1.5363, acc-0.4600, valid loss-1.5974, acc-0.4722, test loss-1.6012, acc-0.4636\n",
      "Iter-640, train loss-1.5765, acc-0.4700, valid loss-1.5911, acc-0.4784, test loss-1.5945, acc-0.4687\n",
      "Iter-650, train loss-1.6184, acc-0.4400, valid loss-1.5862, acc-0.4828, test loss-1.5894, acc-0.4707\n",
      "Iter-660, train loss-1.5161, acc-0.5500, valid loss-1.5801, acc-0.4858, test loss-1.5831, acc-0.4730\n",
      "Iter-670, train loss-1.5706, acc-0.4800, valid loss-1.5747, acc-0.4852, test loss-1.5775, acc-0.4756\n",
      "Iter-680, train loss-1.5533, acc-0.5200, valid loss-1.5698, acc-0.4888, test loss-1.5724, acc-0.4748\n",
      "Iter-690, train loss-1.5566, acc-0.5100, valid loss-1.5641, acc-0.4934, test loss-1.5662, acc-0.4795\n",
      "Iter-700, train loss-1.4898, acc-0.4900, valid loss-1.5575, acc-0.4922, test loss-1.5592, acc-0.4832\n",
      "Iter-710, train loss-1.5187, acc-0.5000, valid loss-1.5541, acc-0.4888, test loss-1.5551, acc-0.4810\n",
      "Iter-720, train loss-1.6191, acc-0.4700, valid loss-1.5485, acc-0.4884, test loss-1.5493, acc-0.4834\n",
      "Iter-730, train loss-1.5850, acc-0.4100, valid loss-1.5405, acc-0.4910, test loss-1.5409, acc-0.4848\n",
      "Iter-740, train loss-1.5481, acc-0.4600, valid loss-1.5332, acc-0.4970, test loss-1.5335, acc-0.4899\n",
      "Iter-750, train loss-1.5658, acc-0.4200, valid loss-1.5279, acc-0.4946, test loss-1.5280, acc-0.4899\n",
      "Iter-760, train loss-1.5258, acc-0.4500, valid loss-1.5221, acc-0.4936, test loss-1.5217, acc-0.4880\n",
      "Iter-770, train loss-1.5714, acc-0.4200, valid loss-1.5155, acc-0.4986, test loss-1.5152, acc-0.4928\n",
      "Iter-780, train loss-1.5696, acc-0.4900, valid loss-1.5124, acc-0.4984, test loss-1.5110, acc-0.4935\n",
      "Iter-790, train loss-1.5812, acc-0.4800, valid loss-1.5069, acc-0.5012, test loss-1.5051, acc-0.4980\n",
      "Iter-800, train loss-1.5103, acc-0.5300, valid loss-1.5001, acc-0.5084, test loss-1.4980, acc-0.5024\n",
      "Iter-810, train loss-1.4914, acc-0.4500, valid loss-1.4961, acc-0.5100, test loss-1.4932, acc-0.5059\n",
      "Iter-820, train loss-1.5550, acc-0.5100, valid loss-1.4907, acc-0.5106, test loss-1.4872, acc-0.5071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-1.3958, acc-0.5300, valid loss-1.4835, acc-0.5134, test loss-1.4799, acc-0.5094\n",
      "Iter-840, train loss-1.4062, acc-0.5700, valid loss-1.4766, acc-0.5186, test loss-1.4729, acc-0.5131\n",
      "Iter-850, train loss-1.5265, acc-0.4500, valid loss-1.4707, acc-0.5170, test loss-1.4663, acc-0.5156\n",
      "Iter-860, train loss-1.4836, acc-0.5200, valid loss-1.4648, acc-0.5248, test loss-1.4597, acc-0.5200\n",
      "Iter-870, train loss-1.5224, acc-0.4900, valid loss-1.4579, acc-0.5288, test loss-1.4527, acc-0.5238\n",
      "Iter-880, train loss-1.4697, acc-0.5100, valid loss-1.4508, acc-0.5320, test loss-1.4454, acc-0.5272\n",
      "Iter-890, train loss-1.4153, acc-0.5300, valid loss-1.4439, acc-0.5310, test loss-1.4380, acc-0.5324\n",
      "Iter-900, train loss-1.4687, acc-0.5000, valid loss-1.4365, acc-0.5360, test loss-1.4305, acc-0.5368\n",
      "Iter-910, train loss-1.4731, acc-0.5600, valid loss-1.4280, acc-0.5404, test loss-1.4218, acc-0.5428\n",
      "Iter-920, train loss-1.4765, acc-0.5000, valid loss-1.4223, acc-0.5458, test loss-1.4156, acc-0.5456\n",
      "Iter-930, train loss-1.3145, acc-0.6000, valid loss-1.4163, acc-0.5480, test loss-1.4088, acc-0.5506\n",
      "Iter-940, train loss-1.4502, acc-0.6700, valid loss-1.4082, acc-0.5552, test loss-1.4003, acc-0.5562\n",
      "Iter-950, train loss-1.4459, acc-0.5400, valid loss-1.4015, acc-0.5568, test loss-1.3935, acc-0.5566\n",
      "Iter-960, train loss-1.3739, acc-0.5800, valid loss-1.3928, acc-0.5594, test loss-1.3847, acc-0.5598\n",
      "Iter-970, train loss-1.2081, acc-0.6400, valid loss-1.3861, acc-0.5602, test loss-1.3775, acc-0.5651\n",
      "Iter-980, train loss-1.3502, acc-0.5000, valid loss-1.3811, acc-0.5618, test loss-1.3718, acc-0.5666\n",
      "Iter-990, train loss-1.3918, acc-0.5200, valid loss-1.3727, acc-0.5678, test loss-1.3635, acc-0.5727\n",
      "Iter-1000, train loss-1.4849, acc-0.5400, valid loss-1.3651, acc-0.5700, test loss-1.3559, acc-0.5764\n",
      "Iter-1010, train loss-1.2388, acc-0.6400, valid loss-1.3569, acc-0.5726, test loss-1.3470, acc-0.5793\n",
      "Iter-1020, train loss-1.2891, acc-0.6100, valid loss-1.3481, acc-0.5814, test loss-1.3380, acc-0.5864\n",
      "Iter-1030, train loss-1.1814, acc-0.7200, valid loss-1.3432, acc-0.5800, test loss-1.3324, acc-0.5875\n",
      "Iter-1040, train loss-1.3004, acc-0.6300, valid loss-1.3371, acc-0.5848, test loss-1.3256, acc-0.5915\n",
      "Iter-1050, train loss-1.2777, acc-0.5500, valid loss-1.3303, acc-0.5908, test loss-1.3183, acc-0.5950\n",
      "Iter-1060, train loss-1.2361, acc-0.6900, valid loss-1.3234, acc-0.5928, test loss-1.3110, acc-0.5966\n",
      "Iter-1070, train loss-1.4284, acc-0.5300, valid loss-1.3148, acc-0.5996, test loss-1.3023, acc-0.6045\n",
      "Iter-1080, train loss-1.2225, acc-0.6600, valid loss-1.3088, acc-0.6036, test loss-1.2957, acc-0.6072\n",
      "Iter-1090, train loss-1.3520, acc-0.5800, valid loss-1.3032, acc-0.6028, test loss-1.2895, acc-0.6056\n",
      "Iter-1100, train loss-1.3485, acc-0.6000, valid loss-1.2946, acc-0.6088, test loss-1.2805, acc-0.6132\n",
      "Iter-1110, train loss-1.2494, acc-0.6200, valid loss-1.2884, acc-0.6124, test loss-1.2740, acc-0.6151\n",
      "Iter-1120, train loss-1.2856, acc-0.5700, valid loss-1.2811, acc-0.6154, test loss-1.2665, acc-0.6188\n",
      "Iter-1130, train loss-1.2240, acc-0.6300, valid loss-1.2738, acc-0.6226, test loss-1.2591, acc-0.6242\n",
      "Iter-1140, train loss-1.3641, acc-0.5900, valid loss-1.2692, acc-0.6278, test loss-1.2543, acc-0.6306\n",
      "Iter-1150, train loss-1.2048, acc-0.6700, valid loss-1.2618, acc-0.6328, test loss-1.2467, acc-0.6366\n",
      "Iter-1160, train loss-1.2482, acc-0.6400, valid loss-1.2550, acc-0.6350, test loss-1.2398, acc-0.6386\n",
      "Iter-1170, train loss-1.2425, acc-0.6400, valid loss-1.2478, acc-0.6394, test loss-1.2321, acc-0.6437\n",
      "Iter-1180, train loss-1.2391, acc-0.6200, valid loss-1.2402, acc-0.6410, test loss-1.2242, acc-0.6455\n",
      "Iter-1190, train loss-1.1898, acc-0.6200, valid loss-1.2325, acc-0.6448, test loss-1.2165, acc-0.6487\n",
      "Iter-1200, train loss-1.2238, acc-0.6800, valid loss-1.2266, acc-0.6470, test loss-1.2099, acc-0.6517\n",
      "Iter-1210, train loss-1.2695, acc-0.6400, valid loss-1.2186, acc-0.6500, test loss-1.2015, acc-0.6562\n",
      "Iter-1220, train loss-1.2990, acc-0.6200, valid loss-1.2114, acc-0.6540, test loss-1.1937, acc-0.6581\n",
      "Iter-1230, train loss-1.2076, acc-0.6800, valid loss-1.2046, acc-0.6552, test loss-1.1872, acc-0.6593\n",
      "Iter-1240, train loss-1.2726, acc-0.6300, valid loss-1.1971, acc-0.6588, test loss-1.1793, acc-0.6639\n",
      "Iter-1250, train loss-1.1245, acc-0.6900, valid loss-1.1905, acc-0.6614, test loss-1.1721, acc-0.6657\n",
      "Iter-1260, train loss-1.1631, acc-0.6900, valid loss-1.1840, acc-0.6624, test loss-1.1654, acc-0.6665\n",
      "Iter-1270, train loss-1.2518, acc-0.6200, valid loss-1.1780, acc-0.6654, test loss-1.1589, acc-0.6698\n",
      "Iter-1280, train loss-1.0819, acc-0.7200, valid loss-1.1711, acc-0.6662, test loss-1.1519, acc-0.6735\n",
      "Iter-1290, train loss-1.2445, acc-0.6000, valid loss-1.1652, acc-0.6684, test loss-1.1457, acc-0.6763\n",
      "Iter-1300, train loss-1.1153, acc-0.7000, valid loss-1.1600, acc-0.6702, test loss-1.1400, acc-0.6766\n",
      "Iter-1310, train loss-1.0752, acc-0.6700, valid loss-1.1567, acc-0.6720, test loss-1.1359, acc-0.6779\n",
      "Iter-1320, train loss-1.2140, acc-0.6600, valid loss-1.1501, acc-0.6740, test loss-1.1294, acc-0.6799\n",
      "Iter-1330, train loss-1.2662, acc-0.6000, valid loss-1.1443, acc-0.6768, test loss-1.1233, acc-0.6840\n",
      "Iter-1340, train loss-1.0231, acc-0.7000, valid loss-1.1370, acc-0.6792, test loss-1.1164, acc-0.6862\n",
      "Iter-1350, train loss-1.2954, acc-0.5700, valid loss-1.1332, acc-0.6784, test loss-1.1125, acc-0.6858\n",
      "Iter-1360, train loss-1.1178, acc-0.6700, valid loss-1.1269, acc-0.6800, test loss-1.1068, acc-0.6876\n",
      "Iter-1370, train loss-1.0826, acc-0.6500, valid loss-1.1224, acc-0.6780, test loss-1.1025, acc-0.6890\n",
      "Iter-1380, train loss-0.9608, acc-0.8100, valid loss-1.1168, acc-0.6788, test loss-1.0968, acc-0.6893\n",
      "Iter-1390, train loss-1.1429, acc-0.6500, valid loss-1.1123, acc-0.6800, test loss-1.0921, acc-0.6897\n",
      "Iter-1400, train loss-1.0874, acc-0.7000, valid loss-1.1069, acc-0.6832, test loss-1.0867, acc-0.6902\n",
      "Iter-1410, train loss-1.1126, acc-0.7100, valid loss-1.1021, acc-0.6858, test loss-1.0812, acc-0.6920\n",
      "Iter-1420, train loss-0.9887, acc-0.7600, valid loss-1.0980, acc-0.6852, test loss-1.0772, acc-0.6921\n",
      "Iter-1430, train loss-1.0985, acc-0.6800, valid loss-1.0923, acc-0.6860, test loss-1.0714, acc-0.6940\n",
      "Iter-1440, train loss-1.1414, acc-0.6400, valid loss-1.0881, acc-0.6874, test loss-1.0669, acc-0.6951\n",
      "Iter-1450, train loss-1.0571, acc-0.6900, valid loss-1.0833, acc-0.6894, test loss-1.0615, acc-0.6969\n",
      "Iter-1460, train loss-1.1120, acc-0.6600, valid loss-1.0777, acc-0.6910, test loss-1.0564, acc-0.6973\n",
      "Iter-1470, train loss-1.0758, acc-0.6500, valid loss-1.0714, acc-0.6932, test loss-1.0503, acc-0.6995\n",
      "Iter-1480, train loss-0.8657, acc-0.7600, valid loss-1.0667, acc-0.6938, test loss-1.0456, acc-0.7016\n",
      "Iter-1490, train loss-1.3632, acc-0.6000, valid loss-1.0609, acc-0.6954, test loss-1.0399, acc-0.7044\n",
      "Iter-1500, train loss-0.9964, acc-0.6800, valid loss-1.0568, acc-0.6952, test loss-1.0355, acc-0.7045\n",
      "Iter-1510, train loss-1.0364, acc-0.7300, valid loss-1.0530, acc-0.6964, test loss-1.0318, acc-0.7062\n",
      "Iter-1520, train loss-1.1255, acc-0.6800, valid loss-1.0489, acc-0.6978, test loss-1.0276, acc-0.7068\n",
      "Iter-1530, train loss-1.1172, acc-0.6900, valid loss-1.0443, acc-0.6996, test loss-1.0227, acc-0.7077\n",
      "Iter-1540, train loss-0.8677, acc-0.7600, valid loss-1.0405, acc-0.6998, test loss-1.0186, acc-0.7083\n",
      "Iter-1550, train loss-0.8451, acc-0.8000, valid loss-1.0364, acc-0.7030, test loss-1.0145, acc-0.7092\n",
      "Iter-1560, train loss-0.9643, acc-0.7600, valid loss-1.0335, acc-0.7028, test loss-1.0111, acc-0.7095\n",
      "Iter-1570, train loss-1.0842, acc-0.6400, valid loss-1.0306, acc-0.7038, test loss-1.0084, acc-0.7106\n",
      "Iter-1580, train loss-1.1503, acc-0.6100, valid loss-1.0243, acc-0.7064, test loss-1.0030, acc-0.7115\n",
      "Iter-1590, train loss-1.0972, acc-0.6800, valid loss-1.0202, acc-0.7060, test loss-0.9990, acc-0.7133\n",
      "Iter-1600, train loss-1.1298, acc-0.6200, valid loss-1.0162, acc-0.7080, test loss-0.9949, acc-0.7146\n",
      "Iter-1610, train loss-0.9870, acc-0.6600, valid loss-1.0120, acc-0.7086, test loss-0.9910, acc-0.7146\n",
      "Iter-1620, train loss-1.0219, acc-0.6900, valid loss-1.0086, acc-0.7088, test loss-0.9874, acc-0.7155\n",
      "Iter-1630, train loss-0.9856, acc-0.7000, valid loss-1.0065, acc-0.7090, test loss-0.9848, acc-0.7162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-1.1400, acc-0.6200, valid loss-1.0027, acc-0.7104, test loss-0.9807, acc-0.7173\n",
      "Iter-1650, train loss-0.9940, acc-0.7000, valid loss-0.9997, acc-0.7120, test loss-0.9777, acc-0.7189\n",
      "Iter-1660, train loss-0.9211, acc-0.7300, valid loss-0.9960, acc-0.7132, test loss-0.9736, acc-0.7212\n",
      "Iter-1670, train loss-0.9081, acc-0.7600, valid loss-0.9931, acc-0.7128, test loss-0.9706, acc-0.7209\n",
      "Iter-1680, train loss-0.8404, acc-0.7400, valid loss-0.9892, acc-0.7150, test loss-0.9667, acc-0.7230\n",
      "Iter-1690, train loss-0.8995, acc-0.7300, valid loss-0.9854, acc-0.7162, test loss-0.9630, acc-0.7240\n",
      "Iter-1700, train loss-0.9356, acc-0.7100, valid loss-0.9820, acc-0.7166, test loss-0.9594, acc-0.7245\n",
      "Iter-1710, train loss-0.9419, acc-0.7100, valid loss-0.9800, acc-0.7152, test loss-0.9570, acc-0.7248\n",
      "Iter-1720, train loss-0.9738, acc-0.7000, valid loss-0.9751, acc-0.7196, test loss-0.9524, acc-0.7257\n",
      "Iter-1730, train loss-0.9318, acc-0.7400, valid loss-0.9719, acc-0.7206, test loss-0.9486, acc-0.7260\n",
      "Iter-1740, train loss-1.0305, acc-0.6100, valid loss-0.9692, acc-0.7206, test loss-0.9460, acc-0.7267\n",
      "Iter-1750, train loss-1.0252, acc-0.6900, valid loss-0.9658, acc-0.7204, test loss-0.9425, acc-0.7277\n",
      "Iter-1760, train loss-0.9217, acc-0.7300, valid loss-0.9616, acc-0.7230, test loss-0.9385, acc-0.7292\n",
      "Iter-1770, train loss-0.8950, acc-0.7700, valid loss-0.9589, acc-0.7228, test loss-0.9358, acc-0.7311\n",
      "Iter-1780, train loss-0.9752, acc-0.6900, valid loss-0.9562, acc-0.7234, test loss-0.9331, acc-0.7325\n",
      "Iter-1790, train loss-0.9324, acc-0.7700, valid loss-0.9541, acc-0.7252, test loss-0.9305, acc-0.7321\n",
      "Iter-1800, train loss-0.9894, acc-0.7400, valid loss-0.9509, acc-0.7258, test loss-0.9275, acc-0.7323\n",
      "Iter-1810, train loss-0.9248, acc-0.7100, valid loss-0.9479, acc-0.7274, test loss-0.9245, acc-0.7350\n",
      "Iter-1820, train loss-0.8853, acc-0.7300, valid loss-0.9455, acc-0.7284, test loss-0.9220, acc-0.7362\n",
      "Iter-1830, train loss-1.0201, acc-0.7000, valid loss-0.9419, acc-0.7292, test loss-0.9186, acc-0.7369\n",
      "Iter-1840, train loss-0.8791, acc-0.7200, valid loss-0.9398, acc-0.7284, test loss-0.9160, acc-0.7383\n",
      "Iter-1850, train loss-0.8944, acc-0.7700, valid loss-0.9377, acc-0.7288, test loss-0.9139, acc-0.7376\n",
      "Iter-1860, train loss-0.9333, acc-0.7300, valid loss-0.9359, acc-0.7294, test loss-0.9117, acc-0.7382\n",
      "Iter-1870, train loss-0.8288, acc-0.7900, valid loss-0.9313, acc-0.7318, test loss-0.9074, acc-0.7402\n",
      "Iter-1880, train loss-1.0709, acc-0.5900, valid loss-0.9285, acc-0.7318, test loss-0.9047, acc-0.7415\n",
      "Iter-1890, train loss-1.2505, acc-0.6200, valid loss-0.9256, acc-0.7334, test loss-0.9024, acc-0.7413\n",
      "Iter-1900, train loss-0.7877, acc-0.7900, valid loss-0.9220, acc-0.7342, test loss-0.8990, acc-0.7429\n",
      "Iter-1910, train loss-1.0457, acc-0.6500, valid loss-0.9189, acc-0.7354, test loss-0.8959, acc-0.7450\n",
      "Iter-1920, train loss-0.9468, acc-0.7100, valid loss-0.9163, acc-0.7366, test loss-0.8938, acc-0.7451\n",
      "Iter-1930, train loss-1.1558, acc-0.6400, valid loss-0.9127, acc-0.7370, test loss-0.8912, acc-0.7467\n",
      "Iter-1940, train loss-0.8815, acc-0.7600, valid loss-0.9105, acc-0.7374, test loss-0.8889, acc-0.7469\n",
      "Iter-1950, train loss-0.9239, acc-0.7300, valid loss-0.9080, acc-0.7384, test loss-0.8866, acc-0.7475\n",
      "Iter-1960, train loss-1.0662, acc-0.6800, valid loss-0.9052, acc-0.7382, test loss-0.8836, acc-0.7489\n",
      "Iter-1970, train loss-1.0102, acc-0.6300, valid loss-0.9034, acc-0.7388, test loss-0.8814, acc-0.7495\n",
      "Iter-1980, train loss-0.8586, acc-0.7300, valid loss-0.9002, acc-0.7402, test loss-0.8781, acc-0.7509\n",
      "Iter-1990, train loss-0.8744, acc-0.7600, valid loss-0.8990, acc-0.7400, test loss-0.8768, acc-0.7511\n",
      "Iter-2000, train loss-0.9571, acc-0.6700, valid loss-0.8963, acc-0.7416, test loss-0.8740, acc-0.7520\n",
      "Iter-2010, train loss-1.1402, acc-0.5700, valid loss-0.8935, acc-0.7418, test loss-0.8713, acc-0.7521\n",
      "Iter-2020, train loss-0.7757, acc-0.8000, valid loss-0.8913, acc-0.7426, test loss-0.8690, acc-0.7535\n",
      "Iter-2030, train loss-0.7755, acc-0.8200, valid loss-0.8884, acc-0.7438, test loss-0.8662, acc-0.7543\n",
      "Iter-2040, train loss-1.0444, acc-0.6600, valid loss-0.8858, acc-0.7438, test loss-0.8636, acc-0.7554\n",
      "Iter-2050, train loss-0.9469, acc-0.7300, valid loss-0.8822, acc-0.7436, test loss-0.8605, acc-0.7554\n",
      "Iter-2060, train loss-0.7616, acc-0.8400, valid loss-0.8799, acc-0.7454, test loss-0.8584, acc-0.7560\n",
      "Iter-2070, train loss-1.0410, acc-0.7300, valid loss-0.8776, acc-0.7460, test loss-0.8565, acc-0.7575\n",
      "Iter-2080, train loss-0.8840, acc-0.8000, valid loss-0.8764, acc-0.7462, test loss-0.8552, acc-0.7571\n",
      "Iter-2090, train loss-0.7345, acc-0.8700, valid loss-0.8741, acc-0.7470, test loss-0.8529, acc-0.7576\n",
      "Iter-2100, train loss-0.9225, acc-0.7800, valid loss-0.8713, acc-0.7480, test loss-0.8501, acc-0.7590\n",
      "Iter-2110, train loss-0.7502, acc-0.7700, valid loss-0.8680, acc-0.7476, test loss-0.8470, acc-0.7594\n",
      "Iter-2120, train loss-0.8068, acc-0.7500, valid loss-0.8654, acc-0.7484, test loss-0.8445, acc-0.7608\n",
      "Iter-2130, train loss-0.7829, acc-0.8200, valid loss-0.8619, acc-0.7494, test loss-0.8415, acc-0.7632\n",
      "Iter-2140, train loss-1.0228, acc-0.7400, valid loss-0.8607, acc-0.7488, test loss-0.8396, acc-0.7630\n",
      "Iter-2150, train loss-0.8689, acc-0.7600, valid loss-0.8594, acc-0.7494, test loss-0.8380, acc-0.7628\n",
      "Iter-2160, train loss-0.9438, acc-0.7000, valid loss-0.8570, acc-0.7508, test loss-0.8359, acc-0.7639\n",
      "Iter-2170, train loss-0.9519, acc-0.7300, valid loss-0.8558, acc-0.7508, test loss-0.8345, acc-0.7628\n",
      "Iter-2180, train loss-0.9226, acc-0.6800, valid loss-0.8532, acc-0.7518, test loss-0.8320, acc-0.7639\n",
      "Iter-2190, train loss-0.9024, acc-0.7000, valid loss-0.8508, acc-0.7510, test loss-0.8296, acc-0.7657\n",
      "Iter-2200, train loss-0.7804, acc-0.7400, valid loss-0.8484, acc-0.7524, test loss-0.8273, acc-0.7653\n",
      "Iter-2210, train loss-0.8988, acc-0.7300, valid loss-0.8467, acc-0.7534, test loss-0.8252, acc-0.7673\n",
      "Iter-2220, train loss-0.8415, acc-0.7600, valid loss-0.8462, acc-0.7534, test loss-0.8241, acc-0.7666\n",
      "Iter-2230, train loss-0.8208, acc-0.7400, valid loss-0.8441, acc-0.7554, test loss-0.8221, acc-0.7660\n",
      "Iter-2240, train loss-0.8410, acc-0.7700, valid loss-0.8426, acc-0.7544, test loss-0.8201, acc-0.7673\n",
      "Iter-2250, train loss-0.8056, acc-0.7500, valid loss-0.8415, acc-0.7546, test loss-0.8187, acc-0.7678\n",
      "Iter-2260, train loss-0.7457, acc-0.7700, valid loss-0.8393, acc-0.7556, test loss-0.8173, acc-0.7683\n",
      "Iter-2270, train loss-0.7537, acc-0.8400, valid loss-0.8360, acc-0.7552, test loss-0.8145, acc-0.7697\n",
      "Iter-2280, train loss-0.9361, acc-0.6800, valid loss-0.8324, acc-0.7566, test loss-0.8112, acc-0.7714\n",
      "Iter-2290, train loss-0.7975, acc-0.8200, valid loss-0.8308, acc-0.7572, test loss-0.8092, acc-0.7719\n",
      "Iter-2300, train loss-0.8079, acc-0.7500, valid loss-0.8304, acc-0.7572, test loss-0.8084, acc-0.7716\n",
      "Iter-2310, train loss-0.7150, acc-0.8100, valid loss-0.8287, acc-0.7572, test loss-0.8068, acc-0.7725\n",
      "Iter-2320, train loss-0.6715, acc-0.8000, valid loss-0.8271, acc-0.7576, test loss-0.8053, acc-0.7726\n",
      "Iter-2330, train loss-0.7727, acc-0.7800, valid loss-0.8253, acc-0.7592, test loss-0.8037, acc-0.7734\n",
      "Iter-2340, train loss-0.7118, acc-0.7900, valid loss-0.8238, acc-0.7600, test loss-0.8018, acc-0.7731\n",
      "Iter-2350, train loss-0.7202, acc-0.7900, valid loss-0.8218, acc-0.7608, test loss-0.8000, acc-0.7740\n",
      "Iter-2360, train loss-0.6434, acc-0.7900, valid loss-0.8194, acc-0.7602, test loss-0.7979, acc-0.7744\n",
      "Iter-2370, train loss-0.9228, acc-0.7100, valid loss-0.8172, acc-0.7614, test loss-0.7959, acc-0.7749\n",
      "Iter-2380, train loss-0.9827, acc-0.7200, valid loss-0.8153, acc-0.7608, test loss-0.7940, acc-0.7755\n",
      "Iter-2390, train loss-0.8443, acc-0.7600, valid loss-0.8137, acc-0.7610, test loss-0.7923, acc-0.7746\n",
      "Iter-2400, train loss-0.7698, acc-0.7600, valid loss-0.8112, acc-0.7624, test loss-0.7901, acc-0.7760\n",
      "Iter-2410, train loss-0.8422, acc-0.7500, valid loss-0.8099, acc-0.7622, test loss-0.7887, acc-0.7758\n",
      "Iter-2420, train loss-0.7963, acc-0.7900, valid loss-0.8090, acc-0.7626, test loss-0.7880, acc-0.7760\n",
      "Iter-2430, train loss-0.8530, acc-0.8100, valid loss-0.8079, acc-0.7630, test loss-0.7867, acc-0.7764\n",
      "Iter-2440, train loss-0.8204, acc-0.7900, valid loss-0.8063, acc-0.7628, test loss-0.7852, acc-0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.7794, acc-0.7900, valid loss-0.8044, acc-0.7636, test loss-0.7828, acc-0.7769\n",
      "Iter-2460, train loss-0.7651, acc-0.7700, valid loss-0.8035, acc-0.7636, test loss-0.7820, acc-0.7772\n",
      "Iter-2470, train loss-0.8655, acc-0.6900, valid loss-0.8012, acc-0.7648, test loss-0.7799, acc-0.7779\n",
      "Iter-2480, train loss-0.8066, acc-0.7700, valid loss-0.7987, acc-0.7662, test loss-0.7777, acc-0.7791\n",
      "Iter-2490, train loss-1.0301, acc-0.6400, valid loss-0.7968, acc-0.7670, test loss-0.7761, acc-0.7793\n",
      "Iter-2500, train loss-0.7432, acc-0.8100, valid loss-0.7960, acc-0.7668, test loss-0.7755, acc-0.7789\n",
      "Iter-2510, train loss-0.8249, acc-0.7300, valid loss-0.7956, acc-0.7670, test loss-0.7746, acc-0.7799\n",
      "Iter-2520, train loss-0.6867, acc-0.8200, valid loss-0.7942, acc-0.7668, test loss-0.7733, acc-0.7812\n",
      "Iter-2530, train loss-0.7031, acc-0.8100, valid loss-0.7917, acc-0.7672, test loss-0.7712, acc-0.7813\n",
      "Iter-2540, train loss-0.7222, acc-0.8000, valid loss-0.7897, acc-0.7676, test loss-0.7693, acc-0.7822\n",
      "Iter-2550, train loss-0.8774, acc-0.7400, valid loss-0.7881, acc-0.7686, test loss-0.7680, acc-0.7830\n",
      "Iter-2560, train loss-0.7968, acc-0.7600, valid loss-0.7868, acc-0.7696, test loss-0.7666, acc-0.7834\n",
      "Iter-2570, train loss-0.7964, acc-0.7600, valid loss-0.7861, acc-0.7696, test loss-0.7654, acc-0.7828\n",
      "Iter-2580, train loss-0.7692, acc-0.7900, valid loss-0.7844, acc-0.7698, test loss-0.7642, acc-0.7839\n",
      "Iter-2590, train loss-0.6616, acc-0.8300, valid loss-0.7836, acc-0.7704, test loss-0.7633, acc-0.7834\n",
      "Iter-2600, train loss-0.6984, acc-0.8200, valid loss-0.7821, acc-0.7704, test loss-0.7618, acc-0.7836\n",
      "Iter-2610, train loss-0.8147, acc-0.7700, valid loss-0.7812, acc-0.7698, test loss-0.7608, acc-0.7836\n",
      "Iter-2620, train loss-0.7834, acc-0.7300, valid loss-0.7803, acc-0.7698, test loss-0.7601, acc-0.7843\n",
      "Iter-2630, train loss-0.7908, acc-0.7900, valid loss-0.7785, acc-0.7716, test loss-0.7583, acc-0.7853\n",
      "Iter-2640, train loss-0.7768, acc-0.7400, valid loss-0.7775, acc-0.7700, test loss-0.7570, acc-0.7854\n",
      "Iter-2650, train loss-0.8874, acc-0.7700, valid loss-0.7759, acc-0.7728, test loss-0.7557, acc-0.7861\n",
      "Iter-2660, train loss-0.7991, acc-0.8000, valid loss-0.7744, acc-0.7724, test loss-0.7543, acc-0.7862\n",
      "Iter-2670, train loss-0.8659, acc-0.7400, valid loss-0.7733, acc-0.7720, test loss-0.7529, acc-0.7863\n",
      "Iter-2680, train loss-0.7362, acc-0.7500, valid loss-0.7721, acc-0.7718, test loss-0.7518, acc-0.7871\n",
      "Iter-2690, train loss-0.8426, acc-0.6900, valid loss-0.7719, acc-0.7728, test loss-0.7516, acc-0.7863\n",
      "Iter-2700, train loss-0.6413, acc-0.8400, valid loss-0.7712, acc-0.7732, test loss-0.7506, acc-0.7865\n",
      "Iter-2710, train loss-0.8002, acc-0.7800, valid loss-0.7697, acc-0.7732, test loss-0.7489, acc-0.7880\n",
      "Iter-2720, train loss-0.7852, acc-0.7200, valid loss-0.7679, acc-0.7744, test loss-0.7475, acc-0.7889\n",
      "Iter-2730, train loss-0.5164, acc-0.8900, valid loss-0.7670, acc-0.7752, test loss-0.7466, acc-0.7885\n",
      "Iter-2740, train loss-0.7227, acc-0.8000, valid loss-0.7657, acc-0.7754, test loss-0.7450, acc-0.7894\n",
      "Iter-2750, train loss-0.7361, acc-0.7700, valid loss-0.7644, acc-0.7758, test loss-0.7440, acc-0.7892\n",
      "Iter-2760, train loss-0.7051, acc-0.8000, valid loss-0.7623, acc-0.7770, test loss-0.7422, acc-0.7896\n",
      "Iter-2770, train loss-0.7840, acc-0.7700, valid loss-0.7613, acc-0.7768, test loss-0.7411, acc-0.7897\n",
      "Iter-2780, train loss-0.8599, acc-0.7600, valid loss-0.7600, acc-0.7768, test loss-0.7398, acc-0.7905\n",
      "Iter-2790, train loss-0.9441, acc-0.6600, valid loss-0.7583, acc-0.7780, test loss-0.7386, acc-0.7912\n",
      "Iter-2800, train loss-0.8180, acc-0.7500, valid loss-0.7573, acc-0.7784, test loss-0.7375, acc-0.7922\n",
      "Iter-2810, train loss-0.6663, acc-0.8100, valid loss-0.7565, acc-0.7792, test loss-0.7366, acc-0.7923\n",
      "Iter-2820, train loss-0.6387, acc-0.7900, valid loss-0.7560, acc-0.7800, test loss-0.7358, acc-0.7925\n",
      "Iter-2830, train loss-0.8177, acc-0.7900, valid loss-0.7549, acc-0.7806, test loss-0.7342, acc-0.7932\n",
      "Iter-2840, train loss-0.8360, acc-0.7200, valid loss-0.7529, acc-0.7806, test loss-0.7327, acc-0.7932\n",
      "Iter-2850, train loss-0.7707, acc-0.7600, valid loss-0.7520, acc-0.7812, test loss-0.7319, acc-0.7930\n",
      "Iter-2860, train loss-0.7229, acc-0.8000, valid loss-0.7507, acc-0.7808, test loss-0.7306, acc-0.7941\n",
      "Iter-2870, train loss-0.7468, acc-0.7600, valid loss-0.7491, acc-0.7810, test loss-0.7297, acc-0.7950\n",
      "Iter-2880, train loss-0.4925, acc-0.8400, valid loss-0.7480, acc-0.7806, test loss-0.7284, acc-0.7948\n",
      "Iter-2890, train loss-0.7567, acc-0.8000, valid loss-0.7472, acc-0.7810, test loss-0.7275, acc-0.7953\n",
      "Iter-2900, train loss-0.8164, acc-0.7700, valid loss-0.7463, acc-0.7810, test loss-0.7264, acc-0.7961\n",
      "Iter-2910, train loss-0.6448, acc-0.8100, valid loss-0.7452, acc-0.7820, test loss-0.7253, acc-0.7961\n",
      "Iter-2920, train loss-0.7472, acc-0.7800, valid loss-0.7438, acc-0.7824, test loss-0.7242, acc-0.7973\n",
      "Iter-2930, train loss-0.6940, acc-0.7900, valid loss-0.7424, acc-0.7818, test loss-0.7230, acc-0.7971\n",
      "Iter-2940, train loss-0.6290, acc-0.8500, valid loss-0.7410, acc-0.7830, test loss-0.7218, acc-0.7974\n",
      "Iter-2950, train loss-0.6854, acc-0.8100, valid loss-0.7398, acc-0.7834, test loss-0.7205, acc-0.7979\n",
      "Iter-2960, train loss-0.6854, acc-0.8300, valid loss-0.7387, acc-0.7842, test loss-0.7195, acc-0.7983\n",
      "Iter-2970, train loss-0.7827, acc-0.7600, valid loss-0.7377, acc-0.7838, test loss-0.7186, acc-0.7984\n",
      "Iter-2980, train loss-0.8743, acc-0.7500, valid loss-0.7372, acc-0.7842, test loss-0.7180, acc-0.7991\n",
      "Iter-2990, train loss-0.6987, acc-0.7700, valid loss-0.7361, acc-0.7852, test loss-0.7168, acc-0.7996\n",
      "Iter-3000, train loss-0.5775, acc-0.8300, valid loss-0.7352, acc-0.7862, test loss-0.7158, acc-0.8006\n",
      "Iter-3010, train loss-0.7470, acc-0.7800, valid loss-0.7336, acc-0.7870, test loss-0.7147, acc-0.8013\n",
      "Iter-3020, train loss-0.8234, acc-0.7800, valid loss-0.7328, acc-0.7860, test loss-0.7142, acc-0.8012\n",
      "Iter-3030, train loss-0.7997, acc-0.7700, valid loss-0.7323, acc-0.7864, test loss-0.7135, acc-0.8015\n",
      "Iter-3040, train loss-0.8077, acc-0.7800, valid loss-0.7311, acc-0.7874, test loss-0.7124, acc-0.8019\n",
      "Iter-3050, train loss-0.6702, acc-0.8000, valid loss-0.7305, acc-0.7870, test loss-0.7117, acc-0.8020\n",
      "Iter-3060, train loss-0.7097, acc-0.7900, valid loss-0.7303, acc-0.7872, test loss-0.7113, acc-0.8016\n",
      "Iter-3070, train loss-0.5449, acc-0.8500, valid loss-0.7295, acc-0.7882, test loss-0.7105, acc-0.8016\n",
      "Iter-3080, train loss-0.7714, acc-0.7400, valid loss-0.7283, acc-0.7872, test loss-0.7095, acc-0.8022\n",
      "Iter-3090, train loss-0.6600, acc-0.8300, valid loss-0.7280, acc-0.7866, test loss-0.7090, acc-0.8022\n",
      "Iter-3100, train loss-0.6240, acc-0.8400, valid loss-0.7274, acc-0.7866, test loss-0.7084, acc-0.8022\n",
      "Iter-3110, train loss-0.7043, acc-0.7900, valid loss-0.7273, acc-0.7868, test loss-0.7078, acc-0.8022\n",
      "Iter-3120, train loss-0.7358, acc-0.7700, valid loss-0.7266, acc-0.7882, test loss-0.7072, acc-0.8019\n",
      "Iter-3130, train loss-0.7159, acc-0.7800, valid loss-0.7254, acc-0.7880, test loss-0.7064, acc-0.8028\n",
      "Iter-3140, train loss-0.4618, acc-0.8800, valid loss-0.7242, acc-0.7872, test loss-0.7056, acc-0.8028\n",
      "Iter-3150, train loss-0.7379, acc-0.7900, valid loss-0.7220, acc-0.7896, test loss-0.7039, acc-0.8040\n",
      "Iter-3160, train loss-0.6322, acc-0.8600, valid loss-0.7216, acc-0.7876, test loss-0.7029, acc-0.8036\n",
      "Iter-3170, train loss-0.7953, acc-0.7900, valid loss-0.7212, acc-0.7884, test loss-0.7028, acc-0.8036\n",
      "Iter-3180, train loss-0.8277, acc-0.7200, valid loss-0.7196, acc-0.7900, test loss-0.7019, acc-0.8038\n",
      "Iter-3190, train loss-0.6282, acc-0.8100, valid loss-0.7188, acc-0.7896, test loss-0.7011, acc-0.8038\n",
      "Iter-3200, train loss-0.6460, acc-0.8100, valid loss-0.7181, acc-0.7894, test loss-0.6998, acc-0.8041\n",
      "Iter-3210, train loss-0.5514, acc-0.8800, valid loss-0.7178, acc-0.7894, test loss-0.6994, acc-0.8041\n",
      "Iter-3220, train loss-0.7823, acc-0.7600, valid loss-0.7169, acc-0.7908, test loss-0.6985, acc-0.8040\n",
      "Iter-3230, train loss-0.6349, acc-0.7900, valid loss-0.7160, acc-0.7918, test loss-0.6978, acc-0.8038\n",
      "Iter-3240, train loss-0.6061, acc-0.8100, valid loss-0.7152, acc-0.7910, test loss-0.6968, acc-0.8043\n",
      "Iter-3250, train loss-0.7573, acc-0.7500, valid loss-0.7142, acc-0.7918, test loss-0.6960, acc-0.8045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.5466, acc-0.8900, valid loss-0.7134, acc-0.7922, test loss-0.6953, acc-0.8048\n",
      "Iter-3270, train loss-0.7672, acc-0.7900, valid loss-0.7127, acc-0.7920, test loss-0.6946, acc-0.8059\n",
      "Iter-3280, train loss-0.7847, acc-0.7900, valid loss-0.7114, acc-0.7924, test loss-0.6936, acc-0.8060\n",
      "Iter-3290, train loss-0.6533, acc-0.8100, valid loss-0.7104, acc-0.7948, test loss-0.6933, acc-0.8061\n",
      "Iter-3300, train loss-0.8254, acc-0.7400, valid loss-0.7095, acc-0.7954, test loss-0.6929, acc-0.8062\n",
      "Iter-3310, train loss-0.7551, acc-0.7700, valid loss-0.7090, acc-0.7950, test loss-0.6925, acc-0.8064\n",
      "Iter-3320, train loss-0.8610, acc-0.7000, valid loss-0.7080, acc-0.7942, test loss-0.6916, acc-0.8067\n",
      "Iter-3330, train loss-0.7641, acc-0.7900, valid loss-0.7070, acc-0.7940, test loss-0.6905, acc-0.8071\n",
      "Iter-3340, train loss-0.7993, acc-0.7800, valid loss-0.7061, acc-0.7954, test loss-0.6895, acc-0.8079\n",
      "Iter-3350, train loss-0.6051, acc-0.8500, valid loss-0.7053, acc-0.7962, test loss-0.6887, acc-0.8081\n",
      "Iter-3360, train loss-0.6872, acc-0.8100, valid loss-0.7044, acc-0.7962, test loss-0.6879, acc-0.8088\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 5000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
