{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        self.W_fixed = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y, _ = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train]) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3087, acc-0.1100, valid loss-2.3165, acc-0.0904, test loss-2.3212, acc-0.0782\n",
      "Iter-20, train loss-2.3131, acc-0.0900, valid loss-2.3123, acc-0.0944, test loss-2.3173, acc-0.0810\n",
      "Iter-30, train loss-2.3156, acc-0.1000, valid loss-2.3091, acc-0.0984, test loss-2.3142, acc-0.0823\n",
      "Iter-40, train loss-2.3062, acc-0.1200, valid loss-2.3037, acc-0.1090, test loss-2.3088, acc-0.0934\n",
      "Iter-50, train loss-2.3060, acc-0.1000, valid loss-2.2982, acc-0.1204, test loss-2.3034, acc-0.1052\n",
      "Iter-60, train loss-2.3041, acc-0.0800, valid loss-2.2925, acc-0.1362, test loss-2.2977, acc-0.1198\n",
      "Iter-70, train loss-2.2926, acc-0.1100, valid loss-2.2863, acc-0.1450, test loss-2.2914, acc-0.1261\n",
      "Iter-80, train loss-2.2872, acc-0.1500, valid loss-2.2792, acc-0.1596, test loss-2.2842, acc-0.1393\n",
      "Iter-90, train loss-2.2717, acc-0.2700, valid loss-2.2716, acc-0.1682, test loss-2.2768, acc-0.1468\n",
      "Iter-100, train loss-2.2874, acc-0.1800, valid loss-2.2642, acc-0.1792, test loss-2.2692, acc-0.1540\n",
      "Iter-110, train loss-2.2547, acc-0.1800, valid loss-2.2556, acc-0.1938, test loss-2.2602, acc-0.1682\n",
      "Iter-120, train loss-2.2491, acc-0.1600, valid loss-2.2478, acc-0.2044, test loss-2.2523, acc-0.1825\n",
      "Iter-130, train loss-2.2459, acc-0.2700, valid loss-2.2393, acc-0.2148, test loss-2.2436, acc-0.1945\n",
      "Iter-140, train loss-2.2627, acc-0.1300, valid loss-2.2317, acc-0.2176, test loss-2.2357, acc-0.1995\n",
      "Iter-150, train loss-2.2466, acc-0.2000, valid loss-2.2217, acc-0.2254, test loss-2.2255, acc-0.2089\n",
      "Iter-160, train loss-2.2290, acc-0.2300, valid loss-2.2120, acc-0.2350, test loss-2.2154, acc-0.2183\n",
      "Iter-170, train loss-2.2323, acc-0.1700, valid loss-2.2013, acc-0.2344, test loss-2.2042, acc-0.2220\n",
      "Iter-180, train loss-2.2013, acc-0.2400, valid loss-2.1899, acc-0.2416, test loss-2.1923, acc-0.2275\n",
      "Iter-190, train loss-2.1974, acc-0.2000, valid loss-2.1783, acc-0.2492, test loss-2.1804, acc-0.2307\n",
      "Iter-200, train loss-2.1643, acc-0.3300, valid loss-2.1664, acc-0.2574, test loss-2.1682, acc-0.2383\n",
      "Iter-210, train loss-2.1402, acc-0.3200, valid loss-2.1536, acc-0.2690, test loss-2.1552, acc-0.2488\n",
      "Iter-220, train loss-2.1421, acc-0.2700, valid loss-2.1404, acc-0.2802, test loss-2.1417, acc-0.2603\n",
      "Iter-230, train loss-2.1359, acc-0.3100, valid loss-2.1252, acc-0.3058, test loss-2.1264, acc-0.2898\n",
      "Iter-240, train loss-2.0935, acc-0.3000, valid loss-2.1088, acc-0.3292, test loss-2.1099, acc-0.3189\n",
      "Iter-250, train loss-2.1277, acc-0.2700, valid loss-2.0911, acc-0.3618, test loss-2.0925, acc-0.3497\n",
      "Iter-260, train loss-2.0834, acc-0.2900, valid loss-2.0746, acc-0.3822, test loss-2.0759, acc-0.3731\n",
      "Iter-270, train loss-2.0531, acc-0.4000, valid loss-2.0568, acc-0.3998, test loss-2.0581, acc-0.3909\n",
      "Iter-280, train loss-2.0698, acc-0.3900, valid loss-2.0382, acc-0.4150, test loss-2.0397, acc-0.4120\n",
      "Iter-290, train loss-2.0242, acc-0.4200, valid loss-2.0196, acc-0.4240, test loss-2.0213, acc-0.4219\n",
      "Iter-300, train loss-1.9966, acc-0.4300, valid loss-2.0003, acc-0.4264, test loss-2.0022, acc-0.4250\n",
      "Iter-310, train loss-1.9927, acc-0.4200, valid loss-1.9820, acc-0.4308, test loss-1.9841, acc-0.4287\n",
      "Iter-320, train loss-1.9811, acc-0.3800, valid loss-1.9614, acc-0.4364, test loss-1.9638, acc-0.4353\n",
      "Iter-330, train loss-2.0031, acc-0.3400, valid loss-1.9414, acc-0.4384, test loss-1.9440, acc-0.4399\n",
      "Iter-340, train loss-1.9349, acc-0.3900, valid loss-1.9227, acc-0.4400, test loss-1.9256, acc-0.4406\n",
      "Iter-350, train loss-1.9201, acc-0.4700, valid loss-1.9039, acc-0.4400, test loss-1.9074, acc-0.4376\n",
      "Iter-360, train loss-1.8914, acc-0.3700, valid loss-1.8856, acc-0.4404, test loss-1.8893, acc-0.4395\n",
      "Iter-370, train loss-1.9131, acc-0.4200, valid loss-1.8682, acc-0.4400, test loss-1.8718, acc-0.4373\n",
      "Iter-380, train loss-1.8914, acc-0.3800, valid loss-1.8498, acc-0.4420, test loss-1.8537, acc-0.4356\n",
      "Iter-390, train loss-1.8481, acc-0.4200, valid loss-1.8335, acc-0.4410, test loss-1.8377, acc-0.4320\n",
      "Iter-400, train loss-1.7603, acc-0.5000, valid loss-1.8166, acc-0.4396, test loss-1.8210, acc-0.4299\n",
      "Iter-410, train loss-1.8010, acc-0.4600, valid loss-1.8004, acc-0.4388, test loss-1.8051, acc-0.4303\n",
      "Iter-420, train loss-1.8431, acc-0.3800, valid loss-1.7855, acc-0.4410, test loss-1.7903, acc-0.4316\n",
      "Iter-430, train loss-1.7829, acc-0.4300, valid loss-1.7717, acc-0.4394, test loss-1.7766, acc-0.4300\n",
      "Iter-440, train loss-1.7560, acc-0.4000, valid loss-1.7589, acc-0.4362, test loss-1.7643, acc-0.4273\n",
      "Iter-450, train loss-1.7707, acc-0.4200, valid loss-1.7464, acc-0.4350, test loss-1.7518, acc-0.4269\n",
      "Iter-460, train loss-1.7051, acc-0.4500, valid loss-1.7337, acc-0.4362, test loss-1.7393, acc-0.4258\n",
      "Iter-470, train loss-1.7558, acc-0.4300, valid loss-1.7223, acc-0.4326, test loss-1.7280, acc-0.4227\n",
      "Iter-480, train loss-1.7243, acc-0.4300, valid loss-1.7121, acc-0.4326, test loss-1.7179, acc-0.4239\n",
      "Iter-490, train loss-1.7049, acc-0.4200, valid loss-1.7020, acc-0.4350, test loss-1.7075, acc-0.4273\n",
      "Iter-500, train loss-1.7697, acc-0.4200, valid loss-1.6934, acc-0.4362, test loss-1.6989, acc-0.4276\n",
      "Iter-510, train loss-1.6502, acc-0.3900, valid loss-1.6848, acc-0.4378, test loss-1.6904, acc-0.4282\n",
      "Iter-520, train loss-1.7706, acc-0.3700, valid loss-1.6770, acc-0.4408, test loss-1.6820, acc-0.4319\n",
      "Iter-530, train loss-1.6602, acc-0.4200, valid loss-1.6669, acc-0.4472, test loss-1.6726, acc-0.4370\n",
      "Iter-540, train loss-1.6683, acc-0.4800, valid loss-1.6578, acc-0.4494, test loss-1.6635, acc-0.4405\n",
      "Iter-550, train loss-1.5535, acc-0.4900, valid loss-1.6496, acc-0.4524, test loss-1.6559, acc-0.4472\n",
      "Iter-560, train loss-1.6358, acc-0.5300, valid loss-1.6425, acc-0.4574, test loss-1.6485, acc-0.4500\n",
      "Iter-570, train loss-1.7142, acc-0.4300, valid loss-1.6358, acc-0.4570, test loss-1.6417, acc-0.4507\n",
      "Iter-580, train loss-1.6960, acc-0.4100, valid loss-1.6293, acc-0.4582, test loss-1.6350, acc-0.4520\n",
      "Iter-590, train loss-1.6223, acc-0.4400, valid loss-1.6223, acc-0.4650, test loss-1.6280, acc-0.4537\n",
      "Iter-600, train loss-1.6950, acc-0.4500, valid loss-1.6170, acc-0.4644, test loss-1.6218, acc-0.4553\n",
      "Iter-610, train loss-1.6502, acc-0.4600, valid loss-1.6108, acc-0.4672, test loss-1.6148, acc-0.4586\n",
      "Iter-620, train loss-1.6524, acc-0.3900, valid loss-1.6047, acc-0.4710, test loss-1.6086, acc-0.4607\n",
      "Iter-630, train loss-1.5363, acc-0.4600, valid loss-1.5974, acc-0.4722, test loss-1.6012, acc-0.4636\n",
      "Iter-640, train loss-1.5765, acc-0.4700, valid loss-1.5911, acc-0.4784, test loss-1.5945, acc-0.4687\n",
      "Iter-650, train loss-1.6184, acc-0.4400, valid loss-1.5862, acc-0.4828, test loss-1.5894, acc-0.4707\n",
      "Iter-660, train loss-1.5161, acc-0.5500, valid loss-1.5801, acc-0.4858, test loss-1.5831, acc-0.4730\n",
      "Iter-670, train loss-1.5706, acc-0.4800, valid loss-1.5747, acc-0.4852, test loss-1.5775, acc-0.4756\n",
      "Iter-680, train loss-1.5533, acc-0.5200, valid loss-1.5698, acc-0.4888, test loss-1.5724, acc-0.4748\n",
      "Iter-690, train loss-1.5566, acc-0.5100, valid loss-1.5641, acc-0.4934, test loss-1.5662, acc-0.4795\n",
      "Iter-700, train loss-1.4898, acc-0.4900, valid loss-1.5575, acc-0.4922, test loss-1.5592, acc-0.4832\n",
      "Iter-710, train loss-1.5187, acc-0.5000, valid loss-1.5541, acc-0.4888, test loss-1.5551, acc-0.4810\n",
      "Iter-720, train loss-1.6191, acc-0.4700, valid loss-1.5485, acc-0.4884, test loss-1.5493, acc-0.4834\n",
      "Iter-730, train loss-1.5850, acc-0.4100, valid loss-1.5405, acc-0.4910, test loss-1.5409, acc-0.4848\n",
      "Iter-740, train loss-1.5481, acc-0.4600, valid loss-1.5332, acc-0.4970, test loss-1.5335, acc-0.4899\n",
      "Iter-750, train loss-1.5658, acc-0.4200, valid loss-1.5279, acc-0.4946, test loss-1.5280, acc-0.4899\n",
      "Iter-760, train loss-1.5258, acc-0.4500, valid loss-1.5221, acc-0.4936, test loss-1.5217, acc-0.4880\n",
      "Iter-770, train loss-1.5714, acc-0.4200, valid loss-1.5155, acc-0.4986, test loss-1.5152, acc-0.4928\n",
      "Iter-780, train loss-1.5696, acc-0.4900, valid loss-1.5124, acc-0.4984, test loss-1.5110, acc-0.4935\n",
      "Iter-790, train loss-1.5812, acc-0.4800, valid loss-1.5069, acc-0.5012, test loss-1.5051, acc-0.4980\n",
      "Iter-800, train loss-1.5103, acc-0.5300, valid loss-1.5001, acc-0.5084, test loss-1.4980, acc-0.5024\n",
      "Iter-810, train loss-1.4914, acc-0.4500, valid loss-1.4961, acc-0.5100, test loss-1.4932, acc-0.5059\n",
      "Iter-820, train loss-1.5550, acc-0.5100, valid loss-1.4907, acc-0.5106, test loss-1.4872, acc-0.5071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-1.3958, acc-0.5300, valid loss-1.4835, acc-0.5134, test loss-1.4799, acc-0.5094\n",
      "Iter-840, train loss-1.4062, acc-0.5700, valid loss-1.4766, acc-0.5186, test loss-1.4729, acc-0.5131\n",
      "Iter-850, train loss-1.5265, acc-0.4500, valid loss-1.4707, acc-0.5170, test loss-1.4663, acc-0.5156\n",
      "Iter-860, train loss-1.4836, acc-0.5200, valid loss-1.4648, acc-0.5248, test loss-1.4597, acc-0.5200\n",
      "Iter-870, train loss-1.5224, acc-0.4900, valid loss-1.4579, acc-0.5288, test loss-1.4527, acc-0.5238\n",
      "Iter-880, train loss-1.4697, acc-0.5100, valid loss-1.4508, acc-0.5320, test loss-1.4454, acc-0.5272\n",
      "Iter-890, train loss-1.4153, acc-0.5300, valid loss-1.4439, acc-0.5310, test loss-1.4380, acc-0.5324\n",
      "Iter-900, train loss-1.4687, acc-0.5000, valid loss-1.4365, acc-0.5360, test loss-1.4305, acc-0.5368\n",
      "Iter-910, train loss-1.4731, acc-0.5600, valid loss-1.4280, acc-0.5404, test loss-1.4218, acc-0.5428\n",
      "Iter-920, train loss-1.4765, acc-0.5000, valid loss-1.4223, acc-0.5458, test loss-1.4156, acc-0.5456\n",
      "Iter-930, train loss-1.3145, acc-0.6000, valid loss-1.4163, acc-0.5480, test loss-1.4088, acc-0.5506\n",
      "Iter-940, train loss-1.4502, acc-0.6700, valid loss-1.4082, acc-0.5552, test loss-1.4003, acc-0.5562\n",
      "Iter-950, train loss-1.4459, acc-0.5400, valid loss-1.4015, acc-0.5568, test loss-1.3935, acc-0.5566\n",
      "Iter-960, train loss-1.3739, acc-0.5800, valid loss-1.3928, acc-0.5594, test loss-1.3847, acc-0.5598\n",
      "Iter-970, train loss-1.2081, acc-0.6400, valid loss-1.3861, acc-0.5602, test loss-1.3775, acc-0.5651\n",
      "Iter-980, train loss-1.3502, acc-0.5000, valid loss-1.3811, acc-0.5618, test loss-1.3718, acc-0.5666\n",
      "Iter-990, train loss-1.3918, acc-0.5200, valid loss-1.3727, acc-0.5678, test loss-1.3635, acc-0.5727\n",
      "Iter-1000, train loss-1.4849, acc-0.5400, valid loss-1.3651, acc-0.5700, test loss-1.3559, acc-0.5764\n",
      "Iter-1010, train loss-1.2388, acc-0.6400, valid loss-1.3569, acc-0.5726, test loss-1.3470, acc-0.5793\n",
      "Iter-1020, train loss-1.2891, acc-0.6100, valid loss-1.3481, acc-0.5814, test loss-1.3380, acc-0.5864\n",
      "Iter-1030, train loss-1.1814, acc-0.7200, valid loss-1.3432, acc-0.5800, test loss-1.3324, acc-0.5875\n",
      "Iter-1040, train loss-1.3004, acc-0.6300, valid loss-1.3371, acc-0.5848, test loss-1.3256, acc-0.5915\n",
      "Iter-1050, train loss-1.2777, acc-0.5500, valid loss-1.3303, acc-0.5908, test loss-1.3183, acc-0.5950\n",
      "Iter-1060, train loss-1.2361, acc-0.6900, valid loss-1.3234, acc-0.5928, test loss-1.3110, acc-0.5966\n",
      "Iter-1070, train loss-1.4284, acc-0.5300, valid loss-1.3148, acc-0.5996, test loss-1.3023, acc-0.6045\n",
      "Iter-1080, train loss-1.2225, acc-0.6600, valid loss-1.3088, acc-0.6036, test loss-1.2957, acc-0.6072\n",
      "Iter-1090, train loss-1.3520, acc-0.5800, valid loss-1.3032, acc-0.6028, test loss-1.2895, acc-0.6056\n",
      "Iter-1100, train loss-1.3485, acc-0.6000, valid loss-1.2946, acc-0.6088, test loss-1.2805, acc-0.6132\n",
      "Iter-1110, train loss-1.2494, acc-0.6200, valid loss-1.2884, acc-0.6124, test loss-1.2740, acc-0.6151\n",
      "Iter-1120, train loss-1.2856, acc-0.5700, valid loss-1.2811, acc-0.6154, test loss-1.2665, acc-0.6188\n",
      "Iter-1130, train loss-1.2240, acc-0.6300, valid loss-1.2738, acc-0.6226, test loss-1.2591, acc-0.6242\n",
      "Iter-1140, train loss-1.3641, acc-0.5900, valid loss-1.2692, acc-0.6278, test loss-1.2543, acc-0.6306\n",
      "Iter-1150, train loss-1.2048, acc-0.6700, valid loss-1.2618, acc-0.6328, test loss-1.2467, acc-0.6366\n",
      "Iter-1160, train loss-1.2482, acc-0.6400, valid loss-1.2550, acc-0.6350, test loss-1.2398, acc-0.6386\n",
      "Iter-1170, train loss-1.2425, acc-0.6400, valid loss-1.2478, acc-0.6394, test loss-1.2321, acc-0.6437\n",
      "Iter-1180, train loss-1.2391, acc-0.6200, valid loss-1.2402, acc-0.6410, test loss-1.2242, acc-0.6455\n",
      "Iter-1190, train loss-1.1898, acc-0.6200, valid loss-1.2325, acc-0.6448, test loss-1.2165, acc-0.6487\n",
      "Iter-1200, train loss-1.2238, acc-0.6800, valid loss-1.2266, acc-0.6470, test loss-1.2099, acc-0.6517\n",
      "Iter-1210, train loss-1.2695, acc-0.6400, valid loss-1.2186, acc-0.6500, test loss-1.2015, acc-0.6562\n",
      "Iter-1220, train loss-1.2990, acc-0.6200, valid loss-1.2114, acc-0.6540, test loss-1.1937, acc-0.6581\n",
      "Iter-1230, train loss-1.2076, acc-0.6800, valid loss-1.2046, acc-0.6552, test loss-1.1872, acc-0.6593\n",
      "Iter-1240, train loss-1.2726, acc-0.6300, valid loss-1.1971, acc-0.6588, test loss-1.1793, acc-0.6639\n",
      "Iter-1250, train loss-1.1245, acc-0.6900, valid loss-1.1905, acc-0.6614, test loss-1.1721, acc-0.6657\n",
      "Iter-1260, train loss-1.1631, acc-0.6900, valid loss-1.1840, acc-0.6624, test loss-1.1654, acc-0.6665\n",
      "Iter-1270, train loss-1.2518, acc-0.6200, valid loss-1.1780, acc-0.6654, test loss-1.1589, acc-0.6698\n",
      "Iter-1280, train loss-1.0819, acc-0.7200, valid loss-1.1711, acc-0.6662, test loss-1.1519, acc-0.6735\n",
      "Iter-1290, train loss-1.2445, acc-0.6000, valid loss-1.1652, acc-0.6684, test loss-1.1457, acc-0.6763\n",
      "Iter-1300, train loss-1.1153, acc-0.7000, valid loss-1.1600, acc-0.6702, test loss-1.1400, acc-0.6766\n",
      "Iter-1310, train loss-1.0752, acc-0.6700, valid loss-1.1567, acc-0.6720, test loss-1.1359, acc-0.6779\n",
      "Iter-1320, train loss-1.2140, acc-0.6600, valid loss-1.1501, acc-0.6740, test loss-1.1294, acc-0.6799\n",
      "Iter-1330, train loss-1.2662, acc-0.6000, valid loss-1.1443, acc-0.6768, test loss-1.1233, acc-0.6840\n",
      "Iter-1340, train loss-1.0231, acc-0.7000, valid loss-1.1370, acc-0.6792, test loss-1.1164, acc-0.6862\n",
      "Iter-1350, train loss-1.2954, acc-0.5700, valid loss-1.1332, acc-0.6784, test loss-1.1125, acc-0.6858\n",
      "Iter-1360, train loss-1.1178, acc-0.6700, valid loss-1.1269, acc-0.6800, test loss-1.1068, acc-0.6876\n",
      "Iter-1370, train loss-1.0826, acc-0.6500, valid loss-1.1224, acc-0.6780, test loss-1.1025, acc-0.6890\n",
      "Iter-1380, train loss-0.9608, acc-0.8100, valid loss-1.1168, acc-0.6788, test loss-1.0968, acc-0.6893\n",
      "Iter-1390, train loss-1.1429, acc-0.6500, valid loss-1.1123, acc-0.6800, test loss-1.0921, acc-0.6897\n",
      "Iter-1400, train loss-1.0874, acc-0.7000, valid loss-1.1069, acc-0.6832, test loss-1.0867, acc-0.6902\n",
      "Iter-1410, train loss-1.1126, acc-0.7100, valid loss-1.1021, acc-0.6858, test loss-1.0812, acc-0.6920\n",
      "Iter-1420, train loss-0.9887, acc-0.7600, valid loss-1.0980, acc-0.6852, test loss-1.0772, acc-0.6921\n",
      "Iter-1430, train loss-1.0985, acc-0.6800, valid loss-1.0923, acc-0.6860, test loss-1.0714, acc-0.6940\n",
      "Iter-1440, train loss-1.1414, acc-0.6400, valid loss-1.0881, acc-0.6874, test loss-1.0669, acc-0.6951\n",
      "Iter-1450, train loss-1.0571, acc-0.6900, valid loss-1.0833, acc-0.6894, test loss-1.0615, acc-0.6969\n",
      "Iter-1460, train loss-1.1120, acc-0.6600, valid loss-1.0777, acc-0.6910, test loss-1.0564, acc-0.6973\n",
      "Iter-1470, train loss-1.0758, acc-0.6500, valid loss-1.0714, acc-0.6932, test loss-1.0503, acc-0.6995\n",
      "Iter-1480, train loss-0.8657, acc-0.7600, valid loss-1.0667, acc-0.6938, test loss-1.0456, acc-0.7016\n",
      "Iter-1490, train loss-1.3632, acc-0.6000, valid loss-1.0609, acc-0.6954, test loss-1.0399, acc-0.7044\n",
      "Iter-1500, train loss-0.9964, acc-0.6800, valid loss-1.0568, acc-0.6952, test loss-1.0355, acc-0.7045\n",
      "Iter-1510, train loss-1.0364, acc-0.7300, valid loss-1.0530, acc-0.6964, test loss-1.0318, acc-0.7062\n",
      "Iter-1520, train loss-1.1255, acc-0.6800, valid loss-1.0489, acc-0.6978, test loss-1.0276, acc-0.7068\n",
      "Iter-1530, train loss-1.1172, acc-0.6900, valid loss-1.0443, acc-0.6996, test loss-1.0227, acc-0.7077\n",
      "Iter-1540, train loss-0.8677, acc-0.7600, valid loss-1.0405, acc-0.6998, test loss-1.0186, acc-0.7083\n",
      "Iter-1550, train loss-0.8451, acc-0.8000, valid loss-1.0364, acc-0.7030, test loss-1.0145, acc-0.7092\n",
      "Iter-1560, train loss-0.9643, acc-0.7600, valid loss-1.0335, acc-0.7028, test loss-1.0111, acc-0.7095\n",
      "Iter-1570, train loss-1.0842, acc-0.6400, valid loss-1.0306, acc-0.7038, test loss-1.0084, acc-0.7106\n",
      "Iter-1580, train loss-1.1503, acc-0.6100, valid loss-1.0243, acc-0.7064, test loss-1.0030, acc-0.7115\n",
      "Iter-1590, train loss-1.0972, acc-0.6800, valid loss-1.0202, acc-0.7060, test loss-0.9990, acc-0.7133\n",
      "Iter-1600, train loss-1.1298, acc-0.6200, valid loss-1.0162, acc-0.7080, test loss-0.9949, acc-0.7146\n",
      "Iter-1610, train loss-0.9870, acc-0.6600, valid loss-1.0120, acc-0.7086, test loss-0.9910, acc-0.7146\n",
      "Iter-1620, train loss-1.0219, acc-0.6900, valid loss-1.0086, acc-0.7088, test loss-0.9874, acc-0.7155\n",
      "Iter-1630, train loss-0.9856, acc-0.7000, valid loss-1.0065, acc-0.7090, test loss-0.9848, acc-0.7162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-1.1400, acc-0.6200, valid loss-1.0027, acc-0.7104, test loss-0.9807, acc-0.7173\n",
      "Iter-1650, train loss-0.9940, acc-0.7000, valid loss-0.9997, acc-0.7120, test loss-0.9777, acc-0.7189\n",
      "Iter-1660, train loss-0.9211, acc-0.7300, valid loss-0.9960, acc-0.7132, test loss-0.9736, acc-0.7212\n",
      "Iter-1670, train loss-0.9081, acc-0.7600, valid loss-0.9931, acc-0.7128, test loss-0.9706, acc-0.7209\n",
      "Iter-1680, train loss-0.8404, acc-0.7400, valid loss-0.9892, acc-0.7150, test loss-0.9667, acc-0.7230\n",
      "Iter-1690, train loss-0.8995, acc-0.7300, valid loss-0.9854, acc-0.7162, test loss-0.9630, acc-0.7240\n",
      "Iter-1700, train loss-0.9356, acc-0.7100, valid loss-0.9820, acc-0.7166, test loss-0.9594, acc-0.7245\n",
      "Iter-1710, train loss-0.9419, acc-0.7100, valid loss-0.9800, acc-0.7152, test loss-0.9570, acc-0.7248\n",
      "Iter-1720, train loss-0.9738, acc-0.7000, valid loss-0.9751, acc-0.7196, test loss-0.9524, acc-0.7257\n",
      "Iter-1730, train loss-0.9318, acc-0.7400, valid loss-0.9719, acc-0.7206, test loss-0.9486, acc-0.7260\n",
      "Iter-1740, train loss-1.0305, acc-0.6100, valid loss-0.9692, acc-0.7206, test loss-0.9460, acc-0.7267\n",
      "Iter-1750, train loss-1.0252, acc-0.6900, valid loss-0.9658, acc-0.7204, test loss-0.9425, acc-0.7277\n",
      "Iter-1760, train loss-0.9217, acc-0.7300, valid loss-0.9616, acc-0.7230, test loss-0.9385, acc-0.7292\n",
      "Iter-1770, train loss-0.8950, acc-0.7700, valid loss-0.9589, acc-0.7228, test loss-0.9358, acc-0.7311\n",
      "Iter-1780, train loss-0.9752, acc-0.6900, valid loss-0.9562, acc-0.7234, test loss-0.9331, acc-0.7325\n",
      "Iter-1790, train loss-0.9324, acc-0.7700, valid loss-0.9541, acc-0.7252, test loss-0.9305, acc-0.7321\n",
      "Iter-1800, train loss-0.9894, acc-0.7400, valid loss-0.9509, acc-0.7258, test loss-0.9275, acc-0.7323\n",
      "Iter-1810, train loss-0.9248, acc-0.7100, valid loss-0.9479, acc-0.7274, test loss-0.9245, acc-0.7350\n",
      "Iter-1820, train loss-0.8853, acc-0.7300, valid loss-0.9455, acc-0.7284, test loss-0.9220, acc-0.7362\n",
      "Iter-1830, train loss-1.0201, acc-0.7000, valid loss-0.9419, acc-0.7292, test loss-0.9186, acc-0.7369\n",
      "Iter-1840, train loss-0.8791, acc-0.7200, valid loss-0.9398, acc-0.7284, test loss-0.9160, acc-0.7383\n",
      "Iter-1850, train loss-0.8944, acc-0.7700, valid loss-0.9377, acc-0.7288, test loss-0.9139, acc-0.7376\n",
      "Iter-1860, train loss-0.9333, acc-0.7300, valid loss-0.9359, acc-0.7294, test loss-0.9117, acc-0.7382\n",
      "Iter-1870, train loss-0.8288, acc-0.7900, valid loss-0.9313, acc-0.7318, test loss-0.9074, acc-0.7402\n",
      "Iter-1880, train loss-1.0709, acc-0.5900, valid loss-0.9285, acc-0.7318, test loss-0.9047, acc-0.7415\n",
      "Iter-1890, train loss-1.2505, acc-0.6200, valid loss-0.9256, acc-0.7334, test loss-0.9024, acc-0.7413\n",
      "Iter-1900, train loss-0.7877, acc-0.7900, valid loss-0.9220, acc-0.7342, test loss-0.8990, acc-0.7429\n",
      "Iter-1910, train loss-1.0457, acc-0.6500, valid loss-0.9189, acc-0.7354, test loss-0.8959, acc-0.7450\n",
      "Iter-1920, train loss-0.9468, acc-0.7100, valid loss-0.9163, acc-0.7366, test loss-0.8938, acc-0.7451\n",
      "Iter-1930, train loss-1.1558, acc-0.6400, valid loss-0.9127, acc-0.7370, test loss-0.8912, acc-0.7467\n",
      "Iter-1940, train loss-0.8815, acc-0.7600, valid loss-0.9105, acc-0.7374, test loss-0.8889, acc-0.7469\n",
      "Iter-1950, train loss-0.9239, acc-0.7300, valid loss-0.9080, acc-0.7384, test loss-0.8866, acc-0.7475\n",
      "Iter-1960, train loss-1.0662, acc-0.6800, valid loss-0.9052, acc-0.7382, test loss-0.8836, acc-0.7489\n",
      "Iter-1970, train loss-1.0102, acc-0.6300, valid loss-0.9034, acc-0.7388, test loss-0.8814, acc-0.7495\n",
      "Iter-1980, train loss-0.8586, acc-0.7300, valid loss-0.9002, acc-0.7402, test loss-0.8781, acc-0.7509\n",
      "Iter-1990, train loss-0.8744, acc-0.7600, valid loss-0.8990, acc-0.7400, test loss-0.8768, acc-0.7511\n",
      "Iter-2000, train loss-0.9571, acc-0.6700, valid loss-0.8963, acc-0.7416, test loss-0.8740, acc-0.7520\n",
      "Iter-2010, train loss-1.1402, acc-0.5700, valid loss-0.8935, acc-0.7418, test loss-0.8713, acc-0.7521\n",
      "Iter-2020, train loss-0.7757, acc-0.8000, valid loss-0.8913, acc-0.7426, test loss-0.8690, acc-0.7535\n",
      "Iter-2030, train loss-0.7755, acc-0.8200, valid loss-0.8884, acc-0.7438, test loss-0.8662, acc-0.7543\n",
      "Iter-2040, train loss-1.0444, acc-0.6600, valid loss-0.8858, acc-0.7438, test loss-0.8636, acc-0.7554\n",
      "Iter-2050, train loss-0.9469, acc-0.7300, valid loss-0.8822, acc-0.7436, test loss-0.8605, acc-0.7554\n",
      "Iter-2060, train loss-0.7616, acc-0.8400, valid loss-0.8799, acc-0.7454, test loss-0.8584, acc-0.7560\n",
      "Iter-2070, train loss-1.0410, acc-0.7300, valid loss-0.8776, acc-0.7460, test loss-0.8565, acc-0.7575\n",
      "Iter-2080, train loss-0.8840, acc-0.8000, valid loss-0.8764, acc-0.7462, test loss-0.8552, acc-0.7571\n",
      "Iter-2090, train loss-0.7345, acc-0.8700, valid loss-0.8741, acc-0.7470, test loss-0.8529, acc-0.7576\n",
      "Iter-2100, train loss-0.9225, acc-0.7800, valid loss-0.8713, acc-0.7480, test loss-0.8501, acc-0.7590\n",
      "Iter-2110, train loss-0.7502, acc-0.7700, valid loss-0.8680, acc-0.7476, test loss-0.8470, acc-0.7594\n",
      "Iter-2120, train loss-0.8068, acc-0.7500, valid loss-0.8654, acc-0.7484, test loss-0.8445, acc-0.7608\n",
      "Iter-2130, train loss-0.7829, acc-0.8200, valid loss-0.8619, acc-0.7494, test loss-0.8415, acc-0.7632\n",
      "Iter-2140, train loss-1.0228, acc-0.7400, valid loss-0.8607, acc-0.7488, test loss-0.8396, acc-0.7630\n",
      "Iter-2150, train loss-0.8689, acc-0.7600, valid loss-0.8594, acc-0.7494, test loss-0.8380, acc-0.7628\n",
      "Iter-2160, train loss-0.9438, acc-0.7000, valid loss-0.8570, acc-0.7508, test loss-0.8359, acc-0.7639\n",
      "Iter-2170, train loss-0.9519, acc-0.7300, valid loss-0.8558, acc-0.7508, test loss-0.8345, acc-0.7628\n",
      "Iter-2180, train loss-0.9226, acc-0.6800, valid loss-0.8532, acc-0.7518, test loss-0.8320, acc-0.7639\n",
      "Iter-2190, train loss-0.9024, acc-0.7000, valid loss-0.8508, acc-0.7510, test loss-0.8296, acc-0.7657\n",
      "Iter-2200, train loss-0.7804, acc-0.7400, valid loss-0.8484, acc-0.7524, test loss-0.8273, acc-0.7653\n",
      "Iter-2210, train loss-0.8988, acc-0.7300, valid loss-0.8467, acc-0.7534, test loss-0.8252, acc-0.7673\n",
      "Iter-2220, train loss-0.8415, acc-0.7600, valid loss-0.8462, acc-0.7534, test loss-0.8241, acc-0.7666\n",
      "Iter-2230, train loss-0.8208, acc-0.7400, valid loss-0.8441, acc-0.7554, test loss-0.8221, acc-0.7660\n",
      "Iter-2240, train loss-0.8410, acc-0.7700, valid loss-0.8426, acc-0.7544, test loss-0.8201, acc-0.7673\n",
      "Iter-2250, train loss-0.8056, acc-0.7500, valid loss-0.8415, acc-0.7546, test loss-0.8187, acc-0.7678\n",
      "Iter-2260, train loss-0.7457, acc-0.7700, valid loss-0.8393, acc-0.7556, test loss-0.8173, acc-0.7683\n",
      "Iter-2270, train loss-0.7537, acc-0.8400, valid loss-0.8360, acc-0.7552, test loss-0.8145, acc-0.7697\n",
      "Iter-2280, train loss-0.9361, acc-0.6800, valid loss-0.8324, acc-0.7566, test loss-0.8112, acc-0.7714\n",
      "Iter-2290, train loss-0.7975, acc-0.8200, valid loss-0.8308, acc-0.7572, test loss-0.8092, acc-0.7719\n",
      "Iter-2300, train loss-0.8079, acc-0.7500, valid loss-0.8304, acc-0.7572, test loss-0.8084, acc-0.7716\n",
      "Iter-2310, train loss-0.7150, acc-0.8100, valid loss-0.8287, acc-0.7572, test loss-0.8068, acc-0.7725\n",
      "Iter-2320, train loss-0.6715, acc-0.8000, valid loss-0.8271, acc-0.7576, test loss-0.8053, acc-0.7726\n",
      "Iter-2330, train loss-0.7727, acc-0.7800, valid loss-0.8253, acc-0.7592, test loss-0.8037, acc-0.7734\n",
      "Iter-2340, train loss-0.7118, acc-0.7900, valid loss-0.8238, acc-0.7600, test loss-0.8018, acc-0.7731\n",
      "Iter-2350, train loss-0.7202, acc-0.7900, valid loss-0.8218, acc-0.7608, test loss-0.8000, acc-0.7740\n",
      "Iter-2360, train loss-0.6434, acc-0.7900, valid loss-0.8194, acc-0.7602, test loss-0.7979, acc-0.7744\n",
      "Iter-2370, train loss-0.9228, acc-0.7100, valid loss-0.8172, acc-0.7614, test loss-0.7959, acc-0.7749\n",
      "Iter-2380, train loss-0.9827, acc-0.7200, valid loss-0.8153, acc-0.7608, test loss-0.7940, acc-0.7755\n",
      "Iter-2390, train loss-0.8443, acc-0.7600, valid loss-0.8137, acc-0.7610, test loss-0.7923, acc-0.7746\n",
      "Iter-2400, train loss-0.7698, acc-0.7600, valid loss-0.8112, acc-0.7624, test loss-0.7901, acc-0.7760\n",
      "Iter-2410, train loss-0.8422, acc-0.7500, valid loss-0.8099, acc-0.7622, test loss-0.7887, acc-0.7758\n",
      "Iter-2420, train loss-0.7963, acc-0.7900, valid loss-0.8090, acc-0.7626, test loss-0.7880, acc-0.7760\n",
      "Iter-2430, train loss-0.8530, acc-0.8100, valid loss-0.8079, acc-0.7630, test loss-0.7867, acc-0.7764\n",
      "Iter-2440, train loss-0.8204, acc-0.7900, valid loss-0.8063, acc-0.7628, test loss-0.7852, acc-0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.7794, acc-0.7900, valid loss-0.8044, acc-0.7636, test loss-0.7828, acc-0.7769\n",
      "Iter-2460, train loss-0.7651, acc-0.7700, valid loss-0.8035, acc-0.7636, test loss-0.7820, acc-0.7772\n",
      "Iter-2470, train loss-0.8655, acc-0.6900, valid loss-0.8012, acc-0.7648, test loss-0.7799, acc-0.7779\n",
      "Iter-2480, train loss-0.8066, acc-0.7700, valid loss-0.7987, acc-0.7662, test loss-0.7777, acc-0.7791\n",
      "Iter-2490, train loss-1.0301, acc-0.6400, valid loss-0.7968, acc-0.7670, test loss-0.7761, acc-0.7793\n",
      "Iter-2500, train loss-0.7432, acc-0.8100, valid loss-0.7960, acc-0.7668, test loss-0.7755, acc-0.7789\n",
      "Iter-2510, train loss-0.8249, acc-0.7300, valid loss-0.7956, acc-0.7670, test loss-0.7746, acc-0.7799\n",
      "Iter-2520, train loss-0.6867, acc-0.8200, valid loss-0.7942, acc-0.7668, test loss-0.7733, acc-0.7812\n",
      "Iter-2530, train loss-0.7031, acc-0.8100, valid loss-0.7917, acc-0.7672, test loss-0.7712, acc-0.7813\n",
      "Iter-2540, train loss-0.7222, acc-0.8000, valid loss-0.7897, acc-0.7676, test loss-0.7693, acc-0.7822\n",
      "Iter-2550, train loss-0.8774, acc-0.7400, valid loss-0.7881, acc-0.7686, test loss-0.7680, acc-0.7830\n",
      "Iter-2560, train loss-0.7968, acc-0.7600, valid loss-0.7868, acc-0.7696, test loss-0.7666, acc-0.7834\n",
      "Iter-2570, train loss-0.7964, acc-0.7600, valid loss-0.7861, acc-0.7696, test loss-0.7654, acc-0.7828\n",
      "Iter-2580, train loss-0.7692, acc-0.7900, valid loss-0.7844, acc-0.7698, test loss-0.7642, acc-0.7839\n",
      "Iter-2590, train loss-0.6616, acc-0.8300, valid loss-0.7836, acc-0.7704, test loss-0.7633, acc-0.7834\n",
      "Iter-2600, train loss-0.6984, acc-0.8200, valid loss-0.7821, acc-0.7704, test loss-0.7618, acc-0.7836\n",
      "Iter-2610, train loss-0.8147, acc-0.7700, valid loss-0.7812, acc-0.7698, test loss-0.7608, acc-0.7836\n",
      "Iter-2620, train loss-0.7834, acc-0.7300, valid loss-0.7803, acc-0.7698, test loss-0.7601, acc-0.7843\n",
      "Iter-2630, train loss-0.7908, acc-0.7900, valid loss-0.7785, acc-0.7716, test loss-0.7583, acc-0.7853\n",
      "Iter-2640, train loss-0.7768, acc-0.7400, valid loss-0.7775, acc-0.7700, test loss-0.7570, acc-0.7854\n",
      "Iter-2650, train loss-0.8874, acc-0.7700, valid loss-0.7759, acc-0.7728, test loss-0.7557, acc-0.7861\n",
      "Iter-2660, train loss-0.7991, acc-0.8000, valid loss-0.7744, acc-0.7724, test loss-0.7543, acc-0.7862\n",
      "Iter-2670, train loss-0.8659, acc-0.7400, valid loss-0.7733, acc-0.7720, test loss-0.7529, acc-0.7863\n",
      "Iter-2680, train loss-0.7362, acc-0.7500, valid loss-0.7721, acc-0.7718, test loss-0.7518, acc-0.7871\n",
      "Iter-2690, train loss-0.8426, acc-0.6900, valid loss-0.7719, acc-0.7728, test loss-0.7516, acc-0.7863\n",
      "Iter-2700, train loss-0.6413, acc-0.8400, valid loss-0.7712, acc-0.7732, test loss-0.7506, acc-0.7865\n",
      "Iter-2710, train loss-0.8002, acc-0.7800, valid loss-0.7697, acc-0.7732, test loss-0.7489, acc-0.7880\n",
      "Iter-2720, train loss-0.7852, acc-0.7200, valid loss-0.7679, acc-0.7744, test loss-0.7475, acc-0.7889\n",
      "Iter-2730, train loss-0.5164, acc-0.8900, valid loss-0.7670, acc-0.7752, test loss-0.7466, acc-0.7885\n",
      "Iter-2740, train loss-0.7227, acc-0.8000, valid loss-0.7657, acc-0.7754, test loss-0.7450, acc-0.7894\n",
      "Iter-2750, train loss-0.7361, acc-0.7700, valid loss-0.7644, acc-0.7758, test loss-0.7440, acc-0.7892\n",
      "Iter-2760, train loss-0.7051, acc-0.8000, valid loss-0.7623, acc-0.7770, test loss-0.7422, acc-0.7896\n",
      "Iter-2770, train loss-0.7840, acc-0.7700, valid loss-0.7613, acc-0.7768, test loss-0.7411, acc-0.7897\n",
      "Iter-2780, train loss-0.8599, acc-0.7600, valid loss-0.7600, acc-0.7768, test loss-0.7398, acc-0.7905\n",
      "Iter-2790, train loss-0.9441, acc-0.6600, valid loss-0.7583, acc-0.7780, test loss-0.7386, acc-0.7912\n",
      "Iter-2800, train loss-0.8180, acc-0.7500, valid loss-0.7573, acc-0.7784, test loss-0.7375, acc-0.7922\n",
      "Iter-2810, train loss-0.6663, acc-0.8100, valid loss-0.7565, acc-0.7792, test loss-0.7366, acc-0.7923\n",
      "Iter-2820, train loss-0.6387, acc-0.7900, valid loss-0.7560, acc-0.7800, test loss-0.7358, acc-0.7925\n",
      "Iter-2830, train loss-0.8177, acc-0.7900, valid loss-0.7549, acc-0.7806, test loss-0.7342, acc-0.7932\n",
      "Iter-2840, train loss-0.8360, acc-0.7200, valid loss-0.7529, acc-0.7806, test loss-0.7327, acc-0.7932\n",
      "Iter-2850, train loss-0.7707, acc-0.7600, valid loss-0.7520, acc-0.7812, test loss-0.7319, acc-0.7930\n",
      "Iter-2860, train loss-0.7229, acc-0.8000, valid loss-0.7507, acc-0.7808, test loss-0.7306, acc-0.7941\n",
      "Iter-2870, train loss-0.7468, acc-0.7600, valid loss-0.7491, acc-0.7810, test loss-0.7297, acc-0.7950\n",
      "Iter-2880, train loss-0.4925, acc-0.8400, valid loss-0.7480, acc-0.7806, test loss-0.7284, acc-0.7948\n",
      "Iter-2890, train loss-0.7567, acc-0.8000, valid loss-0.7472, acc-0.7810, test loss-0.7275, acc-0.7953\n",
      "Iter-2900, train loss-0.8164, acc-0.7700, valid loss-0.7463, acc-0.7810, test loss-0.7264, acc-0.7961\n",
      "Iter-2910, train loss-0.6448, acc-0.8100, valid loss-0.7452, acc-0.7820, test loss-0.7253, acc-0.7961\n",
      "Iter-2920, train loss-0.7472, acc-0.7800, valid loss-0.7438, acc-0.7824, test loss-0.7242, acc-0.7973\n",
      "Iter-2930, train loss-0.6940, acc-0.7900, valid loss-0.7424, acc-0.7818, test loss-0.7230, acc-0.7971\n",
      "Iter-2940, train loss-0.6290, acc-0.8500, valid loss-0.7410, acc-0.7830, test loss-0.7218, acc-0.7974\n",
      "Iter-2950, train loss-0.6854, acc-0.8100, valid loss-0.7398, acc-0.7834, test loss-0.7205, acc-0.7979\n",
      "Iter-2960, train loss-0.6854, acc-0.8300, valid loss-0.7387, acc-0.7842, test loss-0.7195, acc-0.7983\n",
      "Iter-2970, train loss-0.7827, acc-0.7600, valid loss-0.7377, acc-0.7838, test loss-0.7186, acc-0.7984\n",
      "Iter-2980, train loss-0.8743, acc-0.7500, valid loss-0.7372, acc-0.7842, test loss-0.7180, acc-0.7991\n",
      "Iter-2990, train loss-0.6987, acc-0.7700, valid loss-0.7361, acc-0.7852, test loss-0.7168, acc-0.7996\n",
      "Iter-3000, train loss-0.5775, acc-0.8300, valid loss-0.7352, acc-0.7862, test loss-0.7158, acc-0.8006\n",
      "Iter-3010, train loss-0.7470, acc-0.7800, valid loss-0.7336, acc-0.7870, test loss-0.7147, acc-0.8013\n",
      "Iter-3020, train loss-0.8234, acc-0.7800, valid loss-0.7328, acc-0.7860, test loss-0.7142, acc-0.8012\n",
      "Iter-3030, train loss-0.7997, acc-0.7700, valid loss-0.7323, acc-0.7864, test loss-0.7135, acc-0.8015\n",
      "Iter-3040, train loss-0.8077, acc-0.7800, valid loss-0.7311, acc-0.7874, test loss-0.7124, acc-0.8019\n",
      "Iter-3050, train loss-0.6702, acc-0.8000, valid loss-0.7305, acc-0.7870, test loss-0.7117, acc-0.8020\n",
      "Iter-3060, train loss-0.7097, acc-0.7900, valid loss-0.7303, acc-0.7872, test loss-0.7113, acc-0.8016\n",
      "Iter-3070, train loss-0.5449, acc-0.8500, valid loss-0.7295, acc-0.7882, test loss-0.7105, acc-0.8016\n",
      "Iter-3080, train loss-0.7714, acc-0.7400, valid loss-0.7283, acc-0.7872, test loss-0.7095, acc-0.8022\n",
      "Iter-3090, train loss-0.6600, acc-0.8300, valid loss-0.7280, acc-0.7866, test loss-0.7090, acc-0.8022\n",
      "Iter-3100, train loss-0.6240, acc-0.8400, valid loss-0.7274, acc-0.7866, test loss-0.7084, acc-0.8022\n",
      "Iter-3110, train loss-0.7043, acc-0.7900, valid loss-0.7273, acc-0.7868, test loss-0.7078, acc-0.8022\n",
      "Iter-3120, train loss-0.7358, acc-0.7700, valid loss-0.7266, acc-0.7882, test loss-0.7072, acc-0.8019\n",
      "Iter-3130, train loss-0.7159, acc-0.7800, valid loss-0.7254, acc-0.7880, test loss-0.7064, acc-0.8028\n",
      "Iter-3140, train loss-0.4618, acc-0.8800, valid loss-0.7242, acc-0.7872, test loss-0.7056, acc-0.8028\n",
      "Iter-3150, train loss-0.7379, acc-0.7900, valid loss-0.7220, acc-0.7896, test loss-0.7039, acc-0.8040\n",
      "Iter-3160, train loss-0.6322, acc-0.8600, valid loss-0.7216, acc-0.7876, test loss-0.7029, acc-0.8036\n",
      "Iter-3170, train loss-0.7953, acc-0.7900, valid loss-0.7212, acc-0.7884, test loss-0.7028, acc-0.8036\n",
      "Iter-3180, train loss-0.8277, acc-0.7200, valid loss-0.7196, acc-0.7900, test loss-0.7019, acc-0.8038\n",
      "Iter-3190, train loss-0.6282, acc-0.8100, valid loss-0.7188, acc-0.7896, test loss-0.7011, acc-0.8038\n",
      "Iter-3200, train loss-0.6460, acc-0.8100, valid loss-0.7181, acc-0.7894, test loss-0.6998, acc-0.8041\n",
      "Iter-3210, train loss-0.5514, acc-0.8800, valid loss-0.7178, acc-0.7894, test loss-0.6994, acc-0.8041\n",
      "Iter-3220, train loss-0.7823, acc-0.7600, valid loss-0.7169, acc-0.7908, test loss-0.6985, acc-0.8040\n",
      "Iter-3230, train loss-0.6349, acc-0.7900, valid loss-0.7160, acc-0.7918, test loss-0.6978, acc-0.8038\n",
      "Iter-3240, train loss-0.6061, acc-0.8100, valid loss-0.7152, acc-0.7910, test loss-0.6968, acc-0.8043\n",
      "Iter-3250, train loss-0.7573, acc-0.7500, valid loss-0.7142, acc-0.7918, test loss-0.6960, acc-0.8045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.5466, acc-0.8900, valid loss-0.7134, acc-0.7922, test loss-0.6953, acc-0.8048\n",
      "Iter-3270, train loss-0.7672, acc-0.7900, valid loss-0.7127, acc-0.7920, test loss-0.6946, acc-0.8059\n",
      "Iter-3280, train loss-0.7847, acc-0.7900, valid loss-0.7114, acc-0.7924, test loss-0.6936, acc-0.8060\n",
      "Iter-3290, train loss-0.6533, acc-0.8100, valid loss-0.7104, acc-0.7948, test loss-0.6933, acc-0.8061\n",
      "Iter-3300, train loss-0.8254, acc-0.7400, valid loss-0.7095, acc-0.7954, test loss-0.6929, acc-0.8062\n",
      "Iter-3310, train loss-0.7551, acc-0.7700, valid loss-0.7090, acc-0.7950, test loss-0.6925, acc-0.8064\n",
      "Iter-3320, train loss-0.8610, acc-0.7000, valid loss-0.7080, acc-0.7942, test loss-0.6916, acc-0.8067\n",
      "Iter-3330, train loss-0.7641, acc-0.7900, valid loss-0.7070, acc-0.7940, test loss-0.6905, acc-0.8071\n",
      "Iter-3340, train loss-0.7993, acc-0.7800, valid loss-0.7061, acc-0.7954, test loss-0.6895, acc-0.8079\n",
      "Iter-3350, train loss-0.6051, acc-0.8500, valid loss-0.7053, acc-0.7962, test loss-0.6887, acc-0.8081\n",
      "Iter-3360, train loss-0.6872, acc-0.8100, valid loss-0.7044, acc-0.7962, test loss-0.6879, acc-0.8088\n",
      "Iter-3370, train loss-0.5208, acc-0.8900, valid loss-0.7036, acc-0.7962, test loss-0.6868, acc-0.8087\n",
      "Iter-3380, train loss-0.7681, acc-0.7800, valid loss-0.7025, acc-0.7972, test loss-0.6857, acc-0.8090\n",
      "Iter-3390, train loss-0.6677, acc-0.8000, valid loss-0.7011, acc-0.7974, test loss-0.6849, acc-0.8096\n",
      "Iter-3400, train loss-0.4850, acc-0.8700, valid loss-0.7010, acc-0.7974, test loss-0.6843, acc-0.8090\n",
      "Iter-3410, train loss-0.6522, acc-0.8000, valid loss-0.7001, acc-0.7982, test loss-0.6836, acc-0.8096\n",
      "Iter-3420, train loss-0.6864, acc-0.7600, valid loss-0.6987, acc-0.7998, test loss-0.6824, acc-0.8096\n",
      "Iter-3430, train loss-0.7420, acc-0.7600, valid loss-0.6984, acc-0.7988, test loss-0.6820, acc-0.8097\n",
      "Iter-3440, train loss-0.7329, acc-0.7900, valid loss-0.6973, acc-0.8000, test loss-0.6810, acc-0.8101\n",
      "Iter-3450, train loss-0.7186, acc-0.8000, valid loss-0.6963, acc-0.7986, test loss-0.6803, acc-0.8103\n",
      "Iter-3460, train loss-0.6112, acc-0.8000, valid loss-0.6950, acc-0.7998, test loss-0.6789, acc-0.8106\n",
      "Iter-3470, train loss-0.6787, acc-0.7900, valid loss-0.6944, acc-0.8006, test loss-0.6782, acc-0.8111\n",
      "Iter-3480, train loss-0.7736, acc-0.7300, valid loss-0.6936, acc-0.8008, test loss-0.6773, acc-0.8110\n",
      "Iter-3490, train loss-0.7851, acc-0.7400, valid loss-0.6929, acc-0.8010, test loss-0.6768, acc-0.8115\n",
      "Iter-3500, train loss-0.6406, acc-0.8300, valid loss-0.6926, acc-0.8016, test loss-0.6763, acc-0.8119\n",
      "Iter-3510, train loss-0.6344, acc-0.8500, valid loss-0.6923, acc-0.8008, test loss-0.6760, acc-0.8118\n",
      "Iter-3520, train loss-0.7027, acc-0.8400, valid loss-0.6916, acc-0.8004, test loss-0.6753, acc-0.8120\n",
      "Iter-3530, train loss-0.7655, acc-0.7600, valid loss-0.6907, acc-0.7998, test loss-0.6743, acc-0.8115\n",
      "Iter-3540, train loss-0.7036, acc-0.7900, valid loss-0.6900, acc-0.8002, test loss-0.6737, acc-0.8116\n",
      "Iter-3550, train loss-0.6759, acc-0.7500, valid loss-0.6895, acc-0.8006, test loss-0.6729, acc-0.8112\n",
      "Iter-3560, train loss-0.7012, acc-0.7800, valid loss-0.6883, acc-0.8012, test loss-0.6721, acc-0.8115\n",
      "Iter-3570, train loss-0.6627, acc-0.8300, valid loss-0.6873, acc-0.8014, test loss-0.6712, acc-0.8125\n",
      "Iter-3580, train loss-0.5977, acc-0.8100, valid loss-0.6867, acc-0.8024, test loss-0.6706, acc-0.8117\n",
      "Iter-3590, train loss-0.7606, acc-0.8100, valid loss-0.6866, acc-0.8022, test loss-0.6703, acc-0.8120\n",
      "Iter-3600, train loss-0.6858, acc-0.8100, valid loss-0.6852, acc-0.8026, test loss-0.6691, acc-0.8121\n",
      "Iter-3610, train loss-0.7037, acc-0.8100, valid loss-0.6844, acc-0.8030, test loss-0.6683, acc-0.8127\n",
      "Iter-3620, train loss-0.6868, acc-0.7900, valid loss-0.6836, acc-0.8036, test loss-0.6677, acc-0.8130\n",
      "Iter-3630, train loss-0.7741, acc-0.7700, valid loss-0.6826, acc-0.8044, test loss-0.6671, acc-0.8130\n",
      "Iter-3640, train loss-0.8783, acc-0.7700, valid loss-0.6813, acc-0.8046, test loss-0.6663, acc-0.8143\n",
      "Iter-3650, train loss-0.6552, acc-0.8200, valid loss-0.6803, acc-0.8052, test loss-0.6653, acc-0.8147\n",
      "Iter-3660, train loss-0.7681, acc-0.7600, valid loss-0.6796, acc-0.8060, test loss-0.6646, acc-0.8151\n",
      "Iter-3670, train loss-0.6282, acc-0.8600, valid loss-0.6799, acc-0.8052, test loss-0.6645, acc-0.8152\n",
      "Iter-3680, train loss-0.6920, acc-0.7800, valid loss-0.6795, acc-0.8048, test loss-0.6638, acc-0.8145\n",
      "Iter-3690, train loss-0.6381, acc-0.8300, valid loss-0.6795, acc-0.8052, test loss-0.6637, acc-0.8137\n",
      "Iter-3700, train loss-0.8088, acc-0.7400, valid loss-0.6791, acc-0.8052, test loss-0.6633, acc-0.8134\n",
      "Iter-3710, train loss-0.6927, acc-0.8200, valid loss-0.6787, acc-0.8050, test loss-0.6626, acc-0.8133\n",
      "Iter-3720, train loss-1.0511, acc-0.7200, valid loss-0.6774, acc-0.8066, test loss-0.6620, acc-0.8142\n",
      "Iter-3730, train loss-0.6239, acc-0.8400, valid loss-0.6770, acc-0.8064, test loss-0.6614, acc-0.8138\n",
      "Iter-3740, train loss-0.5166, acc-0.8400, valid loss-0.6765, acc-0.8064, test loss-0.6610, acc-0.8139\n",
      "Iter-3750, train loss-0.7095, acc-0.8100, valid loss-0.6765, acc-0.8060, test loss-0.6606, acc-0.8141\n",
      "Iter-3760, train loss-0.7374, acc-0.7900, valid loss-0.6763, acc-0.8062, test loss-0.6605, acc-0.8137\n",
      "Iter-3770, train loss-0.6637, acc-0.8100, valid loss-0.6755, acc-0.8070, test loss-0.6596, acc-0.8139\n",
      "Iter-3780, train loss-0.6539, acc-0.8100, valid loss-0.6749, acc-0.8074, test loss-0.6594, acc-0.8139\n",
      "Iter-3790, train loss-0.6257, acc-0.8600, valid loss-0.6742, acc-0.8084, test loss-0.6585, acc-0.8146\n",
      "Iter-3800, train loss-0.7222, acc-0.7900, valid loss-0.6744, acc-0.8080, test loss-0.6587, acc-0.8148\n",
      "Iter-3810, train loss-0.6292, acc-0.8300, valid loss-0.6740, acc-0.8086, test loss-0.6580, acc-0.8150\n",
      "Iter-3820, train loss-0.6028, acc-0.8300, valid loss-0.6729, acc-0.8082, test loss-0.6569, acc-0.8150\n",
      "Iter-3830, train loss-0.8017, acc-0.7700, valid loss-0.6720, acc-0.8082, test loss-0.6564, acc-0.8151\n",
      "Iter-3840, train loss-0.6843, acc-0.7700, valid loss-0.6710, acc-0.8098, test loss-0.6554, acc-0.8160\n",
      "Iter-3850, train loss-0.8367, acc-0.7200, valid loss-0.6703, acc-0.8094, test loss-0.6546, acc-0.8160\n",
      "Iter-3860, train loss-0.7164, acc-0.8400, valid loss-0.6699, acc-0.8098, test loss-0.6541, acc-0.8173\n",
      "Iter-3870, train loss-0.8230, acc-0.8000, valid loss-0.6693, acc-0.8100, test loss-0.6535, acc-0.8170\n",
      "Iter-3880, train loss-0.6867, acc-0.8100, valid loss-0.6691, acc-0.8098, test loss-0.6529, acc-0.8170\n",
      "Iter-3890, train loss-0.5807, acc-0.8200, valid loss-0.6687, acc-0.8104, test loss-0.6526, acc-0.8172\n",
      "Iter-3900, train loss-0.7475, acc-0.7800, valid loss-0.6681, acc-0.8102, test loss-0.6521, acc-0.8173\n",
      "Iter-3910, train loss-0.6702, acc-0.7700, valid loss-0.6677, acc-0.8106, test loss-0.6516, acc-0.8174\n",
      "Iter-3920, train loss-0.5860, acc-0.7900, valid loss-0.6670, acc-0.8100, test loss-0.6511, acc-0.8173\n",
      "Iter-3930, train loss-0.9325, acc-0.7000, valid loss-0.6668, acc-0.8110, test loss-0.6509, acc-0.8176\n",
      "Iter-3940, train loss-0.7989, acc-0.7500, valid loss-0.6662, acc-0.8110, test loss-0.6505, acc-0.8185\n",
      "Iter-3950, train loss-0.6317, acc-0.8100, valid loss-0.6654, acc-0.8122, test loss-0.6499, acc-0.8187\n",
      "Iter-3960, train loss-0.6785, acc-0.8200, valid loss-0.6641, acc-0.8120, test loss-0.6486, acc-0.8189\n",
      "Iter-3970, train loss-0.7232, acc-0.7800, valid loss-0.6635, acc-0.8122, test loss-0.6482, acc-0.8193\n",
      "Iter-3980, train loss-0.8357, acc-0.7700, valid loss-0.6628, acc-0.8134, test loss-0.6475, acc-0.8199\n",
      "Iter-3990, train loss-0.7124, acc-0.7800, valid loss-0.6626, acc-0.8136, test loss-0.6467, acc-0.8195\n",
      "Iter-4000, train loss-0.6581, acc-0.8600, valid loss-0.6616, acc-0.8136, test loss-0.6457, acc-0.8202\n",
      "Iter-4010, train loss-0.6494, acc-0.8100, valid loss-0.6609, acc-0.8130, test loss-0.6454, acc-0.8198\n",
      "Iter-4020, train loss-0.6549, acc-0.8000, valid loss-0.6603, acc-0.8136, test loss-0.6450, acc-0.8193\n",
      "Iter-4030, train loss-0.6149, acc-0.8000, valid loss-0.6600, acc-0.8140, test loss-0.6447, acc-0.8192\n",
      "Iter-4040, train loss-0.7518, acc-0.8100, valid loss-0.6600, acc-0.8150, test loss-0.6443, acc-0.8193\n",
      "Iter-4050, train loss-0.6426, acc-0.8300, valid loss-0.6599, acc-0.8146, test loss-0.6444, acc-0.8187\n",
      "Iter-4060, train loss-0.7420, acc-0.8000, valid loss-0.6595, acc-0.8146, test loss-0.6440, acc-0.8193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.6382, acc-0.8000, valid loss-0.6586, acc-0.8150, test loss-0.6438, acc-0.8191\n",
      "Iter-4080, train loss-0.6578, acc-0.7900, valid loss-0.6589, acc-0.8138, test loss-0.6438, acc-0.8195\n",
      "Iter-4090, train loss-0.7508, acc-0.7700, valid loss-0.6584, acc-0.8144, test loss-0.6433, acc-0.8191\n",
      "Iter-4100, train loss-0.5577, acc-0.8500, valid loss-0.6578, acc-0.8152, test loss-0.6427, acc-0.8200\n",
      "Iter-4110, train loss-0.5437, acc-0.8500, valid loss-0.6573, acc-0.8148, test loss-0.6421, acc-0.8203\n",
      "Iter-4120, train loss-0.6405, acc-0.7800, valid loss-0.6574, acc-0.8160, test loss-0.6421, acc-0.8197\n",
      "Iter-4130, train loss-0.7036, acc-0.7800, valid loss-0.6568, acc-0.8148, test loss-0.6417, acc-0.8203\n",
      "Iter-4140, train loss-0.5719, acc-0.8100, valid loss-0.6562, acc-0.8164, test loss-0.6414, acc-0.8202\n",
      "Iter-4150, train loss-0.7200, acc-0.8100, valid loss-0.6561, acc-0.8162, test loss-0.6409, acc-0.8205\n",
      "Iter-4160, train loss-0.7373, acc-0.8000, valid loss-0.6560, acc-0.8168, test loss-0.6407, acc-0.8197\n",
      "Iter-4170, train loss-0.4896, acc-0.9000, valid loss-0.6557, acc-0.8176, test loss-0.6405, acc-0.8194\n",
      "Iter-4180, train loss-0.7419, acc-0.8000, valid loss-0.6550, acc-0.8170, test loss-0.6400, acc-0.8201\n",
      "Iter-4190, train loss-0.5481, acc-0.8400, valid loss-0.6551, acc-0.8170, test loss-0.6398, acc-0.8200\n",
      "Iter-4200, train loss-0.7945, acc-0.7800, valid loss-0.6538, acc-0.8164, test loss-0.6388, acc-0.8212\n",
      "Iter-4210, train loss-0.6665, acc-0.8400, valid loss-0.6541, acc-0.8166, test loss-0.6389, acc-0.8211\n",
      "Iter-4220, train loss-0.7688, acc-0.7300, valid loss-0.6536, acc-0.8168, test loss-0.6384, acc-0.8212\n",
      "Iter-4230, train loss-0.5822, acc-0.8300, valid loss-0.6532, acc-0.8172, test loss-0.6379, acc-0.8208\n",
      "Iter-4240, train loss-0.6025, acc-0.8300, valid loss-0.6524, acc-0.8172, test loss-0.6372, acc-0.8215\n",
      "Iter-4250, train loss-0.7081, acc-0.8200, valid loss-0.6516, acc-0.8180, test loss-0.6369, acc-0.8227\n",
      "Iter-4260, train loss-0.6180, acc-0.8100, valid loss-0.6508, acc-0.8178, test loss-0.6360, acc-0.8217\n",
      "Iter-4270, train loss-0.5949, acc-0.8200, valid loss-0.6503, acc-0.8186, test loss-0.6353, acc-0.8224\n",
      "Iter-4280, train loss-0.6499, acc-0.8200, valid loss-0.6506, acc-0.8176, test loss-0.6351, acc-0.8227\n",
      "Iter-4290, train loss-0.6741, acc-0.8000, valid loss-0.6505, acc-0.8182, test loss-0.6348, acc-0.8223\n",
      "Iter-4300, train loss-0.6055, acc-0.8300, valid loss-0.6500, acc-0.8178, test loss-0.6342, acc-0.8227\n",
      "Iter-4310, train loss-0.7292, acc-0.7800, valid loss-0.6497, acc-0.8174, test loss-0.6341, acc-0.8231\n",
      "Iter-4320, train loss-0.7125, acc-0.8000, valid loss-0.6487, acc-0.8182, test loss-0.6333, acc-0.8227\n",
      "Iter-4330, train loss-0.7069, acc-0.8100, valid loss-0.6491, acc-0.8174, test loss-0.6332, acc-0.8231\n",
      "Iter-4340, train loss-0.4888, acc-0.8600, valid loss-0.6489, acc-0.8180, test loss-0.6331, acc-0.8227\n",
      "Iter-4350, train loss-0.7535, acc-0.7700, valid loss-0.6484, acc-0.8174, test loss-0.6327, acc-0.8232\n",
      "Iter-4360, train loss-0.6713, acc-0.7800, valid loss-0.6476, acc-0.8176, test loss-0.6322, acc-0.8231\n",
      "Iter-4370, train loss-0.6558, acc-0.8400, valid loss-0.6470, acc-0.8180, test loss-0.6313, acc-0.8235\n",
      "Iter-4380, train loss-0.9138, acc-0.7300, valid loss-0.6464, acc-0.8182, test loss-0.6305, acc-0.8235\n",
      "Iter-4390, train loss-0.8167, acc-0.7700, valid loss-0.6455, acc-0.8186, test loss-0.6298, acc-0.8245\n",
      "Iter-4400, train loss-0.5987, acc-0.8400, valid loss-0.6448, acc-0.8174, test loss-0.6292, acc-0.8249\n",
      "Iter-4410, train loss-0.5139, acc-0.8700, valid loss-0.6448, acc-0.8184, test loss-0.6291, acc-0.8251\n",
      "Iter-4420, train loss-0.6917, acc-0.8200, valid loss-0.6447, acc-0.8182, test loss-0.6288, acc-0.8258\n",
      "Iter-4430, train loss-0.6680, acc-0.8200, valid loss-0.6434, acc-0.8180, test loss-0.6279, acc-0.8255\n",
      "Iter-4440, train loss-0.7601, acc-0.7700, valid loss-0.6427, acc-0.8182, test loss-0.6271, acc-0.8262\n",
      "Iter-4450, train loss-0.7271, acc-0.8000, valid loss-0.6418, acc-0.8186, test loss-0.6264, acc-0.8271\n",
      "Iter-4460, train loss-0.5776, acc-0.8400, valid loss-0.6416, acc-0.8186, test loss-0.6262, acc-0.8266\n",
      "Iter-4470, train loss-0.5757, acc-0.8400, valid loss-0.6408, acc-0.8188, test loss-0.6256, acc-0.8270\n",
      "Iter-4480, train loss-0.6153, acc-0.8000, valid loss-0.6407, acc-0.8180, test loss-0.6254, acc-0.8270\n",
      "Iter-4490, train loss-0.5549, acc-0.8300, valid loss-0.6401, acc-0.8180, test loss-0.6248, acc-0.8274\n",
      "Iter-4500, train loss-0.7121, acc-0.7700, valid loss-0.6394, acc-0.8176, test loss-0.6240, acc-0.8278\n",
      "Iter-4510, train loss-0.5754, acc-0.8300, valid loss-0.6389, acc-0.8180, test loss-0.6235, acc-0.8273\n",
      "Iter-4520, train loss-0.6514, acc-0.8400, valid loss-0.6387, acc-0.8182, test loss-0.6229, acc-0.8279\n",
      "Iter-4530, train loss-0.6664, acc-0.7900, valid loss-0.6388, acc-0.8178, test loss-0.6227, acc-0.8277\n",
      "Iter-4540, train loss-0.4597, acc-0.8700, valid loss-0.6388, acc-0.8188, test loss-0.6226, acc-0.8278\n",
      "Iter-4550, train loss-0.6206, acc-0.8200, valid loss-0.6375, acc-0.8186, test loss-0.6218, acc-0.8282\n",
      "Iter-4560, train loss-0.6747, acc-0.8100, valid loss-0.6372, acc-0.8190, test loss-0.6215, acc-0.8280\n",
      "Iter-4570, train loss-0.6727, acc-0.7800, valid loss-0.6370, acc-0.8192, test loss-0.6211, acc-0.8288\n",
      "Iter-4580, train loss-0.6459, acc-0.8300, valid loss-0.6367, acc-0.8188, test loss-0.6207, acc-0.8284\n",
      "Iter-4590, train loss-0.6685, acc-0.7700, valid loss-0.6363, acc-0.8188, test loss-0.6203, acc-0.8277\n",
      "Iter-4600, train loss-0.7062, acc-0.7300, valid loss-0.6355, acc-0.8198, test loss-0.6197, acc-0.8280\n",
      "Iter-4610, train loss-0.6898, acc-0.8200, valid loss-0.6354, acc-0.8194, test loss-0.6193, acc-0.8280\n",
      "Iter-4620, train loss-0.6389, acc-0.7800, valid loss-0.6354, acc-0.8194, test loss-0.6193, acc-0.8282\n",
      "Iter-4630, train loss-0.6289, acc-0.8000, valid loss-0.6352, acc-0.8192, test loss-0.6190, acc-0.8281\n",
      "Iter-4640, train loss-0.7153, acc-0.7500, valid loss-0.6344, acc-0.8194, test loss-0.6187, acc-0.8285\n",
      "Iter-4650, train loss-0.8212, acc-0.7600, valid loss-0.6336, acc-0.8202, test loss-0.6180, acc-0.8298\n",
      "Iter-4660, train loss-0.4894, acc-0.8700, valid loss-0.6330, acc-0.8196, test loss-0.6175, acc-0.8287\n",
      "Iter-4670, train loss-0.5078, acc-0.8600, valid loss-0.6328, acc-0.8200, test loss-0.6170, acc-0.8302\n",
      "Iter-4680, train loss-0.7858, acc-0.7400, valid loss-0.6328, acc-0.8192, test loss-0.6170, acc-0.8299\n",
      "Iter-4690, train loss-0.9286, acc-0.7900, valid loss-0.6324, acc-0.8204, test loss-0.6169, acc-0.8300\n",
      "Iter-4700, train loss-0.6330, acc-0.8100, valid loss-0.6317, acc-0.8196, test loss-0.6164, acc-0.8309\n",
      "Iter-4710, train loss-0.7591, acc-0.7800, valid loss-0.6307, acc-0.8194, test loss-0.6159, acc-0.8312\n",
      "Iter-4720, train loss-0.5135, acc-0.8400, valid loss-0.6301, acc-0.8206, test loss-0.6155, acc-0.8303\n",
      "Iter-4730, train loss-0.8377, acc-0.7900, valid loss-0.6298, acc-0.8212, test loss-0.6153, acc-0.8312\n",
      "Iter-4740, train loss-0.7128, acc-0.8200, valid loss-0.6292, acc-0.8222, test loss-0.6150, acc-0.8317\n",
      "Iter-4750, train loss-0.5607, acc-0.8400, valid loss-0.6286, acc-0.8228, test loss-0.6146, acc-0.8325\n",
      "Iter-4760, train loss-0.8430, acc-0.7600, valid loss-0.6288, acc-0.8228, test loss-0.6144, acc-0.8318\n",
      "Iter-4770, train loss-0.4718, acc-0.8300, valid loss-0.6283, acc-0.8220, test loss-0.6139, acc-0.8320\n",
      "Iter-4780, train loss-0.5166, acc-0.8300, valid loss-0.6282, acc-0.8222, test loss-0.6136, acc-0.8313\n",
      "Iter-4790, train loss-0.5822, acc-0.8300, valid loss-0.6275, acc-0.8224, test loss-0.6129, acc-0.8313\n",
      "Iter-4800, train loss-0.5776, acc-0.8400, valid loss-0.6273, acc-0.8224, test loss-0.6128, acc-0.8313\n",
      "Iter-4810, train loss-0.6519, acc-0.7900, valid loss-0.6273, acc-0.8220, test loss-0.6126, acc-0.8312\n",
      "Iter-4820, train loss-0.5880, acc-0.8300, valid loss-0.6269, acc-0.8216, test loss-0.6122, acc-0.8318\n",
      "Iter-4830, train loss-0.6845, acc-0.8200, valid loss-0.6267, acc-0.8224, test loss-0.6121, acc-0.8324\n",
      "Iter-4840, train loss-0.7297, acc-0.7500, valid loss-0.6270, acc-0.8226, test loss-0.6119, acc-0.8315\n",
      "Iter-4850, train loss-0.5572, acc-0.8400, valid loss-0.6262, acc-0.8224, test loss-0.6112, acc-0.8323\n",
      "Iter-4860, train loss-0.4976, acc-0.8600, valid loss-0.6254, acc-0.8222, test loss-0.6108, acc-0.8324\n",
      "Iter-4870, train loss-0.6611, acc-0.7500, valid loss-0.6249, acc-0.8220, test loss-0.6106, acc-0.8319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.5577, acc-0.8400, valid loss-0.6238, acc-0.8230, test loss-0.6097, acc-0.8324\n",
      "Iter-4890, train loss-0.8801, acc-0.8200, valid loss-0.6233, acc-0.8230, test loss-0.6092, acc-0.8328\n",
      "Iter-4900, train loss-0.5265, acc-0.8100, valid loss-0.6229, acc-0.8232, test loss-0.6090, acc-0.8330\n",
      "Iter-4910, train loss-0.5877, acc-0.8000, valid loss-0.6221, acc-0.8234, test loss-0.6085, acc-0.8326\n",
      "Iter-4920, train loss-0.6592, acc-0.8200, valid loss-0.6214, acc-0.8238, test loss-0.6081, acc-0.8329\n",
      "Iter-4930, train loss-0.5959, acc-0.8300, valid loss-0.6210, acc-0.8234, test loss-0.6078, acc-0.8328\n",
      "Iter-4940, train loss-0.6920, acc-0.7800, valid loss-0.6211, acc-0.8236, test loss-0.6076, acc-0.8329\n",
      "Iter-4950, train loss-0.4507, acc-0.9100, valid loss-0.6210, acc-0.8236, test loss-0.6075, acc-0.8322\n",
      "Iter-4960, train loss-0.6269, acc-0.8500, valid loss-0.6210, acc-0.8236, test loss-0.6073, acc-0.8326\n",
      "Iter-4970, train loss-0.6787, acc-0.7800, valid loss-0.6203, acc-0.8244, test loss-0.6069, acc-0.8324\n",
      "Iter-4980, train loss-0.5614, acc-0.8300, valid loss-0.6197, acc-0.8246, test loss-0.6065, acc-0.8334\n",
      "Iter-4990, train loss-0.7126, acc-0.7900, valid loss-0.6196, acc-0.8244, test loss-0.6060, acc-0.8330\n",
      "Iter-5000, train loss-0.8035, acc-0.7600, valid loss-0.6187, acc-0.8246, test loss-0.6053, acc-0.8328\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 5000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119157a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFLliAZ9pZQqRzToK6VUOfL/ATeVolAYRgWSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119157b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
