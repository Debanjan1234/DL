{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        y, nl_cache = self.selu_forward(y)\n",
    "        y, do_cache = self.alpha_dropout_fwd(h=y, q=1.0-self.p_dropout) # q=1-p, 1=keep_prob\n",
    "        #         y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "\n",
    "        cache = (X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        #         dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dy = self.alpha_dropout_bwd(dout=dy, cache=do_cache)\n",
    "        dy = self.selu_backward(dy, nl_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "        dbh = dh * 1.0\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 training loss: 2.6361\n",
      "er 1cheLestinafe\"py co; ttKarat4Dpre In the Pea 30e\", me9uan\n",
      ". as a 1d inat annsio1tP the eaghteaturo\n",
      "Iter-260 training loss: 2.5249\n",
      "e% RunoStary po(Pua4, fiin; B2rJapan7 puJepul aleex,TSKaPeDesslrtcich 日wh MeA.ce日0tEcore cnr8ce\n",
      ". Gnc\n",
      "Iter-390 training loss: 2.5969\n",
      "er FraranRy aneslPore8t inRy cojunte 2\n",
      "5 no6 ale laoJapan, ewe of theLuro-Japao dony % of to Wur vike\n",
      "Iter-520 training loss: 2.4808\n",
      "e Horla日 the wor indd Japandd 3ded 0th thc it–overtzt can\"med liry zhe wery. heChed Lo) worleD-la5FtK\n",
      "Iter-650 training loss: 2.8038\n",
      "er9Brandec本 viche lal 4:s the Cheihen the Inch'stE the hisheatFtTd in, the Japwttare Runnondopucon6 o\n",
      "Iter-780 training loss: 2.2964\n",
      "e  or xan The AstoncederMel turies was bHl cipd 'ipby Chison ymplato the countamEacla\n",
      ".urhorellarLy 9\n",
      "Iter-910 training loss: 2.4829\n",
      "ed fchGly:ciy te mipin Pon r) of es is –jedtceaUd coterMely. in of Japis cojocinod ron aler eiawzher \n",
      "Iter-1040 training loss: 2.2424\n",
      "erporth thirdonted In(, ) TheAped, incounzer, the thirB,d pamainh tt itios worldtes,TySbeu, 2\",. The \n",
      "Iter-1170 training loss: 2.3024\n",
      "er al9, chr Fro, velea the 6Fredon本fxry woLapay glatain \n",
      "Cn, Mor, 0sy 本 u (27 tt-lal in Desed co, 本 –\n",
      "Iter-1300 training loss: 2.3161\n",
      "ernI . Japandd, lres e. Jf(\n",
      ". Japantted conn\"Tnon a wE. Japan本 in whiseporer mil日, 2Apin the was 's3d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x1105a1e48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 1300 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.05 # keep_prob=1.0-p_dropout, q=1-p, 5% to 10% noise is recommanded for p_dropout\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxiBARFTAGASIiArY/wdyiFcReooxqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fa9fb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
