{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Data: time-serie smartwatch or wristband or smartband data\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as spio\n",
    "\n",
    "# Data reading\n",
    "# Linux-Ubuntu\n",
    "data_path = '/home/arasdar/data/Training_data/DATA_01_TYPE01.mat'\n",
    "# # Macbook\n",
    "# data_path = '/Users/arasdar/data/Training_data/DATA_01_TYPE01.mat'\n",
    "watch = spio.loadmat(data_path)\n",
    "data = watch['sig']\n",
    "data = np.array(data)\n",
    "data.shape, watch['sig'].size/6\n",
    "\n",
    "# Normalizing each batch of the data, each batch = each file\n",
    "# Can we normalize them all at the same time?\n",
    "mean = np.mean(data, axis=1)\n",
    "var = np.var(data, axis=1)\n",
    "std = np.sqrt(var)\n",
    "mean.shape, var.shape, std.shape\n",
    "mean = mean.reshape(-1, 1)\n",
    "std = std.reshape(-1, 1)\n",
    "mean.shape, std.shape\n",
    "data_norm = (data - mean)/std\n",
    "data.shape, data_norm.shape\n",
    "\n",
    "# The features/ channels of the data = ['HR-ECG', 'HR-PPG', 'ACC-X', 'ACC-Y', 'ACC-Z']\n",
    "plt.plot(data[0, :1000], label='ECG')\n",
    "plt.plot(data[1, :1000], label='PPG-1')\n",
    "plt.plot(data[2, :1000], label='PPG-2')\n",
    "# plt.plot(x_sig[3, :1000], label='ACC-X')\n",
    "# plt.plot(x_sig[4, :1000], label='SCC-Y')\n",
    "# plt.plot(x_sig[5, :1000], label='ACC-Z')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# The features/ channels of the data = ['HR-ECG', 'HR-PPG', 'ACC-X', 'ACC-Y', 'ACC-Z']\n",
    "plt.plot(data_norm[0, :1000], label='ECG')\n",
    "plt.plot(data_norm[1, :1000], label='PPG-1')\n",
    "plt.plot(data_norm[2, :1000], label='PPG-2')\n",
    "# plt.plot(x_sig[3, :1000], label='ACC-X')\n",
    "# plt.plot(x_sig[4, :1000], label='SCC-Y')\n",
    "# plt.plot(x_sig[5, :1000], label='ACC-Z')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ECG\n",
    "Y = data_norm[0]\n",
    "X = data_norm[1:]\n",
    "Y.shape, X.shape\n",
    "Y = Y.reshape(1, -1)\n",
    "Y.shape, X.shape\n",
    "plt.plot(Y[0, :1000], label='HR-ECG-1: Output signal')\n",
    "plt.plot(X[0, :1000], label='HR-PPG-1: Input signal')\n",
    "plt.plot(X[1, :1000], label='HR-PPG-2: Input signal')\n",
    "plt.plot(X[2, :1000], label='ACC-X-3: Input signal')\n",
    "plt.plot(X[3, :1000], label='ACC-Y-4: Input signal')\n",
    "plt.plot(X[4, :1000], label='ACC-Z-5: Input signal')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "X_train = X.T\n",
    "Y_train = Y.T\n",
    "X_train.shape, Y_train.shape\n",
    "\n",
    "# Preparing the training data for seq2seq learning\n",
    "Y_train_in = Y_train[:-2]\n",
    "Y_train_out = Y_train[:-1]\n",
    "#     Y_train_in.shape, Y_train.shape\n",
    "firstrow = np.zeros([1, 1])\n",
    "#     firstrow.shape, Y_train_in.shape\n",
    "Y_train_in = np.row_stack((firstrow, Y_train_in))\n",
    "#     Y_train_in.shape, Y_train_in[:5], Y_train[:5]\n",
    "#     Y_train_out = Y_train.copy()\n",
    "X_train.shape, Y_train_in.shape, Y_train_out.shape\n",
    "XY_train = (X_train, Y_train)\n",
    "\n",
    "# Read and normalize one batch/file of data\n",
    "def read_data(data_path):\n",
    "    band = spio.loadmat(data_path)\n",
    "    data = band['sig']\n",
    "    data = np.array(data)\n",
    "    #     data.shape, band['sig'].size/6\n",
    "\n",
    "    # Normalizing each batch of the data, each batch = each file\n",
    "    # Can we normalize them all at the same time?\n",
    "    mean = np.mean(data, axis=1)\n",
    "    var = np.var(data, axis=1)\n",
    "    std = np.sqrt(var)\n",
    "    mean.shape, var.shape, std.shape\n",
    "    mean = mean.reshape(-1, 1)\n",
    "    std = std.reshape(-1, 1)\n",
    "    mean.shape, std.shape\n",
    "    data_norm = (data - mean)/std\n",
    "    #     print(data.shape, data_norm.shape)\n",
    "    \n",
    "    # ECG\n",
    "    Y = data_norm[0]\n",
    "    # PPG+ACC\n",
    "    X = data_norm[1:]\n",
    "    # Y.shape, X.shape\n",
    "    Y = Y.reshape(1, -1)\n",
    "    # Y.shape, X.shape\n",
    "    X_train = X.T\n",
    "    Y_train = Y.T\n",
    "    #     print(X_train.shape, Y_train.shape)\n",
    "    \n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.D = D # number of input dimensions\n",
    "        self.H = H # number of hidden units\n",
    "        self.L = L # number of hidden layers\n",
    "        self.C = C # number of output classes/dimensions\n",
    "        self.losses = {'train':[], 'train2':[], 'train3':[]} #, 'valid':[], 'test':[]\n",
    "        \n",
    "        # Input sequence model parameters\n",
    "        Z = H + D\n",
    "        params_in_seq = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(D / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        # This is the last layer in the input mode\n",
    "        params_in_seq_ = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H))\n",
    "        )\n",
    "        \n",
    "        # Output sequence model parameters\n",
    "        Z = H + C\n",
    "        params_out_seq = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, C))\n",
    "        )\n",
    "\n",
    "        # Model parameters\n",
    "        self.model = []\n",
    "        num_modes = 2 # num of modality as the source of sequences\n",
    "        for _ in range(num_modes):\n",
    "            self.model.append([])\n",
    "        \n",
    "        for _ in range(self.L-1):\n",
    "            self.model[0].append(params_in_seq)\n",
    "\n",
    "        # The last layer: self.L-1\n",
    "        self.model[0].append(params_in_seq_)\n",
    "        \n",
    "        # Number of layers for each mode\n",
    "        for _ in range(self.L):\n",
    "            self.model[1].append(params_out_seq)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # The final output\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache,\n",
    "                 hh, hh_cache, hh_tanh_cache, h, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, h, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        # The final output\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def forward_(self, X, h, m):\n",
    "        Wz, Wr, Wh = m['Wz'], m['Wr'], m['Wh']\n",
    "        bz, br, bh = m['bz'], m['br'], m['bh']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # The output\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache,\n",
    "                 hh, hh_cache, hh_tanh_cache)\n",
    "\n",
    "        return h, cache\n",
    "\n",
    "    def backward_(self, dh, cache):\n",
    "        X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache = cache\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        # The final output\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, bz=dbz, br=dbr, bh=dbh)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, XY_train, h):\n",
    "        # Adding the output layer cache\n",
    "        caches = []\n",
    "        num_modes = 2\n",
    "        for _ in range(num_modes):\n",
    "            caches.append([])\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches[0].append([])\n",
    "            caches[1].append([])\n",
    "            \n",
    "        ys = []\n",
    "        X, Y = XY_train\n",
    "        \n",
    "        # Input sequence\n",
    "        for x in X:\n",
    "            #             print(x.shape)\n",
    "            x= x.reshape(1, -1) # mat_1xn\n",
    "            #             print(x.shape)\n",
    "            \n",
    "            for layer in range(self.L-1):\n",
    "                x, h[layer], cache = self.forward(x, h[layer], self.model[0][layer])\n",
    "                caches[0][layer].append(cache)\n",
    "            \n",
    "            # The last layer without output\n",
    "            layer = self.L - 1\n",
    "            h[layer], cache = self.forward_(x, h[layer], self.model[0][layer])\n",
    "            caches[0][layer].append(cache)\n",
    "            \n",
    "        # Output sequence\n",
    "        for y in Y:\n",
    "            #             print(y.shape)\n",
    "            y= y.reshape(1, -1) # mat_1xn\n",
    "            #             print(y.shape)\n",
    "            \n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[1][layer])\n",
    "                caches[1][layer].append(cache)\n",
    "\n",
    "            # Output list\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def l2_regression(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        #         print('y_pred.shape[0]', m)\n",
    "\n",
    "        # (F(x)-y)^2: convex as X^2 or (aX-b)^2\n",
    "        data_loss = 0.5 * np.sum((y_pred - y_train)**2) / m # number of dimensions\n",
    "    \n",
    "        return data_loss\n",
    "\n",
    "    def dl2_regression(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        #         print('y_pred.shape[0]', m)\n",
    "\n",
    "        # (F(x)-y)^2: convex as X^2 or (aX-b)^2\n",
    "        dy = (y_pred - y_train)/ m # number of dimensions\n",
    "\n",
    "        return dy\n",
    "\n",
    "    def loss_function(self, y_pred, y_train):\n",
    "        loss, dys = 0.0, []\n",
    "        #         m = y_train.shape[0]\n",
    "        #         print('y_train.shape[0]', m)\n",
    "        #         print('len(y_pred):', len(y_pred))\n",
    "        #         print('np.array(y_pred).shape:', np.array(y_pred).shape)\n",
    "        #         print('y_train.shape:', y_train.shape)\n",
    "\n",
    "        for y, Y in zip(y_pred, y_train):\n",
    "            loss += self.l2_regression(y_pred=y, y_train=Y) #/ m # t or number of samples for taking the average\n",
    "            dy = self.dl2_regression(y_pred=y, y_train=Y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        num_modes = 2\n",
    "        for _ in range(num_modes):\n",
    "            grad.append([])\n",
    "            grads.append([])\n",
    "        \n",
    "        for _ in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            \n",
    "        for mode in range(num_modes):\n",
    "            for layer in range(self.L):\n",
    "                grad[mode].append({key: np.zeros_like(val) for key, val in self.model[mode][layer].items()})\n",
    "                grads[mode].append({key: np.zeros_like(val) for key, val in self.model[mode][layer].items()})\n",
    "\n",
    "        # Output sequence\n",
    "        mode = 1\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t].copy()\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[mode][layer] = self.backward(dX, dh[layer], caches[mode][layer][t])\n",
    "                for key in grad[mode][layer].keys():\n",
    "                    grads[mode][layer][key] += grad[mode][layer][key]\n",
    "\n",
    "        # Input sequence\n",
    "        mode = 0\n",
    "        for t in reversed(range(len(dys))):\n",
    "            # Output layer or last layer\n",
    "            layer = self.L-1\n",
    "            dX, dh[layer], grad[mode][layer] = self.backward_(dh[layer], caches[mode][layer][t])\n",
    "            for key in grad[mode][layer].keys():\n",
    "                grads[mode][layer][key] += grad[mode][layer][key]\n",
    "\n",
    "            # The depth and number of layers for RNN\n",
    "            for layer in reversed(range(self.L-1)):\n",
    "                dX, dh[layer], grad[mode][layer] = self.backward(dX, dh[layer], caches[mode][layer][t])\n",
    "                for k in grad[mode][layer].keys():\n",
    "                    grads[mode][layer][k] += grad[mode][layer][k]\n",
    "\n",
    "        return dX, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, Y, minibatch_size): # shuffle: this is for static data not dynamic/sequential data\n",
    "    minibatches = []\n",
    "    \n",
    "    # for i in range(start=0, stop=X.shape[0], step=minibatch_size):\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        Y_mini = Y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, Y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, alpha, mb_size, n_iter, print_after, n_files):\n",
    "    M, R = [], []\n",
    "    num_modes = 2\n",
    "    for _ in range(num_modes):\n",
    "        M.append([])\n",
    "        R.append([])\n",
    "\n",
    "    for mode in range(num_modes):\n",
    "        for layer in range(nn.L):\n",
    "            M[mode].append({k: np.zeros_like(v) for k, v in nn.model[mode][layer].items()})\n",
    "            R[mode].append({k: np.zeros_like(v) for k, v in nn.model[mode][layer].items()})\n",
    "\n",
    "    beta1 = .9\n",
    "    beta2 = .99\n",
    "    state = nn.initial_state()\n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    \n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        \n",
    "        # Read the new file, normalize it, and separate it into input and target\n",
    "        for file_num in range(1, n_files + 1):\n",
    "            \n",
    "            #             if file_num == 1: data_path = '/Users/arasdar/data/Training_data/DATA_{:02}_TYPE01.mat'.format(file_num)\n",
    "            #             else: data_path = '/Users/arasdar/data/Training_data/DATA_{:02}_TYPE02.mat'.format(file_num)\n",
    "            if file_num == 1: data_path = '/home/arasdar/data/Training_data/DATA_{:02}_TYPE01.mat'.format(file_num)\n",
    "            else: data_path = '/home/arasdar/data/Training_data/DATA_{:02}_TYPE02.mat'.format(file_num)\n",
    "            print(data_path)\n",
    "\n",
    "            # Read the mat files, normalize each batch of them, and seperate the output and input\n",
    "            X_train, Y_train = read_data(data_path=data_path)\n",
    "            #             X_train, Y_train_in = XY_train\n",
    "            #             print(X_train.shape, Y_train_in.shape, Y_train_out.shape)\n",
    "\n",
    "            # Minibatches/ Stochasticity\n",
    "            #             minibatches = get_minibatch(X_train, Y_train, mb_size, shuffle=False)\n",
    "            minibatches = get_minibatch(X=X_train, Y=Y_train, minibatch_size=mb_size) #, shuffle=False\n",
    "            #             print('The number of minibatches in a sequence for each epoch iteration: {}'.format (len(minibatches)))\n",
    "\n",
    "            # Minibatches/ Stochasticity\n",
    "            #             print('The number of minibatches in a sequence for each epoch iteration: {}'.format (len(minibatches)))\n",
    "            for idx in range(len(minibatches)):\n",
    "                \n",
    "                # Gradients\n",
    "                X_mini, Y_mini = minibatches[idx]\n",
    "                Y_mini_out = Y_mini[:-1] # mat[t, n]== mat_txn\n",
    "                Y_mini_in = np.row_stack((np.array([0.0]), Y_mini[:-2]))\n",
    "                #                 print(X_mini.shape, Y_mini.shape, Y_mini_out.shape)\n",
    "                # print(X_mini[:2], Y_mini_in[:2], Y_mini_out[:2])\n",
    "                #                 print(Y_mini_in[:4], Y_mini_out[:4])\n",
    "                #             print(nn.model[0][0]['Wz'].shape, nn.model[1][0]['Wz'].shape, nn.C, nn.D)\n",
    "                #             print(nn.model[0][0]['Wr'].shape, nn.model[1][0]['Wh'].shape, nn.C, nn.D)\n",
    "\n",
    "                XY_mini = (X_mini, Y_mini_in)\n",
    "                ys, caches = nn.train_forward(XY_mini, state)\n",
    "                loss, dys = nn.loss_function(y_train=Y_mini_out, y_pred=ys)\n",
    "                _, grads = nn.train_backward(dys, caches)\n",
    "\n",
    "                # Descend\n",
    "                for mode in range(num_modes):\n",
    "                    for layer in range(nn.L):\n",
    "                        for key in grads[mode][layer].keys(): #key, value: items\n",
    "                            M[mode][layer][key] = l.exp_running_avg(M[mode][layer][key], grads[mode][layer][key], beta1)\n",
    "                            R[mode][layer][key] = l.exp_running_avg(R[mode][layer][key], grads[mode][layer][key]**2, beta2)\n",
    "\n",
    "                            m_k_hat = M[mode][layer][key] / (1. - (beta1**(iter)))\n",
    "                            r_k_hat = R[mode][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                            nn.model[mode][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                            \n",
    "                # MiniBatches: Drawing the learning curve for loss (Error/loss curve)\n",
    "                nn.losses['train'].append(loss)\n",
    "                \n",
    "            # FullBatches: Drawing the learning curve for loss (Error/loss curve)\n",
    "            # (based on batch normalization/bn at the input layer)\n",
    "            nn.losses['train2'].append(loss)\n",
    "            \n",
    "        # Epochs: Drawing the learning curve for loss (Error/loss curve)\n",
    "        nn.losses['train3'].append(loss)\n",
    "            \n",
    "        # Print training loss\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "                \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-10 training loss: 1.5364\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-20 training loss: 0.8445\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-30 training loss: 1.2125\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-40 training loss: 1.3190\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-50 training loss: 0.5374\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-60 training loss: 0.8697\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-70 training loss: 1.0064\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-80 training loss: 0.9324\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-90 training loss: 0.9188\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "/home/arasdar/data/Training_data/DATA_01_TYPE01.mat\n",
      "Iter-100 training loss: 1.5029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x7f6ce6534be0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper parameters\n",
    "n_iter = 100 # epochs: processing speed and how much time it takes.\n",
    "n_files = 1 # dataset size: maximum number of files in dataset based on 12 subjects\n",
    "print_after = n_iter//10 # print loss of train, valid, and test\n",
    "time_step = 200 # width of the model or minibatch size\n",
    "alpha = 1/time_step # learning_rate: 1e-3=0.001 - This is set to 1/miniatch_size (mb_size) or num_time_steps\n",
    "num_hidden_units = 64 # width of the hidden layers or number of hidden units in hidden layer\n",
    "num_hidden_layers = 1 # depth or number of hidden layer\n",
    "num_input_units = X_train.shape[1] # number of input features/dimensions\n",
    "num_output_units = Y_train.shape[1]\n",
    "# X_train.shape, Y_train.shape\n",
    "\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_hidden_layers, C=num_output_units)\n",
    "\n",
    "adam_rnn(nn=net, alpha=alpha, mb_size=time_step, n_iter=n_iter, n_files=n_files, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKKp+OBV9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6ce33e62e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# % matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss2')\n",
    "# plt.plot(net.losses['train3'], label='Train loss3')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
