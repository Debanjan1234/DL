{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for layer in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache,\n",
    "            hh, hh_cache, hh_tanh_cache, h, y_cache\n",
    "        )\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, h, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for _ in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    idx = 0\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1 # -np.log(1.0 / len(set(X_train)))\n",
    "    eps = 1e-8\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000 loss: 4.2656\n",
      "constitution initionsy netectrembennal ILtlaopecooa wer chrse whtharrd'I nsege \"sta, tok thanore s.ea\n",
      "Iter-2000 loss: 2.5825\n",
      "cy, the thilationtes ftoc tes theicl akeanelician on infine opioa fortatotyInt itl mt. Nrrorearoro to\n",
      "Iter-3000 loss: 2.4785\n",
      "cted legisleuigh rcharbel an dist cht rgeanske l ie in the 1Ntapd.tin bistov hoatel b-leet laisestio \n",
      "Iter-4000 loss: 2.4513\n",
      "conomy by pun isgine n7thathe  reeot loventelat medeand Juntromy aiss paraair g ursuregs 7stht  ofres\n",
      "Iter-5000 loss: 2.4368\n",
      "constitutio ao thene lokukurrrh the Sioledt. ahe atectnte ena asld'. akea Jtger aaifony iss ma uHostr\n",
      "Iter-6000 loss: 2.4277\n",
      "c Games.\n",
      ". Stieo he who nheee plHH0bal anndy  uere of the ntearn inllocimatioven, Chg sh Co t tho Hum\n",
      "Iter-7000 loss: 2.4219\n",
      "cy, the thichegOkhas fon intelerrtpaneleanicitilariseane noHdise wtoulare sh aln,ioi pelareest oy inn\n",
      "Iter-8000 loss: 2.4178\n",
      "cy, the thicatistaitsm theeorrrdal Stn mhexahr mo ean pro,iancinflleked bisith-tal ahte urOeadWoreat \n",
      "Iter-9000 loss: 2.4144\n",
      "capital cithi pp3epor aheran thes tha alan waslerane Wpr cttetivin Cx. Jupan the efel Ocher thytoo th\n",
      "Iter-10000 loss: 2.4120\n",
      "ch includes anto lancleopgs fouu th lalatas co the fomy sevire torrse wo  Sest etetto ants. Jneec ron\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067e8f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
