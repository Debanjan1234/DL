{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    # for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 200)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 233.4436\n",
      " iskeng Aanded s.Wost Mtex, counthi iskofs Rene an guse A0. In cuuntedenlivensmesh thI the hhe Gs Niarkixn wrountrat 2A,extt-Neark Indt if theh cansex,uy wist is hand boben Cany countr-Indr omealdinc A\n",
      "Iter-2 loss: 209.1456\n",
      " in pope. Holi olilale d, wirst In Jadr Inde Gun\"lard Alatitac in of God tof tirs hankes esxNiver the of the Rn the Wrport6an witr Jaan and ine whin the G0tir the the pKoetitif thy,-–anbas lor larse G9\n",
      "Iter-3 loss: 223.8013\n",
      " TSexitire Wores, Index, eac of G9I5I'c andedgita, ofha nlipe popelithe ciunt,os bard6deds Richurlagy in Afecpa eara lipas of Itimliwest ins an the dm-lar-andy in country i xtith the nikonby is the D a\n",
      "Iter-4 loss: 210.0010\n",
      " in Cargente frh savst inf oxfkonstf imxth-and and in 194llan Ditsenan is forlspen the world'ns in porertest-ned bors and Ind any itoly saulese dar\"nx. Japan a id the siHonks in ceninsry the Glaremo an\n",
      "Iter-5 loss: 199.1514\n",
      " in the latakence siumbae eaced foxts. Sisanres of ishkictolatary country in the mowin in is a sevelsing oudidikoly.,s rexzx G975esnos the coxtnti. stesily GB Japan is in the wountidexd ransed Aaslas d\n",
      "Iter-6 loss: 185.8653\n",
      " the pean-larly norld, and fountry ing coxtand, whins the fourth-lamba-mcentinc and ran), first in elente the ratand Divhons Rivened Index,-Fysed ranked fountry ns au diveaned fir rudons rexpirrt ted i\n",
      "Iter-7 loss: 182.2190\n",
      " militan Durchy of the ceatexpen toxtand fiorty7 prenxturesede ons in the has ran3 Douban panked Asirey wirs en the vomtir led the n cuxin the Gobattliens of Divenelirass milinaulary winst the first re\n",
      "Iter-8 loss: 182.3224\n",
      " livins matertan8, country in Asia. Japan is bamginst momal in Gishissrgede e mance drind diveansimimuntr Comolech fram Woctakeves of norloge n milronlalceloban pounsaurens rurerd Japan Apparchallitary\n",
      "Iter-9 loss: 177.1178\n",
      " 7irst Japan h ranked lareLeecenf is ranked pextoe poerankank offexpin\"d canked whect milin in the wast in the fourth-largest enpintimcur enjors leco foulsto decsly. JEpan 394, macenly ussilation in th\n",
      "Iter-10 loss: 169.5057\n",
      " melict of Nobel laureaty. Athincs mepper of Neajed the warl Gile prenal iEnauns enjo fed largest in the name histoude dede, inted issland Sexand Diushiuns a redented lagestirst-the 1orld. Inthe hegal \n",
      "Iter-11 loss: 165.3855\n",
      " Wion enti. allowing aree nimi, Sta, Khompotert the Rumban oth into parter and eserent citse War Inded iny Emperood in the north tary gent enions revesond rukened Asiexand ann Diuse nas forlts and Wous\n",
      "Iter-12 loss: 158.6270\n",
      " and eimildisurd und ranked first in the Fivhevicolyisy in the the namition of in9tEry te nte the nasing's legirecthed strend Asiationan it inth largest metod leconbincruree widepd of norloby. Japan is\n",
      "Iter-13 loss: 141.2086\n",
      " which inta in tuke toly. Urcom thossllthirigon sy bhe d-membin tokyd an nalictir ended Japan Appritistion to devmelepern. ifflankenopetions hego rand Fisidd cofkof coulrdind Asia. Japan is ofrin the G\n",
      "Iter-14 loss: 140.3010\n",
      " and riEse-Japaned and the world, and wistur eigh and Japan wimpirud af Noband itlly. AGpurif WHvead, ind enfinst enring jiment Index, renchich ined Arenging recghins runhird legonss mintry ore folke i\n",
      "Iter-15 loss: 129.2089\n",
      " Hidhgiraly itdan,s minard Ifdextern Eurthy, ChinoP Nand parceceldimfolechenked sexpdirse Word aly bousts inm parino Sino-Japanese-Japan is the world, Japan the numbte eirtyly by oun, the Global Compet\n",
      "Iter-16 loss: 121.9020\n",
      " Chinetered issimas aru ha, island Hunst-mperched siuntrity in the holy ny rests arefins opI, simionikarly 20th century, which camge a military uso-本lo-molctury, whe cound-lorejinchud yud histoolan con\n",
      "Iter-17 loss: 114.4658\n",
      " and ranked Asian country in the Global Peace Index. The the etrounty nory Arhing 1omet liest of the north the Sea of Country in Asia. Japan is ranked sixth in the G20 and is the high ststaononal srien\n",
      "Iter-18 loss: 104.2300\n",
      " mekoulrelese and parececpollictly into 47 prente and et eirst in the ear rundich imally 20th in the Global Competitivenessured its ranked first thi of 1937 expanittind 9reainly . In thicen an emped su\n",
      "Iter-19 loss: 97.5414\n",
      " the Migid into8 poptiny Inded and an Emperor and fourth-largest impornt citera in the Global Peace Index. Japan was the first in the nountry hoby lically. Atho 4ur, maind \"\"7emct d imy is a Seato 19th\n",
      "Iter-20 loss: 92.1599\n",
      " historyo,ts an nuggth blled of Jadd, durdedidedisired. Toved in himainka of 126 am e of Japan, entt ind Hrlasu, in ounityoly. Arege prececoncthy,, sevoren world, and rustired laseroves in thi eycollow\n",
      "Iter-21 loss: 89.7462\n",
      " Upirs insian followed Japant west. wher in to open to expectancy, the third lowest infant mortality in the 12 and minatd idghimsim. pricelatury Sta the Russo-Japanese War a UpirliA and rureaterly 20tt\n",
      "Iter-22 loss: 84.8745\n",
      " and peanese World War IImainas a mend milcthir ear largest military budget, used stratod and early fort of 1shity. stanombarchioichigitimperiod of world, algol of the world War II emconfid bomoborch i\n",
      "Iter-23 loss: 78.5888\n",
      " million is the wholth largest. sivupded country in Chinese and peacity Sino-Japaniny with arena, Archa in the Glmore In5endexts nime prowed washighst interean whir constit of Hirchedelicing is the hic\n",
      "Iter-24 loss: 78.0829\n",
      " mainmy bou tha, surchased largest military bud War, the Global Peace Inwe callicter a uging f ofked which carly 20th ceapitivinald into eave Japat Cthima, souitary with a high standard of living of 19\n",
      "Iter-25 loss: 70.1550\n",
      " Japan was peror largest urban manji to te decade s fromter nme rinotion isa, iscanaly 2\"0loxttlec Japanese peangest ippont to thin nd is thid as meeptitice mectterse rad early 20th centicingk ureencen\n",
      "Iter-26 loss: 65.7492\n",
      " the world's eight lmeee preconctury, wath largest uraba the G7, hi1 Sin. Japanas ransins follJapan's largest ercted ledese fice CiranI iseds enfanke in the early 20th centuries, m9tit of anded and Wou\n",
      "Iter-27 loss: 62.6106\n",
      " and ranked first in the Country Brand Index, ranked sixth in the Global Competut and the watinsme of In clareang is gountry Brand Sea of Japan, whin callan\", is and an. Horladgred ciuest eronfeme ry w\n",
      "Iter-28 loss: 64.0189\n",
      " of 126 million 947, the Global Peaciticsnampiricallical citst-ransk in the Global Competitivenesuncto 7unst-largest impotity and the wational mond allectury, whipoth in the Global Competitiveness Repo\n",
      "Iter-29 loss: 58.8184\n",
      " Asia. Japan is ryke Ind area. The power and Index, sexpoftoly ompe Wir Nuled and hase Risid Japanefes enterend is chasing frolthu, Nation eno'id lobal Purorly with military budd atd in 1945 following \n",
      "Iter-30 loss: 54.5946\n",
      " the world's fourth-largest exportert We tha to dishiguldrby . Frin the voketerld I unalion peror. The Second rinkins hich In'tal laicand country Bran s Woeant the as and is contiin 1945 foustorypin th\n",
      "Iter-31 loss: 53.9001\n",
      " the Mions of Nobel the War of 1294,t en: ear, the Global Ga, the world's eighth largest military budgetpen timin O Fivilicrtlutere centy Index whor glomertir Japan's a Sea, histoulational the Empire b\n",
      "Iter-32 loss: 49.6333\n",
      " in the Global Competat and pokataes offwofealpined ixp9est ciflurese Negollity-alg power .T0rly hixte wath thin c, of the Sea of Japan, which is a dicalles eppol in the G20 and is the UN, the G7, the \n",
      "Iter-33 loss: 51.1377\n",
      " Japan was proclaimed, ma the Nasalitaina sa-ded into part of World The G7, the Gorlat Glarly at mallion in the early 12th end otrod of isolalitasing military with the was ruled by succ, of cauntry of \n",
      "Iter-34 loss: 48.1831\n",
      " is in the G8Pa an is a developed country with a high stanca, is laun entan ar eicinl\" canunccounts Nudiding menounforld Cima, shamatud-Japanitary gislangr Index, ranked sixth in the Global Competitive\n",
      "Iter-35 loss: 52.2750\n",
      " country in the Global Peace Index. Japan was the first country in Asia to host both the Summeepincnged biusto resea, powrounts flool of thal the world, and ranked first in the Country Brand Index, ran\n",
      "Iter-36 loss: 45.0033\n",
      " Nopen a Uictidirgostt in Neutent in the Count ed cost of the nation. In the fourth-largest importer and fourth-largest importer. Although Japan has officilary und as early as the world's eighth larges\n",
      "Iter-37 loss: 44.3464\n",
      " whitudy and rased indean and ranked firght ficlanlyo country in Asia. Japan is ranked first in the Country Brand Index, ranked sixth in the Global Competitiveness Report 2015–2016 and is the highest-r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-38 loss: 41.5855\n",
      " has cofts itstitolal the loustvitg 20rlath the ntomic bed mowis ot sndexth largest minarasto of 1937 expanded intomit the world, and ranked first in the Country Brand Index, ranked sixth in the Global\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 300 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
