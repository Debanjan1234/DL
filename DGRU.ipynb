{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for layer in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    idx = 0\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1 # -np.log(1.0 / len(set(X_train)))\n",
    "    eps = 1e-8\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 80.8186\n",
      "enia FhlleMof \"unor es ryrty RAroury. Japintolrel bosth a3 coan Okexcin corled Han bohe iNthita mand \n",
      "Iter-20 loss: 69.5554\n",
      "exof pars im, Woped lar, Japen's Warury 1855 and in the Sed byrsoukou, the fitrtopa, fysh aed pace ds\n",
      "Iter-30 loss: 60.0279\n",
      "econa Napand Hofillifito intth ended Ginbal the Narg croc was the wored m7in tha Japan, 47, aigedingo\n",
      "Iter-40 loss: 50.9462\n",
      "e the Rishat of Wortgerit opld and. \"Tites rinsy cinsta. Japane\"d asklallolyt Chine, an olitition eno\n",
      "Iter-50 loss: 44.6376\n",
      "erlante corat Ching Kortholy bygs as lounstarly, the Indevcomomally-seceoat of G本, Japan end Naipa, K\n",
      "Iter-60 loss: 40.2347\n",
      "e hirgitan ruked ind Irea, whisi ceatdliol of 7iand, meiclito-JappabanonMic tas ranges thint ardes um\n",
      "Iter-70 loss: 35.5851\n",
      "erited in the Gloungid mike op enie tstedext ryss andic makE rons Ariam. Lowut rodeht Uip1lanth-itopr\n",
      "Iter-80 loss: 32.1593\n",
      "e porun the y. hyril 1\"855 whins fortd narea ded fsture poveld. Japaneas 2018, wheve Sea a. 120 intop\n",
      "Iter-90 loss: 28.9810\n",
      "ed coudtry in hisy of Oly.pr chistound anthinned rto urla, whi usene parth ciletto Japanese whosth ta\n",
      "Iter-100 loss: 26.6966\n",
      "ethol of porencsinghtry the ffrsth-leuper milimake ubons and in the lumtion enello Japambej an's tore\n",
      "Iter-110 loss: 24.5501\n",
      "est lion it mopery Aslatd a dins. Japan Rasea a cevides mintople hary fourto-lortittar 35tumen Figs1 \n",
      "Iter-120 loss: 27.6743\n",
      "ent of 1riy Toky. Frac tar wasline cunesthexte Warter OlerulyCt and intititite Gly urral Covea. Japan\n",
      "Iter-130 loss: 21.1394\n",
      "eandex. The in 1947, Japan ho Japad is in the citsinf-Japan's ragst piontion in the wourth. The nfthe\n",
      "Iter-140 loss: 19.4796\n",
      "eclate, an myd of chomomrin\". and congins conttrules 9. hirghition-largest and Resto-Japanese War an,\n",
      "Iter-150 loss: 19.1786\n",
      "e the Someinate ithory is and region of hicaty kreombe ter Mecin lympira, Japan Ga, Japan, ran has re\n",
      "Iter-160 loss: 17.7855\n",
      "ered fromited thint-reast malldot tect hisha5 it the Risine Stakio ok on the world's figtthin whicy i\n",
      "Iter-170 loss: 16.6424\n",
      "egins. It lightstolitaty wigh an a dealaclite woped contorry 2019 whin, largestry Elfoorthinandsurou \n",
      "Iter-180 loss: 15.7115\n",
      "empin, the G20 cempore eas in the starlaticanecsiverth inatigited arle the , il the aceropet owe cige\n",
      "Iter-190 loss: 15.9276\n",
      "ellambel an of Werto the Wort ligeshamins mare 1st -Japanese War and WomperNest enghon is an 1s, Japa\n",
      "Iter-200 loss: 14.1379\n",
      "e mollome larthy 47 boktounte cade arly, which cast urina Neara, which cagifld's in the Seciate mare \n",
      "Iter-210 loss: 12.9833\n",
      "ellate d maionaliof the world's the Hurcanita I anke noeft is ans. The war 35 late lode Japan is cane\n",
      "Iter-220 loss: 14.1032\n",
      "eland S(aran simized Japan wasI and tee d Inwar cimestopel pamped ureathe cens are the Merou 1s ofel \n",
      "Iter-230 loss: 12.6709\n",
      "e popullopponJapan is in Chinemeeume first Sino-Japanese nargestignticnde:s of Emperou, the Wurte tal\n",
      "Iter-240 loss: 11.5955\n",
      "er atoontivo and regiel Sin\"Lvecemito, of comporse clatmolest comition kerod Was the to peroleen Japa\n",
      "Iter-250 loss: 13.3385\n",
      "es. Jfoneid in 1853 fountry by ran Pferes the Sea l apea. The portolithic thirt-reghos nuryh ithor an\n",
      "Iter-260 loss: 12.8264\n",
      "e5l reeso Japan is chenda ard Hinday ouppre Panje forld Rist wisilaka, which mevile's in the ato the \n",
      "Iter-270 loss: 10.1779\n",
      "eande mompored Japan to expandwand urenty-portt red red, al 17th parte. The the compor daclites1 ghes\n",
      "Iter-280 loss: 9.3683\n",
      "e north ter Te wopre suliomboky weclad ceggsto rest and Wimping ur towod. Japan is a es, Asiand hichi\n",
      "Iter-290 loss: 13.4658\n",
      "e contorty by purcesed forst the fourtr.ly th Olyati f op omel comala unoled inch merole, wsinody in \n",
      "Iter-300 loss: 9.1424\n",
      "ed a the wirndale wop Werla Und and in Checesed and ureana \"Lang ar areath Resere the Meiji gmpar of \n",
      "Iter-310 loss: 8.2987\n",
      "e. Japan is laeded Japan the ouuth mard bopto. The Russre highes a dac\") apcotecc and tho fountry- Em\n",
      "Iter-320 loss: 9.2884\n",
      "ed a diving Sea and Indextren, it tes 1941 arlest wion copat onfirnd corter and pored country witha. \n",
      "Iter-330 loss: 15.2100\n",
      "evengr.ikeplo umpombintry of Asbar as the date toudcbat, the Great hieseds Reuoro, wssuran Dever. Jap\n",
      "Iter-340 loss: 7.4438\n",
      "elation in the mate lare 1st  anand Iumceeist ras, tomol the expred popertore sembol 1868 and the for\n",
      "Iter-350 loss: 6.7472\n",
      "e ton Mecmite hal The rous deved urec, an titidcale tho Ndebou, the 68meed first country in the Rusin\n",
      "Iter-360 loss: 13.2051\n",
      "er and Whm in 1853 hhe worty fourth-largest exporter of the UN, the Sia, which mevinesunanes whictari\n",
      "Iter-370 loss: 7.5025\n",
      "ered into rectured ind lons to devengepin 1858 \"ntate latg op Wigttimitat of the East Gse ins. Nithi \n",
      "Iter-380 loss: 6.5879\n",
      "evend if constowit 35 werly undal whish parulagiggsed a Westare sulea aed sourt presiat cits it the w\n",
      "Iter-390 loss: 6.0496\n",
      "epen the highest lifi, the Global Peace Index. Japan ea east cithor and in the sured peried of inflow\n",
      "Iter-400 loss: 5.6246\n",
      "epro 17, Japan haseric aicy sulige suleopid as in the Riss Regors cins Sead in 1853 when popul copur \n",
      "Iter-410 loss: 7.9111\n",
      "e the world. The Seain, of rast-rishap and Naea dasiande: 日本hing poper Gakeaty highs, wat and itso-er\n",
      "Iter-420 loss: 7.9516\n",
      "e fourto-lowed in Asia. Japan is a s. Japanese Warcane meopal Cstang of Japan to etstan ardes. Alrome\n",
      "Iter-430 loss: 5.6874\n",
      "entari fount on the nati h ar Tevend of to an end in 1B16 as in 1853 whin popul anes. Swaliof wortd l\n",
      "Iter-440 loss: 5.2765\n",
      "e to an Emperor Nihingkaideceled, is dar and is ras of hichice eisith Kyu, of roghe rar has whichar m\n",
      "Iter-450 loss: 4.9657\n",
      "epon the suxporud lowe harchighis and Seded inty conthor comlled ind ctumel cins Nehen it ithic as in\n",
      "Iter-460 loss: 4.6574\n",
      "e mompory thind a giel bompote by surchiny. The poper Seaid sec urea, Japan was maleadise citas with \n",
      "Iter-470 loss: 19.9439\n",
      "ef Japan to er and ureana ceumer Csined prethor. Japan entex iny it the world's narchalicteoces to th\n",
      "Iter-480 loss: 7.1805\n",
      "e populalandevefol papf the a Tiphe war and the world's in 1868 and the East China Ikaidest wrof Toky\n",
      "Iter-490 loss: 5.2478\n",
      "ered and whist rogso purconsidhaticines, waint of the East China efor an ind four1e earlation meobte \n",
      "Iter-500 loss: 4.5337\n",
      "ery sixth largest cithic an Emperor and an engllare cound intarlant of Nihis rons. Asintop lifith. Ne\n",
      "Iter-510 loss: 4.1743\n",
      "e moput the East China Sea, China, Korea and Russia, the Ga, Japan has\"s and cagesthing from the coun\n",
      "Iter-520 loss: 3.8983\n",
      "e moputy in Honshu, Brfan, in the suleoplo unaly be gotsi ham\"kuuapanuuded finto. The nate unal ho, K\n",
      "Iter-530 loss: 5.7085\n",
      "e forst romtolathig th in the Asua and Hhiko hese warn arloped op aichitan west the highest mithit po\n",
      "Iter-540 loss: 3.8600\n",
      "ef Aria eas of the Ease G5 ity 1941, which came dowed conthor estatich cedcopen in has the wortol div\n",
      "Iter-550 loss: 3.4860\n",
      "e forst rogseand. The Seato period of introithar mollomed ant of 126 millitred it on the Risingran so\n",
      "Iter-560 loss: 3.2966\n",
      "ecmanl Hinarchy with and worrl Asinam.\n",
      ". The parnhailandade starea, meititice eigt tat in 1853 whins.\n",
      "Iter-570 loss: 3.9019\n",
      "e moput populicatar Chinaly urre. Japan is a es and fourth lard fors the world's tenth largest. Japan\n",
      "Iter-580 loss: 4.9493\n",
      "es populatary of inoligithis foulop eistan ard Japan's whictorith in the nate of allopur was the f rr\n",
      "Iter-590 loss: 3.6482\n",
      "e north the Wer of Horousy s Resore sin he Gapind Seata in the early 17th canc Neeud is in 1941, whic\n",
      "Iter-600 loss: 3.2402\n",
      "er arey of Japan hascmins. The forst trity has oflith readic Eas, Asiand Niout popurodant ronso the S\n",
      "Iter-610 loss: 3.0492\n",
      "ectanky with the world's eighth largest military budhed is a med of Japan has surchseand f usitomy th\n",
      "Iter-620 loss: 2.9473\n",
      "er and Winter Olympic Games.\n",
      ". Japan to exthinf regiod has the sureat. Neand if raskeake dalikote fix\n",
      "Iter-630 loss: 5.6369\n",
      "ectanc withinf.lJaparesiven\". Japan's. The fare the Singetith pereu, the Ga, -stanked Asia. Japan is \n",
      "Iter-640 loss: 3.6816\n",
      "ed first in the number of Nobel laureates of acylitury it the First Sino-Japanese War, the Rusroun, w\n",
      "Iter-650 loss: 2.8537\n",
      "e roghes puperouer. Japan is the East China Sea and Takwan in reato pentigithtry bithe catira aed sul\n",
      "Iter-660 loss: 2.7327\n",
      "er Tokyo, the Global Peace Index. Japan was the first country in Asia to host ontith2, followed in is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 2.6075\n",
      "e forrth in 1868 ardional Diet baldex. The wurud. The Sian contorid constitrthico Japanese Warke gins\n",
      "Iter-680 loss: 2.4936\n",
      "ed firltwulihe foust firhturally in 1st cansed fortomed Japan ta, with to the comportes a diving coke\n",
      "Iter-690 loss: 2.4074\n",
      "e world's largest metropolitan arke was inte peoflal aivhe aiverceke peried inter centroWes nareast w\n",
      "Iter-700 loss: 9.5475\n",
      "empon earlakione cokode first in the East Chino Seaia aed asd vorte nater OE's sureationy cottopin 18\n",
      "Iter-710 loss: 3.4840\n",
      "e mornd is a med bomperiof of the Easi aree nulconele torag larde tred curection follobed largest mil\n",
      "Iter-720 loss: 2.7747\n",
      "ered into a long perbose narghmicedece eleoti. The populapimake bales, constituted firt the world, an\n",
      "Iter-730 loss: 2.4924\n",
      "e world's nemporeede efeat the wionte nwwopre by fures andesef with the world's erouncland Imand Sian\n",
      "Iter-740 loss: 2.3336\n",
      "e tory third-lare as inter and it porulcD ind 6smand country wat tulal Dilal Dleol the soutot perine \n",
      "Iter-750 loss: 2.2295\n",
      "e lowest in the nomet peries to aly with the Emperth enstand lophe count In and the warl Compent into\n",
      "Iter-760 loss: 2.1612\n",
      "ered into a long period of isolation in the early 17th century, which was ended in 1853 when a United\n",
      "Iter-770 loss: 9.6411\n",
      "e to an hin\", and of incanacity hicgmpecites reats inte nare vemiod Hokaomed and Seata, fourth period\n",
      "Iter-780 loss: 3.8501\n",
      "er and Winter Olympic Games.\n",
      ". Japan is is as of Hing or and whs has rensed fort the world's largest \n",
      "Iter-790 loss: 2.8714\n",
      "ered interly of forrth lergory 20ty n eroun the Firs the Seata intobeld Japan is in Chinese hastored \n",
      "Iter-800 loss: 2.3962\n",
      "e north to the East China Sea and Taiwan ic the Russoundand Divinta ind the Parey siveral Palleoten a\n",
      "Iter-810 loss: 2.1855\n",
      "er and Winter Olympic Games.\n",
      ". cith infoby by suries. Japan is a whi highese canked s ano pmofidicade\n",
      "Iter-820 loss: 2.0792\n",
      "ered into a long. Japan is a developed conthoby canked himes of tiatd in the highestontry sevcrates i\n",
      "Iter-830 loss: 2.0149\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-840 loss: 1.9710\n",
      "er and Winter Olympic Games.\n",
      ". cititat contrrithe warn Cortes a decid of Himoby bathes pang of Japan,\n",
      "Iter-850 loss: 2.0857\n",
      "er and Winter Olympic Games.\n",
      ". Japan ur aist enout thes the world's largest urban aggllaly intopin 18\n",
      "Iter-860 loss: 3.4225\n",
      "eventurings of the narceato ince furstr. Japan is the forsop.lJapan (Japanes, mathin 1853 when mare H\n",
      "Iter-870 loss: 2.0776\n",
      "ered into a long period of isolation in the early 17th century, which war enth peroud's tant owet ral\n",
      "Iter-880 loss: 1.9442\n",
      "es populatarig picfisthins into 47 prefectures in eight regions. Arpano Recod prefighic and Hompenope\n",
      "Iter-890 loss: 1.9239\n",
      "er and Winter Olympic Games.\n",
      ". citititite ingopiciths of isolation maki malitf in the surcerco er ins\n",
      "Iter-900 loss: 1.8980\n",
      "ered into a long period of isored ins. Japanese paried itstr is the suxtrressunand aroy with the high\n",
      "Iter-910 loss: 1.8740\n",
      "e north to the East China Sea and Taiwan in the solitopen the world. The Greater and in pargeth leadi\n",
      "Iter-920 loss: 1.8520\n",
      "ered into a long period of isolation in the early 17th century, which was ended in 1853 when a United\n",
      "Iter-930 loss: 4.1351\n",
      "e north and World Wer endic perolest pombon it the Frrst of Japan is in Chenesh mare stitropev to the\n",
      "Iter-940 loss: 2.9977\n",
      "ed poprlacterized Japan's history. From the Eur eesed reseona. LonE.sy into the Upmenes it the First \n",
      "Iter-950 loss: 2.2401\n",
      "er and Winter Olympic Games.\n",
      ". contounte wion the world's firstiving undin the East China Sea and Tai\n",
      "Iter-960 loss: 1.9581\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-970 loss: 1.8793\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Rising\n",
      "Iter-980 loss: 1.8413\n",
      "er and Winter Olympic Games.\n",
      ". cititat poprla, mainter and Whichaidef Japan (Japanese: 日本 Nippon or N\n",
      "Iter-990 loss: 1.8166\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-1000 loss: 1.7987\n",
      "ered into a long period of isolation in the early 17th century, which was ended in 1853 when a United\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c0aeba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
