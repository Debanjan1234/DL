{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for layer in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    idx = 0\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1 # -np.log(1.0 / len(set(X_train)))\n",
    "    eps = 1e-8\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 29.1611\n",
      "cesomit'l-ady of an, Elpfrmitint , 1hh Japa egeonter cretand , th, the anry iromope hitete ion en hes\n",
      "Iter-20 loss: 26.6186\n",
      "cemicilankest okandRuali. ssaditing winxn inokmlkec andeere of intacese the w85j. ponp-araf alalebof \n",
      "Iter-30 loss: 25.0920\n",
      "can, in an wand mistama\"5t med n-litar or the Gomliand of NargPanmita in the worl.l merolaog Jappese \n",
      "Iter-40 loss: 23.8940\n",
      "cad (,estupthe aoky .0oman an of ith inster and asiso falest Seand patare aeates\", cannceutur mipthe \n",
      "Iter-50 loss: 22.8008\n",
      "cmataly antary in the whoflapapart hich agganCcoupxpare Asea. In If Japar's the Uant mionlie ficctoro\n",
      "Iter-60 loss: 21.7385\n",
      "conlagiontpto en rinticc Sese con Asias. 3hom chas cestered med Guran a co ljbitandes and in the worl\n",
      "Iter-70 loss: 20.7567\n",
      "chitot of int pero5 topc nerty whos WJapan sis the hustugen, aor. Sanis and ceane marlo inthas take a\n",
      "Iter-80 loss: 19.8481\n",
      "cfas , ch apan cand eallomilan's sarcunturenke: the fad softufin Theatas canic figss. Ini, )h th-llar\n",
      "Iter-90 loss: 19.0031\n",
      "cof coullex22 al ina, hisu wrot pore the ward Nated the wustrekel Hof lappexpart te amie in Hero. The\n",
      "Iter-100 loss: 18.2313\n",
      "ced to s the woclstrAloryy of libestorec antihe ulsian Was ranta and viand Japand3 ia high isonouthin\n",
      "Iter-110 loss: 17.5230\n",
      "chulullinain ins ar undurte an Ir mlupmpareh-cthed ching \"GJapan is thing to the tori, larliching is \n",
      "Iter-120 loss: 16.8090\n",
      "cing lintatupine whiseded instaly in, ih the country it mDorea, sh-upmrcanlesiladin. Japan uihi a ior\n",
      "Iter-130 loss: 16.0650\n",
      "coked Japan Nisise urol the the urgllitan che aats ard Nangest inolis sertalt of mperorst Dimpon in A\n",
      "Iter-140 loss: 15.3303\n",
      "ccytomy peobits Anila Che urla anded Nai, histan countelaalesin Kyom mare worlded, the thtalanesed an\n",
      "Iter-150 loss: 14.5941\n",
      "chile forlst by intiviscand counury, sh toth untinad in is Earg mestertor. malsean ulitencan as. Japa\n",
      "Iter-160 loss: 13.8634\n",
      "ccatese and unten a nana, 2oulatizigan the canla,. parg stsidige hist and Koandmea andedOd GaOatiial \n",
      "Iter-170 loss: 13.1522\n",
      "ctecthen of Nipiod the UN,P Japang assitg anked ingl Japan an suancu,, a ankeku, Japanas ioa, hughex(\n",
      "Iter-180 loss: 12.4805\n",
      "cicomsonains arta andedals and ixdmpan aa, suran, thineu; rare, unkaica ancudJanca, halatulainen, Ahh\n",
      "Iter-190 loss: 11.8523\n",
      "chion wo6ruran 4amaine ard dantxka an the Eapat is ingbed seanct Ialumame tren as the uth and in whos\n",
      "Iter-200 loss: 11.2460\n",
      "ce Globeat liesian allHirag cond sareatendy h Saas upamar ankigh atdinl olsthe Emprom the sour onst i\n",
      "Iter-210 loss: 10.6635\n",
      "city of Pares of pfp-290.5 . Japar Japan is dichicu, marinca unded id ountraiges lasirdadigad ih la s\n",
      "Iter-220 loss: 10.1053\n",
      "chipaly Wary. which Gusiad xfaarc allofearlcgencdaland. Tha trexs arghtatconeny, 200th ald ihiul owul\n",
      "Iter-230 loss: 9.5326\n",
      "ctlen and par, his thekakis aigas med ungr's lake apan as. Landy, In. a aima and The Frcn cnded aggst\n",
      "Iter-240 loss: 8.9712\n",
      "co west and area. wEtrean asi Wart in the was the tularihestJeapan th-Jalaais icumicnmexzigan mth ank\n",
      "Iter-250 loss: 8.4023\n",
      "canen 1467-varpangesid. Sinahi a ofmlaturrestof populercankis and eallo bymean Chinese is the Koakuuc\n",
      "Iter-260 loss: 7.8139\n",
      "cryLon the eap iopen make ,W a uast an callolimine pest oulstaraest minja as thin ually Rm merbecturo\n",
      "Iter-270 loss: 7.2326\n",
      "chiok in tho turitys and 41Rm the iagit . koruvinos Wed the s a dict engin is madere ist of in the Me\n",
      "Iter-280 loss: 6.6760\n",
      "c anopexparic a the rIm ationes fiusor mivine wbotil. Toy20arits ih ia ioso-Japanion of 193t count of\n",
      "Iter-290 loss: 6.1829\n",
      "chipenry undoulding Inji toke uh laraally 1Dmpepnchisd intane is otulion ofer and intwox th trca Ease\n",
      "Iter-300 loss: 5.7401\n",
      "ccatonhinf expopctapan isty obolaty , retoling by smaptory peropes kuito ai, Jaahi a ofpan's name are\n",
      "Iter-310 loss: 5.3718\n",
      "cecougsl pedor nhalititiios rea argest citykokkioudint inst uhi. Japaith its-rank, uhea whisal eserce\n",
      "Iter-320 loss: 5.0689\n",
      "che Euse frgatesy fmpepir o so fity in the bopldetery, migh iotios lurthea so nag perded to d It in a\n",
      "Iter-330 loss: 4.8017\n",
      "chipelloped of Wistree i sinesed andeth inting t hi stroloEHot the Pullatoper of Hits ureud laplowed \n",
      "Iter-340 loss: 4.5707\n",
      "c indirakom oar inotounvinolestreanth annuvines fourthilimanense mill Japarcg ceddlan\"sgstulhe 9lutar\n",
      "Iter-350 loss: 4.3597\n",
      "ccied citoladdsind Ga ansist-dand is sturenees instten uret Japarest Gl7 the, , hounad iny. Niopor Ni\n",
      "Iter-360 loss: 4.1563\n",
      "chy nution. poreromist in inos anderopes ofsth aint ant rintoku, whiccatcs in in the sEatitedes inso \n",
      "Iter-370 loss: 3.9953\n",
      "chisy. I, huakk a a,, the CDGatest eaplo peming netious nourtbouldrdarins ala anded in; ofstsk inthio\n",
      "Iter-380 loss: 3.8481\n",
      "cpanee China anul p19Rt arom the econd of wowestron its in hiue loSuretea, G. 17Ived sed sinst-paran'\n",
      "Iter-390 loss: 3.7048\n",
      "cacimpan is ingin r G30an cunecceakest and in to . noud par coust. In e Gbihg a uinar pation forth la\n",
      "Iter-400 loss: 3.5840\n",
      "city pr47 mets ara a thea a divinen iss Koarta ansile Gb47 nts ar thea stheatu, sy at the caatine an \n",
      "Iter-410 loss: 3.4141\n",
      "ch indedas ko ation t peoprrt to narcc Gmporon of woranked is of isoran and if a I, bar aaci,a, Ga en\n",
      "Iter-420 loss: 3.2964\n",
      "cted legisllitol Cwing cititio menie to uthinesy CRitits oarl is tivst cittendes ind-lalg9s ar thexs-\n",
      "Iter-430 loss: 3.2342\n",
      "city properte of woh th trea indicamich ianlist meen as a stheccadovd wand iesoughi In uralopation. m\n",
      "Iter-440 loss: 3.2049\n",
      "chy with anu the anct in the Rus iation. fo wenterym cof thex mpurond In is on ji8s har E, the \"zSSea\n",
      "Iter-450 loss: 3.1753\n",
      "ccied an\"st tota noe 40 narcogend,o hidioe io hiut lion the sullowCDend Itee Korea and Wan\", ater a I\n",
      "Iter-460 loss: 3.1405\n",
      "conomy by peroits ar-a tnreas mog and Hunmar Indada a dila arg th s and Waroa i ot the sexlopeater is\n",
      "Iter-470 loss: 3.1297\n",
      "cadiig l Gxbagtct in ele pbrct in tourlzalyulemarratith thexparapod sive er I Ga, has and the ntot to\n",
      "Iter-480 loss: 3.1083\n",
      "cono-dich igh nerCh aana , h20 anctaich ianan ist par\"-vi\"pnits Sen7 curureml of sive fess as.... par\n",
      "Iter-490 loss: 3.0741\n",
      "cy, the thia ih ih ourilohiuntintenen Paistor sia. fins arna ist rembexdgsd In, th. Thaparot i itar. \n",
      "Iter-500 loss: 3.0414\n",
      "chy with nhus a Sica, , macanaha ansd iapan waskdakadioa st trets. CihinSob-ousur lameec arce ompely \n",
      "Iter-510 loss: 2.9982\n",
      "came to an isdind, a gich are Hass aigly Omeatro soueitee oboEasthola anded ist o Japanas d ith unht \n",
      "Iter-520 loss: 2.9858\n",
      "chislingia lion lemicanoogo. ctatiot in in ioboa to ereromed the \"Ra diced cee tallonopedititii. rrhi\n",
      "Iter-530 loss: 2.9680\n",
      "city properth inst anou the andala InHth ccallppeeioloficteden isst epprrerceasiist andapare hola a o\n",
      "Iter-540 loss: 2.9183\n",
      "c Games.\n",
      ". 27arcciits a ie hu frrvSoGustofiidedan , kaikdida eun t insar mhene, a no-Japangest migin.\n",
      "Iter-550 loss: 2.8807\n",
      "cegisthindidupen Chiay o winhe us nationer \"suxtccenIrcaese fdkn th lara the atom tan eun-repandest a\n",
      "Iter-560 loss: 2.8526\n",
      "chutery im..veMMeets ThollDArelioa of the Ru-se peceaeeseseueapan isdulimn iss iot rre obupte of-6lhe\n",
      "Iter-570 loss: 2.7971\n",
      "conomy by pborouihuuo oroNog. Ine the Emperieos. Pivened G imevorce dostd lest Indas is the ioto ivim\n",
      "Iter-580 loss: 2.7787\n",
      "came to an the Omamarta. Imben. I al pchicgcxIdekad aginc upppnensrs iegit trrlatto abet Ciurla. 9th \n",
      "Iter-590 loss: 2.7653\n",
      "clupameenmram. biiioith urur a annalemolity of anHonss. Im it boust pemperceasitd ana, , isold,, the \n",
      "Iter-600 loss: 2.7366\n",
      "cadint arka aatictaldest enx, Japannsa, bill miellanowin  rithon tot cent of p37erecod sing asst is t\n",
      "Iter-610 loss: 2.7266\n",
      "constitutioy eit ihiss robot the S2a marcelargestdred,, , fhaaiti ih ret-durend ih as the wourld'sthe\n",
      "Iter-620 loss: 2.7226\n",
      "ch includes Semes ofh a terterts the ioRisin in insaraa Stateredof Ni, ..(wrrectiod. ist kaSti e the \n",
      "Iter-630 loss: 2.7305\n",
      "ch inclurcenkedducinmal and is nout l ainan\"La asmaree Ealo perive efdulasls s forund'st CAiince pp\"2\n",
      "Iter-640 loss: 2.7281\n",
      "cted legislliat bidic itst an8, a ist uren as lhesict of is to anst urom andsecturanas a the holaaci,\n",
      "Iter-650 loss: 2.7651\n",
      "ccessive fess dith und  anfart eliTomeencanesd Sin a anden mexbaanith larAmhtrtnsh  hhiio  obthia. ai\n",
      "Iter-660 loss: 2.7404\n",
      "chy with an-n Chinen a Hulandanf II,, a ng sare the worldicd aat thint a cina Wat and ih tithe nhwa t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 2.7752\n",
      "cted legislandex rnIatii. deat. a gich inca unciuncn Iesteded kiloRithe wobal poreld iest mio  rotboa\n",
      "Iter-680 loss: 2.7479\n",
      "conomy by poboubh eitbiri a Eastulilo\"meats. The Troulelipeciug runalystst emim iritiosh obSautolomy \n",
      "Iter-690 loss: 2.7664\n",
      "city properturidgegef durease. Ochitrtekoh bano i ret oems. .5% Aroct ichitu., Japananagen asla Inatr\n",
      "Iter-700 loss: 2.7467\n",
      "chipulamesea, pe ereariclaimei Is China iys. Ficous Hok8.177av,s vavessid fict lecica a norextefisur \n",
      "Iter-710 loss: 2.7629\n",
      "ccespmreace GDytympcraistio t rest cotboten fiess aba coldalacdunadss hits  an, the four Oc \"nest tor\n",
      "Iter-720 loss: 2.7674\n",
      "city propertury. seacienc ad. ais.kkiiii. Tsyianes is hisht as iy e ncatimtet nerliso Sbaructurangih,\n",
      "Iter-730 loss: 2.7538\n",
      "chy Sobect of dicall wishat the OO8EarvegSicesma'sident iank iudionai Et Cwinen eset urrla Intor I ap\n",
      "Iter-740 loss: 2.7719\n",
      "city propertulfofssud ad. wichasf whiraned ie isdanthes Okhicginde ist Chit bys . Japnecedopms, the k\n",
      "Iter-750 loss: 2.7627\n",
      "capital city is dors  routh pan,e barcthest is. ant ian iokiodicg inve fesss an, whirnatiith annauccn\n",
      "Iter-760 loss: 2.7733\n",
      "ctely. I mich ih oo-sive ioloHt. in is biontriem. Irrana, Sttece callollytint isiriurtith kf uopllf28\n",
      "Iter-770 loss: 2.7499\n",
      "c Games.\n",
      ". as dants abhaio h ububof-koka aigicg anderhy thosku th ihe e foRStates tureco, ate indiral\n",
      "Iter-780 loss: 2.7512\n",
      "cafict ins in the woGh tthexcticed deneddexsta,a theich arth. ihitreees ofrlholatal pectapeosidgod po\n",
      "Iter-790 loss: 2.7325\n",
      "city propertery, to mict of imbina s folll, and the Emparas. an'sia the andur and larseve aneseslaagi\n",
      "Iter-800 loss: 2.6999\n",
      "ccessive fess lhina WasHus Jama, iolara aly pec aregoest os mart is the wrocttered aithit preollenter\n",
      "Iter-810 loss: 2.7229\n",
      "cy, the thiuninjo-kourraakaistolarionined it is os andsth ch coracladeste mpencand I anmr tarceansth \n",
      "Iter-820 loss: 2.6579\n",
      "capital Igss arut oxPf and was, mit nevol mrst inst arom banfed d percreanigg gwoll\", ecteses a d pan\n",
      "Iter-830 loss: 2.6768\n",
      "constitutioh it oobbouinooboso-sefeece arom the wargest cextrcn,enn aslalanacimallolombeliose totlino\n",
      "Iter-840 loss: 2.6668\n",
      "cono, ito ent isst it i obyt orulireatulet bannesthumamanananeeeod a Jnpnanan inen a kop Of remeinc e\n",
      "Iter-850 loss: 2.6406\n",
      "city propertulatgest enth has mhe rom Senectaryonhotet litten ougs robout toy latesenin  abbgur-I pef\n",
      "Iter-860 loss: 2.6186\n",
      "chy with anta the warldex, ccenct ingid litPan tug ivsrrea, offld2-a nerneaouss of inolion with ant t\n",
      "Iter-870 loss: 2.6023\n",
      "ccessive fess lyf meAroget of the corunatese and anea, whk a,d a p93, t arsusiliossthim ivicitost his\n",
      "Iter-880 loss: 2.6023\n",
      "c Games.\n",
      ". forddgresdaniitea, bha Siio,. It om tteorealt itbh a goSoOwwhtotourlading theart ih chana \n",
      "Iter-890 loss: 2.6113\n",
      "chy with ant Japarchedest iny. whiopistof Jaraa in erreaili, . Iterla dor canaloLokeita dys tent urtb\n",
      "Iter-900 loss: 2.5687\n",
      "ccessive fesss fount rigong power pariss expoppppero of fionilitaras i ntrWa usse nari the aran ectar\n",
      "Iter-910 loss: 2.5657\n",
      "conomy by poborit  hiuiouJar kfegook; forusieos higiolixy tremed to monindgelisxang ast eiod laats to\n",
      "Iter-920 loss: 2.5805\n",
      "chy with ank Colaed useanac Inear y thicdiret is. perocnd ral ecterdes indisthimp oa tenie soas woth \n",
      "Iter-930 loss: 2.5295\n",
      "came th ka four Ocalopmion o9 t and of with con tres tolorta, bhted thy wJachiacennaa kiil. Japana a \n",
      "Iter-940 loss: 2.5728\n",
      "ch incd ind in tieilabini oJarnv7vtpesolo,ed ped ecetenes d pananes hoJtmeaku a gene mrobodit i an ap\n",
      "Iter-950 loss: 2.5392\n",
      "cnaime. Itcinm i bitieaieiot -rrropototo by poror liio i iiooro-rer29-JLkkkahkaousi ush a t firsts-la\n",
      "Iter-960 loss: 2.5475\n",
      "camiet limor Ialusel 28,-Ix peo, oh ,ivol oulthinuneng poJar aloWcety, \"s madthesy, kainete in rin t \n",
      "Iter-970 loss: 2.5491\n",
      "came to an wivissoudud citom drotee eiosi, totomititin mropea  wSSobo,,t, a th ihanucitasf Jannangise\n",
      "Iter-980 loss: 2.5729\n",
      "cond itot ais t itsr ado the e impere omee the arag tf-Ixpeficty-deseiene, a ppp9,.8. Aroaihaottof11b\n",
      "Iter-990 loss: 2.6014\n",
      "copomiditi ohunnas thek imsita s werteredsaight and Ga.. II mite obbins ofrat arobotofiot. ppevpepppp\n",
      "Iter-1000 loss: 2.6149\n",
      "city on Thesrreat h7ly aant anden t milt opureiol. worlxmperethestthaa oshihen Sha,a. Ste.,.  fomur p\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERARyWAqAiIiGUxFQEQkg6kIiIhksP8Hb4VCw6j1a8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f7da7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
