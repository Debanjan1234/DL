{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        # q = 1-p_dropout\n",
    "        # u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "    \n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "        # equal to\n",
    "        # h = (1.0 - hz) * h_in + hz * hh\n",
    "        # or\n",
    "        # h = h_in + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz\n",
    "        # or\n",
    "        # h = h_in + hh\n",
    "\n",
    "        ## SELU + SELU-Dropout\n",
    "        #         y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        #         y, y_selu_cache = self.selu_forward(y)\n",
    "        #         y = X_in + y\n",
    "        #         if train: # with Dropout\n",
    "        #             y, y_do_cache = self.alpha_dropout_fwd(y, self.p_dropout)\n",
    "        #             cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache, y_do_cache)\n",
    "        #         else: # no Dropout: testing or validation\n",
    "        #             cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        y = X_in + y\n",
    "        \n",
    "        if train: # with Dropout\n",
    "            y, y_do_cache = self.dropout_forward(y, self.p_dropout)\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_do_cache)\n",
    "        else: # no Dropout: testing or validation\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=y_do_cache)\n",
    "        else:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dy_out = dy.copy()\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        \n",
    "        dh += dh_out\n",
    "        dh_in1 = dh * (1.0 - hz) # res\n",
    "\n",
    "        dhh =  dh * hz\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "        \n",
    "        dhz = dh * (hh - h_in)\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "        \n",
    "        dX = dXz + dXh\n",
    "        \n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dh = dh_in1 + dh_in2 # res cells\n",
    "        \n",
    "        dX = dX[:, self.H:]\n",
    "        dX += dy_out # res layers\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t], train=True)                \n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "         \n",
    "        # Hidden layers\n",
    "        for layer in range(nn.L):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                sample = self.test(X_mini[0], state, size=100)\n",
    "                print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 107.4384\n",
      "e3tPbbhin   c hkden  aa eoiieh rtn1dn %odG 'bs fnhrrzy ka)ccioatphh(crfl ei   tp(eemiul:deof nn-dlwsh\n",
      "Iter-20 loss: 104.4844\n",
      "eefodkdGachuyekfileo hF  pnnl   pfF W eg-nee 本h auol aEis.lnnts aJtmrrzaasiehsey ciife i,UU onttpiOlr\n",
      "Iter-30 loss: 100.3015\n",
      "eFh'er,hadUtaomhlhxtesgutRiDt本in,to 9ioxhho tceaaoa hiry rIeou2veWt 3n eaare hsu sStt toofiin rbdtth \n",
      "Iter-40 loss: 96.3419\n",
      "edd0Urttt Snsx dnee ys y een\" nseusooa, ri.;lunAsx LEc%niitoesob  cee WfttdJonopaasl phvehR2tiiimn yd\n",
      "Iter-50 loss: 96.1329\n",
      "eOm)l本Gtear8u 5Imcias't  untd0 cert ekeys snopg日tt e ter rscenreos  GGCifstuo ivaSenoit raria1in'i te\n",
      "Iter-60 loss: 91.8656\n",
      "ec9ur aoiop  Gse amh un  rd hhiicil1ee  ls1rpeuuoknH neMDtoror snlykket   eeid yScbdt ti tnofappintge\n",
      "Iter-70 loss: 92.7276\n",
      "eha ydNn op umiis Gnof1u3s. usaoooneeirlm RebdoDtn tyo fOa4n w8b5y7, one opiie nmEabgprp.DNtretxhe fa\n",
      "Iter-80 loss: 96.6201\n",
      "eeyUrousJyko mTrean nico dory 6fco shepgsasapesaffssohe a3dJWarutoonxpJp anths id shye in r–e Uatscd \n",
      "Iter-90 loss: 95.9244\n",
      "e(Oddrlcfoaiin tacs ld te capgo woor9 s1td Joonon ; openaJaansoyr zeeu1vwd 7d th an,ss Ennseluua, oh,\n",
      "Iter-100 loss: 95.0552\n",
      "e8Aaa Gin Nes .ata. oritJx-O Jelaniti\n",
      "ddd, Nori8ois. wogEf.pprnie coretme i.apan we oniisacaatoiUPrs1\n",
      "Iter-110 loss: 93.4599\n",
      "ec nunes the anes iarrus dbure conlKin0wot,et emN Ta onS aj the,KlalDms ilss anios'A casd wogette s  \n",
      "Iter-120 loss: 98.9676\n",
      "eWWdswa. ah,  arinie EociinaSalgiuten goifoor Fitt fousksi Sipo8linDJaJanan d  feneoJ Gw ta leuieah t\n",
      "Iter-130 loss: 89.4583\n",
      "e:Tr ansthey thg atoyt tp1roinsAoth  so.'' roielaf oiWeel.obbe agulg culeey p\"nebsrehw fl nhy the rhe\n",
      "Iter-140 loss: 89.1160\n",
      "exyk r ds2i8y ao thiiJo eaalp Get muratteop oFn ca anrnreuuni-gad aoc'voms-te lerbe w mvSiis th t1gyr\n",
      "Iter-150 loss: 89.2270\n",
      "e'Tio9 pndeK\n",
      "on\"t a-pane oeti t yia  aroab do'selpvymin thiWd,e. wopco, ii meson. aolat2rrm tuuiI ta.\n",
      "Iter-160 loss: 86.9082\n",
      "e8yir somto 9opt, txd pcidtdIsdhi, to Nhe fh C'alintxpeud lalacy pooch. Nuurt-1cEng iam aTs gatesthf \n",
      "Iter-170 loss: 89.0535\n",
      "e'''s rist ealyy ceses1uit1 1idg yostir angesorreseandrcth omede lenad iist kao rsuoe\" yinl iksoH ino\n",
      "Iter-180 loss: 87.3655\n",
      "e3bs Hen cmd iaad Gu inll-d peeta, pol rhefapapane pypS.perll'r yy carmshel olon cmmytiidiJgnnheoisS \n",
      "Iter-190 loss: 81.7822\n",
      "ezest ev i4L-ontco eoni, bob sodgipesoromoGj 1lamedttl rap nder ig ,nd Hucatof ,ontthise ohed pDy Jal\n",
      "Iter-200 loss: 83.6839\n",
      "ey NasurB;din rne'sa .lsanimd r8dGwon and Ueebo Gt fudeto K-la1y mauryib in efC-hentiteaJapsty .ura t\n",
      "Iter-210 loss: 83.0170\n",
      "egWrris Jareesbe pereohg topos ri ereor bst Anes t,lif, wsinin tirrmfoon tL iturth ,d iBlaf unttomior\n",
      "Iter-220 loss: 79.3819\n",
      "eIggof,.., mpsuriate.yx Wal Toipar 3apen he 2P'lresatdmpurtae thgst  iyt it oenl Shetd'l istit rorthe\n",
      "Iter-230 loss: 83.1862\n",
      "ead Wen ipytoond whor tht H, sG rye thielImthe Ron o8 Gf \"aehcananJ posmiirest iison n'ee pestaaf The\n",
      "Iter-240 loss: 80.0131\n",
      "ey Frpe \"or th8 Cien uneriHd ind hadgh ue es osr-minestoNatvcirnonEe to, Diin the wuri, w-vhd To dipa\n",
      "Iter-250 loss: 86.4975\n",
      "eky r9lnt ximatk9r th n;sunfd Aalao asucmmerebar stcjitoridec iflapto c puyitor d Janaosx s peries an\n",
      "Iter-260 loss: 81.9456\n",
      "ea;5atG p u9es 2y ICy oirilgt本1lo4ai chisek,ar oon coheesurperian .sOed..yJ llmeri JaEpos w8uit ot ir\n",
      "Iter-270 loss: 81.0899\n",
      "et DfLt iocmastthu rnTc asesiy Jy Gl t4g Tids en1,ry topinr eh 8Hirangegiollmcerrord warintint in img\n",
      "Iter-280 loss: 76.5231\n",
      "eF\"of Japand Se iossi we–ifltlor tiol efian an es tieupara pirl Thircand the arlintperHiwcon akoouima\n",
      "Iter-290 loss: 78.6731\n",
      "e\"%os n the toLss s'h finet or ininas, NfaroapAaf h 1yvpntcconcenot Nyd dfkenof Wade, Ufa wupf l0eic \n",
      "Iter-300 loss: 76.6616\n",
      "ec enil 1ape Jana. Ja1Dm iie thh rlvotee ion chllaEuloto,uRIsed comrldaa rtliransirow the Waresta who\n",
      "Iter-310 loss: 75.3912\n",
      "econ-den feticank.cins co mole2ean Mophos Ja, tas hod tounti, th th-\"\"panfly treo mismnE fhiap nhe cS\n",
      "Iter-320 loss: 75.1947\n",
      "ef3bin alitee ta iir z8t ienens the Tar Annce f6Ph on Ja6son C%4i6uoabNa c 4foppaleddd lethe9aoch opr\n",
      "Iter-330 loss: 75.1776\n",
      "e-wane 'e tiy ao8 rotollareny in eumereo, an hmvias ofeicNortof eu itse pas une FhrmanI an SIp 日 o80p\n",
      "Iter-340 loss: 76.8094\n",
      "e, inanin thet of ceudan se Seumer an gorlthira  iun8tsu inicealisl.o4-candrse uue Orstd\"nn tof arics\n",
      "Iter-350 loss: 70.6651\n",
      "er Savirlapesdalangenlelare on the 's whint e the bs ty uuind Japand 'nd onudes fH6a. weaE in asen. T\n",
      "Iter-360 loss: 70.3422\n",
      "ed zid ginee:d. The-hdpenoth\"rmsesth cexmame wavakan Japan ne Fope innkKirler 128W07cesid theearcoflu\n",
      "Iter-370 loss: 74.7546\n",
      "ezxWurthMg. faes' Echekes ir iu. resititiass perperper pohe Ihiog Japan at, lsruarly inyl Jo-natety n\n",
      "Iter-380 loss: 73.9587\n",
      "e(bone. Jaganns, In elolyon th onorys 日H f7realatl.dtumldd pen the Epha I\" ar engont e thind cexttunt\n",
      "Iter-390 loss: 68.0705\n",
      "e tperoon bs7 fye the popllot ArrimDTii incternoneroan JamE9v–00LC2popenthe torloromnlJapomttoi wNp94\n",
      "Iter-400 loss: 70.0239\n",
      "e)eoan . wcredicy su,eros tixthaess palled red the ant,rwaaN Ingestd akiuna ale Empham valacelat thed\n",
      "Iter-410 loss: 68.1996\n",
      "e Farwar Satis' Gantok hy Jopan my fy rfdetolathe Obthryn nfuucop5wof pJanan TaJ ppende f. The tucher\n",
      "Iter-420 loss: 68.1954\n",
      "e(vanofsc %sod cinnteh the 18A1Winwpao Ho. Diioalaswonsex's wolounJaTal wor,. Imse Courind im ientint\n",
      "Iter-430 loss: 64.8872\n",
      "eg ahgin aD eee om, lis ritecyunis. JApanthe aDgent lnborvpchentourca. Sing Tho ariand wurlthisod kep\n",
      "Iter-440 loss: 63.5610\n",
      "eupan so foumt dich ant ercpeauv.. worddthe weete the MrveC inof Suriang anceetrel ongtoc ano tve5 pl\n",
      "Iter-450 loss: 65.3379\n",
      "eBantu-wich aniy the Japkata. aleita ; anthe wad kgimte wesparatatal intri. ThiJasan-NIea chissou en \n",
      "Iter-460 loss: 66.0621\n",
      "ejJd anged Phel-daleeeess rsolopIor ia aol el ahe rob, phmeor's ing 2rhe UDrtye wekliy 4itho weg aris\n",
      "Iter-470 loss: 60.9204\n",
      "ewxng ende coand syd roflona\"gingthe WsrlokfoJa and W8Thuserca d it ripngurlsting dise okpen, pol:on \n",
      "Iter-480 loss: 63.1509\n",
      "e4Weon ol Wfn3t on n Bxpen ens in Jamandno nthro hc Nmobofxji serte eepho G5uby H6 4gurllipy bm isea \n",
      "Iter-490 loss: 67.3759\n",
      "eG, dongo M8imxthu in onUthe wire Rfd wsu en Colititad inand Japandto lale eo an EmhD The fous wh cim\n",
      "Iter-500 loss: 60.2163\n",
      "eh% Nupar Japan mamin aa Csofins deor hicaneiy ppur cuuatr. Ie ex Japases the imatolste Fur; af it0cy\n",
      "Iter-510 loss: 65.3488\n",
      "e teetin as rokhd Je6apkno Jalan Cicthodst CesllesteweriWSon Japang shonac hisal Bomat eo a pyr 11Sth\n",
      "Iter-520 loss: 61.6147\n",
      "e9po ura. Japand Asia ale bacontoruchasFry. AIppr Pppry roxae pens9hi wed curss afin ea,anctrte the l\n",
      "Iter-530 loss: 65.9334\n",
      "eSoU7d Rpetis wop\"b tse lave enge my bhis l7gedteet..panginst. Japande. Jaan gy s, 6turthe ia tithd I\n",
      "Iter-540 loss: 62.5053\n",
      "et 1oll oan \"hens euctero llvendrgUnn the GEwNchhl rithe Rulleg 1mmaybupulald satter arvim ireasth  h\n",
      "Iter-550 loss: 56.7570\n",
      "ealaland unep okgopou, thinf if penokel f inio-the the wortdrn udywarut ofel martid the Gforl, Japarg\n",
      "Iter-560 loss: 62.2971\n",
      "ec ano mourdch desgess the Napeu th in pent of t ry mled whtteruns bountry ia an Ersithe parthioge fo\n",
      "Iter-570 loss: 55.3560\n",
      "elaber. Jlprored ase wog thithog xpetitil Wapte, wa, ho an taa e 1inh instyund of if os altiindod Jap\n",
      "Iter-580 loss: 58.2700\n",
      "ed cobtc teith Japangsith  Pithe aturth fwerlar. 9hithiss  G4e parcinD an ILAape Sec, W7fideand ccuno\n",
      "Iter-590 loss: 58.2583\n",
      "eT2 orshthe toof As6\n",
      "日h3D2  Grantrd Ho grcteedes Wthe eppmrii ,. The bupen issiond celsorutotorlat. T\n",
      "Iter-600 loss: 52.6011\n",
      "et reked s the \"ound t iist Ches redebanted the wari Thiy l, the dHse Sin chenfis . Imlergent ticl aa\n",
      "Iter-610 loss: 58.4284\n",
      "e Toxlicandkealloghess worr an 9258 Dupproon vigitex csulerthe  aobte copemboroan ofimd po. Ja–ndsth \n",
      "Iter-620 loss: 59.8231\n",
      "eNmppiry  fhilalaryommand ghin ,%\n",
      "\n",
      "本本日  ae EEpitO.. Chenant Afmigupun\" UO1503)O\"lsnsit inthe wou ty  \n",
      "Iter-630 loss: 56.2269\n",
      "eiwro s日 Arperind esst. ficunlre ifat. portesa of wed wontho on mteryan te Co cencogetithe whetho wsl\n",
      "Iter-640 loss: 60.0208\n",
      "e Eawe of inh ine tar aonsz Joppokthe whrstr s rorctory Pomchy pest the Se, wic Nasin vinit eisoptrur\n",
      "Iter-650 loss: 57.9464\n",
      "e8 bath nf wiped of 197, Jagan ao 1sowe-theinethento ou mfopppCmese if tolithe GGbtnan o本 Epp7 Japap \n",
      "Iter-660 loss: 51.5672\n",
      "e purud ia ina, Jatanin s chuston marilaren the 1Jonn pinoutico, the Alwir the Kes orse martary ctent\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 72.0981\n",
      "elosld litetho pountrxKalgpita a t pedterfe an meoturl. Brant andes wrrhita corn isa aso gopot oriplo\n",
      "Iter-680 loss: 56.5595\n",
      "ec. es thy wurul thi an ighe woutu4y JaEan, L's sentest the wary. The lored thenensc af cinan ied oy \n",
      "Iter-690 loss: 48.4290\n",
      "e Japan ef counosy if OPons3d ji aasecou, sh 19221(7xpP45prrghe nomme. Arlaailanedelaan pelaleoklopco\n",
      "Iter-700 loss: 55.6233\n",
      "ef 1mpereas th ce Cestany f if Japanese. Actithilang Itvextprpeniny Thi hou1122–26121L本本本本 JDpan o8ba\n",
      "Iter-710 loss: 52.9155\n",
      "en Niglalis a. Japandusu caconsidto  super pipeope rol dioecyrith isllese Arrippandint medentr of m r\n",
      "Iter-720 loss: 55.9924\n",
      "eFfom ha and galcc resturof e ve rot, and rtealionsth chmied ofrce , ol Japan rhs desturcelibid tinti\n",
      "Iter-730 loss: 59.7772\n",
      "en ount . pesme feuntrd ofxth larthy mised Nopptorivl..rUD-Eemferoula, celeateccaner aoun Japap. fIon\n",
      "Iter-740 loss: 52.0836\n",
      "evlidlal ias Deeev ppprod ofte Eaplywicl 1eOjhichunst-tawion mmhiturin Thilc2 worlledked JFpand Citah\n",
      "Iter-750 loss: 48.2428\n",
      "emcotid f ulur.iMa akve analdi. The ho fruatectDemed andth Japas-pehiras Iske-\n",
      "ollaigthe tareea and a\n",
      "Iter-760 loss: 50.5419\n",
      "e heg tokuoxg tarte ing my Jl–app wDil wfithe parted and HoB5bppprias rh0i anden rBaanm ionthi sive b\n",
      "Iter-770 loss: 46.6439\n",
      "e0, n ref ave tho a, hel miuperofl dslantiro epgint of in. an firhe Thiry 124h5 nifi;. Aaper ankualH%\n",
      "Iter-780 loss: 55.6831\n",
      "ectincd licigkpirtevg ea Japan', vin\" anse fopt rald reoded t urea, Som inhesy–upo, wad Ca ane u12whh\n",
      "Iter-790 loss: 57.3374\n",
      "es ind i\"s2oc Ceriand asessures lamgiptrer arth wholUWt eipapao f prrortexteccef ir indSoU r2hicentir\n",
      "Iter-800 loss: 48.1873\n",
      "e olomuheh Hunttryx.o. toeseringhcs orttid a-dersed ias seedor tes out ea of Japan nas  litortaed ah \n",
      "Iter-810 loss: 55.4571\n",
      "e-the tex it wid dmeesorthes nf regeroty yf Iscdwecin9ta icog. Sif iy iss lus tarthes esAd Japaness i\n",
      "Iter-820 loss: 48.2761\n",
      "en deto t up ok6. A15–––日–w% pfuted ofeJapapLr Che puved rastend es the warlbun ry85: iola, hog tomth\n",
      "Iter-830 loss: 45.8387\n",
      "es ruged ssin\"tuyy Il in 13\"ostilithe atte lirld's iasg beverllthitarcal ho trmins lardesturon C0 nth\n",
      "Iter-840 loss: 45.7555\n",
      "erconse8, Japan ea Iryar, ching mo IzCi pareceated oh the hemboll. Thilapares. Toligenckun. ToxJppand\n",
      "Iter-850 loss: 53.7006\n",
      "eccuned F7rof who cuped OE aali, wi25 Rin9,. Jas ngity in 1sliod minanitynn s the U0se wcon ofhica ea\n",
      "Iter-860 loss: 41.4429\n",
      "ew\"uran cees in the roacoase Sea ia es ironsua. merifec pomed of 6Ir4c wat es amgelons Ni3rr85iynits \n",
      "Iter-870 loss: 46.5165\n",
      "es Warec'a e co ard rslitat ent1 Japangid nabe lavith lIpared Ix ina whig u pho mong stxred Wisto-ner\n",
      "Iter-880 loss: 45.5053\n",
      "e's tnomed righe woat the Parion 9955 of Japan1)%I 日P7Eh1 cofthe inging en wrohecEus.-1FJDNN0latest e\n",
      "Iter-890 loss: 51.3918\n",
      "evcoun WoJapapny J:.\n",
      "..).)0y ccan is afwit  Nagin\", wha wivhofg ender unty coplesed dirgdes it Sea,en\n",
      "Iter-900 loss: 44.9454\n",
      "ea for atgy insmanerglnurg\", ndmetpural whi Japan reseapun (a areapos. I8d y insturit tal anith-la0an\n",
      "Iter-910 loss: 39.7640\n",
      "ed of tho EhLt.ov pFmp.  Dmurel-Cormestareecd d pared is loves aid birt et eonlliont couneuc\". Japan \n",
      "Iter-920 loss: 43.6692\n",
      "exmecea. Japan is anorta \"namesstinutes mrpercins. N0fbrtate nsin of tir apea.. Thonantco exunecor in\n",
      "Iter-930 loss: 50.7550\n",
      "exconsid metoporluched on mkouuowhin f ininste in ar hivaopeo p9a, Japan is tan, of mive ma en s.ann \n",
      "Iter-940 loss: 50.5292\n",
      "e\", nts y.\n",
      "Avingestoun Ad ao ppppro of meertan festoumling-toperio, pukeicle, ryllrins ired iteecored\n",
      "Iter-950 loss: 43.2142\n",
      "eTmey lethe four W'ry ar; HJ\n",
      "520522g toloonjt enHt apes a2d Japan'l wes if mielasitit ticlesit d oxpa\n",
      "Iter-960 loss: 40.7294\n",
      "es yrsllinf eropeontity in warixg lariestavusta andenan coprty ofull. jarnitatie toun red Ef larl ef \n",
      "Iter-970 loss: 41.3159\n",
      "et rya, the Ead colati ihala aed f-utheser ssurencoun. Ipprr in O8peloct the insortrthe forld Sist ir\n",
      "Iter-980 loss: 41.8940\n",
      "esicand atron 1565r wh hhang nipered an eeg-lergest ethe pase mimecG, the mpertunistd chuntlSiw. 5sj.\n",
      "Iter-990 loss: 41.8234\n",
      "ed of tho G21–662200J–wa4(, Ecsibo8y Runsta Jrpanted inol rourtand h the9pliind wovind int erpor, ast\n",
      "Iter-1000 loss: 52.2567\n",
      "elKonse mighton depate d phonslo fh15W20Nannag msian Japan\"g Jasas foun ale of Japanp. Iares alsirgec\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067e0240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//100 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, p_dropout=p_dropout, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
