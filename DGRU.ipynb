{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for layer in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache,\n",
    "            hh, hh_cache, hh_tanh_cache, h, y_cache\n",
    "        )\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, h, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for _ in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    idx = 0\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1 # -np.log(1.0 / len(set(X_train)))\n",
    "    eps = 1e-8\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 104.4002\n",
      "e-Retciis,re  s  th ehe nfee stp aaar innuho,n  ea fat5olshaDnoohL  u eineueino med. ptleuay9y in y l\n",
      "Iter-20 loss: 102.6579\n",
      "e'row undmren oi dha1e tfmelaf oB f R e Ne, p hia7 mgnnmrrrdsiMous ake  i toouiles Ua momp-nrinninyia\n",
      "Iter-30 loss: 100.3455\n",
      "ea1G.i  -pltunite z as y rtpe upsarde fantauipa ls, uy as b-dtos usss 1htehoarno'0cln7igemeceviNe rf \n",
      "Iter-40 loss: 98.3492\n",
      "e0Bk JnJ,on tipeh nit ta2 4afe eioaios oJh  heir  rwhuFntdEy,psn,gJmket7  thxIpono ry lan  ssoa ai al\n",
      "Iter-50 loss: 96.6431\n",
      "eaihs uyst-tedaldWpvrinas uhEomeattokina he uumtrkin tb2ir at aninm ten my snl rin teJags eine a tin \n",
      "Iter-60 loss: 95.1867\n",
      "e6-M日gtd7 arat iotfeinat vte'rs  karsh5drhisa.ithe(l8 an orkd ntfs tgitrec heoTlpretsa  Jo tar WTscf \n",
      "Iter-70 loss: 93.9353\n",
      "e0O5r teTen r7o15s phe9bJJWerpatis Aa anneouisiIcegras fgItxru ttretigwiO emid kauapteaRss.  ansn ire\n",
      "Iter-80 loss: 92.8418\n",
      "emabs \"odJm ale cnn9 roWan 0hoic thc rpCoar mash oAd if iee iNathes mm ms rieis lig sire aheitoT fe a\n",
      "Iter-90 loss: 91.8649\n",
      "ev(tr fitte larcape eithr Sas oisthulante sh gin Tho wxvhn he isosscy ateniis an th ia shomCiP anexho\n",
      "Iter-100 loss: 90.9772\n",
      "eSHoh', aar shen toedes yfipaty wiret warpetmn 'mne or rrhs rhvS5rlool mfetpdo ard tsd bn7 of muinI t\n",
      "Iter-110 loss: 90.1610\n",
      "el1bGpigeteinw papeli'l the 1oponanD  Rrothed 1Iaintx ilirvus wsy blrin aiuog sat. lChitrn yo soldlir\n",
      "Iter-120 loss: 89.4038\n",
      "e\" ay the 0lp5fklatol in ,h win y Siuato -kKnon teo wiro,sd oa ae- torompead 1Nppnee srlo ah 1ear isn\n",
      "Iter-130 loss: 88.6953\n",
      "eHorh Afd waf eriosisardec ihinn mKol1iristard aubaam oe thJ yfan iees fonos onapoaoniiy orlee wpomko\n",
      "Iter-140 loss: 88.0267\n",
      "e:v-ont tire cfin the we teeoul nanan andes wegtas the fhet eciistedupinaiato towest A-ra,gdesth ,hsy\n",
      "Iter-150 loss: 87.3902\n",
      "eSN5 ne Ttpins nPpnkile mtpin hiss Jacneronl th cinty isty 1a a6gees wou ayint the loiciev nouecmpinc\n",
      "Iter-160 loss: 86.7791\n",
      "ef panof Jwparges inplito cenxind'n oa-ey  oentil. mailos ofont.8l302r sElbolare Cu te trrtme theA yr\n",
      "Iter-170 loss: 86.1870\n",
      "eJm–he hed d an relac lomuats in an tol coml.t tund, Cnrin EhesliJapan 1-si r7t ED ieg re, gith oiJas\n",
      "Iter-180 loss: 85.6085\n",
      "eb6nthe ttevenlin thestros ind Forp anth'd ind pely ietli'aggean rog cedesa es en maey fs teurent fie\n",
      "Iter-190 loss: 85.0383\n",
      "eb日 tae coo tiveenaI Eunapniun a7 punufrse ionnaris lo mparat herxiC cnruoa ie tee sh aopt pro3loy ea\n",
      "Iter-200 loss: 84.4724\n",
      "epston pap sina an man pyod The .f corttoin gant wase\"is loin t yivnea cesorcsJapan wor le rhd whe to\n",
      "Iter-210 loss: 83.9075\n",
      "es fh ay theshe contote thah ths rxllue lcrithy Wimatle mapins Soveay Japante iectinn mlmen- Nortopin\n",
      "Iter-220 loss: 83.3416\n",
      "e0lb, thith lgincag Lt ind mn Nikgor. thpes7siso feettn bu\"thoty in8r. Toat iolittie Rfgesct apen ELe\n",
      "Iter-230 loss: 82.7741\n",
      "eekrtl Checint en eolan 日JHpanerd % and Te thdp3ugicn.. Jappt4ane hit tito ure enDGy'I Stelin 1h ppan\n",
      "Iter-240 loss: 82.2056\n",
      "ean thish apd Lhear c7xauam mhinD 5o th 1hy pplarees 1B0th reom tIe 3e hith th xwy ubat linkea, the c\n",
      "Iter-250 loss: 81.6374\n",
      "emy in to and Hesgupape, the Teoliming4id acesipimith ie Enrild ie cittto Japanasgin inatini mupebcce\n",
      "Iter-260 loss: 81.0711\n",
      "er 's okllk;irs, aastan Wary chexn9n\n",
      "yD ino eitieephise is laoas . The or aome texHopard coptoleissa\"\n",
      "Iter-270 loss: 80.5080\n",
      "egtin woturan Tne s. Japarlodici, trlea. capor he 8is an Barist de ti.lH. Jupan ererige sa th mhest o\n",
      "Iter-280 loss: 79.9487\n",
      "ecfDcstran ou in lonit r. the Sas lldlinome pephs Wo4l Kfexliont he ward drrdjW ramgelist -pandeg, an\n",
      "Iter-290 loss: 79.3932\n",
      "ee0OMeat minnd sest woro. JaplFnnleate in Riras bokto choperth is cheth opere as ar miole ored be, an\n",
      "Iter-300 loss: 78.8413\n",
      "ey. fftioctite'l Rumden Ian WIbed She hi.ic, tho ankei et in Giphy ard. coch an he uhe pond rerir mea\n",
      "Iter-310 loss: 78.2922\n",
      "eH'seanir ond panjo, chocern, io ah tf che i9 the Hest ilo's ofangeakof Japande. whith. Tto lor oasea\n",
      "Iter-320 loss: 77.7450\n",
      "eBcilaase Wiporit vinndes an mailitn 186, omar ag N7 . ae torled Oring yn-esild 9opand Singtso ffolel\n",
      "Iter-330 loss: 77.1987\n",
      "e thi cnthe ao4w fopenWtudes. mhokhss woxUmpyn ensinc in iotod alecho sfthis larit in and warr t11cil\n",
      "Iter-340 loss: 76.6521\n",
      "easwte in in0 x876;stkominithxeviin the Rorld al wao Chow chesg mwcrhe toe intey io tie the comltyict\n",
      "Iter-350 loss: 76.1038\n",
      "erxFpand the Gwhg voked isths Ia andest ursa iod J9punam Ri. thouU ofevescpar oit divm ias liro oy mi\n",
      "Iter-360 loss: 75.5527\n",
      "eakotiPe4 unan of itiled pimn in eaPl Ch iouin A,yl oumth bo the wethes in's 4SCunowyr  orat , Iand i\n",
      "Iter-370 loss: 74.9975\n",
      "enms oind porlero thi rasld bilraa. fylomalita ceaoledin 1e Gnsh nh it, Japan te est re ster on Sand \n",
      "Iter-380 loss: 74.4373\n",
      "emfer ing coparas Hond Imumie alo. al ohy Nf rethoxhmla lico rhewertome. Aapanloc inthe Checonre Rist\n",
      "Iter-390 loss: 73.8713\n",
      "e3 esthe oulaly Wort my artes vint fofest Orine TSed n85s laletie tn lored tod loving. geand of lhict\n",
      "Iter-400 loss: 73.2990\n",
      "e. Japandes ap Sar t2 un urof The chas viga \"W, Wipgery cinculiny in the Tmuio an Japanes cnmed is ce\n",
      "Iter-410 loss: 72.7196\n",
      "earat. Japanceunge the Hion toury. The have sh rades thimhing feuthd Tureh eulthe histins an omstory \n",
      "Iter-420 loss: 72.1326\n",
      "ecanlxin 10durers. Tth to levearcv the Rlc(vest eirortolutore curlilasa che, is as peWere inad lasege\n",
      "Iter-430 loss: 71.5378\n",
      "e;, Japandes rectepis raus Aether The fisth NAan as inammkyo inth tfrleE, Warghct 19sy pebpenrerclyg \n",
      "Iter-440 loss: 70.9346\n",
      "eaty poper titite ad the Pahes parcounand isttuves Ganes tre Iog the mapes is its icorlage promidti c\n",
      "Iter-450 loss: 70.3232\n",
      "eRun on Hapcitheis ans ahesh ancoin Chinh licciver, Toutir cany d bomnod Uhe hins asa ald iferf miokm\n",
      "Iter-460 loss: 69.7038\n",
      "eg cainaledyd perthes and turlo ell tion . Ahented sal, The sine weshy fpof emCcincy pexcorllte comhi\n",
      "Iter-470 loss: 69.0772\n",
      "e日r. Ie, tha esmanc, Wavlewin A7ge of Japandeas 9,thos eatiom Hourtral Ninnto peritd ro basg Japarec \n",
      "Iter-480 loss: 68.4441\n",
      "es-Nfetirota pease pertaen(iat o9 sfulla,  atinat in ar esofi, the hist or Fonlicgante perperes Wtr o\n",
      "Iter-490 loss: 67.8058\n",
      "ex aonstlinir OEmaliof imofly poc aodiga Onac witt expeoth ghece toreslofld paugeac Ia uroe The m28l \n",
      "Iter-500 loss: 67.1626\n",
      "e\"D. Japan in morlthe canted celcorely. Japarosu. Thr koul Naiceulin Cf obol Niondesi onmper iomd Chi\n",
      "Iter-510 loss: 66.5145\n",
      "eglstlirod the Abilion ofy ir and chane tiontk peppakeo por Gnpar topin mirnd pian \"o storotlornlofs \n",
      "Iter-520 loss: 65.8603\n",
      "engord whi fiestu. Japan es: wer, \"nganes, awecy peppary l6 1c oubu, the coun tuen Inje wara the Nauc\n",
      "Iter-530 loss: 65.1980\n",
      "ex anuicf ie rangest on boxnlamilly in the Ensexrasd mertileto is ard Chinated was eo, melofeet pesn \n",
      "Iter-540 loss: 64.5249\n",
      "eaokoky mina bomn, in tak irging dar ins is the 1exa wokh isttly par tio the Wort chint as and Japand\n",
      "Iter-550 loss: 63.8384\n",
      "ed the Gn;a conlaly iontho fexta listteve  parth ver) woressind hinn-marledgsy eace pobeowinu in an l\n",
      "Iter-560 loss: 63.1364\n",
      "en Japnobe Sus thes. Japan (ait th oi, Rhes anWons ir the parked in tionl Serio sinudive Insea. The w\n",
      "Iter-570 loss: 62.4178\n",
      "efonT. The popanary cigistthise popcirtiio. Aa, thite ard the Gs2ud-y intio mhis d poranggstat Gusilg\n",
      "Iter-580 loss: 61.6832\n",
      "eapdy puppob estivins Shate monliy the \"sund-reas catins. kid sorgestl. Japandey un wopklow tho with \n",
      "Iter-590 loss: 60.9343\n",
      "etpan's lang-nhait . The wex tikerir \"Janase of foumh the wisitoryof Japan's cinldyi expo folnthe wor\n",
      "Iter-600 loss: 60.1744\n",
      "evec-randou. Japan excount dnurte-u, andens resh rerode wisf Japan ser6a's tho euminitisn Ctandtc ren\n",
      "Iter-610 loss: 59.4073\n",
      "einat re oopes Japanjo Ho7stict ixsture the Risst parnaryy mitlore, of Japan ic oGped tient. veloperi\n",
      "Iter-620 loss: 58.6381\n",
      "eapong pite andit pevrt meat piinis 1h. A pareyis om ilgerolelaise tares Wst. Aseated of Japane. Asth\n",
      "Iter-630 loss: 57.8729\n",
      "ekio the wurch sorld's furgh. A85h sarleds whunhi sality perctukeoC ,nandy eaped Rag t1ed Japan was p\n",
      "Iter-640 loss: 57.1181\n",
      "e War'e han, Indine esstry  oaglof-dtonditi ouJdPan s95r ally. 4f leporilagiss. Gfmyt eupar ain an is\n",
      "Iter-650 loss: 56.3792\n",
      "er statirys ar of Japanges aide, tfest cowtouncan-ofeat iin ghe the Japan (owha -naroy ar siomtoduy  \n",
      "Iter-660 loss: 55.6585\n",
      "elbof theand capealins reutien ar bom's,t tipirct tiin an the uwar int an the sorltdivs Sxgt on toe h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 54.9543\n",
      "eWd eunthe Emperia lollestu font lycunses fors aEdestur of Japangi. Japaianas paritalofelthy %il texs\n",
      "Iter-680 loss: 54.2634\n",
      "evtres1. Rp:B teloD. Japan ia o2 Hlama er and by Chinau, wam illor 9, 12s arlicigans lediticasl Lonld\n",
      "Iter-690 loss: 53.5825\n",
      "eiwing in thech wod this, Tomithernly dpper ho mithe inthin mated Seani hflh'  somha ast an expan oth\n",
      "Iter-700 loss: 52.9091\n",
      "ef Japan se poruth. ConWurges fhif the coas the the tao Sea ir China, mtoningtitu id tad 17t on Nibct\n",
      "Iter-710 loss: 52.2407\n",
      "en;. Japapen is the Emperioen, untpis ittary of The Waslo-Japan's mangeccigest pvint to thu Sta is th\n",
      "Iter-720 loss: 51.5753\n",
      "e0tlano latas indexinn Serlo's tho Ease worstoreand is contir of Japan, the Muvio , manden puringead \n",
      "Iter-730 loss: 50.9117\n",
      "et intho, Anof of Ocelast in hud eaman's atudy. Arucearel Purcodent pereate, ind a9 Eare O3 of1\", ank\n",
      "Iter-740 loss: 50.5543\n",
      "ejo Japan Sut ricem. The Guriaa, Japandes tikoredidi, 1nE1this wat p9egea a  Emperea at ty clkofome f\n",
      "Iter-750 loss: 49.8452\n",
      "e-randithe Wirt ry In yor th Fho Ted iintas ind byuper,anh the wand lopulatloin. \"lameyins Eumtere wa\n",
      "Iter-760 loss: 49.3110\n",
      "e6, ofk19.  hach center ke as mlecinesy. hith connd iastir \"his an oI kof GLopan Eon io thich ingso l\n",
      "Iter-770 loss: 48.8406\n",
      "e, 1hiand untoen 日vGneghes. The fourte, the nd aol the withorcal copmecoanere te th aly, the toky. A8\n",
      "Iter-780 loss: 48.3930\n",
      "er5, tha cste perpeanc fidst paone coutlof os dha is apoured in ties and 9tiand i9 sitor . Afion thi \n",
      "Iter-790 loss: 47.9647\n",
      "esandclncbionen, peutee yn inseatt lecanted,  fopficesi. khice tre the Gxoflowy N7ilkou Aeita s.an 1i\n",
      "Iter-800 loss: 47.4504\n",
      "ed coplomilria of Colla, The Oicland ac if ino1 Oheca endenU Sisg-laof itrton ind chanl-foulthorgSst \n",
      "Iter-810 loss: 47.0313\n",
      "eJapan heslowelo,fandstres.o 2algins inger, Hinstary melldes argesth no memledin  meatureobesh was. J\n",
      "Iter-820 loss: 46.6117\n",
      "eCh andesh the 1n6g Nationl-virges a. hecthiv laky ieskai so. Hwarocelanes. Tercamrenicy. Anof rictxy\n",
      "Iter-830 loss: 46.2689\n",
      "es the popen ran\" poalaly. Alobillory x. Werthr h 3h. The Uape  fedtory chalis in ming onet livan mis\n",
      "Iter-840 loss: 45.8268\n",
      "es anded pefuros. Hlaun wfictontlad Th- Wopaid. Teofe ofder esse llritoran, and China Okled uonth pot\n",
      "Iter-850 loss: 45.4711\n",
      "el borthes. Wittory shich cukle ii an Enkui of ntentureacin uesiterclacey bobutorgfa, Chiat of Japan \n",
      "Iter-860 loss: 45.0762\n",
      "e\", 1tiltirc, the Chine Esalate tre Ris a. Japanesex inceaties a mexlere, as the ata inge. Emppopings\n",
      "Iter-870 loss: 44.8853\n",
      "en Bimntlice int . bivha, the world's leptrea in the zom the and deviro ky  E9pestho Fsto seuth- Seat\n",
      "Iter-880 loss: 44.2811\n",
      "ef coraldelicenissinary ictas ea the Eesethentwen tontrist. Japan Was Indivu he hared Wigct om lelpeo\n",
      "Iter-890 loss: 44.0521\n",
      "e anomy of fourth 3inn isstectio thi cliniPe Sea ty wrlutre porl; fourthen intlimexingisean ou welcku\n",
      "Iter-900 loss: 43.7269\n",
      "ercany chaty ind Sua of yof the coun lubyo nctemuobo and umparor , the ha, uhalenuran sod thens. Japa\n",
      "Iter-910 loss: 43.4009\n",
      "estaro's heal Counas bil tho.s Res, camgioed pen oppne andest rotolyoS. 35–207al piocatcedean was war\n",
      "Iter-920 loss: 43.0563\n",
      "ed Solev,punte areH, K3e mopunthing mmpbofitu enstiturentors lares, the. Japanisa E日r. 47 prruakec-po\n",
      "Iter-930 loss: 42.8227\n",
      "ed of io lethina an\". Niasory \"0the p9turice Eusmeare ghina  autiagtiss and the halas dalapesital O9t\n",
      "Iter-940 loss: 42.5192\n",
      "ees arded mingcs ag 1soad ilgins o urletut es a, Sta tha cnalive Eres, whish ixd1th  cound-the mien, \n",
      "Iter-950 loss: 42.1585\n",
      "e: umpeltons ah canceataly. Akiat in the ER4s porgestby porsax,. arata b8l ofobn wo the the 12th al E\n",
      "Iter-960 loss: 41.8883\n",
      "en the Eare world, the hoxnd an of Nearly lad, bad the tye in tionn iof inges intureriont sy is thy C\n",
      "Iter-970 loss: 41.5548\n",
      "e ontury Westr aedttorn\n",
      ". Japan (e wat ored and Repertad wed tores ag o2d iouSt. Dilloumiiced iish .5\n",
      "Iter-980 loss: 41.2820\n",
      "ed iitits is oftititetpira, The fourth the ECpon of Was OR inkies as thalergest is itcititerper War, \n",
      "Iter-990 loss: 40.9926\n",
      "ectrenko hored tiest wiun miunne  injh cintory on 1947s Asiansex. Abrand the hEvent orghist tro Sea S\n",
      "Iter-1000 loss: 40.6645\n",
      "essinadyf i9 1863 whrst roun Japan is andit en Wor dent untouved \"antss  ho hitho Japanisa en East, E\n",
      "Iter-1010 loss: 40.3546\n",
      "e ony for mela, The ary. Aver pentits an ow. Japan euntre ilomevyty esea, The cirndin  9mperolymisa. \n",
      "Iter-1020 loss: 40.1584\n",
      "eand wilmintre wad Sit rysino the the af toy To chitar Dourlaly. The cain 185L the tarled dentorsel. \n",
      "Iter-1030 loss: 39.8627\n",
      "e\" the axgent aree fod thecint tory wa an inclaiosulimg rotidna Uppea, the wigr atd the Empexporsac, \n",
      "Iter-1040 loss: 39.7010\n",
      "e anc aadet pirural Peage gnvsthe world shomndic wins urandet ouran forit anmean Pouma. Japan iti Emp\n",
      "Iter-1050 loss: 39.3144\n",
      "es tietainst percepel\"t inseraaded in the eakomy ir atho sallowe hast Hh paliou. Japan is andste eati\n",
      "Iter-1060 loss: 39.0867\n",
      "em ireitiving strerta-lary Desto aighag silate mantes andececlies or the Gd lame westury largest miin\n",
      "Iter-1070 loss: 38.8563\n",
      "exwand Japan,. Teeinanisithe ,AU7, fitst. Semted of Japan os Japan is minn mary as the and lorlestinc\n",
      "Iter-1080 loss: 38.5887\n",
      "ethe tie the world's peaborlog chicary of the Uppok o Apis an estiry in the sitla wodtin malata. Clic\n",
      "Iter-1090 loss: 38.3405\n",
      "ed \"c3oasove Unsex mreateopen an eurld 2a. Japan is a derst er a d indurisiis an exptral iower purome\n",
      "Iter-1100 loss: 38.1865\n",
      "eun;wurerioulita of ore tio the wovint int enjoulo, amky. Toky. Cren\"sina Nexmandiy tist auked ronste\n",
      "Iter-1110 loss: 37.7784\n",
      "e, laeminorceref oD chaled in e ppof ac. The Gxppiric mising the Ewpor a ded suuntry hinh Suntr celed\n",
      "Iter-1120 loss: 37.6041\n",
      "ejo the country itd istan of toernalito and indure Wurth-shages\", Nappon in 186 -lapitary. Japan's th\n",
      "Iter-1130 loss: 37.3792\n",
      "ex. Thecnupee silate ofr 28laine, malles in the popronitidenare ths , fhomuet. Japan se 1955 madce ni\n",
      "Iter-1140 loss: 37.1816\n",
      "ely prexinto of 6urat tiion minh poproal of in 19553 andcof is teat ryve, whocal s2dthe Paamed porgha\n",
      "Iter-1150 loss: 36.8831\n",
      "e-larges, in pamunren's fommtary ta the G8, and dina andsuthl popural gista vichand-larezof th. Japan\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
