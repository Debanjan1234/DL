{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2804 valid loss: 2.2786, valid accuracy: 0.1362\n",
      "Iter-200 train loss: 2.2655 valid loss: 2.2519, valid accuracy: 0.2030\n",
      "Iter-300 train loss: 2.2429 valid loss: 2.2265, valid accuracy: 0.2796\n",
      "Iter-400 train loss: 2.2063 valid loss: 2.2012, valid accuracy: 0.3504\n",
      "Iter-500 train loss: 2.1563 valid loss: 2.1764, valid accuracy: 0.4120\n",
      "Iter-600 train loss: 2.1500 valid loss: 2.1511, valid accuracy: 0.4618\n",
      "Iter-700 train loss: 2.1393 valid loss: 2.1257, valid accuracy: 0.4950\n",
      "Iter-800 train loss: 2.0970 valid loss: 2.1006, valid accuracy: 0.5164\n",
      "Iter-900 train loss: 2.1108 valid loss: 2.0751, valid accuracy: 0.5380\n",
      "Iter-1000 train loss: 2.0310 valid loss: 2.0497, valid accuracy: 0.5554\n",
      "Iter-1100 train loss: 1.9877 valid loss: 2.0241, valid accuracy: 0.5708\n",
      "Iter-1200 train loss: 2.0185 valid loss: 1.9984, valid accuracy: 0.5800\n",
      "Iter-1300 train loss: 1.9692 valid loss: 1.9716, valid accuracy: 0.5858\n",
      "Iter-1400 train loss: 1.9401 valid loss: 1.9451, valid accuracy: 0.5938\n",
      "Iter-1500 train loss: 1.8805 valid loss: 1.9185, valid accuracy: 0.6012\n",
      "Iter-1600 train loss: 1.8804 valid loss: 1.8918, valid accuracy: 0.6020\n",
      "Iter-1700 train loss: 1.8488 valid loss: 1.8649, valid accuracy: 0.6058\n",
      "Iter-1800 train loss: 1.9411 valid loss: 1.8379, valid accuracy: 0.6094\n",
      "Iter-1900 train loss: 1.8130 valid loss: 1.8114, valid accuracy: 0.6138\n",
      "Iter-2000 train loss: 1.7906 valid loss: 1.7846, valid accuracy: 0.6162\n",
      "Iter-2100 train loss: 1.8048 valid loss: 1.7576, valid accuracy: 0.6178\n",
      "Iter-2200 train loss: 1.7693 valid loss: 1.7313, valid accuracy: 0.6218\n",
      "Iter-2300 train loss: 1.6894 valid loss: 1.7051, valid accuracy: 0.6250\n",
      "Iter-2400 train loss: 1.7366 valid loss: 1.6790, valid accuracy: 0.6302\n",
      "Iter-2500 train loss: 1.7015 valid loss: 1.6533, valid accuracy: 0.6344\n",
      "Iter-2600 train loss: 1.6869 valid loss: 1.6285, valid accuracy: 0.6390\n",
      "Iter-2700 train loss: 1.6628 valid loss: 1.6038, valid accuracy: 0.6430\n",
      "Iter-2800 train loss: 1.6967 valid loss: 1.5792, valid accuracy: 0.6476\n",
      "Iter-2900 train loss: 1.5708 valid loss: 1.5552, valid accuracy: 0.6496\n",
      "Iter-3000 train loss: 1.4758 valid loss: 1.5315, valid accuracy: 0.6544\n",
      "Iter-3100 train loss: 1.4777 valid loss: 1.5086, valid accuracy: 0.6592\n",
      "Iter-3200 train loss: 1.4561 valid loss: 1.4861, valid accuracy: 0.6658\n",
      "Iter-3300 train loss: 1.4679 valid loss: 1.4635, valid accuracy: 0.6696\n",
      "Iter-3400 train loss: 1.4569 valid loss: 1.4421, valid accuracy: 0.6758\n",
      "Iter-3500 train loss: 1.3830 valid loss: 1.4211, valid accuracy: 0.6802\n",
      "Iter-3600 train loss: 1.4805 valid loss: 1.4003, valid accuracy: 0.6848\n",
      "Iter-3700 train loss: 1.3509 valid loss: 1.3801, valid accuracy: 0.6900\n",
      "Iter-3800 train loss: 1.3727 valid loss: 1.3605, valid accuracy: 0.6946\n",
      "Iter-3900 train loss: 1.3558 valid loss: 1.3412, valid accuracy: 0.6982\n",
      "Iter-4000 train loss: 1.2600 valid loss: 1.3222, valid accuracy: 0.7010\n",
      "Iter-4100 train loss: 1.3056 valid loss: 1.3036, valid accuracy: 0.7048\n",
      "Iter-4200 train loss: 1.3606 valid loss: 1.2855, valid accuracy: 0.7106\n",
      "Iter-4300 train loss: 1.1629 valid loss: 1.2676, valid accuracy: 0.7132\n",
      "Iter-4400 train loss: 1.2688 valid loss: 1.2503, valid accuracy: 0.7174\n",
      "Iter-4500 train loss: 1.3177 valid loss: 1.2332, valid accuracy: 0.7198\n",
      "Iter-4600 train loss: 1.2872 valid loss: 1.2163, valid accuracy: 0.7252\n",
      "Iter-4700 train loss: 1.1847 valid loss: 1.2002, valid accuracy: 0.7288\n",
      "Iter-4800 train loss: 1.1441 valid loss: 1.1843, valid accuracy: 0.7326\n",
      "Iter-4900 train loss: 1.1207 valid loss: 1.1688, valid accuracy: 0.7356\n",
      "Iter-5000 train loss: 1.0934 valid loss: 1.1535, valid accuracy: 0.7394\n",
      "Iter-5100 train loss: 1.1665 valid loss: 1.1385, valid accuracy: 0.7422\n",
      "Iter-5200 train loss: 1.0719 valid loss: 1.1240, valid accuracy: 0.7462\n",
      "Iter-5300 train loss: 1.0732 valid loss: 1.1097, valid accuracy: 0.7506\n",
      "Iter-5400 train loss: 1.1508 valid loss: 1.0957, valid accuracy: 0.7546\n",
      "Iter-5500 train loss: 1.1587 valid loss: 1.0820, valid accuracy: 0.7580\n",
      "Iter-5600 train loss: 1.0867 valid loss: 1.0684, valid accuracy: 0.7608\n",
      "Iter-5700 train loss: 1.1859 valid loss: 1.0553, valid accuracy: 0.7632\n",
      "Iter-5800 train loss: 0.9849 valid loss: 1.0423, valid accuracy: 0.7668\n",
      "Iter-5900 train loss: 1.0573 valid loss: 1.0298, valid accuracy: 0.7702\n",
      "Iter-6000 train loss: 0.9276 valid loss: 1.0174, valid accuracy: 0.7738\n",
      "Iter-6100 train loss: 1.0007 valid loss: 1.0053, valid accuracy: 0.7768\n",
      "Iter-6200 train loss: 1.0035 valid loss: 0.9936, valid accuracy: 0.7786\n",
      "Iter-6300 train loss: 0.9923 valid loss: 0.9820, valid accuracy: 0.7808\n",
      "Iter-6400 train loss: 0.9766 valid loss: 0.9706, valid accuracy: 0.7844\n",
      "Iter-6500 train loss: 0.9714 valid loss: 0.9596, valid accuracy: 0.7868\n",
      "Iter-6600 train loss: 0.8422 valid loss: 0.9488, valid accuracy: 0.7902\n",
      "Iter-6700 train loss: 0.9858 valid loss: 0.9382, valid accuracy: 0.7932\n",
      "Iter-6800 train loss: 0.9919 valid loss: 0.9280, valid accuracy: 0.7958\n",
      "Iter-6900 train loss: 0.9652 valid loss: 0.9178, valid accuracy: 0.7992\n",
      "Iter-7000 train loss: 0.9883 valid loss: 0.9078, valid accuracy: 0.8032\n",
      "Iter-7100 train loss: 1.0061 valid loss: 0.8979, valid accuracy: 0.8060\n",
      "Iter-7200 train loss: 0.9264 valid loss: 0.8885, valid accuracy: 0.8086\n",
      "Iter-7300 train loss: 0.9309 valid loss: 0.8789, valid accuracy: 0.8110\n",
      "Iter-7400 train loss: 0.9085 valid loss: 0.8698, valid accuracy: 0.8138\n",
      "Iter-7500 train loss: 1.0017 valid loss: 0.8608, valid accuracy: 0.8156\n",
      "Iter-7600 train loss: 0.7373 valid loss: 0.8518, valid accuracy: 0.8176\n",
      "Iter-7700 train loss: 0.8496 valid loss: 0.8433, valid accuracy: 0.8194\n",
      "Iter-7800 train loss: 0.8015 valid loss: 0.8347, valid accuracy: 0.8220\n",
      "Iter-7900 train loss: 0.7777 valid loss: 0.8264, valid accuracy: 0.8230\n",
      "Iter-8000 train loss: 0.9239 valid loss: 0.8182, valid accuracy: 0.8266\n",
      "Iter-8100 train loss: 0.8411 valid loss: 0.8102, valid accuracy: 0.8284\n",
      "Iter-8200 train loss: 0.8935 valid loss: 0.8025, valid accuracy: 0.8312\n",
      "Iter-8300 train loss: 0.8198 valid loss: 0.7948, valid accuracy: 0.8326\n",
      "Iter-8400 train loss: 0.9150 valid loss: 0.7871, valid accuracy: 0.8344\n",
      "Iter-8500 train loss: 0.9558 valid loss: 0.7798, valid accuracy: 0.8372\n",
      "Iter-8600 train loss: 0.7188 valid loss: 0.7728, valid accuracy: 0.8372\n",
      "Iter-8700 train loss: 0.6267 valid loss: 0.7656, valid accuracy: 0.8392\n",
      "Iter-8800 train loss: 0.8826 valid loss: 0.7585, valid accuracy: 0.8418\n",
      "Iter-8900 train loss: 0.7062 valid loss: 0.7519, valid accuracy: 0.8422\n",
      "Iter-9000 train loss: 0.8773 valid loss: 0.7454, valid accuracy: 0.8428\n",
      "Iter-9100 train loss: 0.8520 valid loss: 0.7390, valid accuracy: 0.8446\n",
      "Iter-9200 train loss: 0.6714 valid loss: 0.7325, valid accuracy: 0.8452\n",
      "Iter-9300 train loss: 0.5944 valid loss: 0.7263, valid accuracy: 0.8472\n",
      "Iter-9400 train loss: 0.6822 valid loss: 0.7203, valid accuracy: 0.8482\n",
      "Iter-9500 train loss: 0.6801 valid loss: 0.7142, valid accuracy: 0.8500\n",
      "Iter-9600 train loss: 0.6067 valid loss: 0.7083, valid accuracy: 0.8514\n",
      "Iter-9700 train loss: 0.6673 valid loss: 0.7026, valid accuracy: 0.8528\n",
      "Iter-9800 train loss: 0.8289 valid loss: 0.6970, valid accuracy: 0.8528\n",
      "Iter-9900 train loss: 0.5317 valid loss: 0.6917, valid accuracy: 0.8530\n",
      "Iter-10000 train loss: 0.6044 valid loss: 0.6863, valid accuracy: 0.8534\n",
      "Iter-10100 train loss: 0.6560 valid loss: 0.6810, valid accuracy: 0.8538\n",
      "Iter-10200 train loss: 0.7279 valid loss: 0.6758, valid accuracy: 0.8552\n",
      "Iter-10300 train loss: 0.7822 valid loss: 0.6706, valid accuracy: 0.8564\n",
      "Iter-10400 train loss: 0.7491 valid loss: 0.6657, valid accuracy: 0.8572\n",
      "Iter-10500 train loss: 0.7187 valid loss: 0.6609, valid accuracy: 0.8572\n",
      "Iter-10600 train loss: 0.6149 valid loss: 0.6561, valid accuracy: 0.8594\n",
      "Iter-10700 train loss: 0.6780 valid loss: 0.6514, valid accuracy: 0.8602\n",
      "Iter-10800 train loss: 0.7636 valid loss: 0.6469, valid accuracy: 0.8604\n",
      "Iter-10900 train loss: 0.6632 valid loss: 0.6425, valid accuracy: 0.8606\n",
      "Iter-11000 train loss: 0.5968 valid loss: 0.6381, valid accuracy: 0.8612\n",
      "Iter-11100 train loss: 0.7713 valid loss: 0.6337, valid accuracy: 0.8628\n",
      "Iter-11200 train loss: 0.5679 valid loss: 0.6292, valid accuracy: 0.8632\n",
      "Iter-11300 train loss: 0.7023 valid loss: 0.6249, valid accuracy: 0.8646\n",
      "Iter-11400 train loss: 0.8263 valid loss: 0.6208, valid accuracy: 0.8664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 0.6516 valid loss: 0.6166, valid accuracy: 0.8666\n",
      "Iter-11600 train loss: 0.4534 valid loss: 0.6127, valid accuracy: 0.8680\n",
      "Iter-11700 train loss: 0.5392 valid loss: 0.6086, valid accuracy: 0.8688\n",
      "Iter-11800 train loss: 0.5684 valid loss: 0.6048, valid accuracy: 0.8688\n",
      "Iter-11900 train loss: 0.8121 valid loss: 0.6010, valid accuracy: 0.8696\n",
      "Iter-12000 train loss: 0.4991 valid loss: 0.5974, valid accuracy: 0.8704\n",
      "Iter-12100 train loss: 0.7544 valid loss: 0.5937, valid accuracy: 0.8716\n",
      "Iter-12200 train loss: 0.4547 valid loss: 0.5900, valid accuracy: 0.8722\n",
      "Iter-12300 train loss: 0.6180 valid loss: 0.5865, valid accuracy: 0.8728\n",
      "Iter-12400 train loss: 0.6942 valid loss: 0.5829, valid accuracy: 0.8734\n",
      "Iter-12500 train loss: 0.6241 valid loss: 0.5795, valid accuracy: 0.8734\n",
      "Iter-12600 train loss: 0.5048 valid loss: 0.5763, valid accuracy: 0.8738\n",
      "Iter-12700 train loss: 0.5702 valid loss: 0.5731, valid accuracy: 0.8750\n",
      "Iter-12800 train loss: 0.6497 valid loss: 0.5700, valid accuracy: 0.8744\n",
      "Iter-12900 train loss: 0.5878 valid loss: 0.5669, valid accuracy: 0.8754\n",
      "Iter-13000 train loss: 0.4428 valid loss: 0.5638, valid accuracy: 0.8760\n",
      "Iter-13100 train loss: 0.3922 valid loss: 0.5607, valid accuracy: 0.8770\n",
      "Iter-13200 train loss: 0.4463 valid loss: 0.5576, valid accuracy: 0.8766\n",
      "Iter-13300 train loss: 0.4537 valid loss: 0.5546, valid accuracy: 0.8770\n",
      "Iter-13400 train loss: 0.5816 valid loss: 0.5515, valid accuracy: 0.8766\n",
      "Iter-13500 train loss: 0.5141 valid loss: 0.5486, valid accuracy: 0.8768\n",
      "Iter-13600 train loss: 0.5395 valid loss: 0.5459, valid accuracy: 0.8780\n",
      "Iter-13700 train loss: 0.4628 valid loss: 0.5431, valid accuracy: 0.8778\n",
      "Iter-13800 train loss: 0.6367 valid loss: 0.5403, valid accuracy: 0.8782\n",
      "Iter-13900 train loss: 0.5848 valid loss: 0.5376, valid accuracy: 0.8800\n",
      "Iter-14000 train loss: 0.6964 valid loss: 0.5350, valid accuracy: 0.8798\n",
      "Iter-14100 train loss: 0.7711 valid loss: 0.5325, valid accuracy: 0.8812\n",
      "Iter-14200 train loss: 0.7393 valid loss: 0.5298, valid accuracy: 0.8812\n",
      "Iter-14300 train loss: 0.5262 valid loss: 0.5272, valid accuracy: 0.8812\n",
      "Iter-14400 train loss: 0.6140 valid loss: 0.5246, valid accuracy: 0.8810\n",
      "Iter-14500 train loss: 0.7740 valid loss: 0.5222, valid accuracy: 0.8810\n",
      "Iter-14600 train loss: 0.5418 valid loss: 0.5197, valid accuracy: 0.8816\n",
      "Iter-14700 train loss: 0.3653 valid loss: 0.5174, valid accuracy: 0.8820\n",
      "Iter-14800 train loss: 0.4969 valid loss: 0.5149, valid accuracy: 0.8826\n",
      "Iter-14900 train loss: 0.4193 valid loss: 0.5128, valid accuracy: 0.8826\n",
      "Iter-15000 train loss: 0.6524 valid loss: 0.5106, valid accuracy: 0.8828\n",
      "Iter-15100 train loss: 0.4628 valid loss: 0.5085, valid accuracy: 0.8836\n",
      "Iter-15200 train loss: 0.7028 valid loss: 0.5061, valid accuracy: 0.8838\n",
      "Iter-15300 train loss: 0.5901 valid loss: 0.5037, valid accuracy: 0.8838\n",
      "Iter-15400 train loss: 0.5624 valid loss: 0.5015, valid accuracy: 0.8836\n",
      "Iter-15500 train loss: 0.5406 valid loss: 0.4992, valid accuracy: 0.8838\n",
      "Iter-15600 train loss: 0.6146 valid loss: 0.4970, valid accuracy: 0.8846\n",
      "Iter-15700 train loss: 0.4934 valid loss: 0.4949, valid accuracy: 0.8848\n",
      "Iter-15800 train loss: 0.4498 valid loss: 0.4930, valid accuracy: 0.8850\n",
      "Iter-15900 train loss: 0.5507 valid loss: 0.4909, valid accuracy: 0.8850\n",
      "Iter-16000 train loss: 0.4425 valid loss: 0.4889, valid accuracy: 0.8856\n",
      "Iter-16100 train loss: 0.4359 valid loss: 0.4869, valid accuracy: 0.8860\n",
      "Iter-16200 train loss: 0.4685 valid loss: 0.4849, valid accuracy: 0.8862\n",
      "Iter-16300 train loss: 0.4564 valid loss: 0.4830, valid accuracy: 0.8864\n",
      "Iter-16400 train loss: 0.5229 valid loss: 0.4810, valid accuracy: 0.8856\n",
      "Iter-16500 train loss: 0.3708 valid loss: 0.4790, valid accuracy: 0.8866\n",
      "Iter-16600 train loss: 0.5885 valid loss: 0.4771, valid accuracy: 0.8868\n",
      "Iter-16700 train loss: 0.4545 valid loss: 0.4753, valid accuracy: 0.8868\n",
      "Iter-16800 train loss: 0.4452 valid loss: 0.4735, valid accuracy: 0.8872\n",
      "Iter-16900 train loss: 0.4942 valid loss: 0.4717, valid accuracy: 0.8878\n",
      "Iter-17000 train loss: 0.6165 valid loss: 0.4700, valid accuracy: 0.8876\n",
      "Iter-17100 train loss: 0.3987 valid loss: 0.4683, valid accuracy: 0.8880\n",
      "Iter-17200 train loss: 0.5910 valid loss: 0.4665, valid accuracy: 0.8880\n",
      "Iter-17300 train loss: 0.4764 valid loss: 0.4648, valid accuracy: 0.8880\n",
      "Iter-17400 train loss: 0.3554 valid loss: 0.4632, valid accuracy: 0.8884\n",
      "Iter-17500 train loss: 0.6987 valid loss: 0.4617, valid accuracy: 0.8894\n",
      "Iter-17600 train loss: 0.6399 valid loss: 0.4598, valid accuracy: 0.8894\n",
      "Iter-17700 train loss: 0.3720 valid loss: 0.4581, valid accuracy: 0.8898\n",
      "Iter-17800 train loss: 0.4328 valid loss: 0.4563, valid accuracy: 0.8902\n",
      "Iter-17900 train loss: 0.5108 valid loss: 0.4548, valid accuracy: 0.8908\n",
      "Iter-18000 train loss: 0.5767 valid loss: 0.4532, valid accuracy: 0.8912\n",
      "Iter-18100 train loss: 0.3022 valid loss: 0.4518, valid accuracy: 0.8914\n",
      "Iter-18200 train loss: 0.5200 valid loss: 0.4501, valid accuracy: 0.8914\n",
      "Iter-18300 train loss: 0.3862 valid loss: 0.4485, valid accuracy: 0.8920\n",
      "Iter-18400 train loss: 0.5639 valid loss: 0.4469, valid accuracy: 0.8912\n",
      "Iter-18500 train loss: 0.4977 valid loss: 0.4454, valid accuracy: 0.8924\n",
      "Iter-18600 train loss: 0.5420 valid loss: 0.4440, valid accuracy: 0.8918\n",
      "Iter-18700 train loss: 0.7124 valid loss: 0.4425, valid accuracy: 0.8918\n",
      "Iter-18800 train loss: 0.5627 valid loss: 0.4412, valid accuracy: 0.8918\n",
      "Iter-18900 train loss: 0.4384 valid loss: 0.4398, valid accuracy: 0.8932\n",
      "Iter-19000 train loss: 0.4971 valid loss: 0.4383, valid accuracy: 0.8934\n",
      "Iter-19100 train loss: 0.3618 valid loss: 0.4371, valid accuracy: 0.8942\n",
      "Iter-19200 train loss: 0.6244 valid loss: 0.4356, valid accuracy: 0.8942\n",
      "Iter-19300 train loss: 0.3187 valid loss: 0.4343, valid accuracy: 0.8948\n",
      "Iter-19400 train loss: 0.3917 valid loss: 0.4330, valid accuracy: 0.8946\n",
      "Iter-19500 train loss: 0.4010 valid loss: 0.4316, valid accuracy: 0.8956\n",
      "Iter-19600 train loss: 0.4304 valid loss: 0.4304, valid accuracy: 0.8964\n",
      "Iter-19700 train loss: 0.4504 valid loss: 0.4291, valid accuracy: 0.8964\n",
      "Iter-19800 train loss: 0.6391 valid loss: 0.4278, valid accuracy: 0.8968\n",
      "Iter-19900 train loss: 0.4624 valid loss: 0.4265, valid accuracy: 0.8970\n",
      "Iter-20000 train loss: 0.4907 valid loss: 0.4252, valid accuracy: 0.8974\n",
      "Iter-20100 train loss: 0.5054 valid loss: 0.4240, valid accuracy: 0.8972\n",
      "Iter-20200 train loss: 0.5800 valid loss: 0.4228, valid accuracy: 0.8970\n",
      "Iter-20300 train loss: 0.3756 valid loss: 0.4217, valid accuracy: 0.8976\n",
      "Iter-20400 train loss: 0.4446 valid loss: 0.4206, valid accuracy: 0.8976\n",
      "Iter-20500 train loss: 0.4073 valid loss: 0.4194, valid accuracy: 0.8980\n",
      "Iter-20600 train loss: 0.3922 valid loss: 0.4181, valid accuracy: 0.8982\n",
      "Iter-20700 train loss: 0.4325 valid loss: 0.4170, valid accuracy: 0.8986\n",
      "Iter-20800 train loss: 0.4089 valid loss: 0.4159, valid accuracy: 0.8990\n",
      "Iter-20900 train loss: 0.3849 valid loss: 0.4147, valid accuracy: 0.8990\n",
      "Iter-21000 train loss: 0.4879 valid loss: 0.4137, valid accuracy: 0.8998\n",
      "Iter-21100 train loss: 0.4738 valid loss: 0.4126, valid accuracy: 0.8998\n",
      "Iter-21200 train loss: 0.5066 valid loss: 0.4116, valid accuracy: 0.9000\n",
      "Iter-21300 train loss: 0.4696 valid loss: 0.4104, valid accuracy: 0.9002\n",
      "Iter-21400 train loss: 0.2646 valid loss: 0.4094, valid accuracy: 0.9000\n",
      "Iter-21500 train loss: 0.3961 valid loss: 0.4083, valid accuracy: 0.9006\n",
      "Iter-21600 train loss: 0.6287 valid loss: 0.4074, valid accuracy: 0.9004\n",
      "Iter-21700 train loss: 0.5248 valid loss: 0.4064, valid accuracy: 0.9004\n",
      "Iter-21800 train loss: 0.5746 valid loss: 0.4052, valid accuracy: 0.9010\n",
      "Iter-21900 train loss: 0.3468 valid loss: 0.4041, valid accuracy: 0.9010\n",
      "Iter-22000 train loss: 0.2886 valid loss: 0.4031, valid accuracy: 0.9012\n",
      "Iter-22100 train loss: 0.3553 valid loss: 0.4021, valid accuracy: 0.9014\n",
      "Iter-22200 train loss: 0.3847 valid loss: 0.4009, valid accuracy: 0.9016\n",
      "Iter-22300 train loss: 0.4176 valid loss: 0.3998, valid accuracy: 0.9016\n",
      "Iter-22400 train loss: 0.3225 valid loss: 0.3990, valid accuracy: 0.9020\n",
      "Iter-22500 train loss: 0.3095 valid loss: 0.3982, valid accuracy: 0.9018\n",
      "Iter-22600 train loss: 0.5191 valid loss: 0.3972, valid accuracy: 0.9020\n",
      "Iter-22700 train loss: 0.4023 valid loss: 0.3962, valid accuracy: 0.9020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 0.3992 valid loss: 0.3953, valid accuracy: 0.9024\n",
      "Iter-22900 train loss: 0.4634 valid loss: 0.3944, valid accuracy: 0.9024\n",
      "Iter-23000 train loss: 0.2536 valid loss: 0.3933, valid accuracy: 0.9028\n",
      "Iter-23100 train loss: 0.5096 valid loss: 0.3924, valid accuracy: 0.9030\n",
      "Iter-23200 train loss: 0.3212 valid loss: 0.3914, valid accuracy: 0.9034\n",
      "Iter-23300 train loss: 0.4637 valid loss: 0.3905, valid accuracy: 0.9034\n",
      "Iter-23400 train loss: 0.4691 valid loss: 0.3896, valid accuracy: 0.9036\n",
      "Iter-23500 train loss: 0.3599 valid loss: 0.3887, valid accuracy: 0.9038\n",
      "Iter-23600 train loss: 0.4547 valid loss: 0.3880, valid accuracy: 0.9040\n",
      "Iter-23700 train loss: 0.4513 valid loss: 0.3871, valid accuracy: 0.9040\n",
      "Iter-23800 train loss: 0.4671 valid loss: 0.3862, valid accuracy: 0.9042\n",
      "Iter-23900 train loss: 0.6746 valid loss: 0.3853, valid accuracy: 0.9042\n",
      "Iter-24000 train loss: 0.4748 valid loss: 0.3845, valid accuracy: 0.9042\n",
      "Iter-24100 train loss: 0.2986 valid loss: 0.3836, valid accuracy: 0.9042\n",
      "Iter-24200 train loss: 0.4089 valid loss: 0.3829, valid accuracy: 0.9042\n",
      "Iter-24300 train loss: 0.3541 valid loss: 0.3821, valid accuracy: 0.9038\n",
      "Iter-24400 train loss: 0.3288 valid loss: 0.3812, valid accuracy: 0.9040\n",
      "Iter-24500 train loss: 0.3855 valid loss: 0.3805, valid accuracy: 0.9042\n",
      "Iter-24600 train loss: 0.2708 valid loss: 0.3798, valid accuracy: 0.9050\n",
      "Iter-24700 train loss: 0.2749 valid loss: 0.3790, valid accuracy: 0.9046\n",
      "Iter-24800 train loss: 0.3733 valid loss: 0.3782, valid accuracy: 0.9050\n",
      "Iter-24900 train loss: 0.5373 valid loss: 0.3775, valid accuracy: 0.9054\n",
      "Iter-25000 train loss: 0.2679 valid loss: 0.3767, valid accuracy: 0.9054\n",
      "Iter-25100 train loss: 0.4747 valid loss: 0.3758, valid accuracy: 0.9054\n",
      "Iter-25200 train loss: 0.5207 valid loss: 0.3749, valid accuracy: 0.9056\n",
      "Iter-25300 train loss: 0.4500 valid loss: 0.3743, valid accuracy: 0.9060\n",
      "Iter-25400 train loss: 0.3469 valid loss: 0.3734, valid accuracy: 0.9060\n",
      "Iter-25500 train loss: 0.2857 valid loss: 0.3726, valid accuracy: 0.9066\n",
      "Iter-25600 train loss: 0.2327 valid loss: 0.3718, valid accuracy: 0.9068\n",
      "Iter-25700 train loss: 0.4761 valid loss: 0.3709, valid accuracy: 0.9074\n",
      "Iter-25800 train loss: 0.2622 valid loss: 0.3702, valid accuracy: 0.9078\n",
      "Iter-25900 train loss: 0.2497 valid loss: 0.3694, valid accuracy: 0.9076\n",
      "Iter-26000 train loss: 0.2520 valid loss: 0.3688, valid accuracy: 0.9082\n",
      "Iter-26100 train loss: 0.5792 valid loss: 0.3678, valid accuracy: 0.9080\n",
      "Iter-26200 train loss: 0.4251 valid loss: 0.3672, valid accuracy: 0.9082\n",
      "Iter-26300 train loss: 0.6276 valid loss: 0.3666, valid accuracy: 0.9082\n",
      "Iter-26400 train loss: 0.4770 valid loss: 0.3661, valid accuracy: 0.9084\n",
      "Iter-26500 train loss: 0.4200 valid loss: 0.3653, valid accuracy: 0.9080\n",
      "Iter-26600 train loss: 0.3906 valid loss: 0.3646, valid accuracy: 0.9078\n",
      "Iter-26700 train loss: 0.3754 valid loss: 0.3638, valid accuracy: 0.9086\n",
      "Iter-26800 train loss: 0.4653 valid loss: 0.3630, valid accuracy: 0.9082\n",
      "Iter-26900 train loss: 0.3511 valid loss: 0.3624, valid accuracy: 0.9084\n",
      "Iter-27000 train loss: 0.3683 valid loss: 0.3618, valid accuracy: 0.9088\n",
      "Iter-27100 train loss: 0.3547 valid loss: 0.3613, valid accuracy: 0.9082\n",
      "Iter-27200 train loss: 0.3328 valid loss: 0.3606, valid accuracy: 0.9082\n",
      "Iter-27300 train loss: 0.4803 valid loss: 0.3601, valid accuracy: 0.9086\n",
      "Iter-27400 train loss: 0.4784 valid loss: 0.3595, valid accuracy: 0.9086\n",
      "Iter-27500 train loss: 0.3288 valid loss: 0.3590, valid accuracy: 0.9090\n",
      "Iter-27600 train loss: 0.3118 valid loss: 0.3584, valid accuracy: 0.9086\n",
      "Iter-27700 train loss: 0.2055 valid loss: 0.3578, valid accuracy: 0.9090\n",
      "Iter-27800 train loss: 0.4566 valid loss: 0.3572, valid accuracy: 0.9088\n",
      "Iter-27900 train loss: 0.3336 valid loss: 0.3566, valid accuracy: 0.9092\n",
      "Iter-28000 train loss: 0.4031 valid loss: 0.3560, valid accuracy: 0.9092\n",
      "Iter-28100 train loss: 0.3332 valid loss: 0.3551, valid accuracy: 0.9090\n",
      "Iter-28200 train loss: 0.5248 valid loss: 0.3547, valid accuracy: 0.9090\n",
      "Iter-28300 train loss: 0.3954 valid loss: 0.3541, valid accuracy: 0.9092\n",
      "Iter-28400 train loss: 0.4163 valid loss: 0.3535, valid accuracy: 0.9100\n",
      "Iter-28500 train loss: 0.2446 valid loss: 0.3528, valid accuracy: 0.9100\n",
      "Iter-28600 train loss: 0.2516 valid loss: 0.3523, valid accuracy: 0.9100\n",
      "Iter-28700 train loss: 0.4427 valid loss: 0.3518, valid accuracy: 0.9106\n",
      "Iter-28800 train loss: 0.3215 valid loss: 0.3513, valid accuracy: 0.9106\n",
      "Iter-28900 train loss: 0.3724 valid loss: 0.3506, valid accuracy: 0.9110\n",
      "Iter-29000 train loss: 0.2971 valid loss: 0.3501, valid accuracy: 0.9104\n",
      "Iter-29100 train loss: 0.5167 valid loss: 0.3496, valid accuracy: 0.9104\n",
      "Iter-29200 train loss: 0.3419 valid loss: 0.3489, valid accuracy: 0.9110\n",
      "Iter-29300 train loss: 0.6042 valid loss: 0.3483, valid accuracy: 0.9116\n",
      "Iter-29400 train loss: 0.3404 valid loss: 0.3477, valid accuracy: 0.9114\n",
      "Iter-29500 train loss: 0.3697 valid loss: 0.3473, valid accuracy: 0.9120\n",
      "Iter-29600 train loss: 0.1411 valid loss: 0.3466, valid accuracy: 0.9118\n",
      "Iter-29700 train loss: 0.4086 valid loss: 0.3461, valid accuracy: 0.9118\n",
      "Iter-29800 train loss: 0.3051 valid loss: 0.3455, valid accuracy: 0.9120\n",
      "Iter-29900 train loss: 0.3459 valid loss: 0.3449, valid accuracy: 0.9122\n",
      "Iter-30000 train loss: 0.2971 valid loss: 0.3441, valid accuracy: 0.9126\n",
      "Iter-30100 train loss: 0.4769 valid loss: 0.3436, valid accuracy: 0.9126\n",
      "Iter-30200 train loss: 0.3495 valid loss: 0.3433, valid accuracy: 0.9118\n",
      "Iter-30300 train loss: 0.5347 valid loss: 0.3428, valid accuracy: 0.9120\n",
      "Iter-30400 train loss: 0.5495 valid loss: 0.3423, valid accuracy: 0.9124\n",
      "Iter-30500 train loss: 0.6600 valid loss: 0.3417, valid accuracy: 0.9122\n",
      "Iter-30600 train loss: 0.4284 valid loss: 0.3412, valid accuracy: 0.9122\n",
      "Iter-30700 train loss: 0.3005 valid loss: 0.3407, valid accuracy: 0.9122\n",
      "Iter-30800 train loss: 0.5952 valid loss: 0.3401, valid accuracy: 0.9124\n",
      "Iter-30900 train loss: 0.3130 valid loss: 0.3395, valid accuracy: 0.9122\n",
      "Iter-31000 train loss: 0.1842 valid loss: 0.3389, valid accuracy: 0.9128\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
