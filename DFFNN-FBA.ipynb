{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        # self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "                 b=np.zeros((1, H)))\n",
    "        #         low, high = (-1. / np.sqrt(D / 2.)), (+1. / np.sqrt(D / 2.))\n",
    "        #         m = dict(W=np.random.uniform(size=(D, H), low=low, high=high),\n",
    "        #                  b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        #         m = np.random.uniform(size=(D, H), low=low, high=high)\n",
    "        m = np.random.randn(D, H) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m = dict(W=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "                 b=np.zeros((1, H)))\n",
    "        #         low, high = (-1. / np.sqrt(H / 2.)), (+1. / np.sqrt(H / 2.))\n",
    "        #         m = dict(W=np.random.uniform(size=(H, H), low=low, high=high),\n",
    "        #                  b=np.zeros((1, H)))\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        #         m = np.random.uniform(size=(H, H), low=low, high=high)\n",
    "        m = np.random.randn(H, H) / np.sqrt(H / 2.)\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "                 b=np.zeros((1, C)))\n",
    "        #         low, high = (-1. / np.sqrt(H / 2.)), (+1. / np.sqrt(H / 2.))\n",
    "        #         m = dict(W=np.random.uniform(size=(H, C), low=low, high=high),\n",
    "        #                  b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        #         m = np.random.uniform(size=(H, C), low=low, high=high)\n",
    "        m = np.random.randn(H, C) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = selu_forward(X=y)\n",
    "        X = y.copy() # pass the previous output to the next layer\n",
    "        caches.append((fc_cache, nl_cache)) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = selu_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            nl_caches.append(nl_cache)\n",
    "        caches.append((fc_caches, nl_caches)) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass it to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = selu_backward(dout=dy, cache=nl_caches[layer])\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = selu_backward(dout=dy, cache=nl_cache)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Momentums\n",
    "        M, R = [], []\n",
    "\n",
    "        # Input layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers momentum\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        # Output layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    " \n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "                # M[0][key] = l.exp_running_avg(M[0][key], grads[0][key], beta1)\n",
    "                # R[0][key] = l.exp_running_avg(R[0][key], grads[0][key]**2, beta2)\n",
    "                # m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                # r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "                # self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "                    # M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grads[1][layer][key], beta1)\n",
    "                    # R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grads[1][layer][key]**2, beta2)\n",
    "                    # m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    # r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "                    # self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                # M[2][key] = l.exp_running_avg(M[2][key], grads[2][key], beta1)\n",
    "                # R[2][key] = l.exp_running_avg(R[2][key], grads[2][key]**2, beta2)\n",
    "                # m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                # r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "                # self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.8678 valid loss: 2.9427, valid accuracy: 0.0498\n",
      "Iter-20 train loss: 2.8740 valid loss: 2.9321, valid accuracy: 0.0516\n",
      "Iter-30 train loss: 2.8428 valid loss: 2.9205, valid accuracy: 0.0524\n",
      "Iter-40 train loss: 2.9644 valid loss: 2.9108, valid accuracy: 0.0526\n",
      "Iter-50 train loss: 2.8272 valid loss: 2.8996, valid accuracy: 0.0526\n",
      "Iter-60 train loss: 3.0085 valid loss: 2.8892, valid accuracy: 0.0530\n",
      "Iter-70 train loss: 2.8316 valid loss: 2.8803, valid accuracy: 0.0526\n",
      "Iter-80 train loss: 2.9764 valid loss: 2.8718, valid accuracy: 0.0528\n",
      "Iter-90 train loss: 2.6769 valid loss: 2.8647, valid accuracy: 0.0534\n",
      "Iter-100 train loss: 2.8713 valid loss: 2.8566, valid accuracy: 0.0530\n",
      "Iter-110 train loss: 2.7562 valid loss: 2.8502, valid accuracy: 0.0534\n",
      "Iter-120 train loss: 2.8034 valid loss: 2.8467, valid accuracy: 0.0522\n",
      "Iter-130 train loss: 2.9874 valid loss: 2.8380, valid accuracy: 0.0526\n",
      "Iter-140 train loss: 2.9532 valid loss: 2.8310, valid accuracy: 0.0518\n",
      "Iter-150 train loss: 2.8315 valid loss: 2.8236, valid accuracy: 0.0516\n",
      "Iter-160 train loss: 2.7872 valid loss: 2.8171, valid accuracy: 0.0536\n",
      "Iter-170 train loss: 2.7833 valid loss: 2.8117, valid accuracy: 0.0540\n",
      "Iter-180 train loss: 2.6807 valid loss: 2.8042, valid accuracy: 0.0544\n",
      "Iter-190 train loss: 2.8764 valid loss: 2.7952, valid accuracy: 0.0550\n",
      "Iter-200 train loss: 2.7452 valid loss: 2.7886, valid accuracy: 0.0564\n",
      "Iter-210 train loss: 2.7490 valid loss: 2.7807, valid accuracy: 0.0584\n",
      "Iter-220 train loss: 2.6819 valid loss: 2.7725, valid accuracy: 0.0598\n",
      "Iter-230 train loss: 2.6424 valid loss: 2.7621, valid accuracy: 0.0612\n",
      "Iter-240 train loss: 2.9908 valid loss: 2.7514, valid accuracy: 0.0618\n",
      "Iter-250 train loss: 2.6465 valid loss: 2.7419, valid accuracy: 0.0618\n",
      "Iter-260 train loss: 2.5069 valid loss: 2.7315, valid accuracy: 0.0644\n",
      "Iter-270 train loss: 2.7300 valid loss: 2.7202, valid accuracy: 0.0660\n",
      "Iter-280 train loss: 2.9636 valid loss: 2.7087, valid accuracy: 0.0666\n",
      "Iter-290 train loss: 2.6159 valid loss: 2.6980, valid accuracy: 0.0672\n",
      "Iter-300 train loss: 2.5677 valid loss: 2.6865, valid accuracy: 0.0680\n",
      "Iter-310 train loss: 2.6149 valid loss: 2.6752, valid accuracy: 0.0700\n",
      "Iter-320 train loss: 2.8099 valid loss: 2.6621, valid accuracy: 0.0744\n",
      "Iter-330 train loss: 2.6432 valid loss: 2.6500, valid accuracy: 0.0756\n",
      "Iter-340 train loss: 2.6091 valid loss: 2.6385, valid accuracy: 0.0772\n",
      "Iter-350 train loss: 2.6171 valid loss: 2.6272, valid accuracy: 0.0788\n",
      "Iter-360 train loss: 2.4771 valid loss: 2.6153, valid accuracy: 0.0816\n",
      "Iter-370 train loss: 2.7940 valid loss: 2.6022, valid accuracy: 0.0842\n",
      "Iter-380 train loss: 2.6034 valid loss: 2.5888, valid accuracy: 0.0858\n",
      "Iter-390 train loss: 2.5788 valid loss: 2.5763, valid accuracy: 0.0888\n",
      "Iter-400 train loss: 2.5116 valid loss: 2.5633, valid accuracy: 0.0914\n",
      "Iter-410 train loss: 2.5069 valid loss: 2.5507, valid accuracy: 0.0954\n",
      "Iter-420 train loss: 2.5633 valid loss: 2.5367, valid accuracy: 0.0986\n",
      "Iter-430 train loss: 2.4750 valid loss: 2.5257, valid accuracy: 0.1006\n",
      "Iter-440 train loss: 2.6696 valid loss: 2.5113, valid accuracy: 0.1022\n",
      "Iter-450 train loss: 2.3946 valid loss: 2.5000, valid accuracy: 0.1054\n",
      "Iter-460 train loss: 2.6243 valid loss: 2.4878, valid accuracy: 0.1084\n",
      "Iter-470 train loss: 2.5893 valid loss: 2.4744, valid accuracy: 0.1104\n",
      "Iter-480 train loss: 2.6456 valid loss: 2.4631, valid accuracy: 0.1122\n",
      "Iter-490 train loss: 2.5144 valid loss: 2.4516, valid accuracy: 0.1156\n",
      "Iter-500 train loss: 2.4016 valid loss: 2.4389, valid accuracy: 0.1202\n",
      "Iter-510 train loss: 2.2861 valid loss: 2.4265, valid accuracy: 0.1212\n",
      "Iter-520 train loss: 2.3410 valid loss: 2.4150, valid accuracy: 0.1230\n",
      "Iter-530 train loss: 2.6173 valid loss: 2.4046, valid accuracy: 0.1264\n",
      "Iter-540 train loss: 2.3048 valid loss: 2.3939, valid accuracy: 0.1292\n",
      "Iter-550 train loss: 2.4000 valid loss: 2.3822, valid accuracy: 0.1314\n",
      "Iter-560 train loss: 2.2188 valid loss: 2.3714, valid accuracy: 0.1328\n",
      "Iter-570 train loss: 2.3670 valid loss: 2.3613, valid accuracy: 0.1358\n",
      "Iter-580 train loss: 2.3290 valid loss: 2.3511, valid accuracy: 0.1392\n",
      "Iter-590 train loss: 2.4503 valid loss: 2.3405, valid accuracy: 0.1420\n",
      "Iter-600 train loss: 2.4454 valid loss: 2.3300, valid accuracy: 0.1452\n",
      "Iter-610 train loss: 2.4516 valid loss: 2.3185, valid accuracy: 0.1464\n",
      "Iter-620 train loss: 2.3412 valid loss: 2.3088, valid accuracy: 0.1500\n",
      "Iter-630 train loss: 2.3050 valid loss: 2.2975, valid accuracy: 0.1524\n",
      "Iter-640 train loss: 2.2074 valid loss: 2.2878, valid accuracy: 0.1552\n",
      "Iter-650 train loss: 2.3557 valid loss: 2.2770, valid accuracy: 0.1572\n",
      "Iter-660 train loss: 2.3926 valid loss: 2.2679, valid accuracy: 0.1602\n",
      "Iter-670 train loss: 2.3580 valid loss: 2.2587, valid accuracy: 0.1624\n",
      "Iter-680 train loss: 2.2335 valid loss: 2.2488, valid accuracy: 0.1646\n",
      "Iter-690 train loss: 2.0510 valid loss: 2.2394, valid accuracy: 0.1684\n",
      "Iter-700 train loss: 2.1848 valid loss: 2.2296, valid accuracy: 0.1706\n",
      "Iter-710 train loss: 2.2306 valid loss: 2.2196, valid accuracy: 0.1730\n",
      "Iter-720 train loss: 2.1315 valid loss: 2.2088, valid accuracy: 0.1742\n",
      "Iter-730 train loss: 2.0782 valid loss: 2.1996, valid accuracy: 0.1768\n",
      "Iter-740 train loss: 2.1091 valid loss: 2.1906, valid accuracy: 0.1800\n",
      "Iter-750 train loss: 2.1966 valid loss: 2.1811, valid accuracy: 0.1828\n",
      "Iter-760 train loss: 2.1325 valid loss: 2.1730, valid accuracy: 0.1858\n",
      "Iter-770 train loss: 2.2496 valid loss: 2.1633, valid accuracy: 0.1882\n",
      "Iter-780 train loss: 2.2027 valid loss: 2.1537, valid accuracy: 0.1910\n",
      "Iter-790 train loss: 2.2706 valid loss: 2.1441, valid accuracy: 0.1918\n",
      "Iter-800 train loss: 2.1957 valid loss: 2.1355, valid accuracy: 0.1936\n",
      "Iter-810 train loss: 2.1894 valid loss: 2.1254, valid accuracy: 0.1960\n",
      "Iter-820 train loss: 1.9358 valid loss: 2.1150, valid accuracy: 0.1992\n",
      "Iter-830 train loss: 2.1317 valid loss: 2.1052, valid accuracy: 0.2028\n",
      "Iter-840 train loss: 2.1310 valid loss: 2.0955, valid accuracy: 0.2044\n",
      "Iter-850 train loss: 1.9786 valid loss: 2.0862, valid accuracy: 0.2074\n",
      "Iter-860 train loss: 2.0425 valid loss: 2.0774, valid accuracy: 0.2120\n",
      "Iter-870 train loss: 1.7961 valid loss: 2.0676, valid accuracy: 0.2152\n",
      "Iter-880 train loss: 1.9531 valid loss: 2.0577, valid accuracy: 0.2160\n",
      "Iter-890 train loss: 2.0068 valid loss: 2.0486, valid accuracy: 0.2170\n",
      "Iter-900 train loss: 2.0737 valid loss: 2.0408, valid accuracy: 0.2208\n",
      "Iter-910 train loss: 2.0218 valid loss: 2.0321, valid accuracy: 0.2234\n",
      "Iter-920 train loss: 2.0341 valid loss: 2.0229, valid accuracy: 0.2276\n",
      "Iter-930 train loss: 2.0287 valid loss: 2.0121, valid accuracy: 0.2312\n",
      "Iter-940 train loss: 1.8343 valid loss: 2.0021, valid accuracy: 0.2354\n",
      "Iter-950 train loss: 2.1254 valid loss: 1.9926, valid accuracy: 0.2390\n",
      "Iter-960 train loss: 1.7944 valid loss: 1.9829, valid accuracy: 0.2438\n",
      "Iter-970 train loss: 2.0707 valid loss: 1.9739, valid accuracy: 0.2470\n",
      "Iter-980 train loss: 1.8990 valid loss: 1.9646, valid accuracy: 0.2524\n",
      "Iter-990 train loss: 1.7365 valid loss: 1.9555, valid accuracy: 0.2584\n",
      "Iter-1000 train loss: 2.0213 valid loss: 1.9453, valid accuracy: 0.2626\n",
      "Iter-1010 train loss: 2.0469 valid loss: 1.9363, valid accuracy: 0.2654\n",
      "Iter-1020 train loss: 1.8291 valid loss: 1.9268, valid accuracy: 0.2690\n",
      "Iter-1030 train loss: 1.8833 valid loss: 1.9176, valid accuracy: 0.2750\n",
      "Iter-1040 train loss: 1.8019 valid loss: 1.9086, valid accuracy: 0.2794\n",
      "Iter-1050 train loss: 1.8672 valid loss: 1.8992, valid accuracy: 0.2840\n",
      "Iter-1060 train loss: 1.8086 valid loss: 1.8899, valid accuracy: 0.2900\n",
      "Iter-1070 train loss: 1.8970 valid loss: 1.8801, valid accuracy: 0.2956\n",
      "Iter-1080 train loss: 1.8908 valid loss: 1.8715, valid accuracy: 0.3004\n",
      "Iter-1090 train loss: 1.8387 valid loss: 1.8627, valid accuracy: 0.3030\n",
      "Iter-1100 train loss: 1.9261 valid loss: 1.8533, valid accuracy: 0.3088\n",
      "Iter-1110 train loss: 1.9871 valid loss: 1.8437, valid accuracy: 0.3160\n",
      "Iter-1120 train loss: 1.9654 valid loss: 1.8346, valid accuracy: 0.3216\n",
      "Iter-1130 train loss: 1.7711 valid loss: 1.8264, valid accuracy: 0.3248\n",
      "Iter-1140 train loss: 1.6358 valid loss: 1.8170, valid accuracy: 0.3312\n",
      "Iter-1150 train loss: 1.7256 valid loss: 1.8084, valid accuracy: 0.3376\n",
      "Iter-1160 train loss: 1.7159 valid loss: 1.7999, valid accuracy: 0.3436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 1.6600 valid loss: 1.7906, valid accuracy: 0.3502\n",
      "Iter-1180 train loss: 1.7591 valid loss: 1.7822, valid accuracy: 0.3558\n",
      "Iter-1190 train loss: 1.7912 valid loss: 1.7742, valid accuracy: 0.3620\n",
      "Iter-1200 train loss: 1.5294 valid loss: 1.7659, valid accuracy: 0.3662\n",
      "Iter-1210 train loss: 1.7213 valid loss: 1.7572, valid accuracy: 0.3722\n",
      "Iter-1220 train loss: 1.7796 valid loss: 1.7485, valid accuracy: 0.3776\n",
      "Iter-1230 train loss: 1.6736 valid loss: 1.7397, valid accuracy: 0.3834\n",
      "Iter-1240 train loss: 1.8302 valid loss: 1.7304, valid accuracy: 0.3894\n",
      "Iter-1250 train loss: 1.8924 valid loss: 1.7226, valid accuracy: 0.3948\n",
      "Iter-1260 train loss: 1.7685 valid loss: 1.7146, valid accuracy: 0.3980\n",
      "Iter-1270 train loss: 1.6990 valid loss: 1.7058, valid accuracy: 0.4044\n",
      "Iter-1280 train loss: 1.4708 valid loss: 1.6978, valid accuracy: 0.4098\n",
      "Iter-1290 train loss: 1.7026 valid loss: 1.6901, valid accuracy: 0.4124\n",
      "Iter-1300 train loss: 1.6913 valid loss: 1.6821, valid accuracy: 0.4156\n",
      "Iter-1310 train loss: 1.6321 valid loss: 1.6735, valid accuracy: 0.4212\n",
      "Iter-1320 train loss: 1.4458 valid loss: 1.6655, valid accuracy: 0.4240\n",
      "Iter-1330 train loss: 1.5427 valid loss: 1.6570, valid accuracy: 0.4286\n",
      "Iter-1340 train loss: 1.7117 valid loss: 1.6486, valid accuracy: 0.4350\n",
      "Iter-1350 train loss: 1.7066 valid loss: 1.6404, valid accuracy: 0.4390\n",
      "Iter-1360 train loss: 1.5495 valid loss: 1.6330, valid accuracy: 0.4414\n",
      "Iter-1370 train loss: 1.7263 valid loss: 1.6252, valid accuracy: 0.4448\n",
      "Iter-1380 train loss: 1.8331 valid loss: 1.6179, valid accuracy: 0.4476\n",
      "Iter-1390 train loss: 1.6787 valid loss: 1.6094, valid accuracy: 0.4516\n",
      "Iter-1400 train loss: 1.5984 valid loss: 1.6023, valid accuracy: 0.4546\n",
      "Iter-1410 train loss: 1.5030 valid loss: 1.5945, valid accuracy: 0.4588\n",
      "Iter-1420 train loss: 1.5732 valid loss: 1.5876, valid accuracy: 0.4620\n",
      "Iter-1430 train loss: 1.5459 valid loss: 1.5804, valid accuracy: 0.4664\n",
      "Iter-1440 train loss: 1.5535 valid loss: 1.5740, valid accuracy: 0.4674\n",
      "Iter-1450 train loss: 1.4948 valid loss: 1.5664, valid accuracy: 0.4704\n",
      "Iter-1460 train loss: 1.6400 valid loss: 1.5586, valid accuracy: 0.4746\n",
      "Iter-1470 train loss: 1.3027 valid loss: 1.5515, valid accuracy: 0.4772\n",
      "Iter-1480 train loss: 1.3704 valid loss: 1.5439, valid accuracy: 0.4804\n",
      "Iter-1490 train loss: 1.5238 valid loss: 1.5369, valid accuracy: 0.4836\n",
      "Iter-1500 train loss: 1.7557 valid loss: 1.5297, valid accuracy: 0.4864\n",
      "Iter-1510 train loss: 1.4700 valid loss: 1.5225, valid accuracy: 0.4876\n",
      "Iter-1520 train loss: 1.5173 valid loss: 1.5156, valid accuracy: 0.4896\n",
      "Iter-1530 train loss: 1.4999 valid loss: 1.5085, valid accuracy: 0.4936\n",
      "Iter-1540 train loss: 1.6585 valid loss: 1.5019, valid accuracy: 0.4962\n",
      "Iter-1550 train loss: 1.5518 valid loss: 1.4950, valid accuracy: 0.5000\n",
      "Iter-1560 train loss: 1.4945 valid loss: 1.4875, valid accuracy: 0.5040\n",
      "Iter-1570 train loss: 1.5070 valid loss: 1.4799, valid accuracy: 0.5058\n",
      "Iter-1580 train loss: 1.3667 valid loss: 1.4735, valid accuracy: 0.5090\n",
      "Iter-1590 train loss: 1.4652 valid loss: 1.4657, valid accuracy: 0.5110\n",
      "Iter-1600 train loss: 1.1952 valid loss: 1.4591, valid accuracy: 0.5144\n",
      "Iter-1610 train loss: 1.6914 valid loss: 1.4522, valid accuracy: 0.5170\n",
      "Iter-1620 train loss: 1.3052 valid loss: 1.4454, valid accuracy: 0.5184\n",
      "Iter-1630 train loss: 1.3777 valid loss: 1.4389, valid accuracy: 0.5212\n",
      "Iter-1640 train loss: 1.4062 valid loss: 1.4327, valid accuracy: 0.5234\n",
      "Iter-1650 train loss: 1.2800 valid loss: 1.4266, valid accuracy: 0.5248\n",
      "Iter-1660 train loss: 1.5343 valid loss: 1.4199, valid accuracy: 0.5270\n",
      "Iter-1670 train loss: 1.3458 valid loss: 1.4141, valid accuracy: 0.5270\n",
      "Iter-1680 train loss: 1.4640 valid loss: 1.4083, valid accuracy: 0.5294\n",
      "Iter-1690 train loss: 1.4084 valid loss: 1.4020, valid accuracy: 0.5308\n",
      "Iter-1700 train loss: 1.3753 valid loss: 1.3965, valid accuracy: 0.5324\n",
      "Iter-1710 train loss: 1.3234 valid loss: 1.3906, valid accuracy: 0.5344\n",
      "Iter-1720 train loss: 1.5082 valid loss: 1.3843, valid accuracy: 0.5352\n",
      "Iter-1730 train loss: 1.2082 valid loss: 1.3784, valid accuracy: 0.5384\n",
      "Iter-1740 train loss: 1.3507 valid loss: 1.3720, valid accuracy: 0.5398\n",
      "Iter-1750 train loss: 1.3198 valid loss: 1.3656, valid accuracy: 0.5424\n",
      "Iter-1760 train loss: 1.2023 valid loss: 1.3596, valid accuracy: 0.5446\n",
      "Iter-1770 train loss: 1.3602 valid loss: 1.3536, valid accuracy: 0.5460\n",
      "Iter-1780 train loss: 1.4024 valid loss: 1.3472, valid accuracy: 0.5484\n",
      "Iter-1790 train loss: 1.5516 valid loss: 1.3417, valid accuracy: 0.5510\n",
      "Iter-1800 train loss: 1.4588 valid loss: 1.3357, valid accuracy: 0.5518\n",
      "Iter-1810 train loss: 1.5002 valid loss: 1.3299, valid accuracy: 0.5540\n",
      "Iter-1820 train loss: 1.2584 valid loss: 1.3243, valid accuracy: 0.5564\n",
      "Iter-1830 train loss: 1.2593 valid loss: 1.3186, valid accuracy: 0.5582\n",
      "Iter-1840 train loss: 1.3619 valid loss: 1.3134, valid accuracy: 0.5606\n",
      "Iter-1850 train loss: 1.1411 valid loss: 1.3084, valid accuracy: 0.5616\n",
      "Iter-1860 train loss: 1.4125 valid loss: 1.3034, valid accuracy: 0.5632\n",
      "Iter-1870 train loss: 1.1482 valid loss: 1.2983, valid accuracy: 0.5646\n",
      "Iter-1880 train loss: 1.3368 valid loss: 1.2936, valid accuracy: 0.5656\n",
      "Iter-1890 train loss: 1.0243 valid loss: 1.2878, valid accuracy: 0.5678\n",
      "Iter-1900 train loss: 1.2591 valid loss: 1.2820, valid accuracy: 0.5696\n",
      "Iter-1910 train loss: 1.4773 valid loss: 1.2773, valid accuracy: 0.5716\n",
      "Iter-1920 train loss: 1.3470 valid loss: 1.2720, valid accuracy: 0.5730\n",
      "Iter-1930 train loss: 1.2119 valid loss: 1.2670, valid accuracy: 0.5764\n",
      "Iter-1940 train loss: 1.2484 valid loss: 1.2618, valid accuracy: 0.5788\n",
      "Iter-1950 train loss: 1.1562 valid loss: 1.2566, valid accuracy: 0.5802\n",
      "Iter-1960 train loss: 1.2416 valid loss: 1.2517, valid accuracy: 0.5832\n",
      "Iter-1970 train loss: 1.3428 valid loss: 1.2464, valid accuracy: 0.5860\n",
      "Iter-1980 train loss: 1.4012 valid loss: 1.2413, valid accuracy: 0.5870\n",
      "Iter-1990 train loss: 1.2093 valid loss: 1.2369, valid accuracy: 0.5888\n",
      "Iter-2000 train loss: 1.1816 valid loss: 1.2320, valid accuracy: 0.5906\n",
      "Iter-2010 train loss: 1.2169 valid loss: 1.2270, valid accuracy: 0.5914\n",
      "Iter-2020 train loss: 1.3127 valid loss: 1.2224, valid accuracy: 0.5922\n",
      "Iter-2030 train loss: 1.4094 valid loss: 1.2177, valid accuracy: 0.5946\n",
      "Iter-2040 train loss: 1.1095 valid loss: 1.2128, valid accuracy: 0.5970\n",
      "Iter-2050 train loss: 1.1967 valid loss: 1.2087, valid accuracy: 0.5976\n",
      "Iter-2060 train loss: 1.1856 valid loss: 1.2040, valid accuracy: 0.5996\n",
      "Iter-2070 train loss: 1.1926 valid loss: 1.1992, valid accuracy: 0.6018\n",
      "Iter-2080 train loss: 1.1305 valid loss: 1.1950, valid accuracy: 0.6030\n",
      "Iter-2090 train loss: 1.2849 valid loss: 1.1903, valid accuracy: 0.6044\n",
      "Iter-2100 train loss: 1.2183 valid loss: 1.1860, valid accuracy: 0.6068\n",
      "Iter-2110 train loss: 1.2002 valid loss: 1.1820, valid accuracy: 0.6082\n",
      "Iter-2120 train loss: 1.1093 valid loss: 1.1775, valid accuracy: 0.6100\n",
      "Iter-2130 train loss: 1.2864 valid loss: 1.1736, valid accuracy: 0.6122\n",
      "Iter-2140 train loss: 1.1069 valid loss: 1.1699, valid accuracy: 0.6136\n",
      "Iter-2150 train loss: 1.0472 valid loss: 1.1657, valid accuracy: 0.6148\n",
      "Iter-2160 train loss: 1.3765 valid loss: 1.1612, valid accuracy: 0.6158\n",
      "Iter-2170 train loss: 1.1153 valid loss: 1.1567, valid accuracy: 0.6178\n",
      "Iter-2180 train loss: 1.1509 valid loss: 1.1525, valid accuracy: 0.6214\n",
      "Iter-2190 train loss: 1.0843 valid loss: 1.1486, valid accuracy: 0.6232\n",
      "Iter-2200 train loss: 1.2087 valid loss: 1.1448, valid accuracy: 0.6240\n",
      "Iter-2210 train loss: 1.1911 valid loss: 1.1407, valid accuracy: 0.6256\n",
      "Iter-2220 train loss: 1.0448 valid loss: 1.1370, valid accuracy: 0.6280\n",
      "Iter-2230 train loss: 1.0254 valid loss: 1.1325, valid accuracy: 0.6306\n",
      "Iter-2240 train loss: 1.0305 valid loss: 1.1286, valid accuracy: 0.6320\n",
      "Iter-2250 train loss: 1.0092 valid loss: 1.1244, valid accuracy: 0.6346\n",
      "Iter-2260 train loss: 1.1332 valid loss: 1.1208, valid accuracy: 0.6348\n",
      "Iter-2270 train loss: 1.2686 valid loss: 1.1165, valid accuracy: 0.6388\n",
      "Iter-2280 train loss: 1.2597 valid loss: 1.1126, valid accuracy: 0.6394\n",
      "Iter-2290 train loss: 1.1988 valid loss: 1.1092, valid accuracy: 0.6414\n",
      "Iter-2300 train loss: 0.9660 valid loss: 1.1055, valid accuracy: 0.6424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 1.3417 valid loss: 1.1017, valid accuracy: 0.6444\n",
      "Iter-2320 train loss: 1.1316 valid loss: 1.0978, valid accuracy: 0.6454\n",
      "Iter-2330 train loss: 1.0463 valid loss: 1.0940, valid accuracy: 0.6476\n",
      "Iter-2340 train loss: 1.1521 valid loss: 1.0903, valid accuracy: 0.6504\n",
      "Iter-2350 train loss: 1.0346 valid loss: 1.0863, valid accuracy: 0.6504\n",
      "Iter-2360 train loss: 0.9417 valid loss: 1.0835, valid accuracy: 0.6516\n",
      "Iter-2370 train loss: 0.9003 valid loss: 1.0798, valid accuracy: 0.6538\n",
      "Iter-2380 train loss: 1.1376 valid loss: 1.0760, valid accuracy: 0.6552\n",
      "Iter-2390 train loss: 1.0774 valid loss: 1.0728, valid accuracy: 0.6568\n",
      "Iter-2400 train loss: 1.2727 valid loss: 1.0695, valid accuracy: 0.6580\n",
      "Iter-2410 train loss: 1.0150 valid loss: 1.0662, valid accuracy: 0.6604\n",
      "Iter-2420 train loss: 1.1242 valid loss: 1.0629, valid accuracy: 0.6608\n",
      "Iter-2430 train loss: 1.0400 valid loss: 1.0594, valid accuracy: 0.6634\n",
      "Iter-2440 train loss: 1.1204 valid loss: 1.0561, valid accuracy: 0.6642\n",
      "Iter-2450 train loss: 1.1245 valid loss: 1.0524, valid accuracy: 0.6664\n",
      "Iter-2460 train loss: 0.8933 valid loss: 1.0490, valid accuracy: 0.6666\n",
      "Iter-2470 train loss: 1.0374 valid loss: 1.0457, valid accuracy: 0.6678\n",
      "Iter-2480 train loss: 1.1011 valid loss: 1.0426, valid accuracy: 0.6696\n",
      "Iter-2490 train loss: 1.1309 valid loss: 1.0389, valid accuracy: 0.6718\n",
      "Iter-2500 train loss: 0.8322 valid loss: 1.0364, valid accuracy: 0.6730\n",
      "Iter-2510 train loss: 1.0504 valid loss: 1.0332, valid accuracy: 0.6734\n",
      "Iter-2520 train loss: 1.0357 valid loss: 1.0300, valid accuracy: 0.6744\n",
      "Iter-2530 train loss: 0.6804 valid loss: 1.0274, valid accuracy: 0.6756\n",
      "Iter-2540 train loss: 1.0765 valid loss: 1.0244, valid accuracy: 0.6768\n",
      "Iter-2550 train loss: 0.9138 valid loss: 1.0214, valid accuracy: 0.6784\n",
      "Iter-2560 train loss: 1.0280 valid loss: 1.0183, valid accuracy: 0.6794\n",
      "Iter-2570 train loss: 0.8808 valid loss: 1.0153, valid accuracy: 0.6804\n",
      "Iter-2580 train loss: 1.0373 valid loss: 1.0122, valid accuracy: 0.6828\n",
      "Iter-2590 train loss: 1.0772 valid loss: 1.0089, valid accuracy: 0.6834\n",
      "Iter-2600 train loss: 0.7879 valid loss: 1.0060, valid accuracy: 0.6846\n",
      "Iter-2610 train loss: 1.0916 valid loss: 1.0031, valid accuracy: 0.6852\n",
      "Iter-2620 train loss: 0.9801 valid loss: 1.0006, valid accuracy: 0.6866\n",
      "Iter-2630 train loss: 0.9770 valid loss: 0.9974, valid accuracy: 0.6892\n",
      "Iter-2640 train loss: 1.1917 valid loss: 0.9945, valid accuracy: 0.6894\n",
      "Iter-2650 train loss: 0.8527 valid loss: 0.9921, valid accuracy: 0.6900\n",
      "Iter-2660 train loss: 0.9370 valid loss: 0.9894, valid accuracy: 0.6914\n",
      "Iter-2670 train loss: 0.9752 valid loss: 0.9864, valid accuracy: 0.6922\n",
      "Iter-2680 train loss: 0.9570 valid loss: 0.9840, valid accuracy: 0.6938\n",
      "Iter-2690 train loss: 0.8971 valid loss: 0.9812, valid accuracy: 0.6958\n",
      "Iter-2700 train loss: 0.8536 valid loss: 0.9786, valid accuracy: 0.6958\n",
      "Iter-2710 train loss: 0.9919 valid loss: 0.9760, valid accuracy: 0.6964\n",
      "Iter-2720 train loss: 0.9915 valid loss: 0.9731, valid accuracy: 0.6958\n",
      "Iter-2730 train loss: 0.8078 valid loss: 0.9705, valid accuracy: 0.6960\n",
      "Iter-2740 train loss: 1.2014 valid loss: 0.9675, valid accuracy: 0.6978\n",
      "Iter-2750 train loss: 0.8366 valid loss: 0.9651, valid accuracy: 0.6984\n",
      "Iter-2760 train loss: 0.8722 valid loss: 0.9627, valid accuracy: 0.6992\n",
      "Iter-2770 train loss: 1.0831 valid loss: 0.9594, valid accuracy: 0.7000\n",
      "Iter-2780 train loss: 0.9460 valid loss: 0.9568, valid accuracy: 0.7012\n",
      "Iter-2790 train loss: 0.9111 valid loss: 0.9543, valid accuracy: 0.7030\n",
      "Iter-2800 train loss: 0.8160 valid loss: 0.9514, valid accuracy: 0.7036\n",
      "Iter-2810 train loss: 0.9284 valid loss: 0.9487, valid accuracy: 0.7054\n",
      "Iter-2820 train loss: 0.9902 valid loss: 0.9463, valid accuracy: 0.7054\n",
      "Iter-2830 train loss: 1.0156 valid loss: 0.9438, valid accuracy: 0.7074\n",
      "Iter-2840 train loss: 1.0400 valid loss: 0.9413, valid accuracy: 0.7086\n",
      "Iter-2850 train loss: 0.9693 valid loss: 0.9384, valid accuracy: 0.7096\n",
      "Iter-2860 train loss: 1.0443 valid loss: 0.9358, valid accuracy: 0.7102\n",
      "Iter-2870 train loss: 0.9992 valid loss: 0.9330, valid accuracy: 0.7110\n",
      "Iter-2880 train loss: 0.9047 valid loss: 0.9306, valid accuracy: 0.7122\n",
      "Iter-2890 train loss: 0.7339 valid loss: 0.9282, valid accuracy: 0.7134\n",
      "Iter-2900 train loss: 0.9679 valid loss: 0.9257, valid accuracy: 0.7142\n",
      "Iter-2910 train loss: 0.7918 valid loss: 0.9234, valid accuracy: 0.7166\n",
      "Iter-2920 train loss: 0.7861 valid loss: 0.9210, valid accuracy: 0.7186\n",
      "Iter-2930 train loss: 0.9350 valid loss: 0.9187, valid accuracy: 0.7202\n",
      "Iter-2940 train loss: 0.8872 valid loss: 0.9163, valid accuracy: 0.7210\n",
      "Iter-2950 train loss: 0.9873 valid loss: 0.9142, valid accuracy: 0.7228\n",
      "Iter-2960 train loss: 0.8009 valid loss: 0.9116, valid accuracy: 0.7232\n",
      "Iter-2970 train loss: 0.8296 valid loss: 0.9095, valid accuracy: 0.7236\n",
      "Iter-2980 train loss: 0.8688 valid loss: 0.9069, valid accuracy: 0.7254\n",
      "Iter-2990 train loss: 0.9757 valid loss: 0.9047, valid accuracy: 0.7264\n",
      "Iter-3000 train loss: 1.0078 valid loss: 0.9023, valid accuracy: 0.7272\n",
      "Iter-3010 train loss: 0.9020 valid loss: 0.8998, valid accuracy: 0.7286\n",
      "Iter-3020 train loss: 0.8107 valid loss: 0.8975, valid accuracy: 0.7292\n",
      "Iter-3030 train loss: 0.7496 valid loss: 0.8957, valid accuracy: 0.7298\n",
      "Iter-3040 train loss: 0.8882 valid loss: 0.8941, valid accuracy: 0.7298\n",
      "Iter-3050 train loss: 0.8931 valid loss: 0.8923, valid accuracy: 0.7306\n",
      "Iter-3060 train loss: 0.8502 valid loss: 0.8903, valid accuracy: 0.7306\n",
      "Iter-3070 train loss: 0.8568 valid loss: 0.8879, valid accuracy: 0.7314\n",
      "Iter-3080 train loss: 0.8158 valid loss: 0.8853, valid accuracy: 0.7328\n",
      "Iter-3090 train loss: 0.7271 valid loss: 0.8829, valid accuracy: 0.7326\n",
      "Iter-3100 train loss: 0.9571 valid loss: 0.8808, valid accuracy: 0.7330\n",
      "Iter-3110 train loss: 0.8860 valid loss: 0.8788, valid accuracy: 0.7328\n",
      "Iter-3120 train loss: 0.8587 valid loss: 0.8768, valid accuracy: 0.7334\n",
      "Iter-3130 train loss: 0.8204 valid loss: 0.8744, valid accuracy: 0.7344\n",
      "Iter-3140 train loss: 0.8727 valid loss: 0.8717, valid accuracy: 0.7352\n",
      "Iter-3150 train loss: 0.7479 valid loss: 0.8696, valid accuracy: 0.7362\n",
      "Iter-3160 train loss: 0.9352 valid loss: 0.8674, valid accuracy: 0.7378\n",
      "Iter-3170 train loss: 0.7784 valid loss: 0.8653, valid accuracy: 0.7388\n",
      "Iter-3180 train loss: 0.9929 valid loss: 0.8629, valid accuracy: 0.7394\n",
      "Iter-3190 train loss: 0.9687 valid loss: 0.8610, valid accuracy: 0.7404\n",
      "Iter-3200 train loss: 1.0541 valid loss: 0.8590, valid accuracy: 0.7410\n",
      "Iter-3210 train loss: 0.8316 valid loss: 0.8566, valid accuracy: 0.7424\n",
      "Iter-3220 train loss: 0.7646 valid loss: 0.8547, valid accuracy: 0.7428\n",
      "Iter-3230 train loss: 0.7316 valid loss: 0.8527, valid accuracy: 0.7426\n",
      "Iter-3240 train loss: 0.6973 valid loss: 0.8511, valid accuracy: 0.7432\n",
      "Iter-3250 train loss: 1.0023 valid loss: 0.8490, valid accuracy: 0.7436\n",
      "Iter-3260 train loss: 0.9877 valid loss: 0.8472, valid accuracy: 0.7446\n",
      "Iter-3270 train loss: 0.9084 valid loss: 0.8456, valid accuracy: 0.7456\n",
      "Iter-3280 train loss: 1.0003 valid loss: 0.8437, valid accuracy: 0.7464\n",
      "Iter-3290 train loss: 0.8404 valid loss: 0.8419, valid accuracy: 0.7466\n",
      "Iter-3300 train loss: 0.8139 valid loss: 0.8404, valid accuracy: 0.7470\n",
      "Iter-3310 train loss: 0.9298 valid loss: 0.8387, valid accuracy: 0.7470\n",
      "Iter-3320 train loss: 0.7314 valid loss: 0.8367, valid accuracy: 0.7482\n",
      "Iter-3330 train loss: 1.0977 valid loss: 0.8347, valid accuracy: 0.7486\n",
      "Iter-3340 train loss: 0.9421 valid loss: 0.8326, valid accuracy: 0.7488\n",
      "Iter-3350 train loss: 0.8478 valid loss: 0.8311, valid accuracy: 0.7496\n",
      "Iter-3360 train loss: 0.9425 valid loss: 0.8289, valid accuracy: 0.7488\n",
      "Iter-3370 train loss: 0.7727 valid loss: 0.8268, valid accuracy: 0.7500\n",
      "Iter-3380 train loss: 0.7376 valid loss: 0.8247, valid accuracy: 0.7520\n",
      "Iter-3390 train loss: 0.9139 valid loss: 0.8229, valid accuracy: 0.7526\n",
      "Iter-3400 train loss: 1.0417 valid loss: 0.8211, valid accuracy: 0.7530\n",
      "Iter-3410 train loss: 0.9547 valid loss: 0.8194, valid accuracy: 0.7534\n",
      "Iter-3420 train loss: 0.7685 valid loss: 0.8175, valid accuracy: 0.7538\n",
      "Iter-3430 train loss: 0.8058 valid loss: 0.8161, valid accuracy: 0.7544\n",
      "Iter-3440 train loss: 1.0294 valid loss: 0.8142, valid accuracy: 0.7554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 0.7596 valid loss: 0.8123, valid accuracy: 0.7546\n",
      "Iter-3460 train loss: 0.8593 valid loss: 0.8105, valid accuracy: 0.7554\n",
      "Iter-3470 train loss: 0.7231 valid loss: 0.8088, valid accuracy: 0.7562\n",
      "Iter-3480 train loss: 0.9332 valid loss: 0.8070, valid accuracy: 0.7570\n",
      "Iter-3490 train loss: 0.6309 valid loss: 0.8054, valid accuracy: 0.7578\n",
      "Iter-3500 train loss: 0.7407 valid loss: 0.8035, valid accuracy: 0.7584\n",
      "Iter-3510 train loss: 1.0173 valid loss: 0.8018, valid accuracy: 0.7592\n",
      "Iter-3520 train loss: 0.9238 valid loss: 0.8003, valid accuracy: 0.7596\n",
      "Iter-3530 train loss: 0.8270 valid loss: 0.7986, valid accuracy: 0.7594\n",
      "Iter-3540 train loss: 0.9657 valid loss: 0.7969, valid accuracy: 0.7600\n",
      "Iter-3550 train loss: 0.8410 valid loss: 0.7952, valid accuracy: 0.7612\n",
      "Iter-3560 train loss: 0.7400 valid loss: 0.7937, valid accuracy: 0.7616\n",
      "Iter-3570 train loss: 0.9227 valid loss: 0.7917, valid accuracy: 0.7626\n",
      "Iter-3580 train loss: 0.9387 valid loss: 0.7902, valid accuracy: 0.7622\n",
      "Iter-3590 train loss: 0.8582 valid loss: 0.7885, valid accuracy: 0.7620\n",
      "Iter-3600 train loss: 0.8810 valid loss: 0.7872, valid accuracy: 0.7626\n",
      "Iter-3610 train loss: 0.6590 valid loss: 0.7856, valid accuracy: 0.7624\n",
      "Iter-3620 train loss: 0.8646 valid loss: 0.7840, valid accuracy: 0.7638\n",
      "Iter-3630 train loss: 0.8118 valid loss: 0.7823, valid accuracy: 0.7640\n",
      "Iter-3640 train loss: 0.6866 valid loss: 0.7807, valid accuracy: 0.7650\n",
      "Iter-3650 train loss: 0.7885 valid loss: 0.7792, valid accuracy: 0.7654\n",
      "Iter-3660 train loss: 0.6981 valid loss: 0.7780, valid accuracy: 0.7662\n",
      "Iter-3670 train loss: 0.8738 valid loss: 0.7763, valid accuracy: 0.7674\n",
      "Iter-3680 train loss: 0.7775 valid loss: 0.7750, valid accuracy: 0.7674\n",
      "Iter-3690 train loss: 0.7895 valid loss: 0.7735, valid accuracy: 0.7684\n",
      "Iter-3700 train loss: 0.8759 valid loss: 0.7721, valid accuracy: 0.7694\n",
      "Iter-3710 train loss: 0.8267 valid loss: 0.7710, valid accuracy: 0.7680\n",
      "Iter-3720 train loss: 0.7944 valid loss: 0.7697, valid accuracy: 0.7684\n",
      "Iter-3730 train loss: 1.0004 valid loss: 0.7684, valid accuracy: 0.7688\n",
      "Iter-3740 train loss: 0.6864 valid loss: 0.7668, valid accuracy: 0.7696\n",
      "Iter-3750 train loss: 0.6331 valid loss: 0.7655, valid accuracy: 0.7700\n",
      "Iter-3760 train loss: 0.6091 valid loss: 0.7640, valid accuracy: 0.7700\n",
      "Iter-3770 train loss: 0.7614 valid loss: 0.7623, valid accuracy: 0.7708\n",
      "Iter-3780 train loss: 0.7536 valid loss: 0.7609, valid accuracy: 0.7708\n",
      "Iter-3790 train loss: 0.6138 valid loss: 0.7596, valid accuracy: 0.7704\n",
      "Iter-3800 train loss: 0.7831 valid loss: 0.7582, valid accuracy: 0.7700\n",
      "Iter-3810 train loss: 0.7473 valid loss: 0.7570, valid accuracy: 0.7706\n",
      "Iter-3820 train loss: 0.8597 valid loss: 0.7554, valid accuracy: 0.7708\n",
      "Iter-3830 train loss: 0.7393 valid loss: 0.7543, valid accuracy: 0.7706\n",
      "Iter-3840 train loss: 0.7224 valid loss: 0.7528, valid accuracy: 0.7718\n",
      "Iter-3850 train loss: 0.5984 valid loss: 0.7514, valid accuracy: 0.7730\n",
      "Iter-3860 train loss: 0.7287 valid loss: 0.7500, valid accuracy: 0.7734\n",
      "Iter-3870 train loss: 0.7940 valid loss: 0.7486, valid accuracy: 0.7742\n",
      "Iter-3880 train loss: 0.9329 valid loss: 0.7470, valid accuracy: 0.7752\n",
      "Iter-3890 train loss: 0.6804 valid loss: 0.7458, valid accuracy: 0.7754\n",
      "Iter-3900 train loss: 0.7389 valid loss: 0.7444, valid accuracy: 0.7754\n",
      "Iter-3910 train loss: 0.8767 valid loss: 0.7431, valid accuracy: 0.7750\n",
      "Iter-3920 train loss: 0.6110 valid loss: 0.7419, valid accuracy: 0.7752\n",
      "Iter-3930 train loss: 0.5983 valid loss: 0.7408, valid accuracy: 0.7748\n",
      "Iter-3940 train loss: 0.5975 valid loss: 0.7398, valid accuracy: 0.7758\n",
      "Iter-3950 train loss: 0.7644 valid loss: 0.7386, valid accuracy: 0.7766\n",
      "Iter-3960 train loss: 0.7160 valid loss: 0.7371, valid accuracy: 0.7772\n",
      "Iter-3970 train loss: 0.6668 valid loss: 0.7359, valid accuracy: 0.7776\n",
      "Iter-3980 train loss: 0.7080 valid loss: 0.7346, valid accuracy: 0.7782\n",
      "Iter-3990 train loss: 0.8514 valid loss: 0.7332, valid accuracy: 0.7788\n",
      "Iter-4000 train loss: 0.6565 valid loss: 0.7319, valid accuracy: 0.7798\n",
      "Iter-4010 train loss: 0.5758 valid loss: 0.7308, valid accuracy: 0.7798\n",
      "Iter-4020 train loss: 0.7424 valid loss: 0.7295, valid accuracy: 0.7798\n",
      "Iter-4030 train loss: 0.9014 valid loss: 0.7284, valid accuracy: 0.7810\n",
      "Iter-4040 train loss: 0.8440 valid loss: 0.7270, valid accuracy: 0.7812\n",
      "Iter-4050 train loss: 0.7094 valid loss: 0.7257, valid accuracy: 0.7814\n",
      "Iter-4060 train loss: 0.7583 valid loss: 0.7242, valid accuracy: 0.7824\n",
      "Iter-4070 train loss: 0.8864 valid loss: 0.7231, valid accuracy: 0.7834\n",
      "Iter-4080 train loss: 0.9799 valid loss: 0.7218, valid accuracy: 0.7840\n",
      "Iter-4090 train loss: 0.7484 valid loss: 0.7205, valid accuracy: 0.7840\n",
      "Iter-4100 train loss: 0.7086 valid loss: 0.7194, valid accuracy: 0.7848\n",
      "Iter-4110 train loss: 0.7574 valid loss: 0.7181, valid accuracy: 0.7850\n",
      "Iter-4120 train loss: 0.7188 valid loss: 0.7170, valid accuracy: 0.7856\n",
      "Iter-4130 train loss: 0.6287 valid loss: 0.7160, valid accuracy: 0.7854\n",
      "Iter-4140 train loss: 0.7716 valid loss: 0.7151, valid accuracy: 0.7858\n",
      "Iter-4150 train loss: 0.8675 valid loss: 0.7140, valid accuracy: 0.7854\n",
      "Iter-4160 train loss: 0.4753 valid loss: 0.7126, valid accuracy: 0.7864\n",
      "Iter-4170 train loss: 0.8560 valid loss: 0.7113, valid accuracy: 0.7876\n",
      "Iter-4180 train loss: 0.7323 valid loss: 0.7103, valid accuracy: 0.7872\n",
      "Iter-4190 train loss: 0.7298 valid loss: 0.7092, valid accuracy: 0.7872\n",
      "Iter-4200 train loss: 0.7958 valid loss: 0.7081, valid accuracy: 0.7878\n",
      "Iter-4210 train loss: 0.7824 valid loss: 0.7069, valid accuracy: 0.7880\n",
      "Iter-4220 train loss: 0.7369 valid loss: 0.7058, valid accuracy: 0.7894\n",
      "Iter-4230 train loss: 0.7954 valid loss: 0.7045, valid accuracy: 0.7892\n",
      "Iter-4240 train loss: 0.5894 valid loss: 0.7034, valid accuracy: 0.7898\n",
      "Iter-4250 train loss: 0.8568 valid loss: 0.7022, valid accuracy: 0.7896\n",
      "Iter-4260 train loss: 0.8292 valid loss: 0.7013, valid accuracy: 0.7900\n",
      "Iter-4270 train loss: 0.6264 valid loss: 0.7001, valid accuracy: 0.7900\n",
      "Iter-4280 train loss: 0.7182 valid loss: 0.6990, valid accuracy: 0.7912\n",
      "Iter-4290 train loss: 0.7012 valid loss: 0.6981, valid accuracy: 0.7910\n",
      "Iter-4300 train loss: 0.8212 valid loss: 0.6972, valid accuracy: 0.7910\n",
      "Iter-4310 train loss: 0.7741 valid loss: 0.6960, valid accuracy: 0.7918\n",
      "Iter-4320 train loss: 0.7746 valid loss: 0.6950, valid accuracy: 0.7926\n",
      "Iter-4330 train loss: 0.6216 valid loss: 0.6938, valid accuracy: 0.7922\n",
      "Iter-4340 train loss: 0.7438 valid loss: 0.6927, valid accuracy: 0.7926\n",
      "Iter-4350 train loss: 0.8107 valid loss: 0.6916, valid accuracy: 0.7926\n",
      "Iter-4360 train loss: 0.6834 valid loss: 0.6907, valid accuracy: 0.7930\n",
      "Iter-4370 train loss: 0.5502 valid loss: 0.6899, valid accuracy: 0.7932\n",
      "Iter-4380 train loss: 0.7083 valid loss: 0.6889, valid accuracy: 0.7934\n",
      "Iter-4390 train loss: 0.5459 valid loss: 0.6881, valid accuracy: 0.7940\n",
      "Iter-4400 train loss: 0.8323 valid loss: 0.6870, valid accuracy: 0.7942\n",
      "Iter-4410 train loss: 0.6766 valid loss: 0.6859, valid accuracy: 0.7958\n",
      "Iter-4420 train loss: 0.5453 valid loss: 0.6848, valid accuracy: 0.7962\n",
      "Iter-4430 train loss: 0.6679 valid loss: 0.6838, valid accuracy: 0.7970\n",
      "Iter-4440 train loss: 0.7822 valid loss: 0.6831, valid accuracy: 0.7972\n",
      "Iter-4450 train loss: 0.6435 valid loss: 0.6822, valid accuracy: 0.7972\n",
      "Iter-4460 train loss: 0.6294 valid loss: 0.6811, valid accuracy: 0.7978\n",
      "Iter-4470 train loss: 0.9127 valid loss: 0.6802, valid accuracy: 0.7980\n",
      "Iter-4480 train loss: 0.6782 valid loss: 0.6793, valid accuracy: 0.7982\n",
      "Iter-4490 train loss: 0.6864 valid loss: 0.6786, valid accuracy: 0.7984\n",
      "Iter-4500 train loss: 0.6621 valid loss: 0.6776, valid accuracy: 0.7992\n",
      "Iter-4510 train loss: 0.5701 valid loss: 0.6767, valid accuracy: 0.7992\n",
      "Iter-4520 train loss: 0.5704 valid loss: 0.6755, valid accuracy: 0.7996\n",
      "Iter-4530 train loss: 0.6904 valid loss: 0.6744, valid accuracy: 0.8006\n",
      "Iter-4540 train loss: 0.4816 valid loss: 0.6732, valid accuracy: 0.8008\n",
      "Iter-4550 train loss: 0.7028 valid loss: 0.6721, valid accuracy: 0.8022\n",
      "Iter-4560 train loss: 0.8149 valid loss: 0.6709, valid accuracy: 0.8022\n",
      "Iter-4570 train loss: 0.7278 valid loss: 0.6701, valid accuracy: 0.8022\n",
      "Iter-4580 train loss: 0.6501 valid loss: 0.6691, valid accuracy: 0.8024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 0.5220 valid loss: 0.6682, valid accuracy: 0.8018\n",
      "Iter-4600 train loss: 0.6101 valid loss: 0.6672, valid accuracy: 0.8024\n",
      "Iter-4610 train loss: 0.5452 valid loss: 0.6664, valid accuracy: 0.8032\n",
      "Iter-4620 train loss: 0.7157 valid loss: 0.6657, valid accuracy: 0.8038\n",
      "Iter-4630 train loss: 0.6860 valid loss: 0.6648, valid accuracy: 0.8040\n",
      "Iter-4640 train loss: 0.6017 valid loss: 0.6637, valid accuracy: 0.8044\n",
      "Iter-4650 train loss: 0.6030 valid loss: 0.6630, valid accuracy: 0.8036\n",
      "Iter-4660 train loss: 0.5693 valid loss: 0.6622, valid accuracy: 0.8042\n",
      "Iter-4670 train loss: 0.6467 valid loss: 0.6612, valid accuracy: 0.8038\n",
      "Iter-4680 train loss: 0.4874 valid loss: 0.6601, valid accuracy: 0.8040\n",
      "Iter-4690 train loss: 0.6005 valid loss: 0.6592, valid accuracy: 0.8042\n",
      "Iter-4700 train loss: 0.7667 valid loss: 0.6584, valid accuracy: 0.8042\n",
      "Iter-4710 train loss: 0.8288 valid loss: 0.6573, valid accuracy: 0.8036\n",
      "Iter-4720 train loss: 0.6439 valid loss: 0.6565, valid accuracy: 0.8040\n",
      "Iter-4730 train loss: 0.7548 valid loss: 0.6557, valid accuracy: 0.8044\n",
      "Iter-4740 train loss: 0.8954 valid loss: 0.6549, valid accuracy: 0.8050\n",
      "Iter-4750 train loss: 0.7679 valid loss: 0.6540, valid accuracy: 0.8048\n",
      "Iter-4760 train loss: 0.8841 valid loss: 0.6532, valid accuracy: 0.8052\n",
      "Iter-4770 train loss: 0.6671 valid loss: 0.6524, valid accuracy: 0.8048\n",
      "Iter-4780 train loss: 0.7936 valid loss: 0.6514, valid accuracy: 0.8052\n",
      "Iter-4790 train loss: 0.6161 valid loss: 0.6506, valid accuracy: 0.8058\n",
      "Iter-4800 train loss: 0.7526 valid loss: 0.6498, valid accuracy: 0.8060\n",
      "Iter-4810 train loss: 0.6167 valid loss: 0.6492, valid accuracy: 0.8062\n",
      "Iter-4820 train loss: 0.7831 valid loss: 0.6481, valid accuracy: 0.8062\n",
      "Iter-4830 train loss: 0.6648 valid loss: 0.6471, valid accuracy: 0.8060\n",
      "Iter-4840 train loss: 0.9655 valid loss: 0.6462, valid accuracy: 0.8062\n",
      "Iter-4850 train loss: 0.7592 valid loss: 0.6453, valid accuracy: 0.8060\n",
      "Iter-4860 train loss: 0.6728 valid loss: 0.6446, valid accuracy: 0.8064\n",
      "Iter-4870 train loss: 0.6161 valid loss: 0.6438, valid accuracy: 0.8074\n",
      "Iter-4880 train loss: 0.7018 valid loss: 0.6428, valid accuracy: 0.8074\n",
      "Iter-4890 train loss: 0.5066 valid loss: 0.6421, valid accuracy: 0.8074\n",
      "Iter-4900 train loss: 0.6871 valid loss: 0.6414, valid accuracy: 0.8080\n",
      "Iter-4910 train loss: 0.5866 valid loss: 0.6406, valid accuracy: 0.8076\n",
      "Iter-4920 train loss: 0.9438 valid loss: 0.6398, valid accuracy: 0.8084\n",
      "Iter-4930 train loss: 0.7087 valid loss: 0.6390, valid accuracy: 0.8086\n",
      "Iter-4940 train loss: 0.6054 valid loss: 0.6381, valid accuracy: 0.8086\n",
      "Iter-4950 train loss: 0.5443 valid loss: 0.6373, valid accuracy: 0.8098\n",
      "Iter-4960 train loss: 0.6587 valid loss: 0.6366, valid accuracy: 0.8100\n",
      "Iter-4970 train loss: 0.4950 valid loss: 0.6355, valid accuracy: 0.8102\n",
      "Iter-4980 train loss: 0.7594 valid loss: 0.6349, valid accuracy: 0.8098\n",
      "Iter-4990 train loss: 0.6914 valid loss: 0.6338, valid accuracy: 0.8104\n",
      "Iter-5000 train loss: 0.8706 valid loss: 0.6330, valid accuracy: 0.8112\n",
      "Iter-5010 train loss: 0.5115 valid loss: 0.6323, valid accuracy: 0.8116\n",
      "Iter-5020 train loss: 0.6466 valid loss: 0.6315, valid accuracy: 0.8118\n",
      "Iter-5030 train loss: 0.7555 valid loss: 0.6308, valid accuracy: 0.8118\n",
      "Iter-5040 train loss: 0.5815 valid loss: 0.6301, valid accuracy: 0.8120\n",
      "Iter-5050 train loss: 0.5733 valid loss: 0.6292, valid accuracy: 0.8124\n",
      "Iter-5060 train loss: 0.5194 valid loss: 0.6286, valid accuracy: 0.8128\n",
      "Iter-5070 train loss: 0.5792 valid loss: 0.6280, valid accuracy: 0.8134\n",
      "Iter-5080 train loss: 0.5595 valid loss: 0.6272, valid accuracy: 0.8138\n",
      "Iter-5090 train loss: 0.5063 valid loss: 0.6265, valid accuracy: 0.8136\n",
      "Iter-5100 train loss: 0.6311 valid loss: 0.6256, valid accuracy: 0.8128\n",
      "Iter-5110 train loss: 0.6214 valid loss: 0.6250, valid accuracy: 0.8142\n",
      "Iter-5120 train loss: 0.7550 valid loss: 0.6242, valid accuracy: 0.8148\n",
      "Iter-5130 train loss: 0.6213 valid loss: 0.6236, valid accuracy: 0.8148\n",
      "Iter-5140 train loss: 0.7001 valid loss: 0.6230, valid accuracy: 0.8140\n",
      "Iter-5150 train loss: 0.6442 valid loss: 0.6221, valid accuracy: 0.8156\n",
      "Iter-5160 train loss: 0.6212 valid loss: 0.6214, valid accuracy: 0.8152\n",
      "Iter-5170 train loss: 0.4926 valid loss: 0.6207, valid accuracy: 0.8158\n",
      "Iter-5180 train loss: 0.4913 valid loss: 0.6204, valid accuracy: 0.8150\n",
      "Iter-5190 train loss: 0.8320 valid loss: 0.6195, valid accuracy: 0.8154\n",
      "Iter-5200 train loss: 0.4733 valid loss: 0.6188, valid accuracy: 0.8156\n",
      "Iter-5210 train loss: 0.6113 valid loss: 0.6182, valid accuracy: 0.8154\n",
      "Iter-5220 train loss: 0.5706 valid loss: 0.6174, valid accuracy: 0.8164\n",
      "Iter-5230 train loss: 0.4314 valid loss: 0.6167, valid accuracy: 0.8166\n",
      "Iter-5240 train loss: 0.5101 valid loss: 0.6160, valid accuracy: 0.8180\n",
      "Iter-5250 train loss: 0.5321 valid loss: 0.6155, valid accuracy: 0.8180\n",
      "Iter-5260 train loss: 0.6883 valid loss: 0.6147, valid accuracy: 0.8170\n",
      "Iter-5270 train loss: 0.6066 valid loss: 0.6139, valid accuracy: 0.8184\n",
      "Iter-5280 train loss: 0.7274 valid loss: 0.6135, valid accuracy: 0.8186\n",
      "Iter-5290 train loss: 0.5213 valid loss: 0.6125, valid accuracy: 0.8186\n",
      "Iter-5300 train loss: 0.4666 valid loss: 0.6119, valid accuracy: 0.8192\n",
      "Iter-5310 train loss: 0.7971 valid loss: 0.6115, valid accuracy: 0.8186\n",
      "Iter-5320 train loss: 0.5642 valid loss: 0.6107, valid accuracy: 0.8198\n",
      "Iter-5330 train loss: 0.5883 valid loss: 0.6100, valid accuracy: 0.8200\n",
      "Iter-5340 train loss: 0.7510 valid loss: 0.6092, valid accuracy: 0.8196\n",
      "Iter-5350 train loss: 0.5528 valid loss: 0.6087, valid accuracy: 0.8198\n",
      "Iter-5360 train loss: 0.5175 valid loss: 0.6079, valid accuracy: 0.8198\n",
      "Iter-5370 train loss: 0.4921 valid loss: 0.6073, valid accuracy: 0.8200\n",
      "Iter-5380 train loss: 0.5810 valid loss: 0.6067, valid accuracy: 0.8200\n",
      "Iter-5390 train loss: 0.5091 valid loss: 0.6059, valid accuracy: 0.8198\n",
      "Iter-5400 train loss: 0.5916 valid loss: 0.6053, valid accuracy: 0.8200\n",
      "Iter-5410 train loss: 0.6363 valid loss: 0.6046, valid accuracy: 0.8210\n",
      "Iter-5420 train loss: 0.6165 valid loss: 0.6038, valid accuracy: 0.8210\n",
      "Iter-5430 train loss: 0.7341 valid loss: 0.6031, valid accuracy: 0.8216\n",
      "Iter-5440 train loss: 0.6092 valid loss: 0.6025, valid accuracy: 0.8214\n",
      "Iter-5450 train loss: 0.7521 valid loss: 0.6020, valid accuracy: 0.8214\n",
      "Iter-5460 train loss: 0.6469 valid loss: 0.6014, valid accuracy: 0.8216\n",
      "Iter-5470 train loss: 0.5552 valid loss: 0.6009, valid accuracy: 0.8210\n",
      "Iter-5480 train loss: 0.5313 valid loss: 0.6002, valid accuracy: 0.8208\n",
      "Iter-5490 train loss: 0.7230 valid loss: 0.5996, valid accuracy: 0.8216\n",
      "Iter-5500 train loss: 0.6348 valid loss: 0.5993, valid accuracy: 0.8220\n",
      "Iter-5510 train loss: 0.4650 valid loss: 0.5983, valid accuracy: 0.8224\n",
      "Iter-5520 train loss: 0.5295 valid loss: 0.5977, valid accuracy: 0.8224\n",
      "Iter-5530 train loss: 0.7420 valid loss: 0.5974, valid accuracy: 0.8226\n",
      "Iter-5540 train loss: 0.6346 valid loss: 0.5968, valid accuracy: 0.8220\n",
      "Iter-5550 train loss: 0.5023 valid loss: 0.5962, valid accuracy: 0.8218\n",
      "Iter-5560 train loss: 0.5917 valid loss: 0.5954, valid accuracy: 0.8226\n",
      "Iter-5570 train loss: 0.7834 valid loss: 0.5947, valid accuracy: 0.8232\n",
      "Iter-5580 train loss: 0.4941 valid loss: 0.5939, valid accuracy: 0.8224\n",
      "Iter-5590 train loss: 0.5840 valid loss: 0.5933, valid accuracy: 0.8224\n",
      "Iter-5600 train loss: 0.5250 valid loss: 0.5927, valid accuracy: 0.8224\n",
      "Iter-5610 train loss: 0.4046 valid loss: 0.5920, valid accuracy: 0.8224\n",
      "Iter-5620 train loss: 0.6146 valid loss: 0.5913, valid accuracy: 0.8224\n",
      "Iter-5630 train loss: 0.7611 valid loss: 0.5906, valid accuracy: 0.8230\n",
      "Iter-5640 train loss: 0.6197 valid loss: 0.5901, valid accuracy: 0.8230\n",
      "Iter-5650 train loss: 0.6073 valid loss: 0.5896, valid accuracy: 0.8230\n",
      "Iter-5660 train loss: 0.6602 valid loss: 0.5889, valid accuracy: 0.8230\n",
      "Iter-5670 train loss: 0.7677 valid loss: 0.5882, valid accuracy: 0.8232\n",
      "Iter-5680 train loss: 0.5234 valid loss: 0.5874, valid accuracy: 0.8236\n",
      "Iter-5690 train loss: 0.7781 valid loss: 0.5868, valid accuracy: 0.8236\n",
      "Iter-5700 train loss: 0.5027 valid loss: 0.5860, valid accuracy: 0.8246\n",
      "Iter-5710 train loss: 0.6381 valid loss: 0.5855, valid accuracy: 0.8250\n",
      "Iter-5720 train loss: 0.7361 valid loss: 0.5849, valid accuracy: 0.8254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 0.5935 valid loss: 0.5843, valid accuracy: 0.8248\n",
      "Iter-5740 train loss: 0.7790 valid loss: 0.5837, valid accuracy: 0.8250\n",
      "Iter-5750 train loss: 0.6441 valid loss: 0.5832, valid accuracy: 0.8256\n",
      "Iter-5760 train loss: 0.6381 valid loss: 0.5827, valid accuracy: 0.8258\n",
      "Iter-5770 train loss: 0.7397 valid loss: 0.5821, valid accuracy: 0.8254\n",
      "Iter-5780 train loss: 0.6265 valid loss: 0.5815, valid accuracy: 0.8264\n",
      "Iter-5790 train loss: 0.5745 valid loss: 0.5808, valid accuracy: 0.8262\n",
      "Iter-5800 train loss: 0.5726 valid loss: 0.5802, valid accuracy: 0.8274\n",
      "Iter-5810 train loss: 0.6612 valid loss: 0.5796, valid accuracy: 0.8272\n",
      "Iter-5820 train loss: 0.5860 valid loss: 0.5792, valid accuracy: 0.8274\n",
      "Iter-5830 train loss: 0.6484 valid loss: 0.5787, valid accuracy: 0.8272\n",
      "Iter-5840 train loss: 0.5522 valid loss: 0.5781, valid accuracy: 0.8272\n",
      "Iter-5850 train loss: 0.5492 valid loss: 0.5775, valid accuracy: 0.8264\n",
      "Iter-5860 train loss: 0.4056 valid loss: 0.5768, valid accuracy: 0.8268\n",
      "Iter-5870 train loss: 0.4002 valid loss: 0.5763, valid accuracy: 0.8274\n",
      "Iter-5880 train loss: 0.5762 valid loss: 0.5758, valid accuracy: 0.8270\n",
      "Iter-5890 train loss: 0.7308 valid loss: 0.5752, valid accuracy: 0.8276\n",
      "Iter-5900 train loss: 0.6244 valid loss: 0.5746, valid accuracy: 0.8274\n",
      "Iter-5910 train loss: 0.6195 valid loss: 0.5742, valid accuracy: 0.8272\n",
      "Iter-5920 train loss: 0.6046 valid loss: 0.5737, valid accuracy: 0.8274\n",
      "Iter-5930 train loss: 0.4871 valid loss: 0.5732, valid accuracy: 0.8274\n",
      "Iter-5940 train loss: 0.4969 valid loss: 0.5727, valid accuracy: 0.8278\n",
      "Iter-5950 train loss: 0.6950 valid loss: 0.5722, valid accuracy: 0.8274\n",
      "Iter-5960 train loss: 0.6056 valid loss: 0.5717, valid accuracy: 0.8282\n",
      "Iter-5970 train loss: 0.5884 valid loss: 0.5711, valid accuracy: 0.8270\n",
      "Iter-5980 train loss: 0.6351 valid loss: 0.5705, valid accuracy: 0.8270\n",
      "Iter-5990 train loss: 0.5076 valid loss: 0.5701, valid accuracy: 0.8278\n",
      "Iter-6000 train loss: 0.5593 valid loss: 0.5697, valid accuracy: 0.8276\n",
      "Iter-6010 train loss: 0.6675 valid loss: 0.5691, valid accuracy: 0.8274\n",
      "Iter-6020 train loss: 0.6663 valid loss: 0.5686, valid accuracy: 0.8272\n",
      "Iter-6030 train loss: 0.4840 valid loss: 0.5680, valid accuracy: 0.8276\n",
      "Iter-6040 train loss: 0.5522 valid loss: 0.5675, valid accuracy: 0.8292\n",
      "Iter-6050 train loss: 0.5440 valid loss: 0.5671, valid accuracy: 0.8300\n",
      "Iter-6060 train loss: 0.7112 valid loss: 0.5667, valid accuracy: 0.8294\n",
      "Iter-6070 train loss: 0.6593 valid loss: 0.5662, valid accuracy: 0.8294\n",
      "Iter-6080 train loss: 0.5650 valid loss: 0.5657, valid accuracy: 0.8298\n",
      "Iter-6090 train loss: 0.6436 valid loss: 0.5652, valid accuracy: 0.8298\n",
      "Iter-6100 train loss: 0.5977 valid loss: 0.5647, valid accuracy: 0.8300\n",
      "Iter-6110 train loss: 0.6998 valid loss: 0.5642, valid accuracy: 0.8298\n",
      "Iter-6120 train loss: 0.4783 valid loss: 0.5637, valid accuracy: 0.8302\n",
      "Iter-6130 train loss: 0.2578 valid loss: 0.5632, valid accuracy: 0.8304\n",
      "Iter-6140 train loss: 0.5946 valid loss: 0.5627, valid accuracy: 0.8308\n",
      "Iter-6150 train loss: 0.6057 valid loss: 0.5622, valid accuracy: 0.8306\n",
      "Iter-6160 train loss: 0.6560 valid loss: 0.5619, valid accuracy: 0.8298\n",
      "Iter-6170 train loss: 0.5204 valid loss: 0.5613, valid accuracy: 0.8308\n",
      "Iter-6180 train loss: 0.6478 valid loss: 0.5608, valid accuracy: 0.8304\n",
      "Iter-6190 train loss: 0.7102 valid loss: 0.5603, valid accuracy: 0.8310\n",
      "Iter-6200 train loss: 0.7251 valid loss: 0.5598, valid accuracy: 0.8314\n",
      "Iter-6210 train loss: 0.6187 valid loss: 0.5593, valid accuracy: 0.8322\n",
      "Iter-6220 train loss: 0.5066 valid loss: 0.5587, valid accuracy: 0.8324\n",
      "Iter-6230 train loss: 0.4558 valid loss: 0.5582, valid accuracy: 0.8326\n",
      "Iter-6240 train loss: 0.6211 valid loss: 0.5576, valid accuracy: 0.8330\n",
      "Iter-6250 train loss: 0.5585 valid loss: 0.5570, valid accuracy: 0.8334\n",
      "Iter-6260 train loss: 0.4669 valid loss: 0.5566, valid accuracy: 0.8334\n",
      "Iter-6270 train loss: 0.5718 valid loss: 0.5561, valid accuracy: 0.8336\n",
      "Iter-6280 train loss: 0.4637 valid loss: 0.5557, valid accuracy: 0.8340\n",
      "Iter-6290 train loss: 0.4764 valid loss: 0.5553, valid accuracy: 0.8340\n",
      "Iter-6300 train loss: 0.5509 valid loss: 0.5549, valid accuracy: 0.8342\n",
      "Iter-6310 train loss: 0.6820 valid loss: 0.5544, valid accuracy: 0.8344\n",
      "Iter-6320 train loss: 0.6896 valid loss: 0.5539, valid accuracy: 0.8344\n",
      "Iter-6330 train loss: 0.5183 valid loss: 0.5534, valid accuracy: 0.8346\n",
      "Iter-6340 train loss: 0.5944 valid loss: 0.5529, valid accuracy: 0.8348\n",
      "Iter-6350 train loss: 0.6137 valid loss: 0.5526, valid accuracy: 0.8350\n",
      "Iter-6360 train loss: 0.4846 valid loss: 0.5521, valid accuracy: 0.8350\n",
      "Iter-6370 train loss: 0.6111 valid loss: 0.5515, valid accuracy: 0.8344\n",
      "Iter-6380 train loss: 0.5664 valid loss: 0.5510, valid accuracy: 0.8350\n",
      "Iter-6390 train loss: 0.5220 valid loss: 0.5504, valid accuracy: 0.8350\n",
      "Iter-6400 train loss: 0.4299 valid loss: 0.5501, valid accuracy: 0.8348\n",
      "Iter-6410 train loss: 0.5336 valid loss: 0.5498, valid accuracy: 0.8352\n",
      "Iter-6420 train loss: 0.5302 valid loss: 0.5493, valid accuracy: 0.8346\n",
      "Iter-6430 train loss: 0.3191 valid loss: 0.5490, valid accuracy: 0.8354\n",
      "Iter-6440 train loss: 0.6111 valid loss: 0.5485, valid accuracy: 0.8358\n",
      "Iter-6450 train loss: 0.4377 valid loss: 0.5481, valid accuracy: 0.8358\n",
      "Iter-6460 train loss: 0.5559 valid loss: 0.5477, valid accuracy: 0.8366\n",
      "Iter-6470 train loss: 0.7006 valid loss: 0.5472, valid accuracy: 0.8360\n",
      "Iter-6480 train loss: 0.5724 valid loss: 0.5468, valid accuracy: 0.8358\n",
      "Iter-6490 train loss: 0.4597 valid loss: 0.5465, valid accuracy: 0.8358\n",
      "Iter-6500 train loss: 0.6015 valid loss: 0.5460, valid accuracy: 0.8362\n",
      "Iter-6510 train loss: 0.4321 valid loss: 0.5456, valid accuracy: 0.8364\n",
      "Iter-6520 train loss: 0.4585 valid loss: 0.5453, valid accuracy: 0.8360\n",
      "Iter-6530 train loss: 0.5788 valid loss: 0.5448, valid accuracy: 0.8364\n",
      "Iter-6540 train loss: 0.6480 valid loss: 0.5444, valid accuracy: 0.8364\n",
      "Iter-6550 train loss: 0.8281 valid loss: 0.5439, valid accuracy: 0.8366\n",
      "Iter-6560 train loss: 0.8056 valid loss: 0.5436, valid accuracy: 0.8368\n",
      "Iter-6570 train loss: 0.4793 valid loss: 0.5432, valid accuracy: 0.8368\n",
      "Iter-6580 train loss: 0.7919 valid loss: 0.5429, valid accuracy: 0.8366\n",
      "Iter-6590 train loss: 0.5289 valid loss: 0.5423, valid accuracy: 0.8372\n",
      "Iter-6600 train loss: 0.4451 valid loss: 0.5418, valid accuracy: 0.8374\n",
      "Iter-6610 train loss: 0.6542 valid loss: 0.5413, valid accuracy: 0.8372\n",
      "Iter-6620 train loss: 0.4059 valid loss: 0.5408, valid accuracy: 0.8370\n",
      "Iter-6630 train loss: 0.5367 valid loss: 0.5404, valid accuracy: 0.8376\n",
      "Iter-6640 train loss: 0.7469 valid loss: 0.5398, valid accuracy: 0.8384\n",
      "Iter-6650 train loss: 0.5950 valid loss: 0.5394, valid accuracy: 0.8386\n",
      "Iter-6660 train loss: 0.4324 valid loss: 0.5389, valid accuracy: 0.8386\n",
      "Iter-6670 train loss: 0.5533 valid loss: 0.5386, valid accuracy: 0.8386\n",
      "Iter-6680 train loss: 0.6148 valid loss: 0.5381, valid accuracy: 0.8386\n",
      "Iter-6690 train loss: 0.4828 valid loss: 0.5377, valid accuracy: 0.8386\n",
      "Iter-6700 train loss: 0.6035 valid loss: 0.5371, valid accuracy: 0.8388\n",
      "Iter-6710 train loss: 0.5613 valid loss: 0.5368, valid accuracy: 0.8384\n",
      "Iter-6720 train loss: 0.6345 valid loss: 0.5365, valid accuracy: 0.8382\n",
      "Iter-6730 train loss: 0.4065 valid loss: 0.5360, valid accuracy: 0.8384\n",
      "Iter-6740 train loss: 0.6287 valid loss: 0.5357, valid accuracy: 0.8386\n",
      "Iter-6750 train loss: 0.5232 valid loss: 0.5354, valid accuracy: 0.8382\n",
      "Iter-6760 train loss: 0.4907 valid loss: 0.5348, valid accuracy: 0.8388\n",
      "Iter-6770 train loss: 0.7328 valid loss: 0.5345, valid accuracy: 0.8380\n",
      "Iter-6780 train loss: 0.4835 valid loss: 0.5340, valid accuracy: 0.8382\n",
      "Iter-6790 train loss: 0.4484 valid loss: 0.5336, valid accuracy: 0.8384\n",
      "Iter-6800 train loss: 0.4465 valid loss: 0.5331, valid accuracy: 0.8386\n",
      "Iter-6810 train loss: 0.4137 valid loss: 0.5329, valid accuracy: 0.8388\n",
      "Iter-6820 train loss: 0.5012 valid loss: 0.5324, valid accuracy: 0.8390\n",
      "Iter-6830 train loss: 0.5632 valid loss: 0.5319, valid accuracy: 0.8390\n",
      "Iter-6840 train loss: 0.4768 valid loss: 0.5316, valid accuracy: 0.8392\n",
      "Iter-6850 train loss: 0.4967 valid loss: 0.5313, valid accuracy: 0.8396\n",
      "Iter-6860 train loss: 0.6434 valid loss: 0.5309, valid accuracy: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 0.4975 valid loss: 0.5304, valid accuracy: 0.8400\n",
      "Iter-6880 train loss: 0.4401 valid loss: 0.5300, valid accuracy: 0.8394\n",
      "Iter-6890 train loss: 0.5563 valid loss: 0.5298, valid accuracy: 0.8396\n",
      "Iter-6900 train loss: 0.3821 valid loss: 0.5295, valid accuracy: 0.8398\n",
      "Iter-6910 train loss: 0.4141 valid loss: 0.5291, valid accuracy: 0.8398\n",
      "Iter-6920 train loss: 0.8608 valid loss: 0.5288, valid accuracy: 0.8398\n",
      "Iter-6930 train loss: 0.5848 valid loss: 0.5283, valid accuracy: 0.8404\n",
      "Iter-6940 train loss: 0.5032 valid loss: 0.5278, valid accuracy: 0.8398\n",
      "Iter-6950 train loss: 0.5878 valid loss: 0.5276, valid accuracy: 0.8398\n",
      "Iter-6960 train loss: 0.7785 valid loss: 0.5273, valid accuracy: 0.8404\n",
      "Iter-6970 train loss: 0.5619 valid loss: 0.5267, valid accuracy: 0.8406\n",
      "Iter-6980 train loss: 0.3878 valid loss: 0.5263, valid accuracy: 0.8402\n",
      "Iter-6990 train loss: 0.6071 valid loss: 0.5259, valid accuracy: 0.8404\n",
      "Iter-7000 train loss: 0.5577 valid loss: 0.5258, valid accuracy: 0.8406\n",
      "Iter-7010 train loss: 0.6313 valid loss: 0.5253, valid accuracy: 0.8408\n",
      "Iter-7020 train loss: 0.5674 valid loss: 0.5248, valid accuracy: 0.8406\n",
      "Iter-7030 train loss: 0.5225 valid loss: 0.5244, valid accuracy: 0.8408\n",
      "Iter-7040 train loss: 0.5979 valid loss: 0.5241, valid accuracy: 0.8408\n",
      "Iter-7050 train loss: 0.3187 valid loss: 0.5239, valid accuracy: 0.8408\n",
      "Iter-7060 train loss: 0.4704 valid loss: 0.5235, valid accuracy: 0.8408\n",
      "Iter-7070 train loss: 0.6453 valid loss: 0.5231, valid accuracy: 0.8412\n",
      "Iter-7080 train loss: 0.5984 valid loss: 0.5229, valid accuracy: 0.8422\n",
      "Iter-7090 train loss: 0.5493 valid loss: 0.5223, valid accuracy: 0.8418\n",
      "Iter-7100 train loss: 0.5144 valid loss: 0.5218, valid accuracy: 0.8418\n",
      "Iter-7110 train loss: 0.4683 valid loss: 0.5216, valid accuracy: 0.8426\n",
      "Iter-7120 train loss: 0.6983 valid loss: 0.5213, valid accuracy: 0.8426\n",
      "Iter-7130 train loss: 0.6875 valid loss: 0.5209, valid accuracy: 0.8428\n",
      "Iter-7140 train loss: 0.8938 valid loss: 0.5206, valid accuracy: 0.8424\n",
      "Iter-7150 train loss: 0.6593 valid loss: 0.5202, valid accuracy: 0.8426\n",
      "Iter-7160 train loss: 0.5209 valid loss: 0.5199, valid accuracy: 0.8426\n",
      "Iter-7170 train loss: 0.4771 valid loss: 0.5196, valid accuracy: 0.8428\n",
      "Iter-7180 train loss: 0.4624 valid loss: 0.5191, valid accuracy: 0.8428\n",
      "Iter-7190 train loss: 0.5194 valid loss: 0.5186, valid accuracy: 0.8426\n",
      "Iter-7200 train loss: 0.4561 valid loss: 0.5183, valid accuracy: 0.8430\n",
      "Iter-7210 train loss: 0.5054 valid loss: 0.5181, valid accuracy: 0.8432\n",
      "Iter-7220 train loss: 0.3987 valid loss: 0.5176, valid accuracy: 0.8432\n",
      "Iter-7230 train loss: 0.6364 valid loss: 0.5175, valid accuracy: 0.8442\n",
      "Iter-7240 train loss: 0.4231 valid loss: 0.5172, valid accuracy: 0.8438\n",
      "Iter-7250 train loss: 0.4149 valid loss: 0.5169, valid accuracy: 0.8442\n",
      "Iter-7260 train loss: 0.4929 valid loss: 0.5165, valid accuracy: 0.8440\n",
      "Iter-7270 train loss: 0.4307 valid loss: 0.5161, valid accuracy: 0.8442\n",
      "Iter-7280 train loss: 0.6476 valid loss: 0.5156, valid accuracy: 0.8440\n",
      "Iter-7290 train loss: 0.6075 valid loss: 0.5151, valid accuracy: 0.8440\n",
      "Iter-7300 train loss: 0.7928 valid loss: 0.5146, valid accuracy: 0.8454\n",
      "Iter-7310 train loss: 0.7278 valid loss: 0.5142, valid accuracy: 0.8460\n",
      "Iter-7320 train loss: 0.7406 valid loss: 0.5139, valid accuracy: 0.8464\n",
      "Iter-7330 train loss: 0.6746 valid loss: 0.5136, valid accuracy: 0.8466\n",
      "Iter-7340 train loss: 0.6227 valid loss: 0.5133, valid accuracy: 0.8462\n",
      "Iter-7350 train loss: 0.5739 valid loss: 0.5131, valid accuracy: 0.8460\n",
      "Iter-7360 train loss: 0.5665 valid loss: 0.5128, valid accuracy: 0.8466\n",
      "Iter-7370 train loss: 0.4518 valid loss: 0.5125, valid accuracy: 0.8458\n",
      "Iter-7380 train loss: 0.6068 valid loss: 0.5120, valid accuracy: 0.8462\n",
      "Iter-7390 train loss: 0.4942 valid loss: 0.5117, valid accuracy: 0.8462\n",
      "Iter-7400 train loss: 0.6460 valid loss: 0.5113, valid accuracy: 0.8468\n",
      "Iter-7410 train loss: 0.5437 valid loss: 0.5109, valid accuracy: 0.8476\n",
      "Iter-7420 train loss: 0.6910 valid loss: 0.5105, valid accuracy: 0.8482\n",
      "Iter-7430 train loss: 0.6288 valid loss: 0.5103, valid accuracy: 0.8480\n",
      "Iter-7440 train loss: 0.4741 valid loss: 0.5101, valid accuracy: 0.8482\n",
      "Iter-7450 train loss: 0.5977 valid loss: 0.5096, valid accuracy: 0.8490\n",
      "Iter-7460 train loss: 0.4703 valid loss: 0.5094, valid accuracy: 0.8482\n",
      "Iter-7470 train loss: 0.5013 valid loss: 0.5090, valid accuracy: 0.8486\n",
      "Iter-7480 train loss: 0.4563 valid loss: 0.5085, valid accuracy: 0.8500\n",
      "Iter-7490 train loss: 0.6334 valid loss: 0.5082, valid accuracy: 0.8498\n",
      "Iter-7500 train loss: 0.3597 valid loss: 0.5078, valid accuracy: 0.8504\n",
      "Iter-7510 train loss: 0.6674 valid loss: 0.5076, valid accuracy: 0.8502\n",
      "Iter-7520 train loss: 0.4661 valid loss: 0.5075, valid accuracy: 0.8496\n",
      "Iter-7530 train loss: 0.3687 valid loss: 0.5071, valid accuracy: 0.8500\n",
      "Iter-7540 train loss: 0.6666 valid loss: 0.5067, valid accuracy: 0.8498\n",
      "Iter-7550 train loss: 0.5041 valid loss: 0.5066, valid accuracy: 0.8502\n",
      "Iter-7560 train loss: 0.6818 valid loss: 0.5062, valid accuracy: 0.8498\n",
      "Iter-7570 train loss: 0.6244 valid loss: 0.5058, valid accuracy: 0.8498\n",
      "Iter-7580 train loss: 0.6435 valid loss: 0.5054, valid accuracy: 0.8500\n",
      "Iter-7590 train loss: 0.4034 valid loss: 0.5050, valid accuracy: 0.8498\n",
      "Iter-7600 train loss: 0.6028 valid loss: 0.5048, valid accuracy: 0.8500\n",
      "Iter-7610 train loss: 0.5273 valid loss: 0.5045, valid accuracy: 0.8498\n",
      "Iter-7620 train loss: 0.5498 valid loss: 0.5042, valid accuracy: 0.8498\n",
      "Iter-7630 train loss: 0.5879 valid loss: 0.5040, valid accuracy: 0.8500\n",
      "Iter-7640 train loss: 0.6388 valid loss: 0.5040, valid accuracy: 0.8508\n",
      "Iter-7650 train loss: 0.6074 valid loss: 0.5039, valid accuracy: 0.8500\n",
      "Iter-7660 train loss: 0.3945 valid loss: 0.5036, valid accuracy: 0.8500\n",
      "Iter-7670 train loss: 0.6583 valid loss: 0.5032, valid accuracy: 0.8508\n",
      "Iter-7680 train loss: 0.5843 valid loss: 0.5030, valid accuracy: 0.8504\n",
      "Iter-7690 train loss: 0.5182 valid loss: 0.5028, valid accuracy: 0.8504\n",
      "Iter-7700 train loss: 0.5388 valid loss: 0.5024, valid accuracy: 0.8506\n",
      "Iter-7710 train loss: 0.5637 valid loss: 0.5020, valid accuracy: 0.8508\n",
      "Iter-7720 train loss: 0.5420 valid loss: 0.5018, valid accuracy: 0.8506\n",
      "Iter-7730 train loss: 0.5083 valid loss: 0.5014, valid accuracy: 0.8504\n",
      "Iter-7740 train loss: 0.4157 valid loss: 0.5010, valid accuracy: 0.8506\n",
      "Iter-7750 train loss: 0.4618 valid loss: 0.5006, valid accuracy: 0.8510\n",
      "Iter-7760 train loss: 0.3875 valid loss: 0.5004, valid accuracy: 0.8516\n",
      "Iter-7770 train loss: 0.6363 valid loss: 0.5000, valid accuracy: 0.8518\n",
      "Iter-7780 train loss: 0.4703 valid loss: 0.4998, valid accuracy: 0.8520\n",
      "Iter-7790 train loss: 0.5273 valid loss: 0.4995, valid accuracy: 0.8518\n",
      "Iter-7800 train loss: 0.4952 valid loss: 0.4994, valid accuracy: 0.8514\n",
      "Iter-7810 train loss: 0.4151 valid loss: 0.4991, valid accuracy: 0.8514\n",
      "Iter-7820 train loss: 0.5088 valid loss: 0.4988, valid accuracy: 0.8518\n",
      "Iter-7830 train loss: 0.5006 valid loss: 0.4984, valid accuracy: 0.8526\n",
      "Iter-7840 train loss: 0.5145 valid loss: 0.4979, valid accuracy: 0.8522\n",
      "Iter-7850 train loss: 0.3690 valid loss: 0.4977, valid accuracy: 0.8524\n",
      "Iter-7860 train loss: 0.3500 valid loss: 0.4974, valid accuracy: 0.8524\n",
      "Iter-7870 train loss: 0.3869 valid loss: 0.4972, valid accuracy: 0.8526\n",
      "Iter-7880 train loss: 0.4810 valid loss: 0.4968, valid accuracy: 0.8524\n",
      "Iter-7890 train loss: 0.4496 valid loss: 0.4966, valid accuracy: 0.8526\n",
      "Iter-7900 train loss: 0.6679 valid loss: 0.4961, valid accuracy: 0.8528\n",
      "Iter-7910 train loss: 0.6228 valid loss: 0.4959, valid accuracy: 0.8528\n",
      "Iter-7920 train loss: 0.5016 valid loss: 0.4956, valid accuracy: 0.8532\n",
      "Iter-7930 train loss: 0.4390 valid loss: 0.4954, valid accuracy: 0.8532\n",
      "Iter-7940 train loss: 0.6274 valid loss: 0.4950, valid accuracy: 0.8532\n",
      "Iter-7950 train loss: 0.6971 valid loss: 0.4947, valid accuracy: 0.8536\n",
      "Iter-7960 train loss: 0.7985 valid loss: 0.4944, valid accuracy: 0.8538\n",
      "Iter-7970 train loss: 0.4195 valid loss: 0.4941, valid accuracy: 0.8536\n",
      "Iter-7980 train loss: 0.4729 valid loss: 0.4938, valid accuracy: 0.8536\n",
      "Iter-7990 train loss: 0.4426 valid loss: 0.4934, valid accuracy: 0.8538\n",
      "Iter-8000 train loss: 0.5435 valid loss: 0.4931, valid accuracy: 0.8536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 0.5117 valid loss: 0.4929, valid accuracy: 0.8538\n",
      "Iter-8020 train loss: 0.5385 valid loss: 0.4926, valid accuracy: 0.8540\n",
      "Iter-8030 train loss: 0.3371 valid loss: 0.4922, valid accuracy: 0.8538\n",
      "Iter-8040 train loss: 0.5707 valid loss: 0.4919, valid accuracy: 0.8534\n",
      "Iter-8050 train loss: 0.5963 valid loss: 0.4916, valid accuracy: 0.8536\n",
      "Iter-8060 train loss: 0.5315 valid loss: 0.4913, valid accuracy: 0.8538\n",
      "Iter-8070 train loss: 0.4368 valid loss: 0.4911, valid accuracy: 0.8538\n",
      "Iter-8080 train loss: 0.5429 valid loss: 0.4907, valid accuracy: 0.8538\n",
      "Iter-8090 train loss: 0.4245 valid loss: 0.4905, valid accuracy: 0.8542\n",
      "Iter-8100 train loss: 0.6170 valid loss: 0.4901, valid accuracy: 0.8544\n",
      "Iter-8110 train loss: 0.5624 valid loss: 0.4898, valid accuracy: 0.8542\n",
      "Iter-8120 train loss: 0.5634 valid loss: 0.4895, valid accuracy: 0.8542\n",
      "Iter-8130 train loss: 0.4665 valid loss: 0.4893, valid accuracy: 0.8540\n",
      "Iter-8140 train loss: 0.6743 valid loss: 0.4891, valid accuracy: 0.8544\n",
      "Iter-8150 train loss: 0.5415 valid loss: 0.4888, valid accuracy: 0.8552\n",
      "Iter-8160 train loss: 0.5948 valid loss: 0.4886, valid accuracy: 0.8548\n",
      "Iter-8170 train loss: 0.4905 valid loss: 0.4884, valid accuracy: 0.8552\n",
      "Iter-8180 train loss: 0.4342 valid loss: 0.4880, valid accuracy: 0.8552\n",
      "Iter-8190 train loss: 0.4688 valid loss: 0.4878, valid accuracy: 0.8554\n",
      "Iter-8200 train loss: 0.3669 valid loss: 0.4875, valid accuracy: 0.8556\n",
      "Iter-8210 train loss: 0.4758 valid loss: 0.4872, valid accuracy: 0.8554\n",
      "Iter-8220 train loss: 0.4909 valid loss: 0.4870, valid accuracy: 0.8556\n",
      "Iter-8230 train loss: 0.5809 valid loss: 0.4868, valid accuracy: 0.8558\n",
      "Iter-8240 train loss: 0.5272 valid loss: 0.4867, valid accuracy: 0.8554\n",
      "Iter-8250 train loss: 0.4149 valid loss: 0.4863, valid accuracy: 0.8554\n",
      "Iter-8260 train loss: 0.5753 valid loss: 0.4862, valid accuracy: 0.8560\n",
      "Iter-8270 train loss: 0.4884 valid loss: 0.4858, valid accuracy: 0.8556\n",
      "Iter-8280 train loss: 0.2889 valid loss: 0.4856, valid accuracy: 0.8562\n",
      "Iter-8290 train loss: 0.3437 valid loss: 0.4854, valid accuracy: 0.8560\n",
      "Iter-8300 train loss: 0.5572 valid loss: 0.4851, valid accuracy: 0.8560\n",
      "Iter-8310 train loss: 0.7150 valid loss: 0.4849, valid accuracy: 0.8562\n",
      "Iter-8320 train loss: 0.4888 valid loss: 0.4846, valid accuracy: 0.8558\n",
      "Iter-8330 train loss: 0.4458 valid loss: 0.4844, valid accuracy: 0.8558\n",
      "Iter-8340 train loss: 0.6269 valid loss: 0.4840, valid accuracy: 0.8558\n",
      "Iter-8350 train loss: 0.4140 valid loss: 0.4836, valid accuracy: 0.8570\n",
      "Iter-8360 train loss: 0.4850 valid loss: 0.4834, valid accuracy: 0.8560\n",
      "Iter-8370 train loss: 0.5050 valid loss: 0.4830, valid accuracy: 0.8570\n",
      "Iter-8380 train loss: 0.7592 valid loss: 0.4827, valid accuracy: 0.8572\n",
      "Iter-8390 train loss: 0.4141 valid loss: 0.4824, valid accuracy: 0.8570\n",
      "Iter-8400 train loss: 0.4709 valid loss: 0.4822, valid accuracy: 0.8576\n",
      "Iter-8410 train loss: 0.4506 valid loss: 0.4821, valid accuracy: 0.8576\n",
      "Iter-8420 train loss: 0.5763 valid loss: 0.4819, valid accuracy: 0.8578\n",
      "Iter-8430 train loss: 0.5223 valid loss: 0.4817, valid accuracy: 0.8572\n",
      "Iter-8440 train loss: 0.5031 valid loss: 0.4815, valid accuracy: 0.8568\n",
      "Iter-8450 train loss: 0.7282 valid loss: 0.4812, valid accuracy: 0.8576\n",
      "Iter-8460 train loss: 0.7000 valid loss: 0.4811, valid accuracy: 0.8578\n",
      "Iter-8470 train loss: 0.4666 valid loss: 0.4808, valid accuracy: 0.8576\n",
      "Iter-8480 train loss: 0.3639 valid loss: 0.4803, valid accuracy: 0.8576\n",
      "Iter-8490 train loss: 0.5573 valid loss: 0.4800, valid accuracy: 0.8582\n",
      "Iter-8500 train loss: 0.4112 valid loss: 0.4797, valid accuracy: 0.8584\n",
      "Iter-8510 train loss: 0.5698 valid loss: 0.4795, valid accuracy: 0.8582\n",
      "Iter-8520 train loss: 0.3547 valid loss: 0.4792, valid accuracy: 0.8580\n",
      "Iter-8530 train loss: 0.5082 valid loss: 0.4790, valid accuracy: 0.8582\n",
      "Iter-8540 train loss: 0.4900 valid loss: 0.4788, valid accuracy: 0.8574\n",
      "Iter-8550 train loss: 0.4050 valid loss: 0.4784, valid accuracy: 0.8582\n",
      "Iter-8560 train loss: 0.4924 valid loss: 0.4783, valid accuracy: 0.8574\n",
      "Iter-8570 train loss: 0.3351 valid loss: 0.4781, valid accuracy: 0.8578\n",
      "Iter-8580 train loss: 0.3948 valid loss: 0.4779, valid accuracy: 0.8582\n",
      "Iter-8590 train loss: 0.3751 valid loss: 0.4776, valid accuracy: 0.8574\n",
      "Iter-8600 train loss: 0.4267 valid loss: 0.4774, valid accuracy: 0.8576\n",
      "Iter-8610 train loss: 0.4680 valid loss: 0.4771, valid accuracy: 0.8576\n",
      "Iter-8620 train loss: 0.4500 valid loss: 0.4769, valid accuracy: 0.8566\n",
      "Iter-8630 train loss: 0.5553 valid loss: 0.4767, valid accuracy: 0.8570\n",
      "Iter-8640 train loss: 0.3473 valid loss: 0.4765, valid accuracy: 0.8576\n",
      "Iter-8650 train loss: 0.5888 valid loss: 0.4762, valid accuracy: 0.8574\n",
      "Iter-8660 train loss: 0.5122 valid loss: 0.4763, valid accuracy: 0.8576\n",
      "Iter-8670 train loss: 0.3775 valid loss: 0.4760, valid accuracy: 0.8576\n",
      "Iter-8680 train loss: 0.5764 valid loss: 0.4757, valid accuracy: 0.8580\n",
      "Iter-8690 train loss: 0.4167 valid loss: 0.4755, valid accuracy: 0.8578\n",
      "Iter-8700 train loss: 0.3094 valid loss: 0.4753, valid accuracy: 0.8580\n",
      "Iter-8710 train loss: 0.6075 valid loss: 0.4750, valid accuracy: 0.8580\n",
      "Iter-8720 train loss: 0.5063 valid loss: 0.4747, valid accuracy: 0.8580\n",
      "Iter-8730 train loss: 0.5329 valid loss: 0.4746, valid accuracy: 0.8582\n",
      "Iter-8740 train loss: 0.2792 valid loss: 0.4743, valid accuracy: 0.8580\n",
      "Iter-8750 train loss: 0.5577 valid loss: 0.4741, valid accuracy: 0.8578\n",
      "Iter-8760 train loss: 0.5598 valid loss: 0.4739, valid accuracy: 0.8582\n",
      "Iter-8770 train loss: 0.5361 valid loss: 0.4737, valid accuracy: 0.8582\n",
      "Iter-8780 train loss: 0.7349 valid loss: 0.4735, valid accuracy: 0.8586\n",
      "Iter-8790 train loss: 0.5011 valid loss: 0.4734, valid accuracy: 0.8582\n",
      "Iter-8800 train loss: 0.5127 valid loss: 0.4732, valid accuracy: 0.8586\n",
      "Iter-8810 train loss: 0.4679 valid loss: 0.4730, valid accuracy: 0.8584\n",
      "Iter-8820 train loss: 0.4745 valid loss: 0.4727, valid accuracy: 0.8586\n",
      "Iter-8830 train loss: 0.8466 valid loss: 0.4726, valid accuracy: 0.8584\n",
      "Iter-8840 train loss: 0.5336 valid loss: 0.4723, valid accuracy: 0.8584\n",
      "Iter-8850 train loss: 0.3861 valid loss: 0.4722, valid accuracy: 0.8586\n",
      "Iter-8860 train loss: 0.5554 valid loss: 0.4719, valid accuracy: 0.8584\n",
      "Iter-8870 train loss: 0.4799 valid loss: 0.4717, valid accuracy: 0.8586\n",
      "Iter-8880 train loss: 0.4601 valid loss: 0.4715, valid accuracy: 0.8590\n",
      "Iter-8890 train loss: 0.4633 valid loss: 0.4712, valid accuracy: 0.8590\n",
      "Iter-8900 train loss: 0.5020 valid loss: 0.4709, valid accuracy: 0.8596\n",
      "Iter-8910 train loss: 0.5431 valid loss: 0.4706, valid accuracy: 0.8594\n",
      "Iter-8920 train loss: 0.8338 valid loss: 0.4702, valid accuracy: 0.8598\n",
      "Iter-8930 train loss: 0.3642 valid loss: 0.4700, valid accuracy: 0.8594\n",
      "Iter-8940 train loss: 0.5299 valid loss: 0.4696, valid accuracy: 0.8598\n",
      "Iter-8950 train loss: 0.8438 valid loss: 0.4693, valid accuracy: 0.8604\n",
      "Iter-8960 train loss: 0.5568 valid loss: 0.4691, valid accuracy: 0.8602\n",
      "Iter-8970 train loss: 0.4840 valid loss: 0.4688, valid accuracy: 0.8604\n",
      "Iter-8980 train loss: 0.3858 valid loss: 0.4686, valid accuracy: 0.8604\n",
      "Iter-8990 train loss: 0.5351 valid loss: 0.4684, valid accuracy: 0.8602\n",
      "Iter-9000 train loss: 0.4257 valid loss: 0.4681, valid accuracy: 0.8604\n",
      "Iter-9010 train loss: 0.4031 valid loss: 0.4678, valid accuracy: 0.8600\n",
      "Iter-9020 train loss: 0.4815 valid loss: 0.4677, valid accuracy: 0.8608\n",
      "Iter-9030 train loss: 0.3868 valid loss: 0.4676, valid accuracy: 0.8598\n",
      "Iter-9040 train loss: 0.6129 valid loss: 0.4673, valid accuracy: 0.8604\n",
      "Iter-9050 train loss: 0.4713 valid loss: 0.4672, valid accuracy: 0.8608\n",
      "Iter-9060 train loss: 0.5473 valid loss: 0.4670, valid accuracy: 0.8612\n",
      "Iter-9070 train loss: 0.5042 valid loss: 0.4669, valid accuracy: 0.8612\n",
      "Iter-9080 train loss: 0.5280 valid loss: 0.4666, valid accuracy: 0.8610\n",
      "Iter-9090 train loss: 0.5195 valid loss: 0.4664, valid accuracy: 0.8612\n",
      "Iter-9100 train loss: 0.6233 valid loss: 0.4662, valid accuracy: 0.8612\n",
      "Iter-9110 train loss: 0.4599 valid loss: 0.4660, valid accuracy: 0.8604\n",
      "Iter-9120 train loss: 0.4391 valid loss: 0.4659, valid accuracy: 0.8604\n",
      "Iter-9130 train loss: 0.4119 valid loss: 0.4656, valid accuracy: 0.8602\n",
      "Iter-9140 train loss: 0.5075 valid loss: 0.4653, valid accuracy: 0.8612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 0.6719 valid loss: 0.4652, valid accuracy: 0.8616\n",
      "Iter-9160 train loss: 0.3615 valid loss: 0.4651, valid accuracy: 0.8614\n",
      "Iter-9170 train loss: 0.5809 valid loss: 0.4648, valid accuracy: 0.8616\n",
      "Iter-9180 train loss: 0.4907 valid loss: 0.4645, valid accuracy: 0.8618\n",
      "Iter-9190 train loss: 0.3691 valid loss: 0.4644, valid accuracy: 0.8622\n",
      "Iter-9200 train loss: 0.5910 valid loss: 0.4641, valid accuracy: 0.8622\n",
      "Iter-9210 train loss: 0.3768 valid loss: 0.4640, valid accuracy: 0.8622\n",
      "Iter-9220 train loss: 0.4390 valid loss: 0.4638, valid accuracy: 0.8622\n",
      "Iter-9230 train loss: 0.3690 valid loss: 0.4637, valid accuracy: 0.8622\n",
      "Iter-9240 train loss: 0.6202 valid loss: 0.4636, valid accuracy: 0.8618\n",
      "Iter-9250 train loss: 0.5426 valid loss: 0.4633, valid accuracy: 0.8620\n",
      "Iter-9260 train loss: 0.4318 valid loss: 0.4631, valid accuracy: 0.8622\n",
      "Iter-9270 train loss: 0.4668 valid loss: 0.4629, valid accuracy: 0.8620\n",
      "Iter-9280 train loss: 0.4538 valid loss: 0.4628, valid accuracy: 0.8628\n",
      "Iter-9290 train loss: 0.7562 valid loss: 0.4627, valid accuracy: 0.8622\n",
      "Iter-9300 train loss: 0.4496 valid loss: 0.4624, valid accuracy: 0.8620\n",
      "Iter-9310 train loss: 0.5250 valid loss: 0.4622, valid accuracy: 0.8624\n",
      "Iter-9320 train loss: 0.3609 valid loss: 0.4621, valid accuracy: 0.8620\n",
      "Iter-9330 train loss: 0.4657 valid loss: 0.4618, valid accuracy: 0.8620\n",
      "Iter-9340 train loss: 0.7297 valid loss: 0.4617, valid accuracy: 0.8618\n",
      "Iter-9350 train loss: 0.4070 valid loss: 0.4615, valid accuracy: 0.8618\n",
      "Iter-9360 train loss: 0.4582 valid loss: 0.4612, valid accuracy: 0.8622\n",
      "Iter-9370 train loss: 0.4994 valid loss: 0.4610, valid accuracy: 0.8620\n",
      "Iter-9380 train loss: 0.3984 valid loss: 0.4607, valid accuracy: 0.8622\n",
      "Iter-9390 train loss: 0.4037 valid loss: 0.4606, valid accuracy: 0.8624\n",
      "Iter-9400 train loss: 0.4141 valid loss: 0.4603, valid accuracy: 0.8622\n",
      "Iter-9410 train loss: 0.4173 valid loss: 0.4601, valid accuracy: 0.8624\n",
      "Iter-9420 train loss: 0.5273 valid loss: 0.4599, valid accuracy: 0.8624\n",
      "Iter-9430 train loss: 0.6555 valid loss: 0.4597, valid accuracy: 0.8620\n",
      "Iter-9440 train loss: 0.5375 valid loss: 0.4597, valid accuracy: 0.8620\n",
      "Iter-9450 train loss: 0.5967 valid loss: 0.4594, valid accuracy: 0.8622\n",
      "Iter-9460 train loss: 0.5760 valid loss: 0.4592, valid accuracy: 0.8626\n",
      "Iter-9470 train loss: 0.4521 valid loss: 0.4590, valid accuracy: 0.8624\n",
      "Iter-9480 train loss: 0.4747 valid loss: 0.4587, valid accuracy: 0.8626\n",
      "Iter-9490 train loss: 0.3699 valid loss: 0.4586, valid accuracy: 0.8628\n",
      "Iter-9500 train loss: 0.6829 valid loss: 0.4586, valid accuracy: 0.8626\n",
      "Iter-9510 train loss: 0.4480 valid loss: 0.4584, valid accuracy: 0.8634\n",
      "Iter-9520 train loss: 0.3819 valid loss: 0.4582, valid accuracy: 0.8630\n",
      "Iter-9530 train loss: 0.4729 valid loss: 0.4579, valid accuracy: 0.8634\n",
      "Iter-9540 train loss: 0.3669 valid loss: 0.4577, valid accuracy: 0.8634\n",
      "Iter-9550 train loss: 0.5396 valid loss: 0.4576, valid accuracy: 0.8634\n",
      "Iter-9560 train loss: 0.6825 valid loss: 0.4573, valid accuracy: 0.8634\n",
      "Iter-9570 train loss: 0.6583 valid loss: 0.4571, valid accuracy: 0.8632\n",
      "Iter-9580 train loss: 0.4522 valid loss: 0.4569, valid accuracy: 0.8636\n",
      "Iter-9590 train loss: 0.4752 valid loss: 0.4567, valid accuracy: 0.8638\n",
      "Iter-9600 train loss: 0.2445 valid loss: 0.4565, valid accuracy: 0.8634\n",
      "Iter-9610 train loss: 0.3994 valid loss: 0.4564, valid accuracy: 0.8638\n",
      "Iter-9620 train loss: 0.5494 valid loss: 0.4562, valid accuracy: 0.8640\n",
      "Iter-9630 train loss: 0.3695 valid loss: 0.4561, valid accuracy: 0.8636\n",
      "Iter-9640 train loss: 0.4406 valid loss: 0.4560, valid accuracy: 0.8640\n",
      "Iter-9650 train loss: 0.4610 valid loss: 0.4559, valid accuracy: 0.8644\n",
      "Iter-9660 train loss: 0.3981 valid loss: 0.4556, valid accuracy: 0.8638\n",
      "Iter-9670 train loss: 0.4302 valid loss: 0.4554, valid accuracy: 0.8640\n",
      "Iter-9680 train loss: 0.4572 valid loss: 0.4550, valid accuracy: 0.8638\n",
      "Iter-9690 train loss: 0.3924 valid loss: 0.4548, valid accuracy: 0.8640\n",
      "Iter-9700 train loss: 0.5337 valid loss: 0.4545, valid accuracy: 0.8644\n",
      "Iter-9710 train loss: 0.3093 valid loss: 0.4543, valid accuracy: 0.8642\n",
      "Iter-9720 train loss: 0.6665 valid loss: 0.4540, valid accuracy: 0.8644\n",
      "Iter-9730 train loss: 0.4574 valid loss: 0.4539, valid accuracy: 0.8646\n",
      "Iter-9740 train loss: 0.3413 valid loss: 0.4537, valid accuracy: 0.8648\n",
      "Iter-9750 train loss: 0.6429 valid loss: 0.4536, valid accuracy: 0.8652\n",
      "Iter-9760 train loss: 0.7662 valid loss: 0.4534, valid accuracy: 0.8648\n",
      "Iter-9770 train loss: 0.3917 valid loss: 0.4533, valid accuracy: 0.8646\n",
      "Iter-9780 train loss: 0.4594 valid loss: 0.4531, valid accuracy: 0.8646\n",
      "Iter-9790 train loss: 0.5518 valid loss: 0.4530, valid accuracy: 0.8650\n",
      "Iter-9800 train loss: 0.2067 valid loss: 0.4528, valid accuracy: 0.8654\n",
      "Iter-9810 train loss: 0.4230 valid loss: 0.4527, valid accuracy: 0.8648\n",
      "Iter-9820 train loss: 0.4801 valid loss: 0.4524, valid accuracy: 0.8640\n",
      "Iter-9830 train loss: 0.5369 valid loss: 0.4521, valid accuracy: 0.8642\n",
      "Iter-9840 train loss: 0.4428 valid loss: 0.4520, valid accuracy: 0.8642\n",
      "Iter-9850 train loss: 0.3561 valid loss: 0.4517, valid accuracy: 0.8644\n",
      "Iter-9860 train loss: 0.4002 valid loss: 0.4516, valid accuracy: 0.8648\n",
      "Iter-9870 train loss: 0.4991 valid loss: 0.4514, valid accuracy: 0.8648\n",
      "Iter-9880 train loss: 0.4942 valid loss: 0.4512, valid accuracy: 0.8646\n",
      "Iter-9890 train loss: 0.3305 valid loss: 0.4510, valid accuracy: 0.8646\n",
      "Iter-9900 train loss: 0.5071 valid loss: 0.4508, valid accuracy: 0.8648\n",
      "Iter-9910 train loss: 0.4000 valid loss: 0.4506, valid accuracy: 0.8650\n",
      "Iter-9920 train loss: 0.4713 valid loss: 0.4504, valid accuracy: 0.8648\n",
      "Iter-9930 train loss: 0.4004 valid loss: 0.4502, valid accuracy: 0.8646\n",
      "Iter-9940 train loss: 0.2800 valid loss: 0.4501, valid accuracy: 0.8648\n",
      "Iter-9950 train loss: 0.4108 valid loss: 0.4499, valid accuracy: 0.8650\n",
      "Iter-9960 train loss: 0.3955 valid loss: 0.4498, valid accuracy: 0.8650\n",
      "Iter-9970 train loss: 0.3746 valid loss: 0.4496, valid accuracy: 0.8656\n",
      "Iter-9980 train loss: 0.6430 valid loss: 0.4494, valid accuracy: 0.8650\n",
      "Iter-9990 train loss: 0.6128 valid loss: 0.4493, valid accuracy: 0.8656\n",
      "Iter-10000 train loss: 0.4403 valid loss: 0.4492, valid accuracy: 0.8660\n",
      "Last iteration - Test accuracy mean: 0.8669, std: 0.0000, loss: 0.4513\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc316733e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZe3SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc316733358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
