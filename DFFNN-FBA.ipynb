{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.20440721,  2.49302746,  0.98729385],\n",
       "        [ 1.20849947,  1.23471936,  0.90151452]]),\n",
       " array([[ 0.08897284,  0.23757702, -0.44273927],\n",
       "        [ 0.24259661, -0.3744405 ,  0.34546899]]),\n",
       " array([[-0.17964842,  0.65576582, -0.66647448],\n",
       "        [ 0.17007776,  0.32311674,  0.09524021]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But, how do we initialize B, the fixed-random matrix?\n",
    "# Let me summarize both initializations for you:\n",
    "# Lillicrap’s B were sampled from a uniform distribution in the range of\n",
    "# -0.5 to 0.5.\n",
    "# Nøkland’s B were sampled from a uniform distribution in the range of\n",
    "# [−1/√fanout, 1/√fanout]\n",
    "# fanout: the number of inputs that can be connected to a specified output.\n",
    "D = 2\n",
    "H= 3\n",
    "np.random.randn(D, H) / np.sqrt(D / 2.), np.random.uniform(low=-0.5, high=0.5, size=(D, H)), np.random.uniform(low= (-1./np.sqrt(D)), high=(1./np.sqrt(D)), size=(D, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "#         self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "#         self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.randn(D, H) / np.sqrt(D / 2.), # ff weight - flexible/ adaptive/ learnable\n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        \n",
    "        # Input layer fixed weight for feedback\n",
    "#         m = np.random.randn(D, H) / np.sqrt(D / 2.) # fb weight - fixed\n",
    "        m = np.random.uniform(low= (-1./np.sqrt(D)), high=(1./np.sqrt(D)), size=(D, H))\n",
    "#         m = np.random.uniform(low=-0.5, high=0.5), size=(D, H))\n",
    "        self.W_fixed.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m = dict(W=np.random.randn(H, H) / np.sqrt(H / 2.), # ff weight - adaptive\n",
    "                 b=np.zeros((1, H)))\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        \n",
    "        # Hidden layers fixed weight feedback alignment\n",
    "#         m = np.random.randn(H, H) / np.sqrt(H / 2.)\n",
    "        m = np.random.uniform(low= (-1./np.sqrt(H)), high=(1./np.sqrt(H)), size=(H, H))\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.randn(H, C) / np.sqrt(H / 2.), # ff weight - adaptive\n",
    "                 b=np.zeros((1, C)),\n",
    "                 W_fixed=np.random.randn(H, C) / np.sqrt(H / 2.)) # fb weight - fixed\n",
    "        self.model.append(m)\n",
    "        \n",
    "        # Output layer fixed weight for feedback alignment\n",
    "#         m = np.random.randn(H, C) / np.sqrt(H / 2.)\n",
    "        m = np.random.uniform(low= (-1./np.sqrt(H)), high=(1./np.sqrt(H)), size=(H, C))\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, model):\n",
    "        W, b = model['W'], model['b']\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "#         print('db.shape', db.shape)\n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "#         y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, fc_cache = self.fc_forward(X=X, model=self.model[0]) # X_1xD, y_1xc\n",
    "        y, nl_cache = selu_forward(X=y)\n",
    "        X = y.copy() # pass the previous output to the next layer\n",
    "        caches.append((fc_cache, nl_cache)) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "#             y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, fc_cache = self.fc_forward(X=X, model=self.model[1][layer])\n",
    "            y, nl_cache = selu_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            nl_caches.append(nl_cache)\n",
    "        caches.append((fc_caches, nl_caches)) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "#         y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y, fc_cache = self.fc_forward(X=X, model=self.model[2])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "#         grads = self.model.copy()\n",
    "#         print('dy.shape', dy.shape)\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass it to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "#         print('''grads[2]['W'].shape, grads[2]['b'].shape''', grads[2]['W'].shape, grads[2]['b'].shape)\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = selu_backward(dout=dy, cache=nl_caches[layer])\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "#             print('''grads[1][]layer['W'].shape, grads[1][layer]['b'].shape''', grads[1][layer]['W'].shape, \n",
    "#                   grads[1][layer]['b'].shape)\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = selu_backward(dout=dy, cache=nl_cache)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "#         print('''grads[0]['W'].shape, grads[0]['b'].shape''', grads[0]['W'].shape, grads[0]['b'].shape)\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Momentums\n",
    "        M, R = [], []\n",
    "\n",
    "        # Input layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers momentum\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        # Output layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    " \n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "#                 M[0][key] = l.exp_running_avg(M[0][key], grads[0][key], beta1)\n",
    "#                 R[0][key] = l.exp_running_avg(R[0][key], grads[0][key]**2, beta2)\n",
    "#                 m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "#                 r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "#                 self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "#                     M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grads[1][layer][key], beta1)\n",
    "#                     R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grads[1][layer][key]**2, beta2)\n",
    "#                     m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "#                     r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "#                     self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "#                 M[2][key] = l.exp_running_avg(M[2][key], grads[2][key], beta1)\n",
    "#                 R[2][key] = l.exp_running_avg(R[2][key], grads[2][key]**2, beta2)\n",
    "#                 m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "#                 r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "#                 self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 3.0591 valid loss: 3.0599, valid accuracy: 0.0608\n",
      "Iter-20 train loss: 2.9647 valid loss: 3.0509, valid accuracy: 0.0632\n",
      "Iter-30 train loss: 3.1849 valid loss: 3.0412, valid accuracy: 0.0636\n",
      "Iter-40 train loss: 2.9782 valid loss: 3.0318, valid accuracy: 0.0646\n",
      "Iter-50 train loss: 3.2150 valid loss: 3.0230, valid accuracy: 0.0648\n",
      "Iter-60 train loss: 2.8700 valid loss: 3.0136, valid accuracy: 0.0658\n",
      "Iter-70 train loss: 2.9629 valid loss: 3.0052, valid accuracy: 0.0658\n",
      "Iter-80 train loss: 2.9305 valid loss: 2.9969, valid accuracy: 0.0672\n",
      "Iter-90 train loss: 3.1197 valid loss: 2.9899, valid accuracy: 0.0674\n",
      "Iter-100 train loss: 3.0295 valid loss: 2.9828, valid accuracy: 0.0672\n",
      "Iter-110 train loss: 2.8472 valid loss: 2.9756, valid accuracy: 0.0674\n",
      "Iter-120 train loss: 2.9774 valid loss: 2.9684, valid accuracy: 0.0688\n",
      "Iter-130 train loss: 3.2362 valid loss: 2.9612, valid accuracy: 0.0690\n",
      "Iter-140 train loss: 3.2176 valid loss: 2.9542, valid accuracy: 0.0698\n",
      "Iter-150 train loss: 2.8371 valid loss: 2.9471, valid accuracy: 0.0694\n",
      "Iter-160 train loss: 2.9118 valid loss: 2.9408, valid accuracy: 0.0694\n",
      "Iter-170 train loss: 3.1486 valid loss: 2.9338, valid accuracy: 0.0700\n",
      "Iter-180 train loss: 2.7450 valid loss: 2.9277, valid accuracy: 0.0708\n",
      "Iter-190 train loss: 2.9993 valid loss: 2.9217, valid accuracy: 0.0726\n",
      "Iter-200 train loss: 2.7418 valid loss: 2.9155, valid accuracy: 0.0740\n",
      "Iter-210 train loss: 3.0755 valid loss: 2.9096, valid accuracy: 0.0742\n",
      "Iter-220 train loss: 2.8844 valid loss: 2.9038, valid accuracy: 0.0758\n",
      "Iter-230 train loss: 2.7935 valid loss: 2.8983, valid accuracy: 0.0766\n",
      "Iter-240 train loss: 2.7544 valid loss: 2.8926, valid accuracy: 0.0768\n",
      "Iter-250 train loss: 2.8232 valid loss: 2.8871, valid accuracy: 0.0774\n",
      "Iter-260 train loss: 2.9396 valid loss: 2.8818, valid accuracy: 0.0774\n",
      "Iter-270 train loss: 3.1104 valid loss: 2.8770, valid accuracy: 0.0776\n",
      "Iter-280 train loss: 3.0039 valid loss: 2.8717, valid accuracy: 0.0782\n",
      "Iter-290 train loss: 3.1283 valid loss: 2.8660, valid accuracy: 0.0790\n",
      "Iter-300 train loss: 2.7997 valid loss: 2.8607, valid accuracy: 0.0796\n",
      "Iter-310 train loss: 2.6943 valid loss: 2.8551, valid accuracy: 0.0794\n",
      "Iter-320 train loss: 2.7599 valid loss: 2.8498, valid accuracy: 0.0800\n",
      "Iter-330 train loss: 2.8761 valid loss: 2.8444, valid accuracy: 0.0804\n",
      "Iter-340 train loss: 2.8221 valid loss: 2.8396, valid accuracy: 0.0810\n",
      "Iter-350 train loss: 2.8737 valid loss: 2.8346, valid accuracy: 0.0814\n",
      "Iter-360 train loss: 2.9993 valid loss: 2.8298, valid accuracy: 0.0820\n",
      "Iter-370 train loss: 2.9389 valid loss: 2.8249, valid accuracy: 0.0824\n",
      "Iter-380 train loss: 2.8835 valid loss: 2.8201, valid accuracy: 0.0832\n",
      "Iter-390 train loss: 2.8740 valid loss: 2.8149, valid accuracy: 0.0830\n",
      "Iter-400 train loss: 2.9165 valid loss: 2.8098, valid accuracy: 0.0834\n",
      "Iter-410 train loss: 2.8260 valid loss: 2.8047, valid accuracy: 0.0834\n",
      "Iter-420 train loss: 2.8897 valid loss: 2.7998, valid accuracy: 0.0838\n",
      "Iter-430 train loss: 2.8080 valid loss: 2.7949, valid accuracy: 0.0844\n",
      "Iter-440 train loss: 2.8202 valid loss: 2.7898, valid accuracy: 0.0850\n",
      "Iter-450 train loss: 2.7737 valid loss: 2.7849, valid accuracy: 0.0864\n",
      "Iter-460 train loss: 2.7713 valid loss: 2.7796, valid accuracy: 0.0870\n",
      "Iter-470 train loss: 2.5422 valid loss: 2.7747, valid accuracy: 0.0884\n",
      "Iter-480 train loss: 2.8122 valid loss: 2.7701, valid accuracy: 0.0890\n",
      "Iter-490 train loss: 2.7239 valid loss: 2.7648, valid accuracy: 0.0894\n",
      "Iter-500 train loss: 2.7243 valid loss: 2.7598, valid accuracy: 0.0896\n",
      "Iter-510 train loss: 2.6622 valid loss: 2.7549, valid accuracy: 0.0896\n",
      "Iter-520 train loss: 2.8047 valid loss: 2.7502, valid accuracy: 0.0902\n",
      "Iter-530 train loss: 2.9822 valid loss: 2.7447, valid accuracy: 0.0908\n",
      "Iter-540 train loss: 2.6027 valid loss: 2.7398, valid accuracy: 0.0914\n",
      "Iter-550 train loss: 2.6732 valid loss: 2.7346, valid accuracy: 0.0908\n",
      "Iter-560 train loss: 2.7458 valid loss: 2.7288, valid accuracy: 0.0914\n",
      "Iter-570 train loss: 2.5526 valid loss: 2.7233, valid accuracy: 0.0914\n",
      "Iter-580 train loss: 2.6832 valid loss: 2.7177, valid accuracy: 0.0912\n",
      "Iter-590 train loss: 2.7851 valid loss: 2.7126, valid accuracy: 0.0918\n",
      "Iter-600 train loss: 2.6979 valid loss: 2.7070, valid accuracy: 0.0924\n",
      "Iter-610 train loss: 2.7271 valid loss: 2.7017, valid accuracy: 0.0932\n",
      "Iter-620 train loss: 2.6308 valid loss: 2.6968, valid accuracy: 0.0944\n",
      "Iter-630 train loss: 2.5286 valid loss: 2.6916, valid accuracy: 0.0960\n",
      "Iter-640 train loss: 2.6748 valid loss: 2.6859, valid accuracy: 0.0968\n",
      "Iter-650 train loss: 2.5789 valid loss: 2.6807, valid accuracy: 0.0982\n",
      "Iter-660 train loss: 2.8522 valid loss: 2.6755, valid accuracy: 0.0982\n",
      "Iter-670 train loss: 2.7325 valid loss: 2.6702, valid accuracy: 0.1000\n",
      "Iter-680 train loss: 2.4486 valid loss: 2.6653, valid accuracy: 0.1010\n",
      "Iter-690 train loss: 2.5118 valid loss: 2.6600, valid accuracy: 0.1016\n",
      "Iter-700 train loss: 2.5994 valid loss: 2.6549, valid accuracy: 0.1024\n",
      "Iter-710 train loss: 2.7628 valid loss: 2.6497, valid accuracy: 0.1026\n",
      "Iter-720 train loss: 2.8615 valid loss: 2.6441, valid accuracy: 0.1040\n",
      "Iter-730 train loss: 2.6483 valid loss: 2.6385, valid accuracy: 0.1040\n",
      "Iter-740 train loss: 2.6914 valid loss: 2.6331, valid accuracy: 0.1050\n",
      "Iter-750 train loss: 2.6840 valid loss: 2.6275, valid accuracy: 0.1052\n",
      "Iter-760 train loss: 2.4795 valid loss: 2.6224, valid accuracy: 0.1048\n",
      "Iter-770 train loss: 2.6907 valid loss: 2.6169, valid accuracy: 0.1056\n",
      "Iter-780 train loss: 2.5878 valid loss: 2.6116, valid accuracy: 0.1062\n",
      "Iter-790 train loss: 2.5514 valid loss: 2.6063, valid accuracy: 0.1066\n",
      "Iter-800 train loss: 2.3620 valid loss: 2.6010, valid accuracy: 0.1080\n",
      "Iter-810 train loss: 2.5444 valid loss: 2.5954, valid accuracy: 0.1078\n",
      "Iter-820 train loss: 2.7323 valid loss: 2.5897, valid accuracy: 0.1080\n",
      "Iter-830 train loss: 2.5987 valid loss: 2.5842, valid accuracy: 0.1084\n",
      "Iter-840 train loss: 2.7567 valid loss: 2.5792, valid accuracy: 0.1100\n",
      "Iter-850 train loss: 2.4772 valid loss: 2.5735, valid accuracy: 0.1116\n",
      "Iter-860 train loss: 2.7496 valid loss: 2.5679, valid accuracy: 0.1130\n",
      "Iter-870 train loss: 2.6102 valid loss: 2.5625, valid accuracy: 0.1152\n",
      "Iter-880 train loss: 2.5247 valid loss: 2.5570, valid accuracy: 0.1156\n",
      "Iter-890 train loss: 2.3573 valid loss: 2.5516, valid accuracy: 0.1162\n",
      "Iter-900 train loss: 2.6328 valid loss: 2.5453, valid accuracy: 0.1174\n",
      "Iter-910 train loss: 2.6072 valid loss: 2.5396, valid accuracy: 0.1182\n",
      "Iter-920 train loss: 2.4539 valid loss: 2.5348, valid accuracy: 0.1190\n",
      "Iter-930 train loss: 2.6976 valid loss: 2.5291, valid accuracy: 0.1198\n",
      "Iter-940 train loss: 2.4330 valid loss: 2.5235, valid accuracy: 0.1204\n",
      "Iter-950 train loss: 2.4785 valid loss: 2.5179, valid accuracy: 0.1210\n",
      "Iter-960 train loss: 2.5846 valid loss: 2.5124, valid accuracy: 0.1214\n",
      "Iter-970 train loss: 2.4580 valid loss: 2.5065, valid accuracy: 0.1218\n",
      "Iter-980 train loss: 2.4668 valid loss: 2.5009, valid accuracy: 0.1232\n",
      "Iter-990 train loss: 2.5651 valid loss: 2.4952, valid accuracy: 0.1246\n",
      "Iter-1000 train loss: 2.5381 valid loss: 2.4895, valid accuracy: 0.1250\n",
      "Iter-1010 train loss: 2.5641 valid loss: 2.4838, valid accuracy: 0.1258\n",
      "Iter-1020 train loss: 2.4542 valid loss: 2.4781, valid accuracy: 0.1264\n",
      "Iter-1030 train loss: 2.3373 valid loss: 2.4727, valid accuracy: 0.1278\n",
      "Iter-1040 train loss: 2.5034 valid loss: 2.4675, valid accuracy: 0.1298\n",
      "Iter-1050 train loss: 2.2922 valid loss: 2.4618, valid accuracy: 0.1308\n",
      "Iter-1060 train loss: 2.2962 valid loss: 2.4562, valid accuracy: 0.1318\n",
      "Iter-1070 train loss: 2.5140 valid loss: 2.4507, valid accuracy: 0.1324\n",
      "Iter-1080 train loss: 2.3606 valid loss: 2.4455, valid accuracy: 0.1338\n",
      "Iter-1090 train loss: 2.4610 valid loss: 2.4404, valid accuracy: 0.1346\n",
      "Iter-1100 train loss: 2.5538 valid loss: 2.4349, valid accuracy: 0.1352\n",
      "Iter-1110 train loss: 2.4114 valid loss: 2.4295, valid accuracy: 0.1370\n",
      "Iter-1120 train loss: 2.4580 valid loss: 2.4242, valid accuracy: 0.1384\n",
      "Iter-1130 train loss: 2.3697 valid loss: 2.4185, valid accuracy: 0.1396\n",
      "Iter-1140 train loss: 2.4752 valid loss: 2.4130, valid accuracy: 0.1414\n",
      "Iter-1150 train loss: 2.3400 valid loss: 2.4081, valid accuracy: 0.1424\n",
      "Iter-1160 train loss: 2.2775 valid loss: 2.4028, valid accuracy: 0.1430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.4458 valid loss: 2.3974, valid accuracy: 0.1440\n",
      "Iter-1180 train loss: 2.4070 valid loss: 2.3916, valid accuracy: 0.1448\n",
      "Iter-1190 train loss: 2.3666 valid loss: 2.3864, valid accuracy: 0.1456\n",
      "Iter-1200 train loss: 2.2057 valid loss: 2.3805, valid accuracy: 0.1468\n",
      "Iter-1210 train loss: 2.3230 valid loss: 2.3751, valid accuracy: 0.1482\n",
      "Iter-1220 train loss: 2.3971 valid loss: 2.3698, valid accuracy: 0.1494\n",
      "Iter-1230 train loss: 2.5170 valid loss: 2.3643, valid accuracy: 0.1504\n",
      "Iter-1240 train loss: 2.3465 valid loss: 2.3589, valid accuracy: 0.1518\n",
      "Iter-1250 train loss: 2.2979 valid loss: 2.3539, valid accuracy: 0.1524\n",
      "Iter-1260 train loss: 2.3899 valid loss: 2.3489, valid accuracy: 0.1528\n",
      "Iter-1270 train loss: 2.5057 valid loss: 2.3431, valid accuracy: 0.1532\n",
      "Iter-1280 train loss: 2.4079 valid loss: 2.3377, valid accuracy: 0.1542\n",
      "Iter-1290 train loss: 2.2328 valid loss: 2.3325, valid accuracy: 0.1564\n",
      "Iter-1300 train loss: 2.4511 valid loss: 2.3278, valid accuracy: 0.1576\n",
      "Iter-1310 train loss: 2.4443 valid loss: 2.3226, valid accuracy: 0.1592\n",
      "Iter-1320 train loss: 2.3571 valid loss: 2.3175, valid accuracy: 0.1596\n",
      "Iter-1330 train loss: 2.2902 valid loss: 2.3126, valid accuracy: 0.1622\n",
      "Iter-1340 train loss: 2.3514 valid loss: 2.3074, valid accuracy: 0.1650\n",
      "Iter-1350 train loss: 2.3761 valid loss: 2.3025, valid accuracy: 0.1656\n",
      "Iter-1360 train loss: 2.3857 valid loss: 2.2974, valid accuracy: 0.1660\n",
      "Iter-1370 train loss: 2.2335 valid loss: 2.2921, valid accuracy: 0.1674\n",
      "Iter-1380 train loss: 2.2172 valid loss: 2.2870, valid accuracy: 0.1678\n",
      "Iter-1390 train loss: 2.3231 valid loss: 2.2818, valid accuracy: 0.1690\n",
      "Iter-1400 train loss: 2.2917 valid loss: 2.2772, valid accuracy: 0.1714\n",
      "Iter-1410 train loss: 2.2580 valid loss: 2.2724, valid accuracy: 0.1726\n",
      "Iter-1420 train loss: 2.4006 valid loss: 2.2673, valid accuracy: 0.1752\n",
      "Iter-1430 train loss: 2.1703 valid loss: 2.2626, valid accuracy: 0.1770\n",
      "Iter-1440 train loss: 2.2894 valid loss: 2.2577, valid accuracy: 0.1774\n",
      "Iter-1450 train loss: 2.2811 valid loss: 2.2525, valid accuracy: 0.1780\n",
      "Iter-1460 train loss: 2.2747 valid loss: 2.2473, valid accuracy: 0.1798\n",
      "Iter-1470 train loss: 2.2376 valid loss: 2.2426, valid accuracy: 0.1818\n",
      "Iter-1480 train loss: 2.2053 valid loss: 2.2380, valid accuracy: 0.1830\n",
      "Iter-1490 train loss: 2.3374 valid loss: 2.2333, valid accuracy: 0.1846\n",
      "Iter-1500 train loss: 2.2039 valid loss: 2.2283, valid accuracy: 0.1862\n",
      "Iter-1510 train loss: 2.2946 valid loss: 2.2238, valid accuracy: 0.1878\n",
      "Iter-1520 train loss: 2.0502 valid loss: 2.2188, valid accuracy: 0.1896\n",
      "Iter-1530 train loss: 2.2233 valid loss: 2.2139, valid accuracy: 0.1916\n",
      "Iter-1540 train loss: 2.2951 valid loss: 2.2097, valid accuracy: 0.1948\n",
      "Iter-1550 train loss: 2.0705 valid loss: 2.2048, valid accuracy: 0.1960\n",
      "Iter-1560 train loss: 2.3410 valid loss: 2.2002, valid accuracy: 0.1978\n",
      "Iter-1570 train loss: 2.0563 valid loss: 2.1957, valid accuracy: 0.2002\n",
      "Iter-1580 train loss: 2.2266 valid loss: 2.1912, valid accuracy: 0.2018\n",
      "Iter-1590 train loss: 2.2390 valid loss: 2.1866, valid accuracy: 0.2028\n",
      "Iter-1600 train loss: 2.3774 valid loss: 2.1824, valid accuracy: 0.2054\n",
      "Iter-1610 train loss: 2.1330 valid loss: 2.1778, valid accuracy: 0.2064\n",
      "Iter-1620 train loss: 2.1684 valid loss: 2.1732, valid accuracy: 0.2084\n",
      "Iter-1630 train loss: 2.2227 valid loss: 2.1685, valid accuracy: 0.2102\n",
      "Iter-1640 train loss: 2.2319 valid loss: 2.1636, valid accuracy: 0.2106\n",
      "Iter-1650 train loss: 2.3544 valid loss: 2.1593, valid accuracy: 0.2142\n",
      "Iter-1660 train loss: 2.1710 valid loss: 2.1549, valid accuracy: 0.2164\n",
      "Iter-1670 train loss: 2.2724 valid loss: 2.1502, valid accuracy: 0.2194\n",
      "Iter-1680 train loss: 1.9913 valid loss: 2.1458, valid accuracy: 0.2220\n",
      "Iter-1690 train loss: 2.3115 valid loss: 2.1414, valid accuracy: 0.2242\n",
      "Iter-1700 train loss: 1.9901 valid loss: 2.1373, valid accuracy: 0.2262\n",
      "Iter-1710 train loss: 2.1607 valid loss: 2.1325, valid accuracy: 0.2286\n",
      "Iter-1720 train loss: 2.1916 valid loss: 2.1279, valid accuracy: 0.2308\n",
      "Iter-1730 train loss: 2.2042 valid loss: 2.1232, valid accuracy: 0.2332\n",
      "Iter-1740 train loss: 2.3385 valid loss: 2.1190, valid accuracy: 0.2348\n",
      "Iter-1750 train loss: 2.2453 valid loss: 2.1148, valid accuracy: 0.2356\n",
      "Iter-1760 train loss: 2.0723 valid loss: 2.1107, valid accuracy: 0.2384\n",
      "Iter-1770 train loss: 2.1277 valid loss: 2.1065, valid accuracy: 0.2408\n",
      "Iter-1780 train loss: 2.3259 valid loss: 2.1022, valid accuracy: 0.2430\n",
      "Iter-1790 train loss: 2.1788 valid loss: 2.0980, valid accuracy: 0.2440\n",
      "Iter-1800 train loss: 2.0281 valid loss: 2.0939, valid accuracy: 0.2446\n",
      "Iter-1810 train loss: 2.1790 valid loss: 2.0893, valid accuracy: 0.2478\n",
      "Iter-1820 train loss: 1.9449 valid loss: 2.0852, valid accuracy: 0.2498\n",
      "Iter-1830 train loss: 2.0362 valid loss: 2.0808, valid accuracy: 0.2516\n",
      "Iter-1840 train loss: 2.2097 valid loss: 2.0769, valid accuracy: 0.2528\n",
      "Iter-1850 train loss: 2.0699 valid loss: 2.0729, valid accuracy: 0.2548\n",
      "Iter-1860 train loss: 2.1429 valid loss: 2.0688, valid accuracy: 0.2578\n",
      "Iter-1870 train loss: 2.1879 valid loss: 2.0650, valid accuracy: 0.2602\n",
      "Iter-1880 train loss: 2.1357 valid loss: 2.0613, valid accuracy: 0.2636\n",
      "Iter-1890 train loss: 1.9656 valid loss: 2.0575, valid accuracy: 0.2648\n",
      "Iter-1900 train loss: 2.1077 valid loss: 2.0531, valid accuracy: 0.2664\n",
      "Iter-1910 train loss: 1.9397 valid loss: 2.0494, valid accuracy: 0.2688\n",
      "Iter-1920 train loss: 2.1789 valid loss: 2.0451, valid accuracy: 0.2694\n",
      "Iter-1930 train loss: 2.0416 valid loss: 2.0412, valid accuracy: 0.2706\n",
      "Iter-1940 train loss: 2.0424 valid loss: 2.0372, valid accuracy: 0.2728\n",
      "Iter-1950 train loss: 2.0351 valid loss: 2.0336, valid accuracy: 0.2752\n",
      "Iter-1960 train loss: 2.0656 valid loss: 2.0296, valid accuracy: 0.2772\n",
      "Iter-1970 train loss: 2.1164 valid loss: 2.0257, valid accuracy: 0.2784\n",
      "Iter-1980 train loss: 2.0468 valid loss: 2.0219, valid accuracy: 0.2798\n",
      "Iter-1990 train loss: 1.9941 valid loss: 2.0182, valid accuracy: 0.2814\n",
      "Iter-2000 train loss: 2.1432 valid loss: 2.0145, valid accuracy: 0.2836\n",
      "Iter-2010 train loss: 2.0079 valid loss: 2.0107, valid accuracy: 0.2842\n",
      "Iter-2020 train loss: 2.0261 valid loss: 2.0072, valid accuracy: 0.2852\n",
      "Iter-2030 train loss: 1.9148 valid loss: 2.0036, valid accuracy: 0.2868\n",
      "Iter-2040 train loss: 2.0678 valid loss: 1.9998, valid accuracy: 0.2890\n",
      "Iter-2050 train loss: 1.9972 valid loss: 1.9962, valid accuracy: 0.2896\n",
      "Iter-2060 train loss: 1.9966 valid loss: 1.9923, valid accuracy: 0.2916\n",
      "Iter-2070 train loss: 1.8923 valid loss: 1.9887, valid accuracy: 0.2938\n",
      "Iter-2080 train loss: 1.9579 valid loss: 1.9848, valid accuracy: 0.2944\n",
      "Iter-2090 train loss: 2.0111 valid loss: 1.9807, valid accuracy: 0.2966\n",
      "Iter-2100 train loss: 1.9254 valid loss: 1.9769, valid accuracy: 0.2980\n",
      "Iter-2110 train loss: 2.1093 valid loss: 1.9733, valid accuracy: 0.3006\n",
      "Iter-2120 train loss: 2.0327 valid loss: 1.9693, valid accuracy: 0.3010\n",
      "Iter-2130 train loss: 2.0696 valid loss: 1.9654, valid accuracy: 0.3024\n",
      "Iter-2140 train loss: 2.0242 valid loss: 1.9616, valid accuracy: 0.3038\n",
      "Iter-2150 train loss: 1.9670 valid loss: 1.9580, valid accuracy: 0.3062\n",
      "Iter-2160 train loss: 1.8720 valid loss: 1.9543, valid accuracy: 0.3082\n",
      "Iter-2170 train loss: 2.0567 valid loss: 1.9506, valid accuracy: 0.3102\n",
      "Iter-2180 train loss: 1.9047 valid loss: 1.9471, valid accuracy: 0.3122\n",
      "Iter-2190 train loss: 2.0518 valid loss: 1.9435, valid accuracy: 0.3138\n",
      "Iter-2200 train loss: 1.9370 valid loss: 1.9396, valid accuracy: 0.3156\n",
      "Iter-2210 train loss: 2.1171 valid loss: 1.9360, valid accuracy: 0.3174\n",
      "Iter-2220 train loss: 2.0557 valid loss: 1.9325, valid accuracy: 0.3186\n",
      "Iter-2230 train loss: 2.0171 valid loss: 1.9287, valid accuracy: 0.3202\n",
      "Iter-2240 train loss: 1.9878 valid loss: 1.9248, valid accuracy: 0.3220\n",
      "Iter-2250 train loss: 1.9952 valid loss: 1.9211, valid accuracy: 0.3234\n",
      "Iter-2260 train loss: 2.0591 valid loss: 1.9172, valid accuracy: 0.3246\n",
      "Iter-2270 train loss: 1.8715 valid loss: 1.9135, valid accuracy: 0.3258\n",
      "Iter-2280 train loss: 1.8934 valid loss: 1.9100, valid accuracy: 0.3268\n",
      "Iter-2290 train loss: 1.9819 valid loss: 1.9061, valid accuracy: 0.3278\n",
      "Iter-2300 train loss: 1.9727 valid loss: 1.9023, valid accuracy: 0.3290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 1.8536 valid loss: 1.8984, valid accuracy: 0.3300\n",
      "Iter-2320 train loss: 1.9083 valid loss: 1.8949, valid accuracy: 0.3318\n",
      "Iter-2330 train loss: 2.0854 valid loss: 1.8912, valid accuracy: 0.3332\n",
      "Iter-2340 train loss: 1.9733 valid loss: 1.8877, valid accuracy: 0.3346\n",
      "Iter-2350 train loss: 1.9227 valid loss: 1.8839, valid accuracy: 0.3358\n",
      "Iter-2360 train loss: 1.9262 valid loss: 1.8803, valid accuracy: 0.3368\n",
      "Iter-2370 train loss: 1.9802 valid loss: 1.8768, valid accuracy: 0.3382\n",
      "Iter-2380 train loss: 1.9217 valid loss: 1.8733, valid accuracy: 0.3404\n",
      "Iter-2390 train loss: 1.7460 valid loss: 1.8702, valid accuracy: 0.3430\n",
      "Iter-2400 train loss: 1.8564 valid loss: 1.8666, valid accuracy: 0.3448\n",
      "Iter-2410 train loss: 1.8110 valid loss: 1.8631, valid accuracy: 0.3452\n",
      "Iter-2420 train loss: 1.9021 valid loss: 1.8598, valid accuracy: 0.3460\n",
      "Iter-2430 train loss: 1.9344 valid loss: 1.8564, valid accuracy: 0.3476\n",
      "Iter-2440 train loss: 1.7646 valid loss: 1.8527, valid accuracy: 0.3506\n",
      "Iter-2450 train loss: 1.8927 valid loss: 1.8491, valid accuracy: 0.3524\n",
      "Iter-2460 train loss: 1.8089 valid loss: 1.8458, valid accuracy: 0.3534\n",
      "Iter-2470 train loss: 1.7524 valid loss: 1.8422, valid accuracy: 0.3556\n",
      "Iter-2480 train loss: 1.8980 valid loss: 1.8388, valid accuracy: 0.3572\n",
      "Iter-2490 train loss: 1.8304 valid loss: 1.8357, valid accuracy: 0.3590\n",
      "Iter-2500 train loss: 1.9317 valid loss: 1.8321, valid accuracy: 0.3592\n",
      "Iter-2510 train loss: 1.6938 valid loss: 1.8285, valid accuracy: 0.3596\n",
      "Iter-2520 train loss: 1.7078 valid loss: 1.8250, valid accuracy: 0.3616\n",
      "Iter-2530 train loss: 1.8174 valid loss: 1.8215, valid accuracy: 0.3626\n",
      "Iter-2540 train loss: 1.7872 valid loss: 1.8180, valid accuracy: 0.3644\n",
      "Iter-2550 train loss: 1.8723 valid loss: 1.8148, valid accuracy: 0.3664\n",
      "Iter-2560 train loss: 1.7689 valid loss: 1.8114, valid accuracy: 0.3670\n",
      "Iter-2570 train loss: 1.7490 valid loss: 1.8079, valid accuracy: 0.3686\n",
      "Iter-2580 train loss: 1.8982 valid loss: 1.8043, valid accuracy: 0.3696\n",
      "Iter-2590 train loss: 1.6571 valid loss: 1.8005, valid accuracy: 0.3716\n",
      "Iter-2600 train loss: 1.7429 valid loss: 1.7971, valid accuracy: 0.3740\n",
      "Iter-2610 train loss: 1.6416 valid loss: 1.7938, valid accuracy: 0.3760\n",
      "Iter-2620 train loss: 1.7905 valid loss: 1.7907, valid accuracy: 0.3766\n",
      "Iter-2630 train loss: 1.8094 valid loss: 1.7876, valid accuracy: 0.3782\n",
      "Iter-2640 train loss: 1.7810 valid loss: 1.7840, valid accuracy: 0.3798\n",
      "Iter-2650 train loss: 1.7461 valid loss: 1.7809, valid accuracy: 0.3800\n",
      "Iter-2660 train loss: 1.7212 valid loss: 1.7777, valid accuracy: 0.3814\n",
      "Iter-2670 train loss: 1.9747 valid loss: 1.7743, valid accuracy: 0.3834\n",
      "Iter-2680 train loss: 1.8025 valid loss: 1.7711, valid accuracy: 0.3842\n",
      "Iter-2690 train loss: 1.7805 valid loss: 1.7677, valid accuracy: 0.3846\n",
      "Iter-2700 train loss: 1.9059 valid loss: 1.7644, valid accuracy: 0.3858\n",
      "Iter-2710 train loss: 1.6050 valid loss: 1.7611, valid accuracy: 0.3868\n",
      "Iter-2720 train loss: 1.7802 valid loss: 1.7580, valid accuracy: 0.3876\n",
      "Iter-2730 train loss: 1.7317 valid loss: 1.7547, valid accuracy: 0.3884\n",
      "Iter-2740 train loss: 1.8001 valid loss: 1.7514, valid accuracy: 0.3908\n",
      "Iter-2750 train loss: 1.6960 valid loss: 1.7481, valid accuracy: 0.3924\n",
      "Iter-2760 train loss: 1.6013 valid loss: 1.7449, valid accuracy: 0.3934\n",
      "Iter-2770 train loss: 1.7572 valid loss: 1.7414, valid accuracy: 0.3946\n",
      "Iter-2780 train loss: 1.7203 valid loss: 1.7379, valid accuracy: 0.3966\n",
      "Iter-2790 train loss: 1.7842 valid loss: 1.7345, valid accuracy: 0.3984\n",
      "Iter-2800 train loss: 1.7538 valid loss: 1.7311, valid accuracy: 0.4002\n",
      "Iter-2810 train loss: 1.7316 valid loss: 1.7278, valid accuracy: 0.4022\n",
      "Iter-2820 train loss: 1.6126 valid loss: 1.7244, valid accuracy: 0.4038\n",
      "Iter-2830 train loss: 1.8148 valid loss: 1.7211, valid accuracy: 0.4052\n",
      "Iter-2840 train loss: 1.7207 valid loss: 1.7175, valid accuracy: 0.4074\n",
      "Iter-2850 train loss: 1.8939 valid loss: 1.7143, valid accuracy: 0.4082\n",
      "Iter-2860 train loss: 1.7380 valid loss: 1.7109, valid accuracy: 0.4100\n",
      "Iter-2870 train loss: 1.7333 valid loss: 1.7076, valid accuracy: 0.4134\n",
      "Iter-2880 train loss: 1.8425 valid loss: 1.7043, valid accuracy: 0.4148\n",
      "Iter-2890 train loss: 1.6370 valid loss: 1.7010, valid accuracy: 0.4168\n",
      "Iter-2900 train loss: 1.8216 valid loss: 1.6979, valid accuracy: 0.4186\n",
      "Iter-2910 train loss: 1.5753 valid loss: 1.6947, valid accuracy: 0.4198\n",
      "Iter-2920 train loss: 1.8217 valid loss: 1.6912, valid accuracy: 0.4224\n",
      "Iter-2930 train loss: 1.6923 valid loss: 1.6880, valid accuracy: 0.4238\n",
      "Iter-2940 train loss: 1.8268 valid loss: 1.6847, valid accuracy: 0.4270\n",
      "Iter-2950 train loss: 1.6327 valid loss: 1.6814, valid accuracy: 0.4278\n",
      "Iter-2960 train loss: 1.8046 valid loss: 1.6779, valid accuracy: 0.4304\n",
      "Iter-2970 train loss: 1.6428 valid loss: 1.6750, valid accuracy: 0.4324\n",
      "Iter-2980 train loss: 1.7346 valid loss: 1.6715, valid accuracy: 0.4342\n",
      "Iter-2990 train loss: 1.7423 valid loss: 1.6685, valid accuracy: 0.4356\n",
      "Iter-3000 train loss: 1.7316 valid loss: 1.6654, valid accuracy: 0.4380\n",
      "Iter-3010 train loss: 1.7328 valid loss: 1.6623, valid accuracy: 0.4392\n",
      "Iter-3020 train loss: 1.5622 valid loss: 1.6590, valid accuracy: 0.4396\n",
      "Iter-3030 train loss: 1.6772 valid loss: 1.6555, valid accuracy: 0.4412\n",
      "Iter-3040 train loss: 1.8101 valid loss: 1.6523, valid accuracy: 0.4426\n",
      "Iter-3050 train loss: 1.5917 valid loss: 1.6492, valid accuracy: 0.4444\n",
      "Iter-3060 train loss: 1.5884 valid loss: 1.6460, valid accuracy: 0.4458\n",
      "Iter-3070 train loss: 1.4137 valid loss: 1.6428, valid accuracy: 0.4478\n",
      "Iter-3080 train loss: 1.7197 valid loss: 1.6397, valid accuracy: 0.4486\n",
      "Iter-3090 train loss: 1.7301 valid loss: 1.6364, valid accuracy: 0.4494\n",
      "Iter-3100 train loss: 1.6462 valid loss: 1.6334, valid accuracy: 0.4506\n",
      "Iter-3110 train loss: 1.7721 valid loss: 1.6303, valid accuracy: 0.4526\n",
      "Iter-3120 train loss: 1.5353 valid loss: 1.6273, valid accuracy: 0.4542\n",
      "Iter-3130 train loss: 1.6827 valid loss: 1.6241, valid accuracy: 0.4556\n",
      "Iter-3140 train loss: 1.5862 valid loss: 1.6209, valid accuracy: 0.4564\n",
      "Iter-3150 train loss: 1.7115 valid loss: 1.6177, valid accuracy: 0.4582\n",
      "Iter-3160 train loss: 1.6399 valid loss: 1.6146, valid accuracy: 0.4600\n",
      "Iter-3170 train loss: 1.7148 valid loss: 1.6116, valid accuracy: 0.4618\n",
      "Iter-3180 train loss: 1.7283 valid loss: 1.6087, valid accuracy: 0.4624\n",
      "Iter-3190 train loss: 1.6360 valid loss: 1.6058, valid accuracy: 0.4638\n",
      "Iter-3200 train loss: 1.7012 valid loss: 1.6026, valid accuracy: 0.4654\n",
      "Iter-3210 train loss: 1.8619 valid loss: 1.5997, valid accuracy: 0.4664\n",
      "Iter-3220 train loss: 1.6685 valid loss: 1.5966, valid accuracy: 0.4676\n",
      "Iter-3230 train loss: 1.7103 valid loss: 1.5936, valid accuracy: 0.4694\n",
      "Iter-3240 train loss: 1.6912 valid loss: 1.5904, valid accuracy: 0.4710\n",
      "Iter-3250 train loss: 1.6127 valid loss: 1.5875, valid accuracy: 0.4728\n",
      "Iter-3260 train loss: 1.6775 valid loss: 1.5844, valid accuracy: 0.4730\n",
      "Iter-3270 train loss: 1.6142 valid loss: 1.5815, valid accuracy: 0.4740\n",
      "Iter-3280 train loss: 1.5204 valid loss: 1.5785, valid accuracy: 0.4760\n",
      "Iter-3290 train loss: 1.4857 valid loss: 1.5757, valid accuracy: 0.4770\n",
      "Iter-3300 train loss: 1.6562 valid loss: 1.5726, valid accuracy: 0.4794\n",
      "Iter-3310 train loss: 1.5250 valid loss: 1.5696, valid accuracy: 0.4812\n",
      "Iter-3320 train loss: 1.4494 valid loss: 1.5667, valid accuracy: 0.4824\n",
      "Iter-3330 train loss: 1.5645 valid loss: 1.5638, valid accuracy: 0.4826\n",
      "Iter-3340 train loss: 1.5803 valid loss: 1.5608, valid accuracy: 0.4844\n",
      "Iter-3350 train loss: 1.6020 valid loss: 1.5581, valid accuracy: 0.4874\n",
      "Iter-3360 train loss: 1.5439 valid loss: 1.5553, valid accuracy: 0.4884\n",
      "Iter-3370 train loss: 1.7463 valid loss: 1.5522, valid accuracy: 0.4900\n",
      "Iter-3380 train loss: 1.6656 valid loss: 1.5493, valid accuracy: 0.4904\n",
      "Iter-3390 train loss: 1.5587 valid loss: 1.5464, valid accuracy: 0.4922\n",
      "Iter-3400 train loss: 1.6559 valid loss: 1.5434, valid accuracy: 0.4934\n",
      "Iter-3410 train loss: 1.6200 valid loss: 1.5404, valid accuracy: 0.4940\n",
      "Iter-3420 train loss: 1.6711 valid loss: 1.5376, valid accuracy: 0.4952\n",
      "Iter-3430 train loss: 1.4506 valid loss: 1.5346, valid accuracy: 0.4958\n",
      "Iter-3440 train loss: 1.6107 valid loss: 1.5317, valid accuracy: 0.4970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 1.5049 valid loss: 1.5290, valid accuracy: 0.4980\n",
      "Iter-3460 train loss: 1.5374 valid loss: 1.5262, valid accuracy: 0.4996\n",
      "Iter-3470 train loss: 1.5282 valid loss: 1.5235, valid accuracy: 0.5000\n",
      "Iter-3480 train loss: 1.6483 valid loss: 1.5208, valid accuracy: 0.5010\n",
      "Iter-3490 train loss: 1.3861 valid loss: 1.5179, valid accuracy: 0.5030\n",
      "Iter-3500 train loss: 1.4984 valid loss: 1.5152, valid accuracy: 0.5034\n",
      "Iter-3510 train loss: 1.5955 valid loss: 1.5125, valid accuracy: 0.5046\n",
      "Iter-3520 train loss: 1.5746 valid loss: 1.5098, valid accuracy: 0.5066\n",
      "Iter-3530 train loss: 1.4634 valid loss: 1.5069, valid accuracy: 0.5080\n",
      "Iter-3540 train loss: 1.5696 valid loss: 1.5041, valid accuracy: 0.5094\n",
      "Iter-3550 train loss: 1.4502 valid loss: 1.5013, valid accuracy: 0.5110\n",
      "Iter-3560 train loss: 1.5068 valid loss: 1.4987, valid accuracy: 0.5128\n",
      "Iter-3570 train loss: 1.4921 valid loss: 1.4959, valid accuracy: 0.5136\n",
      "Iter-3580 train loss: 1.4462 valid loss: 1.4933, valid accuracy: 0.5164\n",
      "Iter-3590 train loss: 1.5671 valid loss: 1.4906, valid accuracy: 0.5182\n",
      "Iter-3600 train loss: 1.3700 valid loss: 1.4878, valid accuracy: 0.5198\n",
      "Iter-3610 train loss: 1.6220 valid loss: 1.4850, valid accuracy: 0.5220\n",
      "Iter-3620 train loss: 1.4579 valid loss: 1.4821, valid accuracy: 0.5230\n",
      "Iter-3630 train loss: 1.3632 valid loss: 1.4795, valid accuracy: 0.5240\n",
      "Iter-3640 train loss: 1.3357 valid loss: 1.4767, valid accuracy: 0.5246\n",
      "Iter-3650 train loss: 1.5518 valid loss: 1.4740, valid accuracy: 0.5266\n",
      "Iter-3660 train loss: 1.5441 valid loss: 1.4715, valid accuracy: 0.5276\n",
      "Iter-3670 train loss: 1.5479 valid loss: 1.4687, valid accuracy: 0.5288\n",
      "Iter-3680 train loss: 1.2389 valid loss: 1.4660, valid accuracy: 0.5300\n",
      "Iter-3690 train loss: 1.4256 valid loss: 1.4632, valid accuracy: 0.5304\n",
      "Iter-3700 train loss: 1.4174 valid loss: 1.4604, valid accuracy: 0.5322\n",
      "Iter-3710 train loss: 1.3855 valid loss: 1.4578, valid accuracy: 0.5330\n",
      "Iter-3720 train loss: 1.4173 valid loss: 1.4552, valid accuracy: 0.5334\n",
      "Iter-3730 train loss: 1.4342 valid loss: 1.4526, valid accuracy: 0.5338\n",
      "Iter-3740 train loss: 1.5586 valid loss: 1.4501, valid accuracy: 0.5342\n",
      "Iter-3750 train loss: 1.4075 valid loss: 1.4473, valid accuracy: 0.5352\n",
      "Iter-3760 train loss: 1.3829 valid loss: 1.4447, valid accuracy: 0.5364\n",
      "Iter-3770 train loss: 1.4276 valid loss: 1.4421, valid accuracy: 0.5382\n",
      "Iter-3780 train loss: 1.3873 valid loss: 1.4396, valid accuracy: 0.5382\n",
      "Iter-3790 train loss: 1.5030 valid loss: 1.4369, valid accuracy: 0.5384\n",
      "Iter-3800 train loss: 1.3846 valid loss: 1.4343, valid accuracy: 0.5408\n",
      "Iter-3810 train loss: 1.3427 valid loss: 1.4316, valid accuracy: 0.5426\n",
      "Iter-3820 train loss: 1.5129 valid loss: 1.4289, valid accuracy: 0.5438\n",
      "Iter-3830 train loss: 1.4655 valid loss: 1.4261, valid accuracy: 0.5450\n",
      "Iter-3840 train loss: 1.5165 valid loss: 1.4235, valid accuracy: 0.5456\n",
      "Iter-3850 train loss: 1.3922 valid loss: 1.4209, valid accuracy: 0.5470\n",
      "Iter-3860 train loss: 1.5536 valid loss: 1.4186, valid accuracy: 0.5484\n",
      "Iter-3870 train loss: 1.4344 valid loss: 1.4160, valid accuracy: 0.5488\n",
      "Iter-3880 train loss: 1.3392 valid loss: 1.4136, valid accuracy: 0.5496\n",
      "Iter-3890 train loss: 1.2906 valid loss: 1.4113, valid accuracy: 0.5512\n",
      "Iter-3900 train loss: 1.4975 valid loss: 1.4087, valid accuracy: 0.5522\n",
      "Iter-3910 train loss: 1.4117 valid loss: 1.4062, valid accuracy: 0.5530\n",
      "Iter-3920 train loss: 1.4404 valid loss: 1.4037, valid accuracy: 0.5544\n",
      "Iter-3930 train loss: 1.5173 valid loss: 1.4012, valid accuracy: 0.5552\n",
      "Iter-3940 train loss: 1.3248 valid loss: 1.3986, valid accuracy: 0.5566\n",
      "Iter-3950 train loss: 1.4572 valid loss: 1.3962, valid accuracy: 0.5570\n",
      "Iter-3960 train loss: 1.2843 valid loss: 1.3936, valid accuracy: 0.5580\n",
      "Iter-3970 train loss: 1.4162 valid loss: 1.3912, valid accuracy: 0.5592\n",
      "Iter-3980 train loss: 1.3564 valid loss: 1.3890, valid accuracy: 0.5602\n",
      "Iter-3990 train loss: 1.2977 valid loss: 1.3865, valid accuracy: 0.5612\n",
      "Iter-4000 train loss: 1.5402 valid loss: 1.3840, valid accuracy: 0.5622\n",
      "Iter-4010 train loss: 1.3571 valid loss: 1.3818, valid accuracy: 0.5636\n",
      "Iter-4020 train loss: 1.2408 valid loss: 1.3793, valid accuracy: 0.5642\n",
      "Iter-4030 train loss: 1.3819 valid loss: 1.3769, valid accuracy: 0.5656\n",
      "Iter-4040 train loss: 1.4156 valid loss: 1.3746, valid accuracy: 0.5672\n",
      "Iter-4050 train loss: 1.3244 valid loss: 1.3724, valid accuracy: 0.5682\n",
      "Iter-4060 train loss: 1.3603 valid loss: 1.3699, valid accuracy: 0.5688\n",
      "Iter-4070 train loss: 1.3211 valid loss: 1.3676, valid accuracy: 0.5696\n",
      "Iter-4080 train loss: 1.3605 valid loss: 1.3653, valid accuracy: 0.5716\n",
      "Iter-4090 train loss: 1.5742 valid loss: 1.3630, valid accuracy: 0.5730\n",
      "Iter-4100 train loss: 1.3039 valid loss: 1.3606, valid accuracy: 0.5742\n",
      "Iter-4110 train loss: 1.3734 valid loss: 1.3585, valid accuracy: 0.5754\n",
      "Iter-4120 train loss: 1.2696 valid loss: 1.3563, valid accuracy: 0.5760\n",
      "Iter-4130 train loss: 1.3764 valid loss: 1.3540, valid accuracy: 0.5768\n",
      "Iter-4140 train loss: 1.4621 valid loss: 1.3516, valid accuracy: 0.5772\n",
      "Iter-4150 train loss: 1.4551 valid loss: 1.3491, valid accuracy: 0.5786\n",
      "Iter-4160 train loss: 1.3085 valid loss: 1.3467, valid accuracy: 0.5786\n",
      "Iter-4170 train loss: 1.3350 valid loss: 1.3445, valid accuracy: 0.5802\n",
      "Iter-4180 train loss: 1.4030 valid loss: 1.3420, valid accuracy: 0.5812\n",
      "Iter-4190 train loss: 1.2802 valid loss: 1.3396, valid accuracy: 0.5828\n",
      "Iter-4200 train loss: 1.2098 valid loss: 1.3373, valid accuracy: 0.5832\n",
      "Iter-4210 train loss: 1.2947 valid loss: 1.3349, valid accuracy: 0.5850\n",
      "Iter-4220 train loss: 1.2574 valid loss: 1.3325, valid accuracy: 0.5856\n",
      "Iter-4230 train loss: 1.4081 valid loss: 1.3302, valid accuracy: 0.5870\n",
      "Iter-4240 train loss: 1.3211 valid loss: 1.3283, valid accuracy: 0.5874\n",
      "Iter-4250 train loss: 1.3953 valid loss: 1.3260, valid accuracy: 0.5888\n",
      "Iter-4260 train loss: 1.3741 valid loss: 1.3239, valid accuracy: 0.5902\n",
      "Iter-4270 train loss: 1.2092 valid loss: 1.3216, valid accuracy: 0.5912\n",
      "Iter-4280 train loss: 1.4102 valid loss: 1.3194, valid accuracy: 0.5912\n",
      "Iter-4290 train loss: 1.1797 valid loss: 1.3172, valid accuracy: 0.5912\n",
      "Iter-4300 train loss: 1.4231 valid loss: 1.3150, valid accuracy: 0.5924\n",
      "Iter-4310 train loss: 1.3592 valid loss: 1.3130, valid accuracy: 0.5942\n",
      "Iter-4320 train loss: 1.2712 valid loss: 1.3109, valid accuracy: 0.5946\n",
      "Iter-4330 train loss: 1.2271 valid loss: 1.3087, valid accuracy: 0.5948\n",
      "Iter-4340 train loss: 1.3572 valid loss: 1.3063, valid accuracy: 0.5968\n",
      "Iter-4350 train loss: 1.5265 valid loss: 1.3041, valid accuracy: 0.5968\n",
      "Iter-4360 train loss: 1.3368 valid loss: 1.3019, valid accuracy: 0.5970\n",
      "Iter-4370 train loss: 1.2779 valid loss: 1.2997, valid accuracy: 0.5978\n",
      "Iter-4380 train loss: 1.2069 valid loss: 1.2975, valid accuracy: 0.5994\n",
      "Iter-4390 train loss: 1.3919 valid loss: 1.2954, valid accuracy: 0.5990\n",
      "Iter-4400 train loss: 1.2411 valid loss: 1.2934, valid accuracy: 0.6000\n",
      "Iter-4410 train loss: 1.3884 valid loss: 1.2913, valid accuracy: 0.6004\n",
      "Iter-4420 train loss: 1.2329 valid loss: 1.2891, valid accuracy: 0.6014\n",
      "Iter-4430 train loss: 1.2450 valid loss: 1.2870, valid accuracy: 0.6024\n",
      "Iter-4440 train loss: 1.2110 valid loss: 1.2848, valid accuracy: 0.6034\n",
      "Iter-4450 train loss: 1.2448 valid loss: 1.2825, valid accuracy: 0.6040\n",
      "Iter-4460 train loss: 1.0860 valid loss: 1.2804, valid accuracy: 0.6058\n",
      "Iter-4470 train loss: 1.4519 valid loss: 1.2780, valid accuracy: 0.6064\n",
      "Iter-4480 train loss: 1.3154 valid loss: 1.2758, valid accuracy: 0.6074\n",
      "Iter-4490 train loss: 1.6017 valid loss: 1.2736, valid accuracy: 0.6074\n",
      "Iter-4500 train loss: 1.3135 valid loss: 1.2716, valid accuracy: 0.6080\n",
      "Iter-4510 train loss: 1.4404 valid loss: 1.2695, valid accuracy: 0.6092\n",
      "Iter-4520 train loss: 1.2683 valid loss: 1.2674, valid accuracy: 0.6108\n",
      "Iter-4530 train loss: 1.3104 valid loss: 1.2652, valid accuracy: 0.6114\n",
      "Iter-4540 train loss: 1.2619 valid loss: 1.2631, valid accuracy: 0.6124\n",
      "Iter-4550 train loss: 1.5906 valid loss: 1.2610, valid accuracy: 0.6134\n",
      "Iter-4560 train loss: 1.2324 valid loss: 1.2592, valid accuracy: 0.6136\n",
      "Iter-4570 train loss: 1.2763 valid loss: 1.2570, valid accuracy: 0.6148\n",
      "Iter-4580 train loss: 1.3476 valid loss: 1.2550, valid accuracy: 0.6160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 1.2610 valid loss: 1.2530, valid accuracy: 0.6170\n",
      "Iter-4600 train loss: 1.3573 valid loss: 1.2510, valid accuracy: 0.6174\n",
      "Iter-4610 train loss: 1.4144 valid loss: 1.2489, valid accuracy: 0.6178\n",
      "Iter-4620 train loss: 1.3578 valid loss: 1.2468, valid accuracy: 0.6188\n",
      "Iter-4630 train loss: 1.1404 valid loss: 1.2449, valid accuracy: 0.6194\n",
      "Iter-4640 train loss: 1.1987 valid loss: 1.2428, valid accuracy: 0.6210\n",
      "Iter-4650 train loss: 1.3425 valid loss: 1.2408, valid accuracy: 0.6216\n",
      "Iter-4660 train loss: 1.1098 valid loss: 1.2391, valid accuracy: 0.6224\n",
      "Iter-4670 train loss: 1.2948 valid loss: 1.2371, valid accuracy: 0.6228\n",
      "Iter-4680 train loss: 1.3002 valid loss: 1.2350, valid accuracy: 0.6238\n",
      "Iter-4690 train loss: 1.3896 valid loss: 1.2329, valid accuracy: 0.6262\n",
      "Iter-4700 train loss: 1.4166 valid loss: 1.2311, valid accuracy: 0.6274\n",
      "Iter-4710 train loss: 1.1647 valid loss: 1.2290, valid accuracy: 0.6286\n",
      "Iter-4720 train loss: 1.2127 valid loss: 1.2271, valid accuracy: 0.6296\n",
      "Iter-4730 train loss: 1.2099 valid loss: 1.2252, valid accuracy: 0.6310\n",
      "Iter-4740 train loss: 1.2582 valid loss: 1.2232, valid accuracy: 0.6316\n",
      "Iter-4750 train loss: 1.2670 valid loss: 1.2213, valid accuracy: 0.6322\n",
      "Iter-4760 train loss: 1.1871 valid loss: 1.2194, valid accuracy: 0.6332\n",
      "Iter-4770 train loss: 1.1139 valid loss: 1.2177, valid accuracy: 0.6340\n",
      "Iter-4780 train loss: 1.1164 valid loss: 1.2158, valid accuracy: 0.6348\n",
      "Iter-4790 train loss: 1.1801 valid loss: 1.2139, valid accuracy: 0.6360\n",
      "Iter-4800 train loss: 1.4232 valid loss: 1.2121, valid accuracy: 0.6366\n",
      "Iter-4810 train loss: 1.2094 valid loss: 1.2103, valid accuracy: 0.6374\n",
      "Iter-4820 train loss: 1.1560 valid loss: 1.2083, valid accuracy: 0.6388\n",
      "Iter-4830 train loss: 1.2371 valid loss: 1.2062, valid accuracy: 0.6396\n",
      "Iter-4840 train loss: 1.3360 valid loss: 1.2042, valid accuracy: 0.6398\n",
      "Iter-4850 train loss: 1.3267 valid loss: 1.2024, valid accuracy: 0.6412\n",
      "Iter-4860 train loss: 1.2813 valid loss: 1.2005, valid accuracy: 0.6418\n",
      "Iter-4870 train loss: 1.1069 valid loss: 1.1989, valid accuracy: 0.6430\n",
      "Iter-4880 train loss: 1.2719 valid loss: 1.1970, valid accuracy: 0.6438\n",
      "Iter-4890 train loss: 1.2439 valid loss: 1.1954, valid accuracy: 0.6444\n",
      "Iter-4900 train loss: 1.1378 valid loss: 1.1935, valid accuracy: 0.6446\n",
      "Iter-4910 train loss: 1.2646 valid loss: 1.1917, valid accuracy: 0.6456\n",
      "Iter-4920 train loss: 1.1960 valid loss: 1.1899, valid accuracy: 0.6462\n",
      "Iter-4930 train loss: 1.1786 valid loss: 1.1879, valid accuracy: 0.6474\n",
      "Iter-4940 train loss: 1.1623 valid loss: 1.1863, valid accuracy: 0.6476\n",
      "Iter-4950 train loss: 1.2223 valid loss: 1.1843, valid accuracy: 0.6484\n",
      "Iter-4960 train loss: 1.1703 valid loss: 1.1826, valid accuracy: 0.6484\n",
      "Iter-4970 train loss: 1.1485 valid loss: 1.1807, valid accuracy: 0.6496\n",
      "Iter-4980 train loss: 1.1538 valid loss: 1.1789, valid accuracy: 0.6502\n",
      "Iter-4990 train loss: 1.2629 valid loss: 1.1771, valid accuracy: 0.6502\n",
      "Iter-5000 train loss: 1.1293 valid loss: 1.1752, valid accuracy: 0.6512\n",
      "Iter-5010 train loss: 1.1917 valid loss: 1.1735, valid accuracy: 0.6522\n",
      "Iter-5020 train loss: 1.1118 valid loss: 1.1718, valid accuracy: 0.6532\n",
      "Iter-5030 train loss: 1.3135 valid loss: 1.1699, valid accuracy: 0.6534\n",
      "Iter-5040 train loss: 1.2228 valid loss: 1.1680, valid accuracy: 0.6542\n",
      "Iter-5050 train loss: 1.2316 valid loss: 1.1662, valid accuracy: 0.6552\n",
      "Iter-5060 train loss: 1.3341 valid loss: 1.1644, valid accuracy: 0.6570\n",
      "Iter-5070 train loss: 1.1946 valid loss: 1.1626, valid accuracy: 0.6588\n",
      "Iter-5080 train loss: 1.3197 valid loss: 1.1610, valid accuracy: 0.6588\n",
      "Iter-5090 train loss: 1.2652 valid loss: 1.1593, valid accuracy: 0.6588\n",
      "Iter-5100 train loss: 1.1700 valid loss: 1.1577, valid accuracy: 0.6594\n",
      "Iter-5110 train loss: 1.2079 valid loss: 1.1560, valid accuracy: 0.6594\n",
      "Iter-5120 train loss: 1.2573 valid loss: 1.1542, valid accuracy: 0.6600\n",
      "Iter-5130 train loss: 1.2543 valid loss: 1.1524, valid accuracy: 0.6616\n",
      "Iter-5140 train loss: 1.0594 valid loss: 1.1509, valid accuracy: 0.6634\n",
      "Iter-5150 train loss: 1.0349 valid loss: 1.1490, valid accuracy: 0.6646\n",
      "Iter-5160 train loss: 1.2439 valid loss: 1.1476, valid accuracy: 0.6652\n",
      "Iter-5170 train loss: 1.3288 valid loss: 1.1458, valid accuracy: 0.6654\n",
      "Iter-5180 train loss: 1.0640 valid loss: 1.1443, valid accuracy: 0.6660\n",
      "Iter-5190 train loss: 1.2190 valid loss: 1.1426, valid accuracy: 0.6668\n",
      "Iter-5200 train loss: 1.3295 valid loss: 1.1408, valid accuracy: 0.6676\n",
      "Iter-5210 train loss: 1.0942 valid loss: 1.1390, valid accuracy: 0.6678\n",
      "Iter-5220 train loss: 1.0248 valid loss: 1.1373, valid accuracy: 0.6684\n",
      "Iter-5230 train loss: 1.1107 valid loss: 1.1358, valid accuracy: 0.6692\n",
      "Iter-5240 train loss: 1.0466 valid loss: 1.1340, valid accuracy: 0.6700\n",
      "Iter-5250 train loss: 1.0332 valid loss: 1.1326, valid accuracy: 0.6698\n",
      "Iter-5260 train loss: 1.0672 valid loss: 1.1310, valid accuracy: 0.6712\n",
      "Iter-5270 train loss: 1.1725 valid loss: 1.1294, valid accuracy: 0.6716\n",
      "Iter-5280 train loss: 1.1881 valid loss: 1.1277, valid accuracy: 0.6730\n",
      "Iter-5290 train loss: 1.0016 valid loss: 1.1259, valid accuracy: 0.6748\n",
      "Iter-5300 train loss: 1.1206 valid loss: 1.1243, valid accuracy: 0.6752\n",
      "Iter-5310 train loss: 1.1122 valid loss: 1.1228, valid accuracy: 0.6762\n",
      "Iter-5320 train loss: 1.1398 valid loss: 1.1212, valid accuracy: 0.6776\n",
      "Iter-5330 train loss: 1.3217 valid loss: 1.1196, valid accuracy: 0.6780\n",
      "Iter-5340 train loss: 1.2720 valid loss: 1.1179, valid accuracy: 0.6786\n",
      "Iter-5350 train loss: 1.2152 valid loss: 1.1164, valid accuracy: 0.6792\n",
      "Iter-5360 train loss: 1.0177 valid loss: 1.1150, valid accuracy: 0.6792\n",
      "Iter-5370 train loss: 1.0464 valid loss: 1.1134, valid accuracy: 0.6794\n",
      "Iter-5380 train loss: 1.2159 valid loss: 1.1116, valid accuracy: 0.6806\n",
      "Iter-5390 train loss: 1.2848 valid loss: 1.1100, valid accuracy: 0.6816\n",
      "Iter-5400 train loss: 1.0993 valid loss: 1.1083, valid accuracy: 0.6820\n",
      "Iter-5410 train loss: 1.2881 valid loss: 1.1067, valid accuracy: 0.6832\n",
      "Iter-5420 train loss: 1.0732 valid loss: 1.1053, valid accuracy: 0.6846\n",
      "Iter-5430 train loss: 1.3835 valid loss: 1.1037, valid accuracy: 0.6846\n",
      "Iter-5440 train loss: 1.1041 valid loss: 1.1022, valid accuracy: 0.6850\n",
      "Iter-5450 train loss: 0.8477 valid loss: 1.1008, valid accuracy: 0.6850\n",
      "Iter-5460 train loss: 1.1720 valid loss: 1.0994, valid accuracy: 0.6856\n",
      "Iter-5470 train loss: 1.1461 valid loss: 1.0979, valid accuracy: 0.6860\n",
      "Iter-5480 train loss: 1.1109 valid loss: 1.0964, valid accuracy: 0.6870\n",
      "Iter-5490 train loss: 1.1468 valid loss: 1.0948, valid accuracy: 0.6874\n",
      "Iter-5500 train loss: 1.2824 valid loss: 1.0933, valid accuracy: 0.6878\n",
      "Iter-5510 train loss: 1.0938 valid loss: 1.0918, valid accuracy: 0.6878\n",
      "Iter-5520 train loss: 1.1233 valid loss: 1.0902, valid accuracy: 0.6894\n",
      "Iter-5530 train loss: 0.9064 valid loss: 1.0888, valid accuracy: 0.6908\n",
      "Iter-5540 train loss: 1.2220 valid loss: 1.0873, valid accuracy: 0.6918\n",
      "Iter-5550 train loss: 1.1599 valid loss: 1.0857, valid accuracy: 0.6914\n",
      "Iter-5560 train loss: 1.1667 valid loss: 1.0843, valid accuracy: 0.6916\n",
      "Iter-5570 train loss: 1.1149 valid loss: 1.0828, valid accuracy: 0.6916\n",
      "Iter-5580 train loss: 1.0945 valid loss: 1.0812, valid accuracy: 0.6920\n",
      "Iter-5590 train loss: 1.1340 valid loss: 1.0797, valid accuracy: 0.6930\n",
      "Iter-5600 train loss: 0.9599 valid loss: 1.0780, valid accuracy: 0.6940\n",
      "Iter-5610 train loss: 0.9965 valid loss: 1.0766, valid accuracy: 0.6930\n",
      "Iter-5620 train loss: 1.1223 valid loss: 1.0753, valid accuracy: 0.6932\n",
      "Iter-5630 train loss: 0.9231 valid loss: 1.0739, valid accuracy: 0.6946\n",
      "Iter-5640 train loss: 0.9752 valid loss: 1.0724, valid accuracy: 0.6964\n",
      "Iter-5650 train loss: 1.0396 valid loss: 1.0709, valid accuracy: 0.6960\n",
      "Iter-5660 train loss: 1.1682 valid loss: 1.0695, valid accuracy: 0.6964\n",
      "Iter-5670 train loss: 1.0345 valid loss: 1.0681, valid accuracy: 0.6966\n",
      "Iter-5680 train loss: 1.1406 valid loss: 1.0665, valid accuracy: 0.6978\n",
      "Iter-5690 train loss: 0.9856 valid loss: 1.0651, valid accuracy: 0.6982\n",
      "Iter-5700 train loss: 1.1478 valid loss: 1.0636, valid accuracy: 0.6992\n",
      "Iter-5710 train loss: 1.0647 valid loss: 1.0623, valid accuracy: 0.6996\n",
      "Iter-5720 train loss: 1.1408 valid loss: 1.0609, valid accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 1.1047 valid loss: 1.0596, valid accuracy: 0.7002\n",
      "Iter-5740 train loss: 1.0243 valid loss: 1.0582, valid accuracy: 0.7002\n",
      "Iter-5750 train loss: 1.0203 valid loss: 1.0569, valid accuracy: 0.7002\n",
      "Iter-5760 train loss: 1.0531 valid loss: 1.0554, valid accuracy: 0.7004\n",
      "Iter-5770 train loss: 0.9926 valid loss: 1.0539, valid accuracy: 0.7004\n",
      "Iter-5780 train loss: 1.0463 valid loss: 1.0525, valid accuracy: 0.7012\n",
      "Iter-5790 train loss: 0.9435 valid loss: 1.0510, valid accuracy: 0.7018\n",
      "Iter-5800 train loss: 1.1811 valid loss: 1.0496, valid accuracy: 0.7026\n",
      "Iter-5810 train loss: 1.0511 valid loss: 1.0483, valid accuracy: 0.7032\n",
      "Iter-5820 train loss: 1.1701 valid loss: 1.0469, valid accuracy: 0.7040\n",
      "Iter-5830 train loss: 1.2344 valid loss: 1.0455, valid accuracy: 0.7044\n",
      "Iter-5840 train loss: 1.3068 valid loss: 1.0440, valid accuracy: 0.7044\n",
      "Iter-5850 train loss: 1.1176 valid loss: 1.0427, valid accuracy: 0.7048\n",
      "Iter-5860 train loss: 1.0873 valid loss: 1.0413, valid accuracy: 0.7050\n",
      "Iter-5870 train loss: 1.1773 valid loss: 1.0399, valid accuracy: 0.7056\n",
      "Iter-5880 train loss: 1.0532 valid loss: 1.0386, valid accuracy: 0.7062\n",
      "Iter-5890 train loss: 0.9221 valid loss: 1.0372, valid accuracy: 0.7066\n",
      "Iter-5900 train loss: 1.0601 valid loss: 1.0358, valid accuracy: 0.7076\n",
      "Iter-5910 train loss: 1.1292 valid loss: 1.0345, valid accuracy: 0.7074\n",
      "Iter-5920 train loss: 1.2239 valid loss: 1.0331, valid accuracy: 0.7080\n",
      "Iter-5930 train loss: 1.1623 valid loss: 1.0317, valid accuracy: 0.7086\n",
      "Iter-5940 train loss: 1.0933 valid loss: 1.0304, valid accuracy: 0.7086\n",
      "Iter-5950 train loss: 0.9791 valid loss: 1.0292, valid accuracy: 0.7092\n",
      "Iter-5960 train loss: 0.8258 valid loss: 1.0280, valid accuracy: 0.7094\n",
      "Iter-5970 train loss: 0.9520 valid loss: 1.0268, valid accuracy: 0.7100\n",
      "Iter-5980 train loss: 1.0541 valid loss: 1.0254, valid accuracy: 0.7106\n",
      "Iter-5990 train loss: 1.1044 valid loss: 1.0241, valid accuracy: 0.7112\n",
      "Iter-6000 train loss: 1.0370 valid loss: 1.0228, valid accuracy: 0.7110\n",
      "Iter-6010 train loss: 0.9216 valid loss: 1.0216, valid accuracy: 0.7112\n",
      "Iter-6020 train loss: 1.2074 valid loss: 1.0203, valid accuracy: 0.7114\n",
      "Iter-6030 train loss: 0.9880 valid loss: 1.0189, valid accuracy: 0.7116\n",
      "Iter-6040 train loss: 1.2650 valid loss: 1.0176, valid accuracy: 0.7118\n",
      "Iter-6050 train loss: 1.1763 valid loss: 1.0164, valid accuracy: 0.7124\n",
      "Iter-6060 train loss: 1.0633 valid loss: 1.0151, valid accuracy: 0.7130\n",
      "Iter-6070 train loss: 0.9748 valid loss: 1.0139, valid accuracy: 0.7130\n",
      "Iter-6080 train loss: 1.1840 valid loss: 1.0127, valid accuracy: 0.7130\n",
      "Iter-6090 train loss: 0.9998 valid loss: 1.0114, valid accuracy: 0.7144\n",
      "Iter-6100 train loss: 1.0924 valid loss: 1.0102, valid accuracy: 0.7152\n",
      "Iter-6110 train loss: 0.9279 valid loss: 1.0091, valid accuracy: 0.7152\n",
      "Iter-6120 train loss: 1.0336 valid loss: 1.0079, valid accuracy: 0.7152\n",
      "Iter-6130 train loss: 1.1432 valid loss: 1.0067, valid accuracy: 0.7156\n",
      "Iter-6140 train loss: 0.9566 valid loss: 1.0053, valid accuracy: 0.7162\n",
      "Iter-6150 train loss: 1.0166 valid loss: 1.0041, valid accuracy: 0.7164\n",
      "Iter-6160 train loss: 1.0866 valid loss: 1.0029, valid accuracy: 0.7168\n",
      "Iter-6170 train loss: 1.0779 valid loss: 1.0018, valid accuracy: 0.7172\n",
      "Iter-6180 train loss: 1.0149 valid loss: 1.0006, valid accuracy: 0.7174\n",
      "Iter-6190 train loss: 1.0177 valid loss: 0.9993, valid accuracy: 0.7188\n",
      "Iter-6200 train loss: 1.0829 valid loss: 0.9979, valid accuracy: 0.7184\n",
      "Iter-6210 train loss: 1.0193 valid loss: 0.9967, valid accuracy: 0.7190\n",
      "Iter-6220 train loss: 1.1641 valid loss: 0.9955, valid accuracy: 0.7196\n",
      "Iter-6230 train loss: 1.0480 valid loss: 0.9943, valid accuracy: 0.7200\n",
      "Iter-6240 train loss: 1.0869 valid loss: 0.9932, valid accuracy: 0.7206\n",
      "Iter-6250 train loss: 1.0238 valid loss: 0.9920, valid accuracy: 0.7208\n",
      "Iter-6260 train loss: 0.8460 valid loss: 0.9909, valid accuracy: 0.7212\n",
      "Iter-6270 train loss: 0.9161 valid loss: 0.9898, valid accuracy: 0.7220\n",
      "Iter-6280 train loss: 1.2070 valid loss: 0.9886, valid accuracy: 0.7218\n",
      "Iter-6290 train loss: 1.0197 valid loss: 0.9873, valid accuracy: 0.7216\n",
      "Iter-6300 train loss: 1.0825 valid loss: 0.9863, valid accuracy: 0.7218\n",
      "Iter-6310 train loss: 0.8612 valid loss: 0.9852, valid accuracy: 0.7216\n",
      "Iter-6320 train loss: 1.1131 valid loss: 0.9840, valid accuracy: 0.7218\n",
      "Iter-6330 train loss: 0.9468 valid loss: 0.9829, valid accuracy: 0.7228\n",
      "Iter-6340 train loss: 1.2762 valid loss: 0.9817, valid accuracy: 0.7226\n",
      "Iter-6350 train loss: 0.9881 valid loss: 0.9806, valid accuracy: 0.7232\n",
      "Iter-6360 train loss: 0.9683 valid loss: 0.9795, valid accuracy: 0.7240\n",
      "Iter-6370 train loss: 0.9868 valid loss: 0.9784, valid accuracy: 0.7246\n",
      "Iter-6380 train loss: 1.0135 valid loss: 0.9772, valid accuracy: 0.7244\n",
      "Iter-6390 train loss: 1.0200 valid loss: 0.9760, valid accuracy: 0.7250\n",
      "Iter-6400 train loss: 1.0257 valid loss: 0.9748, valid accuracy: 0.7260\n",
      "Iter-6410 train loss: 0.7873 valid loss: 0.9737, valid accuracy: 0.7266\n",
      "Iter-6420 train loss: 1.1274 valid loss: 0.9725, valid accuracy: 0.7274\n",
      "Iter-6430 train loss: 0.9780 valid loss: 0.9714, valid accuracy: 0.7272\n",
      "Iter-6440 train loss: 0.8973 valid loss: 0.9703, valid accuracy: 0.7282\n",
      "Iter-6450 train loss: 0.9875 valid loss: 0.9692, valid accuracy: 0.7288\n",
      "Iter-6460 train loss: 0.9526 valid loss: 0.9681, valid accuracy: 0.7290\n",
      "Iter-6470 train loss: 0.8458 valid loss: 0.9670, valid accuracy: 0.7294\n",
      "Iter-6480 train loss: 0.8619 valid loss: 0.9660, valid accuracy: 0.7294\n",
      "Iter-6490 train loss: 1.0783 valid loss: 0.9648, valid accuracy: 0.7300\n",
      "Iter-6500 train loss: 1.1337 valid loss: 0.9638, valid accuracy: 0.7298\n",
      "Iter-6510 train loss: 1.0424 valid loss: 0.9626, valid accuracy: 0.7304\n",
      "Iter-6520 train loss: 1.2278 valid loss: 0.9615, valid accuracy: 0.7310\n",
      "Iter-6530 train loss: 1.0206 valid loss: 0.9604, valid accuracy: 0.7314\n",
      "Iter-6540 train loss: 1.0628 valid loss: 0.9593, valid accuracy: 0.7314\n",
      "Iter-6550 train loss: 0.8413 valid loss: 0.9584, valid accuracy: 0.7316\n",
      "Iter-6560 train loss: 1.0878 valid loss: 0.9573, valid accuracy: 0.7312\n",
      "Iter-6570 train loss: 0.9974 valid loss: 0.9563, valid accuracy: 0.7320\n",
      "Iter-6580 train loss: 0.8137 valid loss: 0.9552, valid accuracy: 0.7326\n",
      "Iter-6590 train loss: 1.0190 valid loss: 0.9540, valid accuracy: 0.7328\n",
      "Iter-6600 train loss: 1.0400 valid loss: 0.9531, valid accuracy: 0.7322\n",
      "Iter-6610 train loss: 1.1360 valid loss: 0.9520, valid accuracy: 0.7326\n",
      "Iter-6620 train loss: 0.9054 valid loss: 0.9509, valid accuracy: 0.7332\n",
      "Iter-6630 train loss: 0.9135 valid loss: 0.9499, valid accuracy: 0.7328\n",
      "Iter-6640 train loss: 1.0058 valid loss: 0.9489, valid accuracy: 0.7334\n",
      "Iter-6650 train loss: 1.1148 valid loss: 0.9479, valid accuracy: 0.7338\n",
      "Iter-6660 train loss: 0.7799 valid loss: 0.9469, valid accuracy: 0.7344\n",
      "Iter-6670 train loss: 0.9632 valid loss: 0.9460, valid accuracy: 0.7340\n",
      "Iter-6680 train loss: 0.9666 valid loss: 0.9449, valid accuracy: 0.7344\n",
      "Iter-6690 train loss: 1.0628 valid loss: 0.9438, valid accuracy: 0.7356\n",
      "Iter-6700 train loss: 1.1312 valid loss: 0.9427, valid accuracy: 0.7356\n",
      "Iter-6710 train loss: 0.9600 valid loss: 0.9417, valid accuracy: 0.7364\n",
      "Iter-6720 train loss: 0.9063 valid loss: 0.9407, valid accuracy: 0.7358\n",
      "Iter-6730 train loss: 1.0255 valid loss: 0.9398, valid accuracy: 0.7358\n",
      "Iter-6740 train loss: 0.9423 valid loss: 0.9388, valid accuracy: 0.7368\n",
      "Iter-6750 train loss: 0.8914 valid loss: 0.9378, valid accuracy: 0.7368\n",
      "Iter-6760 train loss: 1.0654 valid loss: 0.9369, valid accuracy: 0.7366\n",
      "Iter-6770 train loss: 0.9664 valid loss: 0.9359, valid accuracy: 0.7366\n",
      "Iter-6780 train loss: 0.9949 valid loss: 0.9349, valid accuracy: 0.7366\n",
      "Iter-6790 train loss: 0.9653 valid loss: 0.9340, valid accuracy: 0.7364\n",
      "Iter-6800 train loss: 1.2045 valid loss: 0.9330, valid accuracy: 0.7370\n",
      "Iter-6810 train loss: 0.8744 valid loss: 0.9320, valid accuracy: 0.7370\n",
      "Iter-6820 train loss: 0.8153 valid loss: 0.9311, valid accuracy: 0.7374\n",
      "Iter-6830 train loss: 1.0323 valid loss: 0.9300, valid accuracy: 0.7378\n",
      "Iter-6840 train loss: 0.8926 valid loss: 0.9291, valid accuracy: 0.7372\n",
      "Iter-6850 train loss: 0.9763 valid loss: 0.9281, valid accuracy: 0.7382\n",
      "Iter-6860 train loss: 0.8507 valid loss: 0.9271, valid accuracy: 0.7382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 0.8231 valid loss: 0.9262, valid accuracy: 0.7382\n",
      "Iter-6880 train loss: 0.8964 valid loss: 0.9252, valid accuracy: 0.7386\n",
      "Iter-6890 train loss: 1.0318 valid loss: 0.9242, valid accuracy: 0.7388\n",
      "Iter-6900 train loss: 0.9362 valid loss: 0.9232, valid accuracy: 0.7392\n",
      "Iter-6910 train loss: 0.9424 valid loss: 0.9222, valid accuracy: 0.7388\n",
      "Iter-6920 train loss: 1.0194 valid loss: 0.9213, valid accuracy: 0.7394\n",
      "Iter-6930 train loss: 0.7969 valid loss: 0.9203, valid accuracy: 0.7392\n",
      "Iter-6940 train loss: 0.9720 valid loss: 0.9194, valid accuracy: 0.7398\n",
      "Iter-6950 train loss: 0.9532 valid loss: 0.9184, valid accuracy: 0.7404\n",
      "Iter-6960 train loss: 0.8967 valid loss: 0.9175, valid accuracy: 0.7408\n",
      "Iter-6970 train loss: 0.9439 valid loss: 0.9165, valid accuracy: 0.7402\n",
      "Iter-6980 train loss: 0.8140 valid loss: 0.9156, valid accuracy: 0.7398\n",
      "Iter-6990 train loss: 0.9211 valid loss: 0.9147, valid accuracy: 0.7404\n",
      "Iter-7000 train loss: 1.0357 valid loss: 0.9138, valid accuracy: 0.7408\n",
      "Iter-7010 train loss: 1.0303 valid loss: 0.9129, valid accuracy: 0.7414\n",
      "Iter-7020 train loss: 0.9373 valid loss: 0.9120, valid accuracy: 0.7416\n",
      "Iter-7030 train loss: 0.9429 valid loss: 0.9110, valid accuracy: 0.7424\n",
      "Iter-7040 train loss: 0.9844 valid loss: 0.9100, valid accuracy: 0.7422\n",
      "Iter-7050 train loss: 0.9587 valid loss: 0.9091, valid accuracy: 0.7428\n",
      "Iter-7060 train loss: 0.9435 valid loss: 0.9082, valid accuracy: 0.7432\n",
      "Iter-7070 train loss: 1.2158 valid loss: 0.9073, valid accuracy: 0.7438\n",
      "Iter-7080 train loss: 0.9578 valid loss: 0.9063, valid accuracy: 0.7440\n",
      "Iter-7090 train loss: 1.0442 valid loss: 0.9053, valid accuracy: 0.7444\n",
      "Iter-7100 train loss: 0.9084 valid loss: 0.9044, valid accuracy: 0.7446\n",
      "Iter-7110 train loss: 0.9256 valid loss: 0.9035, valid accuracy: 0.7452\n",
      "Iter-7120 train loss: 0.7948 valid loss: 0.9025, valid accuracy: 0.7456\n",
      "Iter-7130 train loss: 0.9148 valid loss: 0.9015, valid accuracy: 0.7464\n",
      "Iter-7140 train loss: 0.7643 valid loss: 0.9006, valid accuracy: 0.7464\n",
      "Iter-7150 train loss: 0.8555 valid loss: 0.8996, valid accuracy: 0.7470\n",
      "Iter-7160 train loss: 0.8144 valid loss: 0.8987, valid accuracy: 0.7470\n",
      "Iter-7170 train loss: 0.8383 valid loss: 0.8978, valid accuracy: 0.7474\n",
      "Iter-7180 train loss: 0.8876 valid loss: 0.8969, valid accuracy: 0.7474\n",
      "Iter-7190 train loss: 0.8202 valid loss: 0.8961, valid accuracy: 0.7480\n",
      "Iter-7200 train loss: 0.8594 valid loss: 0.8952, valid accuracy: 0.7492\n",
      "Iter-7210 train loss: 0.7823 valid loss: 0.8942, valid accuracy: 0.7488\n",
      "Iter-7220 train loss: 0.8714 valid loss: 0.8933, valid accuracy: 0.7494\n",
      "Iter-7230 train loss: 0.8858 valid loss: 0.8925, valid accuracy: 0.7498\n",
      "Iter-7240 train loss: 0.8837 valid loss: 0.8917, valid accuracy: 0.7504\n",
      "Iter-7250 train loss: 0.9861 valid loss: 0.8908, valid accuracy: 0.7506\n",
      "Iter-7260 train loss: 0.9691 valid loss: 0.8899, valid accuracy: 0.7508\n",
      "Iter-7270 train loss: 0.9095 valid loss: 0.8891, valid accuracy: 0.7504\n",
      "Iter-7280 train loss: 0.7744 valid loss: 0.8882, valid accuracy: 0.7502\n",
      "Iter-7290 train loss: 0.8216 valid loss: 0.8873, valid accuracy: 0.7498\n",
      "Iter-7300 train loss: 0.8095 valid loss: 0.8864, valid accuracy: 0.7508\n",
      "Iter-7310 train loss: 1.0446 valid loss: 0.8856, valid accuracy: 0.7512\n",
      "Iter-7320 train loss: 0.9521 valid loss: 0.8848, valid accuracy: 0.7514\n",
      "Iter-7330 train loss: 0.9409 valid loss: 0.8839, valid accuracy: 0.7522\n",
      "Iter-7340 train loss: 0.9945 valid loss: 0.8830, valid accuracy: 0.7522\n",
      "Iter-7350 train loss: 0.9431 valid loss: 0.8821, valid accuracy: 0.7512\n",
      "Iter-7360 train loss: 0.9640 valid loss: 0.8813, valid accuracy: 0.7518\n",
      "Iter-7370 train loss: 0.9393 valid loss: 0.8804, valid accuracy: 0.7526\n",
      "Iter-7380 train loss: 0.8366 valid loss: 0.8795, valid accuracy: 0.7520\n",
      "Iter-7390 train loss: 0.8571 valid loss: 0.8787, valid accuracy: 0.7530\n",
      "Iter-7400 train loss: 0.7724 valid loss: 0.8779, valid accuracy: 0.7530\n",
      "Iter-7410 train loss: 0.9677 valid loss: 0.8771, valid accuracy: 0.7534\n",
      "Iter-7420 train loss: 0.8406 valid loss: 0.8762, valid accuracy: 0.7536\n",
      "Iter-7430 train loss: 1.0727 valid loss: 0.8754, valid accuracy: 0.7542\n",
      "Iter-7440 train loss: 0.9196 valid loss: 0.8745, valid accuracy: 0.7542\n",
      "Iter-7450 train loss: 1.0831 valid loss: 0.8738, valid accuracy: 0.7542\n",
      "Iter-7460 train loss: 0.8275 valid loss: 0.8729, valid accuracy: 0.7540\n",
      "Iter-7470 train loss: 0.9327 valid loss: 0.8721, valid accuracy: 0.7540\n",
      "Iter-7480 train loss: 0.9508 valid loss: 0.8714, valid accuracy: 0.7542\n",
      "Iter-7490 train loss: 0.7731 valid loss: 0.8706, valid accuracy: 0.7542\n",
      "Iter-7500 train loss: 0.9685 valid loss: 0.8698, valid accuracy: 0.7542\n",
      "Iter-7510 train loss: 0.9206 valid loss: 0.8691, valid accuracy: 0.7544\n",
      "Iter-7520 train loss: 1.0629 valid loss: 0.8683, valid accuracy: 0.7552\n",
      "Iter-7530 train loss: 0.7954 valid loss: 0.8675, valid accuracy: 0.7558\n",
      "Iter-7540 train loss: 0.7611 valid loss: 0.8667, valid accuracy: 0.7558\n",
      "Iter-7550 train loss: 0.8724 valid loss: 0.8659, valid accuracy: 0.7560\n",
      "Iter-7560 train loss: 0.9121 valid loss: 0.8652, valid accuracy: 0.7560\n",
      "Iter-7570 train loss: 1.0369 valid loss: 0.8644, valid accuracy: 0.7558\n",
      "Iter-7580 train loss: 0.9486 valid loss: 0.8636, valid accuracy: 0.7560\n",
      "Iter-7590 train loss: 0.9057 valid loss: 0.8628, valid accuracy: 0.7564\n",
      "Iter-7600 train loss: 0.9687 valid loss: 0.8620, valid accuracy: 0.7568\n",
      "Iter-7610 train loss: 0.8571 valid loss: 0.8611, valid accuracy: 0.7572\n",
      "Iter-7620 train loss: 0.9487 valid loss: 0.8604, valid accuracy: 0.7572\n",
      "Iter-7630 train loss: 0.8783 valid loss: 0.8596, valid accuracy: 0.7578\n",
      "Iter-7640 train loss: 0.9297 valid loss: 0.8589, valid accuracy: 0.7588\n",
      "Iter-7650 train loss: 0.8540 valid loss: 0.8581, valid accuracy: 0.7590\n",
      "Iter-7660 train loss: 1.0215 valid loss: 0.8573, valid accuracy: 0.7594\n",
      "Iter-7670 train loss: 0.9430 valid loss: 0.8566, valid accuracy: 0.7596\n",
      "Iter-7680 train loss: 1.0272 valid loss: 0.8558, valid accuracy: 0.7598\n",
      "Iter-7690 train loss: 0.7190 valid loss: 0.8551, valid accuracy: 0.7596\n",
      "Iter-7700 train loss: 0.9394 valid loss: 0.8543, valid accuracy: 0.7596\n",
      "Iter-7710 train loss: 0.7920 valid loss: 0.8536, valid accuracy: 0.7598\n",
      "Iter-7720 train loss: 0.7706 valid loss: 0.8528, valid accuracy: 0.7598\n",
      "Iter-7730 train loss: 0.9043 valid loss: 0.8521, valid accuracy: 0.7606\n",
      "Iter-7740 train loss: 0.7341 valid loss: 0.8514, valid accuracy: 0.7606\n",
      "Iter-7750 train loss: 0.7926 valid loss: 0.8507, valid accuracy: 0.7614\n",
      "Iter-7760 train loss: 0.7901 valid loss: 0.8500, valid accuracy: 0.7616\n",
      "Iter-7770 train loss: 0.8606 valid loss: 0.8493, valid accuracy: 0.7616\n",
      "Iter-7780 train loss: 0.9824 valid loss: 0.8486, valid accuracy: 0.7624\n",
      "Iter-7790 train loss: 0.8339 valid loss: 0.8479, valid accuracy: 0.7628\n",
      "Iter-7800 train loss: 0.8599 valid loss: 0.8471, valid accuracy: 0.7630\n",
      "Iter-7810 train loss: 0.9584 valid loss: 0.8464, valid accuracy: 0.7624\n",
      "Iter-7820 train loss: 0.6422 valid loss: 0.8459, valid accuracy: 0.7628\n",
      "Iter-7830 train loss: 0.7777 valid loss: 0.8451, valid accuracy: 0.7628\n",
      "Iter-7840 train loss: 0.8304 valid loss: 0.8444, valid accuracy: 0.7630\n",
      "Iter-7850 train loss: 0.8620 valid loss: 0.8436, valid accuracy: 0.7632\n",
      "Iter-7860 train loss: 0.8969 valid loss: 0.8429, valid accuracy: 0.7638\n",
      "Iter-7870 train loss: 0.8280 valid loss: 0.8421, valid accuracy: 0.7644\n",
      "Iter-7880 train loss: 0.8791 valid loss: 0.8413, valid accuracy: 0.7644\n",
      "Iter-7890 train loss: 1.0009 valid loss: 0.8406, valid accuracy: 0.7642\n",
      "Iter-7900 train loss: 0.7505 valid loss: 0.8399, valid accuracy: 0.7646\n",
      "Iter-7910 train loss: 0.9659 valid loss: 0.8392, valid accuracy: 0.7640\n",
      "Iter-7920 train loss: 0.7184 valid loss: 0.8385, valid accuracy: 0.7644\n",
      "Iter-7930 train loss: 0.9156 valid loss: 0.8379, valid accuracy: 0.7646\n",
      "Iter-7940 train loss: 0.8147 valid loss: 0.8371, valid accuracy: 0.7656\n",
      "Iter-7950 train loss: 0.8690 valid loss: 0.8365, valid accuracy: 0.7656\n",
      "Iter-7960 train loss: 0.8557 valid loss: 0.8357, valid accuracy: 0.7656\n",
      "Iter-7970 train loss: 0.7917 valid loss: 0.8350, valid accuracy: 0.7660\n",
      "Iter-7980 train loss: 0.9238 valid loss: 0.8344, valid accuracy: 0.7660\n",
      "Iter-7990 train loss: 0.8675 valid loss: 0.8337, valid accuracy: 0.7664\n",
      "Iter-8000 train loss: 0.8375 valid loss: 0.8329, valid accuracy: 0.7666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 0.7803 valid loss: 0.8323, valid accuracy: 0.7670\n",
      "Iter-8020 train loss: 0.7916 valid loss: 0.8317, valid accuracy: 0.7670\n",
      "Iter-8030 train loss: 0.8607 valid loss: 0.8310, valid accuracy: 0.7672\n",
      "Iter-8040 train loss: 1.0156 valid loss: 0.8302, valid accuracy: 0.7672\n",
      "Iter-8050 train loss: 0.7459 valid loss: 0.8295, valid accuracy: 0.7668\n",
      "Iter-8060 train loss: 0.7186 valid loss: 0.8288, valid accuracy: 0.7678\n",
      "Iter-8070 train loss: 0.7762 valid loss: 0.8281, valid accuracy: 0.7672\n",
      "Iter-8080 train loss: 0.9164 valid loss: 0.8274, valid accuracy: 0.7670\n",
      "Iter-8090 train loss: 0.9397 valid loss: 0.8267, valid accuracy: 0.7676\n",
      "Iter-8100 train loss: 0.8950 valid loss: 0.8260, valid accuracy: 0.7682\n",
      "Iter-8110 train loss: 0.7203 valid loss: 0.8254, valid accuracy: 0.7686\n",
      "Iter-8120 train loss: 1.0743 valid loss: 0.8247, valid accuracy: 0.7690\n",
      "Iter-8130 train loss: 0.9433 valid loss: 0.8241, valid accuracy: 0.7698\n",
      "Iter-8140 train loss: 0.8493 valid loss: 0.8234, valid accuracy: 0.7696\n",
      "Iter-8150 train loss: 0.7536 valid loss: 0.8227, valid accuracy: 0.7698\n",
      "Iter-8160 train loss: 0.9177 valid loss: 0.8221, valid accuracy: 0.7708\n",
      "Iter-8170 train loss: 0.7424 valid loss: 0.8214, valid accuracy: 0.7706\n",
      "Iter-8180 train loss: 0.7207 valid loss: 0.8208, valid accuracy: 0.7708\n",
      "Iter-8190 train loss: 1.0200 valid loss: 0.8201, valid accuracy: 0.7710\n",
      "Iter-8200 train loss: 0.8261 valid loss: 0.8195, valid accuracy: 0.7710\n",
      "Iter-8210 train loss: 0.8183 valid loss: 0.8189, valid accuracy: 0.7710\n",
      "Iter-8220 train loss: 0.8863 valid loss: 0.8182, valid accuracy: 0.7714\n",
      "Iter-8230 train loss: 0.9219 valid loss: 0.8176, valid accuracy: 0.7714\n",
      "Iter-8240 train loss: 0.9026 valid loss: 0.8170, valid accuracy: 0.7714\n",
      "Iter-8250 train loss: 0.9194 valid loss: 0.8164, valid accuracy: 0.7712\n",
      "Iter-8260 train loss: 0.8003 valid loss: 0.8157, valid accuracy: 0.7720\n",
      "Iter-8270 train loss: 0.9541 valid loss: 0.8151, valid accuracy: 0.7722\n",
      "Iter-8280 train loss: 0.9363 valid loss: 0.8144, valid accuracy: 0.7732\n",
      "Iter-8290 train loss: 0.7941 valid loss: 0.8137, valid accuracy: 0.7732\n",
      "Iter-8300 train loss: 0.7494 valid loss: 0.8131, valid accuracy: 0.7738\n",
      "Iter-8310 train loss: 0.9162 valid loss: 0.8124, valid accuracy: 0.7742\n",
      "Iter-8320 train loss: 0.8537 valid loss: 0.8117, valid accuracy: 0.7744\n",
      "Iter-8330 train loss: 0.7973 valid loss: 0.8110, valid accuracy: 0.7746\n",
      "Iter-8340 train loss: 0.7904 valid loss: 0.8104, valid accuracy: 0.7754\n",
      "Iter-8350 train loss: 0.7405 valid loss: 0.8098, valid accuracy: 0.7752\n",
      "Iter-8360 train loss: 0.8103 valid loss: 0.8092, valid accuracy: 0.7750\n",
      "Iter-8370 train loss: 0.7273 valid loss: 0.8086, valid accuracy: 0.7754\n",
      "Iter-8380 train loss: 0.7468 valid loss: 0.8080, valid accuracy: 0.7752\n",
      "Iter-8390 train loss: 0.6856 valid loss: 0.8074, valid accuracy: 0.7750\n",
      "Iter-8400 train loss: 0.8561 valid loss: 0.8067, valid accuracy: 0.7752\n",
      "Iter-8410 train loss: 0.5710 valid loss: 0.8061, valid accuracy: 0.7754\n",
      "Iter-8420 train loss: 0.7858 valid loss: 0.8055, valid accuracy: 0.7752\n",
      "Iter-8430 train loss: 1.0394 valid loss: 0.8049, valid accuracy: 0.7754\n",
      "Iter-8440 train loss: 0.7637 valid loss: 0.8043, valid accuracy: 0.7760\n",
      "Iter-8450 train loss: 0.7928 valid loss: 0.8038, valid accuracy: 0.7762\n",
      "Iter-8460 train loss: 0.7562 valid loss: 0.8033, valid accuracy: 0.7762\n",
      "Iter-8470 train loss: 0.5903 valid loss: 0.8027, valid accuracy: 0.7768\n",
      "Iter-8480 train loss: 0.7913 valid loss: 0.8021, valid accuracy: 0.7770\n",
      "Iter-8490 train loss: 0.8503 valid loss: 0.8015, valid accuracy: 0.7772\n",
      "Iter-8500 train loss: 0.9626 valid loss: 0.8008, valid accuracy: 0.7778\n",
      "Iter-8510 train loss: 0.8639 valid loss: 0.8002, valid accuracy: 0.7782\n",
      "Iter-8520 train loss: 0.9531 valid loss: 0.7997, valid accuracy: 0.7786\n",
      "Iter-8530 train loss: 0.7828 valid loss: 0.7990, valid accuracy: 0.7786\n",
      "Iter-8540 train loss: 0.8386 valid loss: 0.7985, valid accuracy: 0.7788\n",
      "Iter-8550 train loss: 0.9095 valid loss: 0.7978, valid accuracy: 0.7786\n",
      "Iter-8560 train loss: 0.6855 valid loss: 0.7973, valid accuracy: 0.7788\n",
      "Iter-8570 train loss: 0.7967 valid loss: 0.7967, valid accuracy: 0.7788\n",
      "Iter-8580 train loss: 0.7437 valid loss: 0.7962, valid accuracy: 0.7790\n",
      "Iter-8590 train loss: 0.8478 valid loss: 0.7956, valid accuracy: 0.7788\n",
      "Iter-8600 train loss: 0.9821 valid loss: 0.7950, valid accuracy: 0.7786\n",
      "Iter-8610 train loss: 0.7084 valid loss: 0.7945, valid accuracy: 0.7794\n",
      "Iter-8620 train loss: 0.7974 valid loss: 0.7939, valid accuracy: 0.7794\n",
      "Iter-8630 train loss: 0.8735 valid loss: 0.7934, valid accuracy: 0.7794\n",
      "Iter-8640 train loss: 0.8297 valid loss: 0.7929, valid accuracy: 0.7798\n",
      "Iter-8650 train loss: 0.7392 valid loss: 0.7923, valid accuracy: 0.7802\n",
      "Iter-8660 train loss: 0.9122 valid loss: 0.7917, valid accuracy: 0.7804\n",
      "Iter-8670 train loss: 0.8268 valid loss: 0.7911, valid accuracy: 0.7808\n",
      "Iter-8680 train loss: 0.8633 valid loss: 0.7906, valid accuracy: 0.7810\n",
      "Iter-8690 train loss: 0.8839 valid loss: 0.7900, valid accuracy: 0.7806\n",
      "Iter-8700 train loss: 0.7299 valid loss: 0.7895, valid accuracy: 0.7808\n",
      "Iter-8710 train loss: 0.9234 valid loss: 0.7889, valid accuracy: 0.7814\n",
      "Iter-8720 train loss: 0.6717 valid loss: 0.7883, valid accuracy: 0.7818\n",
      "Iter-8730 train loss: 0.6921 valid loss: 0.7878, valid accuracy: 0.7822\n",
      "Iter-8740 train loss: 0.8318 valid loss: 0.7873, valid accuracy: 0.7818\n",
      "Iter-8750 train loss: 0.8687 valid loss: 0.7867, valid accuracy: 0.7820\n",
      "Iter-8760 train loss: 0.6366 valid loss: 0.7863, valid accuracy: 0.7822\n",
      "Iter-8770 train loss: 0.7805 valid loss: 0.7857, valid accuracy: 0.7832\n",
      "Iter-8780 train loss: 0.6751 valid loss: 0.7852, valid accuracy: 0.7832\n",
      "Iter-8790 train loss: 0.6956 valid loss: 0.7847, valid accuracy: 0.7834\n",
      "Iter-8800 train loss: 0.8972 valid loss: 0.7842, valid accuracy: 0.7834\n",
      "Iter-8810 train loss: 0.9329 valid loss: 0.7837, valid accuracy: 0.7836\n",
      "Iter-8820 train loss: 0.8459 valid loss: 0.7831, valid accuracy: 0.7836\n",
      "Iter-8830 train loss: 0.8543 valid loss: 0.7826, valid accuracy: 0.7838\n",
      "Iter-8840 train loss: 0.7839 valid loss: 0.7821, valid accuracy: 0.7842\n",
      "Iter-8850 train loss: 0.5892 valid loss: 0.7815, valid accuracy: 0.7848\n",
      "Iter-8860 train loss: 0.6298 valid loss: 0.7811, valid accuracy: 0.7848\n",
      "Iter-8870 train loss: 1.0654 valid loss: 0.7806, valid accuracy: 0.7842\n",
      "Iter-8880 train loss: 0.6168 valid loss: 0.7800, valid accuracy: 0.7846\n",
      "Iter-8890 train loss: 0.8848 valid loss: 0.7795, valid accuracy: 0.7846\n",
      "Iter-8900 train loss: 0.8870 valid loss: 0.7790, valid accuracy: 0.7848\n",
      "Iter-8910 train loss: 0.7021 valid loss: 0.7783, valid accuracy: 0.7844\n",
      "Iter-8920 train loss: 0.7932 valid loss: 0.7778, valid accuracy: 0.7854\n",
      "Iter-8930 train loss: 0.8315 valid loss: 0.7772, valid accuracy: 0.7862\n",
      "Iter-8940 train loss: 0.6389 valid loss: 0.7767, valid accuracy: 0.7864\n",
      "Iter-8950 train loss: 0.7001 valid loss: 0.7762, valid accuracy: 0.7862\n",
      "Iter-8960 train loss: 0.8508 valid loss: 0.7757, valid accuracy: 0.7862\n",
      "Iter-8970 train loss: 0.8529 valid loss: 0.7752, valid accuracy: 0.7860\n",
      "Iter-8980 train loss: 0.8253 valid loss: 0.7747, valid accuracy: 0.7856\n",
      "Iter-8990 train loss: 0.6486 valid loss: 0.7741, valid accuracy: 0.7858\n",
      "Iter-9000 train loss: 0.6302 valid loss: 0.7736, valid accuracy: 0.7860\n",
      "Iter-9010 train loss: 0.6922 valid loss: 0.7729, valid accuracy: 0.7864\n",
      "Iter-9020 train loss: 0.7965 valid loss: 0.7724, valid accuracy: 0.7866\n",
      "Iter-9030 train loss: 0.6543 valid loss: 0.7719, valid accuracy: 0.7866\n",
      "Iter-9040 train loss: 0.8455 valid loss: 0.7714, valid accuracy: 0.7872\n",
      "Iter-9050 train loss: 0.8740 valid loss: 0.7710, valid accuracy: 0.7880\n",
      "Iter-9060 train loss: 0.8600 valid loss: 0.7705, valid accuracy: 0.7878\n",
      "Iter-9070 train loss: 0.8730 valid loss: 0.7699, valid accuracy: 0.7884\n",
      "Iter-9080 train loss: 0.8791 valid loss: 0.7694, valid accuracy: 0.7886\n",
      "Iter-9090 train loss: 0.7329 valid loss: 0.7690, valid accuracy: 0.7886\n",
      "Iter-9100 train loss: 0.8204 valid loss: 0.7685, valid accuracy: 0.7888\n",
      "Iter-9110 train loss: 0.6715 valid loss: 0.7681, valid accuracy: 0.7892\n",
      "Iter-9120 train loss: 0.8389 valid loss: 0.7675, valid accuracy: 0.7898\n",
      "Iter-9130 train loss: 0.7042 valid loss: 0.7670, valid accuracy: 0.7900\n",
      "Iter-9140 train loss: 0.8213 valid loss: 0.7665, valid accuracy: 0.7898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 0.8428 valid loss: 0.7661, valid accuracy: 0.7904\n",
      "Iter-9160 train loss: 0.7144 valid loss: 0.7657, valid accuracy: 0.7902\n",
      "Iter-9170 train loss: 0.7169 valid loss: 0.7652, valid accuracy: 0.7902\n",
      "Iter-9180 train loss: 0.7732 valid loss: 0.7647, valid accuracy: 0.7904\n",
      "Iter-9190 train loss: 0.7828 valid loss: 0.7643, valid accuracy: 0.7906\n",
      "Iter-9200 train loss: 0.9551 valid loss: 0.7639, valid accuracy: 0.7912\n",
      "Iter-9210 train loss: 0.6783 valid loss: 0.7633, valid accuracy: 0.7914\n",
      "Iter-9220 train loss: 0.7757 valid loss: 0.7628, valid accuracy: 0.7916\n",
      "Iter-9230 train loss: 0.7171 valid loss: 0.7624, valid accuracy: 0.7918\n",
      "Iter-9240 train loss: 0.6265 valid loss: 0.7619, valid accuracy: 0.7922\n",
      "Iter-9250 train loss: 0.8507 valid loss: 0.7614, valid accuracy: 0.7926\n",
      "Iter-9260 train loss: 0.6184 valid loss: 0.7609, valid accuracy: 0.7924\n",
      "Iter-9270 train loss: 0.7270 valid loss: 0.7604, valid accuracy: 0.7924\n",
      "Iter-9280 train loss: 0.9912 valid loss: 0.7599, valid accuracy: 0.7930\n",
      "Iter-9290 train loss: 0.7068 valid loss: 0.7594, valid accuracy: 0.7928\n",
      "Iter-9300 train loss: 0.8086 valid loss: 0.7589, valid accuracy: 0.7934\n",
      "Iter-9310 train loss: 0.7422 valid loss: 0.7584, valid accuracy: 0.7932\n",
      "Iter-9320 train loss: 0.8463 valid loss: 0.7579, valid accuracy: 0.7934\n",
      "Iter-9330 train loss: 0.8381 valid loss: 0.7574, valid accuracy: 0.7942\n",
      "Iter-9340 train loss: 0.7139 valid loss: 0.7569, valid accuracy: 0.7942\n",
      "Iter-9350 train loss: 0.7443 valid loss: 0.7565, valid accuracy: 0.7942\n",
      "Iter-9360 train loss: 0.6495 valid loss: 0.7560, valid accuracy: 0.7946\n",
      "Iter-9370 train loss: 0.6995 valid loss: 0.7555, valid accuracy: 0.7944\n",
      "Iter-9380 train loss: 0.8169 valid loss: 0.7550, valid accuracy: 0.7944\n",
      "Iter-9390 train loss: 0.6509 valid loss: 0.7546, valid accuracy: 0.7948\n",
      "Iter-9400 train loss: 0.6450 valid loss: 0.7541, valid accuracy: 0.7948\n",
      "Iter-9410 train loss: 0.8495 valid loss: 0.7537, valid accuracy: 0.7948\n",
      "Iter-9420 train loss: 0.7784 valid loss: 0.7533, valid accuracy: 0.7950\n",
      "Iter-9430 train loss: 0.6971 valid loss: 0.7528, valid accuracy: 0.7946\n",
      "Iter-9440 train loss: 0.8017 valid loss: 0.7523, valid accuracy: 0.7950\n",
      "Iter-9450 train loss: 0.7124 valid loss: 0.7518, valid accuracy: 0.7946\n",
      "Iter-9460 train loss: 0.7720 valid loss: 0.7513, valid accuracy: 0.7950\n",
      "Iter-9470 train loss: 0.7454 valid loss: 0.7508, valid accuracy: 0.7950\n",
      "Iter-9480 train loss: 0.8988 valid loss: 0.7504, valid accuracy: 0.7952\n",
      "Iter-9490 train loss: 0.7947 valid loss: 0.7500, valid accuracy: 0.7954\n",
      "Iter-9500 train loss: 0.6918 valid loss: 0.7495, valid accuracy: 0.7948\n",
      "Iter-9510 train loss: 0.6754 valid loss: 0.7491, valid accuracy: 0.7950\n",
      "Iter-9520 train loss: 0.8608 valid loss: 0.7486, valid accuracy: 0.7950\n",
      "Iter-9530 train loss: 0.8373 valid loss: 0.7481, valid accuracy: 0.7950\n",
      "Iter-9540 train loss: 0.7125 valid loss: 0.7476, valid accuracy: 0.7950\n",
      "Iter-9550 train loss: 0.6324 valid loss: 0.7471, valid accuracy: 0.7950\n",
      "Iter-9560 train loss: 0.6626 valid loss: 0.7466, valid accuracy: 0.7954\n",
      "Iter-9570 train loss: 0.9706 valid loss: 0.7462, valid accuracy: 0.7952\n",
      "Iter-9580 train loss: 0.8159 valid loss: 0.7457, valid accuracy: 0.7954\n",
      "Iter-9590 train loss: 0.7226 valid loss: 0.7453, valid accuracy: 0.7952\n",
      "Iter-9600 train loss: 0.8464 valid loss: 0.7449, valid accuracy: 0.7954\n",
      "Iter-9610 train loss: 0.8864 valid loss: 0.7444, valid accuracy: 0.7956\n",
      "Iter-9620 train loss: 0.7472 valid loss: 0.7440, valid accuracy: 0.7960\n",
      "Iter-9630 train loss: 0.7888 valid loss: 0.7435, valid accuracy: 0.7956\n",
      "Iter-9640 train loss: 0.7992 valid loss: 0.7431, valid accuracy: 0.7962\n",
      "Iter-9650 train loss: 0.7824 valid loss: 0.7427, valid accuracy: 0.7960\n",
      "Iter-9660 train loss: 0.7186 valid loss: 0.7423, valid accuracy: 0.7960\n",
      "Iter-9670 train loss: 0.7433 valid loss: 0.7419, valid accuracy: 0.7962\n",
      "Iter-9680 train loss: 0.7649 valid loss: 0.7415, valid accuracy: 0.7966\n",
      "Iter-9690 train loss: 0.6183 valid loss: 0.7411, valid accuracy: 0.7968\n",
      "Iter-9700 train loss: 0.7791 valid loss: 0.7407, valid accuracy: 0.7968\n",
      "Iter-9710 train loss: 0.5909 valid loss: 0.7403, valid accuracy: 0.7966\n",
      "Iter-9720 train loss: 0.8974 valid loss: 0.7399, valid accuracy: 0.7966\n",
      "Iter-9730 train loss: 0.7989 valid loss: 0.7395, valid accuracy: 0.7968\n",
      "Iter-9740 train loss: 0.7903 valid loss: 0.7391, valid accuracy: 0.7970\n",
      "Iter-9750 train loss: 0.6065 valid loss: 0.7387, valid accuracy: 0.7968\n",
      "Iter-9760 train loss: 0.7351 valid loss: 0.7383, valid accuracy: 0.7974\n",
      "Iter-9770 train loss: 0.6621 valid loss: 0.7380, valid accuracy: 0.7968\n",
      "Iter-9780 train loss: 0.9814 valid loss: 0.7375, valid accuracy: 0.7970\n",
      "Iter-9790 train loss: 0.7373 valid loss: 0.7371, valid accuracy: 0.7976\n",
      "Iter-9800 train loss: 0.8602 valid loss: 0.7368, valid accuracy: 0.7972\n",
      "Iter-9810 train loss: 0.7876 valid loss: 0.7363, valid accuracy: 0.7972\n",
      "Iter-9820 train loss: 0.7981 valid loss: 0.7360, valid accuracy: 0.7968\n",
      "Iter-9830 train loss: 0.7444 valid loss: 0.7356, valid accuracy: 0.7974\n",
      "Iter-9840 train loss: 0.5726 valid loss: 0.7352, valid accuracy: 0.7972\n",
      "Iter-9850 train loss: 0.7806 valid loss: 0.7348, valid accuracy: 0.7974\n",
      "Iter-9860 train loss: 0.9892 valid loss: 0.7344, valid accuracy: 0.7978\n",
      "Iter-9870 train loss: 0.6347 valid loss: 0.7339, valid accuracy: 0.7980\n",
      "Iter-9880 train loss: 0.6701 valid loss: 0.7336, valid accuracy: 0.7984\n",
      "Iter-9890 train loss: 0.8072 valid loss: 0.7332, valid accuracy: 0.7982\n",
      "Iter-9900 train loss: 0.7033 valid loss: 0.7328, valid accuracy: 0.7986\n",
      "Iter-9910 train loss: 0.9532 valid loss: 0.7324, valid accuracy: 0.7990\n",
      "Iter-9920 train loss: 0.7988 valid loss: 0.7320, valid accuracy: 0.7986\n",
      "Iter-9930 train loss: 0.9100 valid loss: 0.7316, valid accuracy: 0.7986\n",
      "Iter-9940 train loss: 0.6429 valid loss: 0.7312, valid accuracy: 0.7986\n",
      "Iter-9950 train loss: 0.5485 valid loss: 0.7308, valid accuracy: 0.7984\n",
      "Iter-9960 train loss: 0.7035 valid loss: 0.7304, valid accuracy: 0.7988\n",
      "Iter-9970 train loss: 0.7602 valid loss: 0.7299, valid accuracy: 0.7986\n",
      "Iter-9980 train loss: 0.6527 valid loss: 0.7295, valid accuracy: 0.7988\n",
      "Iter-9990 train loss: 0.7688 valid loss: 0.7290, valid accuracy: 0.7988\n",
      "Iter-10000 train loss: 0.8728 valid loss: 0.7286, valid accuracy: 0.7990\n",
      "Last iteration - Test accuracy mean: 0.7929, std: 0.0000, loss: 0.7216\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# NOT used now\n",
    "p_dropout = 0.95 #  layer & unit noise: keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119590080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmUVPWZ//H30whGQBqQRcPSOCFG0KAQQSLOWC4J6ETA\niIrkgHFcmBgm0ZgRTMyxNU4ic8aMwYhAXJFB4g9GFiMTgtrjOoKAKMgmKDSLRGRRQKDtfn5/fIum\naHspoapuLZ/XOX36Lt+69dxLU099l/u95u6IiEhhKoo6ABERiY6SgIhIAVMSEBEpYEoCIiIFTElA\nRKSAKQmIiBSwpJKAmQ0ws5VmttrMRteyv4WZzTazt8zsHTP7YcojFRGRlLOG7hMwsyJgNXAhsBlY\nCAx195UJZW4HWrj77WbWBlgFtHf3z9MWuYiIHLVkagJ9gDXuvt7dK4BpwKAaZRw4Pr58PPCxEoCI\nSPZLJgl0AMoT1jfGtyX6A9DdzDYDS4GfpiY8ERFJp1R1DPcHlrj7V4GewINm1jxFxxYRkTQ5Joky\nm4DOCesd49sSXQv8FsDd15rZ+8CpwJuJhcxMExWJiBwBd7d0HDeZJLAQ6GpmJcAWYChwdY0y64GL\ngFfNrD1wCrCutoNpwrqgtLSU0tLSqMPICroWh+haHJKN1+LDD+G992DLFpg/H554AvbvD/vat4c+\nfaBLF9i2DbZuhdat4YQT4NNPw0/HjnDeebB9OwwcCMcdF8o0xCwtn/9AEknA3SvNbBQwj9B89Ii7\nrzCzkWG3TwLuAR43s7fjL7vN3benLWoRkTRxDx/gTZtCo0bw61/DlClQVATl8d7RVq3gO9+BG2+E\na6+Frl3h+OPrP262SqYmgLv/D/CNGtsmJixvIfQLiIjkjHXrYO5cmDgxfCPv1i0s12yw6NwZfvc7\nOPfc8I0/nySVBCT1YrFY1CFkDV2LQ3QtDknFtaiqCj87d4YmmPnz4amnYPFi2Lv38LK33QbNm8Po\n0eFn9274ylegTZujDiOrNXizWErfzMzVJyAiqXbgADRuDJ9/Dhs2wIQJsGQJPP/84eW+/vXwjf/Y\nY+H22+H000M7fbYzs7R1DCsJiESsS5curF+/PuowJAuUlJTwwQcffGG7koBIHov/B486DMkCdf0t\npDMJaBZREZECpiQgIlLAlARERAqYkoCIpMX69espKiqiqqoKgEsuuYQnn3wyqbKSOUoCIlKriy++\nuNZpG2bNmsVJJ52U1Ad24nQHzz33HMOHD0+qrGSOkoCI1Oqaa65hypQpX9g+ZcoUhg8fTlFR4Xx8\n5PPorcL5VxSRL2Xw4MF8/PHHvPLKK9Xbdu7cybPPPsuIESOA8O2+V69eFBcXU1JSwl133VXn8c4/\n/3weffRRAKqqqvj5z39O27Zt6dq1K3/+85/rjWXs2LF07dqVFi1acPrppzNz5szD9v/xj3+ke/fu\n1fvfeustADZu3Mjll19Ou3btaNu2LT/5yU8AuOuuuw6rldRsjjr//PO54447OPfcc2nWrBnvv/8+\njz/+ePV7dO3alUmTJh0Ww6xZs+jZsyfFxcV8/etfZ968eUyfPp2zzjrrsHK/+93vuOyyy+o934xy\n94z9hLcTkUTZ/P/ihhtu8BtuuKF6fcKECd6zZ8/q9f/93//1ZcuWubv7O++84yeeeKLPmjXL3d0/\n+OADLyoq8srKSnd3j8Vi/sgjj7i7+0MPPeTdunXzTZs2+Y4dO/z8888/rGxN06dP9w8//NDd3Z9+\n+mlv1qzZYesdO3b0RYsWubv72rVrfcOGDV5ZWelnnHGG33rrrf7ZZ5/5/v37/dVXX3V399LSUh8+\nfHj18WuLtaSkxFesWOGVlZVeUVHhzz33nL///vvu7v7SSy9506ZNfcmSJe7u/sYbb3hxcbE///zz\n7u6+efNmX7Vqle/fv99POOEEX7lyZfV79ezZ05955plaz7Ouv4X49vR8LqfrwLW+WRb/sYtEpaH/\nF2E6s6P/ORKvvPKKt2zZ0vfv3+/u7v369fP777+/zvI333yz/+xnP3P3+pPABRdc4BMnTqx+3bx5\n8+pNAjWdeeaZPnv2bHd379+/v48bN+4LZV5//XVv165drcdMJgnceeed9cYwePDg6vcdOXJk9XnX\ndNNNN/kdd9zh7u7Lli3z1q1b+4EDB2otG0USUHOQSJZLVRo4Ev369aNt27bMnDmTdevWsXDhQoYN\nG1a9f8GCBVxwwQW0a9eOli1bMnHiRLZt29bgcTdv3kynTp2q10tKSuotP3nyZHr27EmrVq1o1aoV\ny5cvr36f8vJyvva1r33hNeXl5ZSUlBxx30VifABz587l29/+NieccAKtWrVi7ty5DcYAMGLECKZO\nnQqE/pQrr7ySxo0bH1FM6aAkICL1Gj58OE888QRTpkyhf//+tG3btnrfsGHDGDx4MJs2bWLnzp2M\nHDnyYK2/XieddBLl5YceXV7f3EkbNmzgxhtvZPz48ezYsYMdO3Zw2mmnVb9Pp06dWLt27Rde16lT\nJzZs2FDrKKZmzZqxN2Ea0S1btnyhTOJopQMHDjBkyBBuu+02PvroI3bs2MHFF1/cYAwAZ599Nk2a\nNOHll19m6tSp9Y6QioKSgIjUa8SIEcyfP5+HH36Ya6655rB9u3fvplWrVjRu3JgFCxZUf+M9qK6E\ncOWVVzJu3Dg2bdrEjh07GDt2bJ3vv2fPHoqKimjTpg1VVVU89thjLFu2rHr/9ddfz3/8x3+wePFi\nANauXUt5eTl9+vThpJNOYsyYMezdu5f9+/fz2muvAXDmmWfy0ksvUV5ezq5du7j33nvrvQYHDhzg\nwIEDtGnThqKiIubOncu8efOq91933XU89thjvPjii7g7mzdvZtWqVdX7hw8fzqhRo2jSpAnnnHNO\nve+VaUoCIlKvkpISzjnnHPbu3cvAgQMP2zd+/Hh+9atfUVxczD333MNVV1112P7Eb9OJyzfccAP9\n+/fnjDPO4KyzzuLyyy+v8/27devGrbfeSt++fTnxxBNZvnw55557bvX+IUOG8Mtf/pJhw4bRokUL\nLrvsMrZv305RURFz5sxhzZo1dO7cmU6dOvH0008DcNFFF3HVVVfRo0cPevfuzaWXXlpn3ADNmzdn\n3LhxXHHFFbRu3Zpp06YxaNCg6v29e/fmscce4+abb6a4uJhYLMaGDRuq9w8fPpxly5ZlXS0ANIuo\nSOQ0i2j+27dvH+3bt2fx4sV19h2AZhEVEclL48ePp3fv3vUmgKgk9XhJMxsA3M+hB82PrbH/58AP\nAAcaA92ANu6+M7XhiojklpNPPhngCze4ZYsGm4PMrAhYDVwIbAYWAkPdfWUd5b8H3OzuF9WyT81B\nIgn27IHmzdUcJEG2Ngf1Ada4+3p3rwCmAYPqKX818FQqghPJRwcOwE03hefcNm8edTRS6JJJAh2A\n8oT1jfFtX2BmxwEDgBlHH5pI/ti3DzZtguuuCx/+Dz0UksHLL0cdmRS6VHcMXwq8or4AkeDDD+Hm\nm+G446BrV3j00bBeVRXu4k0Y6SgSiWQ6hjcBnRPWO8a31WYoDTQFJc5PHovFiMViSYQgklumTIG/\n/hUmT4ZTT4WlS6FHj9rLlpSUaC59AQ5Nn1FWVkZZWVlG3jOZjuFGwCpCx/AWYAFwtbuvqFGuGFgH\ndHT3z+o4ljqGJW/t2AGtWx++beFCqDGTsMiXFmnHsLtXAqOAecByYJq7rzCzkWZ2Y0LRwcBf6koA\nIvnKHX7zm0MJ4Mor4b33wnYlAMl2umNY5Ah9+imMHh06eQG+/314+mlo1CjauCT/RD1EVEQSVFaG\ntv4WLeC112DWrDD6Z8YMJQDJPUndMSwiwYwZMGRIWO7RAxYt0ge/5DbVBESSNG1aSAC33hqGeC5d\nqgQguU81AZEGbN0KJ54Ylhctgl69oo1HJJVUExCpw+zZYHYoAUyfrgQg+Uc1AZFaPPpomOIB4M03\n4VvfijYekXRRTUAkwZQp4dv/ddfBwIFhrL8SgOQzJQGRuN//Hg4+/e/uu8PQT5F8p+YgKXiffgp9\n+8K778KZZ8KSJVFHJJI5qglIwfr4Y7jssnDT17vvwuuvKwFI4VESkILjDrfcAm3awMyZ8OCDYdx/\n375RRyaSeWoOkoKyZg2cckpYnjgRbryx/vIi+U41ASkYpaWHEsCOHUoAIqCagBSAzz+Hxo3D8h13\nhGSg6R5EAiUByWsffxza/gGeeAJGjIg2HpFso+YgyVtLloQE0LNnSAZKACJfpIfKSF5avz481Wvb\ntjDyR4/wlVymh8qIJMkdrrgCunQJCWDPHiUAkfooCUje2LwZiorCbJ/XXRcSQtOmUUclkt2SSgJm\nNsDMVprZajMbXUeZmJktMbNlZvZiasMUqd9f/wodOoT2/08/hYcfjjoikdzQYBIwsyLgD0B/4DTg\najM7tUaZYuBB4HvufjpwRRpiFfmCqir4xS/gu9+Fu+6CxYuhefOooxLJHckMEe0DrHH39QBmNg0Y\nBKxMKDMMmOHumwDcfVuqAxWpafPm8O0f4C9/CYlARL6cZJqDOgDlCesb49sSnQK0NrMXzWyhmQ1P\nVYAiNbnDQw8dSgAVFUoAIkcqVTeLHQP0Ai4AmgGvm9nr7v5ezYKlpaXVy7FYjFgslqIQpBA8+eSh\n8f5dusDataEzWCSflJWVUVZWlpH3avA+ATPrC5S6+4D4+hjA3X1sQpnRwFfc/a74+sPAXHefUeNY\nuk9AjkhFBZx/Prz6alhfuxb+7u+ijUkkU6K+T2Ah0NXMSsysCTAUmF2jzCzgXDNrZGZNgbOBFakN\nVQrVoEHQpElIAEuXhuYgJQCR1GgwCbh7JTAKmAcsB6a5+wozG2lmN8bLrAT+ArwN/B8wyd3fTV/Y\nUigmTYLZs+Gii2DrVujRI+qIRPKLpo2QrPXDH4ZJ36ZPh8svjzoakeikszlIs4hK1tm7F/7hH2DR\nInjqKSUAkXRSEpCs8uKLcMEFYXnx4nAHsIikj5KAZI25c+GSS8IDYPbuhWP01ymSdhphLVlhwoSQ\nAIYNgwMHlABEMkUdwxK58ePhxz8O8/8vXBh1NCLZJ+r7BETS5n/+JySAsWOVAESioEq3RKKqCmKx\n8ME/axYMHBh1RCKFSUlAMu7AATj22LCsEUAi0VISkIz65BMoKQnLO3dCcXG08YgUOvUJSMYsXx4+\n9Dt1CrUBJQCR6CkJSEb813/B6aeH5cWLw70AIhI9DRGVtNuzJzzy8bzzIENTpIvkFQ0RlZx1330h\nAfzgB0oAItlINQFJi717oVmzsNy/P8yZoyYgkSOlmoDklPLyQwngvvvCDWFKACLZSUNEJaU++gg6\ndw7DQFevDk8EE5HspeYgSZkNG8KHf4sW4R4AS0vlVaTwqDlIsp479OsHt9+uBCCSS1QTkJS4+264\n884wJ5ASgEhqRV4TMLMBZrbSzFab2eha9p9nZjvNbHH8547UhyrZat26kABmzlQCEMk1DXYMm1kR\n8AfgQmAzsNDMZrn7yhpFX3J3zQVZYPbtg8GD4de/hkGDoo5GRL6sZGoCfYA17r7e3SuAaUBt/931\nHbDAVFXBcceFIaG//GXU0YjIkUgmCXQAyhPWN8a31fRtM3vLzP5sZt1TEp1krf37oVGjsLx6tZqB\nRHJVqu4TWAR0dve9ZnYxMBM4pbaCpaWl1cuxWIxYLJaiECRT9uyBvn2hTRtYsgTato06IpH8UlZW\nRlmG5llpcHSQmfUFSt19QHx9DODuPrae17wPfMvdt9fYrtFBOW7FCuger+fpeQAimRH16KCFQFcz\nKzGzJsBQYHaNANsnLPchJJftSF559tmQAG64IfQHKAGI5L4Gm4PcvdLMRgHzCEnjEXdfYWYjw26f\nBAwxsx8BFcBnwFXpDFoyb80auPRSuP56mDQp6mhEJFV0s5g06ODzANq3hw8/jDoakcITdXOQFLiB\n8bs/li6NNg4RST0lAanXjBnwwguwcWOoCYhIflESkDqNGwdDhsBTT0GH2u4MEZGcpz4BqdVrr4VZ\nQX/1qzA5nIhEJ519AkoC8gVbt8KJJ8K998LoL0wXKCKZpiQgGbNpE3TsGJb1TyWSHTQ6SDLmggvC\n788+izYOEckMJQGpdvfdYTK4XbvgK1+JOhoRyQQlAQHgjTfgwQfDUNAWLaKORkQyRUlAuP/+MCvo\nAw9oKKhIoVHHcIF78cXQD/DAAzBqVNTRiEhtNDpI0mLDBigpgQsvhPnzo45GROqiJCApV1EBTZrA\n6afD22/ryWAi2UxDRCXlxo6FHj2UAEQKnWoCBejNN2HAgPBoyE6doo5GRBqimoCkTFUV9O4dOoGV\nAERENYECc//9cMstIRmoGUgkN6gmICmxa1dIAP/5n0oAIhKoJlAgqqqgUSPo1g2WL1cSEMklkdcE\nzGyAma00s9VmVufkwmbW28wqzOz7qQtRUmHevPD71VeVAETkkAaTgJkVAX8A+gOnAVeb2al1lLsX\n+Euqg5Sjs28fXHwx3H47tGoVdTQikk2SqQn0Ada4+3p3rwCmAYNqKfcvwHTgbymMT1Kgc+fw+9/+\nLdo4RCT7JJMEOgDlCesb49uqmdlXgcHu/hCgxoYsMns2FBXBRx+pGUhEvuiYFB3nfiCxr6DOj5vS\n0tLq5VgsRiwWS1EIUtOePfCTn8CUKdCmTdTRiEiyysrKKCsry8h7NTg6yMz6AqXuPiC+PgZwdx+b\nUGbdwUWgDbAHuNHdZ9c4lkYHZYh7qAEMHAizZkUdjYgcjUgnkDOzRsAq4EJgC7AAuNrdV9RR/jFg\njrv/dy37lAQy5KKL4Pnn4eOPoXXrqKMRkaORziTQYHOQu1ea2ShgHqEP4RF3X2FmI8Nun1TzJWmI\nU5LkDnfdFRLAokVKACJSP90slkfcoWVL+OQTeO65MCxURHJfpDUByQ27d8NZZ4UEsG8fHHts1BGJ\nSC5QEsgD+/bBN74BmzfDunVKACKSPCWBPDBmTEgAn34KzZtHHY2I5BLNIprj3n8ffv97mDFDCUBE\nvjx1DOewXbtCR3Dv3rBgQdTRiEi6RD6LqGSfioqQAGIxJQAROXKqCeSgjz6Cdu3C8q5d0KJFtPGI\nSHppiKgc5owzwu+KCjhG/4IichTUHJRjfv1r2LIFtm1TAhCRo6fmoByyZQt89avwzDMweHDU0YhI\npkQ6gVxK30xJ4Ijt2AF9+kCHDpChGWZFJEsoCQiDBoXnA8yfH3UkIpJp6hgucPfeG54QtnNn1JGI\nSL5Rx3CWe+SR8ID4P/0JioujjkZE8o2ag7LYggVw9tkweTIMHx51NCISFd0xXIAmTAgJoFcvJQAR\nSR/VBLLQm2+G+YC6d4fly6OORkSipppAAdm5MySAIUOUAEQk/VQTyCKVlYfuAq6shCKlaBEhC2oC\nZjbAzFaa2WozG13L/oFmttTMlpjZAjPrl/pQ89/994ffq1YpAYhIZjRYEzCzImA1cCGwGVgIDHX3\nlQllmrr73vjyN4Gn3b1bLcdSTaAOn30GTZvCzJnhxjARkYOirgn0Ada4+3p3rwCmAYd9TB1MAHHN\ngarUhVgYbr89PBls4MCoIxGRQpJMEugAlCesb4xvO4yZDTazFcAc4J9SE15hmDoVxo2Dd98FS0uu\nFxGpXcqmjXD3mcBMMzsXuAf4Tm3lSktLq5djsRixWCxVIeSkqVPhBz+A556DTp2ijkZEskFZWRll\nGZopMpk+gb5AqbsPiK+PAdzdx9bzmrVAb3ffXmO7+gQSbN8OJ5ygZwSLSP2i7hNYCHQ1sxIzawIM\nBWbXCPBrCcu9gCY1E4AcrqoKfvazMDX0G29EHY2IFKoGm4PcvdLMRgHzCEnjEXdfYWYjw26fBFxu\nZiOAA8BnwJXpDDof/OhHMG0arFunfgARiY5uFovAxInwz/8cZga9UulSRBqgh8rkkbfegp49Yc4c\n+N73oo5GRHKBkkCe+PxzaNwYLr00PCRGRCQZUXcMS4pMmQKtWsGsWVFHIiIS6PGSGTJjBlx7Lbzy\nijqCRSR7qDkoA3btgm7doE+fMDeQiMiXoT6BHHfHHbB2LTz1VNSRiEguSmcSUHNQmu3fD088oRqA\niGQndQyn2aRJ8I1vhGcFi4hkGzUHpdH69dClCyxbBqedFnU0IpKr1CeQo4YOhSZNYPLkqCMRkVym\nPoEcNG5cmB567dqoIxERqZtqAmmwYgV07x6eFXzKKVFHIyK5Ts1BOaSqCr75TWjRAl5/PepoRCQf\nqDkoh0yZEm4Oe/HFqCMREWmYagIpdOBAuDP40UfhvPOijkZE8oUmkMsRY8fCyScrAYhI7lBNIEU+\n+QSKi0MzUCwWdTQikk9UE8gBEybAP/6jEoCI5BYlgRSorITRo2HgwKgjERH5cpJKAmY2wMxWmtlq\nMxtdy/5hZrY0/vOKmX0z9aFmr4ceCr+vvz7aOEREvqwGk4CZFQF/APoDpwFXm9mpNYqtA/7B3c8A\n7gH+mOpAs9X8+fAv/xKGhhapXiUiOabBjmEz6wvc6e4Xx9fHAO7uY+so3xJ4x9071bIvrzqGKyrC\n3EAlJfDBB1FHIyL5KuqO4Q5AecL6xvi2ulwPzD2aoHLF1VdD795KACKSu1J6x7CZnQ9cC5xbV5nS\n0tLq5VgsRixHh9M8/XR4bvCqVVFHIiL5pqysjLKysoy8V7LNQaXuPiC+XmtzkJn1AGYAA9y91rkz\n86U5yD20/591FixcGHU0IpLvom4OWgh0NbMSM2sCDAVm1wiwMyEBDK8rAeSTBx4Iv994I9o4RESO\nVoPNQe5eaWajgHmEpPGIu68ws5Fht08CfgW0BsabmQEV7t4nnYFHZetW+OlPw/MCNBpIRHKdpo34\nEnbtgpYtYdAgPTheRDJHzxPIAhUV0L8/vPoq7NkDx2gSbhHJED1PIAs0aRJ+v/yyEoCI5A99nCVh\nbvyuhx07QnOQiEi+UNdmA1atgksugdJSJQARyT9KAvV44AE49VQYNQruvDPqaEREUk8dw3XYsyfM\nCXT33XDTTVFHIyKFTKODMqyyEv7+76G8HNav1/0AIhItjQ7KoPJy6NULtm2D/fuVAEQkvykJJHjn\nHejRIywvWnRoWKiISL7S99y4F14ICWDMmDBBXK9eUUckIpJ+Bd8n4A49e8LSpeHxkH8smGeiiUiu\nUJ9AGk2aFBLAvHnwne9EHY2ISGYVdE3gmWfC08GefRYuuijqaEREaqeaQIq5w2WXwaxZMGeOEoCI\nFK6C7Bi++eaQAKZPh+99L+poRESiU1A1gT17QvPPnDmwYQN06hR1RCIi0SqImoA7TJkCZ58N69bB\np58qAYiIQAEkgU2bwhQQI0fCj34UbgJr3jzqqEREskNSScDMBpjZSjNbbWaja9n/DTN7zcz2mdnP\nUh/mkXnpJejYEY47Ljwb+Mc/hmOPjToqEZHs0eAQUTMrAlYDFwKbgYXAUHdfmVCmDVACDAZ2uPvv\n6jhWxoaI7t0bvvHfdx/ccktG3lJEJC3SOUQ0mZpAH2CNu6939wpgGjAosYC7b3P3RcDnaYgxaVVV\nsGIFdOgAzZrBwIFKACIi9UkmCXQAyhPWN8a3ZZU//xkaNYLu3aFrV/jNb8LNYCIiUrecHyK6ezcc\nf3xYfvRRGD5cD4IXEUlWMh+Xm4DOCesd49uOSGlpafVyLBYjFosd6aH429+gffuwvGcPNG16xIcS\nEckaZWVllJWVZeS9kukYbgSsInQMbwEWAFe7+4payt4J7Hb3++o41lF3DH/+OSxYAJs3w3XXwSef\nQEWFvv2LSP6KtGPY3SuBUcA8YDkwzd1XmNlIM7sxHmB7MysHbgF+aWYbzCylo/Hffhu+9S1o3Bj6\n9YMrroB/+qfw9C8lABGRI5P1s4i6w/e/DzNnhm/+l14aRv1YWnKiiEj2KbhZRHfvhsmTD831D/Dv\n/w7/+q/RxiUikm+yJgm8+WaY0mHq1HCnL8Bdd0FZWRj906hRpOGJiOSlrEgCixZB795h+Zpr4KGH\nwnh/ERFJr0iTwN/+Bi+/DEOGwD33wC9+obZ+EZFMynjHcGWlc8cd8Nvfhm2NG8OwYfD44xkLQ0Qk\np+RVx3Bi2/7SpdCjR6YjEBGRgzL+PIHBg8MNX+5KACIiUct4c1BVlavdX0TkS4h6KumUUgIQEcke\nef94SRERqZuSgIhIAVMSEBEpYEoCIiIFTElARKSAKQmIiBQwJQERkQKmJCAiUsCUBERECpiSgIhI\nAUsqCZjZADNbaWarzWx0HWXGmdkaM3vLzM5MbZgiIpIODSYBMysC/gD0B04DrjazU2uUuRj4mrt/\nHRgJTEhDrHmlrKws6hCyhq7FIboWh+haZEYyNYE+wBp3X+/uFcA0YFCNMoOAyQDu/gZQbGbtUxpp\nntEf+CG6FofoWhyia5EZySSBDkB5wvrG+Lb6ymyqpYyIiGQZdQyLiBSwBh8qY2Z9gVJ3HxBfHwO4\nu49NKDMBeNHd/xRfXwmc5+5baxwrc0+wERHJI1E+Y3gh0NXMSoAtwFDg6hplZgM/Bv4UTxo7ayYA\nSN9JiIjIkWkwCbh7pZmNAuYRmo8ecfcVZjYy7PZJ7v6cmV1iZu8Be4Br0xu2iIikQkafMSwiItkl\nYx3DydxwlsvMrKOZvWBmy83sHTP7SXx7KzObZ2arzOwvZlac8Jrb4zfYrTCz7yZs72Vmb8ev1f1R\nnE8qmFmRmS02s9nx9YK8FmZWbGb/L35uy83s7AK+FreY2bL4efyXmTUplGthZo+Y2VYzezthW8rO\nPX4tp8Vf87qZdU4qMHdP+w8h2bwHlACNgbeAUzPx3pn6AU4EzowvNwdWAacCY4Hb4ttHA/fGl7sD\nSwhNcl3i1+dgzewNoHd8+Tmgf9Tnd4TX5BZgCjA7vl6Q1wJ4HLg2vnwMUFyI1wL4KrAOaBJf/xNw\nTaFcC+Bc4Ezg7YRtKTt34EfA+PjyVcC0ZOLKVE0gmRvOcpq7f+jub8WXdwMrgI6E83wiXuwJYHB8\neSDhH+lzd/8AWAP0MbMTgePdfWG83OSE1+QMM+sIXAI8nLC54K6FmbUA/t7dHwOIn+MuCvBaxDUC\nmpnZMcCxM7kXAAACZElEQVRxhHuKCuJauPsrwI4am1N57onHmg5cmExcmUoCydxwljfMrAsh4/8f\n0N7jI6Xc/UOgXbxYXTfYdSBcn4Ny9Vr9J/CvQGKnUyFei5OBbWb2WLxpbJKZNaUAr4W7bwbuAzYQ\nzmuXu8+nAK9FgnYpPPfq17h7JbDTzFo3FIBuFksxM2tOyMI/jdcIava8531PvJn9I7A1XjOqb1hw\n3l8LQnW+F/Cgu/cijJ4bQ2H+XbQkfFstITQNNTOzH1CA16IeqTz3pIbkZyoJbAISOyk6xrfllXgV\ndzrwpLvPim/eenAepXhV7m/x7ZuATgkvP3hN6tqeS/oBA81sHfAUcIGZPQl8WIDXYiNQ7u5vxtdn\nEJJCIf5dXASsc/ft8W+qzwDnUJjX4qBUnnv1PjNrBLRw9+0NBZCpJFB9w5mZNSHccDY7Q++dSY8C\n77r77xO2zQZ+GF++BpiVsH1ovEf/ZKArsCBeJdxlZn3MzIARCa/JCe7+C3fv7O5/R/i3fsHdhwNz\nKLxrsRUoN7NT4psuBJZTgH8XhGagvmb2lfg5XAi8S2FdC+Pwb+ipPPfZ8WMAXAG8kFREGewZH0AY\nMbMGGBNF73yaz68fUEkY+bQEWBw/59bA/Pi5zwNaJrzmdkKv/wrguwnbvwW8E79Wv4/63I7yupzH\nodFBBXktgDMIX4TeAv6bMDqoUK/FnfHzepvQidm4UK4FMBXYDOwnJMRrgVapOnfgWODp+Pb/A7ok\nE5duFhMRKWDqGBYRKWBKAiIiBUxJQESkgCkJiIgUMCUBEZECpiQgIlLAlARERAqYkoCISAH7/4Sf\ndowJCRXSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119a861d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
