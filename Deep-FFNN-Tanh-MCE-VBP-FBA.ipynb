{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train]) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dy @ self.W_fixed[2].T # FBA\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dy @ self.W_fixed[1][layer].T # FBA\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            #             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            #             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3098, acc-0.0800, valid loss-2.3243, acc-0.0982, test loss-2.3315, acc-0.0817\n",
      "Iter-20, train loss-2.3178, acc-0.1500, valid loss-2.3227, acc-0.0968, test loss-2.3298, acc-0.0848\n",
      "Iter-30, train loss-2.3433, acc-0.0600, valid loss-2.3211, acc-0.0966, test loss-2.3281, acc-0.0865\n",
      "Iter-40, train loss-2.3289, acc-0.0700, valid loss-2.3195, acc-0.0992, test loss-2.3264, acc-0.0881\n",
      "Iter-50, train loss-2.3225, acc-0.1000, valid loss-2.3180, acc-0.0996, test loss-2.3248, acc-0.0886\n",
      "Iter-60, train loss-2.3214, acc-0.1400, valid loss-2.3163, acc-0.1010, test loss-2.3230, acc-0.0901\n",
      "Iter-70, train loss-2.2961, acc-0.1000, valid loss-2.3142, acc-0.0994, test loss-2.3208, acc-0.0923\n",
      "Iter-80, train loss-2.3088, acc-0.0700, valid loss-2.3112, acc-0.1010, test loss-2.3176, acc-0.0947\n",
      "Iter-90, train loss-2.3140, acc-0.0800, valid loss-2.3082, acc-0.1020, test loss-2.3146, acc-0.0952\n",
      "Iter-100, train loss-2.3206, acc-0.1100, valid loss-2.3051, acc-0.1024, test loss-2.3113, acc-0.0981\n",
      "Iter-110, train loss-2.2984, acc-0.0800, valid loss-2.3011, acc-0.1076, test loss-2.3072, acc-0.1013\n",
      "Iter-120, train loss-2.2958, acc-0.1100, valid loss-2.2972, acc-0.1090, test loss-2.3032, acc-0.1062\n",
      "Iter-130, train loss-2.3102, acc-0.0800, valid loss-2.2932, acc-0.1128, test loss-2.2990, acc-0.1112\n",
      "Iter-140, train loss-2.2801, acc-0.1200, valid loss-2.2886, acc-0.1214, test loss-2.2943, acc-0.1171\n",
      "Iter-150, train loss-2.2786, acc-0.1400, valid loss-2.2838, acc-0.1296, test loss-2.2893, acc-0.1240\n",
      "Iter-160, train loss-2.2882, acc-0.1500, valid loss-2.2781, acc-0.1414, test loss-2.2834, acc-0.1377\n",
      "Iter-170, train loss-2.2782, acc-0.1500, valid loss-2.2725, acc-0.1494, test loss-2.2777, acc-0.1494\n",
      "Iter-180, train loss-2.2826, acc-0.1600, valid loss-2.2661, acc-0.1622, test loss-2.2710, acc-0.1602\n",
      "Iter-190, train loss-2.2671, acc-0.1700, valid loss-2.2591, acc-0.1778, test loss-2.2639, acc-0.1715\n",
      "Iter-200, train loss-2.2620, acc-0.1800, valid loss-2.2516, acc-0.1902, test loss-2.2563, acc-0.1821\n",
      "Iter-210, train loss-2.2351, acc-0.1800, valid loss-2.2440, acc-0.1974, test loss-2.2485, acc-0.1889\n",
      "Iter-220, train loss-2.2282, acc-0.1900, valid loss-2.2363, acc-0.2010, test loss-2.2405, acc-0.1930\n",
      "Iter-230, train loss-2.2437, acc-0.1900, valid loss-2.2281, acc-0.2052, test loss-2.2322, acc-0.1967\n",
      "Iter-240, train loss-2.2347, acc-0.1800, valid loss-2.2199, acc-0.2084, test loss-2.2240, acc-0.1973\n",
      "Iter-250, train loss-2.2063, acc-0.2200, valid loss-2.2117, acc-0.2108, test loss-2.2156, acc-0.1993\n",
      "Iter-260, train loss-2.2298, acc-0.1700, valid loss-2.2030, acc-0.2134, test loss-2.2068, acc-0.2016\n",
      "Iter-270, train loss-2.1856, acc-0.2500, valid loss-2.1942, acc-0.2146, test loss-2.1979, acc-0.2034\n",
      "Iter-280, train loss-2.1491, acc-0.3000, valid loss-2.1856, acc-0.2170, test loss-2.1891, acc-0.2066\n",
      "Iter-290, train loss-2.1843, acc-0.2100, valid loss-2.1766, acc-0.2216, test loss-2.1800, acc-0.2117\n",
      "Iter-300, train loss-2.1854, acc-0.1800, valid loss-2.1677, acc-0.2268, test loss-2.1710, acc-0.2158\n",
      "Iter-310, train loss-2.1790, acc-0.1600, valid loss-2.1581, acc-0.2290, test loss-2.1612, acc-0.2180\n",
      "Iter-320, train loss-2.1227, acc-0.3000, valid loss-2.1482, acc-0.2326, test loss-2.1513, acc-0.2228\n",
      "Iter-330, train loss-2.1405, acc-0.2200, valid loss-2.1384, acc-0.2356, test loss-2.1414, acc-0.2249\n",
      "Iter-340, train loss-2.1505, acc-0.2300, valid loss-2.1287, acc-0.2396, test loss-2.1317, acc-0.2288\n",
      "Iter-350, train loss-2.0965, acc-0.2300, valid loss-2.1182, acc-0.2442, test loss-2.1211, acc-0.2344\n",
      "Iter-360, train loss-2.1028, acc-0.2300, valid loss-2.1080, acc-0.2494, test loss-2.1109, acc-0.2384\n",
      "Iter-370, train loss-2.1164, acc-0.2800, valid loss-2.0984, acc-0.2534, test loss-2.1012, acc-0.2432\n",
      "Iter-380, train loss-2.1303, acc-0.2700, valid loss-2.0886, acc-0.2596, test loss-2.0913, acc-0.2508\n",
      "Iter-390, train loss-2.0866, acc-0.2300, valid loss-2.0787, acc-0.2672, test loss-2.0814, acc-0.2572\n",
      "Iter-400, train loss-2.0967, acc-0.2300, valid loss-2.0685, acc-0.2772, test loss-2.0712, acc-0.2657\n",
      "Iter-410, train loss-2.0674, acc-0.2800, valid loss-2.0583, acc-0.2870, test loss-2.0610, acc-0.2734\n",
      "Iter-420, train loss-2.0744, acc-0.2600, valid loss-2.0482, acc-0.2942, test loss-2.0507, acc-0.2819\n",
      "Iter-430, train loss-2.0770, acc-0.2600, valid loss-2.0376, acc-0.3050, test loss-2.0400, acc-0.2913\n",
      "Iter-440, train loss-2.0868, acc-0.2700, valid loss-2.0274, acc-0.3156, test loss-2.0298, acc-0.3028\n",
      "Iter-450, train loss-2.0078, acc-0.3500, valid loss-2.0171, acc-0.3264, test loss-2.0194, acc-0.3112\n",
      "Iter-460, train loss-2.0530, acc-0.2900, valid loss-2.0068, acc-0.3394, test loss-2.0093, acc-0.3245\n",
      "Iter-470, train loss-2.0106, acc-0.3500, valid loss-1.9965, acc-0.3528, test loss-1.9989, acc-0.3390\n",
      "Iter-480, train loss-1.9984, acc-0.3200, valid loss-1.9864, acc-0.3642, test loss-1.9889, acc-0.3472\n",
      "Iter-490, train loss-2.0265, acc-0.3000, valid loss-1.9765, acc-0.3764, test loss-1.9790, acc-0.3604\n",
      "Iter-500, train loss-2.0110, acc-0.4400, valid loss-1.9664, acc-0.3884, test loss-1.9688, acc-0.3732\n",
      "Iter-510, train loss-1.9866, acc-0.3900, valid loss-1.9562, acc-0.4006, test loss-1.9586, acc-0.3820\n",
      "Iter-520, train loss-1.9563, acc-0.3800, valid loss-1.9460, acc-0.4118, test loss-1.9485, acc-0.3944\n",
      "Iter-530, train loss-1.9919, acc-0.3200, valid loss-1.9360, acc-0.4186, test loss-1.9385, acc-0.4044\n",
      "Iter-540, train loss-1.9221, acc-0.4300, valid loss-1.9258, acc-0.4268, test loss-1.9283, acc-0.4149\n",
      "Iter-550, train loss-1.9417, acc-0.4200, valid loss-1.9154, acc-0.4380, test loss-1.9179, acc-0.4234\n",
      "Iter-560, train loss-1.9630, acc-0.4100, valid loss-1.9048, acc-0.4466, test loss-1.9073, acc-0.4305\n",
      "Iter-570, train loss-1.8912, acc-0.4700, valid loss-1.8941, acc-0.4532, test loss-1.8966, acc-0.4408\n",
      "Iter-580, train loss-1.8305, acc-0.5300, valid loss-1.8836, acc-0.4600, test loss-1.8860, acc-0.4481\n",
      "Iter-590, train loss-1.8833, acc-0.4100, valid loss-1.8729, acc-0.4670, test loss-1.8754, acc-0.4552\n",
      "Iter-600, train loss-1.8639, acc-0.5000, valid loss-1.8625, acc-0.4718, test loss-1.8649, acc-0.4646\n",
      "Iter-610, train loss-1.9336, acc-0.3800, valid loss-1.8522, acc-0.4784, test loss-1.8546, acc-0.4722\n",
      "Iter-620, train loss-1.8385, acc-0.4400, valid loss-1.8414, acc-0.4844, test loss-1.8439, acc-0.4787\n",
      "Iter-630, train loss-1.8489, acc-0.4300, valid loss-1.8309, acc-0.4882, test loss-1.8333, acc-0.4846\n",
      "Iter-640, train loss-1.8470, acc-0.4200, valid loss-1.8211, acc-0.4966, test loss-1.8235, acc-0.4936\n",
      "Iter-650, train loss-1.8642, acc-0.4900, valid loss-1.8107, acc-0.5036, test loss-1.8130, acc-0.5012\n",
      "Iter-660, train loss-1.8405, acc-0.4500, valid loss-1.8003, acc-0.5086, test loss-1.8025, acc-0.5089\n",
      "Iter-670, train loss-1.7624, acc-0.5500, valid loss-1.7898, acc-0.5152, test loss-1.7920, acc-0.5143\n",
      "Iter-680, train loss-1.7253, acc-0.5600, valid loss-1.7793, acc-0.5218, test loss-1.7815, acc-0.5213\n",
      "Iter-690, train loss-1.7714, acc-0.5200, valid loss-1.7682, acc-0.5246, test loss-1.7704, acc-0.5258\n",
      "Iter-700, train loss-1.7686, acc-0.5200, valid loss-1.7576, acc-0.5294, test loss-1.7597, acc-0.5324\n",
      "Iter-710, train loss-1.7188, acc-0.5400, valid loss-1.7470, acc-0.5334, test loss-1.7491, acc-0.5377\n",
      "Iter-720, train loss-1.7340, acc-0.5800, valid loss-1.7363, acc-0.5368, test loss-1.7382, acc-0.5415\n",
      "Iter-730, train loss-1.7395, acc-0.5100, valid loss-1.7259, acc-0.5422, test loss-1.7277, acc-0.5463\n",
      "Iter-740, train loss-1.7507, acc-0.5700, valid loss-1.7152, acc-0.5442, test loss-1.7169, acc-0.5487\n",
      "Iter-750, train loss-1.7640, acc-0.5300, valid loss-1.7046, acc-0.5480, test loss-1.7063, acc-0.5542\n",
      "Iter-760, train loss-1.7068, acc-0.5800, valid loss-1.6936, acc-0.5510, test loss-1.6951, acc-0.5577\n",
      "Iter-770, train loss-1.6543, acc-0.6400, valid loss-1.6830, acc-0.5568, test loss-1.6843, acc-0.5631\n",
      "Iter-780, train loss-1.6612, acc-0.5900, valid loss-1.6720, acc-0.5608, test loss-1.6731, acc-0.5676\n",
      "Iter-790, train loss-1.6162, acc-0.5200, valid loss-1.6612, acc-0.5628, test loss-1.6622, acc-0.5701\n",
      "Iter-800, train loss-1.5800, acc-0.5800, valid loss-1.6503, acc-0.5666, test loss-1.6512, acc-0.5736\n",
      "Iter-810, train loss-1.6412, acc-0.6200, valid loss-1.6394, acc-0.5706, test loss-1.6401, acc-0.5772\n",
      "Iter-820, train loss-1.5413, acc-0.6700, valid loss-1.6286, acc-0.5726, test loss-1.6292, acc-0.5786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-1.6775, acc-0.5800, valid loss-1.6178, acc-0.5746, test loss-1.6183, acc-0.5824\n",
      "Iter-840, train loss-1.4925, acc-0.6700, valid loss-1.6069, acc-0.5778, test loss-1.6072, acc-0.5843\n",
      "Iter-850, train loss-1.7236, acc-0.5100, valid loss-1.5963, acc-0.5814, test loss-1.5964, acc-0.5901\n",
      "Iter-860, train loss-1.5030, acc-0.6800, valid loss-1.5857, acc-0.5836, test loss-1.5858, acc-0.5924\n",
      "Iter-870, train loss-1.5626, acc-0.6200, valid loss-1.5749, acc-0.5854, test loss-1.5748, acc-0.5949\n",
      "Iter-880, train loss-1.5581, acc-0.5800, valid loss-1.5643, acc-0.5886, test loss-1.5640, acc-0.5973\n",
      "Iter-890, train loss-1.5563, acc-0.5900, valid loss-1.5538, acc-0.5904, test loss-1.5534, acc-0.5990\n",
      "Iter-900, train loss-1.5020, acc-0.5800, valid loss-1.5430, acc-0.5918, test loss-1.5425, acc-0.6011\n",
      "Iter-910, train loss-1.5804, acc-0.6100, valid loss-1.5324, acc-0.5970, test loss-1.5318, acc-0.6055\n",
      "Iter-920, train loss-1.5091, acc-0.6500, valid loss-1.5218, acc-0.5990, test loss-1.5211, acc-0.6101\n",
      "Iter-930, train loss-1.4881, acc-0.6400, valid loss-1.5116, acc-0.6032, test loss-1.5108, acc-0.6144\n",
      "Iter-940, train loss-1.5073, acc-0.6300, valid loss-1.5010, acc-0.6066, test loss-1.5000, acc-0.6165\n",
      "Iter-950, train loss-1.5314, acc-0.5600, valid loss-1.4903, acc-0.6112, test loss-1.4892, acc-0.6205\n",
      "Iter-960, train loss-1.5176, acc-0.6000, valid loss-1.4799, acc-0.6168, test loss-1.4785, acc-0.6263\n",
      "Iter-970, train loss-1.4673, acc-0.6300, valid loss-1.4697, acc-0.6208, test loss-1.4682, acc-0.6305\n",
      "Iter-980, train loss-1.5260, acc-0.6400, valid loss-1.4598, acc-0.6218, test loss-1.4579, acc-0.6338\n",
      "Iter-990, train loss-1.4737, acc-0.5900, valid loss-1.4497, acc-0.6254, test loss-1.4479, acc-0.6365\n",
      "Iter-1000, train loss-1.3880, acc-0.6300, valid loss-1.4401, acc-0.6278, test loss-1.4381, acc-0.6403\n",
      "Iter-1010, train loss-1.4959, acc-0.6300, valid loss-1.4301, acc-0.6306, test loss-1.4280, acc-0.6424\n",
      "Iter-1020, train loss-1.4107, acc-0.6500, valid loss-1.4202, acc-0.6350, test loss-1.4179, acc-0.6469\n",
      "Iter-1030, train loss-1.4042, acc-0.6500, valid loss-1.4103, acc-0.6404, test loss-1.4080, acc-0.6500\n",
      "Iter-1040, train loss-1.4922, acc-0.6000, valid loss-1.4007, acc-0.6448, test loss-1.3982, acc-0.6513\n",
      "Iter-1050, train loss-1.2974, acc-0.7500, valid loss-1.3910, acc-0.6492, test loss-1.3884, acc-0.6555\n",
      "Iter-1060, train loss-1.4501, acc-0.6400, valid loss-1.3818, acc-0.6506, test loss-1.3788, acc-0.6588\n",
      "Iter-1070, train loss-1.3895, acc-0.6500, valid loss-1.3721, acc-0.6528, test loss-1.3691, acc-0.6613\n",
      "Iter-1080, train loss-1.3299, acc-0.6900, valid loss-1.3628, acc-0.6568, test loss-1.3596, acc-0.6631\n",
      "Iter-1090, train loss-1.3204, acc-0.6700, valid loss-1.3536, acc-0.6594, test loss-1.3502, acc-0.6645\n",
      "Iter-1100, train loss-1.3419, acc-0.7100, valid loss-1.3444, acc-0.6620, test loss-1.3410, acc-0.6675\n",
      "Iter-1110, train loss-1.3449, acc-0.6800, valid loss-1.3353, acc-0.6644, test loss-1.3317, acc-0.6691\n",
      "Iter-1120, train loss-1.3036, acc-0.6700, valid loss-1.3260, acc-0.6662, test loss-1.3224, acc-0.6716\n",
      "Iter-1130, train loss-1.4265, acc-0.6000, valid loss-1.3171, acc-0.6698, test loss-1.3135, acc-0.6732\n",
      "Iter-1140, train loss-1.2173, acc-0.7600, valid loss-1.3081, acc-0.6730, test loss-1.3044, acc-0.6762\n",
      "Iter-1150, train loss-1.3826, acc-0.6600, valid loss-1.2991, acc-0.6756, test loss-1.2953, acc-0.6776\n",
      "Iter-1160, train loss-1.2296, acc-0.7100, valid loss-1.2903, acc-0.6776, test loss-1.2866, acc-0.6797\n",
      "Iter-1170, train loss-1.3716, acc-0.6300, valid loss-1.2816, acc-0.6794, test loss-1.2778, acc-0.6824\n",
      "Iter-1180, train loss-1.3344, acc-0.6500, valid loss-1.2730, acc-0.6804, test loss-1.2691, acc-0.6830\n",
      "Iter-1190, train loss-1.3626, acc-0.5700, valid loss-1.2645, acc-0.6816, test loss-1.2605, acc-0.6837\n",
      "Iter-1200, train loss-1.2204, acc-0.7000, valid loss-1.2562, acc-0.6854, test loss-1.2522, acc-0.6858\n",
      "Iter-1210, train loss-1.2368, acc-0.6700, valid loss-1.2478, acc-0.6878, test loss-1.2435, acc-0.6883\n",
      "Iter-1220, train loss-1.1603, acc-0.7100, valid loss-1.2394, acc-0.6896, test loss-1.2351, acc-0.6906\n",
      "Iter-1230, train loss-1.2832, acc-0.6300, valid loss-1.2314, acc-0.6914, test loss-1.2272, acc-0.6921\n",
      "Iter-1240, train loss-1.2512, acc-0.6900, valid loss-1.2238, acc-0.6934, test loss-1.2196, acc-0.6929\n",
      "Iter-1250, train loss-1.4054, acc-0.6100, valid loss-1.2164, acc-0.6946, test loss-1.2120, acc-0.6948\n",
      "Iter-1260, train loss-1.3136, acc-0.6100, valid loss-1.2090, acc-0.6960, test loss-1.2042, acc-0.6961\n",
      "Iter-1270, train loss-1.2164, acc-0.7000, valid loss-1.2011, acc-0.6968, test loss-1.1965, acc-0.6974\n",
      "Iter-1280, train loss-1.2402, acc-0.6500, valid loss-1.1935, acc-0.6976, test loss-1.1888, acc-0.6977\n",
      "Iter-1290, train loss-1.3088, acc-0.6100, valid loss-1.1859, acc-0.6996, test loss-1.1811, acc-0.6996\n",
      "Iter-1300, train loss-1.1088, acc-0.7200, valid loss-1.1785, acc-0.7006, test loss-1.1736, acc-0.6999\n",
      "Iter-1310, train loss-1.0652, acc-0.7900, valid loss-1.1711, acc-0.7020, test loss-1.1662, acc-0.7014\n",
      "Iter-1320, train loss-1.1104, acc-0.7700, valid loss-1.1640, acc-0.7030, test loss-1.1591, acc-0.7023\n",
      "Iter-1330, train loss-1.0378, acc-0.7700, valid loss-1.1567, acc-0.7040, test loss-1.1519, acc-0.7037\n",
      "Iter-1340, train loss-1.2099, acc-0.6800, valid loss-1.1499, acc-0.7060, test loss-1.1450, acc-0.7051\n",
      "Iter-1350, train loss-1.0846, acc-0.7800, valid loss-1.1428, acc-0.7068, test loss-1.1379, acc-0.7059\n",
      "Iter-1360, train loss-1.1172, acc-0.7800, valid loss-1.1359, acc-0.7070, test loss-1.1310, acc-0.7073\n",
      "Iter-1370, train loss-1.1629, acc-0.7100, valid loss-1.1290, acc-0.7084, test loss-1.1242, acc-0.7095\n",
      "Iter-1380, train loss-1.1540, acc-0.6700, valid loss-1.1224, acc-0.7104, test loss-1.1176, acc-0.7107\n",
      "Iter-1390, train loss-1.0527, acc-0.7300, valid loss-1.1159, acc-0.7104, test loss-1.1110, acc-0.7126\n",
      "Iter-1400, train loss-1.0896, acc-0.7300, valid loss-1.1092, acc-0.7128, test loss-1.1044, acc-0.7136\n",
      "Iter-1410, train loss-1.0943, acc-0.6700, valid loss-1.1030, acc-0.7136, test loss-1.0982, acc-0.7149\n",
      "Iter-1420, train loss-1.1057, acc-0.6500, valid loss-1.0965, acc-0.7146, test loss-1.0917, acc-0.7152\n",
      "Iter-1430, train loss-1.0445, acc-0.7900, valid loss-1.0902, acc-0.7170, test loss-1.0853, acc-0.7163\n",
      "Iter-1440, train loss-1.1352, acc-0.6900, valid loss-1.0842, acc-0.7182, test loss-1.0794, acc-0.7169\n",
      "Iter-1450, train loss-1.0548, acc-0.7600, valid loss-1.0782, acc-0.7186, test loss-1.0733, acc-0.7182\n",
      "Iter-1460, train loss-1.1662, acc-0.6300, valid loss-1.0723, acc-0.7200, test loss-1.0674, acc-0.7189\n",
      "Iter-1470, train loss-1.1030, acc-0.7300, valid loss-1.0659, acc-0.7208, test loss-1.0611, acc-0.7204\n",
      "Iter-1480, train loss-1.0597, acc-0.7100, valid loss-1.0601, acc-0.7224, test loss-1.0552, acc-0.7222\n",
      "Iter-1490, train loss-1.0888, acc-0.7100, valid loss-1.0544, acc-0.7228, test loss-1.0493, acc-0.7227\n",
      "Iter-1500, train loss-1.0555, acc-0.7400, valid loss-1.0486, acc-0.7258, test loss-1.0435, acc-0.7238\n",
      "Iter-1510, train loss-1.0497, acc-0.6900, valid loss-1.0429, acc-0.7272, test loss-1.0377, acc-0.7248\n",
      "Iter-1520, train loss-1.0619, acc-0.7000, valid loss-1.0373, acc-0.7266, test loss-1.0320, acc-0.7262\n",
      "Iter-1530, train loss-1.0588, acc-0.6800, valid loss-1.0322, acc-0.7274, test loss-1.0271, acc-0.7269\n",
      "Iter-1540, train loss-1.0073, acc-0.7500, valid loss-1.0270, acc-0.7294, test loss-1.0218, acc-0.7280\n",
      "Iter-1550, train loss-0.9699, acc-0.7600, valid loss-1.0217, acc-0.7306, test loss-1.0164, acc-0.7290\n",
      "Iter-1560, train loss-0.9696, acc-0.7600, valid loss-1.0164, acc-0.7328, test loss-1.0111, acc-0.7299\n",
      "Iter-1570, train loss-0.9985, acc-0.7300, valid loss-1.0111, acc-0.7348, test loss-1.0057, acc-0.7310\n",
      "Iter-1580, train loss-1.0391, acc-0.7400, valid loss-1.0059, acc-0.7360, test loss-1.0007, acc-0.7317\n",
      "Iter-1590, train loss-1.0033, acc-0.7300, valid loss-1.0014, acc-0.7372, test loss-0.9959, acc-0.7321\n",
      "Iter-1600, train loss-1.0348, acc-0.6700, valid loss-0.9963, acc-0.7374, test loss-0.9907, acc-0.7328\n",
      "Iter-1610, train loss-1.0826, acc-0.6900, valid loss-0.9914, acc-0.7388, test loss-0.9858, acc-0.7340\n",
      "Iter-1620, train loss-0.9517, acc-0.7500, valid loss-0.9864, acc-0.7402, test loss-0.9807, acc-0.7348\n",
      "Iter-1630, train loss-1.0037, acc-0.7300, valid loss-0.9816, acc-0.7424, test loss-0.9760, acc-0.7358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.9721, acc-0.7300, valid loss-0.9766, acc-0.7436, test loss-0.9709, acc-0.7376\n",
      "Iter-1650, train loss-1.0361, acc-0.7300, valid loss-0.9719, acc-0.7438, test loss-0.9662, acc-0.7380\n",
      "Iter-1660, train loss-1.0120, acc-0.7000, valid loss-0.9672, acc-0.7450, test loss-0.9615, acc-0.7387\n",
      "Iter-1670, train loss-0.9365, acc-0.7600, valid loss-0.9623, acc-0.7470, test loss-0.9566, acc-0.7400\n",
      "Iter-1680, train loss-1.0090, acc-0.7400, valid loss-0.9578, acc-0.7480, test loss-0.9519, acc-0.7407\n",
      "Iter-1690, train loss-1.0290, acc-0.7000, valid loss-0.9534, acc-0.7494, test loss-0.9475, acc-0.7422\n",
      "Iter-1700, train loss-0.9713, acc-0.7400, valid loss-0.9489, acc-0.7508, test loss-0.9430, acc-0.7439\n",
      "Iter-1710, train loss-0.9240, acc-0.7700, valid loss-0.9446, acc-0.7512, test loss-0.9385, acc-0.7451\n",
      "Iter-1720, train loss-0.9722, acc-0.7100, valid loss-0.9402, acc-0.7522, test loss-0.9342, acc-0.7465\n",
      "Iter-1730, train loss-0.9384, acc-0.7700, valid loss-0.9363, acc-0.7532, test loss-0.9301, acc-0.7476\n",
      "Iter-1740, train loss-0.9061, acc-0.7400, valid loss-0.9318, acc-0.7542, test loss-0.9255, acc-0.7493\n",
      "Iter-1750, train loss-0.9219, acc-0.6900, valid loss-0.9278, acc-0.7548, test loss-0.9215, acc-0.7500\n",
      "Iter-1760, train loss-0.9357, acc-0.7500, valid loss-0.9235, acc-0.7554, test loss-0.9173, acc-0.7502\n",
      "Iter-1770, train loss-0.8971, acc-0.7600, valid loss-0.9194, acc-0.7562, test loss-0.9133, acc-0.7510\n",
      "Iter-1780, train loss-0.9324, acc-0.7700, valid loss-0.9154, acc-0.7572, test loss-0.9092, acc-0.7513\n",
      "Iter-1790, train loss-0.8193, acc-0.8400, valid loss-0.9117, acc-0.7580, test loss-0.9055, acc-0.7524\n",
      "Iter-1800, train loss-0.9583, acc-0.7100, valid loss-0.9079, acc-0.7584, test loss-0.9016, acc-0.7531\n",
      "Iter-1810, train loss-0.9885, acc-0.6900, valid loss-0.9041, acc-0.7586, test loss-0.8978, acc-0.7545\n",
      "Iter-1820, train loss-0.9716, acc-0.7300, valid loss-0.9002, acc-0.7594, test loss-0.8938, acc-0.7552\n",
      "Iter-1830, train loss-0.9323, acc-0.7600, valid loss-0.8967, acc-0.7612, test loss-0.8902, acc-0.7556\n",
      "Iter-1840, train loss-0.8660, acc-0.8100, valid loss-0.8931, acc-0.7624, test loss-0.8865, acc-0.7566\n",
      "Iter-1850, train loss-0.8623, acc-0.7800, valid loss-0.8893, acc-0.7636, test loss-0.8827, acc-0.7575\n",
      "Iter-1860, train loss-0.9943, acc-0.7200, valid loss-0.8856, acc-0.7644, test loss-0.8790, acc-0.7584\n",
      "Iter-1870, train loss-0.7889, acc-0.8400, valid loss-0.8820, acc-0.7658, test loss-0.8754, acc-0.7588\n",
      "Iter-1880, train loss-0.8536, acc-0.7300, valid loss-0.8783, acc-0.7662, test loss-0.8718, acc-0.7600\n",
      "Iter-1890, train loss-0.8682, acc-0.7500, valid loss-0.8745, acc-0.7662, test loss-0.8680, acc-0.7612\n",
      "Iter-1900, train loss-0.7851, acc-0.7600, valid loss-0.8709, acc-0.7678, test loss-0.8643, acc-0.7634\n",
      "Iter-1910, train loss-0.9621, acc-0.7300, valid loss-0.8674, acc-0.7700, test loss-0.8608, acc-0.7648\n",
      "Iter-1920, train loss-0.8837, acc-0.7800, valid loss-0.8638, acc-0.7710, test loss-0.8572, acc-0.7667\n",
      "Iter-1930, train loss-0.8375, acc-0.8000, valid loss-0.8602, acc-0.7712, test loss-0.8537, acc-0.7678\n",
      "Iter-1940, train loss-0.9085, acc-0.7300, valid loss-0.8568, acc-0.7712, test loss-0.8503, acc-0.7688\n",
      "Iter-1950, train loss-1.0051, acc-0.6600, valid loss-0.8534, acc-0.7722, test loss-0.8469, acc-0.7700\n",
      "Iter-1960, train loss-0.8153, acc-0.7100, valid loss-0.8502, acc-0.7732, test loss-0.8438, acc-0.7705\n",
      "Iter-1970, train loss-0.8395, acc-0.7500, valid loss-0.8467, acc-0.7756, test loss-0.8403, acc-0.7721\n",
      "Iter-1980, train loss-0.8986, acc-0.6900, valid loss-0.8434, acc-0.7774, test loss-0.8370, acc-0.7730\n",
      "Iter-1990, train loss-0.6610, acc-0.8500, valid loss-0.8402, acc-0.7778, test loss-0.8339, acc-0.7743\n",
      "Iter-2000, train loss-0.8185, acc-0.7900, valid loss-0.8370, acc-0.7796, test loss-0.8307, acc-0.7747\n",
      "Iter-2010, train loss-0.8402, acc-0.7500, valid loss-0.8339, acc-0.7808, test loss-0.8276, acc-0.7763\n",
      "Iter-2020, train loss-0.8896, acc-0.7600, valid loss-0.8305, acc-0.7816, test loss-0.8242, acc-0.7772\n",
      "Iter-2030, train loss-0.8197, acc-0.8200, valid loss-0.8276, acc-0.7828, test loss-0.8213, acc-0.7777\n",
      "Iter-2040, train loss-0.7138, acc-0.8400, valid loss-0.8246, acc-0.7828, test loss-0.8183, acc-0.7777\n",
      "Iter-2050, train loss-0.8015, acc-0.8000, valid loss-0.8216, acc-0.7834, test loss-0.8153, acc-0.7783\n",
      "Iter-2060, train loss-0.9157, acc-0.7200, valid loss-0.8184, acc-0.7842, test loss-0.8123, acc-0.7790\n",
      "Iter-2070, train loss-0.7555, acc-0.8200, valid loss-0.8158, acc-0.7850, test loss-0.8096, acc-0.7790\n",
      "Iter-2080, train loss-0.9497, acc-0.7400, valid loss-0.8129, acc-0.7852, test loss-0.8067, acc-0.7806\n",
      "Iter-2090, train loss-0.8172, acc-0.7800, valid loss-0.8100, acc-0.7856, test loss-0.8037, acc-0.7815\n",
      "Iter-2100, train loss-0.7666, acc-0.8000, valid loss-0.8072, acc-0.7862, test loss-0.8007, acc-0.7828\n",
      "Iter-2110, train loss-0.6657, acc-0.8200, valid loss-0.8040, acc-0.7876, test loss-0.7976, acc-0.7842\n",
      "Iter-2120, train loss-0.8682, acc-0.7500, valid loss-0.8010, acc-0.7886, test loss-0.7946, acc-0.7858\n",
      "Iter-2130, train loss-0.8087, acc-0.7900, valid loss-0.7984, acc-0.7886, test loss-0.7918, acc-0.7861\n",
      "Iter-2140, train loss-0.7331, acc-0.8000, valid loss-0.7956, acc-0.7890, test loss-0.7890, acc-0.7872\n",
      "Iter-2150, train loss-0.7252, acc-0.8000, valid loss-0.7929, acc-0.7908, test loss-0.7862, acc-0.7885\n",
      "Iter-2160, train loss-0.9416, acc-0.7300, valid loss-0.7899, acc-0.7920, test loss-0.7834, acc-0.7904\n",
      "Iter-2170, train loss-0.7558, acc-0.7500, valid loss-0.7872, acc-0.7928, test loss-0.7808, acc-0.7907\n",
      "Iter-2180, train loss-0.8758, acc-0.7500, valid loss-0.7848, acc-0.7932, test loss-0.7783, acc-0.7911\n",
      "Iter-2190, train loss-0.8860, acc-0.7600, valid loss-0.7823, acc-0.7938, test loss-0.7758, acc-0.7915\n",
      "Iter-2200, train loss-0.8977, acc-0.7300, valid loss-0.7797, acc-0.7936, test loss-0.7732, acc-0.7922\n",
      "Iter-2210, train loss-0.7456, acc-0.8400, valid loss-0.7770, acc-0.7942, test loss-0.7706, acc-0.7933\n",
      "Iter-2220, train loss-0.8781, acc-0.7400, valid loss-0.7747, acc-0.7946, test loss-0.7683, acc-0.7937\n",
      "Iter-2230, train loss-0.7336, acc-0.8200, valid loss-0.7722, acc-0.7952, test loss-0.7658, acc-0.7947\n",
      "Iter-2240, train loss-0.6981, acc-0.8200, valid loss-0.7696, acc-0.7954, test loss-0.7631, acc-0.7952\n",
      "Iter-2250, train loss-0.7481, acc-0.8100, valid loss-0.7669, acc-0.7970, test loss-0.7604, acc-0.7965\n",
      "Iter-2260, train loss-0.7094, acc-0.8200, valid loss-0.7644, acc-0.7982, test loss-0.7579, acc-0.7970\n",
      "Iter-2270, train loss-0.7172, acc-0.8600, valid loss-0.7620, acc-0.7978, test loss-0.7554, acc-0.7975\n",
      "Iter-2280, train loss-0.7857, acc-0.7900, valid loss-0.7596, acc-0.7978, test loss-0.7529, acc-0.7986\n",
      "Iter-2290, train loss-0.7449, acc-0.8000, valid loss-0.7571, acc-0.7980, test loss-0.7506, acc-0.7991\n",
      "Iter-2300, train loss-0.7562, acc-0.8100, valid loss-0.7547, acc-0.7994, test loss-0.7481, acc-0.8000\n",
      "Iter-2310, train loss-0.6767, acc-0.8200, valid loss-0.7523, acc-0.7990, test loss-0.7458, acc-0.8006\n",
      "Iter-2320, train loss-0.7128, acc-0.7800, valid loss-0.7501, acc-0.8006, test loss-0.7437, acc-0.8010\n",
      "Iter-2330, train loss-0.8121, acc-0.7800, valid loss-0.7478, acc-0.8008, test loss-0.7415, acc-0.8017\n",
      "Iter-2340, train loss-0.8067, acc-0.7300, valid loss-0.7455, acc-0.8014, test loss-0.7392, acc-0.8019\n",
      "Iter-2350, train loss-0.7400, acc-0.7600, valid loss-0.7431, acc-0.8020, test loss-0.7368, acc-0.8030\n",
      "Iter-2360, train loss-0.7182, acc-0.8200, valid loss-0.7407, acc-0.8026, test loss-0.7345, acc-0.8036\n",
      "Iter-2370, train loss-0.7837, acc-0.8000, valid loss-0.7386, acc-0.8034, test loss-0.7324, acc-0.8039\n",
      "Iter-2380, train loss-0.7657, acc-0.8000, valid loss-0.7361, acc-0.8054, test loss-0.7300, acc-0.8049\n",
      "Iter-2390, train loss-0.7180, acc-0.8000, valid loss-0.7338, acc-0.8058, test loss-0.7278, acc-0.8064\n",
      "Iter-2400, train loss-0.7644, acc-0.7700, valid loss-0.7315, acc-0.8066, test loss-0.7256, acc-0.8067\n",
      "Iter-2410, train loss-0.6728, acc-0.7700, valid loss-0.7293, acc-0.8076, test loss-0.7235, acc-0.8071\n",
      "Iter-2420, train loss-0.7464, acc-0.7800, valid loss-0.7273, acc-0.8078, test loss-0.7213, acc-0.8074\n",
      "Iter-2430, train loss-0.7339, acc-0.8600, valid loss-0.7253, acc-0.8086, test loss-0.7193, acc-0.8081\n",
      "Iter-2440, train loss-0.7366, acc-0.8300, valid loss-0.7230, acc-0.8104, test loss-0.7170, acc-0.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.6903, acc-0.8400, valid loss-0.7209, acc-0.8114, test loss-0.7150, acc-0.8098\n",
      "Iter-2460, train loss-0.6722, acc-0.8100, valid loss-0.7189, acc-0.8118, test loss-0.7128, acc-0.8103\n",
      "Iter-2470, train loss-0.7241, acc-0.8300, valid loss-0.7168, acc-0.8122, test loss-0.7108, acc-0.8111\n",
      "Iter-2480, train loss-0.6999, acc-0.8000, valid loss-0.7146, acc-0.8124, test loss-0.7084, acc-0.8127\n",
      "Iter-2490, train loss-0.6401, acc-0.8200, valid loss-0.7126, acc-0.8130, test loss-0.7064, acc-0.8129\n",
      "Iter-2500, train loss-0.6573, acc-0.8300, valid loss-0.7105, acc-0.8140, test loss-0.7044, acc-0.8140\n",
      "Iter-2510, train loss-0.9012, acc-0.7200, valid loss-0.7086, acc-0.8142, test loss-0.7026, acc-0.8144\n",
      "Iter-2520, train loss-0.6749, acc-0.8400, valid loss-0.7067, acc-0.8152, test loss-0.7008, acc-0.8150\n",
      "Iter-2530, train loss-0.8123, acc-0.7000, valid loss-0.7049, acc-0.8158, test loss-0.6989, acc-0.8159\n",
      "Iter-2540, train loss-0.6849, acc-0.8300, valid loss-0.7029, acc-0.8166, test loss-0.6969, acc-0.8160\n",
      "Iter-2550, train loss-0.6689, acc-0.8100, valid loss-0.7009, acc-0.8174, test loss-0.6949, acc-0.8166\n",
      "Iter-2560, train loss-0.7185, acc-0.7800, valid loss-0.6988, acc-0.8176, test loss-0.6929, acc-0.8180\n",
      "Iter-2570, train loss-0.8922, acc-0.7100, valid loss-0.6969, acc-0.8186, test loss-0.6911, acc-0.8187\n",
      "Iter-2580, train loss-0.6228, acc-0.8900, valid loss-0.6949, acc-0.8194, test loss-0.6891, acc-0.8191\n",
      "Iter-2590, train loss-0.6106, acc-0.8500, valid loss-0.6931, acc-0.8196, test loss-0.6874, acc-0.8191\n",
      "Iter-2600, train loss-0.7231, acc-0.8100, valid loss-0.6912, acc-0.8196, test loss-0.6856, acc-0.8206\n",
      "Iter-2610, train loss-0.7155, acc-0.8000, valid loss-0.6892, acc-0.8212, test loss-0.6836, acc-0.8217\n",
      "Iter-2620, train loss-0.7399, acc-0.8000, valid loss-0.6874, acc-0.8214, test loss-0.6818, acc-0.8224\n",
      "Iter-2630, train loss-0.7877, acc-0.7900, valid loss-0.6856, acc-0.8212, test loss-0.6800, acc-0.8235\n",
      "Iter-2640, train loss-0.7266, acc-0.7800, valid loss-0.6838, acc-0.8214, test loss-0.6782, acc-0.8241\n",
      "Iter-2650, train loss-0.8237, acc-0.7400, valid loss-0.6820, acc-0.8216, test loss-0.6763, acc-0.8246\n",
      "Iter-2660, train loss-0.6768, acc-0.7800, valid loss-0.6803, acc-0.8226, test loss-0.6746, acc-0.8255\n",
      "Iter-2670, train loss-0.6397, acc-0.8500, valid loss-0.6785, acc-0.8232, test loss-0.6729, acc-0.8271\n",
      "Iter-2680, train loss-0.7557, acc-0.7400, valid loss-0.6766, acc-0.8242, test loss-0.6710, acc-0.8278\n",
      "Iter-2690, train loss-0.8581, acc-0.7500, valid loss-0.6747, acc-0.8250, test loss-0.6692, acc-0.8284\n",
      "Iter-2700, train loss-0.6718, acc-0.8100, valid loss-0.6729, acc-0.8258, test loss-0.6675, acc-0.8289\n",
      "Iter-2710, train loss-0.6851, acc-0.8000, valid loss-0.6710, acc-0.8266, test loss-0.6656, acc-0.8299\n",
      "Iter-2720, train loss-0.6815, acc-0.8300, valid loss-0.6693, acc-0.8260, test loss-0.6639, acc-0.8302\n",
      "Iter-2730, train loss-0.5802, acc-0.8300, valid loss-0.6677, acc-0.8264, test loss-0.6624, acc-0.8309\n",
      "Iter-2740, train loss-0.6469, acc-0.8400, valid loss-0.6660, acc-0.8270, test loss-0.6607, acc-0.8314\n",
      "Iter-2750, train loss-0.7492, acc-0.8000, valid loss-0.6644, acc-0.8274, test loss-0.6591, acc-0.8320\n",
      "Iter-2760, train loss-0.7453, acc-0.8100, valid loss-0.6628, acc-0.8280, test loss-0.6575, acc-0.8322\n",
      "Iter-2770, train loss-0.6962, acc-0.8400, valid loss-0.6610, acc-0.8282, test loss-0.6557, acc-0.8338\n",
      "Iter-2780, train loss-0.5662, acc-0.9000, valid loss-0.6593, acc-0.8282, test loss-0.6540, acc-0.8334\n",
      "Iter-2790, train loss-0.6564, acc-0.8600, valid loss-0.6577, acc-0.8286, test loss-0.6525, acc-0.8333\n",
      "Iter-2800, train loss-0.7459, acc-0.7800, valid loss-0.6559, acc-0.8288, test loss-0.6508, acc-0.8336\n",
      "Iter-2810, train loss-0.5470, acc-0.9000, valid loss-0.6542, acc-0.8286, test loss-0.6493, acc-0.8343\n",
      "Iter-2820, train loss-0.6440, acc-0.7800, valid loss-0.6526, acc-0.8292, test loss-0.6476, acc-0.8347\n",
      "Iter-2830, train loss-0.6175, acc-0.8400, valid loss-0.6511, acc-0.8294, test loss-0.6462, acc-0.8353\n",
      "Iter-2840, train loss-0.5895, acc-0.8500, valid loss-0.6495, acc-0.8300, test loss-0.6447, acc-0.8357\n",
      "Iter-2850, train loss-0.6426, acc-0.8200, valid loss-0.6479, acc-0.8300, test loss-0.6432, acc-0.8362\n",
      "Iter-2860, train loss-0.7546, acc-0.7700, valid loss-0.6464, acc-0.8302, test loss-0.6417, acc-0.8365\n",
      "Iter-2870, train loss-0.5279, acc-0.8500, valid loss-0.6448, acc-0.8312, test loss-0.6401, acc-0.8370\n",
      "Iter-2880, train loss-0.5671, acc-0.8400, valid loss-0.6434, acc-0.8308, test loss-0.6387, acc-0.8372\n",
      "Iter-2890, train loss-0.5922, acc-0.8300, valid loss-0.6418, acc-0.8312, test loss-0.6371, acc-0.8372\n",
      "Iter-2900, train loss-0.6651, acc-0.8800, valid loss-0.6400, acc-0.8312, test loss-0.6354, acc-0.8374\n",
      "Iter-2910, train loss-0.6494, acc-0.8300, valid loss-0.6385, acc-0.8318, test loss-0.6339, acc-0.8378\n",
      "Iter-2920, train loss-0.6525, acc-0.8000, valid loss-0.6369, acc-0.8318, test loss-0.6323, acc-0.8381\n",
      "Iter-2930, train loss-0.6264, acc-0.7900, valid loss-0.6355, acc-0.8324, test loss-0.6309, acc-0.8382\n",
      "Iter-2940, train loss-0.6398, acc-0.8000, valid loss-0.6340, acc-0.8322, test loss-0.6295, acc-0.8384\n",
      "Iter-2950, train loss-0.5902, acc-0.8500, valid loss-0.6325, acc-0.8330, test loss-0.6280, acc-0.8393\n",
      "Iter-2960, train loss-0.6926, acc-0.8200, valid loss-0.6310, acc-0.8330, test loss-0.6265, acc-0.8399\n",
      "Iter-2970, train loss-0.6370, acc-0.7700, valid loss-0.6294, acc-0.8334, test loss-0.6250, acc-0.8402\n",
      "Iter-2980, train loss-0.5625, acc-0.9000, valid loss-0.6281, acc-0.8344, test loss-0.6238, acc-0.8407\n",
      "Iter-2990, train loss-0.7761, acc-0.8000, valid loss-0.6266, acc-0.8344, test loss-0.6225, acc-0.8410\n",
      "Iter-3000, train loss-0.6539, acc-0.8200, valid loss-0.6251, acc-0.8348, test loss-0.6210, acc-0.8420\n",
      "Iter-3010, train loss-0.6358, acc-0.8100, valid loss-0.6237, acc-0.8354, test loss-0.6198, acc-0.8422\n",
      "Iter-3020, train loss-0.5778, acc-0.8800, valid loss-0.6223, acc-0.8350, test loss-0.6184, acc-0.8426\n",
      "Iter-3030, train loss-0.7516, acc-0.8100, valid loss-0.6209, acc-0.8352, test loss-0.6171, acc-0.8423\n",
      "Iter-3040, train loss-0.6852, acc-0.7800, valid loss-0.6194, acc-0.8352, test loss-0.6158, acc-0.8427\n",
      "Iter-3050, train loss-0.6381, acc-0.8100, valid loss-0.6178, acc-0.8356, test loss-0.6143, acc-0.8435\n",
      "Iter-3060, train loss-0.7036, acc-0.7600, valid loss-0.6164, acc-0.8364, test loss-0.6130, acc-0.8437\n",
      "Iter-3070, train loss-0.4586, acc-0.9400, valid loss-0.6151, acc-0.8366, test loss-0.6117, acc-0.8440\n",
      "Iter-3080, train loss-0.5837, acc-0.8700, valid loss-0.6136, acc-0.8372, test loss-0.6103, acc-0.8447\n",
      "Iter-3090, train loss-0.6557, acc-0.8200, valid loss-0.6124, acc-0.8374, test loss-0.6091, acc-0.8448\n",
      "Iter-3100, train loss-0.6888, acc-0.8100, valid loss-0.6111, acc-0.8372, test loss-0.6077, acc-0.8448\n",
      "Iter-3110, train loss-0.6454, acc-0.8300, valid loss-0.6098, acc-0.8378, test loss-0.6065, acc-0.8452\n",
      "Iter-3120, train loss-0.5833, acc-0.8300, valid loss-0.6085, acc-0.8380, test loss-0.6052, acc-0.8458\n",
      "Iter-3130, train loss-0.7438, acc-0.7800, valid loss-0.6071, acc-0.8382, test loss-0.6039, acc-0.8459\n",
      "Iter-3140, train loss-0.6185, acc-0.8500, valid loss-0.6057, acc-0.8384, test loss-0.6026, acc-0.8468\n",
      "Iter-3150, train loss-0.5917, acc-0.8200, valid loss-0.6044, acc-0.8386, test loss-0.6013, acc-0.8467\n",
      "Iter-3160, train loss-0.5383, acc-0.8900, valid loss-0.6029, acc-0.8386, test loss-0.5999, acc-0.8472\n",
      "Iter-3170, train loss-0.6997, acc-0.8100, valid loss-0.6016, acc-0.8388, test loss-0.5986, acc-0.8477\n",
      "Iter-3180, train loss-0.5371, acc-0.8800, valid loss-0.6003, acc-0.8392, test loss-0.5973, acc-0.8482\n",
      "Iter-3190, train loss-0.6305, acc-0.8200, valid loss-0.5989, acc-0.8392, test loss-0.5959, acc-0.8485\n",
      "Iter-3200, train loss-0.5034, acc-0.8800, valid loss-0.5978, acc-0.8396, test loss-0.5949, acc-0.8488\n",
      "Iter-3210, train loss-0.6975, acc-0.8200, valid loss-0.5968, acc-0.8396, test loss-0.5939, acc-0.8489\n",
      "Iter-3220, train loss-0.7298, acc-0.8000, valid loss-0.5956, acc-0.8398, test loss-0.5926, acc-0.8497\n",
      "Iter-3230, train loss-0.6307, acc-0.8400, valid loss-0.5943, acc-0.8404, test loss-0.5914, acc-0.8495\n",
      "Iter-3240, train loss-0.6404, acc-0.8200, valid loss-0.5931, acc-0.8408, test loss-0.5903, acc-0.8497\n",
      "Iter-3250, train loss-0.6094, acc-0.8400, valid loss-0.5919, acc-0.8416, test loss-0.5891, acc-0.8498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.5974, acc-0.8200, valid loss-0.5906, acc-0.8416, test loss-0.5880, acc-0.8501\n",
      "Iter-3270, train loss-0.5416, acc-0.8800, valid loss-0.5894, acc-0.8422, test loss-0.5869, acc-0.8506\n",
      "Iter-3280, train loss-0.5529, acc-0.8700, valid loss-0.5883, acc-0.8424, test loss-0.5859, acc-0.8508\n",
      "Iter-3290, train loss-0.6616, acc-0.8600, valid loss-0.5871, acc-0.8426, test loss-0.5847, acc-0.8515\n",
      "Iter-3300, train loss-0.5568, acc-0.8500, valid loss-0.5859, acc-0.8430, test loss-0.5834, acc-0.8517\n",
      "Iter-3310, train loss-0.5868, acc-0.8300, valid loss-0.5847, acc-0.8430, test loss-0.5823, acc-0.8520\n",
      "Iter-3320, train loss-0.4824, acc-0.8700, valid loss-0.5835, acc-0.8434, test loss-0.5811, acc-0.8522\n",
      "Iter-3330, train loss-0.6975, acc-0.8200, valid loss-0.5823, acc-0.8440, test loss-0.5800, acc-0.8520\n",
      "Iter-3340, train loss-0.5013, acc-0.8700, valid loss-0.5811, acc-0.8446, test loss-0.5789, acc-0.8517\n",
      "Iter-3350, train loss-0.5979, acc-0.8300, valid loss-0.5800, acc-0.8448, test loss-0.5777, acc-0.8525\n",
      "Iter-3360, train loss-0.5872, acc-0.8400, valid loss-0.5789, acc-0.8446, test loss-0.5766, acc-0.8522\n",
      "Iter-3370, train loss-0.6000, acc-0.8500, valid loss-0.5778, acc-0.8446, test loss-0.5756, acc-0.8533\n",
      "Iter-3380, train loss-0.7260, acc-0.8400, valid loss-0.5765, acc-0.8452, test loss-0.5744, acc-0.8536\n",
      "Iter-3390, train loss-0.5508, acc-0.8600, valid loss-0.5755, acc-0.8450, test loss-0.5735, acc-0.8536\n",
      "Iter-3400, train loss-0.6373, acc-0.8100, valid loss-0.5745, acc-0.8454, test loss-0.5725, acc-0.8542\n",
      "Iter-3410, train loss-0.5554, acc-0.8400, valid loss-0.5734, acc-0.8460, test loss-0.5714, acc-0.8537\n",
      "Iter-3420, train loss-0.5778, acc-0.8300, valid loss-0.5724, acc-0.8458, test loss-0.5704, acc-0.8541\n",
      "Iter-3430, train loss-0.5038, acc-0.9000, valid loss-0.5711, acc-0.8456, test loss-0.5692, acc-0.8541\n",
      "Iter-3440, train loss-0.6632, acc-0.8400, valid loss-0.5701, acc-0.8466, test loss-0.5683, acc-0.8543\n",
      "Iter-3450, train loss-0.4622, acc-0.9000, valid loss-0.5690, acc-0.8464, test loss-0.5672, acc-0.8541\n",
      "Iter-3460, train loss-0.5671, acc-0.8600, valid loss-0.5681, acc-0.8468, test loss-0.5663, acc-0.8542\n",
      "Iter-3470, train loss-0.5275, acc-0.8800, valid loss-0.5672, acc-0.8476, test loss-0.5654, acc-0.8550\n",
      "Iter-3480, train loss-0.5724, acc-0.8600, valid loss-0.5661, acc-0.8476, test loss-0.5645, acc-0.8555\n",
      "Iter-3490, train loss-0.7190, acc-0.7300, valid loss-0.5650, acc-0.8476, test loss-0.5635, acc-0.8556\n",
      "Iter-3500, train loss-0.6419, acc-0.8100, valid loss-0.5640, acc-0.8480, test loss-0.5625, acc-0.8557\n",
      "Iter-3510, train loss-0.5913, acc-0.8300, valid loss-0.5628, acc-0.8482, test loss-0.5615, acc-0.8556\n",
      "Iter-3520, train loss-0.5908, acc-0.8200, valid loss-0.5618, acc-0.8484, test loss-0.5605, acc-0.8563\n",
      "Iter-3530, train loss-0.6208, acc-0.8500, valid loss-0.5608, acc-0.8484, test loss-0.5595, acc-0.8562\n",
      "Iter-3540, train loss-0.5473, acc-0.8500, valid loss-0.5599, acc-0.8486, test loss-0.5586, acc-0.8565\n",
      "Iter-3550, train loss-0.6323, acc-0.8000, valid loss-0.5589, acc-0.8484, test loss-0.5575, acc-0.8563\n",
      "Iter-3560, train loss-0.5845, acc-0.8300, valid loss-0.5579, acc-0.8486, test loss-0.5566, acc-0.8569\n",
      "Iter-3570, train loss-0.5465, acc-0.8600, valid loss-0.5569, acc-0.8488, test loss-0.5557, acc-0.8570\n",
      "Iter-3580, train loss-0.5000, acc-0.8600, valid loss-0.5560, acc-0.8488, test loss-0.5548, acc-0.8569\n",
      "Iter-3590, train loss-0.5249, acc-0.8500, valid loss-0.5549, acc-0.8492, test loss-0.5538, acc-0.8573\n",
      "Iter-3600, train loss-0.6279, acc-0.8400, valid loss-0.5538, acc-0.8496, test loss-0.5528, acc-0.8572\n",
      "Iter-3610, train loss-0.4803, acc-0.8600, valid loss-0.5529, acc-0.8496, test loss-0.5519, acc-0.8575\n",
      "Iter-3620, train loss-0.5361, acc-0.8400, valid loss-0.5520, acc-0.8498, test loss-0.5510, acc-0.8575\n",
      "Iter-3630, train loss-0.4876, acc-0.9200, valid loss-0.5511, acc-0.8500, test loss-0.5502, acc-0.8578\n",
      "Iter-3640, train loss-0.5492, acc-0.8200, valid loss-0.5502, acc-0.8504, test loss-0.5491, acc-0.8578\n",
      "Iter-3650, train loss-0.5500, acc-0.8600, valid loss-0.5494, acc-0.8506, test loss-0.5483, acc-0.8585\n",
      "Iter-3660, train loss-0.4503, acc-0.9100, valid loss-0.5484, acc-0.8506, test loss-0.5474, acc-0.8589\n",
      "Iter-3670, train loss-0.5923, acc-0.8200, valid loss-0.5476, acc-0.8504, test loss-0.5467, acc-0.8593\n",
      "Iter-3680, train loss-0.5509, acc-0.8400, valid loss-0.5466, acc-0.8504, test loss-0.5457, acc-0.8594\n",
      "Iter-3690, train loss-0.5322, acc-0.8500, valid loss-0.5458, acc-0.8508, test loss-0.5448, acc-0.8596\n",
      "Iter-3700, train loss-0.6090, acc-0.8500, valid loss-0.5449, acc-0.8506, test loss-0.5441, acc-0.8600\n",
      "Iter-3710, train loss-0.6391, acc-0.8000, valid loss-0.5441, acc-0.8512, test loss-0.5432, acc-0.8600\n",
      "Iter-3720, train loss-0.6687, acc-0.8200, valid loss-0.5432, acc-0.8512, test loss-0.5425, acc-0.8600\n",
      "Iter-3730, train loss-0.4425, acc-0.8800, valid loss-0.5424, acc-0.8514, test loss-0.5416, acc-0.8604\n",
      "Iter-3740, train loss-0.5515, acc-0.8100, valid loss-0.5416, acc-0.8520, test loss-0.5408, acc-0.8610\n",
      "Iter-3750, train loss-0.6137, acc-0.7900, valid loss-0.5406, acc-0.8512, test loss-0.5400, acc-0.8612\n",
      "Iter-3760, train loss-0.4704, acc-0.8400, valid loss-0.5398, acc-0.8518, test loss-0.5390, acc-0.8611\n",
      "Iter-3770, train loss-0.6226, acc-0.8000, valid loss-0.5389, acc-0.8522, test loss-0.5381, acc-0.8611\n",
      "Iter-3780, train loss-0.4920, acc-0.9000, valid loss-0.5380, acc-0.8524, test loss-0.5373, acc-0.8614\n",
      "Iter-3790, train loss-0.4700, acc-0.8600, valid loss-0.5370, acc-0.8528, test loss-0.5364, acc-0.8616\n",
      "Iter-3800, train loss-0.5376, acc-0.8200, valid loss-0.5361, acc-0.8532, test loss-0.5355, acc-0.8622\n",
      "Iter-3810, train loss-0.5241, acc-0.8500, valid loss-0.5351, acc-0.8538, test loss-0.5347, acc-0.8621\n",
      "Iter-3820, train loss-0.6921, acc-0.8100, valid loss-0.5343, acc-0.8540, test loss-0.5339, acc-0.8622\n",
      "Iter-3830, train loss-0.5673, acc-0.8000, valid loss-0.5333, acc-0.8546, test loss-0.5330, acc-0.8623\n",
      "Iter-3840, train loss-0.4738, acc-0.8700, valid loss-0.5325, acc-0.8544, test loss-0.5322, acc-0.8624\n",
      "Iter-3850, train loss-0.4163, acc-0.8900, valid loss-0.5316, acc-0.8546, test loss-0.5314, acc-0.8627\n",
      "Iter-3860, train loss-0.4482, acc-0.8700, valid loss-0.5308, acc-0.8552, test loss-0.5305, acc-0.8623\n",
      "Iter-3870, train loss-0.5129, acc-0.8400, valid loss-0.5298, acc-0.8556, test loss-0.5297, acc-0.8627\n",
      "Iter-3880, train loss-0.5608, acc-0.8000, valid loss-0.5289, acc-0.8560, test loss-0.5289, acc-0.8630\n",
      "Iter-3890, train loss-0.5224, acc-0.8900, valid loss-0.5281, acc-0.8568, test loss-0.5281, acc-0.8626\n",
      "Iter-3900, train loss-0.5745, acc-0.8200, valid loss-0.5274, acc-0.8566, test loss-0.5274, acc-0.8629\n",
      "Iter-3910, train loss-0.4821, acc-0.8700, valid loss-0.5266, acc-0.8570, test loss-0.5266, acc-0.8632\n",
      "Iter-3920, train loss-0.4758, acc-0.8600, valid loss-0.5258, acc-0.8568, test loss-0.5259, acc-0.8631\n",
      "Iter-3930, train loss-0.5201, acc-0.8600, valid loss-0.5251, acc-0.8570, test loss-0.5252, acc-0.8633\n",
      "Iter-3940, train loss-0.5221, acc-0.8700, valid loss-0.5243, acc-0.8576, test loss-0.5245, acc-0.8634\n",
      "Iter-3950, train loss-0.6086, acc-0.8700, valid loss-0.5236, acc-0.8566, test loss-0.5238, acc-0.8635\n",
      "Iter-3960, train loss-0.3641, acc-0.9400, valid loss-0.5227, acc-0.8568, test loss-0.5230, acc-0.8635\n",
      "Iter-3970, train loss-0.7456, acc-0.7800, valid loss-0.5219, acc-0.8572, test loss-0.5222, acc-0.8630\n",
      "Iter-3980, train loss-0.6059, acc-0.8300, valid loss-0.5211, acc-0.8576, test loss-0.5215, acc-0.8633\n",
      "Iter-3990, train loss-0.5864, acc-0.8500, valid loss-0.5203, acc-0.8578, test loss-0.5208, acc-0.8639\n",
      "Iter-4000, train loss-0.3978, acc-0.8900, valid loss-0.5195, acc-0.8586, test loss-0.5201, acc-0.8636\n",
      "Iter-4010, train loss-0.4679, acc-0.8600, valid loss-0.5188, acc-0.8592, test loss-0.5194, acc-0.8638\n",
      "Iter-4020, train loss-0.5398, acc-0.8600, valid loss-0.5181, acc-0.8594, test loss-0.5187, acc-0.8637\n",
      "Iter-4030, train loss-0.4792, acc-0.8700, valid loss-0.5173, acc-0.8588, test loss-0.5179, acc-0.8638\n",
      "Iter-4040, train loss-0.6080, acc-0.8500, valid loss-0.5166, acc-0.8596, test loss-0.5172, acc-0.8644\n",
      "Iter-4050, train loss-0.6276, acc-0.8300, valid loss-0.5158, acc-0.8602, test loss-0.5166, acc-0.8644\n",
      "Iter-4060, train loss-0.4555, acc-0.8800, valid loss-0.5151, acc-0.8598, test loss-0.5160, acc-0.8648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.5517, acc-0.8600, valid loss-0.5145, acc-0.8598, test loss-0.5153, acc-0.8649\n",
      "Iter-4080, train loss-0.6130, acc-0.8500, valid loss-0.5137, acc-0.8602, test loss-0.5146, acc-0.8652\n",
      "Iter-4090, train loss-0.6377, acc-0.7900, valid loss-0.5129, acc-0.8598, test loss-0.5138, acc-0.8650\n",
      "Iter-4100, train loss-0.4868, acc-0.8800, valid loss-0.5123, acc-0.8598, test loss-0.5132, acc-0.8654\n",
      "Iter-4110, train loss-0.6415, acc-0.8200, valid loss-0.5116, acc-0.8600, test loss-0.5126, acc-0.8660\n",
      "Iter-4120, train loss-0.6321, acc-0.8200, valid loss-0.5109, acc-0.8596, test loss-0.5120, acc-0.8660\n",
      "Iter-4130, train loss-0.5216, acc-0.8200, valid loss-0.5102, acc-0.8596, test loss-0.5113, acc-0.8661\n",
      "Iter-4140, train loss-0.4447, acc-0.8700, valid loss-0.5096, acc-0.8590, test loss-0.5107, acc-0.8662\n",
      "Iter-4150, train loss-0.6836, acc-0.8000, valid loss-0.5088, acc-0.8596, test loss-0.5100, acc-0.8662\n",
      "Iter-4160, train loss-0.5607, acc-0.8300, valid loss-0.5080, acc-0.8604, test loss-0.5094, acc-0.8661\n",
      "Iter-4170, train loss-0.6437, acc-0.8600, valid loss-0.5073, acc-0.8610, test loss-0.5089, acc-0.8666\n",
      "Iter-4180, train loss-0.5742, acc-0.8400, valid loss-0.5066, acc-0.8606, test loss-0.5082, acc-0.8666\n",
      "Iter-4190, train loss-0.5210, acc-0.8700, valid loss-0.5059, acc-0.8614, test loss-0.5076, acc-0.8672\n",
      "Iter-4200, train loss-0.5646, acc-0.8500, valid loss-0.5052, acc-0.8618, test loss-0.5069, acc-0.8676\n",
      "Iter-4210, train loss-0.5911, acc-0.8300, valid loss-0.5046, acc-0.8620, test loss-0.5062, acc-0.8683\n",
      "Iter-4220, train loss-0.4830, acc-0.8800, valid loss-0.5039, acc-0.8620, test loss-0.5056, acc-0.8681\n",
      "Iter-4230, train loss-0.5733, acc-0.8100, valid loss-0.5032, acc-0.8626, test loss-0.5050, acc-0.8683\n",
      "Iter-4240, train loss-0.5837, acc-0.8500, valid loss-0.5025, acc-0.8626, test loss-0.5043, acc-0.8682\n",
      "Iter-4250, train loss-0.5698, acc-0.8100, valid loss-0.5019, acc-0.8634, test loss-0.5036, acc-0.8688\n",
      "Iter-4260, train loss-0.4039, acc-0.9100, valid loss-0.5012, acc-0.8632, test loss-0.5032, acc-0.8688\n",
      "Iter-4270, train loss-0.4454, acc-0.8800, valid loss-0.5005, acc-0.8628, test loss-0.5026, acc-0.8689\n",
      "Iter-4280, train loss-0.4447, acc-0.8600, valid loss-0.4999, acc-0.8632, test loss-0.5020, acc-0.8694\n",
      "Iter-4290, train loss-0.4977, acc-0.8400, valid loss-0.4992, acc-0.8632, test loss-0.5013, acc-0.8692\n",
      "Iter-4300, train loss-0.4346, acc-0.8700, valid loss-0.4986, acc-0.8634, test loss-0.5007, acc-0.8693\n",
      "Iter-4310, train loss-0.4888, acc-0.9000, valid loss-0.4979, acc-0.8628, test loss-0.5001, acc-0.8698\n",
      "Iter-4320, train loss-0.4488, acc-0.8700, valid loss-0.4973, acc-0.8632, test loss-0.4995, acc-0.8700\n",
      "Iter-4330, train loss-0.5478, acc-0.8200, valid loss-0.4967, acc-0.8638, test loss-0.4990, acc-0.8704\n",
      "Iter-4340, train loss-0.4502, acc-0.9000, valid loss-0.4960, acc-0.8638, test loss-0.4983, acc-0.8703\n",
      "Iter-4350, train loss-0.4791, acc-0.8700, valid loss-0.4955, acc-0.8646, test loss-0.4977, acc-0.8707\n",
      "Iter-4360, train loss-0.7337, acc-0.7300, valid loss-0.4948, acc-0.8646, test loss-0.4971, acc-0.8709\n",
      "Iter-4370, train loss-0.4001, acc-0.9000, valid loss-0.4942, acc-0.8644, test loss-0.4967, acc-0.8712\n",
      "Iter-4380, train loss-0.6310, acc-0.8000, valid loss-0.4936, acc-0.8646, test loss-0.4961, acc-0.8711\n",
      "Iter-4390, train loss-0.5127, acc-0.8500, valid loss-0.4929, acc-0.8652, test loss-0.4954, acc-0.8711\n",
      "Iter-4400, train loss-0.4457, acc-0.8700, valid loss-0.4924, acc-0.8644, test loss-0.4950, acc-0.8709\n",
      "Iter-4410, train loss-0.4685, acc-0.8800, valid loss-0.4920, acc-0.8650, test loss-0.4946, acc-0.8713\n",
      "Iter-4420, train loss-0.4941, acc-0.8500, valid loss-0.4915, acc-0.8656, test loss-0.4941, acc-0.8712\n",
      "Iter-4430, train loss-0.5236, acc-0.8800, valid loss-0.4907, acc-0.8658, test loss-0.4935, acc-0.8711\n",
      "Iter-4440, train loss-0.4209, acc-0.9100, valid loss-0.4902, acc-0.8654, test loss-0.4929, acc-0.8711\n",
      "Iter-4450, train loss-0.5076, acc-0.8300, valid loss-0.4896, acc-0.8654, test loss-0.4923, acc-0.8711\n",
      "Iter-4460, train loss-0.5242, acc-0.8300, valid loss-0.4891, acc-0.8658, test loss-0.4918, acc-0.8714\n",
      "Iter-4470, train loss-0.6321, acc-0.8500, valid loss-0.4885, acc-0.8656, test loss-0.4912, acc-0.8714\n",
      "Iter-4480, train loss-0.5721, acc-0.8300, valid loss-0.4879, acc-0.8654, test loss-0.4906, acc-0.8716\n",
      "Iter-4490, train loss-0.4294, acc-0.8700, valid loss-0.4873, acc-0.8658, test loss-0.4900, acc-0.8718\n",
      "Iter-4500, train loss-0.5255, acc-0.8400, valid loss-0.4868, acc-0.8654, test loss-0.4894, acc-0.8718\n",
      "Iter-4510, train loss-0.5505, acc-0.8000, valid loss-0.4862, acc-0.8656, test loss-0.4888, acc-0.8718\n",
      "Iter-4520, train loss-0.5027, acc-0.8500, valid loss-0.4857, acc-0.8658, test loss-0.4883, acc-0.8719\n",
      "Iter-4530, train loss-0.6536, acc-0.8000, valid loss-0.4852, acc-0.8662, test loss-0.4878, acc-0.8720\n",
      "Iter-4540, train loss-0.5951, acc-0.8200, valid loss-0.4846, acc-0.8662, test loss-0.4873, acc-0.8722\n",
      "Iter-4550, train loss-0.5352, acc-0.8600, valid loss-0.4840, acc-0.8658, test loss-0.4868, acc-0.8720\n",
      "Iter-4560, train loss-0.4641, acc-0.8600, valid loss-0.4834, acc-0.8668, test loss-0.4863, acc-0.8723\n",
      "Iter-4570, train loss-0.5246, acc-0.8800, valid loss-0.4829, acc-0.8670, test loss-0.4858, acc-0.8724\n",
      "Iter-4580, train loss-0.3852, acc-0.9000, valid loss-0.4823, acc-0.8666, test loss-0.4852, acc-0.8723\n",
      "Iter-4590, train loss-0.3350, acc-0.9300, valid loss-0.4819, acc-0.8666, test loss-0.4846, acc-0.8723\n",
      "Iter-4600, train loss-0.5281, acc-0.8900, valid loss-0.4813, acc-0.8668, test loss-0.4840, acc-0.8724\n",
      "Iter-4610, train loss-0.5299, acc-0.8200, valid loss-0.4807, acc-0.8666, test loss-0.4835, acc-0.8726\n",
      "Iter-4620, train loss-0.4365, acc-0.8700, valid loss-0.4801, acc-0.8672, test loss-0.4830, acc-0.8725\n",
      "Iter-4630, train loss-0.5726, acc-0.8400, valid loss-0.4797, acc-0.8672, test loss-0.4826, acc-0.8727\n",
      "Iter-4640, train loss-0.6171, acc-0.8400, valid loss-0.4792, acc-0.8668, test loss-0.4820, acc-0.8727\n",
      "Iter-4650, train loss-0.4422, acc-0.8800, valid loss-0.4787, acc-0.8668, test loss-0.4815, acc-0.8726\n",
      "Iter-4660, train loss-0.5196, acc-0.8600, valid loss-0.4782, acc-0.8672, test loss-0.4810, acc-0.8729\n",
      "Iter-4670, train loss-0.5022, acc-0.8500, valid loss-0.4776, acc-0.8676, test loss-0.4805, acc-0.8728\n",
      "Iter-4680, train loss-0.5195, acc-0.8900, valid loss-0.4771, acc-0.8676, test loss-0.4801, acc-0.8731\n",
      "Iter-4690, train loss-0.5012, acc-0.8100, valid loss-0.4765, acc-0.8676, test loss-0.4795, acc-0.8730\n",
      "Iter-4700, train loss-0.5626, acc-0.8700, valid loss-0.4760, acc-0.8680, test loss-0.4791, acc-0.8732\n",
      "Iter-4710, train loss-0.4690, acc-0.8500, valid loss-0.4755, acc-0.8686, test loss-0.4787, acc-0.8736\n",
      "Iter-4720, train loss-0.5176, acc-0.8300, valid loss-0.4751, acc-0.8688, test loss-0.4782, acc-0.8735\n",
      "Iter-4730, train loss-0.4455, acc-0.8700, valid loss-0.4746, acc-0.8690, test loss-0.4778, acc-0.8738\n",
      "Iter-4740, train loss-0.4918, acc-0.8600, valid loss-0.4740, acc-0.8686, test loss-0.4773, acc-0.8737\n",
      "Iter-4750, train loss-0.5499, acc-0.8300, valid loss-0.4736, acc-0.8690, test loss-0.4769, acc-0.8739\n",
      "Iter-4760, train loss-0.4650, acc-0.9000, valid loss-0.4731, acc-0.8692, test loss-0.4764, acc-0.8740\n",
      "Iter-4770, train loss-0.5340, acc-0.8500, valid loss-0.4726, acc-0.8690, test loss-0.4758, acc-0.8741\n",
      "Iter-4780, train loss-0.4263, acc-0.8800, valid loss-0.4720, acc-0.8692, test loss-0.4753, acc-0.8743\n",
      "Iter-4790, train loss-0.4385, acc-0.8800, valid loss-0.4714, acc-0.8690, test loss-0.4747, acc-0.8745\n",
      "Iter-4800, train loss-0.6945, acc-0.8200, valid loss-0.4710, acc-0.8692, test loss-0.4743, acc-0.8738\n",
      "Iter-4810, train loss-0.5968, acc-0.8300, valid loss-0.4705, acc-0.8694, test loss-0.4738, acc-0.8738\n",
      "Iter-4820, train loss-0.4238, acc-0.8800, valid loss-0.4700, acc-0.8692, test loss-0.4733, acc-0.8744\n",
      "Iter-4830, train loss-0.4541, acc-0.8800, valid loss-0.4695, acc-0.8694, test loss-0.4729, acc-0.8744\n",
      "Iter-4840, train loss-0.5638, acc-0.8300, valid loss-0.4690, acc-0.8696, test loss-0.4725, acc-0.8745\n",
      "Iter-4850, train loss-0.5260, acc-0.8700, valid loss-0.4685, acc-0.8698, test loss-0.4721, acc-0.8746\n",
      "Iter-4860, train loss-0.5891, acc-0.8300, valid loss-0.4680, acc-0.8698, test loss-0.4716, acc-0.8743\n",
      "Iter-4870, train loss-0.5278, acc-0.8400, valid loss-0.4675, acc-0.8698, test loss-0.4712, acc-0.8744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.4493, acc-0.8800, valid loss-0.4670, acc-0.8702, test loss-0.4707, acc-0.8743\n",
      "Iter-4890, train loss-0.4163, acc-0.8900, valid loss-0.4665, acc-0.8696, test loss-0.4703, acc-0.8745\n",
      "Iter-4900, train loss-0.4830, acc-0.8700, valid loss-0.4660, acc-0.8706, test loss-0.4698, acc-0.8746\n",
      "Iter-4910, train loss-0.3893, acc-0.8800, valid loss-0.4656, acc-0.8706, test loss-0.4695, acc-0.8749\n",
      "Iter-4920, train loss-0.3878, acc-0.8900, valid loss-0.4652, acc-0.8710, test loss-0.4690, acc-0.8747\n",
      "Iter-4930, train loss-0.4989, acc-0.8500, valid loss-0.4648, acc-0.8712, test loss-0.4686, acc-0.8754\n",
      "Iter-4940, train loss-0.6983, acc-0.7700, valid loss-0.4644, acc-0.8710, test loss-0.4682, acc-0.8751\n",
      "Iter-4950, train loss-0.5467, acc-0.8700, valid loss-0.4640, acc-0.8708, test loss-0.4678, acc-0.8756\n",
      "Iter-4960, train loss-0.4526, acc-0.8700, valid loss-0.4635, acc-0.8708, test loss-0.4673, acc-0.8757\n",
      "Iter-4970, train loss-0.4980, acc-0.8900, valid loss-0.4631, acc-0.8706, test loss-0.4670, acc-0.8758\n",
      "Iter-4980, train loss-0.5149, acc-0.8500, valid loss-0.4626, acc-0.8712, test loss-0.4665, acc-0.8757\n",
      "Iter-4990, train loss-0.5506, acc-0.8100, valid loss-0.4621, acc-0.8714, test loss-0.4661, acc-0.8755\n",
      "Iter-5000, train loss-0.4424, acc-0.8600, valid loss-0.4616, acc-0.8718, test loss-0.4657, acc-0.8758\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 5000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ea7470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FNX6wPHvSSC0kEYNLQESEQUEpIpgABUEFBAQ4Uez\nYgHFa1e4gHq9yrVfRfGCiqBio1kQLAREpKmAIEjvvYWehOT9/TG72ZLdZBM2m7Lv53n22ZkzZ2bO\nTpJ3T86cOceICEoppYJDSGEXQCmlVOBo0FdKqSCiQV8ppYKIBn2llAoiGvSVUiqIaNBXSqkgkmvQ\nN8ZMMcYcNMaszSHPG8aYzcaY1caYpv4tolJKKX/xpab/PtDF20ZjzA1AfRFJBIYD7/ipbEoppfws\n16AvIkuA4zlk6Ql8aMu7HIg0xlTzT/GUUkr5kz/a9GsCu53W99rSlFJKFTF6I1cppYJIKT8cYy9Q\n22m9li0tG2OMDvSjlFL5ICLGH8fxtaZvbC9P5gJDAIwxbYATInLQ24FERF8ijB07ttDLUFReei30\nWui1yPnlT7nW9I0xHwNJQCVjzC5gLBBmxW95V0S+NcZ0M8ZsAc4At/m1hEoppfwm16AvIgN9yDPC\nP8VRSilVkPRGbiFJSkoq7CIUGXotHPRaOOi1KBjG3+1FOZ7MGAnk+ZRSqiQwxiB+upHrj947Sql8\nio+PZ+fOnYVdDFVExMXFsWPHjgI9h9b0lSpEthpcYRdDFRHefh/8WdPXNn2llAoiGvSVUiqIaNBX\nSqkgokFfKVXgMjMzqVixInv27CnsogQ9DfpKqWwqVqxIREQEERERhIaGUr58+ay0Tz75JM/HCwkJ\n4dSpU9SqVasASluy/PFHwR5fe+8oVYiKQ++devXqMWXKFDp27Og1T0ZGBqGhoQEsVeAE8rMZYwDB\n/VdCe+8opQLG06BfY8aM4dZbb2XgwIFERkby0UcfsWzZMtq2bUt0dDQ1a9bkwQcfJCMjA7ACZ0hI\nCLt27QJg8ODBPPjgg3Tr1o2IiAjatWvn9XkFEaFfv37ExsYSExNDp06d2LhxY9b2c+fO8dBDDxEX\nF0d0dDRJSUmkp6cDsHjxYtq2bUtUVBRxcXF89NFHALRv354PP/ww6xjOX2r2sr799tskJibSsGFD\nAEaOHEnt2rWJioqidevW/Prrr1n7Z2Rk8Oyzz5KQkEBkZCStWrXiwIED3HPPPTzxxBMun6d79+68\n9dZbef9B+IkGfaVUvsyePZtBgwaRkpJC//79KV26NG+88QbHjh3jl19+Yf78+UyaNCkrv1WLdfjk\nk0/417/+xfHjx6lduzZjxozxeq4bb7yRrVu3cuDAARo1asTgwYOzto0aNYp169axcuVKjh07xvPP\nP09ISAjbt2+ne/fuPPLIIxw7dow//viDxo0bez2He/m++uorVq1axZ9//glAmzZtWLduHceOHaNv\n377069cv68tlwoQJzJw5kwULFpCSksLkyZMpW7YsQ4cOZcaMGVnHPHToEIsWLWLgwFyHNCs4AR4e\nVJRSDrn9TYB/XhcjPj5efvzxR5e00aNHS+fOnXPc76WXXpJbbrlFREQuXLggxhjZuXOniIgMGjRI\n7r333qy8c+fOlcaNG/tUnsOHD4sxRs6ePSsZGRlSpkwZ2bBhQ7Z8zz77bNb53V199dUyderUrPXJ\nkydLx44dXcq6ZMkSr2XIzMyUihUryl9//SUiIvXr15d58+Z5zNugQQNJTk4WEZHXXntNevbs6fW4\ngMefl+33xC9xWGv6ShVh/gr7BaF27dou63///Tc9evQgNjaWyMhIxo4dy5EjR7zuX7169azl8uXL\nc/r0aY/5MjMzeeyxx6hfvz5RUVEkJiZijOHIkSMcPHiQ9PR06tWrl22/3bt3U79+/Xx+OrLddJ4w\nYQINGzYkOjqamJgYzp49m/X5du/e7bEMYDVlTZ8+HYDp06e7/JeSm6ZNYfjwfH4ALzToK6Xyxb05\nZPjw4TRu3Jht27aRkpLC+PHj/XKT+sMPP+S7774jOTmZEydOsGXLlqxaa7Vq1QgLC2Pr1q3Z9qtd\nuzZbtmzxeMwKFSpw9uzZrPUDBw5ky+P8+ZKTk3n11VeZNWsWx48f5/jx41SoUCHr89WpU8djGcAK\n+rNmzWL16tVs27aNG2+80efPvmYN/Pijz9l9okFfKeD55+Hf/4Yvv4ShQ33bZ9gw+OKL7OnDh8PH\nHzvWR4yAqVP9Uswi7dSpU0RGRlKuXDk2bNjg0p5/scctU6YM0dHRnDlzhqeeeiorIIeEhDBs2DBG\njRrFwYMHyczMZOnSpWRkZDBo0CDmz5/PrFmzyMjI4OjRo6xduxaApk2b8uWXX3L+/Hk2bdrEe++9\nl2sZSpcuTUxMDGlpaYwdO9blS+OOO+5g9OjRbNu2DYA1a9Zw4sQJwPpCaNKkCUOHDqVfv36EhYXl\n+plTUhzL/v5PTYO+UsDTT8NTT8HkyeDUqSNHU6fClCnZ0999FyZOdKy/9Ra88YZ/ylkY3Gv03rz8\n8st88MEHREREcO+993Lrrbd6PY6vxwS47bbbiI2NpUaNGjRu3Jirr77aZfsrr7xCw4YNufLKK6lU\nqRJPP/00IkJ8fDxfffUVL7zwAjExMVx55ZWsW7cOgEceeQSAatWqceedd2ZrcnEvX7du3ejcuTOJ\niYnUq1ePqKgoYmNjs7Y/+uij9OrVi86dOxMZGcnw4cM5f/581vahQ4eybt06hgwZ4tNn3r3b58uT\nd/66OeDLC72Rq4qYAwdEJk1ytH7fcEPuNz4nThQ5csTKd8klIg0bOrYdPuxInzjRSgORZs1Ezp4V\n+c9/RLZuFZk+3b5N/yaCwU8//ST16tXLNR+2G7lr1tjXRerV0xu5SvnN5MmuN8p8qYDed5+j+WbT\nJtiwwbFt2jRH+n33ue63YgU8+iiMHw+DBl1cuVXxkZaWxuuvv87dd9/t8z62xxsAbd5RyicHD8L+\n/dYfzOrV3vO5B/kLFxzLO3aArVk2mzNnPKevWeO6br+PaIzjXDpnSvBYt24dMTExnDhxgpEjR/q8\n38yZ8Ntv1rK/g74Ow6BKpFq14ORJmDMHOnXy/ofz/PNWe747EStId+0K8+a5bvP034AInD4NFSt6\nPk+zZvDaa3DNNe7nKPrDMKjAsQ/D4CwuDnbu1OkSVRGWlgalSkFIIf4fefiwVY6TJy/uOMePO5bP\nnYPMTM/50tO9bwNITQWn+3pKFRpt3lF+Fx4ODz9c2KWw9Orln+MsWADly1ufzZPExJz/Df/rL+jS\nxT9lUSVbJY4Qz3auYDXN+Y26aX/79fha01d+l54Otu7QRV5uN27tgTy3YeB37iy4J19VcNlFbQ6b\nGFJCKpIRmkGNI0epnvtuPtOgrwpEIAPggQMQG+t6zrQ0z3n37IHataFBA9i40bfeOr52KU9I8C2f\n3U035S2/Cg4VHo6GjFKQeQHCD4CEwL/9d3wN+qrYs43W65PNm633v338jzkvX15Hj/qeF+Crr/KW\nXwWJl/d5SPTLPVxA2/SVn+zYAWPH+udYP/4ITz5pDYvg7NNP4Ztvsudfv956f/tt+Oc/rbK4W7wY\nXnkF7r/fkTZ6NHz+uecy2M+9cmWei6+AnTt3EhISQqbt7na3bt2YZn+IIZe8qoD56ykvX17o04cl\n1vjxjidZQSQpKf/Hql/f8YSsMxCJisqev0kT1zEl//Wv7ONMNm+e13EpA/Uqmn8TXbt2lbFjx2ZL\nnz17tlSvXl0yMjJy3H/Hjh0SEhKSa7685i3psD2R6+X3RJ/IVcXLgQOOp1eTk60uji+/DKdO5b5v\ncrL1bm9uee89WLjQegLWl66hv/+enxIHr6FDh2YNB+zMPjRwSGH2xw0wKWF36IPnJ6cK3c03w2WX\nWcsdO1oB+5FHrGEJnLn/jWVkWPmd3XGH9dDVkCHZg34J+xstFL169eLo0aMsWbIkK+3EiRN8/fXX\nWYOGffvttzRv3pzIyEji4uIY7/6DdNKxY8eskSwzMzN55JFHqFKlCgkJCXzjqc3OyYsvvkhCQgIR\nERE0atSI2bNnu2z/3//+x2WXXZa1fbXtEew9e/bQp08fqlatSpUqVXjggQcAGD9+vMsAa+7NSx07\ndmT06NFcffXVVKhQge3bt/PBBx9knSMhIYF3333XpQxz5syhWbNmREZGkpiYyIIFC/jiiy9o0aKF\nS75XXnmF3r175/h5C5oGfZWjjAzvQxGA9fBTerr14JIze+A9dszxbs9jf2DK3sNm/37rIagTJ6zz\nOQ8rm5lpbbdz3mbnHvS9zMWh8qBs2bL069fPZR7ZTz/9lIYNG9KoUSMAwsPDmTZtGikpKXzzzTe8\n8847zJ07N9djv/vuu3z77besWbOGVatW8YWn8amdJCQk8Msvv3Dy5EnGjh3LoEGDOHjwIACff/45\nzzzzDNOnT+fkyZPMnTuXSpUqkZmZSY8ePahbty67du1i7969LqN+uo+i6b4+ffp0Jk+ezKlTp6hT\npw7VqlXj22+/5eTJk7z//vs89NBDWV8uK1asYOjQobz88sukpKSwePFi4uPjuemmm9ixYwd/O/Ua\nmD59OkN9Hbu7gGjvHZWjV1+1BgnzVnuOjIRRo6whBtwtWQLt21u9a+rUsWYBsu8DjmN+/LFjALPn\nn3ftBTN/PnTr5npc95ur7l0qX3gh988VGEIN9pHAFhqygXh2UJVDRHCSShylBvu4NJcjmPH+6bUh\nY/P+78/QoUPp0aMHb775JmFhYUybNs0lYHXo0CFruVGjRtx6660sWrSIm3Lpi/r5558zatQoatSo\nAcCTTz7JokWLvObv06dP1nK/fv14/vnnWbFiBTfeeCNTpkzhscceo3nz5gBZs1ctW7aM/fv3M2HC\nhKymqKuuusrnzz5s2DAuvdT66YSEhHDDDTdkbWvfvj3XX389P//8M02bNuW9997jjjvuoFOnTgDE\nxsZmDbvcv39/pk+fzrPPPsv69evZuXMn3bt397kcBUGDvsqRL90ht2/3nG4P3t4GJ/P0ReI+GNnh\nw9nz2P97KCrCOUU7fuEqljKMDzhEVU4SwZX8Rgah/E0D1psGbAuvwLbQapwKr8CRMnEcLd8c/nwl\nx2PnJ1j7S7t27ahSpQqzZ8+mRYsWrFy5klmzZmVtX7FiBU888QTr1q0jLS2NtLQ0+vXrl+tx9+3b\n5zLVYlxcXI75P/zwQ1599VV22LplnTlzxmWaQk9TIu7evZu4uLh833twnwpy3rx5PPPMM2zatInM\nzEzOnTtHkyZNss7lLZAPGTKEgQMH8uyzzzJ9+nRuueUWSpcuna8y+YsGfeUTY6ymlYgIa71jR0c7\n+5w5Oe/r7b8ET+nuky15+k+4a1fX9UB3q4xnOwP4hMas5YqQVdTP3MWyspdytGwob8UksiMmg2Pl\n01hXM5EDVU5A+FoI/Q2OJVgP2pysCRllIKU2/BnYsufV4MGDmTp1Khs3bqRLly5UqVIla9vAgQN5\n4IEHmD9/PqVLl+ahhx7iqA8PK8TGxrLbaZaQnTkMO7pr1y7uvvtuFi5cSNu2bQFo1qxZ1s3V2rVr\ne50qcdeuXWRmZmYL/O5TJe53bj+0cW7uSUtLo2/fvkyfPp2ePXsSEhJC7969cy0DQOvWrQkLC+Pn\nn3/m448/5pNPPvH6WQNFg77y2alTjqCfnJxzW39WRzOK/43VCE7wf2YazWQ1Pc0cwjnFBwnVmXf5\nLl6qHMKflcqSfqASnKwFJ2vDqRpwpBrsiYKUOnCmKpyPwvMDNhM9pBUdQ4YM4bnnnuPPP//k1Vdf\nddl2+vRpoqOjKV26NCtWrODjjz+mi9MAQ956vdxyyy288cYbdO/enfLly/Piiy96Pf+ZM2cICQmh\ncuXKZGZmMnXq1KzZrwDuvPNOHn74Ydq1a0fz5s3ZunUrYWFhtGrVitjYWJ544gnGjRtHaGgov/32\nG1dddRVNmzZlwoQJ7N69m4iICF7IpT3Q/l9M5cqVCQkJYd68eSxYsIDGjRsD1lSJXbp0oUePHiQl\nJbF//35OnTpFgwYNAOuLc8SIEYSFheWpianA+Kvvpy8vimifZOXdAw84+grv3u1I99bv3L6tQweR\nWbOs5fXrrfemTQu7T7xvL1PmqHSK/bc8WaeLLKlUSY6XMbKuCjK+vZH/61Feyg7oJLR8Syh/SDAZ\nJbKfvrOkpCSpVKmSpKWluaR/+eWXEhcXJxEREXLjjTfKyJEjZfDgwSKSve99x44dZcqUKSIicuHC\nBfnHP/4hlSpVknr16snEiRNz7Kc/evRoiYmJkSpVqsjDDz8sSUlJWccSEZk0aZI0aNBAKlasKI0b\nN5bVq1eLiMju3bulV69eUqlSJalSpYo8+OCDWfuMGDFCoqKiJDExUSZPnuy1rHYTJ06UatWqSXR0\ntAwZMkQGDBggY8aMydo+e/ZsadKkiVSsWFESExNlwYIFWdt27dolISEhMn78+FyvNQHop+/TePrG\nmK7Aa1i9faaIyItu2yOA6UAdIBR4WUQ+8HAc8eV8quh48EHH/K67d1vj1IP38WhErG0dOsBDD0Hv\n3rBuHTRqZN3IzWlCk0IRmkatCitpXv09rpIVXHVyJy2OnqLcBZhYtwFHTzdkYunbOLDvRvz5KLyD\njqdf0p0/f55q1arx+++/e7z/4MzTePq2LYifxtPP9S6HMSYEeBPoAlwODDDGuHc6uB9YLyJNgY7A\ny8YYbTry0aFDVk+X/Ni9O29t2pmZ4HQvzqOZMx1NMnmYvxpw3LxdvNiRVtRiWmTEGnq07Mq/rjac\nCinD32evZsTxbzmdXpnnSz9IXbZiyOT+7Rv55+FZHNh3EwUT8FUwmDhxIi1btsw14AeKL4G5FbBZ\nRHYCGGNmAD2BjU55BLDPGVQROCoiF1A+GTUKPvkkf8GxXz9Yvtz3fbdutR6Syil/nz5Wv/moKNd0\nX87x5JO+lSNQynKOWpFL6FZ7DL1PLeeywxB+Fv78uzoLSt3KY5mteDdjJBmHS4GHnkJKXYy6desC\nZHugLDdlylgT7xQEX4J+TWC30/oerC8CZ28Cc40x+4BwoL9/ihcc8hLsd+60pk/ztq+I1c0yLs56\nGOrkSahWzbE9tzGt7EMieCrTrl3WsMQ52bYte5q9S2cgmnbi2Ebbct9QP/xXGoeu4vrjWzklmSzJ\nKM170Un8cvoRtp29AU7qc4mq4G331p85F5dcAn8WUM8ufzXBdAH+EJFOxpj6wPfGmCYiku3ZyHHj\nxmUtJyUlkZSU5KcilHzp6RAf7xqQ3YPzvHnQvbuVfv/98P77Oed3Z+9u7Kl55+qrradjq+cwo8OP\nP2ZPK7Bx48sep2rUMq6tNJGO6cu5fm8KkRfS+CkOtobU4buMZoyKfoADO2+FDZULqBBK+d/p08kk\nJCSzZYv/j+1L0N+LdYPWrpYtzdlt2Ib5F5GtxpjtwKXAKveDOQd9lTcZGbnncR6mYK/7T4ncg769\ny7S3fN4mJylwpc8QXe8zLqn+GXFl/qTt4RS67TzNJQdgVplofj53E5MqdGBNRgvSNzamMNvgJ0+G\nO+8stNOrEiA8PInHHkvCMUSQ93GN8sqXoL8SSDDGxAH7gVuBAW55dgLXAr8YY6oBlwAe/tFXnvjj\nRqe9Ru787MeCBY7l+vWhf3+yfomMgXr1rOYYEZg92+ppY1e5sqMnjrNPPoEnnvCtTBf1uULToPpq\nuGwGl5VZzp37/+Ch386x4mAl9qRexp8hDRl49jY20JizOytcxImUKprsgxP6W65BX0QyjDEjgAU4\numxuMMYMtzbLu8BzwAfGGPvMqI+JSBF7WD44eOtts20bLFoEgwa5ptn52t7+22/5L5t3ApfOhsob\nocYquGwm4anw0MJIeq4oTdUzwvzQ7jTkGTaeaFgQBVCqSBGB5s3ho4/g//7Pv8f2qU1fRL4DGril\nTXJa3o/Vrq8KkL3m7KkG7s55u/MDh24jvWbxdIP3xRetAddyy+fNzTfnkqHKX9BrGCZ2JVdsq0z8\nujZc8tsZus1uStO0bWykAWMYy3y6kHkh1PcTK1VC5LXLtC+0L30xYg+4OQV9T+nO3Sjdh0B2P7Yz\nT804fulzH34Arn2UtpGf0W9pJR5aD9sJ508M56nM19zAQAayj5p+OJlSypkG/WJgzRrrAS7beFM5\nBt7PPvO+belSz+kvv+zbTWLIW03fReOPodWb1Cy1mfvXHaHnwnKEnq7BxxnDuJZ2/Mi1+TxwYISE\n+P7Zi9rDaPlRsWLFrEHHzpw5Q5kyZQgNDcUYw6RJkxgwwP22nm/atm3LyJEjGThwoD+LW+KEhVnv\nBTEgpwb9YuCWW2DTJsfkIzkFlS+/zPvxH3nEGjLBFz4HtNA0aDkRGn8ENVfR5e9QvptifbNMZDh3\nM4hfaEdhP+l6+eWOidVzUr067NtnLf/rX/D00/k/56WXWt1YJ0zI/zEK2imnOSzr1avHlClT6Og+\nfVkJlJGRQWho4TYlrlsH0dHWcs+e/j++PqFShM2a5doc49ymn5FhBSt/PcAxb55v+XKs7ZoMiNkC\n3e+DR6vCZTO44vsBTH+5Ax9/EsHrPEAFTnM/7/ALV1PYAR98D7zp6Y7l6667uHNedRWEh1/cMQLJ\nPlCXs8zMTJ599lnq169P1apVGTx4MCdttZKzZ88yYMAAKlWqRHR0NG3btiUlJYVHHnmElStXcued\ndxIREcGjjz6a7VwZGRn07duX6tWrExMTQ+fOndm0aVPW9rNnz/LAAw9Qp04doqOj6dixY9Y0h8nJ\nybRt25aoqCji4+OZMWMGYP138bF9lh5g0qRJXGf7IaamphISEsI777xDQkJC1siZ9913H7Vr1yYy\nMpI2bdqwfPlylzKOHz+e+vXrExkZSevWrTl06BB33nkno0ePdvk8Xbp0YZL7eOG5uPxysM0vUyA1\nfb+M2ubri2IwomBh6N9fxP3SpKRYaY8+KpKYaC0fP269p6aKTJvm35ElfX117+60HrFbqPWr0Okp\n4f5LhTGlpcmgqnLH5Z3ki7Id5RCVZSe15WmelQqcKpTy5vb6+muRZ57JPd9XXzmWV61yLN9+e/a8\nM2c6lp1HKbW/7r1XZOtWkSeeECkOfxPx8fHy448/uqS98MIL0qFDBzlw4ICkpqbKbbfdJrfffruI\niLz++uvSr18/SU1NlYyMDFm1apWcPXtWRETatGkjH3/8sddzXbhwQaZNmyZnz56V1NRUue+++6RN\nmzZZ22+//Xbp0qWLHDp0SDIzM2XJkiWSmZkpmzdvlvDwcJk1a5ZkZGTIkSNHZO3atVnn/Oijj7KO\n8c4778h1110nIiLnz58XY4z06NFDUlJS5Pz58yIiMm3aNElJSZELFy7I888/L7Vr15YLFy6IiMgz\nzzwjzZs3l23btomIyOrVqyUlJUUWL14sdevWzTrPvn37pEKFCnL8+HGfr7Wn34ewsKx0/8Rhfx3I\np5MVg1/wwnDzzdZPwvY7JSKOoH/PPSLx8dbyvn3W+5kzIhMnFkaQzJSrbtooNP+fcGdrKTUG6dyz\npvRpeY28VjVJ1ptLREC+43q5jzelDjsKPaj7EvRXrMiefsMNrusijuXffvOcbn/NnetYXrgw+/aR\nIx0/51z/Jvz1QS+Cp6Bft25dWbp0adb6tm3bpHz58iJiDUOclJQk69aty3Ys9wCcm/3790tISIik\npqZKenq6lC5dWjZv3pwt39ixY2XgwIEej+FL0F+2bJnXMmRmZkr58uVl06ZNIiISFxcn33//vce8\n9evXlyVLloiIyEsvvSR9+vTx7YPaePt98GfQ1zb9QnbkiDWqJcAVV1jteeDohWMM2GaJy/qXr2bN\nnCcwKRBxi6DDcyyt/wNlS93MxC/K0utEFNHsZRH12UwC/2EIn9Kfc5QPcOEsHTq4ju7pyaWXwkan\noQJFPOdznmypdWvXbbl1o2vYEEJDrSY494EVL78cuuSlc7O3Ahay3bt3061bt6ybvWIr57Fjx7jj\njjs4cOAAffv25cyZMwwePJjnnnsu2+TjnmRkZPDYY48xe/Zsjh49mrXP0aNHuXDhAhkZGVnz4LqX\n52JGsaxlHzPc5t///jdTp07NmoA9NTWVI0eOkJiYyN69ez2WAawJU6ZPn067du2YPn16kRyBQIN+\nITvtNDqR8w3FnP7WAxbwo7dCm9eh3g9QZQMN5jzM+OkRdJMFrOdybmIuS2gfoMLk7pVXvD+HYLdh\ng2vQ9nad7XkGDYJp01y35TbtakICXHAbY7ZGDetGsNOkT8VarVq1mDlzJs2aNfO4ffz48YwfP54d\nO3Zw/fXX06hRIwYMGJBr4H///ff58ccfWbRoEbVq1eLgwYPUqFEDESE2NpZSpUqxdetWEhMTXfar\nXbu2S9u/M/fpEQ8cOJAtj3O5fvjhB958801++uknGjRogIhQsWLFrC+2WrVqsXXrVo+Bf8iQIbRq\n1Yq77rqLPXv2FPok6J7ojdxCYoxjREtnGzZYPUXsN0zz3UUyv8JOW7X6a5+ABxNoaP7kyS8vZ+mz\nzVj4x0eckUiuYA1tWVakAr4vWrbMnlatmqOnhDN7YL/kkuzbnMc38lVMTN73KcqGDx/O448/zp49\newA4dOgQX3/9NQA//vgjGzZsQEQIDw+nVKlSWT1iqlWrxjZPQ7HanDp1irJlyxIdHc3p06d52qmb\nVKlSpRgyZAgPPvgghw4dIjMzk19++QURYfDgwXzzzTfMmTOHjIwMjhw5wp+2Xg5Nmzbliy++IDU1\nlY0bN/LBBx/k+NlOnTpFWFgYlSpVIjU1lTFjxpDqNM7xHXfcwVNPPZU1gubq1auzbmLXrVuXhg0b\nctttt9G/f39KlSqC9Wp/tRP58kLb9LOAyI4dItu3uza9Tp9uvR87Zr3fdVdg2rZBhCbTJOSfSLvb\nkFGt4uWn8pdJChXlA4bIjcyRUqQVeht8Ti/ntnb317FjIvbZ+Dw1dV9yiWt6z57Wu/MMfvZtM2da\nN9VPnrTSz5yxXjVquB7T7swZkQEDPG8rDn8TdevWzdamn5mZKRMmTJDExESJiIiQxMREeeaZZ0RE\nZOrUqZIKiC6HAAAgAElEQVSYmCjh4eESGxsrjz76aNZ+ixYtkoSEBImJiZHHH38827lSUlKke/fu\nEh4eLvXq1ZOpU6dKSEiI7N27V0REzpw5IyNGjJAaNWpIdHS0dOrUKWuaw4ULF0rLli0lIiJC4uPj\nZcaMGSIicvDgQenUqZNERETINddcI2PGjHFp03c+vohIenq6DB48WCIiIqRWrVry+uuvS2xsrPzy\nyy9Z28eOHSvx8fESEREhbdq0kUOHDmXtb59+cfny5Xm+1t5+H/Bjm77fArpPJysGv+CBYg/6CQmu\nwaZMGev9u+8CFyxN/AJpP7CCvNm0nBwKiRIBeZ+h0pfPJIYjhR7MfX2tXet9m/u1d09v1co1vXdv\n7/t9/bXnn6n9Z+mJ/cs7+++B/k2UNAsWLJDExMR87RuIoK/NO4VIhGzjZdv/i3zssYI9d+nQk3S8\nrhkfNzIcPXA9/50n7N0wgraZKzEIt/EBX9CPY1Qq2ILkgXv/ePs0kfam1UaNHENHNGni/TieBrBy\nz59Tu739aUl3OTVXv/yy6w1kVTKlpaXxxhtvMHz48MIuilca9INMrSrfMbdabc5IJJ8v+os/No7j\n8tSdND1+hn+nTmArCYVdRK+aN3ddt9+0tbfJG+OYJcz5ocqqVV33sw9ZW7asI809YOcUwK1/WvOm\nYkVo0CD3fKr4WrNmDTExMZw5c4b77ruvsIvjVRG8yxA8/B1YPCnDebqWmkX9JuO45vQmrt5l+LXc\nJTSK+JZNJ27wz0kKWN26MHIkeOh0kY37dZsyxXuwdX4K+YknoE0buOMOuOcezz+b776Drl29/2ym\nT7dmFlPB6YorruD06WyTBRY5WtMvIOfPWy9P7F0ucwrsFxv0K5jjPBg7mPOU47UKA2l3dhPz9oyj\n7vlj9Di+sdgEfLB6M/k6NpD7wHE33ADt2nnO6zxTZ716cPvt1vL48eCp04W9f723n02rVgUzVopS\n/qQ1/QLSurU1bsYqtwkjly1zjJaZ08Ql+Q365Wos5NbaD/LK6j9ZVh5a3FKL375dCRtzmNi2iLvn\nHuu9Rw9YsQKSk123O9fKO3VypIWHQ1RU9uN56qLprndvz23wdetaD1gpVVxpTb+ArF0Lf/yRPf3Q\nIcfy0aPe989T0A9Jp0Hbu5jQsB7HD3ai96Et9IobxQ3b0/jts91wumAD/qWXuq4/+6znfD/9lHPA\n9DYs9JAh1nv79rBwYc5lsbf725+DKFcue57yPjww3LGj63STdtu2Qe3aue+vVFGlNX0/M8Z6wMq+\nDFbzQr9+MGqUa/NDTm36f/2V+7kqh69hYNy93Hn4V+qtgl/C63FVua/4fXuP/H8AP8jpCyunXjFx\ncfk7X9262f+jyuk8Xp6gz+Lpi6KgxMXF+TQ8gQoOcfn9I8gDrekXgL//dl1fuhTmzLGWnZ+wzd/f\nutCy8gfMTCjD4dNNaZ/5K/+tdD2x6cfpcnwrv58u3IAPrp/RvetpTsG4VSvHsm1UXJ+0aZP9iyan\na9u+vfcvJhGrp02g7Nixwy99rwP9AqFevcIvR0l77bAPtFWAtKZfAHr1st4zMmDoUGvZHoSca/r2\nG4e+ql96DZ9W6EjCqeOMuaQmdx2fwdENV198gf1MxPMyWIPFrVmT+zEqVPD9fJGR2dO08qyUZ1rT\nL2Affmi922u4+RlLpwKn6Rw5mRUhzVkdU4HYkJ38d+kejh4t/IC/aFH2wO687v55vdXgd+50Xc9t\nnKobb7Tet2yBYcOyb9egr5RnGvQD5MIFuPfevAf9vuYT9oVU4dXQ+7k38Ubu3L6Tc+fqFEwh8+Gq\nq7Knudf0ndvIvTWd1HH7SMZ4f/IVHF+i9eu7PojlvL9SKjtt3gmQRYusl6cg6UkljvB+6VtoWm4h\nvTrWYeHml2H9zSBF43t6yhQr2DoH3KVLrc9njOuDTMuXZx/moFs3q71/0ya4+27P51i5MvsQxQA/\n/OB59EtnGvQL1q+/eu4Oq4o+Dfr5sGcP2OdcELECV7VqrmPje3P4cO55buFTPmQwbzdPp2/5R0mb\n8yKBmE922DBwH3W2dGnX+WHthgzJ/gCT/fmDMmVcH2SyTTvqondvuOYaK6833sbP6dzZ+z52GvQL\nVps2hV0ClV9Fo9pYzNSu7ehvP3Om1U89Otq3/tsPP5zTVmEG/fmUW3n4hnQekjdIWziBQE0gfvvt\n2Wvdb7zhOa+3iUj69XPcyHbf5sye7m37xdKgr5RnGvTzyV77PXbMP8drw6+coQItyv5A7MPw1vnR\nsGKkfw7uo/btYdIk1zT707DunLteOgfuzz5zDGgG3u9haNBXqnBo885FaNwYbFNoXpRO/MgMbmVc\ns1r85/qD8OEq2H/lxR+4ADkH1T594IUXPOfzFtSdvxjcXXFF/ssF1s8lT/PQKhVENOhfBH/Md3oN\nyXxKf/oMO8ri+CPw8l44VePiD+wH7rXx5s09Dy3x739br5yOAY4vipz68QM8+WTey+ps7dqL21+p\nkkybd3KRnp77Y/v5NYCPSaYjd3SuwuJ44J3fi0zA95fcuqhGRASmHEopiwb9XJw+Dbb5j/3qGpL5\nH3fRr0Un5taNgNe3wIFmfj2HvWeMfTapV17Jnuf++30/Xn7a33Pbp3Fj38bJV0r5hzbv5GL5csfy\nhQswebK1PHVq/o+ZxEIW0okxtbrxRY9vC6xJJyzMmn6xShVr3VOt2lNf69Kl/Xcj1JeH0eyzXdl5\nethKKeUfWtPPxQ1Oc41s2GA9VQvw9NP5O15vZvIFfRkW+jbP3fktfPtfvwf82Fjr3d7DxtsgZ/Pm\ned7211+wfn329IKo6bv7/nudiESpgqRBP4CasIaZ9OExJjB1wCw4WRNWjPDb8e01Zvt48fbaureg\n723qv4QE6+UuP0E/r8NOXHut9Z+GUqpgaNAPkLKc4ytuZAgf8N69r0PCAnjv5zwfx32s9/r1Hcv2\nNnt70LQH/U6dYNAga+Ypd4MGwdU+jtv2n/9Y748/7nt5PfXeUUoVHg36TkTyNwqmL4bwIUeJZlq3\nZVBpE/znIJyom+fj/Pe/8MgjjvW337beZ8+2au7gGB7BHmSrVYNp06x399p6gwbw4ou+nfv6613f\nfVFQD18ppfLHp6BvjOlqjNlojNlkjPFYzzPGJBlj/jDGrDPG5DKpXdE0ahRUrer/47ZgJf/mSYZ3\nrgSxa+C/m+CMf05kb7oxxhHkb7019/2cx81xH+EyN87/XeSkSRPXh6Tq5v07TinlZ7n23jHGhABv\nAp2BfcBKY8wcEdnolCcSeAu4XkT2GmMqF1SBC9KKFTnPW5sf1dnP91zHMzWvZ2XCZpiyFC7kfz4+\n9/Z55yYT+/KIETB6tOc8kL32XauW7zXyvNTcnSdL0Rq/UkWDLzX9VsBmEdkpIunADMC9f8VA4EsR\n2QsgIkf8W8zAyK3NOe+BS5jEcN4MvYtXe6+FH164qIAP1sxTzuzdMZ1r+vYvBvtwBhpwlVJ2vgT9\nmsBup/U9tjRnlwAxxpiFxpiVxpjB/ipgIPn7RuNjTCCOHTz7f7/CrnawNQ+N4Xgevta9Pd0+bLFz\nc429n/unn+bpdEqpIOCvh7NKAc2BTkAF4FdjzK8issVPxw+InIL+iy86nmzNTWnS+Ij/ox9fENf7\natJqroaPfiKvQySHh/uet1QpR/ntQd+9F49SSvkS9PcCzrf6atnSnO0BjojIeeC8MWYxcAWQLeiP\nGzcuazkpKYmkpKS8lbgA5RQcfQ344Zzia3pwDYu5tUUHdl2xGKZ/Cxk5zP1nY5/EpGtXa+YpXy1f\nDi1aOCZxKV0ali2znrZdtgwqF8s7LEoFr+TkZJKTkwvk2L4E/ZVAgjEmDtgP3AoMcMszB/ivMSYU\nKAO0BjyM9OIa9EuaGI7yNw1YaloT2udmMhvNhLfWweHLc933ppscNXTnHjm+aNXKereP8R8SAq1b\nW8v2d6VU8eFeIR4/frzfjp1rm76IZAAjgAXAemCGiGwwxgw3xtxty7MRmA+sBZYB74rIX34rZYBc\nbDPIa4zik1K96Pn4EivgT17qU8C38/SMwG23+X7+qChr3lmllPLGpzZ9EfkOaOCWNslt/SXgJf8V\nLfDcg/6pU77uKYjt+7PCY8CR5jB5GWT6Pp6AMZ572fzjH/D++74dIzTU9wetlFLBSZ/IdeIc9P/+\n27ex3quzn+VYbSi9elTm7A9vwP9W5ing2/Xp41s+7YKplMovHVrZiXPQP3Ei9/yhXOBTbsGE7yNm\neBjHV424qHltPY2No5RS/hT0Nf2uXR0DieWlTd+Qyf+4i7OVdtPmH3s5vmYULBqb73Jc6TYlboUK\n0LKl57yNGuX7NEqpIBf0Nf358+HIEXj00bztN5ObuZbvqTk4jMyJG+DIpRdVDudhE8BxP8E+D29G\nhuNLaehQa7z7l4r1HRSlVGEI+qAP1qQh4FrTH5tDpb0zP9CLOfQYACdXjb7ogO9+bk/rzmPuGAMx\nMRd9SqVUEAr65h2Ac+esd+dAO3++57zV2c8PXMeeivBNWBIseTLf53UfRycvHn4YthSr552VUkWB\nBn2bvXsdQX/ZMu/5HuZl5lWLpfatLWDqxY0gbZ/WMCfeeuqEhfk+xLFSStlp0Lfp3dsR9Nu29Zyn\nNcsYVvotRvTfD3Ny7zzfsGHO26dPz2MhlVLqImnQt0lNzXl7JY4ws/T1DOl3nm3ffAeHsnehcZ/K\nsHNn671Dh+zHa93amrUqN9onXynlTxr0bZx7x3jyQVgfPmx1inlLF8LWLh7zODfXDBrkmMHK+bjP\nPeeHwiqlVD5p0LfJKeh3DJnPZeUXM7ZhE9iR5PUY5cs7lqdNg3btHOsjbc9sPf209W4/V//++S+z\nUkrlVdB22TQGfv3Vsb5xI+zc6SmnMKH87TzZog5pk1fnekxf0+3NNu7NN3XqwDXXONbtM2MppZQ/\nBG3QB2t8HWf2rpvO7qw2ktLs4/NVm8ltEhR7X3r3J2a9DabmifsXT40a2q6vlPKfoA76w4blvD00\n9DTjzr5Fz2rjkIMJuR7PXqOPi3NNr1Mne974eJ+KqJRSfhXUQT83t9S9h+1nyvHbln/6lN9e0//s\nM0fakSPWODrOwzwcPZq9p49SSgWCBn0vQs05/nn0U0aEvoyvc9vaa/rON3QrVbIdL9SRpkMoKKUK\ni/be8aJXk34cDb/Aj0d8Hyo5JIerOW4c/PzzxZdLKaUuhgZ9T8JOcdfpb3j75LP4WsuHnPv5R0XB\n1VdffNGUUupiaPOOB5fEv8MVu0vR69zDPu8zZgwsWJD3c40a5duTuUop5Q9GAtgf0BgjgTxfTnKq\nlT/XqC5l9jXi0WNf+XSsqCg4fhyuusrq+19EPqJSqoQwxiAieZjmyTut6buruob+e3Zxy+m8j4bW\nty+UKVMAZVJKKT/Rmr6z0DSSbkzgnXlpXJq6H1/b8+01faWUKgha0y8odX/k4d9TeDH1VfJyA1cp\npYoL7b3jJLZrX9rvTeMzbvGax/lJWvs0i0opVVxo0Le7dBZTvjvL0swOnCHca7bq1R3LERHWe7j3\n7EopVaRomz5ASDoRI6ux883zXJKxg8NUdcnbrBn88Ye1fPAgpKTA4cNWb50dO6B06Yub71YppXLi\nzzb9oKzpL1nilnD5Z3TZKizNSMoW8AEeesixXLUqJCZaAR+s5h4N+Eqp4iIog3779m4JV/6Pfosv\nYTa9PObPzCz4MimlVCBo7524xVQO28J1J09zl4cbuMOGadBXSpUcGvRbv8Gg71oyhwhSiPKYpU2b\nAJdJKaUKSHAH/bInCKk7n1GfRTGQGR6zhIRAw4Y6tIJSqmQIuqDv0mvnkq+5ccllHACW0i73/Eop\nVcwF5Y3cLHV+pu/GDD5gmNcsGvSVUiVJ8AZ9k0lYw4/odmwzc+jpPZsGfaVUCRJ0zTtZai/l5t9j\n+EvqsJ8aXrNp0FdKlSQlvqa/ezds2OBhQ8Mv6bouhi/pk+P+GvSVUiWJT0HfGNPVGLPRGLPJGPN4\nDvlaGmPSjTE3+6+IF6dzZ7jsMrfEMimUvfwDbji022vTTo0a8PnnMHp0wZdRKaUCJdfmHWNMCPAm\n0BnYB6w0xswRkY0e8r0AzC+IguZXaqpj+cQJ28IlX9N3aTyrJJbt1PO4X8uW1qQoSilVkvhS028F\nbBaRnSKSDswAj9XjkcAXwCE/lu+iOTfPREfbFi7/nOv/LscsehdKmZRSqrD4EvRrArud1vfY0rIY\nY2oAvUTkbYr67COlzhES9wOdj29nIR0LuzRKKRVQ/uq98xrg3NZf5AJ/Vo2/2lrabKzBKQxbSfCa\nX5/AVUqVRL4E/b1AHaf1WrY0Zy2AGcYYA1QGbjDGpIvIXPeDjRs3Lms5KSmJpKSkPBb5ItVfQL8/\nS7GKZoE9r1JK+Sg5OZnk5OQCOXauk6gYY0KBv7Fu5O4HVgADRMRTR0iMMe8DX4nITA/bAj6JSrYu\nl0+X47s343kn5Xlm59Cmf+ONMDfbV5ZSSgVeQCdREZEMYASwAFgPzBCRDcaY4caYuz3t4o+CFYjQ\nVEIzQmibspfFdHDZ9O67hVQmpZQKIJ/a9EXkO6CBW9okL3lv90O5CkaN32i/riZ/E8kxKrlsqlbN\neq9QAc6cKYSyKaVUAATXMAx1fqb32vIeZ8gyBtavh1KloEEDvZGrlCqZgivoN5hFr+V7ucHLtIjZ\nntxVSqkSpsSPvZOl3FGa8CepF6L4i9yju9b0lVIlUfDU9GusosXG2vxKKzw9RqADqymlgkGJrekb\nY7XNZ6m5gqa7KrCGK7zmV0qpkq7EBn2ATZucVuIX0enQIZeumq1be99Xm3eUUiVRiQ76WcJOExOz\nlNppKfxO86xkl/8E3CR4H6FBKaWKreBo06+5gq5/1OQXEsgkNCv5n/+EpUthyxbX5p2TJ6FcuUIo\np1JKFbASGfT/8x+3hJorGLg+g6kMzZY3Ksp6dw76FSsWXNmUUqowlcjmnccec10PqfErVx87wA9c\n65Ku7fZKqWBTIoO+K6FBhZ85RDWOUtl1i2jgV0oFl5If9Kuup9WeEFZltsm2yTnoa5dNpVQwKFZB\n//z5fOzUYA5XbYhlKVdl25SZqTV9pVRwKVZBv1w5q6dNnsT9TLtDKfxCuxyzaU1fKRUMil3vnWPH\n8pDZZBBV+VfqpGWwlibZNoeFaU1fKRVcilVNPyfGwPXXuyXG/kHHjRVZRlsyPHy/RUZCixaO/ZVS\nqqQrMUEf4Pvv3RISv6Xf6nC+pI9L8rvvWvcHKlWCSR6nglFKqZKp2DXviMDEiVC7Nnz3nTXLlfNU\nh8419rD6c7nh5z08yM0uxwgLgzJlrOUQ29eefV0ppUqyYhf0Ae6/33V9/HgPmSoc5MrUv9mWmchh\nquZ4vDVroHFj/5VPKaWKqmIX9Bctyp7mcVLzRp/SfVUc3+Pe0J+9/b5J9nu8SilVIhW7Nv3HH8+e\n9vzzHjI2nkafbSnZ2vOVUiqYFYugLwJHj+Zhh6jtNMzYSvl0w0paZtusPXWUUsGqWAT9Dz+EypVz\nz5flsi/ps7QeM7kZT1MjKqVUsCoWQf/gwTzu0Gg6g7Ye5Av6Fkh5lFKquCryQf/RRz2343tV5S9a\nn9wHF8p7HXpBm3eUUsGqyPfe+eyzPO7Q+CPuWVSVyQzF3rQjooFeKaWgGNT0d+3KQ2aTSUzih/Q8\nsIv3uc1rtsjIiy+XUkoVR0U+6OdJg7nctvYCc6V3tglTFi+23rdvhx49CqFsSilVBBTpoJ+n8fNL\nncN0eoJ7V8FE7su22V67j4/Xph6lVPAq0m36X3yRh8w3D6bLhvIcTw9nBa2ybdYhlJVSqgjX9M+e\nhQMHfMxcaxlc9iUjlkTYavlalVdKKU+KbE1/2DD4/HMfMpY7Brf0pfPUx7ks/VP6MqCgi6aUUsVW\nkQ36u3f7kKnUebirJZGb2vL59kkMYjrnKVfgZVNKqeKqyDbv5NoGX+o89L8ZJISnv4/mW7rxLd2z\nZbvyyoIpn1JKFUdFtqafa9Dv+iBU2sTIt+6mb8bbtGSlx2yrVvl4PKWUCgLFs6ZfZwk0n0y/KSN5\nOOMtrmFRtn75SimlsvMp6BtjuhpjNhpjNhljso2EY4wZaIxZY3stMcZc9DxUnoO+QOen4Pb23Dx1\nBP878096Mofd1MmW87774Pjxiy2FUkqVLLk27xhjQoA3gc7APmClMWaOiGx0yrYN6CAiKcaYrsD/\ngDYXU7BsQd9kwqCuUP97ohbfw9s7P+FafmANTT3uX748REU51kOK7P80SikVOL606bcCNovITgBj\nzAygJ5AV9EVkmVP+ZUDNiy1YRobTSkg6/DMMgHIvbeHT0/fyKf1Z5WGCFLC6enbp4prWuDH89NPF\nlkoppYo3X+q/NQHnDpR7yDmo3wnMy09h9u2DQ4esicpXr3ba8H/dALjkX7+x5nRXDlKNh3jV63H6\n9oWKFV3TjIGOHfNTKqWUKjn82nvHGNMRuA242luecePGZS0nJSWRlJSUtV6/PtSoAdu2Oe1Q90dC\n43/gjecHMTj9Gp7hn7zEo9mOGxsL+/df/GdQSqnClpycTHJycoEc20gufRmNMW2AcSLS1bb+BCAi\n8qJbvibAl0BXEdnq5ViS0/mMgYgIOHnSKfGWvnzz3d8knjxPRxayl1oe9+3b1zFWj3bPVEqVJMYY\nRMQv48v40ryzEkgwxsQZY8KAW4G5bgWqgxXwB3sL+PkSdpqkUt9w3cmNdGCx14APMEBHX1BKqVzl\n2rwjIhnGmBHAAqwviSkissEYM9zaLO8CY4AYYKIxxgDpIpJ9qMu8qrmcRxaX4x7+wwFicyjjRZ9J\nKaWCQq7NO349mYfmnRMnIDra85SG1ds8wl+r3iT2wglSKev1uPZD2vfXLwGlVEkS6OadAnXwoPdt\nXc085pdrlWPAdzdqlB8KpZRSJVShBv19+2DMGC8bTSa9D29i4fmeeTqmzn+rlFLeFeqAa19+6X3M\n/LKRG+i4M4PB6Xf6fLz166FePT8VTimlSqCA1/Sd5711bsP/+WfXfFdEzWVTeEVOEunz8MiXXQZl\nfW8JUkqpoBPwoP/WW45l56DfoYNrvi5hX7CoTDMAnnrK+/G++caPhVNKqRIu4EH/0CHH6Jepqd5y\nCdeeXs935wYCEB/v/XjduvmzdEopVbIFvMsmWOc7ccJ1FExn5aPXcPB0M6qln+Qs4axaBS1aeM6r\n3TOVUiVdieiymZ7ufds1lSexKqIqZwkHsgf28+ehV68CLJxSSpVQhRb0q1Txvu06M5/vS7XPWq/s\nNilWSAhkZhZQwZRSqgQr9IezsglJ55qU7fx04nYAWra02vSda/shIdqso5RS+VHkgn5Io+lcelRY\nl2qNzuxpxitjoGrVABdMKaVKgCIX9K+qNJVN5apwGmsWFOdunWvXkpX2xhuwZ08hFFAppYqxQn0i\nN5uQdDqlreCntP6OJKevJfvTtsZYc+CWLx/g8imlVDFXpIJ+mToLGP7HBW5IdYya5lzT13Z8pZS6\nOEWnecdk8Eile1hWuhFrucKRrEFfKaX8pmjU9EPTqDOyDP9429AsdbHLJuegX64cNG0a4LIppVQJ\nUnhBPzQNLp0NoanQ+g1e+w7eyvgHu6jrks25Tb9UKfjjjwCXUymlSpDAB/1xbk8SC/xnVjV6b4Qh\njM2W/aqrAlQupZQKAoVT09/bAhaN5Y5t25h84UEOIvRiVlY3TXBMnzhwYKGUUCmlSqRCqOlnciNf\n8TIPkcgWXuNBnuAFr1MiVqgQ4PIppVQJFvCgL7YOQ5tI5EpW8TveZ0g5eFCfvFVKKX8K+NDKv9Ka\nq1iaFfy90e6ZSill8efQygEP+iFcIJPQXPNq0FdKKUuxHk/fl4CvlFKqYBSdJ3KdhIcXdgmUUqpk\nKhpP5DrRZh2llCo4RbKmr5RSqmBo0FdKqSBSZIL+lVdqW75SShW0gAf906dd1//xDytt2TI4fDjQ\npVFKqeAS8Bu57sMqVK7sSCtV5G4rK6VUyRLwh7NEhDVrrFr9ddfBuXNQ1vOwO0oppSjmD2cBXHEF\nxMZayxrwlVIqcArtRm5iIoweXVhnV0qp4FQozTtKKaV8F/DmHWNMV2PMRmPMJmPM417yvGGM2WyM\nWW2M0ZlslVKqCMo16BtjQoA3gS7A5cAAY8ylbnluAOqLSCIwHHinAMpaoiQnJxd2EYoMvRYOei0c\n9FoUDF9q+q2AzSKyU0TSgRlAT7c8PYEPAURkORBpjKnm15KWMPoL7aDXwkGvhYNei4LhS9CvCex2\nWt9jS8spz14PeZRSShWyIjMMg1JKqYKXa+8dY0wbYJyIdLWtPwGIiLzolOcdYKGIfGpb3whcIyIH\n3Y6lXXeUUiof/NV7x5eBD1YCCcaYOGA/cCswwC3PXOB+4FPbl8QJ94AP/iu0Ukqp/Mk16ItIhjFm\nBLAAqzloiohsMMYMtzbLuyLyrTGmmzFmC3AGuK1gi62UUio/AvpwllJKqcIVsBu5vjzgVdwZY6YY\nYw4aY9Y6pUUbYxYYY/42xsw3xkQ6bXvS9kDbBmPM9U7pzY0xa23X6rVAf46LZYypZYz5yRiz3hjz\npzHmAVt6MF6LMsaY5caYP2zXYqwtPeiuhZ0xJsQY87sxZq5tPSivhTFmhzFmje13Y4UtreCvhYgU\n+Avry2ULEAeUBlYDlwbi3IF8AVcDTYG1TmkvAo/Zlh8HXrAtXwb8gdXEFm+7Pvb/vJYDLW3L3wJd\nCvuz5fE6VAea2pbDgb+BS4PxWtjKXd72Hgosw3r2JSivha3sDwHTgbm29aC8FsA2INotrcCvRaBq\n+r484FXsicgS4Lhbck9gqm15KtDLtnwTMENELojIDmAz0MoYUx2oKCIrbfk+dNqnWBCRAyKy2rZ8\nGtgA1CIIrwWAiJy1LZbB+qMVgvRaGGNqAd2AyU7JQXktAEP21pYCvxaBCvq+POBVUlUVW08mETkA\nVK4uYVgAAAISSURBVLWle3ugrSbW9bEr1tfKGBOP9d/PMqBaMF4LW3PGH8AB4HvbH2hQXgvgVeBR\nrC8+u2C9FgJ8b4xZaYy505ZW4NdC56oKvKC5c26MCQe+AB4UkdMentMIimshIplAM2NMBDDLGHM5\n2T97ib8WxpjuwEERWW2MScoha4m/FjbtRGS/MaYKsMAY8zcB+L0IVE1/L1DHab2WLS0YHLSPQ2T7\nV+yQLX0vUNspn/2aeEsvVowxpbAC/jQRmWNLDsprYSciJ4FkoCvBeS3aATcZY7YBnwCdjDHTgANB\neC0Qkf2298PAbKxm8AL/vQhU0M96wMsYE4b1gNfcAJ070IztZTcXGGZbHgrMcUq/1RgTZoypCyQA\nK2z/0qUYY1oZYwwwxGmf4uQ94C8Red0pLeiuhTGmsr0HhjGmHHAd1j2OoLsWIvKUiNQRkXpYMeAn\nERkMfEWQXQtjTHnbf8IYYyoA1wN/EojfiwDeqe6K1YtjM/BEYd85L6DP+DGwD0gFdmE9pBYN/GD7\n7AuAKKf8T2Ldhd8AXO+UfqXtF2Az8Hphf658XId2QAZWL60/gN9tP/+YILwWjW2ffzWwFnjalh50\n18LtulyDo/dO0F0LoK7T38ef9pgYiGuhD2cppVQQ0VE2lVIqiGjQV0qpIKJBXymlgogGfaWUCiIa\n9JVSKoho0FdKqSCiQV8ppYKIBn2llAoi/w9I8/wi1HlYXgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ea74e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
