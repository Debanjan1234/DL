{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train]) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dy @ self.W_fixed[2].T # FBA\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dy @ self.W_fixed[1][layer].T # FBA\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            #             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            #             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3063, acc-0.0800, valid loss-2.3124, acc-0.1004, test loss-2.3078, acc-0.0917\n",
      "Iter-20, train loss-2.3058, acc-0.0900, valid loss-2.3145, acc-0.0992, test loss-2.3097, acc-0.0880\n",
      "Iter-30, train loss-2.3123, acc-0.1000, valid loss-2.3164, acc-0.0958, test loss-2.3112, acc-0.0872\n",
      "Iter-40, train loss-2.3164, acc-0.1400, valid loss-2.3176, acc-0.0922, test loss-2.3121, acc-0.0876\n",
      "Iter-50, train loss-2.3111, acc-0.1800, valid loss-2.3188, acc-0.0926, test loss-2.3129, acc-0.0874\n",
      "Iter-60, train loss-2.3077, acc-0.1100, valid loss-2.3198, acc-0.0900, test loss-2.3136, acc-0.0872\n",
      "Iter-70, train loss-2.3061, acc-0.0700, valid loss-2.3198, acc-0.0872, test loss-2.3133, acc-0.0862\n",
      "Iter-80, train loss-2.2967, acc-0.1100, valid loss-2.3190, acc-0.0874, test loss-2.3122, acc-0.0873\n",
      "Iter-90, train loss-2.3222, acc-0.0500, valid loss-2.3180, acc-0.0888, test loss-2.3109, acc-0.0875\n",
      "Iter-100, train loss-2.3100, acc-0.1000, valid loss-2.3165, acc-0.0898, test loss-2.3091, acc-0.0898\n",
      "Iter-110, train loss-2.3126, acc-0.0400, valid loss-2.3146, acc-0.0886, test loss-2.3069, acc-0.0912\n",
      "Iter-120, train loss-2.3196, acc-0.0500, valid loss-2.3115, acc-0.0900, test loss-2.3035, acc-0.0942\n",
      "Iter-130, train loss-2.3011, acc-0.1100, valid loss-2.3083, acc-0.0924, test loss-2.2999, acc-0.0967\n",
      "Iter-140, train loss-2.3026, acc-0.0900, valid loss-2.3041, acc-0.0966, test loss-2.2955, acc-0.1003\n",
      "Iter-150, train loss-2.2602, acc-0.1300, valid loss-2.2993, acc-0.0994, test loss-2.2903, acc-0.1040\n",
      "Iter-160, train loss-2.2922, acc-0.1000, valid loss-2.2938, acc-0.1052, test loss-2.2846, acc-0.1106\n",
      "Iter-170, train loss-2.3000, acc-0.0800, valid loss-2.2875, acc-0.1098, test loss-2.2781, acc-0.1163\n",
      "Iter-180, train loss-2.2479, acc-0.1600, valid loss-2.2807, acc-0.1150, test loss-2.2710, acc-0.1214\n",
      "Iter-190, train loss-2.2829, acc-0.0900, valid loss-2.2733, acc-0.1234, test loss-2.2635, acc-0.1285\n",
      "Iter-200, train loss-2.2860, acc-0.1200, valid loss-2.2652, acc-0.1284, test loss-2.2552, acc-0.1363\n",
      "Iter-210, train loss-2.2755, acc-0.1600, valid loss-2.2564, acc-0.1344, test loss-2.2463, acc-0.1441\n",
      "Iter-220, train loss-2.2585, acc-0.1000, valid loss-2.2468, acc-0.1424, test loss-2.2366, acc-0.1534\n",
      "Iter-230, train loss-2.2466, acc-0.1000, valid loss-2.2366, acc-0.1516, test loss-2.2263, acc-0.1620\n",
      "Iter-240, train loss-2.1984, acc-0.1800, valid loss-2.2259, acc-0.1630, test loss-2.2155, acc-0.1721\n",
      "Iter-250, train loss-2.2187, acc-0.1300, valid loss-2.2150, acc-0.1744, test loss-2.2046, acc-0.1823\n",
      "Iter-260, train loss-2.2075, acc-0.1600, valid loss-2.2030, acc-0.1886, test loss-2.1924, acc-0.1941\n",
      "Iter-270, train loss-2.1737, acc-0.2300, valid loss-2.1911, acc-0.2086, test loss-2.1804, acc-0.2094\n",
      "Iter-280, train loss-2.1951, acc-0.2600, valid loss-2.1780, acc-0.2380, test loss-2.1672, acc-0.2361\n",
      "Iter-290, train loss-2.1439, acc-0.3800, valid loss-2.1647, acc-0.2676, test loss-2.1541, acc-0.2716\n",
      "Iter-300, train loss-2.1628, acc-0.2500, valid loss-2.1513, acc-0.2960, test loss-2.1407, acc-0.3055\n",
      "Iter-310, train loss-2.1262, acc-0.3100, valid loss-2.1373, acc-0.3174, test loss-2.1267, acc-0.3295\n",
      "Iter-320, train loss-2.1161, acc-0.3600, valid loss-2.1230, acc-0.3344, test loss-2.1124, acc-0.3472\n",
      "Iter-330, train loss-2.1211, acc-0.2800, valid loss-2.1083, acc-0.3516, test loss-2.0978, acc-0.3628\n",
      "Iter-340, train loss-2.0734, acc-0.3900, valid loss-2.0934, acc-0.3664, test loss-2.0829, acc-0.3766\n",
      "Iter-350, train loss-2.0652, acc-0.3500, valid loss-2.0775, acc-0.3768, test loss-2.0671, acc-0.3864\n",
      "Iter-360, train loss-2.0839, acc-0.3600, valid loss-2.0623, acc-0.3886, test loss-2.0520, acc-0.3970\n",
      "Iter-370, train loss-2.0002, acc-0.4600, valid loss-2.0458, acc-0.4012, test loss-2.0358, acc-0.4116\n",
      "Iter-380, train loss-1.9732, acc-0.4900, valid loss-2.0301, acc-0.4202, test loss-2.0202, acc-0.4287\n",
      "Iter-390, train loss-2.0390, acc-0.3300, valid loss-2.0146, acc-0.4356, test loss-2.0047, acc-0.4420\n",
      "Iter-400, train loss-2.0128, acc-0.4000, valid loss-1.9989, acc-0.4476, test loss-1.9892, acc-0.4529\n",
      "Iter-410, train loss-1.9422, acc-0.5100, valid loss-1.9827, acc-0.4580, test loss-1.9730, acc-0.4638\n",
      "Iter-420, train loss-1.8877, acc-0.5700, valid loss-1.9668, acc-0.4616, test loss-1.9572, acc-0.4694\n",
      "Iter-430, train loss-2.0296, acc-0.4000, valid loss-1.9511, acc-0.4634, test loss-1.9418, acc-0.4707\n",
      "Iter-440, train loss-1.9136, acc-0.5000, valid loss-1.9346, acc-0.4680, test loss-1.9256, acc-0.4748\n",
      "Iter-450, train loss-1.9065, acc-0.4500, valid loss-1.9188, acc-0.4738, test loss-1.9101, acc-0.4772\n",
      "Iter-460, train loss-1.9384, acc-0.4100, valid loss-1.9035, acc-0.4770, test loss-1.8949, acc-0.4772\n",
      "Iter-470, train loss-1.8948, acc-0.5100, valid loss-1.8875, acc-0.4764, test loss-1.8791, acc-0.4764\n",
      "Iter-480, train loss-1.8379, acc-0.5400, valid loss-1.8716, acc-0.4746, test loss-1.8636, acc-0.4787\n",
      "Iter-490, train loss-1.9048, acc-0.4500, valid loss-1.8565, acc-0.4744, test loss-1.8488, acc-0.4779\n",
      "Iter-500, train loss-1.8606, acc-0.4900, valid loss-1.8417, acc-0.4790, test loss-1.8344, acc-0.4805\n",
      "Iter-510, train loss-1.8539, acc-0.4400, valid loss-1.8266, acc-0.4810, test loss-1.8195, acc-0.4822\n",
      "Iter-520, train loss-1.8294, acc-0.5100, valid loss-1.8116, acc-0.4846, test loss-1.8048, acc-0.4858\n",
      "Iter-530, train loss-1.8066, acc-0.5000, valid loss-1.7971, acc-0.4852, test loss-1.7907, acc-0.4862\n",
      "Iter-540, train loss-1.8170, acc-0.4600, valid loss-1.7824, acc-0.4854, test loss-1.7764, acc-0.4867\n",
      "Iter-550, train loss-1.7886, acc-0.4700, valid loss-1.7690, acc-0.4856, test loss-1.7633, acc-0.4867\n",
      "Iter-560, train loss-1.8013, acc-0.4400, valid loss-1.7556, acc-0.4852, test loss-1.7500, acc-0.4870\n",
      "Iter-570, train loss-1.7548, acc-0.4900, valid loss-1.7426, acc-0.4878, test loss-1.7373, acc-0.4881\n",
      "Iter-580, train loss-1.6750, acc-0.5100, valid loss-1.7292, acc-0.4910, test loss-1.7242, acc-0.4895\n",
      "Iter-590, train loss-1.7074, acc-0.4700, valid loss-1.7165, acc-0.4932, test loss-1.7117, acc-0.4933\n",
      "Iter-600, train loss-1.7390, acc-0.4600, valid loss-1.7041, acc-0.4960, test loss-1.6995, acc-0.4946\n",
      "Iter-610, train loss-1.6653, acc-0.5300, valid loss-1.6915, acc-0.4980, test loss-1.6873, acc-0.4966\n",
      "Iter-620, train loss-1.6658, acc-0.4900, valid loss-1.6796, acc-0.5004, test loss-1.6756, acc-0.4981\n",
      "Iter-630, train loss-1.6447, acc-0.5400, valid loss-1.6679, acc-0.4990, test loss-1.6641, acc-0.4976\n",
      "Iter-640, train loss-1.6603, acc-0.5100, valid loss-1.6564, acc-0.5044, test loss-1.6529, acc-0.5028\n",
      "Iter-650, train loss-1.6865, acc-0.5000, valid loss-1.6456, acc-0.5058, test loss-1.6423, acc-0.5053\n",
      "Iter-660, train loss-1.6793, acc-0.5200, valid loss-1.6348, acc-0.5096, test loss-1.6318, acc-0.5092\n",
      "Iter-670, train loss-1.6698, acc-0.4300, valid loss-1.6240, acc-0.5122, test loss-1.6212, acc-0.5118\n",
      "Iter-680, train loss-1.6013, acc-0.5100, valid loss-1.6134, acc-0.5160, test loss-1.6107, acc-0.5149\n",
      "Iter-690, train loss-1.6066, acc-0.5000, valid loss-1.6035, acc-0.5172, test loss-1.6011, acc-0.5159\n",
      "Iter-700, train loss-1.6816, acc-0.4800, valid loss-1.5934, acc-0.5220, test loss-1.5914, acc-0.5195\n",
      "Iter-710, train loss-1.5732, acc-0.4800, valid loss-1.5838, acc-0.5246, test loss-1.5819, acc-0.5209\n",
      "Iter-720, train loss-1.5830, acc-0.5200, valid loss-1.5745, acc-0.5280, test loss-1.5727, acc-0.5252\n",
      "Iter-730, train loss-1.5391, acc-0.5500, valid loss-1.5652, acc-0.5304, test loss-1.5636, acc-0.5286\n",
      "Iter-740, train loss-1.5834, acc-0.5200, valid loss-1.5558, acc-0.5324, test loss-1.5546, acc-0.5299\n",
      "Iter-750, train loss-1.5735, acc-0.5100, valid loss-1.5472, acc-0.5342, test loss-1.5459, acc-0.5317\n",
      "Iter-760, train loss-1.5779, acc-0.5200, valid loss-1.5385, acc-0.5362, test loss-1.5376, acc-0.5328\n",
      "Iter-770, train loss-1.5763, acc-0.5000, valid loss-1.5300, acc-0.5366, test loss-1.5292, acc-0.5345\n",
      "Iter-780, train loss-1.5305, acc-0.5800, valid loss-1.5217, acc-0.5390, test loss-1.5212, acc-0.5385\n",
      "Iter-790, train loss-1.3624, acc-0.6400, valid loss-1.5129, acc-0.5452, test loss-1.5127, acc-0.5412\n",
      "Iter-800, train loss-1.5752, acc-0.4700, valid loss-1.5050, acc-0.5502, test loss-1.5049, acc-0.5454\n",
      "Iter-810, train loss-1.4636, acc-0.5900, valid loss-1.4968, acc-0.5532, test loss-1.4969, acc-0.5479\n",
      "Iter-820, train loss-1.4650, acc-0.5300, valid loss-1.4890, acc-0.5552, test loss-1.4895, acc-0.5509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-1.4414, acc-0.6200, valid loss-1.4810, acc-0.5592, test loss-1.4816, acc-0.5550\n",
      "Iter-840, train loss-1.4116, acc-0.5900, valid loss-1.4735, acc-0.5608, test loss-1.4742, acc-0.5559\n",
      "Iter-850, train loss-1.5203, acc-0.5600, valid loss-1.4662, acc-0.5632, test loss-1.4672, acc-0.5582\n",
      "Iter-860, train loss-1.4296, acc-0.4900, valid loss-1.4590, acc-0.5652, test loss-1.4600, acc-0.5601\n",
      "Iter-870, train loss-1.4756, acc-0.5400, valid loss-1.4519, acc-0.5668, test loss-1.4533, acc-0.5621\n",
      "Iter-880, train loss-1.5104, acc-0.5400, valid loss-1.4452, acc-0.5684, test loss-1.4469, acc-0.5636\n",
      "Iter-890, train loss-1.3461, acc-0.6500, valid loss-1.4381, acc-0.5700, test loss-1.4400, acc-0.5669\n",
      "Iter-900, train loss-1.4585, acc-0.5100, valid loss-1.4313, acc-0.5722, test loss-1.4335, acc-0.5680\n",
      "Iter-910, train loss-1.4628, acc-0.5800, valid loss-1.4249, acc-0.5728, test loss-1.4271, acc-0.5697\n",
      "Iter-920, train loss-1.3564, acc-0.6300, valid loss-1.4180, acc-0.5732, test loss-1.4206, acc-0.5706\n",
      "Iter-930, train loss-1.4428, acc-0.5300, valid loss-1.4118, acc-0.5748, test loss-1.4145, acc-0.5724\n",
      "Iter-940, train loss-1.4784, acc-0.5700, valid loss-1.4058, acc-0.5752, test loss-1.4087, acc-0.5727\n",
      "Iter-950, train loss-1.5231, acc-0.4800, valid loss-1.3997, acc-0.5766, test loss-1.4027, acc-0.5725\n",
      "Iter-960, train loss-1.4387, acc-0.6100, valid loss-1.3937, acc-0.5772, test loss-1.3970, acc-0.5734\n",
      "Iter-970, train loss-1.3921, acc-0.5300, valid loss-1.3876, acc-0.5776, test loss-1.3910, acc-0.5748\n",
      "Iter-980, train loss-1.4448, acc-0.4800, valid loss-1.3813, acc-0.5790, test loss-1.3849, acc-0.5748\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
