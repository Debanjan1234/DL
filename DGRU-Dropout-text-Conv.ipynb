{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = (hh * dh) - (h_old * dh)\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:] + dX_prime[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        layer = 0 # self.L = 1\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "            caches[layer].append(cache)\n",
    "            ys.append(y)\n",
    "            \n",
    "        for layer in range(1, self.L):\n",
    "            X_train = ys.copy()\n",
    "            ys = []\n",
    "            for X in X_train:\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            if layer < (self.L - 1): dys = dXs.copy()\n",
    "            dXs = []\n",
    "            for t in reversed(range(len(dys))):\n",
    "                dX = dys[t]\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        # Test is different than train since y[t+1] is related to y[t] \n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L): # start, stop, step\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                if layer == self.L-1: # this is the last layer\n",
    "                    y_logit = y\n",
    "                else: \n",
    "                    X = y # y: output for this layer, X: input for this layer\n",
    "            y_prob = l.softmax(y_logit)\n",
    "            idx = np.random.choice(idx_list, p=y_prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # # Backprop\n",
    "        # from sklearn.utils import shuffle as skshuffle\n",
    "        #     if shuffle:\n",
    "        #         X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "        #     for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "        for layer in range(nn.L):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # No batches/ full batches/ batch files\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                sample = self.test(X_mini[0], state, size=100)\n",
    "                print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 37.0196\n",
      "cefhio p2uemdrumseGsaaiae \n",
      "Ktintc2ui .tiliir g 1ila cphf eCg;hi e.wobltEs6,aslsioeatdlsirnepl8osdieha\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c3cecf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 150 # depth\n",
    "n_iter = 13 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//1 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
