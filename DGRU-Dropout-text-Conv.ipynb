{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        # Layer first and then lateral or recurrency\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "            ys.append(y)\n",
    "\n",
    "#         # Recurrency first and then layer\n",
    "#         for layer in range(self.L):\n",
    "#             # for X in X_train:\n",
    "#             for t in range(len(X_train)):\n",
    "#                 X = X_train[t]\n",
    "#                 if layer == 0:\n",
    "#                     X_one_hot = np.zeros(self.D)\n",
    "#                     X_one_hot[X] = 1.\n",
    "#                     y = X_one_hot.reshape(1, -1)\n",
    "#                 else:\n",
    "#                     y = X\n",
    "#                 y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "#                 caches[layer].append(cache)\n",
    "#                 ys.append(y)\n",
    "#             X_train = ys\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        # layer 1st and then recurrency\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "#         # rec 1st and then layer\n",
    "#         dXs = []\n",
    "#         for layer in reversed(range(self.L)):\n",
    "#             for t in reversed(range(len(dys))):\n",
    "#                 dX = dys[t]\n",
    "#                 dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "#                 dXs.append(dX)\n",
    "#                 for key in grad[0].keys():\n",
    "#                     grads[layer][key] += grad[layer][key]\n",
    "#             dys = dXs\n",
    "            \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    # import impl.constant as c\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.0\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            \n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[0].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 41.3132\n",
      "in GRexP 19\n",
      "Iter-2 loss: 42.0188\n",
      "in tast ar \n",
      "Iter-3 loss: 39.9001\n",
      "is ked, fis\n",
      "Iter-4 loss: 39.7168\n",
      "ins expert \n",
      "Iter-5 loss: 36.6313\n",
      "ins the Gll\n",
      "Iter-6 loss: 38.0502\n",
      "ire is is t\n",
      "Iter-7 loss: 30.2753\n",
      "in the Glob\n",
      "Iter-8 loss: 25.1090\n",
      "ith the bio\n",
      "Iter-9 loss: 19.1139\n",
      "i. Astarst \n",
      "Iter-10 loss: 20.9871\n",
      "iskedf tie \n",
      "Iter-11 loss: 20.1773\n",
      "ing andery \n",
      "Iter-12 loss: 18.8063\n",
      "idorn is th\n",
      "Iter-13 loss: 16.6719\n",
      "imatery tht\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x1106e15f8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 13 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEACAYAAAByG0uxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1107a2e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
