{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        layer = 0 # self.L = 1\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "            caches[layer].append(cache)\n",
    "            ys.append(y)\n",
    "            \n",
    "        for layer in range(1, self.L):\n",
    "            X_train = ys.copy()\n",
    "            ys = []\n",
    "            for X in X_train:\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            if layer < (self.L - 1): dys = dXs.copy()\n",
    "            dXs = []\n",
    "            for t in reversed(range(len(dys))):\n",
    "                dX = dys[t]\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.0\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_train=y_mini, ys=ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 35.5884\n",
      "imallyodan wistay lrig.chobh cureatinaa nageso, par to ande. Japan sstalis, 's lecild's largy tlitiea\n",
      "Iter-20 loss: 33.1052\n",
      "inxthe world's vast acalarly Tomof whind a tees boltty-Japanese Staedes in ant y arly 18, dedes easlm\n",
      "Iter-30 loss: 29.2653\n",
      "ided 2xpstitenfonity of Japan torexth largest lion en Sulihina, and Shal CNian D Worterutbplowest can\n",
      "Iter-40 loss: 27.3598\n",
      "int rJapan iasllo4t rimy. Apar 20th can the aliji nimperoit Ains preosoku, which is the siwhth leomsl\n",
      "Iter-50 loss: 25.0296\n",
      "ints and e20 the world, and in 1937 encego of Nobel liwity in the Global 92t leotilalict ing xpand im\n",
      "Iter-60 loss: 19.1432\n",
      "ion militaicda in the kurbe forl city ox of Herored a writhec eict. rights of islorrstion ant In the \n",
      "Iter-70 loss: 20.6732\n",
      "ing it ic ald's leally ts of interades fountr le tion e first urith. II and us aby ty. In lea. Japan \n",
      "Iter-80 loss: 14.5648\n",
      "ing paruthe Ther and parthy wort lity in thirod of Neate ry bud xth the 's toter in the alare ma, in \n",
      "Iter-90 loss: 13.5755\n",
      "isithura lade un is the asingesant of lithing of 6,852olarcent reselo-lect in the Riging rymiot pipe \n",
      "Iter-100 loss: 12.7519\n",
      "idsinderelto-insian ase caunwir Olyobal sma, us the a momalary cong, \"h. Sen ousturea, solmpirt. Japa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x10c09a898>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c09abe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
