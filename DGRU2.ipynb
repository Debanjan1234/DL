{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        # q = 1-p_dropout\n",
    "        # u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "        \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "        # equal to\n",
    "        # h = (1.0 - hz) * h_in + hz * hh\n",
    "        # or\n",
    "        # h = h_in + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz\n",
    "        # or\n",
    "        # h = h_in + hh\n",
    "\n",
    "        ## SELU + SELU-Dropout\n",
    "        #         y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        #         y, y_selu_cache = self.selu_forward(y)\n",
    "        #         y = X_in + y\n",
    "        #         if train: # with Dropout\n",
    "        #             y, y_do_cache = self.alpha_dropout_fwd(y, self.p_dropout)\n",
    "        #             cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache, y_do_cache)\n",
    "        #         else: # no Dropout: testing or validation\n",
    "        #             cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "#         y = X_in + y\n",
    "        \n",
    "        if train: # with Dropout\n",
    "            y, y_do_cache = self.dropout_forward(y, self.p_dropout)\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_do_cache)\n",
    "        else: # no Dropout: testing or validation\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=y_do_cache)\n",
    "        else:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dy_out = dy.copy()\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        \n",
    "        dh += dh_out\n",
    "        dh_in1 = dh * (1.0 - hz) # res\n",
    "\n",
    "        dhh =  dh * hz\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "        \n",
    "        dhz = dh * (hh - h_in)\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "        \n",
    "        dX = dXz + dXh\n",
    "        \n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dh = dh_in1 + dh_in2 # res cells\n",
    "        \n",
    "        dX = dX[:, self.H:]\n",
    "#         dX += dy_out # res layers\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t], train=True)                \n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        # for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "         \n",
    "        # Hidden layers\n",
    "        for layer in range(nn.L):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                sample = self.test(X_mini[0], state, size=100)\n",
    "                print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 45.5901\n",
      "iy -oo-yisamNo  ci  scio n lp,leasfdh saise xh ioir e1h eoiI reSbrdstlkldbyvDiIionoet5x rf onoa -erdn\n",
      "Iter-2 loss: 44.0345\n",
      "ide   1ryanmttfyt.r deid  rntnnt f.tshhdFedhsi   f ineil tta  n0iea  nwtlwhCtjtasltnGfaahoa v hipnnnk\n",
      "Iter-3 loss: 45.3869\n",
      "iSatemmrnic   heonirrotauhtai2nyisl n0etGctet-g aensrersiadsIuei sp o  hfyn ud arug \n",
      " aphdy.  tth iha\n",
      "Iter-4 loss: 43.6595\n",
      "ici itvabl iodhft- t opnepl onsl8sl f ,5  Ndtmeftlfen s. lS ea oa uos en aoroLsEshihuhe e0  tbasefphf\n",
      "Iter-5 loss: 44.5928\n",
      "iohassft iei tttsnaw roultrNkane Fmy .biuresyn oa tsea  u  ytuni pl rro fl.6isoadenno,io 5ini ndfhr9n\n",
      "Iter-6 loss: 38.8179\n",
      "ittetewdtelpbr ph t tsn  ree bmyilodshiatneh lomabaltuto ee Hnost otn,Wnnanetliaeoih p   tophtaiopoo5\n",
      "Iter-7 loss: 45.4129\n",
      "inodsdieaasn6p n  crn itbha    tatFycoeutyglmeaiiotpfh loph  atnptdnitno nf o, ftwtrlhJy ensntiptio r\n",
      "Iter-8 loss: 45.2979\n",
      "irh aI lepycts a raah cv hNela fLt  t ctnnrfredt eem set ue e dr,rlfera  bnn estnyt 9dsl,e pghI neawr\n",
      "Iter-9 loss: 44.7632\n",
      "i.ietlei ht,t-esirsopclmta e9sneat1sl aaef c ei  o hshll,toLeti nrpeia r4aotis att , JJgt  en dtnwocg\n",
      "Iter-10 loss: 46.0923\n",
      "ietunhaenhdndi r cen a.ml tdpse tocsymtwac eiehd  iplyticetirdaaJsrrraeimh.dnCs t  n iemtanlew oeftc \n",
      "Iter-11 loss: 48.7933\n",
      "iatse np9- anWnfklhrta Csd leerrceonatprntahtletn5o-sNhapeooie ten nCrhi  rrscHctal9rmJe n rnscadn hk\n",
      "Iter-12 loss: 45.9017\n",
      "idIeikohw  yns looini wJaaienol fl.tp  .ns inihraaabpadnn.i yndalud1laee f eewironie ui me-sre e adnC\n",
      "Iter-13 loss: 38.7799\n",
      "io pi  bo  kraeror m ainEdteac io16lin npiaereheaysnserdaa oEw.li eraileiv oesdie9 m nntow anrtaiu ai\n",
      "Iter-14 loss: 48.9237\n",
      "i8 ctuis ha u npluilsox52 nns NednjSnornrneoni citis nuug sted allpehmrfr oIfoianhr4Irlltl  tnunJa d \n",
      "Iter-15 loss: 46.1604\n",
      "insl  rpirhir . Aic ie an sit uetutl sPnoe inaaotior,hhi snetaireace oxohnthoc mhc net- nst l tnhpc o\n",
      "Iter-16 loss: 45.5988\n",
      "ie84dystotychnulstln d deyttJta,lr ncoiotuidesaeRC ouGj 4te2deo It oaugtniitslssB6eseo reinw nI  vles\n",
      "Iter-17 loss: 45.7963\n",
      "iitf,orp2atpp6'shetrnwtnncde u1dvnrr eruns ne9eefu rph .st 3 uCsnu p lysgeferre  oddfeile ntubraorlea\n",
      "Iter-18 loss: 45.5759\n",
      "it rCt ttayuWx1nrs iU a ci eaedoie  a1iyei igiio ntneiee  et  ir a die wtsaepcenic  giioa eiga udyt5f\n",
      "Iter-19 loss: 46.1917\n",
      "i jbEdn srDs1iTnlw digrt ngasadimxrge pil ot ls tipepai t  unytJesE l 8t,aeesttlnenle oEi Joa aibcg9 \n",
      "Iter-20 loss: 47.0998\n",
      "iyiah'sne dhle  hyoraceylnyr0  lahla ps- ac  mnyEi 2oh tehmerunSevnicustsoans nhed9rd n hlcaJai8sIf h\n",
      "Iter-21 loss: 45.2919\n",
      "i15lec p nid enth-hkotisihdrll sHieienrhdatb  atftiama ralhAx2saeydhi   nsng mltfrlsineJuiyhsetisl  r\n",
      "Iter-22 loss: 44.8141\n",
      "iduethg gwsa sn,snha nat,hs  eedoact2h ond4mreisstnrwaiuiel r e ilnuh ecnarr.dnaeenulu  lraaltJuaIdea\n",
      "Iter-23 loss: 44.8666\n",
      "itnnoo igssyd u ngmnivnt fan aa  rw1o  diirOimstto we t iwnt ciuagetmaaa iadreteiaa 1eaTeeeeNo e terf\n",
      "Iter-24 loss: 44.6449\n",
      "iiPo,s,n7iioeiohleeile nmtf osta ne ntpcf ooda'ssdr wSoh n Wyrrhaoi a Jte s a alh n  s2 diops hhci yW\n",
      "Iter-25 loss: 45.4308\n",
      "ia a ekueda',aagvaneandech,ndirar6-rdcn   h8s o  enjoeEeeieig alhuUhadsdawst4ent titr con   ildchilIt\n",
      "Iter-26 loss: 43.8482\n",
      "i ira w,  rsa-e4etcretssruirre cpwrynesiy ufaindoi te r fw hlt rn g eltotpb.lttsbt thaob pnn d ulh-ei\n",
      "Iter-27 loss: 47.0399\n",
      "ihPJJtta neerodesnr aacttiJnodrii fgoul etwiahbixredtetr eaeWnocedi,lus m ngtnlonnfdeteypuwog.eh9eisy\n",
      "Iter-28 loss: 44.1683\n",
      "ifepaxteo isliaiGeoteneh8Jdtd Wornp nh ganUscfadi m eepy8 t  r twhridvtueeeo e th urSad hseyseheio ia\n",
      "Iter-29 loss: 44.1388\n",
      "iagdi rltitpfaece lh nausoeJh piv 8prutstrer np  8rophatfoet7eh8esm ryceafla ost,Csrlzn rmsedeoneayle\n",
      "Iter-30 loss: 44.7027\n",
      "i  h  o oreair aserr ehuanvio,ieor rtneya ipsef psphgneyvhpsreb1lda. nerdrrs'lnSsatiltmhGaiier t xar-\n",
      "Iter-31 loss: 44.9975\n",
      "imo tafcw if es e  oodn,crnil.g ah i eotI  gooro easrtnbikJ ai\"n ese,esi dom hirendrpn.swwltr llecda \n",
      "Iter-32 loss: 44.4709\n",
      "iarn,pi eTiet ra  .ai  nnvdh aiin1em,olf9Nni hci npnaeeaBn t uuoWe or t tisti'eeat ol i\"hn   Tcfatnhl\n",
      "Iter-33 loss: 44.4656\n",
      "in fDileeop ee WWseyv temhaosn iwplt taa Ot tdan haaatieaiturro Nloh.enl  ltfweDe wtWioJraiwethi  t f\n",
      "Iter-34 loss: 44.1220\n",
      "igdnig  rbm.ees  ,Wi eh io.tn a  oys chfUto rKrsdreotatetna cehp erddr rrsuardnteiaam.ce,ulrninalniea\n",
      "Iter-35 loss: 46.2074\n",
      "ishd a edelhn,eairau su s jetlta a tir awscses r lea1eosrntiopynpl att JihAyod su. ttfnh esp h unGtal\n",
      "Iter-36 loss: 44.4718\n",
      "i hhefiarl.2   eaxacu,dgd G i etrinirhieJnfttysE 1oisrpyPset efvSeaeeioietyrlWsottndaeu tihr aarhho e\n",
      "Iter-37 loss: 44.4462\n",
      "iontA ifrnherWeefni 9 il\"anld'p oeigeri'tIdag ucrfel erenv no ipo1triloh9tuneo  idochc ocr pd rSc ssT\n",
      "Iter-38 loss: 44.3015\n",
      "iadteonoh yrt r umaox dtnlfh esa si etifD ntyel unclvThsd wd zsechunn at eyi tredt  .nttdnT.e,nri  ef\n",
      "Iter-39 loss: 44.7613\n",
      "i rmkaysnsel a  ol iaisiafineberd1 sbpn tdreubliE  iranomanhunerrrpnmbru snetea1nl cit  hy r oal,sh n\n",
      "Iter-40 loss: 44.6235\n",
      "i c ts  se(nrlleirm.o luddl yTne csie s cgEhan6uncnoiEyos,tchoo y ng nrfediodiek'ee  son sriafe iitsh\n",
      "Iter-41 loss: 44.3376\n",
      "infenr\"n;ta  elvurya f plum,pwrj,ditW6lor1n seantocSrrsentpntdc,reinme looTtNJls2 neernaana,inf  yfof\n",
      "Iter-42 loss: 44.4272\n",
      "ioeaid  matrhooNtrroikeEtnhn di Snsnson osnj5ide. nyteDectlrmo  dtr-ln ea fpbnyd  iet nohn pl,haeiswd\n",
      "Iter-43 loss: 43.9537\n",
      "idPr a  i   pneed'Jd la anr9yde  h esrt dypnnoeHen5e,n'  ,issrO spoeratehweaiaeoeaaerrs0Eoer seeoeiip\n",
      "Iter-44 loss: 45.6022\n",
      "iliuttb'rL f orotltdude itico9eteho  rtedenbanan uIisuh.c desogo4 claJ ttdsN  tdt eorcnwtn-ta n vohhd\n",
      "Iter-45 loss: 44.6355\n",
      "iJJgr op obent tnJr 5oeJ  lnA8o8t dmaekdw dtnsp hulihcDsvtddnrd,h retrurr JwnaSolhgoud. ihGr.naoa hrp\n",
      "Iter-46 loss: 43.9333\n",
      "irl'ahstryoefehnrorxglr oi ctti  pi rpcdAfpee,p trrettdktrglr n  ve ehlGwandttiE i2  sEusdre,e mtfl i\n",
      "Iter-47 loss: 44.6707\n",
      "ie ynd  ae1tdlcmDio rnl iy4srn JwaahhO4pdns leashonno a ttafihtsioon rtodlaua w ndes.1arisd  eahhh1ri\n",
      "Iter-48 loss: 43.5395\n",
      "is1tdt  l ods at2hfSrbeoun resmenrt9ra stseeh.nseeJtiat  rliiisr  se tsliel tectina  rrKcvufottr op.s\n",
      "Iter-49 loss: 43.1898\n",
      "iardrnnopla.rd wxrohhAlnanLeiesdai l- nI8e intoadoh,wodidnDtdfeep trsitcecinsciadcu iaoitld eailfm rh\n",
      "Iter-50 loss: 43.9167\n",
      "it leW cte   fichwcwho s ctexdan  hdscaaSbritrAtr ndriehe'-so-nI 6unstad w1a ooe icat o otnreltS Secr\n",
      "Iter-51 loss: 44.0746\n",
      "i rrpna pionlox o Dri tteliida Nmhp rJaoxee trTPenSnnrseels scsp  n isr esdd nh eghoedit smta orpheix\n",
      "Iter-52 loss: 43.3560\n",
      "ioeeniph err rl's,taR eKdg6ctrvnasOerdIld h yn,cdl,es tTabc1c i o)ts lptaF,telaadeg aw7ts dls, recggo\n",
      "Iter-53 loss: 42.5556\n",
      "incnalm oEmth ut i ou o tapGr.l de arafJIsnr nc dsoEiswu rhigc    r9ds1iou  n woree ishb arl,dtsthno,\n",
      "Iter-54 loss: 42.8503\n",
      "i1oe lyEy eiite.iSgfredy nystCot  utsi p gp o:iohdpireie,jornUpa'drrdlfmllddlfeaWndl siu3 mennnfld oy\n",
      "Iter-55 loss: 43.0817\n",
      "itt myel rrafucehyh   yts ct reiWocuy 8tluep  ythitiau  snt ddoee8e ig,invrrr da  fllndtn,onhe- rde f\n",
      "Iter-56 loss: 40.9304\n",
      "i Paiieatchcah1erniee,rn-Sar itea.t Eimflo ieurihnihow   nswag i,end-no yn wnlp eott  .ullu h6am rc h\n",
      "Iter-57 loss: 43.5738\n",
      "ipadeai nhnpiwelepaeetbponlte ordia sJtnsh.tr Jiareaofnte eKht1lr 0 ne cn otnpn shehe aufabuc iabei2f\n",
      "Iter-58 loss: 38.5465\n",
      "ihe本ae  eiA recudrihlctrn  tsn do ,aT oo  uipr nkwecd f teNhttpply e h  aiihh lonscotcri acpeee rdonn\n",
      "Iter-59 loss: 45.8514\n",
      "iu7fs1l rsi  e c xtekeinwaiwnPe tor thraec hyNinad uslnpeotgocfzihlo eeeeaenpreedi inpees  ioa5ngitnd\n",
      "Iter-60 loss: 44.0937\n",
      "i f1foeinsoreesnsh xt lrloeeesis 1ble ggaonni8ou eaa  oattto  an  aToh'yotnwndgae2atsosna2dtpswsaGW  \n",
      "Iter-61 loss: 44.7642\n",
      "ioa, ddpyirtnaeyaiyouxPatifhp h ar p onNsatrdhturycfnh 5rdif.-h-t m6n ocee5hd9rad G,epefxreaoy,o Saei\n",
      "Iter-62 loss: 44.6450\n",
      "ies3ehan nhnpho  Seicd erhxlnt ae  onyehenalddfds,ha iiianon dencpiai ymg eld ooalw2me ein1lvr,ona mh\n",
      "Iter-63 loss: 45.7065\n",
      "io tn)g nru8ev1Jeo euNcksxgd ety  1cctre rtfmaNa oolexye 'paafr y2 aaeaara rsrrts lr oe sh.aednihanwi\n",
      "Iter-64 loss: 43.6731\n",
      "ip tlaliritiafardaorewdxeian.al fii   eIoi  nairedramRa ihaa fet sansdAina    pd neoC cel oid otnie n\n",
      "Iter-65 loss: 43.8661\n",
      "is,ohu andioaal p a24wdwnN\"ledihs.Inmft  yuedWkpp wrpsonissoeLJi aig   etra,de.n1orte cdrnaltaea nstu\n",
      "Iter-66 loss: 44.8236\n",
      "iec hrfgarehntinnia mcnbdim Tn ateiG f e5o er ono本mn i batbaaeo 1 uns 5o2tyih tea tl  a1tn e do-tS df\n",
      "Iter-67 loss: 43.8359\n",
      "it,-hpsmdpohiy riltleia nhiifsn en p   cu Ils Ame undic  w h  at tlgl latuoreijeeee  estgrrmleaiatao.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68 loss: 42.3682\n",
      "ieettlTtran e  o aHnehlladNi  nm y9Edlonenefin   orp   fc n mooostaJiyo,, p  oiatagnsmpsu aotfdp,taid\n",
      "Iter-69 loss: 44.1125\n",
      "im eni neo-smrbr rdesueoS r ioitass ti  atala, afad nHeiiwam CetheAH5owtdrnusntt i nr acd-2ohyuon  ne\n",
      "Iter-70 loss: 44.0578\n",
      "it-ttste–n A nnneyso lees e ltndteoe6t'ic mynmsaeshctmtecHereaa e \"eI rhr snrtba oJtHatalmre hetiSaho\n",
      "Iter-71 loss: 44.7404\n",
      "ia hasi,ctgmidhnwtdrl1eforl  neodneuse hcgnaot epnxh  n eee oathludm dha1itildhGw o eo  ntllogmn Dyeo\n",
      "Iter-72 loss: 44.2395\n",
      "ii1Tan   yrrfg eanenaes andcat s myihubemm afdatineaJemsAehnofiebesinJW gf nar nr antism iek tNsirwrc\n",
      "Iter-73 loss: 43.3567\n",
      "imwee vneJg eran eoetihg krIecace2i\" 'i islen1w wWdsesnuhh adra ssla ,l sSudn iaeJaooefW lri dhlyho  \n",
      "Iter-74 loss: 43.7413\n",
      "ir nbGlrdx T aeuedelth ienoo-soe Cmapt snntdrltfht Ali  i,imoe1 yee dgaot-eeasnnteerailbteaosplism d \n",
      "Iter-75 loss: 44.6724\n",
      "in  ia tnb ohg1loawhfl mn6ne1fai r,nsdggii1J yrnyseoeaacyo.,ttst v) oor-no p(tariBexatttrpiiliobta,er\n",
      "Iter-76 loss: 42.4435\n",
      "iooto s sflsE9htyhealni 5ev  e gphbhjwha piissnaph  ltun of smhsnhh  asp  tsentlftnDeehdoh exdmn op t\n",
      "Iter-77 loss: 43.1563\n",
      "iaohmax2h e9 a an r dprstchpeeyohocatraltdNi.  oilort tn iolmyr rul-cn esi lo a1ma li olipb xy d pbee\n",
      "Iter-78 loss: 44.0860\n",
      "iaabo'Iyuo  nhn nemoort riEghhnn Ddeueaid aanapwne1tm Whsiri st n6Ihil  Alsfmsaotedrdhhsaioxurhsmwdrt\n",
      "Iter-79 loss: 45.6666\n",
      "ineneo2oE haeteelu flenvee4s gihnlg so2txnWifJlsnorts5ueaHeh oeOe aeftopeWrSaohTlgu9h e.i wafap hihsr\n",
      "Iter-80 loss: 44.7388\n",
      "iarJesJuu epwih ethnrg rocrerfosl afieig6iwia lsr- Gc e ng.  stnT,fann aft  G ift 2mJshoo eBts P naa \n",
      "Iter-81 loss: 37.1253\n",
      "i vieeat satofotnids tihalntvrnkp\"ei Eiwarlrixuppul  nanom rceerar sg'fhsel p  pteo aov,itnlantd'PJia\n",
      "Iter-82 loss: 42.1429\n",
      "iJRdamlla tn   esahi  t kydanif Iec cuK  hlolttn rl,aghratotS i. ti otne tllula mWnsTaeyyRnoinAss tn'\n",
      "Iter-83 loss: 43.6445\n",
      "itps tpieryyigaiu op Jg e dpsooln nD og  ahoe2lfahos,uh nptJgacabe fp ehornano hadwrbuiewi nlcualpn r\n",
      "Iter-84 loss: 44.2343\n",
      "it fpue nscct e lbli hna7 nmJs  mtia,gnnwJ d pitlwC kannasalidrcho   os  isewer6 hoat0ie  jl.dm  ts  \n",
      "Iter-85 loss: 44.3666\n",
      "iW pf-nlnMrpngeE os nuaoJl.hnlln imeEteo a  au l mtrngnhmdmeetn, iv t u8ysefebhii d groopo\"sfvgno toJ\n",
      "Iter-86 loss: 38.4415\n",
      "ipsflte trufhep shpeyp uct8mr   tfaicorl1da idset r3teni cee yt   idn eihehofrvde is e  nr5aSi t s .t\n",
      "Iter-87 loss: 38.2642\n",
      "i ti ma Seo slealohosshy enrshh.Jliy ooocd  Jddofs'  srset a iaet  rytouu cgwraf d is rmo   til cTni \n",
      "Iter-88 loss: 44.3021\n",
      "itua i Ngmeia  sfN stiSrldrhsen oteedOct ueu.pn2dNehulPdoi ,iuhaas ,thhd ysdOe.a vreo tp2t oxeo-n ppm\n",
      "Iter-89 loss: 45.0444\n",
      "ioineelo ngaospf tnh tN,eISt ty'issmes.tpeysoseimnsansnbninn kr an 1tdte2ilJahio Guarn ewtaolntjonr本J\n",
      "Iter-90 loss: 45.8463\n",
      "into,ttyUedomncA putbper.doeo tr ss2cwad -h'gt 9kbii'elgt rt ,onuerssr,swmeu nB tnrnotgfhtsrdstrnidrp\n",
      "Iter-91 loss: 45.8524\n",
      "in yElootn,renpshcgr W mbttktwam vxclhr-ine reiphruc r xrauc  eabPdn,eya 5dplatrtow  e  sd r p.sty lr\n",
      "Iter-92 loss: 44.2528\n",
      "i pHrliga.rai o enarId esnrrGeacoraswead   wn6 istrU e1gwna galeht, .dwxari kapnfp olfJadn nco a aanh\n",
      "Iter-93 loss: 48.5769\n",
      "iavoo1 hoeaeetOa,grrhi y-npa,foSto rJts1bectFacn ue yur -  c nennrachhry.t fJnrceteo nrdeae y trTrfie\n",
      "Iter-94 loss: 45.6003\n",
      "i ai n.yhusnpnreawrrsef-ve1nSiy. haitd g.oeim crkaa i ontonenidNrn'on aril utns Jtn ie  c1met en 1a p\n",
      "Iter-95 loss: 44.3425\n",
      "in eJgdns JaW.:menrPtteorf rE n1lpv,haatnoet  euiir to    r pies n fp mshh nhlouetcyiStfaodS dkrrreio\n",
      "Iter-96 loss: 44.2176\n",
      "i otodncnhkiisr id ohguten nliEsw, ieesam cuJdmocuerieioiWoaiaeosdyilywree oii  hli hs1m tldrea rci e\n",
      "Iter-97 loss: 47.0992\n",
      "ieoryol T etl ly rrgreodihpt   iet ronwocihio  l ts nditnrupp , lrr i nSred.prrinrdtor pn r un yrrr6 \n",
      "Iter-98 loss: 44.4673\n",
      "iGltaAbde xeye,l  lsiate rhfiun  tso idnyi  nsntrolntrit neSnsic p ihowr ssnfdtua utelfrepdrsdnarh t8\n",
      "Iter-99 loss: 43.8358\n",
      "ir,ppGwpdeee aeaoplrsmctrliridhrd doenrtsnceeasnnpsiysnit nhnnhst  ttal,oeir eJnGfkiolvla oaG toee te\n",
      "Iter-100 loss: 43.8223\n",
      "iuhhJn yn pAaga  p Reltr  ph awNsaaeedapr leaadtio .wasntghpJenttee ds rrcoibrtailieiepenoSnndey ddis\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10685b940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 10 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//100 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, p_dropout=p_dropout, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
