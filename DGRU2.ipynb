{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.), \n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for layer in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        # h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        # dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        dhz = (hh - h_in) * dh\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXh + dXz\n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1\n",
    "    eps = 1e-8\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 79.3746\n",
      "enareresird locectho Gorerino canme,ttlinyd o louret pirJoror periwest Sfglpetar wocthe the theld, th\n",
      "Iter-20 loss: 58.9756\n",
      "egented of Japan, Che weptot. The Gounna the Intys in and ciroutio Loortu, tf. chrsu the Gthel. yerdi\n",
      "Iter-30 loss: 48.1479\n",
      "e and the G2opao thich icd oatel bry cruthe Aring Noman, ars noucon ict of Jaapay mesced 2y lagon inu\n",
      "Iter-40 loss: 35.3538\n",
      "e the wount Nipit tas ofkin 190g1 lirgion fis as ath sefgly tea 1h 11d uhrmis of, ith ind chekion is \n",
      "Iter-50 loss: 28.8724\n",
      "esthy multoricy inst Chilaio hacine agmroce ay itan, Hon tian Poan us 3本;, Japparsed Japanases fixtot\n",
      "Iter-60 loss: 24.3199\n",
      "er and Itrire an sured in the G8Lan of intoke Uacteks  an akca egbrdet-ar cameld ic \"ounkud bith lacs\n",
      "Iter-70 loss: 19.4152\n",
      "ercon-xas laled the toritelte in of tlsears the peollycichin wesing manof, in 18AsHrnko N)ve, fo esti\n",
      "Iter-80 loss: 17.4825\n",
      "estionly vomiopepperete poutloJapan est macs ion earlom, vousaly Nisiuliaanve mivinix. Aprohon, Japan\n",
      "Iter-90 loss: 18.3749\n",
      "e forth trease war and loucofas esed Sanje4)ile a pared in trer phe Eenons an the lourtr lar Henor an\n",
      "Iter-100 loss: 13.5600\n",
      "epxly wisinolennthe bort an sount. Japanestkof Krest experterlogrsud wige firen an pealor Gran the wa\n",
      "Iter-110 loss: 12.1891\n",
      "elely Asta an fourto Tan oand Oansmimes oaterpan Amea poanke'd in it hurst in antes of Japan taoa, Ja\n",
      "Iter-120 loss: 11.6344\n",
      "e to the Krore are carcod eabel vogiohy Gcjedipee the Pimita Poan Agelar, Tardd-this the thuudic be s\n",
      "Iter-130 loss: 9.3178\n",
      "euga. Il an ofa. Japaneseansufif Japan es and centern. In the wort al at haveditury ctveded Japan's h\n",
      "Iter-140 loss: 8.2335\n",
      "ery wotet faved id Nomrofecime of Japan shu sgbitaly usiaalist rountitulirg, firth wurs of of itoang \n",
      "Iter-150 loss: 7.5292\n",
      "e un the )perer Th fouct alust of To Noarangesth. Tor. allatin ampergy 2in'r anjiman de iti1 iso arce\n",
      "Iter-160 loss: 7.2655\n",
      "epan waty  or hian worlat periondas off itto an dgester on hars maraaly ofilationtennthe 1st nonrorl3\n",
      "Iter-170 loss: 6.7191\n",
      "er and ed largestEiwdod exltregeltertf. Napentst rand-d thikg aid Dgltounan Afurorempurthy and wont. \n",
      "Iter-180 loss: 5.2190\n",
      "es of 19hhld century Is is na,gent sanded al nmampertapito in the EFiwor fkiutivin ighinidg is ins an\n",
      "Iter-190 loss: 8.3926\n",
      "ed in the wort tulitmin estes larlerdt apen the worl\".TJapan tar an the laigh the Savin Chinto  ort t\n",
      "Iter-200 loss: 5.3147\n",
      "evepry tas of inenunor hestar ng mabo empy a. Japan ens and ir care ceans man aigyl ohonkulamal egand\n",
      "Iter-210 loss: 4.2414\n",
      "e world's largest metropolitan area with over 35 million of Japan. Japan o ceste piltto in the fiminl\n",
      "Iter-220 loss: 4.1188\n",
      "epexpiat in ess and empered To parant mentatel an condy of fared Japan er and In 19 Dnsinid country w\n",
      "Iter-230 loss: 5.0499\n",
      "e parltit of Wistur n iten and ion wite thi feved in the maimty Jopan it opan tary Cnlomin 125 intt J\n",
      "Iter-240 loss: 3.8651\n",
      "e an wored bon the world's emuror larl, Soha worh\",han anmaid peaperomokira. 12hsst pueneiis a des am\n",
      "Iter-250 loss: 3.4143\n",
      "e north to in the Earos 19t exbpleolnm(unutuuila. The Empeeted nuss of it ass of the GrGllyC–CDyod ta\n",
      "Iter-260 loss: 5.6575\n",
      "er and fored lligasary itchar an parito 17 bhe mren os Gandecnse perorin Indexcl merory act of Was rl\n",
      "Iter-270 loss: 3.8615\n",
      "er and in of ith rostor an and ereseas ns Warldexllyiricedes Thickest miiud bodaccanced ers veanly ag\n",
      "Iter-280 loss: 3.0997\n",
      "e un the fourtry by lury wtt on manl Chilaveviver and sonro bexon the OEwodess bithingedscad elescarr\n",
      "Iter-290 loss: 2.8795\n",
      "e un the forrons. Toriningegnleumir s9 mglateedad itm boman fate congy nomen ronkest argest arman mog\n",
      "Iter-300 loss: 9.1653\n",
      "er and Winter Ol Gon ercou, in the world's ferlth leseclulersing g8nt ceveses hintercon cintots amcev\n",
      "Iter-310 loss: 3.4878\n",
      "er and Winter Olyitfse ulinito  amte bo the foummran in the natise whito bon eanly 1hy the G8, the nt\n",
      "Iter-320 loss: 2.5954\n",
      "er and Winter Olympic Games.\n",
      ". Taldy bitto an ther an-ats thes kran uccanory ir in canod of shicd an \n",
      "Iter-330 loss: 2.5161\n",
      "ed into Sea erenigm edad ins. Japan to endint Hunad an Eont on esen fered forstrensemion Chin sinese \n",
      "Iter-340 loss: 2.4681\n",
      "er and Winter Olycoman o0 anty ory perter Bpandy. I and whe son of, in ry make ar buese pilir oran's \n",
      "Iter-350 loss: 16.0068\n",
      "e world's largest urmnemintt Chen and is the larled forst Npcrory. Ippes alelasire cigy, the pirth- O\n",
      "Iter-360 loss: 3.9088\n",
      "eof. Taf. 1. Ton Sean; Sonlowtute fow ow6 wicon ac88, athicidir Pmandeiizth of warked tingercth ir Ja\n",
      "Iter-370 loss: 2.9545\n",
      "e north of Japan's The serenthy cogyl yrcym, the coandir of the latind Neaner anto ty a ended Japan J\n",
      "Iter-380 loss: 2.8741\n",
      "ed first in the Pmarerit the \". Japan tes of Wirl, 9hic nared this canked riatiture par tiov. To porr\n",
      "Iter-390 loss: 2.6818\n",
      "e un the efpand Japanexe: Oithorly ith th of Japiaecina om Nobbartses an torlorsinj edf. Japanese War\n",
      "Iter-400 loss: 2.5695\n",
      "ered into a long period ov as raced bofest empired ofed poke thice thiradith amitaring gan a morth mm\n",
      "Iter-410 loss: 2.4622\n",
      "ercond icotar int The malle the Eaiter O. Jpapiiitit a thirld tiontan Chy Gond in the nothorl, in ofp\n",
      "Iter-420 loss: 7.6089\n",
      "e worl beft the wound par er ilatad 193 8hichaich cake SingSSwun \"ag16 ad NLaond of is hass rum resid\n",
      "Iter-430 loss: 3.0762\n",
      "er and Winter Olympic Games.\n",
      ". Taalcedaad a eomtoan orgect Chelary Intisian mimcrangy fecertetiget of\n",
      "Iter-440 loss: 2.5022\n",
      "ed first in the number of Nobel laureaseact uich and editin Westicth in ranked Japan te of tas Tistar\n",
      "Iter-450 loss: 2.3970\n",
      "er and Winter Olympic Games.\n",
      ". Talcy aind earsto pasto and ithis is andec, the anth dict an 17d or an\n",
      "Iter-460 loss: 2.2960\n",
      "ed into he Weete cangess of teel intorCesich medirh ande Sonko Jpaas. Sreat\", ly the fy the the th ch\n",
      "Iter-470 loss: 2.2149\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n",
      "Iter-480 loss: 2.1504\n",
      "e far denst in the Conto la,nienly conmathy axm larankeSkU, it lost thirt OStoa th th t cont. Jpapal \n",
      "Iter-490 loss: 2.0975\n",
      "er and Winter Olympic Games.\n",
      ". Tainlyy ist  lally ad whion a rlth ta Hokel interconke ticland inkitil\n",
      "Iter-500 loss: 2.0531\n",
      "e north to the East China Sea and Taiwan in the south pae ofigh the Seatercan Che Asoa on, the the So\n",
      "Iter-510 loss: 9.9946\n",
      "ered into a lores 1r ind an toren, the fatin marlens mmpreolim bintis 1. whint ohe Japan h's names pe\n",
      "Iter-520 loss: 2.8132\n",
      "ed ffessrr Tone fousthy lmow orparan China, Ky canccudar, is the pirorizind it is orican the Goullard\n",
      "Iter-530 loss: 2.2801\n",
      "er and Winter Olympic GaRease ulitaly istihwert citetel cobrdrsing miloicy athuricl pitihir sivin, in\n",
      "Iter-540 loss: 2.1787\n",
      "ed pfeteopry Ching han con tetinma mald of the natis The nirst peo er ind in the nuudil lmy randed is\n",
      "Iter-550 loss: 2.1510\n",
      "er and Winter Olympic Games.\n",
      ". Tokatation is the cervinste efithy ty cseter O. the world's vimer. The\n",
      "Iter-560 loss: 2.1220\n",
      "ered into a long period of isolation cylitonry centhe ceded pre targ tog and the wiond Seaoan CDPated\n",
      "Iter-570 loss: 2.0855\n",
      "eed int ed ingD Wor an 1ltes resthe world Parexpecotoree wlett ofs perto 1 lecterecity an e omint peo\n",
      "Iter-580 loss: 2.0438\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-590 loss: 2.0034\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n",
      "Iter-600 loss: 1.9685\n",
      "er and Winter Olympic Games.\n",
      ". TTy GleeepingAthicecy in the country ic Wad icveviving expandy dus t t\n",
      "Iter-610 loss: 1.9387\n",
      "ed first in the number ton ecbor. Alea capesetit of itgen er adglty the Eapanata Japan,sesrand eaeane\n",
      "Iter-620 loss: 1.9119\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n",
      "Iter-630 loss: 1.8893\n",
      "ed first in the numbed lare Halast perory. an and the wist wich an aeden the wort tuanompea the nanjm\n",
      "Iter-640 loss: 21.5705\n",
      "e world's largent meparan Index on the maod pecserest of Japan's nimelepeletel copon Taem en inte san\n",
      "Iter-650 loss: 4.4812\n",
      "e world's largest metropelatiana parrowirlad ingeatshich a mard the sorlas an tar wel the bestory a, \n",
      "Iter-660 loss: 2.1833\n",
      "er and Winter Olympic Nisuress. 日本f itfof otserexperceufed is fst sontry in the Glopan Gesearces of i\n",
      "Iter-670 loss: 2.0430\n",
      "e world's largest metropolitaringe and Seater an Eeaty. Japan to 1987. Japan hake nal Cunto an 191d a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-680 loss: 2.0173\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-690 loss: 1.9876\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-700 loss: 1.9524\n",
      "er ard The sturol toten wor as the sons. Ase miaind bas aigett prentury us the fourticpas the s prton\n",
      "Iter-710 loss: 1.9200\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n",
      "Iter-720 loss: 1.8929\n",
      "er and Winter Olympic Games.\n",
      ".. Japa whe Frita to tEeteent6. alct the Obcord in e4int-Asiataime, Japa\n",
      "Iter-730 loss: 1.8698\n",
      "er and Winter Olympic Games.\n",
      ".. Japaa carese icinereccesced oss ficeton Sibcan, the Efpiesthichas cam\n",
      "Iter-740 loss: 1.8480\n",
      "er and Winter Olympic Games.\n",
      ". T945日f femokh-lur s0 Shoan a incan, China the Cotstrecint copolt tfeve\n",
      "Iter-750 loss: 1.8295\n",
      "ered into a long period of isolation in the early 17th century, which was ended in 1853 when a United\n",
      "Iter-760 loss: 1.8158\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name an country\n",
      "Iter-770 loss: 1.8051\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-780 loss: 1.7958\n",
      "e north to the East China Sea and Taian, Sux\". Japaa, is the ctribo turin 2SsSev\"and contis velbald c\n",
      "Iter-790 loss: 3.0226\n",
      "er and Winter Olympic Games Worss iclareo ofity of the Easlr'st efats lreaten nhury the fourlr treas \n",
      "Iter-800 loss: 6.5879\n",
      "ered into a rons. Id in rons Inde, it the Easter in the Unust oke Japan, is the Erese preater of 186 \n",
      "Iter-810 loss: 2.1936\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-820 loss: 1.9219\n",
      "ed first in the numwer al bentoy tulon of Nison-rerses lurestereas ompprimenerexc. mand eduving Jama \n",
      "Iter-830 loss: 1.8491\n",
      "er and Winter Olympic Games.\n",
      ".. Japminme G本mPenuly ArNopich aring Suhe mppith-liscEpeagit endio consd\n",
      "Iter-840 loss: 1.8203\n",
      "ered into a long period of isolation in the early 17th century, which was ended in 1853 hhic maind in\n",
      "Iter-850 loss: 1.8044\n",
      "ered into a long permorto Ompopin tar a sterith in Anaadecan or ar angess in the eader Was ressurg pa\n",
      "Iter-860 loss: 1.7951\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-870 loss: 1.7884\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n",
      "Iter-880 loss: 1.7832\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-890 loss: 1.7782\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-900 loss: 1.7732\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-910 loss: 1.7686\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n",
      "Iter-920 loss: 1.7643\n",
      "e world's largest metropolitan area with over 35 million residents and the world's largest urban aggl\n",
      "Iter-930 loss: 1.7605\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-940 loss: 1.7573\n",
      "ered into a long period of isolation in the early 17th century, which was ended in 1853 when a United\n",
      "Iter-950 loss: 1.7549\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-960 loss: 1.7528\n",
      "er and Winter Olympic Games.\n",
      ". T98%\"ftsiran eecllictty berty tiy the the torind by cothoutoan Ascan l\n",
      "Iter-970 loss: 1.7509\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-980 loss: 1.7493\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"ennu\n",
      "Iter-990 loss: 1.7478\n",
      "e north to the East China Sea and Taiwan in the south. The kanji that make up Japan's name mean \"sun \n",
      "Iter-1000 loss: 1.7461\n",
      "ed first in the number of Nobel laureates of any country in Asia. Japan is ranked first in the Countr\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1104c85f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
