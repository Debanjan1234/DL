{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = X_train.mean(axis=0) # mean on the number of training images\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = l.sigmoid(X=y)\n",
    "#         y = np.exp(y) # func and dfunc are the same!\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = l.sigmoid(X=y)\n",
    "#             y = np.exp(y) # func and dfunc are the same!\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2858 valid loss: 2.3236, valid accuracy: 0.0924\n",
      "Iter-200 train loss: 2.3640 valid loss: 2.3206, valid accuracy: 0.0924\n",
      "Iter-300 train loss: 2.2929 valid loss: 2.3182, valid accuracy: 0.0920\n",
      "Iter-400 train loss: 2.3380 valid loss: 2.3164, valid accuracy: 0.0792\n",
      "Iter-500 train loss: 2.3258 valid loss: 2.3149, valid accuracy: 0.0406\n",
      "Iter-600 train loss: 2.3200 valid loss: 2.3137, valid accuracy: 0.0364\n",
      "Iter-700 train loss: 2.2933 valid loss: 2.3127, valid accuracy: 0.1040\n",
      "Iter-800 train loss: 2.2967 valid loss: 2.3121, valid accuracy: 0.1126\n",
      "Iter-900 train loss: 2.2720 valid loss: 2.3115, valid accuracy: 0.1126\n",
      "Iter-1000 train loss: 2.2908 valid loss: 2.3113, valid accuracy: 0.1126\n",
      "Iter-1100 train loss: 2.3004 valid loss: 2.3111, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.3302 valid loss: 2.3109, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.3036 valid loss: 2.3108, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3318 valid loss: 2.3108, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.3206 valid loss: 2.3110, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3205 valid loss: 2.3111, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3225 valid loss: 2.3112, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3078 valid loss: 2.3111, valid accuracy: 0.1118\n",
      "Iter-1900 train loss: 2.3140 valid loss: 2.3113, valid accuracy: 0.1120\n",
      "Iter-2000 train loss: 2.3169 valid loss: 2.3113, valid accuracy: 0.1096\n",
      "Iter-2100 train loss: 2.3189 valid loss: 2.3114, valid accuracy: 0.1006\n",
      "Iter-2200 train loss: 2.3117 valid loss: 2.3114, valid accuracy: 0.0980\n",
      "Iter-2300 train loss: 2.3061 valid loss: 2.3116, valid accuracy: 0.1054\n",
      "Iter-2400 train loss: 2.3117 valid loss: 2.3117, valid accuracy: 0.1076\n",
      "Iter-2500 train loss: 2.3201 valid loss: 2.3118, valid accuracy: 0.0984\n",
      "Iter-2600 train loss: 2.3127 valid loss: 2.3119, valid accuracy: 0.1086\n",
      "Iter-2700 train loss: 2.3116 valid loss: 2.3121, valid accuracy: 0.1064\n",
      "Iter-2800 train loss: 2.3122 valid loss: 2.3121, valid accuracy: 0.1118\n",
      "Iter-2900 train loss: 2.3154 valid loss: 2.3122, valid accuracy: 0.1122\n",
      "Iter-3000 train loss: 2.3098 valid loss: 2.3122, valid accuracy: 0.1058\n",
      "Iter-3100 train loss: 2.3191 valid loss: 2.3121, valid accuracy: 0.0948\n",
      "Iter-3200 train loss: 2.3154 valid loss: 2.3122, valid accuracy: 0.0950\n",
      "Iter-3300 train loss: 2.3139 valid loss: 2.3121, valid accuracy: 0.1026\n",
      "Iter-3400 train loss: 2.3155 valid loss: 2.3119, valid accuracy: 0.1084\n",
      "Iter-3500 train loss: 2.3244 valid loss: 2.3118, valid accuracy: 0.1024\n",
      "Iter-3600 train loss: 2.3245 valid loss: 2.3116, valid accuracy: 0.1046\n",
      "Iter-3700 train loss: 2.3227 valid loss: 2.3115, valid accuracy: 0.1018\n",
      "Iter-3800 train loss: 2.3170 valid loss: 2.3113, valid accuracy: 0.0974\n",
      "Iter-3900 train loss: 2.3210 valid loss: 2.3111, valid accuracy: 0.1014\n",
      "Iter-4000 train loss: 2.3041 valid loss: 2.3110, valid accuracy: 0.0926\n",
      "Iter-4100 train loss: 2.3032 valid loss: 2.3107, valid accuracy: 0.0930\n",
      "Iter-4200 train loss: 2.3068 valid loss: 2.3104, valid accuracy: 0.0962\n",
      "Iter-4300 train loss: 2.3161 valid loss: 2.3100, valid accuracy: 0.0894\n",
      "Iter-4400 train loss: 2.3024 valid loss: 2.3097, valid accuracy: 0.0890\n",
      "Iter-4500 train loss: 2.3032 valid loss: 2.3094, valid accuracy: 0.0818\n",
      "Iter-4600 train loss: 2.3095 valid loss: 2.3090, valid accuracy: 0.0934\n",
      "Iter-4700 train loss: 2.3107 valid loss: 2.3086, valid accuracy: 0.0892\n",
      "Iter-4800 train loss: 2.3198 valid loss: 2.3081, valid accuracy: 0.0868\n",
      "Iter-4900 train loss: 2.3061 valid loss: 2.3075, valid accuracy: 0.0976\n",
      "Iter-5000 train loss: 2.3070 valid loss: 2.3069, valid accuracy: 0.1080\n",
      "Iter-5100 train loss: 2.3145 valid loss: 2.3063, valid accuracy: 0.1024\n",
      "Iter-5200 train loss: 2.3067 valid loss: 2.3056, valid accuracy: 0.1058\n",
      "Iter-5300 train loss: 2.3075 valid loss: 2.3050, valid accuracy: 0.1054\n",
      "Iter-5400 train loss: 2.3030 valid loss: 2.3042, valid accuracy: 0.1102\n",
      "Iter-5500 train loss: 2.3134 valid loss: 2.3035, valid accuracy: 0.1102\n",
      "Iter-5600 train loss: 2.3088 valid loss: 2.3026, valid accuracy: 0.1158\n",
      "Iter-5700 train loss: 2.3079 valid loss: 2.3018, valid accuracy: 0.1224\n",
      "Iter-5800 train loss: 2.2998 valid loss: 2.3010, valid accuracy: 0.1266\n",
      "Iter-5900 train loss: 2.3097 valid loss: 2.3001, valid accuracy: 0.1296\n",
      "Iter-6000 train loss: 2.3198 valid loss: 2.2993, valid accuracy: 0.1190\n",
      "Iter-6100 train loss: 2.2936 valid loss: 2.2983, valid accuracy: 0.1282\n",
      "Iter-6200 train loss: 2.2931 valid loss: 2.2972, valid accuracy: 0.1314\n",
      "Iter-6300 train loss: 2.3175 valid loss: 2.2962, valid accuracy: 0.1376\n",
      "Iter-6400 train loss: 2.3029 valid loss: 2.2949, valid accuracy: 0.1430\n",
      "Iter-6500 train loss: 2.2865 valid loss: 2.2938, valid accuracy: 0.1512\n",
      "Iter-6600 train loss: 2.3162 valid loss: 2.2925, valid accuracy: 0.1540\n",
      "Iter-6700 train loss: 2.3030 valid loss: 2.2913, valid accuracy: 0.1522\n",
      "Iter-6800 train loss: 2.2652 valid loss: 2.2899, valid accuracy: 0.1656\n",
      "Iter-6900 train loss: 2.2956 valid loss: 2.2884, valid accuracy: 0.1698\n",
      "Iter-7000 train loss: 2.2769 valid loss: 2.2869, valid accuracy: 0.1842\n",
      "Iter-7100 train loss: 2.2877 valid loss: 2.2853, valid accuracy: 0.1854\n",
      "Iter-7200 train loss: 2.2812 valid loss: 2.2837, valid accuracy: 0.1916\n",
      "Iter-7300 train loss: 2.2679 valid loss: 2.2820, valid accuracy: 0.1904\n",
      "Iter-7400 train loss: 2.2665 valid loss: 2.2801, valid accuracy: 0.1906\n",
      "Iter-7500 train loss: 2.2891 valid loss: 2.2782, valid accuracy: 0.1930\n",
      "Iter-7600 train loss: 2.2758 valid loss: 2.2764, valid accuracy: 0.1940\n",
      "Iter-7700 train loss: 2.2949 valid loss: 2.2744, valid accuracy: 0.2006\n",
      "Iter-7800 train loss: 2.2610 valid loss: 2.2723, valid accuracy: 0.2036\n",
      "Iter-7900 train loss: 2.2715 valid loss: 2.2701, valid accuracy: 0.2112\n",
      "Iter-8000 train loss: 2.2832 valid loss: 2.2679, valid accuracy: 0.2152\n",
      "Iter-8100 train loss: 2.2503 valid loss: 2.2655, valid accuracy: 0.2156\n",
      "Iter-8200 train loss: 2.2784 valid loss: 2.2631, valid accuracy: 0.2200\n",
      "Iter-8300 train loss: 2.2552 valid loss: 2.2605, valid accuracy: 0.2230\n",
      "Iter-8400 train loss: 2.2607 valid loss: 2.2581, valid accuracy: 0.2212\n",
      "Iter-8500 train loss: 2.2655 valid loss: 2.2555, valid accuracy: 0.2222\n",
      "Iter-8600 train loss: 2.2676 valid loss: 2.2528, valid accuracy: 0.2268\n",
      "Iter-8700 train loss: 2.2513 valid loss: 2.2501, valid accuracy: 0.2296\n",
      "Iter-8800 train loss: 2.2228 valid loss: 2.2473, valid accuracy: 0.2328\n",
      "Iter-8900 train loss: 2.2929 valid loss: 2.2444, valid accuracy: 0.2344\n",
      "Iter-9000 train loss: 2.2419 valid loss: 2.2414, valid accuracy: 0.2382\n",
      "Iter-9100 train loss: 2.2446 valid loss: 2.2383, valid accuracy: 0.2422\n",
      "Iter-9200 train loss: 2.1953 valid loss: 2.2352, valid accuracy: 0.2438\n",
      "Iter-9300 train loss: 2.1969 valid loss: 2.2320, valid accuracy: 0.2470\n",
      "Iter-9400 train loss: 2.2058 valid loss: 2.2286, valid accuracy: 0.2482\n",
      "Iter-9500 train loss: 2.2522 valid loss: 2.2252, valid accuracy: 0.2506\n",
      "Iter-9600 train loss: 2.2188 valid loss: 2.2217, valid accuracy: 0.2502\n",
      "Iter-9700 train loss: 2.2237 valid loss: 2.2182, valid accuracy: 0.2536\n",
      "Iter-9800 train loss: 2.1770 valid loss: 2.2146, valid accuracy: 0.2544\n",
      "Iter-9900 train loss: 2.2167 valid loss: 2.2110, valid accuracy: 0.2566\n",
      "Iter-10000 train loss: 2.2220 valid loss: 2.2073, valid accuracy: 0.2580\n",
      "Iter-10100 train loss: 2.2190 valid loss: 2.2036, valid accuracy: 0.2598\n",
      "Iter-10200 train loss: 2.2062 valid loss: 2.1998, valid accuracy: 0.2594\n",
      "Iter-10300 train loss: 2.1933 valid loss: 2.1957, valid accuracy: 0.2636\n",
      "Iter-10400 train loss: 2.1831 valid loss: 2.1916, valid accuracy: 0.2666\n",
      "Iter-10500 train loss: 2.2093 valid loss: 2.1875, valid accuracy: 0.2692\n",
      "Iter-10600 train loss: 2.1715 valid loss: 2.1835, valid accuracy: 0.2726\n",
      "Iter-10700 train loss: 2.1735 valid loss: 2.1792, valid accuracy: 0.2742\n",
      "Iter-10800 train loss: 2.2009 valid loss: 2.1750, valid accuracy: 0.2808\n",
      "Iter-10900 train loss: 2.1418 valid loss: 2.1706, valid accuracy: 0.2812\n",
      "Iter-11000 train loss: 2.1851 valid loss: 2.1662, valid accuracy: 0.2832\n",
      "Iter-11100 train loss: 2.1221 valid loss: 2.1618, valid accuracy: 0.2876\n",
      "Iter-11200 train loss: 2.1441 valid loss: 2.1571, valid accuracy: 0.2924\n",
      "Iter-11300 train loss: 2.1450 valid loss: 2.1524, valid accuracy: 0.3030\n",
      "Iter-11400 train loss: 2.1522 valid loss: 2.1477, valid accuracy: 0.3112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.1752 valid loss: 2.1428, valid accuracy: 0.3118\n",
      "Iter-11600 train loss: 2.1744 valid loss: 2.1381, valid accuracy: 0.3136\n",
      "Iter-11700 train loss: 2.1566 valid loss: 2.1333, valid accuracy: 0.3186\n",
      "Iter-11800 train loss: 2.0828 valid loss: 2.1284, valid accuracy: 0.3208\n",
      "Iter-11900 train loss: 2.1152 valid loss: 2.1235, valid accuracy: 0.3224\n",
      "Iter-12000 train loss: 2.1246 valid loss: 2.1185, valid accuracy: 0.3254\n",
      "Iter-12100 train loss: 2.1097 valid loss: 2.1134, valid accuracy: 0.3290\n",
      "Iter-12200 train loss: 2.1615 valid loss: 2.1084, valid accuracy: 0.3332\n",
      "Iter-12300 train loss: 2.1441 valid loss: 2.1033, valid accuracy: 0.3342\n",
      "Iter-12400 train loss: 2.1230 valid loss: 2.0982, valid accuracy: 0.3344\n",
      "Iter-12500 train loss: 2.0796 valid loss: 2.0929, valid accuracy: 0.3394\n",
      "Iter-12600 train loss: 2.0738 valid loss: 2.0878, valid accuracy: 0.3424\n",
      "Iter-12700 train loss: 2.0996 valid loss: 2.0825, valid accuracy: 0.3446\n",
      "Iter-12800 train loss: 2.0889 valid loss: 2.0773, valid accuracy: 0.3482\n",
      "Iter-12900 train loss: 2.0482 valid loss: 2.0720, valid accuracy: 0.3524\n",
      "Iter-13000 train loss: 2.0451 valid loss: 2.0666, valid accuracy: 0.3554\n",
      "Iter-13100 train loss: 2.0780 valid loss: 2.0613, valid accuracy: 0.3574\n",
      "Iter-13200 train loss: 2.0371 valid loss: 2.0558, valid accuracy: 0.3610\n",
      "Iter-13300 train loss: 2.0716 valid loss: 2.0502, valid accuracy: 0.3622\n",
      "Iter-13400 train loss: 2.0533 valid loss: 2.0446, valid accuracy: 0.3686\n",
      "Iter-13500 train loss: 2.0293 valid loss: 2.0391, valid accuracy: 0.3738\n",
      "Iter-13600 train loss: 2.0572 valid loss: 2.0336, valid accuracy: 0.3748\n",
      "Iter-13700 train loss: 1.9969 valid loss: 2.0281, valid accuracy: 0.3764\n",
      "Iter-13800 train loss: 2.0681 valid loss: 2.0226, valid accuracy: 0.3784\n",
      "Iter-13900 train loss: 2.0316 valid loss: 2.0170, valid accuracy: 0.3820\n",
      "Iter-14000 train loss: 2.0008 valid loss: 2.0114, valid accuracy: 0.3856\n",
      "Iter-14100 train loss: 1.9944 valid loss: 2.0057, valid accuracy: 0.3912\n",
      "Iter-14200 train loss: 2.0327 valid loss: 2.0000, valid accuracy: 0.3922\n",
      "Iter-14300 train loss: 1.9895 valid loss: 1.9942, valid accuracy: 0.3952\n",
      "Iter-14400 train loss: 1.9698 valid loss: 1.9887, valid accuracy: 0.4006\n",
      "Iter-14500 train loss: 1.9800 valid loss: 1.9831, valid accuracy: 0.4026\n",
      "Iter-14600 train loss: 1.9925 valid loss: 1.9774, valid accuracy: 0.4058\n",
      "Iter-14700 train loss: 2.0073 valid loss: 1.9717, valid accuracy: 0.4072\n",
      "Iter-14800 train loss: 1.9450 valid loss: 1.9659, valid accuracy: 0.4064\n",
      "Iter-14900 train loss: 1.9739 valid loss: 1.9602, valid accuracy: 0.4088\n",
      "Iter-15000 train loss: 2.0445 valid loss: 1.9544, valid accuracy: 0.4110\n",
      "Iter-15100 train loss: 1.9768 valid loss: 1.9488, valid accuracy: 0.4148\n",
      "Iter-15200 train loss: 1.9884 valid loss: 1.9430, valid accuracy: 0.4168\n",
      "Iter-15300 train loss: 1.9439 valid loss: 1.9373, valid accuracy: 0.4186\n",
      "Iter-15400 train loss: 1.8772 valid loss: 1.9316, valid accuracy: 0.4178\n",
      "Iter-15500 train loss: 1.9452 valid loss: 1.9259, valid accuracy: 0.4182\n",
      "Iter-15600 train loss: 1.9534 valid loss: 1.9202, valid accuracy: 0.4236\n",
      "Iter-15700 train loss: 1.8644 valid loss: 1.9146, valid accuracy: 0.4242\n",
      "Iter-15800 train loss: 1.9399 valid loss: 1.9090, valid accuracy: 0.4244\n",
      "Iter-15900 train loss: 1.9299 valid loss: 1.9035, valid accuracy: 0.4256\n",
      "Iter-16000 train loss: 1.8227 valid loss: 1.8979, valid accuracy: 0.4280\n",
      "Iter-16100 train loss: 1.9713 valid loss: 1.8923, valid accuracy: 0.4312\n",
      "Iter-16200 train loss: 1.9278 valid loss: 1.8867, valid accuracy: 0.4314\n",
      "Iter-16300 train loss: 1.8838 valid loss: 1.8811, valid accuracy: 0.4354\n",
      "Iter-16400 train loss: 1.8477 valid loss: 1.8755, valid accuracy: 0.4364\n",
      "Iter-16500 train loss: 1.9068 valid loss: 1.8699, valid accuracy: 0.4372\n",
      "Iter-16600 train loss: 1.8347 valid loss: 1.8643, valid accuracy: 0.4372\n",
      "Iter-16700 train loss: 1.8330 valid loss: 1.8588, valid accuracy: 0.4408\n",
      "Iter-16800 train loss: 1.8590 valid loss: 1.8533, valid accuracy: 0.4414\n",
      "Iter-16900 train loss: 1.8201 valid loss: 1.8478, valid accuracy: 0.4432\n",
      "Iter-17000 train loss: 1.8358 valid loss: 1.8422, valid accuracy: 0.4464\n",
      "Iter-17100 train loss: 1.8727 valid loss: 1.8368, valid accuracy: 0.4480\n",
      "Iter-17200 train loss: 1.8833 valid loss: 1.8313, valid accuracy: 0.4478\n",
      "Iter-17300 train loss: 1.9052 valid loss: 1.8258, valid accuracy: 0.4480\n",
      "Iter-17400 train loss: 1.7697 valid loss: 1.8205, valid accuracy: 0.4488\n",
      "Iter-17500 train loss: 1.8645 valid loss: 1.8152, valid accuracy: 0.4508\n",
      "Iter-17600 train loss: 1.8384 valid loss: 1.8097, valid accuracy: 0.4524\n",
      "Iter-17700 train loss: 1.7543 valid loss: 1.8043, valid accuracy: 0.4540\n",
      "Iter-17800 train loss: 1.8167 valid loss: 1.7989, valid accuracy: 0.4550\n",
      "Iter-17900 train loss: 1.7677 valid loss: 1.7938, valid accuracy: 0.4572\n",
      "Iter-18000 train loss: 1.8195 valid loss: 1.7885, valid accuracy: 0.4594\n",
      "Iter-18100 train loss: 1.7573 valid loss: 1.7832, valid accuracy: 0.4622\n",
      "Iter-18200 train loss: 1.7315 valid loss: 1.7779, valid accuracy: 0.4660\n",
      "Iter-18300 train loss: 1.7311 valid loss: 1.7727, valid accuracy: 0.4678\n",
      "Iter-18400 train loss: 1.8260 valid loss: 1.7675, valid accuracy: 0.4694\n",
      "Iter-18500 train loss: 1.7608 valid loss: 1.7623, valid accuracy: 0.4706\n",
      "Iter-18600 train loss: 1.8006 valid loss: 1.7572, valid accuracy: 0.4722\n",
      "Iter-18700 train loss: 1.7664 valid loss: 1.7522, valid accuracy: 0.4748\n",
      "Iter-18800 train loss: 1.7240 valid loss: 1.7472, valid accuracy: 0.4732\n",
      "Iter-18900 train loss: 1.7443 valid loss: 1.7423, valid accuracy: 0.4750\n",
      "Iter-19000 train loss: 1.7388 valid loss: 1.7374, valid accuracy: 0.4778\n",
      "Iter-19100 train loss: 1.7438 valid loss: 1.7324, valid accuracy: 0.4800\n",
      "Iter-19200 train loss: 1.7105 valid loss: 1.7275, valid accuracy: 0.4814\n",
      "Iter-19300 train loss: 1.7836 valid loss: 1.7225, valid accuracy: 0.4844\n",
      "Iter-19400 train loss: 1.7001 valid loss: 1.7177, valid accuracy: 0.4860\n",
      "Iter-19500 train loss: 1.6371 valid loss: 1.7129, valid accuracy: 0.4862\n",
      "Iter-19600 train loss: 1.7375 valid loss: 1.7081, valid accuracy: 0.4852\n",
      "Iter-19700 train loss: 1.7087 valid loss: 1.7034, valid accuracy: 0.4858\n",
      "Iter-19800 train loss: 1.7781 valid loss: 1.6986, valid accuracy: 0.4874\n",
      "Iter-19900 train loss: 1.6673 valid loss: 1.6938, valid accuracy: 0.4926\n",
      "Iter-20000 train loss: 1.6832 valid loss: 1.6891, valid accuracy: 0.4934\n",
      "Last iteration - Test accuracy mean: 0.4888, std: 0.0000, loss: 1.6913\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD7CAYAAACMlyg3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11385b2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113518a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
