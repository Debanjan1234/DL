{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = X_train.mean(axis=0) # mean on the number of training images\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = l.sigmoid(X=y)\n",
    "#         y = np.exp(y) # func and dfunc are the same!\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = l.sigmoid(X=y)\n",
    "#             y = np.exp(y) # func and dfunc are the same!\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2858 valid loss: 2.3236, valid accuracy: 0.0924\n",
      "Iter-200 train loss: 2.3640 valid loss: 2.3206, valid accuracy: 0.0924\n",
      "Iter-300 train loss: 2.2929 valid loss: 2.3182, valid accuracy: 0.0920\n",
      "Iter-400 train loss: 2.3380 valid loss: 2.3164, valid accuracy: 0.0792\n",
      "Iter-500 train loss: 2.3258 valid loss: 2.3149, valid accuracy: 0.0406\n",
      "Iter-600 train loss: 2.3200 valid loss: 2.3137, valid accuracy: 0.0364\n",
      "Iter-700 train loss: 2.2933 valid loss: 2.3127, valid accuracy: 0.1040\n",
      "Iter-800 train loss: 2.2967 valid loss: 2.3121, valid accuracy: 0.1126\n",
      "Iter-900 train loss: 2.2720 valid loss: 2.3115, valid accuracy: 0.1126\n",
      "Iter-1000 train loss: 2.2908 valid loss: 2.3113, valid accuracy: 0.1126\n",
      "Iter-1100 train loss: 2.3004 valid loss: 2.3111, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.3302 valid loss: 2.3109, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.3036 valid loss: 2.3108, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3318 valid loss: 2.3108, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.3206 valid loss: 2.3110, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.3205 valid loss: 2.3111, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3225 valid loss: 2.3112, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3078 valid loss: 2.3111, valid accuracy: 0.1118\n",
      "Iter-1900 train loss: 2.3140 valid loss: 2.3113, valid accuracy: 0.1120\n",
      "Iter-2000 train loss: 2.3169 valid loss: 2.3113, valid accuracy: 0.1096\n",
      "Iter-2100 train loss: 2.3189 valid loss: 2.3114, valid accuracy: 0.1006\n",
      "Iter-2200 train loss: 2.3117 valid loss: 2.3114, valid accuracy: 0.0980\n",
      "Iter-2300 train loss: 2.3061 valid loss: 2.3116, valid accuracy: 0.1054\n",
      "Iter-2400 train loss: 2.3117 valid loss: 2.3117, valid accuracy: 0.1076\n",
      "Iter-2500 train loss: 2.3201 valid loss: 2.3118, valid accuracy: 0.0984\n",
      "Iter-2600 train loss: 2.3127 valid loss: 2.3119, valid accuracy: 0.1086\n",
      "Iter-2700 train loss: 2.3116 valid loss: 2.3121, valid accuracy: 0.1064\n",
      "Iter-2800 train loss: 2.3122 valid loss: 2.3121, valid accuracy: 0.1118\n",
      "Iter-2900 train loss: 2.3154 valid loss: 2.3122, valid accuracy: 0.1122\n",
      "Iter-3000 train loss: 2.3098 valid loss: 2.3122, valid accuracy: 0.1058\n",
      "Iter-3100 train loss: 2.3191 valid loss: 2.3121, valid accuracy: 0.0948\n",
      "Iter-3200 train loss: 2.3154 valid loss: 2.3122, valid accuracy: 0.0950\n",
      "Iter-3300 train loss: 2.3139 valid loss: 2.3121, valid accuracy: 0.1026\n",
      "Iter-3400 train loss: 2.3155 valid loss: 2.3119, valid accuracy: 0.1084\n",
      "Iter-3500 train loss: 2.3244 valid loss: 2.3118, valid accuracy: 0.1024\n",
      "Iter-3600 train loss: 2.3245 valid loss: 2.3116, valid accuracy: 0.1046\n",
      "Iter-3700 train loss: 2.3227 valid loss: 2.3115, valid accuracy: 0.1018\n",
      "Iter-3800 train loss: 2.3170 valid loss: 2.3113, valid accuracy: 0.0974\n",
      "Iter-3900 train loss: 2.3210 valid loss: 2.3111, valid accuracy: 0.1014\n",
      "Iter-4000 train loss: 2.3041 valid loss: 2.3110, valid accuracy: 0.0926\n",
      "Iter-4100 train loss: 2.3032 valid loss: 2.3107, valid accuracy: 0.0930\n",
      "Iter-4200 train loss: 2.3068 valid loss: 2.3104, valid accuracy: 0.0962\n",
      "Iter-4300 train loss: 2.3161 valid loss: 2.3100, valid accuracy: 0.0894\n",
      "Iter-4400 train loss: 2.3024 valid loss: 2.3097, valid accuracy: 0.0890\n",
      "Iter-4500 train loss: 2.3032 valid loss: 2.3094, valid accuracy: 0.0818\n",
      "Iter-4600 train loss: 2.3095 valid loss: 2.3090, valid accuracy: 0.0934\n",
      "Iter-4700 train loss: 2.3107 valid loss: 2.3086, valid accuracy: 0.0892\n",
      "Iter-4800 train loss: 2.3198 valid loss: 2.3081, valid accuracy: 0.0868\n",
      "Iter-4900 train loss: 2.3061 valid loss: 2.3075, valid accuracy: 0.0976\n",
      "Iter-5000 train loss: 2.3070 valid loss: 2.3069, valid accuracy: 0.1080\n",
      "Iter-5100 train loss: 2.3145 valid loss: 2.3063, valid accuracy: 0.1024\n",
      "Iter-5200 train loss: 2.3067 valid loss: 2.3056, valid accuracy: 0.1058\n",
      "Iter-5300 train loss: 2.3075 valid loss: 2.3050, valid accuracy: 0.1054\n",
      "Iter-5400 train loss: 2.3030 valid loss: 2.3042, valid accuracy: 0.1102\n",
      "Iter-5500 train loss: 2.3134 valid loss: 2.3035, valid accuracy: 0.1102\n",
      "Iter-5600 train loss: 2.3088 valid loss: 2.3026, valid accuracy: 0.1158\n",
      "Iter-5700 train loss: 2.3079 valid loss: 2.3018, valid accuracy: 0.1224\n",
      "Iter-5800 train loss: 2.2998 valid loss: 2.3010, valid accuracy: 0.1266\n",
      "Iter-5900 train loss: 2.3097 valid loss: 2.3001, valid accuracy: 0.1296\n",
      "Iter-6000 train loss: 2.3198 valid loss: 2.2993, valid accuracy: 0.1190\n",
      "Iter-6100 train loss: 2.2936 valid loss: 2.2983, valid accuracy: 0.1282\n",
      "Iter-6200 train loss: 2.2931 valid loss: 2.2972, valid accuracy: 0.1314\n",
      "Iter-6300 train loss: 2.3175 valid loss: 2.2962, valid accuracy: 0.1376\n",
      "Iter-6400 train loss: 2.3029 valid loss: 2.2949, valid accuracy: 0.1430\n",
      "Iter-6500 train loss: 2.2865 valid loss: 2.2938, valid accuracy: 0.1512\n",
      "Iter-6600 train loss: 2.3162 valid loss: 2.2925, valid accuracy: 0.1540\n",
      "Iter-6700 train loss: 2.3030 valid loss: 2.2913, valid accuracy: 0.1522\n",
      "Iter-6800 train loss: 2.2652 valid loss: 2.2899, valid accuracy: 0.1656\n",
      "Iter-6900 train loss: 2.2956 valid loss: 2.2884, valid accuracy: 0.1698\n",
      "Iter-7000 train loss: 2.2769 valid loss: 2.2869, valid accuracy: 0.1842\n",
      "Iter-7100 train loss: 2.2877 valid loss: 2.2853, valid accuracy: 0.1854\n",
      "Iter-7200 train loss: 2.2812 valid loss: 2.2837, valid accuracy: 0.1916\n",
      "Iter-7300 train loss: 2.2679 valid loss: 2.2820, valid accuracy: 0.1904\n",
      "Iter-7400 train loss: 2.2665 valid loss: 2.2801, valid accuracy: 0.1906\n",
      "Iter-7500 train loss: 2.2891 valid loss: 2.2782, valid accuracy: 0.1930\n",
      "Iter-7600 train loss: 2.2758 valid loss: 2.2764, valid accuracy: 0.1940\n",
      "Iter-7700 train loss: 2.2949 valid loss: 2.2744, valid accuracy: 0.2006\n",
      "Iter-7800 train loss: 2.2610 valid loss: 2.2723, valid accuracy: 0.2036\n",
      "Iter-7900 train loss: 2.2715 valid loss: 2.2701, valid accuracy: 0.2112\n",
      "Iter-8000 train loss: 2.2832 valid loss: 2.2679, valid accuracy: 0.2152\n",
      "Iter-8100 train loss: 2.2503 valid loss: 2.2655, valid accuracy: 0.2156\n",
      "Iter-8200 train loss: 2.2784 valid loss: 2.2631, valid accuracy: 0.2200\n",
      "Iter-8300 train loss: 2.2552 valid loss: 2.2605, valid accuracy: 0.2230\n",
      "Iter-8400 train loss: 2.2607 valid loss: 2.2581, valid accuracy: 0.2212\n",
      "Iter-8500 train loss: 2.2655 valid loss: 2.2555, valid accuracy: 0.2222\n",
      "Iter-8600 train loss: 2.2676 valid loss: 2.2528, valid accuracy: 0.2268\n",
      "Iter-8700 train loss: 2.2513 valid loss: 2.2501, valid accuracy: 0.2296\n",
      "Iter-8800 train loss: 2.2228 valid loss: 2.2473, valid accuracy: 0.2328\n",
      "Iter-8900 train loss: 2.2929 valid loss: 2.2444, valid accuracy: 0.2344\n",
      "Iter-9000 train loss: 2.2419 valid loss: 2.2414, valid accuracy: 0.2382\n",
      "Iter-9100 train loss: 2.2446 valid loss: 2.2383, valid accuracy: 0.2422\n",
      "Iter-9200 train loss: 2.1953 valid loss: 2.2352, valid accuracy: 0.2438\n",
      "Iter-9300 train loss: 2.1969 valid loss: 2.2320, valid accuracy: 0.2470\n",
      "Iter-9400 train loss: 2.2058 valid loss: 2.2286, valid accuracy: 0.2482\n",
      "Iter-9500 train loss: 2.2522 valid loss: 2.2252, valid accuracy: 0.2506\n",
      "Iter-9600 train loss: 2.2188 valid loss: 2.2217, valid accuracy: 0.2502\n",
      "Iter-9700 train loss: 2.2237 valid loss: 2.2182, valid accuracy: 0.2536\n",
      "Iter-9800 train loss: 2.1770 valid loss: 2.2146, valid accuracy: 0.2544\n",
      "Iter-9900 train loss: 2.2167 valid loss: 2.2110, valid accuracy: 0.2566\n",
      "Iter-10000 train loss: 2.2220 valid loss: 2.2073, valid accuracy: 0.2580\n",
      "Iter-10100 train loss: 2.2190 valid loss: 2.2036, valid accuracy: 0.2598\n",
      "Iter-10200 train loss: 2.2062 valid loss: 2.1998, valid accuracy: 0.2594\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
