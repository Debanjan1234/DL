{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # row or hight\n",
    "# fh = 5 \n",
    "# fw = 4 \n",
    "# C = 3\n",
    "# i0 = np.arange(fh) # field hight\n",
    "# i0, i0.shape\n",
    "# i0 = np.repeat(i0, fw) # field width\n",
    "# i0, i0.shape\n",
    "# i0 = np.tile(i0, C) # C\n",
    "# i0.shape, i0\n",
    "# oh = 1\n",
    "# ow = 2\n",
    "# i1 = np.repeat(np.arange(oh), ow)\n",
    "# i1, i1.shape\n",
    "# i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "# i.shape, i1.shape, i0.shape, i1, i0, i[-4:], \n",
    "# # i.astype(int)\n",
    "# i.T.shape, i.T\n",
    "\n",
    "# # for cols or width\n",
    "# j0 = np.tile(np.arange(fw), fh * C)\n",
    "# j1 = np.tile(np.arange(ow), oh)\n",
    "# j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "# j0.shape, j1.shape, j.shape, j0, j1, j[:10]\n",
    "# j.T.shape, j.T\n",
    "\n",
    "# k = np.tile(np.arange(C), fh * fw).reshape(-1, 1).T\n",
    "# k.shape, k\n",
    "\n",
    "# X = np.random.rand(1, 2, 3, 4)\n",
    "# X.shape, X\n",
    "# np.pad(X, ((0, 0), (0, 0), (2, 2), (0, 0)), mode='constant')\n",
    "# # np.pad(X, ((0, 0), (0, 0)), mode='constant') # zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution impl\n",
    "# from impl.im2col import *\n",
    "# or\n",
    "# import impl.im2col as im2col\n",
    "# out_height = int(((H + (2 * pad) - kernel_height) / stride) + 1), \n",
    "# stride == 1, ALWAYS\n",
    "# pad == kernel//2, ALWAYS\n",
    "# kernel == min size ALWAYS, i.e. one past, one pres, one post (if exist), i.e. three or two\n",
    "# kernel == 3 or 2 ALWAYS\n",
    "# input=X, kernel=3or2, padding=kernel//2, stride=1, output=y\n",
    "def get_im2col_indices(X_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    # Input shape\n",
    "    N, C, H, W = X_shape\n",
    "    \n",
    "    # Kernel shape\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C\n",
    "    \n",
    "    # Output shape\n",
    "    assert (H + (2 * padding) - field_height) % stride == 0\n",
    "    assert (W + (2 * padding) - field_width) % stride == 0\n",
    "    out_height = int(((H + (2 * padding) - field_height) / stride) + 1)\n",
    "    out_width = int(((W + (2 * padding) - field_width) / stride) + 1)\n",
    "    out_C = 1 # the output channel/ depth\n",
    "\n",
    "    # Row, Height, i\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, field_C)\n",
    "    i1 = np.repeat(np.arange(out_height), out_width)\n",
    "    i1 = np.tile(i1, out_C)\n",
    "    i1 *= stride\n",
    "    \n",
    "    # Column, Width, j\n",
    "    j0 = np.tile(np.arange(field_width), field_height * field_C)\n",
    "    j1 = np.tile(np.arange(out_width), out_height * out_C)\n",
    "    j1 *= stride\n",
    "    \n",
    "    # Channel, Depth, K\n",
    "    k0 = np.repeat(np.arange(field_C), field_height * field_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 = np.repeat(np.arange(out_C), out_height * out_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 *= stride\n",
    "    \n",
    "    # Indices: i, j, k index\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = k0.reshape(-1, 1) + k1.reshape(1, -1)\n",
    "    \n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(X, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant') # X_NxCxHxW\n",
    "\n",
    "    k, i, j = get_im2col_indices(X.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    X_col = X_padded[:, k, i, j] # X_col_txkxn\n",
    "    \n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "    \n",
    "    X_col = X_col.transpose(1, 2, 0).reshape(kernel_size, -1)\n",
    "    \n",
    "    return X_col\n",
    "\n",
    "def col2im_indices(X_col, X_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = X_shape\n",
    "    H_padded, W_padded = H + (2 * padding), W + (2 * padding)\n",
    "    X_padded = np.zeros((N, C, H_padded, W_padded), dtype=X_col.dtype)\n",
    "    \n",
    "    k, i, j = get_im2col_indices(X_shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "\n",
    "    X_col = X_col.reshape(kernel_size, -1, N).transpose(2, 0, 1) # N, K, H * W\n",
    "    np.add.at(X_padded, (slice(None), k, i, j), X_col) # slice(None)== ':'\n",
    "    \n",
    "    return X_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    \n",
    "    # Input X\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    \n",
    "    # Kernel W\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "    \n",
    "    # Output\n",
    "    h_out = ((h_x + (2 * padding) - h_filter) / stride) + 1\n",
    "    w_out = ((w_x + (2 * padding) - w_filter) / stride) + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filter, -1)\n",
    "\n",
    "    out = (W_col @ X_col) + b\n",
    "    out = out.reshape(n_filter, h_out, w_out, n_x).transpose(3, 0, 1, 2)\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W = W.reshape(n_filter, -1)\n",
    "    dX_col = W.T @ dout\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob in this case. \n",
    "# Is this true in other cases as well? Yes.\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.arange(24).reshape(2,3,4)\n",
    "# a\n",
    "# # np.add.at(a, (slice(None), 0, 1, 2), cols_reshaped) # slice(None)== ':'\n",
    "# # a[:, 0, 1]\n",
    "# # a[0,:,1] \n",
    "# # # or \n",
    "# # a[0,slice(None),1],\n",
    "# # # outputs array([1, 5, 9])\n",
    "\n",
    "# # a[0,None,1] \n",
    "# # a[0, 1]\n",
    "# # # gives array([[4, 5, 6, 7]])\n",
    "\n",
    "# # # Could sb explain the latter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "# # Becoming more familiar with y_train data structure, abstraction, shape, and type\n",
    "# y_train.max(), y_train\n",
    "# data = y_train\n",
    "# data.shape, data[0]\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "# X_train[0, :10, :10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout, lam):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lam = lam\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = selu_dropout_forward(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy_reg(self.model[2], y, y_train, lam=self.lam)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = selu_dropout_backward(dout=dy, cache=h_do_cache)\n",
    "            dh = selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = conv_backward(dout=dh, cache=X_conv_cache)\n",
    "        # dX: TODO: hast not been used but this basically should be 0 \n",
    "        # which means input can be perfectly recontructed!\n",
    "        # dX is the grad_input or delta for input or the calculated error or difference or delta\n",
    "        # Can be used as noise which is because it is unwanted and can be added to data to be calculated again\n",
    "        # when the data is not abundantly available!\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        # else: # self.mode == 'regression'\n",
    "        # return np.round(y_logit)\n",
    "        # y_prob for accuracy & y_logit for loss\n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        # if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Train the model\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Test the model \n",
    "            # (validate the model)\n",
    "            # if val_set:\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            # def accuracy(y_true, y_pred):\n",
    "            # return np.mean(y_pred == y_true)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "\n",
    "            # Update the model\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Print the model \n",
    "            # loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(iter, \n",
    "                                                                                                     loss, \n",
    "                                                                                                     valid_loss, \n",
    "                                                                                                     valid_acc))\n",
    "                \n",
    "        # Test the model\n",
    "        # The test data has NOT been used before and has NOT been seen by the model\n",
    "        # # Kernel dead problem sometimes!\n",
    "        y_pred, _ = nn.test(X_test)\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}'.format(acc.mean(), acc.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 train loss: 3.4597 valid loss: 3.5155, valid accuracy: 0.1010\n",
      "Iter-2 train loss: 3.0996 valid loss: 3.3037, valid accuracy: 0.0998\n",
      "Iter-3 train loss: 3.0725 valid loss: 3.0973, valid accuracy: 0.1154\n",
      "Iter-4 train loss: 2.8372 valid loss: 2.9177, valid accuracy: 0.1386\n",
      "Iter-5 train loss: 2.8313 valid loss: 2.7748, valid accuracy: 0.1704\n",
      "Iter-6 train loss: 2.8796 valid loss: 2.6518, valid accuracy: 0.2014\n",
      "Iter-7 train loss: 2.3727 valid loss: 2.5377, valid accuracy: 0.2302\n",
      "Iter-8 train loss: 2.2528 valid loss: 2.4281, valid accuracy: 0.2642\n",
      "Iter-9 train loss: 2.2641 valid loss: 2.3234, valid accuracy: 0.2900\n",
      "Iter-10 train loss: 2.3026 valid loss: 2.2228, valid accuracy: 0.3208\n",
      "Iter-11 train loss: 2.1518 valid loss: 2.1244, valid accuracy: 0.3464\n",
      "Iter-12 train loss: 2.0770 valid loss: 2.0282, valid accuracy: 0.3710\n",
      "Iter-13 train loss: 2.1188 valid loss: 1.9337, valid accuracy: 0.3916\n",
      "Iter-14 train loss: 1.9553 valid loss: 1.8438, valid accuracy: 0.4140\n",
      "Iter-15 train loss: 1.5336 valid loss: 1.7567, valid accuracy: 0.4360\n",
      "Iter-16 train loss: 1.9046 valid loss: 1.6811, valid accuracy: 0.4576\n",
      "Iter-17 train loss: 1.6951 valid loss: 1.6105, valid accuracy: 0.4812\n",
      "Iter-18 train loss: 1.6890 valid loss: 1.5435, valid accuracy: 0.5040\n",
      "Iter-19 train loss: 1.6255 valid loss: 1.4811, valid accuracy: 0.5258\n",
      "Iter-20 train loss: 1.8933 valid loss: 1.4233, valid accuracy: 0.5410\n",
      "Last iteration - Test accuracy mean: 0.5580, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 20 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10 # number of kernels/ filters in each layer\n",
    "num_input_units = D # noise added at the input lavel\n",
    "num_output_units = C # number of classes in this classification problem\n",
    "p_dropout = 0.95 #  keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "lam = 1e-3 # reg at the feedback or loss function or function loss level as noise or loss_reg added.\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = CNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers, lam=lam)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b62d2c128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "# plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4VeW5/vHvEwU9oiAgKAWMA6iIMhUQhaNBrID9CXhE\nFI7BOgCtIrWOVPQQp3OkVY8DBxBFFK2lFBSwilKHiCgKihOYMInMqCg4UQHJ8/vjjTHEDDthJytZ\n+/5cVy732ntlr8ft5ub1Xe9g7o6IiMRLWtQFiIhI8incRURiSOEuIhJDCncRkRhSuIuIxJDCXUQk\nhhIKdzPrZWa5ZrbczG4o4ZwMM3vXzJaY2SvJLVNERMrDyhrnbmZpwHKgB7ARWARc4O65hc6pB7wB\nnOnuG8zsEHffUnlli4hIaRJpuXcGVrj7GnffBUwF+hY5ZxAww903ACjYRUSilUi4NwXWFTpen/9c\nYccADczsFTNbZGaZySpQRETKb98kvk8H4HSgDrDAzBa4+8okvb+IiJRDIuG+ATi80HGz/OcKWw9s\ncffvge/NbB7QFtgj3M1MC9mIiFSAu1t5zk+kW2YR0MLM0s2sNnABMLvIObOAbma2j5kdAJwE5JRQ\noH6S9DN69OjIa4jTjz5PfZbV9aciymy5u/tuMxsOzCX8ZTDJ3XPMbFh42Se6e66ZvQB8AOwGJrr7\nRxWqSERE9lpCfe7u/jxwbJHnHixyfBdwV/JKExGRitIM1RosIyMj6hJiRZ9n8uizjF6Zk5iSejEz\nr8rriYjEgZnh5byhmqyhkCJSQUcccQRr1qyJugypBtLT0/nkk0+S8l5quYtELL9VFnUZUg2U9F2o\nSMtdfe4iIjGkcBcRiSGFu4hIDCncRaRSrFmzhrS0NPLy8gA466yzePzxxxM6V/aewl1EitW7d2+y\nsrJ+9vysWbNo0qRJQkFs9tM9wOeee47MzJIXjC18ruw9hbuIFOuiiy7iiSee+NnzTzzxBJmZmaSl\npU581MTRTKnzX0dEyqVfv3588cUXzJ8/v+C5bdu28Y9//IPBgwcDoTXeoUMH6tWrR3p6OrfcckuJ\n79e9e3ceeeQRAPLy8rj22mtp1KgRLVq04Nlnny21ljFjxtCiRQvq1q3LCSecwMyZM/d4/aGHHuL4\n448veP29994DYP369Zx77rk0btyYRo0aMWLECABuueWWPf4vomi3UPfu3bnpppvo1q0bderUYfXq\n1Tz66KMF12jRogUTJ07co4ZZs2bRvn176tWrR8uWLZk7dy7Tp0+nY8eOe5x3zz33cM4555T675sU\nVbyymYvInqrzn4shQ4b4kCFDCo4nTJjg7du3Lzh+9dVXfcmSJe7u/uGHH/phhx3ms2bNcnf3Tz75\nxNPS0nz37t3u7p6RkeGTJk1yd/fx48d7q1atfMOGDb5161bv3r37HucWNX36dN+8ebO7u0+bNs3r\n1Kmzx3GzZs38nXfecXf3VatW+dq1a3337t3etm1bv+aaa/xf//qX79ixw19//XV3d8/KyvLMzMyC\n9y+u1vT0dM/JyfHdu3f7rl27/LnnnvPVq1e7u/u8efP8gAMO8Hfffdfd3d966y2vV6+ev/TSS+7u\nvnHjRl+2bJnv2LHDGzZs6Lm5uQXXat++vT/99NPF/nuW9F3If758eVveX9ibn+r8JRaJSll/LiA5\nPxUxf/58P/jgg33Hjh3u7t61a1e/9957Szz/qquu8quvvtrdSw/3008/3R988MGC35s7d26p4V5U\nu3btfPbs2e7u3rNnT7///vt/ds6CBQu8cePGxb5nIuE+evToUmvo169fwXWHDRtW8O9d1OWXX+43\n3XSTu7svWbLEGzRo4Dt37iz23GSGu7plRKq5ZMV7RXTt2pVGjRoxc+ZMPv74YxYtWsSgQYMKXl+4\ncCGnn346jRs35uCDD+bBBx9ky5ayt1DeuHEjzZs3LzhOT08v9fwpU6bQvn176tevT/369Vm6dGnB\nddatW8fRRx/9s99Zt24d6enpFb43ULg+gDlz5nDyySfTsGFD6tevz5w5c8qsAWDw4ME8+eSTQLhf\nMWDAAGrVqlWhmspD4S4ipcrMzOSxxx7jiSeeoGfPnjRq1KjgtUGDBtGvXz82bNjAtm3bGDZsWEI3\nH5s0acK6dT9tzVza2jpr165l6NChjBs3jq1bt7J161Zat25dcJ3mzZuzatWqn/1e8+bNWbt2bbGj\neurUqcP27dsLjjdt2vSzcwqP3tm5cyf9+/fn+uuv5/PPP2fr1q307t27zBoATjrpJGrXrs1rr73G\nk08+WeqIoWRSuItIqQYPHsyLL77Iww8/zEUXXbTHa99++y3169enVq1aLFy4sKCF+qOSgn7AgAHc\nf//9bNiwga1btzJmzJgSr//dd9+RlpbGIYccQl5eHpMnT2bJkiUFr1922WXcddddLF68GIBVq1ax\nbt06OnfuTJMmTRg5ciTbt29nx44dvPHGGwC0a9eOefPmsW7dOr766ivuvPPOUj+DnTt3snPnTg45\n5BDS0tKYM2cOc+fOLXj90ksvZfLkybzyyiu4Oxs3bmTZsmUFr2dmZjJ8+HBq167NKaecUuq1kkXh\nLiKlSk9P55RTTmH79u306dNnj9fGjRvHzTffTL169bj99ts5//zz93i9cOu38OMhQ4bQs2dP2rZt\nS8eOHTn33HNLvH6rVq245ppr6NKlC4cddhhLly6lW7duBa/379+fUaNGMWjQIOrWrcs555zDl19+\nSVpaGs888wwrVqzg8MMPp3nz5kybNg2AM844g/PPP582bdrQqVMnzj777BLrBjjwwAO5//77Oe+8\n82jQoAFTp06lb9++Ba936tSJyZMnc9VVV1GvXj0yMjJYu3ZtweuZmZksWbKkylrtoFUhRSKnVSHj\n7/vvv+fQQw9l8eLFJfbNg1aFFBGpUcaNG0enTp1KDfZk02YdIiKV6MgjjwT42cSryqZuGZGIqVtG\nfqRuGRERKZXCXUQkhhTuIiIxpBuqIhFLT0/XWuYClL0MQ3nohqqIpLTvvoOJE+Guu6BdOxg1Cqpo\nEmnCKnJDVS13EUlJX38N//d/cO+90K0bPPMMdOgQdVXJo3AXkZTy5Zdw330h2Hv2hJdfhtato64q\n+RK6oWpmvcws18yWm9kNxbx+mpltM7PF+T83Jb9UEZGK+/RTuOEGaNkS1q+HBQvgL3+JZ7BDAi13\nM0sDxgI9gI3AIjOb5e65RU6d5+59fvYGIiIRWr8e/vxnePxxGDgQFi+GJN63rLYSabl3Bla4+xp3\n3wVMBfoWc55u94tItbF6Nfz2t9CmDey7LyxZErpiUiHYIbFwbwqsK3S8Pv+5ok42s/fM7FkzOz4p\n1YmIlNOaNTBkCHTsCA0awLJlcPfd8ItfRF1Z1UrWDdV3gMPdfbuZ9QZmAsck6b1FRMq0bh3893/D\ntGmhxb58OTRsGHVV0Ukk3DcAhxc6bpb/XAF3/7bQ4zlmNs7MGrj7l0XfLCsrq+BxRkYGGRkZ5SxZ\nROQnGzbA//wPPPlkaLHn5kKhnQBrpOzsbLKzs/fqPcqcxGRm+wDLCDdUNwELgYHunlPonEPd/dP8\nx52Bae5+RDHvpUlMIpIUmzbBmDEwZQpccglcfz00bhx1VZWjUiYxuftuMxsOzCX00U9y9xwzGxZe\n9olAfzP7HbAL+BdwfsnvKCJScZ9+Cn/6E0yeDIMHw9Kl0KRJ1FVVP1p+QERqhC1bwpDGhx6C//xP\nGDkSmhY3tCOGtJ67iMTOF1/AjTfCscfCN9/A++/DAw+kTrBXlMJdRKqlrVvh5pvhmGNCq33xYhg3\nDpo3j7qymkHhLiLVyjffwK23hmUCNm6Et98OqzamyuSjZFG4i0i18P338L//Cy1ahDHqb74JkyZB\n/v7SUk5aFVJEIrVrFzz6aGit//KX8OKLcOKJUVdV8yncRSQSeXlhNul//VfoR58+HU46Keqq4kPh\nLiJVyh2efTbseLT//jB+PPToEXVV8aNwF5Eqk50dhjV+/TXccQf06QPaPrZyKNxFpNK9/XZoqa9Y\nEfrWBw6EffaJuqp402gZEak0H30E554LffvCOeeERb0uvFDBXhUU7iKSdJ98Ar/5DWRkQJcuocX+\n299C7doRF5ZCFO4ikjSrVsHQoWFIY3p6CPXrroMDDoi6stSjcBeRvbZ0aVjM66ST4NBDw+5Ht9wC\n9epFXVnqUriLSIW9/XboS+/RI0w8+vhjuO02OOSQqCsTjZYRkXKbNy8MZfzoo9Dt8pe/qOululG4\ni0hC3OH558M+pZs2hfXUBw/WTdLqSuEuIqXKy4Onnw6hvmNHmIQ0YADsq/So1vSfR0SK9cMP8Ne/\nhs2nDzwwrAFz9tmQpjt1NYLCXUT2sGNHWKVxzBg4/HC47z444wwtE1DTKNxFBIDt28P+pH/+M7Rp\nA48/Dl27Rl2VVJTCXSTFffttWJnxnnvCbNJZs8IkJKnZFO4iKeqrr2Ds2NDt0r07vPBCaLFLPCjc\nRVLM1q0h0MeOhd694dVXoVWrqKuSZNN9b5EU8fnnYRhjixawdi0sWBD61RXs8aRwF4m5zZvh2mvh\n2GPhyy/hnXfgkUegZcuoK5PKpHAXian162HECDj++DC88f33YcIEOOKIqCuTqqBwF4mZNWvgd78L\nN0dr1QorNj7wQNiEWlKHwl0kJj7+GC67DDp0gIMPDsvu3n03NGkSdWUSBYW7SA23ahVccgl06hSC\nfPnysGRAo0ZRVyZRSijczayXmeWa2XIzu6GU8zqZ2S4z+4/klSgixVm5Ei6+OGyQ0axZ2PXottug\nYcOoK5PqoMxwN7M0YCzQE2gNDDSz40o4707ghWQXKSI/Wbky7E/apctPW9ndeis0aBB1ZVKdJNJy\n7wyscPc17r4LmAr0Lea8K4HpwGdJrE9E8q1YARddFEL9yCNDyGdlQf36UVcm1VEi4d4UWFfoeH3+\ncwXM7BdAP3cfD2jtOJEkWrYMMjPh5JPh6KNDqI8eHW6aipQkWcsP3AsU7osvMeCzsrIKHmdkZJCR\nkZGkEkTiZdmy0If+wgthvPrYsdpwOlVkZ2eTnZ29V+9h7l76CWZdgCx375V/PBJwdx9T6JyPf3wI\nHAJ8Bwx199lF3svLup5IqsvNDaH+z3/C738PV14JdetGXZVEycxw93L1iiTSLbMIaGFm6WZWG7gA\n2CO03f2o/J8jCf3ulxcNdhEp3dKlMGgQnHoqtG4dul9GjVKwS8WUGe7uvhsYDswFlgJT3T3HzIaZ\n2dDifiXJNYrEljtkZ8Ovfx12OzrxxDBu/cYbFeqyd8rslknqxdQtIwKE/UmfeirsevT112Fhr8xM\n2H//qCuT6qgi3TJaz12kCm3fDpMnh12PDjssdLv06aNNpyX5FO4iVeDzz8Nol/Hjw76kjz8Op5wS\ndVUSZ2oviFSilSvh8svDWuqbNsFrr8HTTyvYpfIp3EUqwVtvQf/+YeJRgwaQkwMTJ4aQF6kK6pYR\nSZK8PHjuuXCTdM0auPpqePRROPDAqCuTVKRwF9lL7jBjRlgSYL/94Lrr4LzzYF/96ZII6esnshfm\nzw9h/v33YQTMmWeCaXUlqQYU7iIVkJsLI0fCu+/CHXeEmaUazijVib6OIuWweXPYn/Tf/x26dQuL\ne114oYJdqh99JUUS8O23Ye301q2hTp0Q6tdeqxmlUn0p3EVKsWsXTJgALVuGMevvvAN33aVdj6T6\nU5+7SDHcYdas0K/etCk8+yx06BB1VSKJU7iLFLFgQRgB8/XXcO+90LOnRsBIzaNuGZF8y5eHWaXn\nnw+XXRZGwvTqpWCXmknhLilvyxYYPjws6NWpU7hZ+pvfwD77RF2ZSMUp3CVl7dwZJh61ahWGMubk\nwA03wL/9W9SView99blLynGH2bPDUMZjjoF580LAi8SJwl1SygcfwB/+ECYjjR0bbpaKxJG6ZSQl\nfPopDB0Kv/pVuGn6/vsKdok3hbvE2o4d8Kc/hZmlBx0Ubpb+7ndasVHiT19xiSX3sAH1dddBmzZh\n7HrLllFXJVJ1FO4SO4sXh371bdvg4Yfh9NOjrkik6qlbRmJj0ya45BL49a/DSo2LFyvYJXUp3KXG\n+9e/wprqJ54IjRuHfvUhQzQJSVKbumWkRnv99TCbtE0bWLgQjjoq6opEqgeFu9RIO3aEPUsfewzG\nj4d+/aKuSKR6UbhLjfPeezB4MLRoEcarN24cdUUi1Y/63KXG+OGH0Ld+5plh6YAZMxTsIiVJKNzN\nrJeZ5ZrZcjO7oZjX+5jZ+2b2rpktNLOuyS9VUtny5WHP0uzssBvS4MFailekNGWGu5mlAWOBnkBr\nYKCZHVfktBfdva27twcuBR5OeqWSkvLy4IEH4JRTIDMTXngBmjePuiqR6i+RPvfOwAp3XwNgZlOB\nvkDujye4+/ZC5x8I5CWzSElNa9fCxRfD9u3wxhthBUcRSUwi3TJNgXWFjtfnP7cHM+tnZjnAM8Al\nySlPUpF7GAXzy1/CGWfAa68p2EXKK2mjZdx9JjDTzLoBtwO/Ku68rKysgscZGRlkZGQkqwSJgc8+\nC6s3fvwxvPgitG0bdUUiVS87O5vs7Oy9eg9z99JPMOsCZLl7r/zjkYC7+5hSfmcV0MndvyzyvJd1\nPUldTz0FV1wRJiVlZcF++0VdkUj1YGa4e7mGECTScl8EtDCzdGATcAEwsMiFj3b3VfmPOwC1iwa7\nSEm2bYMRI8LKjTNmhJunIrJ3yuxzd/fdwHBgLrAUmOruOWY2zMyG5p92rpktMbPFwAPAgEqrWGLj\nx+3u2rSBunXD5CQFu0hylNktk9SLqVtG8i1ZEpbl3bAhDHXs0SPqikSqr4p0y2iGqlSpLVvg8svD\nUrx9+oTlAxTsIsmncJcqsWsX3HsvtGoVtrjLzYUrr4RataKuTCSetHCYVCp3eO45uOYaOOIIePVV\nOP74qKsSiT+Fu1Sajz6Cq6+G1avhnnvgrLO0HoxIVVG3jCTdl1+GoY2nnQa9esGHH4at7xTsIlVH\n4S5Js2tXGPly3HFhed6cHLjqKqhdO+rKRFKPumUkKZ5/PnTBNG0KL78MJ5wQdUUiqU3hLntl2bIQ\n6suXw913w9lnq/tFpDpQt4xUiHsI865doXv3MCmpTx8Fu0h1oZa7lNvWrWFxr82b4e23wxBHEale\n1HKXclm0CDp0gCOPDOusK9hFqieFuyTEHe6/PwxpvOuuMNtUo2BEqi91y0iZvvoKLr00bKCxYAEc\nfXTUFYlIWdRyl1K9+27Y7q5x47CPqYJdpGZQuEux3GHCBDjzTLj9dhg3DvbfP+qqRCRR6paRn/nm\nGxg2LAxvnD8fjj026opEpLzUcpc9fPghdOwIBxwAb76pYBepqRTuAoRumEceCROSRo2Chx8OAS8i\nNZO6ZYTvvoMrroCFC2HePK23LhIHarmnuJwc6NwZ8vLCBCUFu0g8KNxTlDtMmQKnnhoW/nrsMahT\nJ+qqRCRZ1C2TgtauDZtUf/IJvPQStGkTdUUikmxquaeQ3bth7NiwNsxJJ8HixQp2kbhSyz1FLF0K\nQ4ZAWlpY8KtVq6grEpHKpJZ7zO3YAaNHQ0YGZGaG0TAKdpH4U8s9xl5/PbTWW7YMa8Q0axZ1RSJS\nVRTuMfT11/DHP8LMmXDffXDuudohSSTVqFsmZmbPhtatYefOsDZM//4KdpFUlFC4m1kvM8s1s+Vm\ndkMxrw8ys/fzf+ab2YnJL1VKs3kzDBgA11wTxq8/9BDUrx91VSISlTLD3czSgLFAT6A1MNDMjity\n2sfAqe7eFrgdeCjZhUrxflwTpk2bsNb6Bx+E9WFEJLUl0ufeGVjh7msAzGwq0BfI/fEEd3+z0Plv\nAk2TWaQUb+XKsDTvV1/B3LnQrl3UFYlIdZFIt0xTYF2h4/WUHt6XAXP2pigp3e7dYR/TLl3CnqZv\nvqlgF5E9JXW0jJl1By4GupV0TlZWVsHjjIwMMjIykllC7K1eDRddFG6SLlwIRx0VdUUikmzZ2dlk\nZ2fv1XuYu5d+glkXIMvde+UfjwTc3ccUOa8NMAPo5e6rSngvL+t6Ujx3mDQpDHEcORL+8Icw21RE\n4s/McPdyjXtLpOW+CGhhZunAJuACYGCRCx9OCPbMkoJdKm7z5jAZaf16eOUVOOGEqCsSkequzLaf\nu+8GhgNzgaXAVHfPMbNhZjY0/7SbgQbAODN718wWVlrFKWbGjNCf3rYtvPWWgl1EElNmt0xSL6Zu\nmYRt2wYjRoSbpVOmhJunIpKaKtIto17baujHNdYPOiisCaNgF5Hy0toy1cj27eGG6VNPhQ2qe/aM\nuiIRqanUcq8mFi0Km2hs2RJmmSrYRWRvqOUesV274I47YPx4eOCBsD6MiMjeUrhHKCcnbKDRuHHo\nW//FL6KuSETiQt0yEcjLC+usn3pqGL/+7LMKdhFJLrXcq9iGDWH5gO3bYcECaNEi6opEJI7Ucq9C\n06eHm6YZGWEvUwW7iFQWtdyrwDffhAlJ8+eHnZJOOinqikQk7tRyr2QLFoTlA/bZJ9w0VbCLSFVQ\ny72S/PAD3H47TJgQhjmec07UFYlIKlG4V4JVq+DCC6FuXVi8WCNhRKTqqVsmidxh8uSwFswFF8Cc\nOQp2EYmGWu5J8sUXYT/T5cvh5ZfhxBOjrkhEUpla7knw4ovhpml6etj6TsEuIlFTy30vfP893Hgj\nTJsGjz4KZ5wRdUUiIoHCvYKWLIFBg6BlS3j/fWjYMOqKRER+om6Zctq9O6wL07172KR6+nQFu4hU\nP2q5JygvLwT56NFw6KFh+7ujj466KhGR4incy+AOzzwDN98M++0XWu2/+hVYuXYzFBGpWgr3ErjD\nP/8JN90EO3bAbbfB2Wcr1EWkZlC4F2PevBDqn30Gt94K/ftDmu5OiEgNonAv5K23QvfLypWQlRVG\nw+yrT0hEaiC1R4H33oM+fUILvX9/WLYMBg9WsItIzZXS4Z6TEzak7t07TEBasQKGDoVataKuTERk\n76RkuK9aFVrmp50GHTuGbpgRI2D//aOuTEQkOVKm42H37nCjdMqUMLTxyitDqNetG3VlIiLJZ+5e\ndRcz86q8nntYyGvq1LD+S+PGMHAgXHqpZpWKSM1hZrh7uQZiJ9QtY2a9zCzXzJab2Q3FvH6smb1h\nZt+b2dXlKaAyLFkCo0aFDagzM0Pr/KWXwjZ311+vYBeR+CuzW8bM0oCxQA9gI7DIzGa5e26h074A\nrgT6VUqVCVi1KrTQp06FbdvCZhl//zu0b6+JRyKSehLpc+8MrHD3NQBmNhXoCxSEu7tvAbaY2f+r\nlCpLsHEj/O1vIdBXr4bzzgv7lZ5yiiYdiUhqSyTcmwLrCh2vJwR+hcybF9ZoKfxTu/bPj0tqbX/x\nBcyYAX/9a1hqt2/fMIu0Rw+NSxcR+VGVx+GoUWGtlsI/O3fuebxrVwj44kJ/0ybo2TMMXezdW8MX\nRUSKk0i4bwAOL3TcLP+5CunRI6vgcUZGBhkZGT87Jy/vp8AvGvzNm8NBB1X06iIi1V92djbZ2dl7\n9R5lDoU0s32AZYQbqpuAhcBAd88p5tzRwLfufncJ71WlQyFFROKgIkMhExrnbma9gPsIQycnufud\nZjYMcHefaGaHAm8DBwF5wLfA8e7+bZH3UbiLiJRTpYV7sijcRUTKr9ImMYmISM2icBcRiSGFu4hI\nDCncRURiSOEuIhJDCncRkRhSuIuIxJDCXUQkhhTuIiIxpHAXEYkhhbuISAwp3EVEYkjhLiISQwp3\nEZEYUriLiMSQwl1EJIYU7iIiMaRwFxGJIYW7iEgMKdxFRGJI4S4iEkMKdxGRGFK4i4jEkMJdRCSG\nFO4iIjGkcBcRiSGFu4hIDCncRURiKKFwN7NeZpZrZsvN7IYSzrnfzFaY2Xtm1i65ZYqISHmUGe5m\nlgaMBXoCrYGBZnZckXN6A0e7e0tgGDChEmqVIrKzs6MuIVb0eSaPPsvoJdJy7wyscPc17r4LmAr0\nLXJOX2AKgLu/BdQzs0OTWqn8jP4AJZc+z+TRZxm9RMK9KbCu0PH6/OdKO2dDMeeIiEgV0Q1VEZEY\nMncv/QSzLkCWu/fKPx4JuLuPKXTOBOAVd/9b/nEucJq7f1rkvUq/mIiIFMvdrTzn75vAOYuAFmaW\nDmwCLgAGFjlnNnAF8Lf8vwy2FQ32ihQnIiIVU2a4u/tuMxsOzCV040xy9xwzGxZe9onu/pyZnWVm\nK4HvgIsrt2wRESlNmd0yIiJS81TZDdVEJkJJ4szsEzN738zeNbOFUddTk5jZJDP71Mw+KPRcfTOb\na2bLzOwFM6sXZY01SQmf52gzW29mi/N/ekVZY01hZs3M7GUzW2pmH5rZiPzny/39rJJwT2QilJRb\nHpDh7u3dvXPUxdQwkwnfxcJGAi+6+7HAy8Afq7yqmqu4zxPgHnfvkP/zfFUXVUP9AFzt7q2Bk4Er\n8rOy3N/Pqmq5JzIRSsrH0FDWCnH3+cDWIk/3BR7Lf/wY0K9Ki6rBSvg8IXxHpRzcfbO7v5f/+Fsg\nB2hGBb6fVRUOiUyEkvJx4J9mtsjMhkRdTAw0/nGEl7tvBhpHXE8cDM9fa+phdXOVn5kdAbQD3gQO\nLe/3Uy2/mquru3cAziL8r1u3qAuKGY002DvjgKPcvR2wGbgn4npqFDM7EJgO/D6/BV/0+1jm97Oq\nwn0DcHih42b5z0kFufum/H9+DjxN6PqSivv0x/WQzOww4LOI66nR3P1z/2ko3kNApyjrqUnMbF9C\nsD/u7rPyny7397Oqwr1gIpSZ1SZMhJpdRdeOHTM7IP9vdsysDnAmsCTaqmocY88+4dnAb/IfXwTM\nKvoLUqo9Ps/8APrRf6DvZ3k8Anzk7vcVeq7c388qG+eePxTqPn6aCHVnlVw4hszsSEJr3QkT0f6i\nzzNxZvYkkAE0BD4FRgMzgb8DzYE1wAB33xZVjTVJCZ9nd0J/cR7wCTCsuFnrsicz6wrMAz4k/Pl2\n4EZgITCNcnw/NYlJRCSGdENVRCSGFO4iIjGkcBcRiSGFu4hIDCncRURiSOEuIhJDCncRkRhSuIuI\nxND/BwtC9e4HAAAAA0lEQVSrJZbdeBq1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7b62c746d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
