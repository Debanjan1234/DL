{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # row or hight\n",
    "# fh = 5 \n",
    "# fw = 4 \n",
    "# C = 3\n",
    "# i0 = np.arange(fh) # field hight\n",
    "# i0, i0.shape\n",
    "# i0 = np.repeat(i0, fw) # field width\n",
    "# i0, i0.shape\n",
    "# i0 = np.tile(i0, C) # C\n",
    "# i0.shape, i0\n",
    "# oh = 1\n",
    "# ow = 2\n",
    "# i1 = np.repeat(np.arange(oh), ow)\n",
    "# i1, i1.shape\n",
    "# i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "# i.shape, i1.shape, i0.shape, i1, i0, i[-4:], \n",
    "# # i.astype(int)\n",
    "# i.T.shape, i.T\n",
    "\n",
    "# # for cols or width\n",
    "# j0 = np.tile(np.arange(fw), fh * C)\n",
    "# j1 = np.tile(np.arange(ow), oh)\n",
    "# j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "# j0.shape, j1.shape, j.shape, j0, j1, j[:10]\n",
    "# j.T.shape, j.T\n",
    "\n",
    "# k = np.tile(np.arange(C), fh * fw).reshape(-1, 1).T\n",
    "# k.shape, k\n",
    "\n",
    "# X = np.random.rand(1, 2, 3, 4)\n",
    "# X.shape, X\n",
    "# np.pad(X, ((0, 0), (0, 0), (2, 2), (0, 0)), mode='constant')\n",
    "# # np.pad(X, ((0, 0), (0, 0)), mode='constant') # zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution impl\n",
    "# from impl.im2col import *\n",
    "# or\n",
    "# import impl.im2col as im2col\n",
    "# out_height = int(((H + (2 * pad) - kernel_height) / stride) + 1), \n",
    "# stride == 1, ALWAYS\n",
    "# pad == kernel//2, ALWAYS\n",
    "# kernel == min size ALWAYS, i.e. one past, one pres, one post (if exist), i.e. three or two\n",
    "# kernel == 3 or 2 ALWAYS\n",
    "# input=X, kernel=3or2, padding=kernel//2, stride=1, output=y\n",
    "def get_im2col_indices(X_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    # Input shape\n",
    "    N, C, H, W = X_shape\n",
    "    \n",
    "    # Kernel shape\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C\n",
    "    \n",
    "    # Output shape\n",
    "    assert (H + (2 * padding) - field_height) % stride == 0\n",
    "    assert (W + (2 * padding) - field_width) % stride == 0\n",
    "    out_height = int(((H + (2 * padding) - field_height) / stride) + 1)\n",
    "    out_width = int(((W + (2 * padding) - field_width) / stride) + 1)\n",
    "    out_C = 1 # the output channel/ depth\n",
    "\n",
    "    # Row, Height, i\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, field_C)\n",
    "    i1 = np.repeat(np.arange(out_height), out_width)\n",
    "    i1 = np.tile(i1, out_C)\n",
    "    i1 *= stride\n",
    "    \n",
    "    # Column, Width, j\n",
    "    j0 = np.tile(np.arange(field_width), field_height * field_C)\n",
    "    j1 = np.tile(np.arange(out_width), out_height * out_C)\n",
    "    j1 *= stride\n",
    "    \n",
    "    # Channel, Depth, K\n",
    "    k0 = np.repeat(np.arange(field_C), field_height * field_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 = np.repeat(np.arange(out_C), out_height * out_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 *= stride\n",
    "    \n",
    "    # Indices: i, j, k index\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = k0.reshape(-1, 1) + k1.reshape(1, -1)\n",
    "    \n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(X, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant') # X_NxCxHxW\n",
    "\n",
    "    k, i, j = get_im2col_indices(X.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    X_col = X_padded[:, k, i, j] # X_col_txkxn\n",
    "    \n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "    \n",
    "    X_col = X_col.transpose(1, 2, 0).reshape(kernel_size, -1)\n",
    "    \n",
    "    return X_col\n",
    "\n",
    "def col2im_indices(X_col, X_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = X_shape\n",
    "    H_padded, W_padded = H + (2 * padding), W + (2 * padding)\n",
    "    X_padded = np.zeros((N, C, H_padded, W_padded), dtype=X_col.dtype)\n",
    "    \n",
    "    k, i, j = get_im2col_indices(X_shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "\n",
    "    X_col = X_col.reshape(kernel_size, -1, N).transpose(2, 0, 1) # N, K, H * W\n",
    "    np.add.at(X_padded, (slice(None), k, i, j), X_col) # slice(None)== ':'\n",
    "    \n",
    "    return X_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    \n",
    "    # Input X\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    \n",
    "    # Kernel W\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "    \n",
    "    # Output\n",
    "    h_out = ((h_x + (2 * padding) - h_filter) / stride) + 1\n",
    "    w_out = ((w_x + (2 * padding) - w_filter) / stride) + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filter, -1)\n",
    "\n",
    "    out = (W_col @ X_col) + b\n",
    "    out = out.reshape(n_filter, h_out, w_out, n_x).transpose(3, 0, 1, 2)\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W = W.reshape(n_filter, -1)\n",
    "    dX_col = W.T @ dout\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob in this case. \n",
    "# Is this true in other cases as well? Yes.\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.arange(24).reshape(2,3,4)\n",
    "# a\n",
    "# # np.add.at(a, (slice(None), 0, 1, 2), cols_reshaped) # slice(None)== ':'\n",
    "# # a[:, 0, 1]\n",
    "# # a[0,:,1] \n",
    "# # # or \n",
    "# # a[0,slice(None),1],\n",
    "# # # outputs array([1, 5, 9])\n",
    "\n",
    "# # a[0,None,1] \n",
    "# # a[0, 1]\n",
    "# # # gives array([[4, 5, 6, 7]])\n",
    "\n",
    "# # # Could sb explain the latter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "# X_train[0, :10, :10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout): #, lam):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]} #, 'valid'}\n",
    "#         self.lam = lam\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = selu_dropout_forward(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy(y, y_train)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = selu_dropout_backward(dout=dy, cache=h_do_cache)\n",
    "            dh = selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 3.1620 validation accuracy: 0.146400\n",
      "Iter-2 training loss: 3.1547 validation accuracy: 0.162800\n",
      "Iter-3 training loss: 2.8213 validation accuracy: 0.190600\n",
      "Iter-4 training loss: 2.8988 validation accuracy: 0.213200\n",
      "Iter-5 training loss: 2.8277 validation accuracy: 0.238400\n",
      "Iter-6 training loss: 2.5567 validation accuracy: 0.273000\n",
      "Iter-7 training loss: 2.6064 validation accuracy: 0.299000\n",
      "Iter-8 training loss: 2.6393 validation accuracy: 0.329400\n",
      "Iter-9 training loss: 2.5585 validation accuracy: 0.356600\n",
      "Iter-10 training loss: 2.0187 validation accuracy: 0.378000\n",
      "Iter-11 training loss: 2.0411 validation accuracy: 0.402000\n",
      "Iter-12 training loss: 2.0907 validation accuracy: 0.425400\n",
      "Iter-13 training loss: 1.8452 validation accuracy: 0.447200\n",
      "Iter-14 training loss: 1.6393 validation accuracy: 0.476000\n",
      "Iter-15 training loss: 1.6912 validation accuracy: 0.505200\n",
      "Iter-16 training loss: 1.4405 validation accuracy: 0.529200\n",
      "Iter-17 training loss: 1.5059 validation accuracy: 0.549000\n",
      "Iter-18 training loss: 1.6269 validation accuracy: 0.568600\n",
      "Iter-19 training loss: 1.3246 validation accuracy: 0.585600\n",
      "Iter-20 training loss: 1.4622 validation accuracy: 0.603000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nchcRiaD/B3dqXk6pqspuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b0075358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 20 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10 # number of kernels/ filters in each layer\n",
    "# num_input_units = # noise added at the input lavel\n",
    "p_dropout = 0.95 #  keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "# lam = 1e-3 # reg at the feedback or loss function or function loss level as noise or loss_reg added.\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Kernel dead problem\n",
    "# y_pred = nn.test(X_test)\n",
    "# accs = np.mean(y_pred == y_test)\n",
    "# print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
