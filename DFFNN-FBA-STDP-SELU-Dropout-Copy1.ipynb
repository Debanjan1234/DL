{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.selu_forward(X=y)\n",
    "#         if train:\n",
    "#             y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#             caches.append((fc_cache, do_cache)) # caches[0]\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.selu_forward(X=y)\n",
    "#             if train:\n",
    "#                 y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#                 do_caches.append(do_cache)\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "#         if train:\n",
    "#             caches.append((fc_caches, do_caches)) # caches[1]\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "            caches.append((fc_cache, do_cache)) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache, do_cache = caches[2]\n",
    "        dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "#         fc_caches, do_caches = caches[1]\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.dropout_backward(cache=do_caches[layer], dout=dy)\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "#         fc_cache, do_cache = caches[0]\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3149 valid loss: 2.3169, valid accuracy: 0.1036\n",
      "Iter-200 train loss: 2.3083 valid loss: 2.3165, valid accuracy: 0.1038\n",
      "Iter-300 train loss: 2.3061 valid loss: 2.3162, valid accuracy: 0.1040\n",
      "Iter-400 train loss: 2.3044 valid loss: 2.3159, valid accuracy: 0.1044\n",
      "Iter-500 train loss: 2.3039 valid loss: 2.3155, valid accuracy: 0.1046\n",
      "Iter-600 train loss: 2.2960 valid loss: 2.3151, valid accuracy: 0.1046\n",
      "Iter-700 train loss: 2.3109 valid loss: 2.3147, valid accuracy: 0.1046\n",
      "Iter-800 train loss: 2.3041 valid loss: 2.3143, valid accuracy: 0.1046\n",
      "Iter-900 train loss: 2.3007 valid loss: 2.3140, valid accuracy: 0.1046\n",
      "Iter-1000 train loss: 2.3120 valid loss: 2.3137, valid accuracy: 0.1048\n",
      "Iter-1100 train loss: 2.2916 valid loss: 2.3133, valid accuracy: 0.1048\n",
      "Iter-1200 train loss: 2.3091 valid loss: 2.3129, valid accuracy: 0.1050\n",
      "Iter-1300 train loss: 2.2946 valid loss: 2.3126, valid accuracy: 0.1056\n",
      "Iter-1400 train loss: 2.3088 valid loss: 2.3122, valid accuracy: 0.1056\n",
      "Iter-1500 train loss: 2.3078 valid loss: 2.3119, valid accuracy: 0.1060\n",
      "Iter-1600 train loss: 2.3065 valid loss: 2.3115, valid accuracy: 0.1068\n",
      "Iter-1700 train loss: 2.2973 valid loss: 2.3111, valid accuracy: 0.1070\n",
      "Iter-1800 train loss: 2.3053 valid loss: 2.3107, valid accuracy: 0.1078\n",
      "Iter-1900 train loss: 2.3034 valid loss: 2.3104, valid accuracy: 0.1080\n",
      "Iter-2000 train loss: 2.3009 valid loss: 2.3101, valid accuracy: 0.1080\n",
      "Iter-2100 train loss: 2.2950 valid loss: 2.3097, valid accuracy: 0.1084\n",
      "Iter-2200 train loss: 2.3026 valid loss: 2.3093, valid accuracy: 0.1084\n",
      "Iter-2300 train loss: 2.3068 valid loss: 2.3089, valid accuracy: 0.1090\n",
      "Iter-2400 train loss: 2.3048 valid loss: 2.3085, valid accuracy: 0.1094\n",
      "Iter-2500 train loss: 2.2997 valid loss: 2.3082, valid accuracy: 0.1098\n",
      "Iter-2600 train loss: 2.2993 valid loss: 2.3079, valid accuracy: 0.1110\n",
      "Iter-2700 train loss: 2.3068 valid loss: 2.3076, valid accuracy: 0.1114\n",
      "Iter-2800 train loss: 2.3002 valid loss: 2.3072, valid accuracy: 0.1118\n",
      "Iter-2900 train loss: 2.3005 valid loss: 2.3069, valid accuracy: 0.1118\n",
      "Iter-3000 train loss: 2.3004 valid loss: 2.3065, valid accuracy: 0.1124\n",
      "Iter-3100 train loss: 2.3026 valid loss: 2.3062, valid accuracy: 0.1130\n",
      "Iter-3200 train loss: 2.3033 valid loss: 2.3058, valid accuracy: 0.1132\n",
      "Iter-3300 train loss: 2.3070 valid loss: 2.3054, valid accuracy: 0.1132\n",
      "Iter-3400 train loss: 2.3070 valid loss: 2.3050, valid accuracy: 0.1140\n",
      "Iter-3500 train loss: 2.3038 valid loss: 2.3047, valid accuracy: 0.1142\n",
      "Iter-3600 train loss: 2.3112 valid loss: 2.3044, valid accuracy: 0.1148\n",
      "Iter-3700 train loss: 2.3076 valid loss: 2.3040, valid accuracy: 0.1154\n",
      "Iter-3800 train loss: 2.3069 valid loss: 2.3037, valid accuracy: 0.1160\n",
      "Iter-3900 train loss: 2.3032 valid loss: 2.3033, valid accuracy: 0.1162\n",
      "Iter-4000 train loss: 2.3079 valid loss: 2.3030, valid accuracy: 0.1168\n",
      "Iter-4100 train loss: 2.3000 valid loss: 2.3027, valid accuracy: 0.1176\n",
      "Iter-4200 train loss: 2.3047 valid loss: 2.3024, valid accuracy: 0.1176\n",
      "Iter-4300 train loss: 2.3062 valid loss: 2.3020, valid accuracy: 0.1178\n",
      "Iter-4400 train loss: 2.3055 valid loss: 2.3017, valid accuracy: 0.1178\n",
      "Iter-4500 train loss: 2.2893 valid loss: 2.3014, valid accuracy: 0.1182\n",
      "Iter-4600 train loss: 2.2970 valid loss: 2.3011, valid accuracy: 0.1188\n",
      "Iter-4700 train loss: 2.3043 valid loss: 2.3007, valid accuracy: 0.1192\n",
      "Iter-4800 train loss: 2.2988 valid loss: 2.3004, valid accuracy: 0.1196\n",
      "Iter-4900 train loss: 2.2967 valid loss: 2.3000, valid accuracy: 0.1200\n",
      "Iter-5000 train loss: 2.2996 valid loss: 2.2997, valid accuracy: 0.1198\n",
      "Iter-5100 train loss: 2.3020 valid loss: 2.2994, valid accuracy: 0.1202\n",
      "Iter-5200 train loss: 2.3035 valid loss: 2.2990, valid accuracy: 0.1206\n",
      "Iter-5300 train loss: 2.3026 valid loss: 2.2987, valid accuracy: 0.1212\n",
      "Iter-5400 train loss: 2.2955 valid loss: 2.2984, valid accuracy: 0.1214\n",
      "Iter-5500 train loss: 2.3096 valid loss: 2.2980, valid accuracy: 0.1220\n",
      "Iter-5600 train loss: 2.2950 valid loss: 2.2977, valid accuracy: 0.1224\n",
      "Iter-5700 train loss: 2.2952 valid loss: 2.2974, valid accuracy: 0.1226\n",
      "Iter-5800 train loss: 2.3034 valid loss: 2.2970, valid accuracy: 0.1226\n",
      "Iter-5900 train loss: 2.2959 valid loss: 2.2967, valid accuracy: 0.1228\n",
      "Iter-6000 train loss: 2.3081 valid loss: 2.2964, valid accuracy: 0.1232\n",
      "Iter-6100 train loss: 2.3006 valid loss: 2.2961, valid accuracy: 0.1232\n",
      "Iter-6200 train loss: 2.2966 valid loss: 2.2957, valid accuracy: 0.1232\n",
      "Iter-6300 train loss: 2.2924 valid loss: 2.2954, valid accuracy: 0.1234\n",
      "Iter-6400 train loss: 2.3016 valid loss: 2.2951, valid accuracy: 0.1242\n",
      "Iter-6500 train loss: 2.3040 valid loss: 2.2947, valid accuracy: 0.1242\n",
      "Iter-6600 train loss: 2.2873 valid loss: 2.2944, valid accuracy: 0.1244\n",
      "Iter-6700 train loss: 2.2992 valid loss: 2.2941, valid accuracy: 0.1254\n",
      "Iter-6800 train loss: 2.2987 valid loss: 2.2937, valid accuracy: 0.1252\n",
      "Iter-6900 train loss: 2.3093 valid loss: 2.2934, valid accuracy: 0.1258\n",
      "Iter-7000 train loss: 2.3068 valid loss: 2.2931, valid accuracy: 0.1258\n",
      "Iter-7100 train loss: 2.3082 valid loss: 2.2928, valid accuracy: 0.1256\n",
      "Iter-7200 train loss: 2.3030 valid loss: 2.2924, valid accuracy: 0.1260\n",
      "Iter-7300 train loss: 2.3031 valid loss: 2.2921, valid accuracy: 0.1264\n",
      "Iter-7400 train loss: 2.3020 valid loss: 2.2918, valid accuracy: 0.1264\n",
      "Iter-7500 train loss: 2.3124 valid loss: 2.2915, valid accuracy: 0.1266\n",
      "Iter-7600 train loss: 2.2998 valid loss: 2.2912, valid accuracy: 0.1272\n",
      "Iter-7700 train loss: 2.2980 valid loss: 2.2909, valid accuracy: 0.1278\n",
      "Iter-7800 train loss: 2.3032 valid loss: 2.2905, valid accuracy: 0.1278\n",
      "Iter-7900 train loss: 2.3072 valid loss: 2.2902, valid accuracy: 0.1282\n",
      "Iter-8000 train loss: 2.3055 valid loss: 2.2898, valid accuracy: 0.1286\n",
      "Iter-8100 train loss: 2.3075 valid loss: 2.2895, valid accuracy: 0.1292\n",
      "Iter-8200 train loss: 2.3036 valid loss: 2.2892, valid accuracy: 0.1292\n",
      "Iter-8300 train loss: 2.3050 valid loss: 2.2889, valid accuracy: 0.1294\n",
      "Iter-8400 train loss: 2.2980 valid loss: 2.2886, valid accuracy: 0.1298\n",
      "Iter-8500 train loss: 2.3001 valid loss: 2.2882, valid accuracy: 0.1304\n",
      "Iter-8600 train loss: 2.3080 valid loss: 2.2879, valid accuracy: 0.1310\n",
      "Iter-8700 train loss: 2.3087 valid loss: 2.2876, valid accuracy: 0.1310\n",
      "Iter-8800 train loss: 2.3056 valid loss: 2.2872, valid accuracy: 0.1312\n",
      "Iter-8900 train loss: 2.3084 valid loss: 2.2869, valid accuracy: 0.1316\n",
      "Iter-9000 train loss: 2.3050 valid loss: 2.2865, valid accuracy: 0.1318\n",
      "Iter-9100 train loss: 2.3123 valid loss: 2.2863, valid accuracy: 0.1318\n",
      "Iter-9200 train loss: 2.3084 valid loss: 2.2859, valid accuracy: 0.1328\n",
      "Iter-9300 train loss: 2.2844 valid loss: 2.2856, valid accuracy: 0.1332\n",
      "Iter-9400 train loss: 2.3135 valid loss: 2.2853, valid accuracy: 0.1336\n",
      "Iter-9500 train loss: 2.3014 valid loss: 2.2850, valid accuracy: 0.1340\n",
      "Iter-9600 train loss: 2.2962 valid loss: 2.2847, valid accuracy: 0.1344\n",
      "Iter-9700 train loss: 2.3053 valid loss: 2.2844, valid accuracy: 0.1350\n",
      "Iter-9800 train loss: 2.3008 valid loss: 2.2841, valid accuracy: 0.1350\n",
      "Iter-9900 train loss: 2.3060 valid loss: 2.2837, valid accuracy: 0.1350\n",
      "Iter-10000 train loss: 2.3106 valid loss: 2.2834, valid accuracy: 0.1348\n",
      "Iter-10100 train loss: 2.2952 valid loss: 2.2831, valid accuracy: 0.1352\n",
      "Iter-10200 train loss: 2.3104 valid loss: 2.2827, valid accuracy: 0.1362\n",
      "Iter-10300 train loss: 2.3005 valid loss: 2.2824, valid accuracy: 0.1366\n",
      "Iter-10400 train loss: 2.3044 valid loss: 2.2821, valid accuracy: 0.1366\n",
      "Iter-10500 train loss: 2.3089 valid loss: 2.2817, valid accuracy: 0.1368\n",
      "Iter-10600 train loss: 2.2968 valid loss: 2.2814, valid accuracy: 0.1370\n",
      "Iter-10700 train loss: 2.2951 valid loss: 2.2811, valid accuracy: 0.1376\n",
      "Iter-10800 train loss: 2.2912 valid loss: 2.2808, valid accuracy: 0.1382\n",
      "Iter-10900 train loss: 2.3012 valid loss: 2.2805, valid accuracy: 0.1388\n",
      "Iter-11000 train loss: 2.3024 valid loss: 2.2802, valid accuracy: 0.1390\n",
      "Iter-11100 train loss: 2.2925 valid loss: 2.2799, valid accuracy: 0.1392\n",
      "Iter-11200 train loss: 2.2973 valid loss: 2.2795, valid accuracy: 0.1390\n",
      "Iter-11300 train loss: 2.2776 valid loss: 2.2792, valid accuracy: 0.1390\n",
      "Iter-11400 train loss: 2.2984 valid loss: 2.2789, valid accuracy: 0.1394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.3015 valid loss: 2.2786, valid accuracy: 0.1404\n",
      "Iter-11600 train loss: 2.2951 valid loss: 2.2783, valid accuracy: 0.1406\n",
      "Iter-11700 train loss: 2.3049 valid loss: 2.2780, valid accuracy: 0.1416\n",
      "Iter-11800 train loss: 2.2989 valid loss: 2.2777, valid accuracy: 0.1426\n",
      "Iter-11900 train loss: 2.3081 valid loss: 2.2774, valid accuracy: 0.1428\n",
      "Iter-12000 train loss: 2.3098 valid loss: 2.2770, valid accuracy: 0.1438\n",
      "Iter-12100 train loss: 2.3014 valid loss: 2.2767, valid accuracy: 0.1444\n",
      "Iter-12200 train loss: 2.3171 valid loss: 2.2764, valid accuracy: 0.1448\n",
      "Iter-12300 train loss: 2.3075 valid loss: 2.2760, valid accuracy: 0.1454\n",
      "Iter-12400 train loss: 2.3021 valid loss: 2.2756, valid accuracy: 0.1460\n",
      "Iter-12500 train loss: 2.3024 valid loss: 2.2753, valid accuracy: 0.1460\n",
      "Iter-12600 train loss: 2.3110 valid loss: 2.2750, valid accuracy: 0.1462\n",
      "Iter-12700 train loss: 2.2906 valid loss: 2.2747, valid accuracy: 0.1462\n",
      "Iter-12800 train loss: 2.2969 valid loss: 2.2743, valid accuracy: 0.1468\n",
      "Iter-12900 train loss: 2.3025 valid loss: 2.2740, valid accuracy: 0.1470\n",
      "Iter-13000 train loss: 2.3118 valid loss: 2.2737, valid accuracy: 0.1472\n",
      "Iter-13100 train loss: 2.3068 valid loss: 2.2734, valid accuracy: 0.1480\n",
      "Iter-13200 train loss: 2.3072 valid loss: 2.2730, valid accuracy: 0.1486\n",
      "Iter-13300 train loss: 2.3037 valid loss: 2.2727, valid accuracy: 0.1484\n",
      "Iter-13400 train loss: 2.3036 valid loss: 2.2724, valid accuracy: 0.1484\n",
      "Iter-13500 train loss: 2.3046 valid loss: 2.2720, valid accuracy: 0.1484\n",
      "Iter-13600 train loss: 2.3024 valid loss: 2.2717, valid accuracy: 0.1492\n",
      "Iter-13700 train loss: 2.3001 valid loss: 2.2714, valid accuracy: 0.1492\n",
      "Iter-13800 train loss: 2.3001 valid loss: 2.2711, valid accuracy: 0.1496\n",
      "Iter-13900 train loss: 2.3028 valid loss: 2.2708, valid accuracy: 0.1494\n",
      "Iter-14000 train loss: 2.3015 valid loss: 2.2705, valid accuracy: 0.1502\n",
      "Iter-14100 train loss: 2.2814 valid loss: 2.2702, valid accuracy: 0.1500\n",
      "Iter-14200 train loss: 2.2945 valid loss: 2.2700, valid accuracy: 0.1502\n",
      "Iter-14300 train loss: 2.2991 valid loss: 2.2696, valid accuracy: 0.1502\n",
      "Iter-14400 train loss: 2.2977 valid loss: 2.2693, valid accuracy: 0.1502\n",
      "Iter-14500 train loss: 2.3056 valid loss: 2.2690, valid accuracy: 0.1508\n",
      "Iter-14600 train loss: 2.2956 valid loss: 2.2687, valid accuracy: 0.1514\n",
      "Iter-14700 train loss: 2.3056 valid loss: 2.2683, valid accuracy: 0.1522\n",
      "Iter-14800 train loss: 2.3013 valid loss: 2.2680, valid accuracy: 0.1524\n",
      "Iter-14900 train loss: 2.3089 valid loss: 2.2677, valid accuracy: 0.1528\n",
      "Iter-15000 train loss: 2.3005 valid loss: 2.2674, valid accuracy: 0.1532\n",
      "Iter-15100 train loss: 2.2928 valid loss: 2.2671, valid accuracy: 0.1536\n",
      "Iter-15200 train loss: 2.3049 valid loss: 2.2668, valid accuracy: 0.1542\n",
      "Iter-15300 train loss: 2.2992 valid loss: 2.2664, valid accuracy: 0.1550\n",
      "Iter-15400 train loss: 2.3023 valid loss: 2.2662, valid accuracy: 0.1552\n",
      "Iter-15500 train loss: 2.3248 valid loss: 2.2659, valid accuracy: 0.1554\n",
      "Iter-15600 train loss: 2.2926 valid loss: 2.2655, valid accuracy: 0.1560\n",
      "Iter-15700 train loss: 2.2984 valid loss: 2.2653, valid accuracy: 0.1564\n",
      "Iter-15800 train loss: 2.3046 valid loss: 2.2650, valid accuracy: 0.1562\n",
      "Iter-15900 train loss: 2.2970 valid loss: 2.2646, valid accuracy: 0.1568\n",
      "Iter-16000 train loss: 2.2985 valid loss: 2.2643, valid accuracy: 0.1572\n",
      "Iter-16100 train loss: 2.3093 valid loss: 2.2641, valid accuracy: 0.1572\n",
      "Iter-16200 train loss: 2.2966 valid loss: 2.2638, valid accuracy: 0.1576\n",
      "Iter-16300 train loss: 2.3134 valid loss: 2.2635, valid accuracy: 0.1576\n",
      "Iter-16400 train loss: 2.2992 valid loss: 2.2631, valid accuracy: 0.1582\n",
      "Iter-16500 train loss: 2.3004 valid loss: 2.2628, valid accuracy: 0.1584\n",
      "Iter-16600 train loss: 2.3082 valid loss: 2.2625, valid accuracy: 0.1592\n",
      "Iter-16700 train loss: 2.2995 valid loss: 2.2622, valid accuracy: 0.1594\n",
      "Iter-16800 train loss: 2.3033 valid loss: 2.2618, valid accuracy: 0.1596\n",
      "Iter-16900 train loss: 2.3002 valid loss: 2.2615, valid accuracy: 0.1602\n",
      "Iter-17000 train loss: 2.3124 valid loss: 2.2612, valid accuracy: 0.1610\n",
      "Iter-17100 train loss: 2.2989 valid loss: 2.2609, valid accuracy: 0.1614\n",
      "Iter-17200 train loss: 2.3038 valid loss: 2.2605, valid accuracy: 0.1618\n",
      "Iter-17300 train loss: 2.2961 valid loss: 2.2602, valid accuracy: 0.1620\n",
      "Iter-17400 train loss: 2.3041 valid loss: 2.2600, valid accuracy: 0.1622\n",
      "Iter-17500 train loss: 2.2927 valid loss: 2.2596, valid accuracy: 0.1624\n",
      "Iter-17600 train loss: 2.2900 valid loss: 2.2593, valid accuracy: 0.1630\n",
      "Iter-17700 train loss: 2.3031 valid loss: 2.2590, valid accuracy: 0.1636\n",
      "Iter-17800 train loss: 2.2923 valid loss: 2.2588, valid accuracy: 0.1640\n",
      "Iter-17900 train loss: 2.2965 valid loss: 2.2585, valid accuracy: 0.1646\n",
      "Iter-18000 train loss: 2.2833 valid loss: 2.2582, valid accuracy: 0.1648\n",
      "Iter-18100 train loss: 2.3019 valid loss: 2.2579, valid accuracy: 0.1650\n",
      "Iter-18200 train loss: 2.3006 valid loss: 2.2576, valid accuracy: 0.1652\n",
      "Iter-18300 train loss: 2.3060 valid loss: 2.2572, valid accuracy: 0.1656\n",
      "Iter-18400 train loss: 2.3114 valid loss: 2.2570, valid accuracy: 0.1658\n",
      "Iter-18500 train loss: 2.2898 valid loss: 2.2566, valid accuracy: 0.1660\n",
      "Iter-18600 train loss: 2.2979 valid loss: 2.2563, valid accuracy: 0.1664\n",
      "Iter-18700 train loss: 2.3104 valid loss: 2.2560, valid accuracy: 0.1668\n",
      "Iter-18800 train loss: 2.2974 valid loss: 2.2557, valid accuracy: 0.1672\n",
      "Iter-18900 train loss: 2.3015 valid loss: 2.2554, valid accuracy: 0.1672\n",
      "Iter-19000 train loss: 2.3070 valid loss: 2.2550, valid accuracy: 0.1672\n",
      "Iter-19100 train loss: 2.2988 valid loss: 2.2547, valid accuracy: 0.1672\n",
      "Iter-19200 train loss: 2.2992 valid loss: 2.2544, valid accuracy: 0.1672\n",
      "Iter-19300 train loss: 2.3037 valid loss: 2.2541, valid accuracy: 0.1676\n",
      "Iter-19400 train loss: 2.2996 valid loss: 2.2537, valid accuracy: 0.1680\n",
      "Iter-19500 train loss: 2.3107 valid loss: 2.2534, valid accuracy: 0.1680\n",
      "Iter-19600 train loss: 2.3042 valid loss: 2.2531, valid accuracy: 0.1684\n",
      "Iter-19700 train loss: 2.2903 valid loss: 2.2528, valid accuracy: 0.1688\n",
      "Iter-19800 train loss: 2.2879 valid loss: 2.2525, valid accuracy: 0.1694\n",
      "Iter-19900 train loss: 2.3063 valid loss: 2.2521, valid accuracy: 0.1704\n",
      "Iter-20000 train loss: 2.2893 valid loss: 2.2518, valid accuracy: 0.1708\n",
      "Iter-20100 train loss: 2.2977 valid loss: 2.2516, valid accuracy: 0.1708\n",
      "Iter-20200 train loss: 2.2999 valid loss: 2.2512, valid accuracy: 0.1712\n",
      "Iter-20300 train loss: 2.3023 valid loss: 2.2509, valid accuracy: 0.1718\n",
      "Iter-20400 train loss: 2.3094 valid loss: 2.2506, valid accuracy: 0.1722\n",
      "Iter-20500 train loss: 2.3045 valid loss: 2.2503, valid accuracy: 0.1732\n",
      "Iter-20600 train loss: 2.2956 valid loss: 2.2499, valid accuracy: 0.1740\n",
      "Iter-20700 train loss: 2.3001 valid loss: 2.2497, valid accuracy: 0.1740\n",
      "Iter-20800 train loss: 2.2852 valid loss: 2.2493, valid accuracy: 0.1746\n",
      "Iter-20900 train loss: 2.3003 valid loss: 2.2490, valid accuracy: 0.1752\n",
      "Iter-21000 train loss: 2.2969 valid loss: 2.2487, valid accuracy: 0.1752\n",
      "Iter-21100 train loss: 2.3004 valid loss: 2.2484, valid accuracy: 0.1760\n",
      "Iter-21200 train loss: 2.3078 valid loss: 2.2481, valid accuracy: 0.1762\n",
      "Iter-21300 train loss: 2.2881 valid loss: 2.2478, valid accuracy: 0.1768\n",
      "Iter-21400 train loss: 2.3060 valid loss: 2.2475, valid accuracy: 0.1768\n",
      "Iter-21500 train loss: 2.3009 valid loss: 2.2472, valid accuracy: 0.1770\n",
      "Iter-21600 train loss: 2.2896 valid loss: 2.2468, valid accuracy: 0.1776\n",
      "Iter-21700 train loss: 2.2912 valid loss: 2.2466, valid accuracy: 0.1776\n",
      "Iter-21800 train loss: 2.3048 valid loss: 2.2463, valid accuracy: 0.1776\n",
      "Iter-21900 train loss: 2.3142 valid loss: 2.2460, valid accuracy: 0.1774\n",
      "Iter-22000 train loss: 2.2894 valid loss: 2.2456, valid accuracy: 0.1780\n",
      "Iter-22100 train loss: 2.2975 valid loss: 2.2453, valid accuracy: 0.1782\n",
      "Iter-22200 train loss: 2.3023 valid loss: 2.2451, valid accuracy: 0.1786\n",
      "Iter-22300 train loss: 2.3105 valid loss: 2.2447, valid accuracy: 0.1792\n",
      "Iter-22400 train loss: 2.3004 valid loss: 2.2444, valid accuracy: 0.1800\n",
      "Iter-22500 train loss: 2.2972 valid loss: 2.2441, valid accuracy: 0.1798\n",
      "Iter-22600 train loss: 2.2989 valid loss: 2.2437, valid accuracy: 0.1802\n",
      "Iter-22700 train loss: 2.3030 valid loss: 2.2434, valid accuracy: 0.1810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.3076 valid loss: 2.2431, valid accuracy: 0.1814\n",
      "Iter-22900 train loss: 2.3056 valid loss: 2.2428, valid accuracy: 0.1818\n",
      "Iter-23000 train loss: 2.3059 valid loss: 2.2425, valid accuracy: 0.1828\n",
      "Iter-23100 train loss: 2.3009 valid loss: 2.2422, valid accuracy: 0.1828\n",
      "Iter-23200 train loss: 2.2896 valid loss: 2.2419, valid accuracy: 0.1830\n",
      "Iter-23300 train loss: 2.3064 valid loss: 2.2415, valid accuracy: 0.1828\n",
      "Iter-23400 train loss: 2.3049 valid loss: 2.2413, valid accuracy: 0.1832\n",
      "Iter-23500 train loss: 2.2966 valid loss: 2.2409, valid accuracy: 0.1830\n",
      "Iter-23600 train loss: 2.3010 valid loss: 2.2406, valid accuracy: 0.1832\n",
      "Iter-23700 train loss: 2.2932 valid loss: 2.2403, valid accuracy: 0.1836\n",
      "Iter-23800 train loss: 2.3096 valid loss: 2.2400, valid accuracy: 0.1840\n",
      "Iter-23900 train loss: 2.3011 valid loss: 2.2397, valid accuracy: 0.1844\n",
      "Iter-24000 train loss: 2.2952 valid loss: 2.2394, valid accuracy: 0.1846\n",
      "Iter-24100 train loss: 2.3040 valid loss: 2.2391, valid accuracy: 0.1848\n",
      "Iter-24200 train loss: 2.2968 valid loss: 2.2388, valid accuracy: 0.1854\n",
      "Iter-24300 train loss: 2.2977 valid loss: 2.2385, valid accuracy: 0.1860\n",
      "Iter-24400 train loss: 2.2898 valid loss: 2.2383, valid accuracy: 0.1868\n",
      "Iter-24500 train loss: 2.3020 valid loss: 2.2379, valid accuracy: 0.1878\n",
      "Iter-24600 train loss: 2.2949 valid loss: 2.2376, valid accuracy: 0.1880\n",
      "Iter-24700 train loss: 2.3109 valid loss: 2.2373, valid accuracy: 0.1886\n",
      "Iter-24800 train loss: 2.3048 valid loss: 2.2370, valid accuracy: 0.1890\n",
      "Iter-24900 train loss: 2.2975 valid loss: 2.2367, valid accuracy: 0.1894\n",
      "Iter-25000 train loss: 2.3142 valid loss: 2.2365, valid accuracy: 0.1898\n",
      "Iter-25100 train loss: 2.3040 valid loss: 2.2361, valid accuracy: 0.1904\n",
      "Iter-25200 train loss: 2.3141 valid loss: 2.2358, valid accuracy: 0.1908\n",
      "Iter-25300 train loss: 2.2876 valid loss: 2.2356, valid accuracy: 0.1908\n",
      "Iter-25400 train loss: 2.2947 valid loss: 2.2353, valid accuracy: 0.1914\n",
      "Iter-25500 train loss: 2.3004 valid loss: 2.2350, valid accuracy: 0.1918\n",
      "Iter-25600 train loss: 2.2941 valid loss: 2.2347, valid accuracy: 0.1924\n",
      "Iter-25700 train loss: 2.2991 valid loss: 2.2344, valid accuracy: 0.1936\n",
      "Iter-25800 train loss: 2.2984 valid loss: 2.2340, valid accuracy: 0.1934\n",
      "Iter-25900 train loss: 2.2999 valid loss: 2.2337, valid accuracy: 0.1936\n",
      "Iter-26000 train loss: 2.2961 valid loss: 2.2334, valid accuracy: 0.1944\n",
      "Iter-26100 train loss: 2.3018 valid loss: 2.2331, valid accuracy: 0.1952\n",
      "Iter-26200 train loss: 2.2964 valid loss: 2.2328, valid accuracy: 0.1954\n",
      "Iter-26300 train loss: 2.3052 valid loss: 2.2325, valid accuracy: 0.1954\n",
      "Iter-26400 train loss: 2.3015 valid loss: 2.2322, valid accuracy: 0.1960\n",
      "Iter-26500 train loss: 2.2853 valid loss: 2.2319, valid accuracy: 0.1960\n",
      "Iter-26600 train loss: 2.2966 valid loss: 2.2316, valid accuracy: 0.1964\n",
      "Iter-26700 train loss: 2.2944 valid loss: 2.2313, valid accuracy: 0.1968\n",
      "Iter-26800 train loss: 2.3007 valid loss: 2.2310, valid accuracy: 0.1970\n",
      "Iter-26900 train loss: 2.3006 valid loss: 2.2307, valid accuracy: 0.1972\n",
      "Iter-27000 train loss: 2.3009 valid loss: 2.2305, valid accuracy: 0.1972\n",
      "Iter-27100 train loss: 2.3046 valid loss: 2.2302, valid accuracy: 0.1974\n",
      "Iter-27200 train loss: 2.2988 valid loss: 2.2299, valid accuracy: 0.1976\n",
      "Iter-27300 train loss: 2.3021 valid loss: 2.2296, valid accuracy: 0.1976\n",
      "Iter-27400 train loss: 2.3014 valid loss: 2.2294, valid accuracy: 0.1980\n",
      "Iter-27500 train loss: 2.2968 valid loss: 2.2291, valid accuracy: 0.1984\n",
      "Iter-27600 train loss: 2.3025 valid loss: 2.2288, valid accuracy: 0.1990\n",
      "Iter-27700 train loss: 2.3047 valid loss: 2.2285, valid accuracy: 0.1994\n",
      "Iter-27800 train loss: 2.2942 valid loss: 2.2282, valid accuracy: 0.1990\n",
      "Iter-27900 train loss: 2.3003 valid loss: 2.2279, valid accuracy: 0.1998\n",
      "Iter-28000 train loss: 2.3022 valid loss: 2.2276, valid accuracy: 0.1998\n",
      "Iter-28100 train loss: 2.3166 valid loss: 2.2273, valid accuracy: 0.2002\n",
      "Iter-28200 train loss: 2.3043 valid loss: 2.2270, valid accuracy: 0.2002\n",
      "Iter-28300 train loss: 2.2969 valid loss: 2.2268, valid accuracy: 0.2008\n",
      "Iter-28400 train loss: 2.2981 valid loss: 2.2265, valid accuracy: 0.2018\n",
      "Iter-28500 train loss: 2.3056 valid loss: 2.2262, valid accuracy: 0.2016\n",
      "Iter-28600 train loss: 2.2974 valid loss: 2.2259, valid accuracy: 0.2018\n",
      "Iter-28700 train loss: 2.3028 valid loss: 2.2256, valid accuracy: 0.2024\n",
      "Iter-28800 train loss: 2.2920 valid loss: 2.2253, valid accuracy: 0.2026\n",
      "Iter-28900 train loss: 2.3076 valid loss: 2.2250, valid accuracy: 0.2036\n",
      "Iter-29000 train loss: 2.3098 valid loss: 2.2247, valid accuracy: 0.2040\n",
      "Iter-29100 train loss: 2.2886 valid loss: 2.2244, valid accuracy: 0.2048\n",
      "Iter-29200 train loss: 2.2914 valid loss: 2.2241, valid accuracy: 0.2048\n",
      "Iter-29300 train loss: 2.2962 valid loss: 2.2239, valid accuracy: 0.2052\n",
      "Iter-29400 train loss: 2.3085 valid loss: 2.2235, valid accuracy: 0.2058\n",
      "Iter-29500 train loss: 2.2967 valid loss: 2.2233, valid accuracy: 0.2068\n",
      "Iter-29600 train loss: 2.2993 valid loss: 2.2230, valid accuracy: 0.2072\n",
      "Iter-29700 train loss: 2.3033 valid loss: 2.2227, valid accuracy: 0.2076\n",
      "Iter-29800 train loss: 2.3010 valid loss: 2.2224, valid accuracy: 0.2082\n",
      "Iter-29900 train loss: 2.3008 valid loss: 2.2221, valid accuracy: 0.2084\n",
      "Iter-30000 train loss: 2.2968 valid loss: 2.2218, valid accuracy: 0.2086\n",
      "Iter-30100 train loss: 2.2890 valid loss: 2.2215, valid accuracy: 0.2084\n",
      "Iter-30200 train loss: 2.2942 valid loss: 2.2212, valid accuracy: 0.2088\n",
      "Iter-30300 train loss: 2.2862 valid loss: 2.2210, valid accuracy: 0.2090\n",
      "Iter-30400 train loss: 2.2870 valid loss: 2.2207, valid accuracy: 0.2090\n",
      "Iter-30500 train loss: 2.2895 valid loss: 2.2204, valid accuracy: 0.2092\n",
      "Iter-30600 train loss: 2.3072 valid loss: 2.2201, valid accuracy: 0.2098\n",
      "Iter-30700 train loss: 2.3064 valid loss: 2.2198, valid accuracy: 0.2098\n",
      "Iter-30800 train loss: 2.2921 valid loss: 2.2195, valid accuracy: 0.2098\n",
      "Iter-30900 train loss: 2.3013 valid loss: 2.2192, valid accuracy: 0.2100\n",
      "Iter-31000 train loss: 2.3053 valid loss: 2.2189, valid accuracy: 0.2096\n",
      "Iter-31100 train loss: 2.3017 valid loss: 2.2187, valid accuracy: 0.2096\n",
      "Iter-31200 train loss: 2.3058 valid loss: 2.2184, valid accuracy: 0.2096\n",
      "Iter-31300 train loss: 2.3014 valid loss: 2.2180, valid accuracy: 0.2098\n",
      "Iter-31400 train loss: 2.3023 valid loss: 2.2177, valid accuracy: 0.2102\n",
      "Iter-31500 train loss: 2.2948 valid loss: 2.2174, valid accuracy: 0.2104\n",
      "Iter-31600 train loss: 2.3056 valid loss: 2.2171, valid accuracy: 0.2106\n",
      "Iter-31700 train loss: 2.3033 valid loss: 2.2168, valid accuracy: 0.2108\n",
      "Iter-31800 train loss: 2.2733 valid loss: 2.2165, valid accuracy: 0.2110\n",
      "Iter-31900 train loss: 2.2880 valid loss: 2.2163, valid accuracy: 0.2112\n",
      "Iter-32000 train loss: 2.3122 valid loss: 2.2160, valid accuracy: 0.2118\n",
      "Iter-32100 train loss: 2.3013 valid loss: 2.2156, valid accuracy: 0.2124\n",
      "Iter-32200 train loss: 2.2937 valid loss: 2.2154, valid accuracy: 0.2130\n",
      "Iter-32300 train loss: 2.2870 valid loss: 2.2151, valid accuracy: 0.2132\n",
      "Iter-32400 train loss: 2.2860 valid loss: 2.2148, valid accuracy: 0.2136\n",
      "Iter-32500 train loss: 2.2982 valid loss: 2.2145, valid accuracy: 0.2142\n",
      "Iter-32600 train loss: 2.3088 valid loss: 2.2142, valid accuracy: 0.2142\n",
      "Iter-32700 train loss: 2.2838 valid loss: 2.2139, valid accuracy: 0.2148\n",
      "Iter-32800 train loss: 2.3012 valid loss: 2.2136, valid accuracy: 0.2148\n",
      "Iter-32900 train loss: 2.2954 valid loss: 2.2133, valid accuracy: 0.2158\n",
      "Iter-33000 train loss: 2.3009 valid loss: 2.2130, valid accuracy: 0.2158\n",
      "Iter-33100 train loss: 2.2970 valid loss: 2.2127, valid accuracy: 0.2162\n",
      "Iter-33200 train loss: 2.2917 valid loss: 2.2124, valid accuracy: 0.2166\n",
      "Iter-33300 train loss: 2.2933 valid loss: 2.2121, valid accuracy: 0.2172\n",
      "Iter-33400 train loss: 2.2970 valid loss: 2.2118, valid accuracy: 0.2174\n",
      "Iter-33500 train loss: 2.3020 valid loss: 2.2115, valid accuracy: 0.2178\n",
      "Iter-33600 train loss: 2.3156 valid loss: 2.2112, valid accuracy: 0.2182\n",
      "Iter-33700 train loss: 2.2998 valid loss: 2.2109, valid accuracy: 0.2186\n",
      "Iter-33800 train loss: 2.2902 valid loss: 2.2107, valid accuracy: 0.2188\n",
      "Iter-33900 train loss: 2.3021 valid loss: 2.2104, valid accuracy: 0.2188\n",
      "Iter-34000 train loss: 2.2994 valid loss: 2.2101, valid accuracy: 0.2192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.3021 valid loss: 2.2098, valid accuracy: 0.2194\n",
      "Iter-34200 train loss: 2.3087 valid loss: 2.2096, valid accuracy: 0.2200\n",
      "Iter-34300 train loss: 2.2928 valid loss: 2.2093, valid accuracy: 0.2204\n",
      "Iter-34400 train loss: 2.2982 valid loss: 2.2089, valid accuracy: 0.2218\n",
      "Iter-34500 train loss: 2.2946 valid loss: 2.2086, valid accuracy: 0.2224\n",
      "Iter-34600 train loss: 2.3005 valid loss: 2.2083, valid accuracy: 0.2228\n",
      "Iter-34700 train loss: 2.2936 valid loss: 2.2080, valid accuracy: 0.2230\n",
      "Iter-34800 train loss: 2.3029 valid loss: 2.2077, valid accuracy: 0.2240\n",
      "Iter-34900 train loss: 2.2982 valid loss: 2.2074, valid accuracy: 0.2242\n",
      "Iter-35000 train loss: 2.3041 valid loss: 2.2072, valid accuracy: 0.2242\n",
      "Iter-35100 train loss: 2.3096 valid loss: 2.2069, valid accuracy: 0.2246\n",
      "Iter-35200 train loss: 2.2977 valid loss: 2.2066, valid accuracy: 0.2248\n",
      "Iter-35300 train loss: 2.3028 valid loss: 2.2064, valid accuracy: 0.2252\n",
      "Iter-35400 train loss: 2.3023 valid loss: 2.2061, valid accuracy: 0.2256\n",
      "Iter-35500 train loss: 2.3035 valid loss: 2.2058, valid accuracy: 0.2260\n",
      "Iter-35600 train loss: 2.2970 valid loss: 2.2056, valid accuracy: 0.2262\n",
      "Iter-35700 train loss: 2.2945 valid loss: 2.2053, valid accuracy: 0.2266\n",
      "Iter-35800 train loss: 2.3023 valid loss: 2.2050, valid accuracy: 0.2268\n",
      "Iter-35900 train loss: 2.2987 valid loss: 2.2047, valid accuracy: 0.2270\n",
      "Iter-36000 train loss: 2.2991 valid loss: 2.2044, valid accuracy: 0.2274\n",
      "Iter-36100 train loss: 2.2892 valid loss: 2.2041, valid accuracy: 0.2278\n",
      "Iter-36200 train loss: 2.2922 valid loss: 2.2039, valid accuracy: 0.2280\n",
      "Iter-36300 train loss: 2.2993 valid loss: 2.2035, valid accuracy: 0.2282\n",
      "Iter-36400 train loss: 2.3028 valid loss: 2.2032, valid accuracy: 0.2284\n",
      "Iter-36500 train loss: 2.3119 valid loss: 2.2029, valid accuracy: 0.2284\n",
      "Iter-36600 train loss: 2.3034 valid loss: 2.2026, valid accuracy: 0.2290\n",
      "Iter-36700 train loss: 2.2997 valid loss: 2.2023, valid accuracy: 0.2294\n",
      "Iter-36800 train loss: 2.3036 valid loss: 2.2020, valid accuracy: 0.2298\n",
      "Iter-36900 train loss: 2.3057 valid loss: 2.2017, valid accuracy: 0.2300\n",
      "Iter-37000 train loss: 2.2949 valid loss: 2.2015, valid accuracy: 0.2302\n",
      "Iter-37100 train loss: 2.2800 valid loss: 2.2012, valid accuracy: 0.2304\n",
      "Iter-37200 train loss: 2.3078 valid loss: 2.2009, valid accuracy: 0.2312\n",
      "Iter-37300 train loss: 2.2897 valid loss: 2.2006, valid accuracy: 0.2316\n",
      "Iter-37400 train loss: 2.3021 valid loss: 2.2004, valid accuracy: 0.2316\n",
      "Iter-37500 train loss: 2.2896 valid loss: 2.2001, valid accuracy: 0.2324\n",
      "Iter-37600 train loss: 2.3037 valid loss: 2.1999, valid accuracy: 0.2324\n",
      "Iter-37700 train loss: 2.2902 valid loss: 2.1996, valid accuracy: 0.2328\n",
      "Iter-37800 train loss: 2.2977 valid loss: 2.1993, valid accuracy: 0.2330\n",
      "Iter-37900 train loss: 2.3002 valid loss: 2.1990, valid accuracy: 0.2338\n",
      "Iter-38000 train loss: 2.2963 valid loss: 2.1987, valid accuracy: 0.2344\n",
      "Iter-38100 train loss: 2.3044 valid loss: 2.1984, valid accuracy: 0.2348\n",
      "Iter-38200 train loss: 2.2980 valid loss: 2.1981, valid accuracy: 0.2350\n",
      "Iter-38300 train loss: 2.3040 valid loss: 2.1979, valid accuracy: 0.2356\n",
      "Iter-38400 train loss: 2.2937 valid loss: 2.1976, valid accuracy: 0.2360\n",
      "Iter-38500 train loss: 2.2924 valid loss: 2.1973, valid accuracy: 0.2360\n",
      "Iter-38600 train loss: 2.2859 valid loss: 2.1970, valid accuracy: 0.2366\n",
      "Iter-38700 train loss: 2.2938 valid loss: 2.1968, valid accuracy: 0.2362\n",
      "Iter-38800 train loss: 2.3020 valid loss: 2.1965, valid accuracy: 0.2364\n",
      "Iter-38900 train loss: 2.2911 valid loss: 2.1962, valid accuracy: 0.2364\n",
      "Iter-39000 train loss: 2.2934 valid loss: 2.1959, valid accuracy: 0.2368\n",
      "Iter-39100 train loss: 2.3059 valid loss: 2.1956, valid accuracy: 0.2374\n",
      "Iter-39200 train loss: 2.2995 valid loss: 2.1952, valid accuracy: 0.2382\n",
      "Iter-39300 train loss: 2.2867 valid loss: 2.1950, valid accuracy: 0.2384\n",
      "Iter-39400 train loss: 2.3038 valid loss: 2.1947, valid accuracy: 0.2388\n",
      "Iter-39500 train loss: 2.2834 valid loss: 2.1944, valid accuracy: 0.2392\n",
      "Iter-39600 train loss: 2.2862 valid loss: 2.1941, valid accuracy: 0.2392\n",
      "Iter-39700 train loss: 2.2941 valid loss: 2.1939, valid accuracy: 0.2398\n",
      "Iter-39800 train loss: 2.3127 valid loss: 2.1936, valid accuracy: 0.2404\n",
      "Iter-39900 train loss: 2.3045 valid loss: 2.1933, valid accuracy: 0.2408\n",
      "Iter-40000 train loss: 2.2958 valid loss: 2.1931, valid accuracy: 0.2408\n",
      "Iter-40100 train loss: 2.2980 valid loss: 2.1928, valid accuracy: 0.2418\n",
      "Iter-40200 train loss: 2.2932 valid loss: 2.1925, valid accuracy: 0.2424\n",
      "Iter-40300 train loss: 2.2968 valid loss: 2.1922, valid accuracy: 0.2426\n",
      "Iter-40400 train loss: 2.3067 valid loss: 2.1919, valid accuracy: 0.2432\n",
      "Iter-40500 train loss: 2.3030 valid loss: 2.1916, valid accuracy: 0.2440\n",
      "Iter-40600 train loss: 2.3050 valid loss: 2.1914, valid accuracy: 0.2440\n",
      "Iter-40700 train loss: 2.3093 valid loss: 2.1911, valid accuracy: 0.2440\n",
      "Iter-40800 train loss: 2.2760 valid loss: 2.1908, valid accuracy: 0.2444\n",
      "Iter-40900 train loss: 2.3013 valid loss: 2.1905, valid accuracy: 0.2444\n",
      "Iter-41000 train loss: 2.3077 valid loss: 2.1902, valid accuracy: 0.2452\n",
      "Iter-41100 train loss: 2.2995 valid loss: 2.1899, valid accuracy: 0.2454\n",
      "Iter-41200 train loss: 2.2803 valid loss: 2.1896, valid accuracy: 0.2458\n",
      "Iter-41300 train loss: 2.3013 valid loss: 2.1893, valid accuracy: 0.2462\n",
      "Iter-41400 train loss: 2.2954 valid loss: 2.1891, valid accuracy: 0.2466\n",
      "Iter-41500 train loss: 2.2678 valid loss: 2.1888, valid accuracy: 0.2468\n",
      "Iter-41600 train loss: 2.2932 valid loss: 2.1885, valid accuracy: 0.2474\n",
      "Iter-41700 train loss: 2.3017 valid loss: 2.1882, valid accuracy: 0.2476\n",
      "Iter-41800 train loss: 2.3017 valid loss: 2.1879, valid accuracy: 0.2482\n",
      "Iter-41900 train loss: 2.3011 valid loss: 2.1877, valid accuracy: 0.2484\n",
      "Iter-42000 train loss: 2.2976 valid loss: 2.1873, valid accuracy: 0.2488\n",
      "Iter-42100 train loss: 2.2920 valid loss: 2.1870, valid accuracy: 0.2490\n",
      "Iter-42200 train loss: 2.2872 valid loss: 2.1868, valid accuracy: 0.2496\n",
      "Iter-42300 train loss: 2.2868 valid loss: 2.1865, valid accuracy: 0.2504\n",
      "Iter-42400 train loss: 2.3005 valid loss: 2.1862, valid accuracy: 0.2504\n",
      "Iter-42500 train loss: 2.3050 valid loss: 2.1860, valid accuracy: 0.2504\n",
      "Iter-42600 train loss: 2.3008 valid loss: 2.1857, valid accuracy: 0.2506\n",
      "Iter-42700 train loss: 2.3049 valid loss: 2.1854, valid accuracy: 0.2510\n",
      "Iter-42800 train loss: 2.3054 valid loss: 2.1852, valid accuracy: 0.2508\n",
      "Iter-42900 train loss: 2.3176 valid loss: 2.1849, valid accuracy: 0.2508\n",
      "Iter-43000 train loss: 2.3023 valid loss: 2.1846, valid accuracy: 0.2516\n",
      "Iter-43100 train loss: 2.2896 valid loss: 2.1843, valid accuracy: 0.2518\n",
      "Iter-43200 train loss: 2.3144 valid loss: 2.1840, valid accuracy: 0.2518\n",
      "Iter-43300 train loss: 2.3179 valid loss: 2.1837, valid accuracy: 0.2524\n",
      "Iter-43400 train loss: 2.2926 valid loss: 2.1834, valid accuracy: 0.2526\n",
      "Iter-43500 train loss: 2.2851 valid loss: 2.1832, valid accuracy: 0.2526\n",
      "Iter-43600 train loss: 2.2997 valid loss: 2.1829, valid accuracy: 0.2526\n",
      "Iter-43700 train loss: 2.2985 valid loss: 2.1826, valid accuracy: 0.2526\n",
      "Iter-43800 train loss: 2.2948 valid loss: 2.1823, valid accuracy: 0.2532\n",
      "Iter-43900 train loss: 2.2945 valid loss: 2.1820, valid accuracy: 0.2532\n",
      "Iter-44000 train loss: 2.2964 valid loss: 2.1817, valid accuracy: 0.2536\n",
      "Iter-44100 train loss: 2.2986 valid loss: 2.1814, valid accuracy: 0.2540\n",
      "Iter-44200 train loss: 2.3052 valid loss: 2.1812, valid accuracy: 0.2546\n",
      "Iter-44300 train loss: 2.2989 valid loss: 2.1809, valid accuracy: 0.2550\n",
      "Iter-44400 train loss: 2.2895 valid loss: 2.1806, valid accuracy: 0.2554\n",
      "Iter-44500 train loss: 2.2973 valid loss: 2.1804, valid accuracy: 0.2556\n",
      "Iter-44600 train loss: 2.3100 valid loss: 2.1801, valid accuracy: 0.2556\n",
      "Iter-44700 train loss: 2.2960 valid loss: 2.1798, valid accuracy: 0.2560\n",
      "Iter-44800 train loss: 2.2770 valid loss: 2.1796, valid accuracy: 0.2562\n",
      "Iter-44900 train loss: 2.3036 valid loss: 2.1793, valid accuracy: 0.2566\n",
      "Iter-45000 train loss: 2.3070 valid loss: 2.1790, valid accuracy: 0.2566\n",
      "Iter-45100 train loss: 2.3004 valid loss: 2.1788, valid accuracy: 0.2566\n",
      "Iter-45200 train loss: 2.2881 valid loss: 2.1785, valid accuracy: 0.2574\n",
      "Iter-45300 train loss: 2.3074 valid loss: 2.1782, valid accuracy: 0.2574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 2.2990 valid loss: 2.1779, valid accuracy: 0.2576\n",
      "Iter-45500 train loss: 2.3062 valid loss: 2.1776, valid accuracy: 0.2574\n",
      "Iter-45600 train loss: 2.2980 valid loss: 2.1773, valid accuracy: 0.2580\n",
      "Iter-45700 train loss: 2.2918 valid loss: 2.1770, valid accuracy: 0.2588\n",
      "Iter-45800 train loss: 2.2843 valid loss: 2.1767, valid accuracy: 0.2592\n",
      "Iter-45900 train loss: 2.2924 valid loss: 2.1765, valid accuracy: 0.2594\n",
      "Iter-46000 train loss: 2.3106 valid loss: 2.1762, valid accuracy: 0.2594\n",
      "Iter-46100 train loss: 2.3041 valid loss: 2.1759, valid accuracy: 0.2598\n",
      "Iter-46200 train loss: 2.3030 valid loss: 2.1756, valid accuracy: 0.2598\n",
      "Iter-46300 train loss: 2.2923 valid loss: 2.1754, valid accuracy: 0.2600\n",
      "Iter-46400 train loss: 2.3065 valid loss: 2.1751, valid accuracy: 0.2604\n",
      "Iter-46500 train loss: 2.2892 valid loss: 2.1749, valid accuracy: 0.2606\n",
      "Iter-46600 train loss: 2.3184 valid loss: 2.1747, valid accuracy: 0.2610\n",
      "Iter-46700 train loss: 2.3012 valid loss: 2.1744, valid accuracy: 0.2616\n",
      "Iter-46800 train loss: 2.2951 valid loss: 2.1742, valid accuracy: 0.2620\n",
      "Iter-46900 train loss: 2.3005 valid loss: 2.1739, valid accuracy: 0.2626\n",
      "Iter-47000 train loss: 2.2923 valid loss: 2.1736, valid accuracy: 0.2624\n",
      "Iter-47100 train loss: 2.2988 valid loss: 2.1734, valid accuracy: 0.2628\n",
      "Iter-47200 train loss: 2.2942 valid loss: 2.1731, valid accuracy: 0.2632\n",
      "Iter-47300 train loss: 2.2923 valid loss: 2.1728, valid accuracy: 0.2634\n",
      "Iter-47400 train loss: 2.2840 valid loss: 2.1726, valid accuracy: 0.2634\n",
      "Iter-47500 train loss: 2.3068 valid loss: 2.1723, valid accuracy: 0.2640\n",
      "Iter-47600 train loss: 2.2949 valid loss: 2.1721, valid accuracy: 0.2648\n",
      "Iter-47700 train loss: 2.2936 valid loss: 2.1718, valid accuracy: 0.2648\n",
      "Iter-47800 train loss: 2.3017 valid loss: 2.1715, valid accuracy: 0.2654\n",
      "Iter-47900 train loss: 2.3006 valid loss: 2.1713, valid accuracy: 0.2656\n",
      "Iter-48000 train loss: 2.2859 valid loss: 2.1710, valid accuracy: 0.2656\n",
      "Iter-48100 train loss: 2.2964 valid loss: 2.1707, valid accuracy: 0.2664\n",
      "Iter-48200 train loss: 2.2869 valid loss: 2.1704, valid accuracy: 0.2666\n",
      "Iter-48300 train loss: 2.2668 valid loss: 2.1701, valid accuracy: 0.2664\n",
      "Iter-48400 train loss: 2.2977 valid loss: 2.1698, valid accuracy: 0.2666\n",
      "Iter-48500 train loss: 2.3032 valid loss: 2.1695, valid accuracy: 0.2668\n",
      "Iter-48600 train loss: 2.2875 valid loss: 2.1693, valid accuracy: 0.2672\n",
      "Iter-48700 train loss: 2.3005 valid loss: 2.1690, valid accuracy: 0.2676\n",
      "Iter-48800 train loss: 2.2820 valid loss: 2.1687, valid accuracy: 0.2676\n",
      "Iter-48900 train loss: 2.3040 valid loss: 2.1684, valid accuracy: 0.2678\n",
      "Iter-49000 train loss: 2.2943 valid loss: 2.1682, valid accuracy: 0.2682\n",
      "Iter-49100 train loss: 2.2882 valid loss: 2.1679, valid accuracy: 0.2684\n",
      "Iter-49200 train loss: 2.2975 valid loss: 2.1676, valid accuracy: 0.2684\n",
      "Iter-49300 train loss: 2.2979 valid loss: 2.1674, valid accuracy: 0.2688\n",
      "Iter-49400 train loss: 2.2900 valid loss: 2.1671, valid accuracy: 0.2694\n",
      "Iter-49500 train loss: 2.2986 valid loss: 2.1668, valid accuracy: 0.2698\n",
      "Iter-49600 train loss: 2.2957 valid loss: 2.1665, valid accuracy: 0.2700\n",
      "Iter-49700 train loss: 2.2795 valid loss: 2.1663, valid accuracy: 0.2700\n",
      "Iter-49800 train loss: 2.2836 valid loss: 2.1660, valid accuracy: 0.2706\n",
      "Iter-49900 train loss: 2.2899 valid loss: 2.1657, valid accuracy: 0.2704\n",
      "Iter-50000 train loss: 2.3003 valid loss: 2.1654, valid accuracy: 0.2704\n",
      "Iter-50100 train loss: 2.3022 valid loss: 2.1651, valid accuracy: 0.2704\n",
      "Iter-50200 train loss: 2.2876 valid loss: 2.1649, valid accuracy: 0.2706\n",
      "Iter-50300 train loss: 2.2996 valid loss: 2.1646, valid accuracy: 0.2712\n",
      "Iter-50400 train loss: 2.2900 valid loss: 2.1644, valid accuracy: 0.2716\n",
      "Iter-50500 train loss: 2.2950 valid loss: 2.1641, valid accuracy: 0.2716\n",
      "Iter-50600 train loss: 2.3037 valid loss: 2.1638, valid accuracy: 0.2716\n",
      "Iter-50700 train loss: 2.2997 valid loss: 2.1636, valid accuracy: 0.2720\n",
      "Iter-50800 train loss: 2.2920 valid loss: 2.1633, valid accuracy: 0.2716\n",
      "Iter-50900 train loss: 2.2959 valid loss: 2.1631, valid accuracy: 0.2722\n",
      "Iter-51000 train loss: 2.2981 valid loss: 2.1628, valid accuracy: 0.2722\n",
      "Iter-51100 train loss: 2.2886 valid loss: 2.1625, valid accuracy: 0.2724\n",
      "Iter-51200 train loss: 2.2978 valid loss: 2.1623, valid accuracy: 0.2722\n",
      "Iter-51300 train loss: 2.2886 valid loss: 2.1620, valid accuracy: 0.2724\n",
      "Iter-51400 train loss: 2.2874 valid loss: 2.1618, valid accuracy: 0.2730\n",
      "Iter-51500 train loss: 2.2945 valid loss: 2.1615, valid accuracy: 0.2730\n",
      "Iter-51600 train loss: 2.2836 valid loss: 2.1613, valid accuracy: 0.2730\n",
      "Iter-51700 train loss: 2.2970 valid loss: 2.1610, valid accuracy: 0.2730\n",
      "Iter-51800 train loss: 2.2970 valid loss: 2.1607, valid accuracy: 0.2736\n",
      "Iter-51900 train loss: 2.2993 valid loss: 2.1605, valid accuracy: 0.2736\n",
      "Iter-52000 train loss: 2.3081 valid loss: 2.1602, valid accuracy: 0.2736\n",
      "Iter-52100 train loss: 2.3005 valid loss: 2.1600, valid accuracy: 0.2738\n",
      "Iter-52200 train loss: 2.3097 valid loss: 2.1597, valid accuracy: 0.2742\n",
      "Iter-52300 train loss: 2.3019 valid loss: 2.1594, valid accuracy: 0.2744\n",
      "Iter-52400 train loss: 2.2995 valid loss: 2.1591, valid accuracy: 0.2748\n",
      "Iter-52500 train loss: 2.3019 valid loss: 2.1589, valid accuracy: 0.2752\n",
      "Iter-52600 train loss: 2.3055 valid loss: 2.1586, valid accuracy: 0.2758\n",
      "Iter-52700 train loss: 2.2700 valid loss: 2.1583, valid accuracy: 0.2758\n",
      "Iter-52800 train loss: 2.3045 valid loss: 2.1581, valid accuracy: 0.2762\n",
      "Iter-52900 train loss: 2.2957 valid loss: 2.1578, valid accuracy: 0.2768\n",
      "Iter-53000 train loss: 2.2822 valid loss: 2.1575, valid accuracy: 0.2772\n",
      "Iter-53100 train loss: 2.3019 valid loss: 2.1573, valid accuracy: 0.2772\n",
      "Iter-53200 train loss: 2.2889 valid loss: 2.1570, valid accuracy: 0.2772\n",
      "Iter-53300 train loss: 2.2852 valid loss: 2.1567, valid accuracy: 0.2772\n",
      "Iter-53400 train loss: 2.3053 valid loss: 2.1565, valid accuracy: 0.2774\n",
      "Iter-53500 train loss: 2.2877 valid loss: 2.1562, valid accuracy: 0.2776\n",
      "Iter-53600 train loss: 2.3010 valid loss: 2.1559, valid accuracy: 0.2782\n",
      "Iter-53700 train loss: 2.2774 valid loss: 2.1556, valid accuracy: 0.2786\n",
      "Iter-53800 train loss: 2.3009 valid loss: 2.1553, valid accuracy: 0.2790\n",
      "Iter-53900 train loss: 2.2892 valid loss: 2.1551, valid accuracy: 0.2794\n",
      "Iter-54000 train loss: 2.2979 valid loss: 2.1548, valid accuracy: 0.2796\n",
      "Iter-54100 train loss: 2.3030 valid loss: 2.1545, valid accuracy: 0.2802\n",
      "Iter-54200 train loss: 2.3034 valid loss: 2.1543, valid accuracy: 0.2804\n",
      "Iter-54300 train loss: 2.2972 valid loss: 2.1540, valid accuracy: 0.2804\n",
      "Iter-54400 train loss: 2.2825 valid loss: 2.1538, valid accuracy: 0.2804\n",
      "Iter-54500 train loss: 2.3008 valid loss: 2.1535, valid accuracy: 0.2804\n",
      "Iter-54600 train loss: 2.3004 valid loss: 2.1533, valid accuracy: 0.2808\n",
      "Iter-54700 train loss: 2.2962 valid loss: 2.1530, valid accuracy: 0.2812\n",
      "Iter-54800 train loss: 2.2786 valid loss: 2.1528, valid accuracy: 0.2814\n",
      "Iter-54900 train loss: 2.2907 valid loss: 2.1525, valid accuracy: 0.2820\n",
      "Iter-55000 train loss: 2.2974 valid loss: 2.1523, valid accuracy: 0.2820\n",
      "Iter-55100 train loss: 2.2825 valid loss: 2.1520, valid accuracy: 0.2822\n",
      "Iter-55200 train loss: 2.3094 valid loss: 2.1518, valid accuracy: 0.2822\n",
      "Iter-55300 train loss: 2.3017 valid loss: 2.1515, valid accuracy: 0.2822\n",
      "Iter-55400 train loss: 2.3011 valid loss: 2.1512, valid accuracy: 0.2822\n",
      "Iter-55500 train loss: 2.2964 valid loss: 2.1510, valid accuracy: 0.2824\n",
      "Iter-55600 train loss: 2.3068 valid loss: 2.1507, valid accuracy: 0.2826\n",
      "Iter-55700 train loss: 2.3002 valid loss: 2.1504, valid accuracy: 0.2828\n",
      "Iter-55800 train loss: 2.2860 valid loss: 2.1502, valid accuracy: 0.2830\n",
      "Iter-55900 train loss: 2.2799 valid loss: 2.1499, valid accuracy: 0.2834\n",
      "Iter-56000 train loss: 2.2861 valid loss: 2.1496, valid accuracy: 0.2834\n",
      "Iter-56100 train loss: 2.2834 valid loss: 2.1494, valid accuracy: 0.2834\n",
      "Iter-56200 train loss: 2.2994 valid loss: 2.1491, valid accuracy: 0.2836\n",
      "Iter-56300 train loss: 2.3050 valid loss: 2.1489, valid accuracy: 0.2838\n",
      "Iter-56400 train loss: 2.2989 valid loss: 2.1486, valid accuracy: 0.2840\n",
      "Iter-56500 train loss: 2.2958 valid loss: 2.1483, valid accuracy: 0.2846\n",
      "Iter-56600 train loss: 2.2933 valid loss: 2.1481, valid accuracy: 0.2850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 2.2998 valid loss: 2.1478, valid accuracy: 0.2850\n",
      "Iter-56800 train loss: 2.2894 valid loss: 2.1476, valid accuracy: 0.2850\n",
      "Iter-56900 train loss: 2.2776 valid loss: 2.1473, valid accuracy: 0.2852\n",
      "Iter-57000 train loss: 2.2992 valid loss: 2.1470, valid accuracy: 0.2852\n",
      "Iter-57100 train loss: 2.3068 valid loss: 2.1468, valid accuracy: 0.2856\n",
      "Iter-57200 train loss: 2.2918 valid loss: 2.1465, valid accuracy: 0.2858\n",
      "Iter-57300 train loss: 2.2923 valid loss: 2.1462, valid accuracy: 0.2864\n",
      "Iter-57400 train loss: 2.2722 valid loss: 2.1460, valid accuracy: 0.2866\n",
      "Iter-57500 train loss: 2.2935 valid loss: 2.1457, valid accuracy: 0.2870\n",
      "Iter-57600 train loss: 2.2854 valid loss: 2.1455, valid accuracy: 0.2870\n",
      "Iter-57700 train loss: 2.2836 valid loss: 2.1453, valid accuracy: 0.2870\n",
      "Iter-57800 train loss: 2.2921 valid loss: 2.1450, valid accuracy: 0.2878\n",
      "Iter-57900 train loss: 2.2977 valid loss: 2.1447, valid accuracy: 0.2884\n",
      "Iter-58000 train loss: 2.3022 valid loss: 2.1445, valid accuracy: 0.2884\n",
      "Iter-58100 train loss: 2.2971 valid loss: 2.1442, valid accuracy: 0.2886\n",
      "Iter-58200 train loss: 2.2982 valid loss: 2.1439, valid accuracy: 0.2894\n",
      "Iter-58300 train loss: 2.2943 valid loss: 2.1437, valid accuracy: 0.2896\n",
      "Iter-58400 train loss: 2.3002 valid loss: 2.1434, valid accuracy: 0.2898\n",
      "Iter-58500 train loss: 2.2980 valid loss: 2.1432, valid accuracy: 0.2900\n",
      "Iter-58600 train loss: 2.2818 valid loss: 2.1429, valid accuracy: 0.2904\n",
      "Iter-58700 train loss: 2.2946 valid loss: 2.1427, valid accuracy: 0.2904\n",
      "Iter-58800 train loss: 2.2921 valid loss: 2.1424, valid accuracy: 0.2904\n",
      "Iter-58900 train loss: 2.3023 valid loss: 2.1422, valid accuracy: 0.2902\n",
      "Iter-59000 train loss: 2.2857 valid loss: 2.1419, valid accuracy: 0.2904\n",
      "Iter-59100 train loss: 2.2830 valid loss: 2.1416, valid accuracy: 0.2906\n",
      "Iter-59200 train loss: 2.2942 valid loss: 2.1414, valid accuracy: 0.2910\n",
      "Iter-59300 train loss: 2.2994 valid loss: 2.1412, valid accuracy: 0.2914\n",
      "Iter-59400 train loss: 2.2920 valid loss: 2.1409, valid accuracy: 0.2920\n",
      "Iter-59500 train loss: 2.2747 valid loss: 2.1406, valid accuracy: 0.2920\n",
      "Iter-59600 train loss: 2.2978 valid loss: 2.1404, valid accuracy: 0.2922\n",
      "Iter-59700 train loss: 2.2984 valid loss: 2.1402, valid accuracy: 0.2924\n",
      "Iter-59800 train loss: 2.2903 valid loss: 2.1399, valid accuracy: 0.2928\n",
      "Iter-59900 train loss: 2.2952 valid loss: 2.1397, valid accuracy: 0.2934\n",
      "Iter-60000 train loss: 2.2986 valid loss: 2.1394, valid accuracy: 0.2938\n",
      "Iter-60100 train loss: 2.3067 valid loss: 2.1391, valid accuracy: 0.2938\n",
      "Iter-60200 train loss: 2.3011 valid loss: 2.1389, valid accuracy: 0.2940\n",
      "Iter-60300 train loss: 2.3031 valid loss: 2.1386, valid accuracy: 0.2946\n",
      "Iter-60400 train loss: 2.3046 valid loss: 2.1384, valid accuracy: 0.2950\n",
      "Iter-60500 train loss: 2.2984 valid loss: 2.1381, valid accuracy: 0.2950\n",
      "Iter-60600 train loss: 2.2936 valid loss: 2.1379, valid accuracy: 0.2964\n",
      "Iter-60700 train loss: 2.3001 valid loss: 2.1376, valid accuracy: 0.2966\n",
      "Iter-60800 train loss: 2.3055 valid loss: 2.1373, valid accuracy: 0.2964\n",
      "Iter-60900 train loss: 2.2946 valid loss: 2.1371, valid accuracy: 0.2972\n",
      "Iter-61000 train loss: 2.3008 valid loss: 2.1368, valid accuracy: 0.2974\n",
      "Iter-61100 train loss: 2.3103 valid loss: 2.1365, valid accuracy: 0.2974\n",
      "Iter-61200 train loss: 2.2785 valid loss: 2.1363, valid accuracy: 0.2972\n",
      "Iter-61300 train loss: 2.3083 valid loss: 2.1360, valid accuracy: 0.2974\n",
      "Iter-61400 train loss: 2.2887 valid loss: 2.1358, valid accuracy: 0.2978\n",
      "Iter-61500 train loss: 2.2959 valid loss: 2.1355, valid accuracy: 0.2980\n",
      "Iter-61600 train loss: 2.2708 valid loss: 2.1353, valid accuracy: 0.2984\n",
      "Iter-61700 train loss: 2.2942 valid loss: 2.1350, valid accuracy: 0.2992\n",
      "Iter-61800 train loss: 2.3011 valid loss: 2.1348, valid accuracy: 0.2994\n",
      "Iter-61900 train loss: 2.2817 valid loss: 2.1345, valid accuracy: 0.2996\n",
      "Iter-62000 train loss: 2.2948 valid loss: 2.1342, valid accuracy: 0.2998\n",
      "Iter-62100 train loss: 2.3019 valid loss: 2.1340, valid accuracy: 0.3000\n",
      "Iter-62200 train loss: 2.2979 valid loss: 2.1338, valid accuracy: 0.3004\n",
      "Iter-62300 train loss: 2.3047 valid loss: 2.1336, valid accuracy: 0.3006\n",
      "Iter-62400 train loss: 2.2842 valid loss: 2.1333, valid accuracy: 0.3008\n",
      "Iter-62500 train loss: 2.2960 valid loss: 2.1330, valid accuracy: 0.3010\n",
      "Iter-62600 train loss: 2.2823 valid loss: 2.1328, valid accuracy: 0.3012\n",
      "Iter-62700 train loss: 2.2912 valid loss: 2.1325, valid accuracy: 0.3008\n",
      "Iter-62800 train loss: 2.2990 valid loss: 2.1323, valid accuracy: 0.3018\n",
      "Iter-62900 train loss: 2.2893 valid loss: 2.1320, valid accuracy: 0.3024\n",
      "Iter-63000 train loss: 2.3009 valid loss: 2.1318, valid accuracy: 0.3026\n",
      "Iter-63100 train loss: 2.3052 valid loss: 2.1315, valid accuracy: 0.3028\n",
      "Iter-63200 train loss: 2.3009 valid loss: 2.1313, valid accuracy: 0.3032\n",
      "Iter-63300 train loss: 2.2963 valid loss: 2.1310, valid accuracy: 0.3036\n",
      "Iter-63400 train loss: 2.2885 valid loss: 2.1308, valid accuracy: 0.3040\n",
      "Iter-63500 train loss: 2.3057 valid loss: 2.1305, valid accuracy: 0.3042\n",
      "Iter-63600 train loss: 2.2863 valid loss: 2.1303, valid accuracy: 0.3052\n",
      "Iter-63700 train loss: 2.2904 valid loss: 2.1300, valid accuracy: 0.3054\n",
      "Iter-63800 train loss: 2.2746 valid loss: 2.1298, valid accuracy: 0.3056\n",
      "Iter-63900 train loss: 2.2994 valid loss: 2.1295, valid accuracy: 0.3060\n",
      "Iter-64000 train loss: 2.2919 valid loss: 2.1293, valid accuracy: 0.3060\n",
      "Iter-64100 train loss: 2.3103 valid loss: 2.1290, valid accuracy: 0.3062\n",
      "Iter-64200 train loss: 2.2913 valid loss: 2.1288, valid accuracy: 0.3066\n",
      "Iter-64300 train loss: 2.2921 valid loss: 2.1285, valid accuracy: 0.3070\n",
      "Iter-64400 train loss: 2.2943 valid loss: 2.1282, valid accuracy: 0.3074\n",
      "Iter-64500 train loss: 2.2828 valid loss: 2.1280, valid accuracy: 0.3076\n",
      "Iter-64600 train loss: 2.3024 valid loss: 2.1277, valid accuracy: 0.3076\n",
      "Iter-64700 train loss: 2.3000 valid loss: 2.1275, valid accuracy: 0.3078\n",
      "Iter-64800 train loss: 2.3011 valid loss: 2.1272, valid accuracy: 0.3084\n",
      "Iter-64900 train loss: 2.2906 valid loss: 2.1270, valid accuracy: 0.3084\n",
      "Iter-65000 train loss: 2.2709 valid loss: 2.1268, valid accuracy: 0.3088\n",
      "Iter-65100 train loss: 2.2739 valid loss: 2.1265, valid accuracy: 0.3090\n",
      "Iter-65200 train loss: 2.3031 valid loss: 2.1262, valid accuracy: 0.3094\n",
      "Iter-65300 train loss: 2.2973 valid loss: 2.1260, valid accuracy: 0.3094\n",
      "Iter-65400 train loss: 2.2987 valid loss: 2.1258, valid accuracy: 0.3096\n",
      "Iter-65500 train loss: 2.2708 valid loss: 2.1255, valid accuracy: 0.3098\n",
      "Iter-65600 train loss: 2.2947 valid loss: 2.1253, valid accuracy: 0.3102\n",
      "Iter-65700 train loss: 2.2862 valid loss: 2.1251, valid accuracy: 0.3100\n",
      "Iter-65800 train loss: 2.2918 valid loss: 2.1248, valid accuracy: 0.3100\n",
      "Iter-65900 train loss: 2.2998 valid loss: 2.1246, valid accuracy: 0.3102\n",
      "Iter-66000 train loss: 2.2917 valid loss: 2.1243, valid accuracy: 0.3106\n",
      "Iter-66100 train loss: 2.2899 valid loss: 2.1241, valid accuracy: 0.3108\n",
      "Iter-66200 train loss: 2.3085 valid loss: 2.1238, valid accuracy: 0.3108\n",
      "Iter-66300 train loss: 2.2978 valid loss: 2.1235, valid accuracy: 0.3112\n",
      "Iter-66400 train loss: 2.3070 valid loss: 2.1233, valid accuracy: 0.3112\n",
      "Iter-66500 train loss: 2.2911 valid loss: 2.1231, valid accuracy: 0.3118\n",
      "Iter-66600 train loss: 2.3014 valid loss: 2.1228, valid accuracy: 0.3118\n",
      "Iter-66700 train loss: 2.3014 valid loss: 2.1225, valid accuracy: 0.3124\n",
      "Iter-66800 train loss: 2.2962 valid loss: 2.1223, valid accuracy: 0.3126\n",
      "Iter-66900 train loss: 2.2975 valid loss: 2.1221, valid accuracy: 0.3128\n",
      "Iter-67000 train loss: 2.2826 valid loss: 2.1218, valid accuracy: 0.3132\n",
      "Iter-67100 train loss: 2.2912 valid loss: 2.1216, valid accuracy: 0.3134\n",
      "Iter-67200 train loss: 2.3035 valid loss: 2.1214, valid accuracy: 0.3134\n",
      "Iter-67300 train loss: 2.3089 valid loss: 2.1211, valid accuracy: 0.3140\n",
      "Iter-67400 train loss: 2.3022 valid loss: 2.1209, valid accuracy: 0.3140\n",
      "Iter-67500 train loss: 2.2966 valid loss: 2.1206, valid accuracy: 0.3142\n",
      "Iter-67600 train loss: 2.3122 valid loss: 2.1204, valid accuracy: 0.3144\n",
      "Iter-67700 train loss: 2.3074 valid loss: 2.1202, valid accuracy: 0.3142\n",
      "Iter-67800 train loss: 2.3002 valid loss: 2.1200, valid accuracy: 0.3142\n",
      "Iter-67900 train loss: 2.3048 valid loss: 2.1197, valid accuracy: 0.3144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 2.2961 valid loss: 2.1195, valid accuracy: 0.3150\n",
      "Iter-68100 train loss: 2.2999 valid loss: 2.1193, valid accuracy: 0.3152\n",
      "Iter-68200 train loss: 2.3010 valid loss: 2.1190, valid accuracy: 0.3162\n",
      "Iter-68300 train loss: 2.2866 valid loss: 2.1187, valid accuracy: 0.3166\n",
      "Iter-68400 train loss: 2.2869 valid loss: 2.1185, valid accuracy: 0.3170\n",
      "Iter-68500 train loss: 2.2883 valid loss: 2.1183, valid accuracy: 0.3172\n",
      "Iter-68600 train loss: 2.2787 valid loss: 2.1181, valid accuracy: 0.3182\n",
      "Iter-68700 train loss: 2.2988 valid loss: 2.1179, valid accuracy: 0.3184\n",
      "Iter-68800 train loss: 2.3060 valid loss: 2.1177, valid accuracy: 0.3182\n",
      "Iter-68900 train loss: 2.2942 valid loss: 2.1175, valid accuracy: 0.3182\n",
      "Iter-69000 train loss: 2.3036 valid loss: 2.1172, valid accuracy: 0.3184\n",
      "Iter-69100 train loss: 2.2986 valid loss: 2.1169, valid accuracy: 0.3184\n",
      "Iter-69200 train loss: 2.2740 valid loss: 2.1167, valid accuracy: 0.3190\n",
      "Iter-69300 train loss: 2.2915 valid loss: 2.1164, valid accuracy: 0.3200\n",
      "Iter-69400 train loss: 2.3062 valid loss: 2.1162, valid accuracy: 0.3204\n",
      "Iter-69500 train loss: 2.2863 valid loss: 2.1160, valid accuracy: 0.3202\n",
      "Iter-69600 train loss: 2.2909 valid loss: 2.1157, valid accuracy: 0.3204\n",
      "Iter-69700 train loss: 2.2902 valid loss: 2.1155, valid accuracy: 0.3206\n",
      "Iter-69800 train loss: 2.2886 valid loss: 2.1152, valid accuracy: 0.3210\n",
      "Iter-69900 train loss: 2.2905 valid loss: 2.1150, valid accuracy: 0.3210\n",
      "Iter-70000 train loss: 2.2890 valid loss: 2.1147, valid accuracy: 0.3208\n",
      "Iter-70100 train loss: 2.2984 valid loss: 2.1145, valid accuracy: 0.3210\n",
      "Iter-70200 train loss: 2.2933 valid loss: 2.1142, valid accuracy: 0.3216\n",
      "Iter-70300 train loss: 2.2939 valid loss: 2.1140, valid accuracy: 0.3216\n",
      "Iter-70400 train loss: 2.2873 valid loss: 2.1137, valid accuracy: 0.3222\n",
      "Iter-70500 train loss: 2.2981 valid loss: 2.1135, valid accuracy: 0.3222\n",
      "Iter-70600 train loss: 2.2939 valid loss: 2.1133, valid accuracy: 0.3222\n",
      "Iter-70700 train loss: 2.2891 valid loss: 2.1130, valid accuracy: 0.3226\n",
      "Iter-70800 train loss: 2.2755 valid loss: 2.1128, valid accuracy: 0.3226\n",
      "Iter-70900 train loss: 2.2926 valid loss: 2.1126, valid accuracy: 0.3224\n",
      "Iter-71000 train loss: 2.2963 valid loss: 2.1123, valid accuracy: 0.3224\n",
      "Iter-71100 train loss: 2.3044 valid loss: 2.1121, valid accuracy: 0.3232\n",
      "Iter-71200 train loss: 2.3014 valid loss: 2.1118, valid accuracy: 0.3232\n",
      "Iter-71300 train loss: 2.2820 valid loss: 2.1116, valid accuracy: 0.3236\n",
      "Iter-71400 train loss: 2.3011 valid loss: 2.1114, valid accuracy: 0.3242\n",
      "Iter-71500 train loss: 2.3027 valid loss: 2.1112, valid accuracy: 0.3248\n",
      "Iter-71600 train loss: 2.2864 valid loss: 2.1110, valid accuracy: 0.3246\n",
      "Iter-71700 train loss: 2.3074 valid loss: 2.1107, valid accuracy: 0.3248\n",
      "Iter-71800 train loss: 2.2972 valid loss: 2.1105, valid accuracy: 0.3248\n",
      "Iter-71900 train loss: 2.2910 valid loss: 2.1102, valid accuracy: 0.3250\n",
      "Iter-72000 train loss: 2.2981 valid loss: 2.1100, valid accuracy: 0.3252\n",
      "Iter-72100 train loss: 2.2831 valid loss: 2.1097, valid accuracy: 0.3254\n",
      "Iter-72200 train loss: 2.3033 valid loss: 2.1095, valid accuracy: 0.3260\n",
      "Iter-72300 train loss: 2.3004 valid loss: 2.1093, valid accuracy: 0.3262\n",
      "Iter-72400 train loss: 2.3004 valid loss: 2.1090, valid accuracy: 0.3264\n",
      "Iter-72500 train loss: 2.2783 valid loss: 2.1088, valid accuracy: 0.3268\n",
      "Iter-72600 train loss: 2.2850 valid loss: 2.1085, valid accuracy: 0.3270\n",
      "Iter-72700 train loss: 2.2902 valid loss: 2.1083, valid accuracy: 0.3268\n",
      "Iter-72800 train loss: 2.2905 valid loss: 2.1080, valid accuracy: 0.3274\n",
      "Iter-72900 train loss: 2.2766 valid loss: 2.1078, valid accuracy: 0.3274\n",
      "Iter-73000 train loss: 2.2868 valid loss: 2.1076, valid accuracy: 0.3274\n",
      "Iter-73100 train loss: 2.3035 valid loss: 2.1074, valid accuracy: 0.3272\n",
      "Iter-73200 train loss: 2.3029 valid loss: 2.1071, valid accuracy: 0.3272\n",
      "Iter-73300 train loss: 2.2914 valid loss: 2.1069, valid accuracy: 0.3278\n",
      "Iter-73400 train loss: 2.2899 valid loss: 2.1067, valid accuracy: 0.3282\n",
      "Iter-73500 train loss: 2.3020 valid loss: 2.1065, valid accuracy: 0.3282\n",
      "Iter-73600 train loss: 2.2872 valid loss: 2.1062, valid accuracy: 0.3282\n",
      "Iter-73700 train loss: 2.2954 valid loss: 2.1060, valid accuracy: 0.3282\n",
      "Iter-73800 train loss: 2.2922 valid loss: 2.1058, valid accuracy: 0.3286\n",
      "Iter-73900 train loss: 2.3085 valid loss: 2.1056, valid accuracy: 0.3294\n",
      "Iter-74000 train loss: 2.2875 valid loss: 2.1054, valid accuracy: 0.3298\n",
      "Iter-74100 train loss: 2.2660 valid loss: 2.1051, valid accuracy: 0.3294\n",
      "Iter-74200 train loss: 2.2750 valid loss: 2.1049, valid accuracy: 0.3294\n",
      "Iter-74300 train loss: 2.3048 valid loss: 2.1046, valid accuracy: 0.3294\n",
      "Iter-74400 train loss: 2.2987 valid loss: 2.1044, valid accuracy: 0.3294\n",
      "Iter-74500 train loss: 2.2878 valid loss: 2.1041, valid accuracy: 0.3294\n",
      "Iter-74600 train loss: 2.2921 valid loss: 2.1039, valid accuracy: 0.3292\n",
      "Iter-74700 train loss: 2.2893 valid loss: 2.1037, valid accuracy: 0.3292\n",
      "Iter-74800 train loss: 2.3081 valid loss: 2.1034, valid accuracy: 0.3292\n",
      "Iter-74900 train loss: 2.2839 valid loss: 2.1032, valid accuracy: 0.3294\n",
      "Iter-75000 train loss: 2.2866 valid loss: 2.1030, valid accuracy: 0.3294\n",
      "Iter-75100 train loss: 2.3034 valid loss: 2.1027, valid accuracy: 0.3296\n",
      "Iter-75200 train loss: 2.2921 valid loss: 2.1025, valid accuracy: 0.3302\n",
      "Iter-75300 train loss: 2.2909 valid loss: 2.1022, valid accuracy: 0.3306\n",
      "Iter-75400 train loss: 2.2873 valid loss: 2.1020, valid accuracy: 0.3310\n",
      "Iter-75500 train loss: 2.2837 valid loss: 2.1018, valid accuracy: 0.3312\n",
      "Iter-75600 train loss: 2.2545 valid loss: 2.1016, valid accuracy: 0.3312\n",
      "Iter-75700 train loss: 2.2841 valid loss: 2.1013, valid accuracy: 0.3316\n",
      "Iter-75800 train loss: 2.2893 valid loss: 2.1011, valid accuracy: 0.3314\n",
      "Iter-75900 train loss: 2.3109 valid loss: 2.1009, valid accuracy: 0.3316\n",
      "Iter-76000 train loss: 2.2963 valid loss: 2.1007, valid accuracy: 0.3322\n",
      "Iter-76100 train loss: 2.3100 valid loss: 2.1004, valid accuracy: 0.3324\n",
      "Iter-76200 train loss: 2.2963 valid loss: 2.1002, valid accuracy: 0.3330\n",
      "Iter-76300 train loss: 2.2929 valid loss: 2.1000, valid accuracy: 0.3334\n",
      "Iter-76400 train loss: 2.3027 valid loss: 2.0998, valid accuracy: 0.3336\n",
      "Iter-76500 train loss: 2.2959 valid loss: 2.0996, valid accuracy: 0.3336\n",
      "Iter-76600 train loss: 2.2638 valid loss: 2.0993, valid accuracy: 0.3340\n",
      "Iter-76700 train loss: 2.2916 valid loss: 2.0990, valid accuracy: 0.3340\n",
      "Iter-76800 train loss: 2.3010 valid loss: 2.0988, valid accuracy: 0.3344\n",
      "Iter-76900 train loss: 2.2977 valid loss: 2.0986, valid accuracy: 0.3350\n",
      "Iter-77000 train loss: 2.2874 valid loss: 2.0983, valid accuracy: 0.3350\n",
      "Iter-77100 train loss: 2.2877 valid loss: 2.0981, valid accuracy: 0.3350\n",
      "Iter-77200 train loss: 2.3027 valid loss: 2.0978, valid accuracy: 0.3352\n",
      "Iter-77300 train loss: 2.2987 valid loss: 2.0976, valid accuracy: 0.3358\n",
      "Iter-77400 train loss: 2.2835 valid loss: 2.0973, valid accuracy: 0.3368\n",
      "Iter-77500 train loss: 2.2694 valid loss: 2.0971, valid accuracy: 0.3370\n",
      "Iter-77600 train loss: 2.2945 valid loss: 2.0969, valid accuracy: 0.3372\n",
      "Iter-77700 train loss: 2.2959 valid loss: 2.0967, valid accuracy: 0.3372\n",
      "Iter-77800 train loss: 2.2864 valid loss: 2.0964, valid accuracy: 0.3374\n",
      "Iter-77900 train loss: 2.3024 valid loss: 2.0963, valid accuracy: 0.3376\n",
      "Iter-78000 train loss: 2.2839 valid loss: 2.0960, valid accuracy: 0.3374\n",
      "Iter-78100 train loss: 2.2868 valid loss: 2.0958, valid accuracy: 0.3376\n",
      "Iter-78200 train loss: 2.2919 valid loss: 2.0956, valid accuracy: 0.3376\n",
      "Iter-78300 train loss: 2.2914 valid loss: 2.0953, valid accuracy: 0.3378\n",
      "Iter-78400 train loss: 2.2842 valid loss: 2.0951, valid accuracy: 0.3380\n",
      "Iter-78500 train loss: 2.2961 valid loss: 2.0949, valid accuracy: 0.3382\n",
      "Iter-78600 train loss: 2.2858 valid loss: 2.0947, valid accuracy: 0.3384\n",
      "Iter-78700 train loss: 2.2715 valid loss: 2.0945, valid accuracy: 0.3386\n",
      "Iter-78800 train loss: 2.3022 valid loss: 2.0942, valid accuracy: 0.3386\n",
      "Iter-78900 train loss: 2.2932 valid loss: 2.0940, valid accuracy: 0.3386\n",
      "Iter-79000 train loss: 2.2735 valid loss: 2.0937, valid accuracy: 0.3392\n",
      "Iter-79100 train loss: 2.2882 valid loss: 2.0935, valid accuracy: 0.3392\n",
      "Iter-79200 train loss: 2.2979 valid loss: 2.0933, valid accuracy: 0.3394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 2.2993 valid loss: 2.0930, valid accuracy: 0.3396\n",
      "Iter-79400 train loss: 2.2820 valid loss: 2.0928, valid accuracy: 0.3394\n",
      "Iter-79500 train loss: 2.2896 valid loss: 2.0926, valid accuracy: 0.3396\n",
      "Iter-79600 train loss: 2.3006 valid loss: 2.0924, valid accuracy: 0.3398\n",
      "Iter-79700 train loss: 2.3031 valid loss: 2.0921, valid accuracy: 0.3402\n",
      "Iter-79800 train loss: 2.2876 valid loss: 2.0919, valid accuracy: 0.3402\n",
      "Iter-79900 train loss: 2.2843 valid loss: 2.0917, valid accuracy: 0.3400\n",
      "Iter-80000 train loss: 2.3016 valid loss: 2.0914, valid accuracy: 0.3400\n",
      "Iter-80100 train loss: 2.3071 valid loss: 2.0912, valid accuracy: 0.3400\n",
      "Iter-80200 train loss: 2.2896 valid loss: 2.0910, valid accuracy: 0.3402\n",
      "Iter-80300 train loss: 2.2993 valid loss: 2.0907, valid accuracy: 0.3404\n",
      "Iter-80400 train loss: 2.2960 valid loss: 2.0905, valid accuracy: 0.3406\n",
      "Iter-80500 train loss: 2.3155 valid loss: 2.0903, valid accuracy: 0.3406\n",
      "Iter-80600 train loss: 2.2840 valid loss: 2.0901, valid accuracy: 0.3410\n",
      "Iter-80700 train loss: 2.2760 valid loss: 2.0898, valid accuracy: 0.3412\n",
      "Iter-80800 train loss: 2.2932 valid loss: 2.0896, valid accuracy: 0.3414\n",
      "Iter-80900 train loss: 2.3058 valid loss: 2.0894, valid accuracy: 0.3418\n",
      "Iter-81000 train loss: 2.3041 valid loss: 2.0891, valid accuracy: 0.3414\n",
      "Iter-81100 train loss: 2.2788 valid loss: 2.0889, valid accuracy: 0.3418\n",
      "Iter-81200 train loss: 2.3051 valid loss: 2.0887, valid accuracy: 0.3428\n",
      "Iter-81300 train loss: 2.2931 valid loss: 2.0885, valid accuracy: 0.3430\n",
      "Iter-81400 train loss: 2.2645 valid loss: 2.0882, valid accuracy: 0.3434\n",
      "Iter-81500 train loss: 2.2695 valid loss: 2.0880, valid accuracy: 0.3438\n",
      "Iter-81600 train loss: 2.2991 valid loss: 2.0878, valid accuracy: 0.3438\n",
      "Iter-81700 train loss: 2.2913 valid loss: 2.0876, valid accuracy: 0.3440\n",
      "Iter-81800 train loss: 2.2831 valid loss: 2.0874, valid accuracy: 0.3442\n",
      "Iter-81900 train loss: 2.2758 valid loss: 2.0871, valid accuracy: 0.3450\n",
      "Iter-82000 train loss: 2.2955 valid loss: 2.0869, valid accuracy: 0.3456\n",
      "Iter-82100 train loss: 2.2844 valid loss: 2.0867, valid accuracy: 0.3452\n",
      "Iter-82200 train loss: 2.2770 valid loss: 2.0864, valid accuracy: 0.3454\n",
      "Iter-82300 train loss: 2.2978 valid loss: 2.0862, valid accuracy: 0.3452\n",
      "Iter-82400 train loss: 2.2934 valid loss: 2.0860, valid accuracy: 0.3458\n",
      "Iter-82500 train loss: 2.2892 valid loss: 2.0858, valid accuracy: 0.3458\n",
      "Iter-82600 train loss: 2.2849 valid loss: 2.0856, valid accuracy: 0.3460\n",
      "Iter-82700 train loss: 2.2843 valid loss: 2.0853, valid accuracy: 0.3464\n",
      "Iter-82800 train loss: 2.2706 valid loss: 2.0851, valid accuracy: 0.3468\n",
      "Iter-82900 train loss: 2.2984 valid loss: 2.0849, valid accuracy: 0.3468\n",
      "Iter-83000 train loss: 2.3060 valid loss: 2.0846, valid accuracy: 0.3470\n",
      "Iter-83100 train loss: 2.2701 valid loss: 2.0844, valid accuracy: 0.3470\n",
      "Iter-83200 train loss: 2.3016 valid loss: 2.0842, valid accuracy: 0.3470\n",
      "Iter-83300 train loss: 2.3026 valid loss: 2.0840, valid accuracy: 0.3474\n",
      "Iter-83400 train loss: 2.2753 valid loss: 2.0838, valid accuracy: 0.3476\n",
      "Iter-83500 train loss: 2.2754 valid loss: 2.0835, valid accuracy: 0.3476\n",
      "Iter-83600 train loss: 2.2912 valid loss: 2.0833, valid accuracy: 0.3476\n",
      "Iter-83700 train loss: 2.3033 valid loss: 2.0831, valid accuracy: 0.3476\n",
      "Iter-83800 train loss: 2.2769 valid loss: 2.0829, valid accuracy: 0.3474\n",
      "Iter-83900 train loss: 2.2991 valid loss: 2.0826, valid accuracy: 0.3480\n",
      "Iter-84000 train loss: 2.3047 valid loss: 2.0824, valid accuracy: 0.3482\n",
      "Iter-84100 train loss: 2.2823 valid loss: 2.0822, valid accuracy: 0.3486\n",
      "Iter-84200 train loss: 2.2977 valid loss: 2.0820, valid accuracy: 0.3490\n",
      "Iter-84300 train loss: 2.3103 valid loss: 2.0818, valid accuracy: 0.3490\n",
      "Iter-84400 train loss: 2.2983 valid loss: 2.0815, valid accuracy: 0.3492\n",
      "Iter-84500 train loss: 2.2956 valid loss: 2.0813, valid accuracy: 0.3496\n",
      "Iter-84600 train loss: 2.2927 valid loss: 2.0811, valid accuracy: 0.3496\n",
      "Iter-84700 train loss: 2.2933 valid loss: 2.0809, valid accuracy: 0.3498\n",
      "Iter-84800 train loss: 2.2856 valid loss: 2.0806, valid accuracy: 0.3500\n",
      "Iter-84900 train loss: 2.2673 valid loss: 2.0804, valid accuracy: 0.3500\n",
      "Iter-85000 train loss: 2.2989 valid loss: 2.0802, valid accuracy: 0.3500\n",
      "Iter-85100 train loss: 2.2915 valid loss: 2.0800, valid accuracy: 0.3506\n",
      "Iter-85200 train loss: 2.2691 valid loss: 2.0797, valid accuracy: 0.3510\n",
      "Iter-85300 train loss: 2.2780 valid loss: 2.0795, valid accuracy: 0.3508\n",
      "Iter-85400 train loss: 2.2913 valid loss: 2.0793, valid accuracy: 0.3512\n",
      "Iter-85500 train loss: 2.2888 valid loss: 2.0791, valid accuracy: 0.3512\n",
      "Iter-85600 train loss: 2.3010 valid loss: 2.0789, valid accuracy: 0.3514\n",
      "Iter-85700 train loss: 2.2913 valid loss: 2.0786, valid accuracy: 0.3510\n",
      "Iter-85800 train loss: 2.2709 valid loss: 2.0784, valid accuracy: 0.3512\n",
      "Iter-85900 train loss: 2.2720 valid loss: 2.0782, valid accuracy: 0.3520\n",
      "Iter-86000 train loss: 2.2746 valid loss: 2.0780, valid accuracy: 0.3524\n",
      "Iter-86100 train loss: 2.2992 valid loss: 2.0777, valid accuracy: 0.3524\n",
      "Iter-86200 train loss: 2.2866 valid loss: 2.0775, valid accuracy: 0.3524\n",
      "Iter-86300 train loss: 2.3123 valid loss: 2.0772, valid accuracy: 0.3524\n",
      "Iter-86400 train loss: 2.2916 valid loss: 2.0770, valid accuracy: 0.3526\n",
      "Iter-86500 train loss: 2.2946 valid loss: 2.0768, valid accuracy: 0.3530\n",
      "Iter-86600 train loss: 2.2994 valid loss: 2.0766, valid accuracy: 0.3530\n",
      "Iter-86700 train loss: 2.3023 valid loss: 2.0764, valid accuracy: 0.3530\n",
      "Iter-86800 train loss: 2.3027 valid loss: 2.0761, valid accuracy: 0.3534\n",
      "Iter-86900 train loss: 2.2958 valid loss: 2.0759, valid accuracy: 0.3538\n",
      "Iter-87000 train loss: 2.3014 valid loss: 2.0757, valid accuracy: 0.3540\n",
      "Iter-87100 train loss: 2.2988 valid loss: 2.0755, valid accuracy: 0.3540\n",
      "Iter-87200 train loss: 2.3018 valid loss: 2.0753, valid accuracy: 0.3544\n",
      "Iter-87300 train loss: 2.2894 valid loss: 2.0751, valid accuracy: 0.3542\n",
      "Iter-87400 train loss: 2.2892 valid loss: 2.0748, valid accuracy: 0.3548\n",
      "Iter-87500 train loss: 2.2937 valid loss: 2.0746, valid accuracy: 0.3550\n",
      "Iter-87600 train loss: 2.3013 valid loss: 2.0744, valid accuracy: 0.3550\n",
      "Iter-87700 train loss: 2.3045 valid loss: 2.0742, valid accuracy: 0.3548\n",
      "Iter-87800 train loss: 2.2717 valid loss: 2.0740, valid accuracy: 0.3552\n",
      "Iter-87900 train loss: 2.2943 valid loss: 2.0738, valid accuracy: 0.3554\n",
      "Iter-88000 train loss: 2.2932 valid loss: 2.0736, valid accuracy: 0.3556\n",
      "Iter-88100 train loss: 2.2893 valid loss: 2.0733, valid accuracy: 0.3556\n",
      "Iter-88200 train loss: 2.2960 valid loss: 2.0731, valid accuracy: 0.3564\n",
      "Iter-88300 train loss: 2.2960 valid loss: 2.0729, valid accuracy: 0.3564\n",
      "Iter-88400 train loss: 2.2789 valid loss: 2.0726, valid accuracy: 0.3566\n",
      "Iter-88500 train loss: 2.3031 valid loss: 2.0724, valid accuracy: 0.3568\n",
      "Iter-88600 train loss: 2.3003 valid loss: 2.0722, valid accuracy: 0.3568\n",
      "Iter-88700 train loss: 2.3019 valid loss: 2.0720, valid accuracy: 0.3568\n",
      "Iter-88800 train loss: 2.2891 valid loss: 2.0718, valid accuracy: 0.3570\n",
      "Iter-88900 train loss: 2.2899 valid loss: 2.0715, valid accuracy: 0.3570\n",
      "Iter-89000 train loss: 2.2846 valid loss: 2.0713, valid accuracy: 0.3572\n",
      "Iter-89100 train loss: 2.2915 valid loss: 2.0711, valid accuracy: 0.3570\n",
      "Iter-89200 train loss: 2.2983 valid loss: 2.0709, valid accuracy: 0.3574\n",
      "Iter-89300 train loss: 2.2927 valid loss: 2.0706, valid accuracy: 0.3578\n",
      "Iter-89400 train loss: 2.3099 valid loss: 2.0704, valid accuracy: 0.3578\n",
      "Iter-89500 train loss: 2.2775 valid loss: 2.0702, valid accuracy: 0.3578\n",
      "Iter-89600 train loss: 2.2733 valid loss: 2.0699, valid accuracy: 0.3578\n",
      "Iter-89700 train loss: 2.2707 valid loss: 2.0697, valid accuracy: 0.3580\n",
      "Iter-89800 train loss: 2.2999 valid loss: 2.0695, valid accuracy: 0.3580\n",
      "Iter-89900 train loss: 2.2913 valid loss: 2.0693, valid accuracy: 0.3580\n",
      "Iter-90000 train loss: 2.2891 valid loss: 2.0690, valid accuracy: 0.3584\n",
      "Iter-90100 train loss: 2.2939 valid loss: 2.0688, valid accuracy: 0.3586\n",
      "Iter-90200 train loss: 2.2683 valid loss: 2.0686, valid accuracy: 0.3590\n",
      "Iter-90300 train loss: 2.2895 valid loss: 2.0684, valid accuracy: 0.3588\n",
      "Iter-90400 train loss: 2.3051 valid loss: 2.0682, valid accuracy: 0.3588\n",
      "Iter-90500 train loss: 2.3008 valid loss: 2.0680, valid accuracy: 0.3590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 2.2708 valid loss: 2.0678, valid accuracy: 0.3596\n",
      "Iter-90700 train loss: 2.2940 valid loss: 2.0676, valid accuracy: 0.3592\n",
      "Iter-90800 train loss: 2.2788 valid loss: 2.0674, valid accuracy: 0.3596\n",
      "Iter-90900 train loss: 2.2552 valid loss: 2.0672, valid accuracy: 0.3596\n",
      "Iter-91000 train loss: 2.2689 valid loss: 2.0670, valid accuracy: 0.3600\n",
      "Iter-91100 train loss: 2.2776 valid loss: 2.0668, valid accuracy: 0.3600\n",
      "Iter-91200 train loss: 2.2521 valid loss: 2.0666, valid accuracy: 0.3602\n",
      "Iter-91300 train loss: 2.2667 valid loss: 2.0664, valid accuracy: 0.3602\n",
      "Iter-91400 train loss: 2.3099 valid loss: 2.0662, valid accuracy: 0.3602\n",
      "Iter-91500 train loss: 2.2916 valid loss: 2.0660, valid accuracy: 0.3606\n",
      "Iter-91600 train loss: 2.2918 valid loss: 2.0657, valid accuracy: 0.3608\n",
      "Iter-91700 train loss: 2.2724 valid loss: 2.0655, valid accuracy: 0.3612\n",
      "Iter-91800 train loss: 2.2971 valid loss: 2.0653, valid accuracy: 0.3618\n",
      "Iter-91900 train loss: 2.2891 valid loss: 2.0651, valid accuracy: 0.3622\n",
      "Iter-92000 train loss: 2.3006 valid loss: 2.0648, valid accuracy: 0.3624\n",
      "Iter-92100 train loss: 2.2841 valid loss: 2.0646, valid accuracy: 0.3624\n",
      "Iter-92200 train loss: 2.3029 valid loss: 2.0644, valid accuracy: 0.3628\n",
      "Iter-92300 train loss: 2.3051 valid loss: 2.0641, valid accuracy: 0.3626\n",
      "Iter-92400 train loss: 2.2924 valid loss: 2.0640, valid accuracy: 0.3632\n",
      "Iter-92500 train loss: 2.2887 valid loss: 2.0637, valid accuracy: 0.3632\n",
      "Iter-92600 train loss: 2.2760 valid loss: 2.0635, valid accuracy: 0.3634\n",
      "Iter-92700 train loss: 2.2781 valid loss: 2.0632, valid accuracy: 0.3632\n",
      "Iter-92800 train loss: 2.3024 valid loss: 2.0630, valid accuracy: 0.3632\n",
      "Iter-92900 train loss: 2.2779 valid loss: 2.0629, valid accuracy: 0.3632\n",
      "Iter-93000 train loss: 2.3051 valid loss: 2.0626, valid accuracy: 0.3634\n",
      "Iter-93100 train loss: 2.2849 valid loss: 2.0624, valid accuracy: 0.3640\n",
      "Iter-93200 train loss: 2.2888 valid loss: 2.0622, valid accuracy: 0.3640\n",
      "Iter-93300 train loss: 2.2963 valid loss: 2.0620, valid accuracy: 0.3642\n",
      "Iter-93400 train loss: 2.2993 valid loss: 2.0618, valid accuracy: 0.3644\n",
      "Iter-93500 train loss: 2.2881 valid loss: 2.0616, valid accuracy: 0.3644\n",
      "Iter-93600 train loss: 2.2694 valid loss: 2.0614, valid accuracy: 0.3644\n",
      "Iter-93700 train loss: 2.3027 valid loss: 2.0611, valid accuracy: 0.3642\n",
      "Iter-93800 train loss: 2.2775 valid loss: 2.0609, valid accuracy: 0.3642\n",
      "Iter-93900 train loss: 2.3021 valid loss: 2.0607, valid accuracy: 0.3642\n",
      "Iter-94000 train loss: 2.2912 valid loss: 2.0605, valid accuracy: 0.3642\n",
      "Iter-94100 train loss: 2.2993 valid loss: 2.0602, valid accuracy: 0.3642\n",
      "Iter-94200 train loss: 2.2861 valid loss: 2.0600, valid accuracy: 0.3644\n",
      "Iter-94300 train loss: 2.2769 valid loss: 2.0598, valid accuracy: 0.3644\n",
      "Iter-94400 train loss: 2.3034 valid loss: 2.0596, valid accuracy: 0.3644\n",
      "Iter-94500 train loss: 2.2784 valid loss: 2.0594, valid accuracy: 0.3644\n",
      "Iter-94600 train loss: 2.2934 valid loss: 2.0591, valid accuracy: 0.3640\n",
      "Iter-94700 train loss: 2.3022 valid loss: 2.0589, valid accuracy: 0.3642\n",
      "Iter-94800 train loss: 2.2717 valid loss: 2.0587, valid accuracy: 0.3644\n",
      "Iter-94900 train loss: 2.2823 valid loss: 2.0585, valid accuracy: 0.3646\n",
      "Iter-95000 train loss: 2.2787 valid loss: 2.0582, valid accuracy: 0.3646\n",
      "Iter-95100 train loss: 2.3099 valid loss: 2.0580, valid accuracy: 0.3646\n",
      "Iter-95200 train loss: 2.2647 valid loss: 2.0578, valid accuracy: 0.3646\n",
      "Iter-95300 train loss: 2.2999 valid loss: 2.0576, valid accuracy: 0.3646\n",
      "Iter-95400 train loss: 2.2967 valid loss: 2.0574, valid accuracy: 0.3652\n",
      "Iter-95500 train loss: 2.2911 valid loss: 2.0572, valid accuracy: 0.3652\n",
      "Iter-95600 train loss: 2.2958 valid loss: 2.0570, valid accuracy: 0.3656\n",
      "Iter-95700 train loss: 2.2883 valid loss: 2.0568, valid accuracy: 0.3656\n",
      "Iter-95800 train loss: 2.2930 valid loss: 2.0566, valid accuracy: 0.3654\n",
      "Iter-95900 train loss: 2.3050 valid loss: 2.0564, valid accuracy: 0.3656\n",
      "Iter-96000 train loss: 2.2995 valid loss: 2.0562, valid accuracy: 0.3656\n",
      "Iter-96100 train loss: 2.2987 valid loss: 2.0560, valid accuracy: 0.3654\n",
      "Iter-96200 train loss: 2.2771 valid loss: 2.0557, valid accuracy: 0.3658\n",
      "Iter-96300 train loss: 2.2975 valid loss: 2.0555, valid accuracy: 0.3664\n",
      "Iter-96400 train loss: 2.2792 valid loss: 2.0553, valid accuracy: 0.3668\n",
      "Iter-96500 train loss: 2.2860 valid loss: 2.0551, valid accuracy: 0.3672\n",
      "Iter-96600 train loss: 2.2764 valid loss: 2.0548, valid accuracy: 0.3672\n",
      "Iter-96700 train loss: 2.2789 valid loss: 2.0546, valid accuracy: 0.3672\n",
      "Iter-96800 train loss: 2.2933 valid loss: 2.0544, valid accuracy: 0.3678\n",
      "Iter-96900 train loss: 2.2800 valid loss: 2.0542, valid accuracy: 0.3678\n",
      "Iter-97000 train loss: 2.2979 valid loss: 2.0540, valid accuracy: 0.3672\n",
      "Iter-97100 train loss: 2.2794 valid loss: 2.0538, valid accuracy: 0.3672\n",
      "Iter-97200 train loss: 2.2680 valid loss: 2.0536, valid accuracy: 0.3676\n",
      "Iter-97300 train loss: 2.2978 valid loss: 2.0534, valid accuracy: 0.3674\n",
      "Iter-97400 train loss: 2.2940 valid loss: 2.0532, valid accuracy: 0.3680\n",
      "Iter-97500 train loss: 2.3008 valid loss: 2.0530, valid accuracy: 0.3682\n",
      "Iter-97600 train loss: 2.2953 valid loss: 2.0528, valid accuracy: 0.3678\n",
      "Iter-97700 train loss: 2.3086 valid loss: 2.0526, valid accuracy: 0.3682\n",
      "Iter-97800 train loss: 2.3026 valid loss: 2.0523, valid accuracy: 0.3682\n",
      "Iter-97900 train loss: 2.2889 valid loss: 2.0521, valid accuracy: 0.3684\n",
      "Iter-98000 train loss: 2.2894 valid loss: 2.0519, valid accuracy: 0.3684\n",
      "Iter-98100 train loss: 2.2694 valid loss: 2.0517, valid accuracy: 0.3686\n",
      "Iter-98200 train loss: 2.2962 valid loss: 2.0515, valid accuracy: 0.3690\n",
      "Iter-98300 train loss: 2.2763 valid loss: 2.0513, valid accuracy: 0.3692\n",
      "Iter-98400 train loss: 2.2730 valid loss: 2.0511, valid accuracy: 0.3694\n",
      "Iter-98500 train loss: 2.2992 valid loss: 2.0509, valid accuracy: 0.3692\n",
      "Iter-98600 train loss: 2.2602 valid loss: 2.0507, valid accuracy: 0.3694\n",
      "Iter-98700 train loss: 2.2940 valid loss: 2.0505, valid accuracy: 0.3694\n",
      "Iter-98800 train loss: 2.3089 valid loss: 2.0503, valid accuracy: 0.3700\n",
      "Iter-98900 train loss: 2.2835 valid loss: 2.0501, valid accuracy: 0.3702\n",
      "Iter-99000 train loss: 2.2944 valid loss: 2.0498, valid accuracy: 0.3706\n",
      "Iter-99100 train loss: 2.2818 valid loss: 2.0496, valid accuracy: 0.3712\n",
      "Iter-99200 train loss: 2.2911 valid loss: 2.0494, valid accuracy: 0.3712\n",
      "Iter-99300 train loss: 2.3008 valid loss: 2.0492, valid accuracy: 0.3714\n",
      "Iter-99400 train loss: 2.2846 valid loss: 2.0490, valid accuracy: 0.3718\n",
      "Iter-99500 train loss: 2.2594 valid loss: 2.0488, valid accuracy: 0.3718\n",
      "Iter-99600 train loss: 2.2928 valid loss: 2.0486, valid accuracy: 0.3718\n",
      "Iter-99700 train loss: 2.2733 valid loss: 2.0484, valid accuracy: 0.3720\n",
      "Iter-99800 train loss: 2.3059 valid loss: 2.0481, valid accuracy: 0.3718\n",
      "Iter-99900 train loss: 2.3079 valid loss: 2.0479, valid accuracy: 0.3720\n",
      "Iter-100000 train loss: 2.2932 valid loss: 2.0477, valid accuracy: 0.3722\n",
      "Last iteration - Test accuracy mean: 0.3757, std: 0.0000, loss: 2.0532\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FNX6wPHvCUmAUELvVXoPIIgCEkQEUYFrpaP+RDoi\nFhBB4AKKXiyA5AKKqDS9VhBpIkVQqVIiJHTpIL1IT97fH7PLbpJtCUl2k7yf55lnZ86cmTkzSfbN\nnHPmjBERlFJKKXeC/F0ApZRSgU0DhVJKKY80UCillPJIA4VSSimPNFAopZTySAOFUkopj7wGCmNM\nKWPMcmPMdmNMtDFmgIs8bY0xW40xm40x640xjZ3W/eW8LrVPQCmlVNoy3p6jMMYUA4qJyBZjTG5g\nE9BORGKd8oSJyGXbfC3gfyJSzba8D6gvImfT6iSUUkqlHa93FCJyXES22OYvATFAyUR5Ljst5gbi\nnZaNL8dRSikVmJL1BW6MKQdEAOtcrGtvjIkBfgCedVolwE/GmA3GmB4pL6pSSil/8Fr1dCujVe20\nEhgtIvM85GsCjBCRlrbl4iJyzBhTGPgJ6Ccia2675EoppdKFT4HCGBMMLAAWicgEH/LvBRqIyJlE\n6SOAiyLynottdNAppZRKJhExaX0MX6uePgF2uAsSxpgKTvP1gFAROWOMCbPdiWCMyQU8APzp7iAi\nopMII0aM8HsZAmHS66DXQq+F5ym9BHvLYOvq2hmINsZsxmpzGAqUBUREpgGPGWO6AdeBK8CTts2L\nAt/Z7haCgdkisjT1T0MppVRa8RooRORXIJuXPO8A77hI34/V+K2UUiqD0m6rASgyMtLfRQgIeh0c\n9Fo46LVIfz73ekprxhgJlLIopVRGYIxB0qEx22vVk1IqaylXrhwHDhzwdzGUk7Jly/LXX3/57fh6\nR+HFZ59Zn927+7ccSqUX23+p/i6GcuLuZ5JedxQaKLwwth9BABZNqTShgSLw+DtQaGO2UkopjwIq\nUNy4kbr7i46G8PDU3adSSmU1ARUo3n0Xrl/3nu/CBThzxnu+zZutvO6k1t316dOOKiqlVMYQHx9P\nnjx5OHz4cLK33bt3L0FBAfX1maYC6kxfew2yZ7fmP/zQ+vJ1FRCaN4c77vC8rwULYPFiz3mCguB/\n//NeLm9B4Px57/vwl0OH4No1f5dCqduXJ08e8ubNS968ecmWLRthYWG30ubOnZvs/QUFBXHx4kVK\nlSqVovKYLPTfYcB2j33xReuzYEHHf/4iEBcH+/d7/3J+5BHfjhMb6z1PcvzxB5QpA4UKpe5+U6pM\nGRg8GMaN83dJlLo9Fy9evDV/xx13MH36dJo3b+42f1xcHNmyeRxUQvkooO4onN28mTRt1CgICfFc\nZbRhQ/L+g/al+slbHuf19etD4cK+H98Xn30GV686lk+ccNzlHDgAP/zgefvTp2HvXtfrjh6FpTr6\nlspgXA2KN3z4cDp06ECnTp0IDw9n9uzZrF27lrvvvpv8+fNTsmRJXnjhBeLi4gArkAQFBXHw4EEA\nunbtygsvvECbNm3ImzcvjRs39vl5kiNHjvDII49QsGBBqlSpwowZM26tW7duHfXr1yc8PJzixYsz\nePBgAK5cuULnzp0pVKgQ+fPnp1GjRpzxpU7dDwIyUERFJVzu3RtWrICtWxOm//QTHDtm3WXYNWwI\nL72UMN+wYTBmjOtjnT0L7dtb8ytXWl/Av/wCJ09Cs2aOfPv3O+avXUvY8O4qkNjvfpLrn3/g+HH4\n9VfrE+Dpp62y2W3f7pgfPBjatnW9r7//tj537oSKFV3neeklaNXKfXmuXIGPPnIsL14MGzd6O4us\nxxiYPDn5212/DoMGpX55sqrvv/+eLl26cP78eZ566ilCQkKYOHEiZ86c4ddff2XJkiVMnTr1Vv7E\n1Udz585l7NixnD17ltKlSzN8+HCfjvvUU09RoUIFjh8/zhdffMGrr77K6tWrAejfvz+vvvoq58+f\nZ8+ePTz++OMAzJgxgytXrnD06FHOnDlDVFQUOXLkSKUrkcr8PUyu038GYn29up9at3ad3ratyMmT\nItaDGO6nc+dEPvpI5LvvRKpWTbjO27YgkiePdZySJUVathRp1crabsMGa/3ffzvyjh3r2K/djRsi\ndeuKR02bui7XPfc48tjXff21SOXK1nz//iKXLiXcV5ky1roGDazPoCCRiRMT5unUKWk5nc2bl3A9\niNxxh+dzGDZMZP58z3kCwblzItevp86+QOT555O/3a5dnq9/aunZU+S993zLi5cCefs78XW6HeXK\nlZOff/45QdqwYcOkRYsWHrcbP368PPnkkyIicvPmTTHGyIEDB0REpEuXLtK7d+9beefPny+1atVy\nuZ89e/ZIUFCQiIjs27dPQkND5cqVK7fWv/LKK9KjRw8REWncuLGMHj1aTp8+nWAf06ZNk6ZNm0p0\ndLTX83X3M7Glp/n3c0DeUbjjrnF6/nyrusdb21K+fNCjB/zrX0nbJmbO9H78ixet4xw5Yt3NLFli\nHbNBA2t9kSKOvPa7n27drPaBf/6B77+3emIBXLpklad9eyhXzurxtWULrF/v+ti//QY1a0LPno60\nxx+HXbus+UmTYMeOhNucOpVwOT4eBgxImObtmomLu6V9+6zyuzNmjOc2ERHH3VJKnToFuXIlTNu8\n2VHeatW8d7fOlw8GDrTmjXFcy+RYvdpRrZects3jx60OF2np3DnH/NSpVgeR1JBaoSItlC5dOsHy\nzp07efjhhylevDjh4eGMGDGCU4n/MJwUK1bs1nxYWBiXPP2i2xw7doxChQoluBsoW7YsR44cAaw7\nh+3bt1OlShUaNWrEokWLAHj66ae5//77efLJJyldujRDhw4lPj4+WeebXjJUoEhLzzyTuvuz96aa\nOdPqzZU7NzzxhJUmAnnyWA3y8+ZZ7Qwvvwx16yZtX9mzxzG/fTtMm+bb8adOhcuXrfl//nGfL/GX\n299/W12K4+KsdfY773nzEnZdnjTJKq+9U4GI1RZi5+mLYN48KF4c/vwT7roLvvvO+/m8+irs3u1Y\nPnTIOj8RR1CuV8/xD0BsrOP8AWJirHavuLiEQW7fPsf8zp3ey5HYvfdCp07J327ECKvDhXOVpitH\nj1rB6OefrWXna2wXHZ30et+8CfnzJ69MmWF4p8RVST179qRWrVrs27eP8+fPM2rUKHsNRqopUaIE\np06d4sqVK7fSDh48SMmSJQGoVKkSc+fO5eTJkwwaNIjHHnuM69evExISwhtvvMGOHTtYs2YN3377\nLbNnz07VsqUWDRQ2KWlPSKnp033PW6mS73ntv/9hYdCrlyM98Z3Gww9bwWDSJFi1ypFerRoULWqt\nt3/JRkdbn+3bw8KFCc9hyxbo399qXF+82OrplbhhvVUrx5fbzZswcSIMGWItv/qqdQdlL+uFC9ad\nSOJguGYN/Oc/MGWK1SblbONGiIiwAi1Y++vSJem1qV7d2u8bbziCNCQMlMOGJbxrLVAAbP/8JXH2\nrKODgf0u0L6vMmWsfdm9/rp1vR59NGnQdm4f2rXL6rjQpImjd1/JklYwuv9+64vc3pvu9GnH8WvX\nttqanMcjc3cn6M6+fdadbWZz8eJFwsPDyZkzJzExMQnaJ26XPeCUK1eOO++8k6FDh3L9+nW2bNnC\njBkz6Nq1KwCzZs3itO2PIG/evAQFBREUFMSKFSvYvn07IkLu3LkJCQkJ3Gcz0qN+y5cJEIKup1r9\nZyBPTz2VNvtdtUpk0ybf8n74YcLlBx5IuPzxx0m3qVbN/f4GDUqa9tpr1metWq63eeghx/y1a0nX\nP/CAvR424SQi8scf1nyjRu7LdOKEtV/7PoYPF3nySWt+2zZHvhEjEm539qzI+vWO5TNnRG7etNoT\ntm8XKVvW9fF69xaJjbXmS5VyrkcWKVfOkS821mrPcN72ySdFOnRwLNep4/68fv7Z+uzcWWTwYGs+\ne3br8+BBkStXrLYX+7VyvoYbNjjSTp4UOXXKak8bNcpany2bCM4bBqjy5cu7bKN45plnEqStWLFC\nqlSpInny5JFmzZrJ8OHDpXnz5iJitVEEBQXdaqPo2rWrjBo16ta2y5Ytk/Lly7s8vnMbhYjIoUOH\npE2bNpI/f36pVKmSTJ8+/da6jh07SuHChSVv3rxSq1Yt+fHHH0VEZNasWVK5cmXJkyePFC9eXAYN\nGiTx8fEuj+fqZ/LZZ7fSSespzQ/gc0FA6FlXKLgzTb5Es8LUqpX/y5DSyd7gnnjaty9pWmysSPXq\n3vdZrJjIffeJ/Phj0nVbt7rfbsqUpGn/+Y/344WHJ007dSppmjFJAwUkDBSepnff9by+fHmRmTOt\neccXjTW1bSsyYYJI9+4iwcFW3qT7cNpQBQRAfvhBZPFi65+VM2cS/KxI6ymwHrj74zn4v3tg1XBY\n3x8kQG/DAtSSJf4uQcpt2OA63dUT+FWr+rbP48et6pmHHkq6rk4d99uJJE3zpf7e1UOgrh68dLV/\nSPisjCfe2jv37wdbrQebNsGddzrWzZ9vTXYpGL1C+YmvDxGnhYAaZhwECuyG9s9AfDB89xmcL+vv\noimVxegw44HGaqR39TPJqsOMn6kEM1bB7jbwfAOI+BTXF0gppVR6CLw7CmfFtkD7p+F8afhhGlwq\n7peyKZW16B1FoNE7Ck+OR8BH663PXhFQazZ6d6GUUunLa6AwxpQyxiw3xmw3xkQbYwa4yNPWGLPV\nGLPZGLPeGNPYaV1rY0ysMWaXMWZwsksYFworRsPshdD0TXi8I+QMzIGzlFIqM/Ja9WSMKQYUE5Et\nxpjcwCagnYjEOuUJE5HLtvlawP9EpJoxJgjYBbQAjgIbgA7O2zrtI2nVU2LBV+D+IVDjK1g0AXY8\nDmSdMeGVSh9a9RRoAr7qSUSOi8gW2/wlIAYomSiP02AJ5AbsHfgaArtF5ICI3AC+ANqluLQ3c8Li\nCfC/r6H5COjQHvIeSvHulFJKeZesNgpjTDkgAljnYl17Y0wM8APwrC25JOD8TX6YREEmRQ7dA1M2\nw7F60Ksu3P0emHQcg0MpleEcOHCAoKCgWwPvtWnThpluRgNNnDex8uXLs3z58jQra6Dx+YE7W7XT\n18ALtjuLBETke+B7Y0wTYAzQMvnFGek0H2mb3IjLDqtGQHRneKQH1PwCvp8BJ2sk/7BKqYD34IMP\nctdddzFy5MgE6fPmzaNXr14cOXLE61hJzoMGLnQevMxL3sCx0jalL5/uKIwxwVhBYqaIzPOUV0TW\nAHcYYwoAR4AyTqtL2dLcGOk0RfpSNDhTET5bDpufhacjoflwqy1DKZWpdO/enVmzZiVJnzVrFl27\ndg3cAfVSVSQJvyfTh69X9hNgh4hMcLXSGFPBab4eECoiZ7AarysaY8oaY0KBDsB8V/u4PQY29oIp\nW6FQLPSpCRXdvLxCKZUhtW/fntOnT7NmzZpbaefOnWPBggV069YNsO4S6tWrR3h4OGXLlmXUqFFu\n99e8eXM++eQTAOLj43n55ZcpXLgwFStW5Mcff/S5XNevX2fgwIGULFmSUqVK8eKLL3LD9jKU06dP\n88gjj5A/f34KFixIM6fXZr799tuUKlWKvHnzUq1aNVasWJGs65GevFY92bq6dgaijTGbsZrehwJl\nsQakmgY8ZozpBlwHrgBPYq2MM8b0A5ZiBaXpIhKTJmcCcLEEfPWVFSQe6gOHG8Hi9+Gfoml2SKVU\n+siRIwdPPPEEn3/+OU2aNAHgyy+/pFq1atSsWROA3LlzM3PmTGrUqMGff/5Jy5YtqVu3Lm3dvS/Y\nZtq0aSxcuJCtW7cSFhbGo48+6nO5xowZw/r169m2bRsAbdu2ZcyYMYwaNYp3332X0qVLc/r0aUSE\ntWvXArBr1y4mT57Mpk2bKFq0KAcPHrz1Lu9A5DVQiMivQDYved4B3nGzbjFQJUWlS6k9rSHqT2g2\nCvrUgmVvWVVT2pVWqdtmRqXO35GMSH4X3O7du/Pwww/z4YcfEhoaysyZM+nu9CKOe++999Z8zZo1\n6dChA6tWrfIaKL766isGDhxIiRIlAHjttddY5fyyFg/mzJnD5MmTKViwIAAjRoygV69ejBo1ipCQ\nEI4dO8b+/fupUKECjRtbj5hly5aN69ev8+eff1KwYEHKlCnj6RD+lx5D1Poygfd3ZqdoKrZZ6NFA\neKapUHRr2hxDJ50y1YQvI1/7TaVKleTLL7+UvXv3SmhoqPz999+31q1bt06aN28uhQsXlvDwcMmZ\nM6d069ZNRET++usvCQoKkri4OBERiYyMvPXeiKpVq8rChQtv7Wfnzp0J8ibm/M7unDlzyo4dO26t\ni42NlezZs4uIyMWLF+Wll16SO+64QypUqCDjxo27lW/u3LnSpEkTKVCggHTs2FGOHj3q9pzdfz8i\nImn//Zz5W3+OR8D03yC6E3S7H1q+AiGXvW+nlApIXbt25bPPPmPWrFm0atWKwoUL31rXqVMn2rdv\nz5EjRzh37hw9e/bE+p71rHjx4hw65OjJfyAZ74UtUaJEgvwHDhy4dWeSO3duxo8fz969e5k/fz7v\nvfferbaIDh06sHr16lvbDrG/+jEAZf5AAdaQ5Rt7WdVReY9oY7dSGVi3bt1YtmwZH3/8cYJqJ4BL\nly6RP39+QkJCWL9+PXPmzEmw3l3QePLJJ5k4cSJHjhzh7NmzvP322z6Xp2PHjowZM4ZTp05x6tQp\nRo8efes1qD/++CN79+4FIE+ePAQHBxMUFMSuXbtYsWIF169fJzQ0lJw5cwZ0r63ALVla+KcIfDMH\nFvwX2vSFxztAnqP+LpVSKhnKli3LPffcw+XLl5O0PURFRTF8+HDCw8MZM2YMTz31VIL1zs9GOM/3\n6NGDVq1aUadOHe68804ee+wxj2Vw3nbYsGHceeed1K5d+9b2r7/+OgC7d+/m/vvvJ0+ePDRu3Ji+\nffvSrFkzrl27xpAhQyhcuDAlSpTg5MmTvPXWWym+JmktsIcZT0vBV+DeMXDnVFj9GqwbAPEh6Xd8\npQKWjvUUaAJ+rKdA0bhxwuX//AdsnQxuad7ceg2kT27mhOVj4ePfocJP1jDmZX3r5aCUUllJwAWK\nK7aHqvPmhZMnHelOz9gAULgwOD27AkCOHFCunOf9J3mO5kwlmLWIwn+Ohsc6w8M9eWHI6QRZPL1f\n2Z1ffkn+NkopFYgCKlD8+9/Wl33btvD449aL6d1VFYaEWMHEFeeXySfWoIGrVMPfqx7l6LA/6dsr\nlDkFqlG3x5RbAw1+9VXSLTZudMxXq5Z0fdOm7suglFIZSnr0wfVlAsSpG/MtcXEi167Z+xI7phs3\nRC5cSJj2yivO/Y6TTiIi8fEic+ZYy23binz+uUjLlgmPueXYFomY0ER4vr5Qcq3cvJl0X9evO/a5\nebPrY/m/P7xOOqVkIukfovIr8O9zFGl+AJ8L4sMv56efisycKRIZ6XwBRQoWFNm50woqzulz5ois\nWeO4qAkvvMj69e6PtWNHvFB7pvBScXlu3nNCztMJfkDOgcLuoYestDVrHMeIiBA5cSJlf7CFCvn7\nC0OnrDnh9W9RpS9/B4qAqnrypnt36NIFEo+dlS0bVK4Mibsht2+ftBHcV9WqGQoc7gIfxpIjOAe5\nXq0OVb8DrDIEB1vVY67Yj1mvnlV1VqRIwvXr18P27e6rzmxDxgC2X4VUZBsix6v770/d4yqlMq4M\nFSjc8WXY+NdeS/5+O3eGiqXzMqnNJJY8+w3lnnsNOrTjUvABjEnadpG4HJs2wbBhSffboAFUrw7F\ni1vLlSsnzfP00/D8847lQoUg8TtWfv/ddbkHDoS6da35iIiE6xIHp+vX4dNPk+6jY0fX+1aZX/bs\nZTHG6BRAU/bsZf37S5Eety2+TKTwdveJJxK2TdiFhlrtGO6A56onV67euCrcO1pyjigo7/32ntyM\nu5lg/cMPW/t15a67HLeLdocPi8TGilSubKXXqWN9btuWtKzPPWfNx8cn3MeFCyJlyya8HRURuXzZ\nmm/a1LoOn39uLbdpI9KrV8K8S5cmvaWdMcPf1R/ep4ULEy7XqOH/MumkU/pOiIi2UaSZf/1L5Pz5\n5G93/rzI9uM7JfLTSKk7pa788tcvt9YtXiwyYID7bb/8UqRz56Tp9gAxd65IyZIiZ88mXA/uA4WI\ntU97sLGvsweKe+9NuJ82bUQuXRIZONCR98YNke+/F6lSxUp7/XWRH3907C9/fv/8EUyaJFKzpvv1\n9nOyT8OG+aecOunkvwkR0UARsOLj42XOtjlS5v0y0nZuW9l3Zl+K93X0qPWTuHLF9fpHHhFZssRT\nWayGfPsvj4gjUDRr5sgH1r5ErIA3d27SfYHIV185AhKIdOlye7/M9eu7Tl+0SKRfP6uDQny8yLRp\nVvpzz1mfX3wh8vvvjvxRUQm3t5fXPqV3oIiISN/j6aRT0gkRSfvv50zRRuEPxhg61urIzn47ubvU\n3TT4qAHv/PoO125eS/a+ihe3fuQ5crheP38+PPCAp7Ikbch3Xme3cSN8/LE1nzcvdOiQNP+yZdCu\nnWO7ixfB3VD5BQo45rdtg8OHE663D4ZZqpTVeA9w332O9a1bw6RJVucAYyB/fiv9oYesz+rVoVEj\nR/5KlRzz2ZzekJK4fNmzuy5vcrh442YShQrd/nGUygg0UNymHME5GNJkCGufW8uag2uoHlWdH3b+\ngIike1mmTnUqly3oOD9VXr9+0h5YibVoYT3M6ErjxlYjd2ystVy6tGNdrVpQsmTCL3Cnd8hQvbr1\nGR6esHzOitpeRNiunRU4a9Wylpcvd+yvRg1rPjrasd2CBQn3U6qUtf35867PI6VsI0crleVooEgl\nFQtUZH7H+US1iWLwssE8OPtBYk/FpmsZijq98dUYuHYN3nsv9fa/Zg3MmQNVqsDOnbBoUdI8ruKj\n812Nfd7VUCtNm1p3MIl7jzVv7phfvdr6dPU0fGJ580KvXt7zuZL4PO66K+m+fOltl1Ljx6fdvpVK\nLg0UqaxVxVZs7bWV1hVb03RGU15a8hLnr6byv7Y+Cg11XyXlK2OgYUPruRFnlSs7uvc6K1kyaVpy\nbq5y5/Zclly5rMnuwQehfPmk+eycuxgnh3OZO3eGlSth+HDXx/H2jpt8+axPe1XdsWOejytiVcvZ\n78JSg7c7SaU80UCRBkKyhTCw0UC299nO+WvnqTa5GnOi56R5dVRqv/dk40brS7ldO7hxw3WeJ55I\nus3x4+73eTv/hRtjBb9LlxxpCxcmDS6+HGPnzqQDTboza5brqrJWraw7G2+vO7b/2O0BzvnXwLma\n7+GHHfM1ajjadXxRr57n9Sm57gMGJH8blTlpoEhDRXIV4eO2H/PNk98w/rfxtPi8BdEnor1vmEIP\nPghLl6be/urX955n2jTrS9euSBFHFVji//TBfYO9NzduJL2rSeyOO6xP5y9F5y/luXMd85UrW20u\niWN3zpxJt3OlfHkYNAh27PCcD+Bf/7I+ne+EAMqWTVhW584Bzlq18n4Mb1KjgV8l5W50hcxGA0U6\nuLv03azvsZ5Hqz1Ki89b0HtBb07+c9L7hskUHAwtW6b6bj3Kl8/1k+UiULWqNW//Mty61erlBAnb\nU3zhLUhcuWI9ze58PHs5AMaOdd3LK7GwsITbuZI9O+zb531fdjNmWJ/2xn37MSpUcOSZORMmTnS9\n/fvv+34sV2JjrbadxFVkmzc75j/5xPqMi3OMfGy/BqNHW8Pm//FH8o89d27yqh5VYNJAkU6Cg4Lp\n17Afsf1iCc0WSvWo6nyw9gNuxLmp08mEate2usAeOwbz5qXuvnPkcAQIV8PMDx2avP21bGl9obsK\nvCmtPrNvFx4OH34IY8Y41lWr5ugRlljiO5FEb/f0qkoVq3rMuYrs668TBip72YyxumOD4wt+2DAr\neCS+Q7x82fuxfQnOnv5pcNVhwlfOd7ppxd6VO7PzGiiMMaWMMcuNMduNMdHGmCQ1l8aYTsaYrbZp\njTGmttO6v2zpm40x61P7BDKaAjkLMOHBCfzy9C/8uPtH6k6tyy8HstZbjooVc/+lmBoGDXLMV6mS\n9Msq8TMpYWGOtgL7l2OJEvDPP75X5S1blrwy9u0Ld9/tPd/p00nbQF55JXnHcqVJk4TVgImDESRt\n+7E3yoMVNOzVdLejfXsraLpjr05MCVd3uqlp6FDv7VOZhrcn8oBiQIRtPjewE6iaKE8jINw23xpY\n67RuH5Dfh+O4f/Q4k4qPj5evt38tpd4rJd2/6y5/X/rb30VKde3bi/zvf+l3PBDZtMn9+lOnRC5e\nTJh24oTIoUMi99xjDU0fFeV5/zlyuF731VcJn5pt1kzk778d2/XsaX06y5fPStu40ft52Sfn5VKl\nrM/oaJHgYNdP77raj/Py6tXWk/32YfrPnrXSL18WiYlJun21aq7LZZ/mznV9HHdlmz8/YZrz8Pq7\ndrne7oMPEi5PnChijPvrlBbT0KEiP/2UtsfwPiHi5bs1NSavdxQiclxEttjmLwExQMlEedaKiL0P\n6NpE6w1axeWSMYbHqj/Gjj47KJizIDWiajB141Ruxt/0d9FSzXffJe0Z5U8FCyb9T7lIEeshvV9/\ntR4o7N07Zft+/PGEd0q5clmv7LUrWjRpVdaWLb7tOzLS+nQeYv/xx+GHH6y0mjXh229TVGyqV7d6\nzNl7zYntripnTkc7kzf2n3GNGt6rmxK/694TX0YcAOsO53buPlLi0UfT9lmaQJKsL3BjTDkgAljn\nIdtzgHPNogA/GWM2GGN6JLeAWUGe7Hl4t9W7LO26lDl/zqHhRw1Zd9jTJVb+8tpr8Prr7tfbv2Sb\nNrWqVZzlyJG0KqtsWd+O+/PPVkOzPWDYRUQ40h55JGHDsa9f8qGhvuWzc37i3m7kSOvTly/Ob76x\nGu7t+0n8emLnc3AOFP37W5+1ayctQ8mSyf/STum7auzq1095L74Mx9dbD6xqp41AOw95mgPbcapq\nAorbPgsDW4AmbraVESNG3JpWrFghWVF8fLzM3DpTio8vLs9+/6ycuHTC30XKUEBk927/HT9vXke1\nhzMQefPhkU0PAAAf20lEQVRN19v4UvXkapvHH3e97oUXRHbscIwGnHg757TE1XAiImfOuD4HEZEV\nKxIO3797t5V3+3brs2ZNKz3xkPvuqsKc148bZ80XLOjId/CgY7v+/V3vs1Ila7lVK5Hs2UVOnnTk\nc1clBiIdO7quzomPF2nYMGl6pUrWyMsgcscd1v6dB8/0ZUpcZZb8aYXACKcpfaqefA0SwcBi4AUP\neWoDu4EKHvKMAAa5Wef6NyiLOnflnAxaPEgKvl1Q3l7ztly7ec3fRcoQzp3z7/Fbt7aGjU8MrNFx\nXalaVeT06eQdx1OgsHMVKMaNc/9lbXf6tPc8ictiDxS1allprgLFY4/5FigKFHDkO3TI8SU5eXLC\n7b/+2lquUsVa/ucfq33FeSj+xAGqfHnHcqdOrr+MRUSGDEmafuiQY58VKiQsu7dp61br03mUZ3fT\nlClJ37XifkqfQOFr1dMnwA4RmeBqpTGmDPAN0FVE9jqlhxljctvmcwEPAH/6eMwsLTxHOO+2epff\n/u83fjnwC7X+W4vFexb7u1gBLy17U/liwQLr6fTEjh6F//s/19vExLh/2M6dF17w3pZi/f+VPnyp\n9smTJ+X7/fFH63ydz+mxxxLmDQuzemYZ4/45l+ho37r1JpY9u9WOZWcfsNJX9gE0fRk9oWRJx+CX\ngcLLY0xgjGkMdAaijTGbAQGGAmWxotk0YDhQAIgyxhjghog0BIoC3xljxHas2SKSis8OZ36VC1Zm\nQacFLNi1gP6L+lOzSE0mtp5I6fDS3jdW6c559FxnrsbFuh0ffJC6+0stngJGqVJw6lTK9tumjev0\njz923UXV1agA4LobsC8SB117m4yv3F2XJ55wvFI5NNR6NbFz/hs33I/mnJ68BgoR+RVw8+t/K08P\nIElDtYjsx2r8Vrfp4coP0/KOloxbM466U+vyyj2vMLDRQLIH69gMyrXFi+Hs2eRvdzt3It4ad33p\n8dSzp2O8MG93Ku7u0hJzFSCc950njzVysd3998O4ca73tXdv6vWw+uwzK1B8/rk1plp4eMLAl3hE\ngvLlYf/+1Dl2cmi31Qwke3B2RkSO4Pf/+53fDv9GjagazN85397Go1QC5ct7HywwtRhjVaF5euLe\n14fT3nzTMaSI80N+t8N5IEk752og+7Aqdi1aJBw00vk6+hoknnzS0UPO259ojhzWuFEiVq+uxGrX\nhrZt4YsvfDt2atNAkQFVKliJeR3mMbnNZAYvG0zr2a2JORnj72KpDKBNm7QbdqJqVeupe1cuXIDn\nnvO8fd26SdPS6jmFhg2tJ9yXLrXezuhtJOGVK92vmzjR9Rf4l1/63taQOPgkPu/Ro60gbA84qfFU\nfHJ4rXpSgatVxVZsK7+NyRsmc++n99KlVhfeaPYG+XPm93fRVICqVSvpGwET8zYAoyvevtC9NWSn\n903xOttjSr42Snsafbd/f/dDwv/rX442CHd8OffE1/fdd6FPH+/bpRa9o8jgnN99ceXmFapOrsrE\ndRO5Hnfd30VTGVR4OOzZc3v7SO7owIHIPrikLz2VqleHn35yLNsftsyRw3qC/nbZA4W/apk1UGQS\nRXIVYcrDU/ip608s2rOI6pP99+5ulfE5jyzri8TDokyaZHUJvl3uepGlhenT4aOPHMvJeYeHMVYD\n+MSJEBVlDV3jSevWySubvwOFVj1lMrWL1mZR50Us3buUAYsGMO2PaYxvOZ4qhar4u2gqk7p0KWmv\nopw5U6cePTQ0ZT23nP33v77le/bZ2zsOOIYZSSxv3oSv5fXWOyzQxpDSO4pM6oEKD7C111buLXMv\nTWY0oe+Pffn7n7/9XSyVCaX02QRf3W7Pp9R+RXBKZMsGU6c6lpMbCOz5/TWseQBcQpVWsgdn55XG\nrxDbN5aQbCFUn1ydN1e/yeUbKXg0VSmValIaKEqWtKqfKlZM/TJ5ooEiCygYVpAPWn/A2ufWsvn4\nZqp8WIVPt3xKXHycv4umVEDr1g26d0/9/ebKZb2wyV0VVOHCnofnb9kS4uNTv1zumEBp7DTGSKCU\nJbP7/dDvvPzTy/xz/R/+0/I/tKyQzi/aViqdGGNV+Ti3D/hbTIz1BkVfxyUzxnolrKsGcGMMIpLm\nLRp6R5EF3V36btY8s4bh9w6nz8I+tJ7Vmm0ntvm7WEqliUBrGPb0fnR3/H0OGiiyKPvb9bb32c5D\nlR6i5cyWPDvvWY5eTIU+jUqpVFWpkn+Pr4EiiwvNFkr/u/qzs99OCoUVotZ/azFy5UguXrvofWOl\nVJoTSf/XvCamgUIBkC9HPt5p+Q6bnt/EnjN7qPxhZSavn8yNuBv+LppSys80UKgEyuUrx6xHZ7Go\n8yLm7ZxH9ajqfB/7vT7hrTKs5D5lrpLSXk/KoyV7lvDS0pcokqsI77d6nzrF6vi7SEopm/Tq9aSB\nQnl1M/4mH236iJGrRtKmUhveavEWxXK7GU9aKZVutHusChjBQcH0btCbXf12USSsCDWiajB61Wh9\nwlupLELvKFSy7T+7n1eXvcqGIxsYd/84nqrxFMbfHb2VyoK06kkFvFV/rWLQ0kGEBIXwTst3uLfs\nvf4uklJZigYKlSHESzxzoucwfMVwqhaqytv3v03toi5e+quUSnXaRqEyhCATRJfaXdjZbycPVnyQ\nljNb0m9hPx3SXKlMRAOFShWh2UIZcNcAtvfZTpAJotrkaoxcOZIL1y74u2hKqdvkNVAYY0oZY5Yb\nY7YbY6KNMQNc5OlkjNlqm9YYY2o7rWttjIk1xuwyxgxO7RNQgaVQWCEmPjiRjT02su/sPipNqsQH\naz/g6s2r/i6aUiqFvLZRGGOKAcVEZIsxJjewCWgnIrFOeRoBMSJy3hjTGhgpIo2MMUHALqAFcBTY\nAHRw3tZpH9pGkQlFn4jm9eWvs/XEVsa1GEeHmh20h5RSqSRgG7ONMd8Dk0TkZzfr8wHRIlLaFkBG\niMiDtnVDABGRt11sp4EiE1t9YDUDlwwkNFso41uOp3GZxv4uklIZXkA2ZhtjygERwDoP2Z4DFtnm\nSwKHnNYdtqWpLKZp2aZs6LGBPnf2odO3nWj/RXu2Ht/q72IppXwQ7GtGW7XT18ALInLJTZ7mwDNA\nk5QUZuTIkbfmIyMjiYyMTMluVIAKMkF0rdOVx6s/zpSNU2g9uzVNyzTlrRZvUaGAjtymlDcrV65k\n5cqV6X5cn6qejDHBwAJgkYhMcJOnNvAN0FpE9trSGmG1V7S2LWvVk7rl8o3LvPf7e3yw9gOeqvEU\nbzR7g6K5i/q7WEplGIFW9fQJsMNDkCiDFSS62oOEzQagojGmrDEmFOgAzL+dAqvMIywkjGH3DiO2\nXyyh2UKpHlVdX5qkVADypddTY+AXIBoQ2zQUKIt1dzDNGPMR8ChwADDADRFpaNu+NTABKyhNF5Fx\nbo6jdxRZ3F/n/mLY8mH8vP9nhjUdxvP1nyckW4i/i6VUwArYXk9pRQOFstt8bDNDfh7CvrP7GHvf\nWJ6o/oR2qVXKBQ0UKstbtm8ZQ5YNIU7ieOf+d2hZoaW/i6RUQNFAoRQgInwb8y2v/fwa5fKVY9z9\n46hXvJ6/i6VUQNBAoZSTG3E3+PiPj/n3L//mvvL3Mab5GMrnL+/vYinlV4HW60kpvwrJFkLvBr3Z\n3X83VQpWocFHDRi4eCAn/znp76IplelpoFAZSu7Q3LzR7A129N1BXHwc1SZXY8wvY/jn+j/+LppS\nmZYGCpUhFclVhEltJrH2ubX8+fefVP6wMlM3TuVm/E1/F02pTEfbKFSmsPHoRgYvG8yRC0cY3Xw0\nj1V/jCCj/wepzE0bs5VKJhFhyd4lvLHiDa7FXWNM8zE8XPlhfQZDZVoaKJRKIRHhh10/MGz5MMJC\nwhh731ha3NHC38VSKtVpoFDqNsVLPF/++SVvrHyD0nlLM/a+sdxd+m5/F0upVKOBQqlUciPuBp9t\n/Yx/r/o3dYrVYXTz0UQUi/B3sZS6bRoolEplV29eZerGqYz7dRwNSzZkWNNhNCjZwN/FUirFNFAo\nlUau3LjC9M3TefvXt6lZpCb/jvy3BgyVIWmgUCqNXbt5jRlbZjD6l9HcXepuxt43liqFqvi7WEr5\nTIfwUCqNZQ/OTq87e7G7/24alGhAkxlN6PNjH45dPObvoikVUDRQqCwvLCSMwU0GE9M3hrCQMGpE\n1eDFxS9y/NJxfxdNqYCggUIpm0JhhRj/wHi299mOINSIqsGIFSM4f/W8v4umlF9poFAqkeJ5ivNB\n6w/Y9PwmDpw/QIWJFRi9ajTnrp7zd9GU8gttzFbKi12ndzF29Vh+3PUjfRv05cW7XyRfjnz+LpZS\n2pitVKCoXLAyn7X/jHXPrePghYNUnFiREStGcObKGX8XTal0oYFCKR9VKFCBGe1msPa5tRy+cJhK\nkyoxbPkwDRgq09NAoVQyVSxQkentprOxx0aOXzpO5UmVGfPLGC5eu+jvoimVJjRQKJVC5fOX5+O2\nH/P7//1OzKkYKkyswNtr3ta37alMx2ugMMaUMsYsN8ZsN8ZEG2MGuMhTxRjzmzHmqjFmUKJ1fxlj\nthpjNhtj1qdm4ZUKBJUKVmL2o7NZ9fQqNh3bRIWJFRj/23gu37js76IplSq89noyxhQDionIFmNM\nbmAT0E5EYp3yFALKAu2BsyLyntO6fUB9ETnr5Tja60llCtEnohm1ahS/HvqVwY0H83z95wkLCfN3\nsVQmFDC9nkTkuIhssc1fAmKAkonynBKRTYCrFxYbX46jVGZRq2gtvn7yaxZ1XsSqA6so+0FZRq8a\nzYVrF/xdNKVSJFlf4MaYckAEsC4ZmwnwkzFmgzGmR3KOp1RGFlEsgu+e+o7fnv2Nnad3UnFiRcat\nGcel65f8XTSlkiXY14y2aqevgRdsdxa+aiwix4wxhbECRoyIrHGVceTIkbfmIyMjiYyMTMZhlApM\nlQpWYtajs4g5GcO/f/k3FSZWYFCjQfRu0Ju82fP6u3gqA1m5ciUrV65M9+P69GS2MSYYWAAsEpEJ\nHvKNAC46t1H4ul7bKFRWsf3v7by55k2W7l1K/4b9GXDXAH3SW6VIwLRR2HwC7PAUJJzcKrQxJsx2\nJ4IxJhfwAPBnskupVCZSo0gNZj86m1+f/ZV9Z/dRcWJFfXBPBTRfej01Bn4BorHaGwQYitXLSURk\nmjGmKLARyAPEA5eA6kBh4DvbNsHAbBEZ5+Y4ekehsqR9Z/fx1uq3+Db2W3rW78mguwdRKKyQv4ul\nMgB9w51SWczB8wcZ88sYvt7xNU9HPM3L97xMiTwl/F0sFcACrepJKZXGyoSXYdoj09jWexsiQs2o\nmvRe0Jv9Z/f7u2gqi9NAoVSAKZW3FO+3fp+d/XaSL0c+7vzoTrp/353YU7HeN1YqDWjVk1IB7tzV\nc0xaN4lJ6yfRrFwzhjYZSt3idf1dLBUAtI1CKZXApeuXmLZpGu/+/i4RxSJ4venr3FP6Hn8XS/mR\nBgqllEtXb17l0y2f8vavb1MuXzleb/o6Lcq3wJg0/75QAUYDhVLKoxtxN5j751zeWvMWebPn5fWm\nr/NI5Uc0YGQhGiiUUj6Ji4/ju9jvGLt6LHHxcQxtOpQnqj9BtqBs/i6aSmMaKJRSySIiLNqziLGr\nx3Lq8imGNB5Cl9pdCMkW4u+iqTSigUIplSIiwqoDqxi7eiy7Tu/i1Xte5dm6z5IzJKe/i6ZSmQYK\npdRtW3d4HW+ueZP1R9YzqNEget3ZizzZ8/i7WCqVaKBQSqWabSe28daat1i6dym96vdiwF0DKJq7\nqL+LpW6TDuGhlEo1tYvWZu5jc9nYYyNnr56l6uSq9FvYjwPnDvi7aCoD0EChVBZSPn95oh6KIrZv\nLGEhYdSbVo/O33Zm6/Gt/i6aCmBa9aRUFnb+6nmmbprKB2s/oHbR2rzY6EUeqPCAPouRQWgbhVIq\n3Vy7eY3Z0bOZsG4CcfFxvHzPy3Sq1YnQbKH+LpryQAOFUirdiQjL9i3jnd/eIeZkDAPuGkDP+j0J\nzxHu76IpFzRQKKX8asvxLYz/bTwLdy/kmYhnGNhoIKXDS/u7WMqJ9npSSvlVRLEIZj06iy29tiAI\ndabUodt33dh2Ypu/i6bSmd5RKKV8cvbKWaZsnMKHGz6kaqGqDG48mJZ3tNSGbz/SqielVEC6EXeD\n2dGzGf/beOIlngF3DaBr7a7kCs3l76JlORoolFIBTURY+ddKJqybwG+HfqNvg770a9iPgmEF/V20\nLEMDhVIqw4g9Fcv438bzbcy3dKvTjX4N+1GxQEV/FyvT00ChlMpwjlw4wqT1k/hk8yc0KNmAAQ0H\n6AN8aShgAoUxphTwOVAUiAc+EpGJifJUAWYA9YChIvKe07rWwAdYPaymi8jbbo6jgUKpTOLKjSvM\niZ7DxPUTuXrzKi/d/RLd63Qne3B2fxctUwmkQFEMKCYiW4wxuYFNQDsRiXXKUwgoC7QHztoDhTEm\nCNgFtACOAhuADs7bOu1DA4VSmYyIsPrgasatGccfx/6gb4O+9GnQR9sxUknAPEchIsdFZItt/hIQ\nA5RMlOeUiGwCbibavCGwW0QOiMgN4AugXaqUXCkV8Iwx3Fv2XhZ2Xsjy7sv569xfVJpUid4LehN7\nKsn/iypAJeuBO2NMOSACWOfjJiWBQ07Lh0kUZJRSWUP1wtWZ3m462/tsp0iuIkR+GskDMx9g/s75\nxMXH+bt4yoNgXzPaqp2+Bl6w3VmkupEjR96aj4yMJDIyMi0Oo5Tyo+J5ijOq+SiGNh3KVzu+Yswv\nY3h56cu82vhVutbuqu0YHqxcuZKVK1em+3F96vVkjAkGFgCLRGSCh3wjgItObRSNgJEi0tq2PAQQ\nVw3a2kahVNZkf8f3uDXj2HZiG73v7M3z9Z/XN/D5IGDaKGw+AXZ4ChJOnAu9AahojClrjAkFOgDz\nk1lGpVQmZowhslwki7ssZnGXxRw8f5Cqk6vS8ZuObDiywd/FU/jW66kx8AsQDYhtGorVy0lEZJox\npiiwEciD1YX2ElBdRC7ZusdOwNE9dpyb4+gdhVIKgAvXLjBt0zQmb5hM0VxFeenul/hXtX8RHORz\nbXmWEDDdY9OLBgqlVGJx8XHM2zmP935/j8MXDtOnQR961OtB/pz5/V20gKCBQimlnGw6uokJ6yaw\nYNcCOtbsyIC7BlClUBV/F8uvNFAopZQLRy8eJWpDFNM2Tbs1TEjLCi0JMlnv9ToaKJRSygP7MCGT\n1k/iys0r9GvQj+4R3cmbPa+/i5ZuNFAopZQPRIQ1B9cwaf0klu1bRpfaXejXsB+VC1b2d9HSnAYK\npZRKpsMXDvPfDf/l480fU7dYXQbcNYDWFVtn2mopDRRKKZVCV29e5cs/v2Ti+olcuHaBvg368kzE\nM4TnCPd30VKVBgqllLpNIsLvh39n0vpJLNmzhKdqPEXfhn2pWaSmv4uWKjRQKKVUKjp68SgfbfqI\nqZumUqlgJfo16Ef7qu0JyRbi76KlmAYKpZRKAzfibvBd7HdM3jCZPWf28Hy95+lRvwcl8pTwd9GS\nTQOFUkqlsegT0URtiOKL7V/Q8o6W9GvYj6ZlmmaYV7dqoFBKqXRy/up5Pt/6OVEbowgOCqZvg750\nqd2F3KG5/V00jzRQKKVUOhMRlu9fzuQNk1l1YBWda3WmT4M+VC1U1d9Fc0kDhVJK+dGh84eYumkq\nH//xMTWL1KRvg748UuWRgBrBVgOFUkoFgGs3r/FNzDdM3jCZQ+cP0bN+T3rU70GRXEX8XTQNFEop\nFWg2H9vM5A2T+SbmG9pUakO/Bv1oVKqR3xq/NVAopVSAOnvlLDO2zCBqQxR5s+elb4O+dKzVkbCQ\nsHQthwYKpZQKcPESz9K9S5m8YTK/H/qd7nW607tBbyoWqJgux9dAoZRSGcj+s/uZsnEKM7bMoH6J\n+vRt0JcHKz5ItqBsaXZMDRRKKZUB2QcknLxhMicvn6RX/V48U/eZNGn81kChlFIZ3IYjG4jaGMX3\nsd/TplIbBjQcQMOSDVOt8VsDhVJKZRLnrp5j+h/TidoYRb4c+ejXoB8da3UkR3CO29qvBgqllMpk\n4iWeJXuWMGHdBDYf38zTdZ6mR/0eKW781kChlFKZ2K7Tu5i2aRqfb/2cOsXq8Hy952lXtR2h2UJ9\n3kfABApjTCngc6AoEA98JCITXeSbCDwI/AM8IyKbbel/Aedt294QkYZujqOBQimV5Vy7eY1vY75l\n2h/TiDkZw9MRT9OjXg8qFKjgddtAChTFgGIissUYkxvYBLQTkVinPA8C/UTkIWPMXcAEEWlkW7cP\nqC8iZ70cRwOFUipL23lqJx/98RGfbf2MiGIRXu8yAiZQJNnAmO+BSSLys1PaFGCFiHxpW44BIkXk\nhDFmP3CniJz2sl8NFEophdXF9tuYb5m2aRqxp2J5JuIZnqv3XJK7jPQKFEHJyWyMKQdEAOsSrSoJ\nHHJaPmJLAxDgJ2PMBmNMj5QVUymlso4cwTnoVKsTK59eycqnV3I97jqNpjfigZkP8PWOr7kRdyNd\ny+PzHYWt2mklMFpE5iVa9wPwloj8ZlteBrwqIn8YY4qLyDFjTGHgJ6wqqjUu9i8jRoy4tRwZGUlk\nZGTKzkoppTKZqzevMubzMcyaP4tTl08RUSyCX2f+GjhVT8aYYGABsEhEJrhYn7jqKRZoJiInEuUb\nAVwUkfdc7EOrnpRSygexp2KZtmka77d+P6ACxefAKREZ5GZ9G6CvrTG7EfCBiDQyxoQBQSJyyRiT\nC1gKjBKRpS72oYFCKaWSIb3aKLy+qskY0xjoDEQbYzZjtTkMBcoCIiLTRGShMaaNMWYPtu6xts2L\nAt8ZY8R2rNmugoRSSqnApQ/cKaVUBhWQvZ6UUkplPRoolFJKeaSBQimllEcaKJRSSnmkgUIppZRH\nGiiUUkp5pIFCKaWURxoolFJKeaSBQimllEcaKJRSSnmkgUIppZRHGiiUUkp5pIFCKaWURxoolFJK\neaSBQimllEcaKJRSSnmkgUIppZRHGiiUUkp5pIFCKaWURxoolFJKeaSBQimllEcaKJRSSnnkNVAY\nY0oZY5YbY7YbY6KNMQPc5JtojNltjNlijIlwSm9tjIk1xuwyxgxOzcIrpZRKe77cUdwEBolIDeBu\noK8xpqpzBmPMg0AFEakE9ASm2NKDgA+BVkANoGPibVVSK1eu9HcRAoJeBwe9Fg56LdKf10AhIsdF\nZItt/hIQA5RMlK0d8Lktzzog3BhTFGgI7BaRAyJyA/jClld5oH8IFr0ODnotHPRapL9ktVEYY8oB\nEcC6RKtKAoeclg/b0tylK6WUyiB8DhTGmNzA18ALtjsLj9lvq1RKKaUChhER75mMCQYWAItEZIKL\n9VOAFSLypW05FmgGlAdGikhrW/oQQETkbRf78F4QpZRSCYhImv9jHuxjvk+AHa6ChM18oC/wpTGm\nEXBORE4YY04BFY0xZYFjQAego6sdpMfJKqWUSj6vgcIY0xjoDEQbYzYDAgwFymLdHUwTkYXGmDbG\nmD3AP8AzWCvjjDH9gKVY1VzTRSQmjc5FKaVUGvCp6kkppVTW5fcnszPjA3nuHlI0xuQ3xiw1xuw0\nxiwxxoQ7bfOa7YHFGGPMA07p9Ywx22zX5wOn9FBjzBe2bX43xpRJ37NMHmNMkDHmD2PMfNtylrwW\nxphwY8xXtnPbboy5KwtfixeNMX/azmO2rexZ4loYY6YbY04YY7Y5paXLuRtjutvy7zTGdPOpwCLi\ntwkrUO3BqsYKAbYAVf1ZplQ6r2JAhG0+N7ATqAq8DbxqSx8MjLPNVwc2Y1UFlrNdE/vd3jqggW1+\nIdDKNt8biLLNPwV84e/z9nJNXgRmAfNty1nyWgCfAs/Y5oOB8Kx4LYASwD4g1Lb8JdA9q1wLoAnW\nowbbnNLS/NyB/MBe2+9dPvu81/L6+WI1wupJZV8eAgz29w8xDc7ze+B+IBYoaksrBsS6Om9gEXCX\nLc8Op/QOwH9t84uBu2zz2YCT/j5PD+dfCvgJiMQRKLLctQDyAntdpGfFa1ECOGD74grG6hCTpf5G\nsP5Bdg4UaXnufyfOY1v+L/CUt7L6u+op0z+QZxwPKa7F+iU4AdYT70ARW7bE1+EIjgcWDzulO1+f\nW9uISBxwzhhTIE1O4va9D7yC1RHCLitei/LAKWPMDFs13DRjTBhZ8FqIyFHgXeAg1nmdF5FlZMFr\n4aRIGp77edu5u9uXR/4OFJmaSfqQYuKeA6nZkyAguxcbYx4CTog1DIynMmb6a4H1n3M9YLKI1MPq\nITiErPl7kQ9rOJ+yWHcXuYwxncmC18KDgDl3fweKI4BzA1MpW1qGZ6yHFL8GZorIPFvyCWONgYUx\nphjwty39CFDaaXP7dXCXnmAbY0w2IK+InEmDU7ldjYG2xph9wFzgPmPMTOB4FrwWh4FDIrLRtvwN\nVuDIir8X9wP7ROSM7T/e74B7yJrXwi49zj1F37n+DhQbsD2QZ4wJxao/m+/nMqUWVw8pzgeets13\nB+Y5pXew9VQoD1QE1ttuP88bYxoaYwzQLdE23W3zTwDL0+xMboOIDBWRMiJyB9bPd7mIdAV+IOtd\nixPAIWNMZVtSC2A7WfD3AqvKqZExJoftHFoAO8ha18KQ8D/99Dj3JUBLY/W+yw+0tKV5FgANOq2x\negXtBob4uzypdE6NgTisXlybgT9s51kAWGY736VAPqdtXsPqzRADPOCUXh+Itl2fCU7p2YH/2dLX\nAuX8fd4+XJdmOBqzs+S1AOpg/YO0BfgWq/dJVr0WI2zntQ34DKvnY5a4FsAc4ChwDStoPoPVsJ/m\n544VjHYDu4BuvpRXH7hTSinlkb+rnpRSSgU4DRRKKaU80kChlFLKIw0USimlPNJAoZRSyiMNFEop\npTzSQKGUUsojDRRKKaU8+n8HuMKGN4637gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12100d470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEACAYAAABRQBpkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be3f940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_train = np.array(nn.losses['train'])\n",
    "loss_train_norm = (loss_train - loss_train.mean(axis=0))/ loss_train.std(axis=0)\n",
    "\n",
    "loss_valid = np.array(nn.losses['valid'])\n",
    "loss_valid_norm = (loss_valid - loss_valid.mean(axis=0))/ loss_valid.std(axis=0)\n",
    "\n",
    "plt.plot(loss_train_norm, label='Train loss normalized')\n",
    "plt.plot(loss_valid_norm, label='Valid loss normalized')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKZcShYiIlOv/A7AKutLU8b+XAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11355d320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
