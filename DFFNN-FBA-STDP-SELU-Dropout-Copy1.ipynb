{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.selu_forward(X=y)\n",
    "#         if train:\n",
    "#             y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#             caches.append((fc_cache, do_cache)) # caches[0]\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.selu_forward(X=y)\n",
    "#             if train:\n",
    "#                 y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#                 do_caches.append(do_cache)\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "#         if train:\n",
    "#             caches.append((fc_caches, do_caches)) # caches[1]\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "            caches.append((fc_cache, do_cache)) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache, do_cache = caches[2]\n",
    "        dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "#         fc_caches, do_caches = caches[1]\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.dropout_backward(cache=do_caches[layer], dout=dy)\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "#         fc_cache, do_cache = caches[0]\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3149 valid loss: 2.3169, valid accuracy: 0.1036\n",
      "Iter-200 train loss: 2.3083 valid loss: 2.3165, valid accuracy: 0.1038\n",
      "Iter-300 train loss: 2.3061 valid loss: 2.3162, valid accuracy: 0.1040\n",
      "Iter-400 train loss: 2.3044 valid loss: 2.3159, valid accuracy: 0.1044\n",
      "Iter-500 train loss: 2.3039 valid loss: 2.3155, valid accuracy: 0.1046\n",
      "Iter-600 train loss: 2.2960 valid loss: 2.3151, valid accuracy: 0.1046\n",
      "Iter-700 train loss: 2.3109 valid loss: 2.3147, valid accuracy: 0.1046\n",
      "Iter-800 train loss: 2.3041 valid loss: 2.3143, valid accuracy: 0.1046\n",
      "Iter-900 train loss: 2.3007 valid loss: 2.3140, valid accuracy: 0.1046\n",
      "Iter-1000 train loss: 2.3120 valid loss: 2.3137, valid accuracy: 0.1048\n",
      "Iter-1100 train loss: 2.2916 valid loss: 2.3133, valid accuracy: 0.1048\n",
      "Iter-1200 train loss: 2.3091 valid loss: 2.3129, valid accuracy: 0.1050\n",
      "Iter-1300 train loss: 2.2946 valid loss: 2.3126, valid accuracy: 0.1056\n",
      "Iter-1400 train loss: 2.3088 valid loss: 2.3122, valid accuracy: 0.1056\n",
      "Iter-1500 train loss: 2.3078 valid loss: 2.3119, valid accuracy: 0.1060\n",
      "Iter-1600 train loss: 2.3065 valid loss: 2.3115, valid accuracy: 0.1068\n",
      "Iter-1700 train loss: 2.2973 valid loss: 2.3111, valid accuracy: 0.1070\n",
      "Iter-1800 train loss: 2.3053 valid loss: 2.3107, valid accuracy: 0.1078\n",
      "Iter-1900 train loss: 2.3034 valid loss: 2.3104, valid accuracy: 0.1080\n",
      "Iter-2000 train loss: 2.3009 valid loss: 2.3101, valid accuracy: 0.1080\n",
      "Iter-2100 train loss: 2.2950 valid loss: 2.3097, valid accuracy: 0.1084\n",
      "Iter-2200 train loss: 2.3026 valid loss: 2.3093, valid accuracy: 0.1084\n",
      "Iter-2300 train loss: 2.3068 valid loss: 2.3089, valid accuracy: 0.1090\n",
      "Iter-2400 train loss: 2.3048 valid loss: 2.3085, valid accuracy: 0.1094\n",
      "Iter-2500 train loss: 2.2997 valid loss: 2.3082, valid accuracy: 0.1098\n",
      "Iter-2600 train loss: 2.2993 valid loss: 2.3079, valid accuracy: 0.1110\n",
      "Iter-2700 train loss: 2.3068 valid loss: 2.3076, valid accuracy: 0.1114\n",
      "Iter-2800 train loss: 2.3002 valid loss: 2.3072, valid accuracy: 0.1118\n",
      "Iter-2900 train loss: 2.3005 valid loss: 2.3069, valid accuracy: 0.1118\n",
      "Iter-3000 train loss: 2.3004 valid loss: 2.3065, valid accuracy: 0.1124\n",
      "Iter-3100 train loss: 2.3026 valid loss: 2.3062, valid accuracy: 0.1130\n",
      "Iter-3200 train loss: 2.3033 valid loss: 2.3058, valid accuracy: 0.1132\n",
      "Iter-3300 train loss: 2.3070 valid loss: 2.3054, valid accuracy: 0.1132\n",
      "Iter-3400 train loss: 2.3070 valid loss: 2.3050, valid accuracy: 0.1140\n",
      "Iter-3500 train loss: 2.3038 valid loss: 2.3047, valid accuracy: 0.1142\n",
      "Iter-3600 train loss: 2.3112 valid loss: 2.3044, valid accuracy: 0.1148\n",
      "Iter-3700 train loss: 2.3076 valid loss: 2.3040, valid accuracy: 0.1154\n",
      "Iter-3800 train loss: 2.3069 valid loss: 2.3037, valid accuracy: 0.1160\n",
      "Iter-3900 train loss: 2.3032 valid loss: 2.3033, valid accuracy: 0.1162\n",
      "Iter-4000 train loss: 2.3079 valid loss: 2.3030, valid accuracy: 0.1168\n",
      "Iter-4100 train loss: 2.3000 valid loss: 2.3027, valid accuracy: 0.1176\n",
      "Iter-4200 train loss: 2.3047 valid loss: 2.3024, valid accuracy: 0.1176\n",
      "Iter-4300 train loss: 2.3062 valid loss: 2.3020, valid accuracy: 0.1178\n",
      "Iter-4400 train loss: 2.3055 valid loss: 2.3017, valid accuracy: 0.1178\n",
      "Iter-4500 train loss: 2.2893 valid loss: 2.3014, valid accuracy: 0.1182\n",
      "Iter-4600 train loss: 2.2970 valid loss: 2.3011, valid accuracy: 0.1188\n",
      "Iter-4700 train loss: 2.3043 valid loss: 2.3007, valid accuracy: 0.1192\n",
      "Iter-4800 train loss: 2.2988 valid loss: 2.3004, valid accuracy: 0.1196\n",
      "Iter-4900 train loss: 2.2967 valid loss: 2.3000, valid accuracy: 0.1200\n",
      "Iter-5000 train loss: 2.2996 valid loss: 2.2997, valid accuracy: 0.1198\n",
      "Iter-5100 train loss: 2.3020 valid loss: 2.2994, valid accuracy: 0.1202\n",
      "Iter-5200 train loss: 2.3035 valid loss: 2.2990, valid accuracy: 0.1206\n",
      "Iter-5300 train loss: 2.3026 valid loss: 2.2987, valid accuracy: 0.1212\n",
      "Iter-5400 train loss: 2.2955 valid loss: 2.2984, valid accuracy: 0.1214\n",
      "Iter-5500 train loss: 2.3096 valid loss: 2.2980, valid accuracy: 0.1220\n",
      "Iter-5600 train loss: 2.2950 valid loss: 2.2977, valid accuracy: 0.1224\n",
      "Iter-5700 train loss: 2.2952 valid loss: 2.2974, valid accuracy: 0.1226\n",
      "Iter-5800 train loss: 2.3034 valid loss: 2.2970, valid accuracy: 0.1226\n",
      "Iter-5900 train loss: 2.2959 valid loss: 2.2967, valid accuracy: 0.1228\n",
      "Iter-6000 train loss: 2.3081 valid loss: 2.2964, valid accuracy: 0.1232\n",
      "Iter-6100 train loss: 2.3006 valid loss: 2.2961, valid accuracy: 0.1232\n",
      "Iter-6200 train loss: 2.2966 valid loss: 2.2957, valid accuracy: 0.1232\n",
      "Iter-6300 train loss: 2.2924 valid loss: 2.2954, valid accuracy: 0.1234\n",
      "Iter-6400 train loss: 2.3016 valid loss: 2.2951, valid accuracy: 0.1242\n",
      "Iter-6500 train loss: 2.3040 valid loss: 2.2947, valid accuracy: 0.1242\n",
      "Iter-6600 train loss: 2.2873 valid loss: 2.2944, valid accuracy: 0.1244\n",
      "Iter-6700 train loss: 2.2992 valid loss: 2.2941, valid accuracy: 0.1254\n",
      "Iter-6800 train loss: 2.2987 valid loss: 2.2937, valid accuracy: 0.1252\n",
      "Iter-6900 train loss: 2.3093 valid loss: 2.2934, valid accuracy: 0.1258\n",
      "Iter-7000 train loss: 2.3068 valid loss: 2.2931, valid accuracy: 0.1258\n",
      "Iter-7100 train loss: 2.3082 valid loss: 2.2928, valid accuracy: 0.1256\n",
      "Iter-7200 train loss: 2.3030 valid loss: 2.2924, valid accuracy: 0.1260\n",
      "Iter-7300 train loss: 2.3031 valid loss: 2.2921, valid accuracy: 0.1264\n",
      "Iter-7400 train loss: 2.3020 valid loss: 2.2918, valid accuracy: 0.1264\n",
      "Iter-7500 train loss: 2.3124 valid loss: 2.2915, valid accuracy: 0.1266\n",
      "Iter-7600 train loss: 2.2998 valid loss: 2.2912, valid accuracy: 0.1272\n",
      "Iter-7700 train loss: 2.2980 valid loss: 2.2909, valid accuracy: 0.1278\n",
      "Iter-7800 train loss: 2.3032 valid loss: 2.2905, valid accuracy: 0.1278\n",
      "Iter-7900 train loss: 2.3072 valid loss: 2.2902, valid accuracy: 0.1282\n",
      "Iter-8000 train loss: 2.3055 valid loss: 2.2898, valid accuracy: 0.1286\n",
      "Iter-8100 train loss: 2.3075 valid loss: 2.2895, valid accuracy: 0.1292\n",
      "Iter-8200 train loss: 2.3036 valid loss: 2.2892, valid accuracy: 0.1292\n",
      "Iter-8300 train loss: 2.3050 valid loss: 2.2889, valid accuracy: 0.1294\n",
      "Iter-8400 train loss: 2.2980 valid loss: 2.2886, valid accuracy: 0.1298\n",
      "Iter-8500 train loss: 2.3001 valid loss: 2.2882, valid accuracy: 0.1304\n",
      "Iter-8600 train loss: 2.3080 valid loss: 2.2879, valid accuracy: 0.1310\n",
      "Iter-8700 train loss: 2.3087 valid loss: 2.2876, valid accuracy: 0.1310\n",
      "Iter-8800 train loss: 2.3056 valid loss: 2.2872, valid accuracy: 0.1312\n",
      "Iter-8900 train loss: 2.3084 valid loss: 2.2869, valid accuracy: 0.1316\n",
      "Iter-9000 train loss: 2.3050 valid loss: 2.2865, valid accuracy: 0.1318\n",
      "Iter-9100 train loss: 2.3123 valid loss: 2.2863, valid accuracy: 0.1318\n",
      "Iter-9200 train loss: 2.3084 valid loss: 2.2859, valid accuracy: 0.1328\n",
      "Iter-9300 train loss: 2.2844 valid loss: 2.2856, valid accuracy: 0.1332\n",
      "Iter-9400 train loss: 2.3135 valid loss: 2.2853, valid accuracy: 0.1336\n",
      "Iter-9500 train loss: 2.3014 valid loss: 2.2850, valid accuracy: 0.1340\n",
      "Iter-9600 train loss: 2.2962 valid loss: 2.2847, valid accuracy: 0.1344\n",
      "Iter-9700 train loss: 2.3053 valid loss: 2.2844, valid accuracy: 0.1350\n",
      "Iter-9800 train loss: 2.3008 valid loss: 2.2841, valid accuracy: 0.1350\n",
      "Iter-9900 train loss: 2.3060 valid loss: 2.2837, valid accuracy: 0.1350\n",
      "Iter-10000 train loss: 2.3106 valid loss: 2.2834, valid accuracy: 0.1348\n",
      "Iter-10100 train loss: 2.2952 valid loss: 2.2831, valid accuracy: 0.1352\n",
      "Iter-10200 train loss: 2.3104 valid loss: 2.2827, valid accuracy: 0.1362\n",
      "Iter-10300 train loss: 2.3005 valid loss: 2.2824, valid accuracy: 0.1366\n",
      "Iter-10400 train loss: 2.3044 valid loss: 2.2821, valid accuracy: 0.1366\n",
      "Iter-10500 train loss: 2.3089 valid loss: 2.2817, valid accuracy: 0.1368\n",
      "Iter-10600 train loss: 2.2968 valid loss: 2.2814, valid accuracy: 0.1370\n",
      "Iter-10700 train loss: 2.2951 valid loss: 2.2811, valid accuracy: 0.1376\n",
      "Iter-10800 train loss: 2.2912 valid loss: 2.2808, valid accuracy: 0.1382\n",
      "Iter-10900 train loss: 2.3012 valid loss: 2.2805, valid accuracy: 0.1388\n",
      "Iter-11000 train loss: 2.3024 valid loss: 2.2802, valid accuracy: 0.1390\n",
      "Iter-11100 train loss: 2.2925 valid loss: 2.2799, valid accuracy: 0.1392\n",
      "Iter-11200 train loss: 2.2973 valid loss: 2.2795, valid accuracy: 0.1390\n",
      "Iter-11300 train loss: 2.2776 valid loss: 2.2792, valid accuracy: 0.1390\n",
      "Iter-11400 train loss: 2.2984 valid loss: 2.2789, valid accuracy: 0.1394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.3015 valid loss: 2.2786, valid accuracy: 0.1404\n",
      "Iter-11600 train loss: 2.2951 valid loss: 2.2783, valid accuracy: 0.1406\n",
      "Iter-11700 train loss: 2.3049 valid loss: 2.2780, valid accuracy: 0.1416\n",
      "Iter-11800 train loss: 2.2989 valid loss: 2.2777, valid accuracy: 0.1426\n",
      "Iter-11900 train loss: 2.3081 valid loss: 2.2774, valid accuracy: 0.1428\n",
      "Iter-12000 train loss: 2.3098 valid loss: 2.2770, valid accuracy: 0.1438\n",
      "Iter-12100 train loss: 2.3014 valid loss: 2.2767, valid accuracy: 0.1444\n",
      "Iter-12200 train loss: 2.3171 valid loss: 2.2764, valid accuracy: 0.1448\n",
      "Iter-12300 train loss: 2.3075 valid loss: 2.2760, valid accuracy: 0.1454\n",
      "Iter-12400 train loss: 2.3021 valid loss: 2.2756, valid accuracy: 0.1460\n",
      "Iter-12500 train loss: 2.3024 valid loss: 2.2753, valid accuracy: 0.1460\n",
      "Iter-12600 train loss: 2.3110 valid loss: 2.2750, valid accuracy: 0.1462\n",
      "Iter-12700 train loss: 2.2906 valid loss: 2.2747, valid accuracy: 0.1462\n",
      "Iter-12800 train loss: 2.2969 valid loss: 2.2743, valid accuracy: 0.1468\n",
      "Iter-12900 train loss: 2.3025 valid loss: 2.2740, valid accuracy: 0.1470\n",
      "Iter-13000 train loss: 2.3118 valid loss: 2.2737, valid accuracy: 0.1472\n",
      "Iter-13100 train loss: 2.3068 valid loss: 2.2734, valid accuracy: 0.1480\n",
      "Iter-13200 train loss: 2.3072 valid loss: 2.2730, valid accuracy: 0.1486\n",
      "Iter-13300 train loss: 2.3037 valid loss: 2.2727, valid accuracy: 0.1484\n",
      "Iter-13400 train loss: 2.3036 valid loss: 2.2724, valid accuracy: 0.1484\n",
      "Iter-13500 train loss: 2.3046 valid loss: 2.2720, valid accuracy: 0.1484\n",
      "Iter-13600 train loss: 2.3024 valid loss: 2.2717, valid accuracy: 0.1492\n",
      "Iter-13700 train loss: 2.3001 valid loss: 2.2714, valid accuracy: 0.1492\n",
      "Iter-13800 train loss: 2.3001 valid loss: 2.2711, valid accuracy: 0.1496\n",
      "Iter-13900 train loss: 2.3028 valid loss: 2.2708, valid accuracy: 0.1494\n",
      "Iter-14000 train loss: 2.3015 valid loss: 2.2705, valid accuracy: 0.1502\n",
      "Iter-14100 train loss: 2.2814 valid loss: 2.2702, valid accuracy: 0.1500\n",
      "Iter-14200 train loss: 2.2945 valid loss: 2.2700, valid accuracy: 0.1502\n",
      "Iter-14300 train loss: 2.2991 valid loss: 2.2696, valid accuracy: 0.1502\n",
      "Iter-14400 train loss: 2.2977 valid loss: 2.2693, valid accuracy: 0.1502\n",
      "Iter-14500 train loss: 2.3056 valid loss: 2.2690, valid accuracy: 0.1508\n",
      "Iter-14600 train loss: 2.2956 valid loss: 2.2687, valid accuracy: 0.1514\n",
      "Iter-14700 train loss: 2.3056 valid loss: 2.2683, valid accuracy: 0.1522\n",
      "Iter-14800 train loss: 2.3013 valid loss: 2.2680, valid accuracy: 0.1524\n",
      "Iter-14900 train loss: 2.3089 valid loss: 2.2677, valid accuracy: 0.1528\n",
      "Iter-15000 train loss: 2.3005 valid loss: 2.2674, valid accuracy: 0.1532\n",
      "Iter-15100 train loss: 2.2928 valid loss: 2.2671, valid accuracy: 0.1536\n",
      "Iter-15200 train loss: 2.3049 valid loss: 2.2668, valid accuracy: 0.1542\n",
      "Iter-15300 train loss: 2.2992 valid loss: 2.2664, valid accuracy: 0.1550\n",
      "Iter-15400 train loss: 2.3023 valid loss: 2.2662, valid accuracy: 0.1552\n",
      "Iter-15500 train loss: 2.3248 valid loss: 2.2659, valid accuracy: 0.1554\n",
      "Iter-15600 train loss: 2.2926 valid loss: 2.2655, valid accuracy: 0.1560\n",
      "Iter-15700 train loss: 2.2984 valid loss: 2.2653, valid accuracy: 0.1564\n",
      "Iter-15800 train loss: 2.3046 valid loss: 2.2650, valid accuracy: 0.1562\n",
      "Iter-15900 train loss: 2.2970 valid loss: 2.2646, valid accuracy: 0.1568\n",
      "Iter-16000 train loss: 2.2985 valid loss: 2.2643, valid accuracy: 0.1572\n",
      "Iter-16100 train loss: 2.3093 valid loss: 2.2641, valid accuracy: 0.1572\n",
      "Iter-16200 train loss: 2.2966 valid loss: 2.2638, valid accuracy: 0.1576\n",
      "Iter-16300 train loss: 2.3134 valid loss: 2.2635, valid accuracy: 0.1576\n",
      "Iter-16400 train loss: 2.2992 valid loss: 2.2631, valid accuracy: 0.1582\n",
      "Iter-16500 train loss: 2.3004 valid loss: 2.2628, valid accuracy: 0.1584\n",
      "Iter-16600 train loss: 2.3082 valid loss: 2.2625, valid accuracy: 0.1592\n",
      "Iter-16700 train loss: 2.2995 valid loss: 2.2622, valid accuracy: 0.1594\n",
      "Iter-16800 train loss: 2.3033 valid loss: 2.2618, valid accuracy: 0.1596\n",
      "Iter-16900 train loss: 2.3002 valid loss: 2.2615, valid accuracy: 0.1602\n",
      "Iter-17000 train loss: 2.3124 valid loss: 2.2612, valid accuracy: 0.1610\n",
      "Iter-17100 train loss: 2.2989 valid loss: 2.2609, valid accuracy: 0.1614\n",
      "Iter-17200 train loss: 2.3038 valid loss: 2.2605, valid accuracy: 0.1618\n",
      "Iter-17300 train loss: 2.2961 valid loss: 2.2602, valid accuracy: 0.1620\n",
      "Iter-17400 train loss: 2.3041 valid loss: 2.2600, valid accuracy: 0.1622\n",
      "Iter-17500 train loss: 2.2927 valid loss: 2.2596, valid accuracy: 0.1624\n",
      "Iter-17600 train loss: 2.2900 valid loss: 2.2593, valid accuracy: 0.1630\n",
      "Iter-17700 train loss: 2.3031 valid loss: 2.2590, valid accuracy: 0.1636\n",
      "Iter-17800 train loss: 2.2923 valid loss: 2.2588, valid accuracy: 0.1640\n",
      "Iter-17900 train loss: 2.2965 valid loss: 2.2585, valid accuracy: 0.1646\n",
      "Iter-18000 train loss: 2.2833 valid loss: 2.2582, valid accuracy: 0.1648\n",
      "Iter-18100 train loss: 2.3019 valid loss: 2.2579, valid accuracy: 0.1650\n",
      "Iter-18200 train loss: 2.3006 valid loss: 2.2576, valid accuracy: 0.1652\n",
      "Iter-18300 train loss: 2.3060 valid loss: 2.2572, valid accuracy: 0.1656\n",
      "Iter-18400 train loss: 2.3114 valid loss: 2.2570, valid accuracy: 0.1658\n",
      "Iter-18500 train loss: 2.2898 valid loss: 2.2566, valid accuracy: 0.1660\n",
      "Iter-18600 train loss: 2.2979 valid loss: 2.2563, valid accuracy: 0.1664\n",
      "Iter-18700 train loss: 2.3104 valid loss: 2.2560, valid accuracy: 0.1668\n",
      "Iter-18800 train loss: 2.2974 valid loss: 2.2557, valid accuracy: 0.1672\n",
      "Iter-18900 train loss: 2.3015 valid loss: 2.2554, valid accuracy: 0.1672\n",
      "Iter-19000 train loss: 2.3070 valid loss: 2.2550, valid accuracy: 0.1672\n",
      "Iter-19100 train loss: 2.2988 valid loss: 2.2547, valid accuracy: 0.1672\n",
      "Iter-19200 train loss: 2.2992 valid loss: 2.2544, valid accuracy: 0.1672\n",
      "Iter-19300 train loss: 2.3037 valid loss: 2.2541, valid accuracy: 0.1676\n",
      "Iter-19400 train loss: 2.2996 valid loss: 2.2537, valid accuracy: 0.1680\n",
      "Iter-19500 train loss: 2.3107 valid loss: 2.2534, valid accuracy: 0.1680\n",
      "Iter-19600 train loss: 2.3042 valid loss: 2.2531, valid accuracy: 0.1684\n",
      "Iter-19700 train loss: 2.2903 valid loss: 2.2528, valid accuracy: 0.1688\n",
      "Iter-19800 train loss: 2.2879 valid loss: 2.2525, valid accuracy: 0.1694\n",
      "Iter-19900 train loss: 2.3063 valid loss: 2.2521, valid accuracy: 0.1704\n",
      "Iter-20000 train loss: 2.2893 valid loss: 2.2518, valid accuracy: 0.1708\n",
      "Iter-20100 train loss: 2.2977 valid loss: 2.2516, valid accuracy: 0.1708\n",
      "Iter-20200 train loss: 2.2999 valid loss: 2.2512, valid accuracy: 0.1712\n",
      "Iter-20300 train loss: 2.3023 valid loss: 2.2509, valid accuracy: 0.1718\n",
      "Iter-20400 train loss: 2.3094 valid loss: 2.2506, valid accuracy: 0.1722\n",
      "Iter-20500 train loss: 2.3045 valid loss: 2.2503, valid accuracy: 0.1732\n",
      "Iter-20600 train loss: 2.2956 valid loss: 2.2499, valid accuracy: 0.1740\n",
      "Iter-20700 train loss: 2.3001 valid loss: 2.2497, valid accuracy: 0.1740\n",
      "Iter-20800 train loss: 2.2852 valid loss: 2.2493, valid accuracy: 0.1746\n",
      "Iter-20900 train loss: 2.3003 valid loss: 2.2490, valid accuracy: 0.1752\n",
      "Iter-21000 train loss: 2.2969 valid loss: 2.2487, valid accuracy: 0.1752\n",
      "Iter-21100 train loss: 2.3004 valid loss: 2.2484, valid accuracy: 0.1760\n",
      "Iter-21200 train loss: 2.3078 valid loss: 2.2481, valid accuracy: 0.1762\n",
      "Iter-21300 train loss: 2.2881 valid loss: 2.2478, valid accuracy: 0.1768\n",
      "Iter-21400 train loss: 2.3060 valid loss: 2.2475, valid accuracy: 0.1768\n",
      "Iter-21500 train loss: 2.3009 valid loss: 2.2472, valid accuracy: 0.1770\n",
      "Iter-21600 train loss: 2.2896 valid loss: 2.2468, valid accuracy: 0.1776\n",
      "Iter-21700 train loss: 2.2912 valid loss: 2.2466, valid accuracy: 0.1776\n",
      "Iter-21800 train loss: 2.3048 valid loss: 2.2463, valid accuracy: 0.1776\n",
      "Iter-21900 train loss: 2.3142 valid loss: 2.2460, valid accuracy: 0.1774\n",
      "Iter-22000 train loss: 2.2894 valid loss: 2.2456, valid accuracy: 0.1780\n",
      "Iter-22100 train loss: 2.2975 valid loss: 2.2453, valid accuracy: 0.1782\n",
      "Iter-22200 train loss: 2.3023 valid loss: 2.2451, valid accuracy: 0.1786\n",
      "Iter-22300 train loss: 2.3105 valid loss: 2.2447, valid accuracy: 0.1792\n",
      "Iter-22400 train loss: 2.3004 valid loss: 2.2444, valid accuracy: 0.1800\n",
      "Iter-22500 train loss: 2.2972 valid loss: 2.2441, valid accuracy: 0.1798\n",
      "Iter-22600 train loss: 2.2989 valid loss: 2.2437, valid accuracy: 0.1802\n",
      "Iter-22700 train loss: 2.3030 valid loss: 2.2434, valid accuracy: 0.1810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.3076 valid loss: 2.2431, valid accuracy: 0.1814\n",
      "Iter-22900 train loss: 2.3056 valid loss: 2.2428, valid accuracy: 0.1818\n",
      "Iter-23000 train loss: 2.3059 valid loss: 2.2425, valid accuracy: 0.1828\n",
      "Iter-23100 train loss: 2.3009 valid loss: 2.2422, valid accuracy: 0.1828\n",
      "Iter-23200 train loss: 2.2896 valid loss: 2.2419, valid accuracy: 0.1830\n",
      "Iter-23300 train loss: 2.3064 valid loss: 2.2415, valid accuracy: 0.1828\n",
      "Iter-23400 train loss: 2.3049 valid loss: 2.2413, valid accuracy: 0.1832\n",
      "Iter-23500 train loss: 2.2966 valid loss: 2.2409, valid accuracy: 0.1830\n",
      "Iter-23600 train loss: 2.3010 valid loss: 2.2406, valid accuracy: 0.1832\n",
      "Iter-23700 train loss: 2.2932 valid loss: 2.2403, valid accuracy: 0.1836\n",
      "Iter-23800 train loss: 2.3096 valid loss: 2.2400, valid accuracy: 0.1840\n",
      "Iter-23900 train loss: 2.3011 valid loss: 2.2397, valid accuracy: 0.1844\n",
      "Iter-24000 train loss: 2.2952 valid loss: 2.2394, valid accuracy: 0.1846\n",
      "Iter-24100 train loss: 2.3040 valid loss: 2.2391, valid accuracy: 0.1848\n",
      "Iter-24200 train loss: 2.2968 valid loss: 2.2388, valid accuracy: 0.1854\n",
      "Iter-24300 train loss: 2.2977 valid loss: 2.2385, valid accuracy: 0.1860\n",
      "Iter-24400 train loss: 2.2898 valid loss: 2.2383, valid accuracy: 0.1868\n",
      "Iter-24500 train loss: 2.3020 valid loss: 2.2379, valid accuracy: 0.1878\n",
      "Iter-24600 train loss: 2.2949 valid loss: 2.2376, valid accuracy: 0.1880\n",
      "Iter-24700 train loss: 2.3109 valid loss: 2.2373, valid accuracy: 0.1886\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
