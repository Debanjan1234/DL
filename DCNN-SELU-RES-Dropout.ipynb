{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "            W1_res=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1_res=np.zeros((H, 1))\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        self.bn_caches = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "                    W2_res=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2_res=np.zeros((H, 1))\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "\n",
    "        # 1st layer: Input to Conv\n",
    "        h1_res, conv1_res_cache = l.conv_forward(X=X, W=self.model[0]['W1_res'], b=self.model[0]['b1_res']) \n",
    "        h1, conv1_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1']) \n",
    "        #         h2, nl1_cache = l.relu_forward(h1)\n",
    "        h2, nl1_cache = self.selu_forward(h1)\n",
    "        if train: \n",
    "            do1_cache = None\n",
    "            #         if train: h2, do1_cache = self.dropout_selu_forward(X=h2, p_dropout=self.p_dropout)\n",
    "            h2, do1_cache = self.alpha_dropout_fwd(h=h2, q=self.p_dropout)\n",
    "            # x = x + f(x)\n",
    "            h1_res += h2\n",
    "            h1_cache = (conv1_res_cache, conv1_cache, nl1_cache, do1_cache)\n",
    "        else: \n",
    "            # x = x + f(x)\n",
    "            h1_res += h2\n",
    "            h1_cache = (conv1_res_cache, conv1_cache, nl1_cache)\n",
    "\n",
    "        ###########################################################################################\n",
    "        # midst layer: Convnet 1\n",
    "        h2_cache = []\n",
    "        for layer in range(self.L):\n",
    "            if not layer == 0: h2 = h2.reshape(nl1_cache.shape)\n",
    "\n",
    "            h2_res, conv2_res_cache = l.conv_forward(X=h2, W=self.model[1][layer]['W2_res'], \n",
    "                                                     b=self.model[1][layer]['b2_res'])\n",
    "            h2_res = h2_res.reshape([nl1_cache.shape[0], -1])\n",
    "\n",
    "            h2, conv2_cache = l.conv_forward(X=h2, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "            \n",
    "            #             h2, nl2_cache = l.relu_forward(h2)\n",
    "            h2, nl2_cache = self.selu_forward(X=h2)\n",
    "            if train: \n",
    "                do2_cache = None # ERROR: referenced before assigned!\n",
    "                #             if train: h2, do2_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "                #             if train: h2, do2_cache = self.dropout_selu_forward(X=h2, p_dropout=self.p_dropout)\n",
    "                h2, do2_cache = self.alpha_dropout_fwd(h=h2, q=self.p_dropout)\n",
    "                h2_res += h2    \n",
    "                cache = (conv2_res_cache, conv2_cache, nl2_cache, do2_cache)\n",
    "            else:\n",
    "                h2_res += h2    \n",
    "                cache = (conv2_res_cache, conv2_cache, nl2_cache)\n",
    "\n",
    "            h2_cache.append(cache)\n",
    "        ############################################################################################\n",
    "            \n",
    "        # last layer : FC to Output\n",
    "        h3, h3_cache = l.fc_forward(X=h2, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "\n",
    "        cache = (h1_cache, h2_cache, h3_cache)\n",
    "        return h3, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train) #/ y_train.shape[0] # (t, n)=txn, t: minibatch_size/ num_samples\n",
    "        dy = self.dcross_entropy(y, y_train) #/ y_train.shape[0]\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        h1_cache, h2_cache, h3_cache = cache\n",
    "        if train: conv1_res_cache, conv1_cache, nl1_cache, do1_cache = h1_cache\n",
    "        else: conv1_res_cache, conv1_cache, nl1_cache = h1_cache\n",
    "\n",
    "        # last layer\n",
    "        dh2, dw3, db3 = l.fc_backward(dout=dy, cache=h3_cache)\n",
    "        \n",
    "        # midst layer 2\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            \n",
    "            conv2_res_cache, conv2_cache, nl2_cache, do2_cache = h2_cache[layer]\n",
    "            dh2_res = dh2.reshape(nl1_cache.shape)\n",
    "            dh2_res, dw2_res, db2_res = l.conv_backward(dout=dh2_res, cache=conv2_res_cache)\n",
    "            \n",
    "            if train:\n",
    "                #             dh2 = l.dropout_backward(dout=dh2, cache=do2_cache)\n",
    "                #             dh2 = self.dropout_selu_backward(dout=dh2, cache=do2_cache)\n",
    "                dh2 = self.alpha_dropout_bwd(dout=dh2, cache=do2_cache)\n",
    "            #             dh2 = l.relu_backward(dout=dh2, cache=nl2_cache)\n",
    "            dh2 = self.selu_backward(dout=dh2, cache=nl2_cache)\n",
    "            dh2 = dh2.reshape(nl1_cache.shape)\n",
    "            dh2, dw2, db2 = l.conv_backward(dout=dh2, cache=conv2_cache)\n",
    "            dh2 += dh2_res\n",
    "            if not layer==0: dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2,\n",
    "                    W2_res=dw2_res,\n",
    "                    b2_res=db2_res\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer\n",
    "        #         conv1_res_cache, conv1_cache, nl1_cache, do1_cache = h1_cache\n",
    "        dX_res, dw1_res, db1_res = l.conv_backward(dout=dh2, cache=conv1_res_cache)\n",
    "        #         dh2 = self.dropout_selu_backward(dout=dh2, cache=do1_cache)\n",
    "        if train: dh2 = self.alpha_dropout_bwd(dout=dh2, cache=do1_cache)\n",
    "        #         dh1 = l.relu_backward(dout=dh2, cache=nl1_cache)\n",
    "        dh1 = self.selu_backward(dout=dh2, cache=nl1_cache)\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh1, cache=conv1_cache)\n",
    "        dX += dX_res\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1,\n",
    "            W1_res=dw1_res, \n",
    "            b1_res=db1_res\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam(nn, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "    if val_set:\n",
    "        X_val, y_val = val_set\n",
    "        \n",
    "    M, R = [], []\n",
    "    M.append({key: np.zeros_like(val) for key, val in nn.model[0].items()})\n",
    "    R.append({key: np.zeros_like(val) for key, val in nn.model[0].items()})\n",
    "    \n",
    "    M_, R_ = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M_.append({key: np.zeros_like(val) for key, val in nn.model[1][layer].items()})\n",
    "        R_.append({key: np.zeros_like(val) for key, val in nn.model[1][layer].items()})\n",
    "    M.append(M_)\n",
    "    R.append(R_)\n",
    "\n",
    "    M.append({key: np.zeros_like(val) for key, val in nn.model[2].items()})\n",
    "    R.append({key: np.zeros_like(val) for key, val in nn.model[2].items()})\n",
    "    \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        \n",
    "        # Minibatches\n",
    "        #         \"\"\"\n",
    "        #         Single training step over minibatch: forward, loss, backprop\n",
    "        #         \"\"\"\n",
    "        # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        #         for idx in range(len(minibatches)):\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "        y, cache = nn.forward(X_mini, train=True)\n",
    "        loss, dy = nn.loss_function(y, y_mini)\n",
    "        dX, grad = nn.backward(dy, cache, train=True)\n",
    "        nn.losses['train'].append(loss)\n",
    "\n",
    "        for k in grad[0]:\n",
    "            M[0][k] = l.exp_running_avg(M[0][k], grad[0][k], beta1)\n",
    "            R[0][k] = l.exp_running_avg(R[0][k], grad[0][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[0][k] / (1. - (beta1**(iter)))\n",
    "            r_k_hat = R[0][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "            nn.model[0][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        for layer in range(nn.L):\n",
    "            for k in grad[1][layer]:\n",
    "                M[1][layer][k] = l.exp_running_avg(M[1][layer][k], grad[1][layer][k], beta1)\n",
    "                R[1][layer][k] = l.exp_running_avg(R[1][layer][k], grad[1][layer][k]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[1][layer][k] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[1][layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                nn.model[1][layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        for k in grad[2]:\n",
    "            M[2][k] = l.exp_running_avg(M[2][k], grad[2][k], beta1)\n",
    "            R[2][k] = l.exp_running_avg(R[2][k], grad[2][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[2][k] / (1. - (beta1**(iter)))\n",
    "            r_k_hat = R[2][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "            nn.model[2][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Epochs\n",
    "        if iter % print_after == 0:\n",
    "            if val_set:\n",
    "                val_acc = l.accuracy(y_val, nn.test(X_val))\n",
    "                print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))\n",
    "                \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 training loss: 6.7085 validation accuracy: 0.599800\n",
      "Iter-20 training loss: 4.5305 validation accuracy: 0.747200\n",
      "Iter-30 training loss: 4.7672 validation accuracy: 0.751400\n",
      "Iter-40 training loss: 2.1928 validation accuracy: 0.813400\n",
      "Iter-50 training loss: 3.9182 validation accuracy: 0.801000\n",
      "Iter-60 training loss: 3.2063 validation accuracy: 0.818200\n",
      "Iter-70 training loss: 2.8427 validation accuracy: 0.848600\n",
      "Iter-80 training loss: 2.6343 validation accuracy: 0.859400\n",
      "Iter-90 training loss: 2.1074 validation accuracy: 0.860600\n",
      "Iter-100 training loss: 2.6925 validation accuracy: 0.862200\n",
      "Test Mean accuracy: 0.8625, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 2 # depth \n",
    "print_after = n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 8\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "net = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "net = adam(nn=net, X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = net.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkgW8v8Bz6FqCUhpEv0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f87b9f74cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
