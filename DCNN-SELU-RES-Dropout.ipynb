{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "            W1_res=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1_res=np.zeros((H, 1))\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        self.bn_caches = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "                    W2_res=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2_res=np.zeros((H, 1))\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "\n",
    "        # 1st layer: Input to Conv\n",
    "        h1_res, conv1_res_cache = l.conv_forward(X=X, W=self.model[0]['W1_res'], b=self.model[0]['b1_res']) \n",
    "        h1, conv1_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1']) \n",
    "        #         h2, nl1_cache = l.relu_forward(h1)\n",
    "        h2, nl1_cache = self.selu_forward(h1)\n",
    "        if train: \n",
    "            do1_cache = None\n",
    "            #         if train: h2, do1_cache = self.dropout_selu_forward(X=h2, p_dropout=self.p_dropout)\n",
    "            h2, do1_cache = self.alpha_dropout_fwd(h=h2, q=self.p_dropout)\n",
    "            # x = x + f(x)\n",
    "            h1_res += h2\n",
    "            h1_cache = (conv1_res_cache, conv1_cache, nl1_cache, do1_cache)\n",
    "        else: \n",
    "            # x = x + f(x)\n",
    "            h1_res += h2\n",
    "            h1_cache = (conv1_res_cache, conv1_cache, nl1_cache)\n",
    "\n",
    "        ###########################################################################################\n",
    "        # midst layer: Convnet 1\n",
    "        h2_cache = []\n",
    "        for layer in range(self.L):\n",
    "            if not layer == 0: h2 = h2.reshape(nl1_cache.shape)\n",
    "\n",
    "            h2_res, conv2_res_cache = l.conv_forward(X=h2, W=self.model[1][layer]['W2_res'], \n",
    "                                                     b=self.model[1][layer]['b2_res'])\n",
    "            h2_res = h2_res.reshape([nl1_cache.shape[0], -1])\n",
    "\n",
    "            h2, conv2_cache = l.conv_forward(X=h2, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "            \n",
    "            #             h2, nl2_cache = l.relu_forward(h2)\n",
    "            h2, nl2_cache = self.selu_forward(X=h2)\n",
    "            if train: \n",
    "                do2_cache = None # ERROR: referenced before assigned!\n",
    "                #             if train: h2, do2_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "                #             if train: h2, do2_cache = self.dropout_selu_forward(X=h2, p_dropout=self.p_dropout)\n",
    "                h2, do2_cache = self.alpha_dropout_fwd(h=h2, q=self.p_dropout)\n",
    "                h2_res += h2    \n",
    "                cache = (conv2_res_cache, conv2_cache, nl2_cache, do2_cache)\n",
    "            else:\n",
    "                h2_res += h2    \n",
    "                cache = (conv2_res_cache, conv2_cache, nl2_cache)\n",
    "\n",
    "            h2_cache.append(cache)\n",
    "        ############################################################################################\n",
    "            \n",
    "        # last layer : FC to Output\n",
    "        h3, h3_cache = l.fc_forward(X=h2, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "\n",
    "        cache = (h1_cache, h2_cache, h3_cache)\n",
    "        return h3, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)/ y_train.shape[0] # (t, n)=txn, t: minibatch_size/ num_samples\n",
    "        dy = self.dcross_entropy(y, y_train) #/ y_train.shape[0]\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        h1_cache, h2_cache, h3_cache = cache\n",
    "        if train: conv1_res_cache, conv1_cache, nl1_cache, do1_cache = h1_cache\n",
    "        else: conv1_res_cache, conv1_cache, nl1_cache = h1_cache\n",
    "\n",
    "        # last layer\n",
    "        dh2, dw3, db3 = l.fc_backward(dout=dy, cache=h3_cache)\n",
    "        \n",
    "        # midst layer 2\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            \n",
    "            conv2_res_cache, conv2_cache, nl2_cache, do2_cache = h2_cache[layer]\n",
    "            dh2_res = dh2.reshape(nl1_cache.shape)\n",
    "            dh2_res, dw2_res, db2_res = l.conv_backward(dout=dh2_res, cache=conv2_res_cache)\n",
    "            \n",
    "            if train:\n",
    "                #             dh2 = l.dropout_backward(dout=dh2, cache=do2_cache)\n",
    "                #             dh2 = self.dropout_selu_backward(dout=dh2, cache=do2_cache)\n",
    "                dh2 = self.alpha_dropout_bwd(dout=dh2, cache=do2_cache)\n",
    "            #             dh2 = l.relu_backward(dout=dh2, cache=nl2_cache)\n",
    "            dh2 = self.selu_backward(dout=dh2, cache=nl2_cache)\n",
    "            dh2 = dh2.reshape(nl1_cache.shape)\n",
    "            dh2, dw2, db2 = l.conv_backward(dout=dh2, cache=conv2_cache)\n",
    "            dh2 += dh2_res\n",
    "            if not layer==0: dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2,\n",
    "                    W2_res=dw2_res,\n",
    "                    b2_res=db2_res\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer\n",
    "        #         conv1_res_cache, conv1_cache, nl1_cache, do1_cache = h1_cache\n",
    "        dX_res, dw1_res, db1_res = l.conv_backward(dout=dh2, cache=conv1_res_cache)\n",
    "        #         dh2 = self.dropout_selu_backward(dout=dh2, cache=do1_cache)\n",
    "        if train: dh2 = self.alpha_dropout_bwd(dout=dh2, cache=do1_cache)\n",
    "        #         dh1 = l.relu_backward(dout=dh2, cache=nl1_cache)\n",
    "        dh1 = self.selu_backward(dout=dh2, cache=nl1_cache)\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh1, cache=conv1_cache)\n",
    "        dX += dX_res\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1,\n",
    "            W1_res=dw1_res, \n",
    "            b1_res=db1_res\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam(nn, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "    if val_set:\n",
    "        X_val, y_val = val_set\n",
    "        \n",
    "    M, R = [], []\n",
    "    M.append({key: np.zeros_like(val) for key, val in nn.model[0].items()})\n",
    "    R.append({key: np.zeros_like(val) for key, val in nn.model[0].items()})\n",
    "    \n",
    "    M_, R_ = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M_.append({key: np.zeros_like(val) for key, val in nn.model[1][layer].items()})\n",
    "        R_.append({key: np.zeros_like(val) for key, val in nn.model[1][layer].items()})\n",
    "    M.append(M_)\n",
    "    R.append(R_)\n",
    "\n",
    "    M.append({key: np.zeros_like(val) for key, val in nn.model[2].items()})\n",
    "    R.append({key: np.zeros_like(val) for key, val in nn.model[2].items()})\n",
    "    \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        \n",
    "        # Minibatches\n",
    "        #         \"\"\"\n",
    "        #         Single training step over minibatch: forward, loss, backprop\n",
    "        #         \"\"\"\n",
    "        #         idx = np.random.randint(0, len(minibatches))\n",
    "        # Shuffle for each epochs\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = nn.forward(X_mini, train=True)\n",
    "            loss, dy = nn.loss_function(y, y_mini)\n",
    "            dX, grad = nn.backward(dy, cache, train=True)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for k in grad[0]:\n",
    "                M[0][k] = l.exp_running_avg(M[0][k], grad[0][k], beta1)\n",
    "                R[0][k] = l.exp_running_avg(R[0][k], grad[0][k]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][k] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                nn.model[0][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grad[1][0]:\n",
    "                    M[1][layer][k] = l.exp_running_avg(M[1][layer][k], grad[1][layer][k], beta1)\n",
    "                    R[1][layer][k] = l.exp_running_avg(R[1][layer][k], grad[1][layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[1][layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for k in grad[2]:\n",
    "                M[2][k] = l.exp_running_avg(M[2][k], grad[2][k], beta1)\n",
    "                R[2][k] = l.exp_running_avg(R[2][k], grad[2][k]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][k] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                nn.model[2][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, nn.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))\n",
    "                \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 0.1526 validation accuracy: 0.078400\n",
      "Iter-1 training loss: 0.1228 validation accuracy: 0.098400\n",
      "Iter-1 training loss: 0.1379 validation accuracy: 0.126600\n",
      "Iter-1 training loss: 0.1020 validation accuracy: 0.158200\n",
      "Iter-1 training loss: 0.1175 validation accuracy: 0.207600\n",
      "Iter-1 training loss: 0.0872 validation accuracy: 0.276800\n",
      "Iter-1 training loss: 0.0817 validation accuracy: 0.351400\n",
      "Iter-1 training loss: 0.0775 validation accuracy: 0.424400\n",
      "Iter-1 training loss: 0.0602 validation accuracy: 0.490200\n",
      "Iter-1 training loss: 0.0542 validation accuracy: 0.532800\n",
      "Iter-1 training loss: 0.0494 validation accuracy: 0.569000\n",
      "Iter-1 training loss: 0.0448 validation accuracy: 0.596600\n",
      "Iter-1 training loss: 0.0414 validation accuracy: 0.617000\n",
      "Iter-1 training loss: 0.0356 validation accuracy: 0.647200\n",
      "Iter-1 training loss: 0.0360 validation accuracy: 0.673800\n",
      "Iter-1 training loss: 0.0302 validation accuracy: 0.700200\n",
      "Iter-1 training loss: 0.0331 validation accuracy: 0.713600\n",
      "Iter-1 training loss: 0.0344 validation accuracy: 0.729600\n",
      "Iter-1 training loss: 0.0269 validation accuracy: 0.741400\n",
      "Iter-1 training loss: 0.0199 validation accuracy: 0.750400\n",
      "Iter-1 training loss: 0.0196 validation accuracy: 0.756600\n",
      "Iter-1 training loss: 0.0264 validation accuracy: 0.761800\n",
      "Iter-1 training loss: 0.0182 validation accuracy: 0.763600\n",
      "Iter-1 training loss: 0.0248 validation accuracy: 0.766200\n",
      "Iter-1 training loss: 0.0207 validation accuracy: 0.770000\n",
      "Iter-1 training loss: 0.0358 validation accuracy: 0.771200\n",
      "Iter-1 training loss: 0.0246 validation accuracy: 0.777000\n",
      "Iter-1 training loss: 0.0316 validation accuracy: 0.783600\n",
      "Iter-1 training loss: 0.0288 validation accuracy: 0.788400\n",
      "Iter-1 training loss: 0.0172 validation accuracy: 0.793600\n",
      "Iter-1 training loss: 0.0325 validation accuracy: 0.798000\n",
      "Iter-1 training loss: 0.0243 validation accuracy: 0.800600\n",
      "Iter-1 training loss: 0.0257 validation accuracy: 0.804400\n",
      "Iter-1 training loss: 0.0273 validation accuracy: 0.808800\n",
      "Iter-1 training loss: 0.0265 validation accuracy: 0.810000\n",
      "Iter-1 training loss: 0.0240 validation accuracy: 0.812400\n",
      "Iter-1 training loss: 0.0441 validation accuracy: 0.810200\n",
      "Iter-1 training loss: 0.0364 validation accuracy: 0.812200\n",
      "Iter-1 training loss: 0.0240 validation accuracy: 0.810600\n",
      "Iter-1 training loss: 0.0227 validation accuracy: 0.808200\n",
      "Iter-1 training loss: 0.0349 validation accuracy: 0.808400\n",
      "Iter-1 training loss: 0.0469 validation accuracy: 0.808200\n",
      "Iter-1 training loss: 0.0304 validation accuracy: 0.810800\n",
      "Iter-1 training loss: 0.0151 validation accuracy: 0.809600\n",
      "Iter-1 training loss: 0.0254 validation accuracy: 0.808200\n",
      "Iter-1 training loss: 0.0307 validation accuracy: 0.810200\n",
      "Iter-1 training loss: 0.0358 validation accuracy: 0.815200\n",
      "Iter-1 training loss: 0.0152 validation accuracy: 0.818400\n",
      "Iter-1 training loss: 0.0154 validation accuracy: 0.816600\n",
      "Iter-1 training loss: 0.0226 validation accuracy: 0.817600\n",
      "Iter-1 training loss: 0.0244 validation accuracy: 0.818400\n",
      "Iter-1 training loss: 0.0306 validation accuracy: 0.817400\n",
      "Iter-1 training loss: 0.0192 validation accuracy: 0.815600\n",
      "Iter-1 training loss: 0.0196 validation accuracy: 0.812200\n",
      "Iter-1 training loss: 0.0147 validation accuracy: 0.810400\n",
      "Iter-1 training loss: 0.0226 validation accuracy: 0.808600\n",
      "Iter-1 training loss: 0.0311 validation accuracy: 0.808600\n",
      "Iter-1 training loss: 0.0213 validation accuracy: 0.807200\n",
      "Iter-1 training loss: 0.0193 validation accuracy: 0.805200\n",
      "Iter-1 training loss: 0.0180 validation accuracy: 0.803800\n",
      "Iter-1 training loss: 0.0183 validation accuracy: 0.800600\n",
      "Iter-1 training loss: 0.0138 validation accuracy: 0.800400\n",
      "Iter-1 training loss: 0.0340 validation accuracy: 0.800000\n",
      "Iter-1 training loss: 0.0196 validation accuracy: 0.801400\n",
      "Iter-1 training loss: 0.0212 validation accuracy: 0.803000\n",
      "Iter-1 training loss: 0.0328 validation accuracy: 0.805200\n",
      "Iter-1 training loss: 0.0422 validation accuracy: 0.807000\n",
      "Iter-1 training loss: 0.0253 validation accuracy: 0.809000\n",
      "Iter-1 training loss: 0.0256 validation accuracy: 0.812200\n",
      "Iter-1 training loss: 0.0138 validation accuracy: 0.813800\n",
      "Iter-1 training loss: 0.0165 validation accuracy: 0.816000\n",
      "Iter-1 training loss: 0.0342 validation accuracy: 0.817400\n",
      "Iter-1 training loss: 0.0274 validation accuracy: 0.819000\n",
      "Iter-1 training loss: 0.0344 validation accuracy: 0.822800\n",
      "Iter-1 training loss: 0.0077 validation accuracy: 0.825000\n",
      "Iter-1 training loss: 0.0141 validation accuracy: 0.826400\n",
      "Iter-1 training loss: 0.0339 validation accuracy: 0.827200\n",
      "Iter-1 training loss: 0.0169 validation accuracy: 0.826800\n",
      "Iter-1 training loss: 0.0253 validation accuracy: 0.826400\n",
      "Iter-1 training loss: 0.0279 validation accuracy: 0.826000\n",
      "Iter-1 training loss: 0.0227 validation accuracy: 0.826800\n",
      "Iter-1 training loss: 0.0284 validation accuracy: 0.828200\n",
      "Iter-1 training loss: 0.0196 validation accuracy: 0.827400\n",
      "Iter-1 training loss: 0.0259 validation accuracy: 0.827400\n",
      "Iter-1 training loss: 0.0335 validation accuracy: 0.826600\n",
      "Iter-1 training loss: 0.0094 validation accuracy: 0.824200\n",
      "Iter-1 training loss: 0.0164 validation accuracy: 0.824200\n",
      "Iter-1 training loss: 0.0151 validation accuracy: 0.824200\n",
      "Iter-1 training loss: 0.0279 validation accuracy: 0.821800\n",
      "Iter-1 training loss: 0.0252 validation accuracy: 0.820400\n",
      "Iter-1 training loss: 0.0164 validation accuracy: 0.821600\n",
      "Iter-1 training loss: 0.0207 validation accuracy: 0.821000\n",
      "Iter-1 training loss: 0.0328 validation accuracy: 0.820600\n",
      "Iter-1 training loss: 0.0276 validation accuracy: 0.819400\n",
      "Iter-1 training loss: 0.0387 validation accuracy: 0.819400\n",
      "Iter-1 training loss: 0.0095 validation accuracy: 0.818600\n",
      "Iter-1 training loss: 0.0132 validation accuracy: 0.816800\n",
      "Iter-1 training loss: 0.0143 validation accuracy: 0.815800\n",
      "Iter-1 training loss: 0.0162 validation accuracy: 0.813800\n",
      "Iter-1 training loss: 0.0264 validation accuracy: 0.814200\n",
      "Iter-1 training loss: 0.0264 validation accuracy: 0.813600\n",
      "Iter-1 training loss: 0.0142 validation accuracy: 0.812800\n",
      "Iter-1 training loss: 0.0231 validation accuracy: 0.811400\n",
      "Iter-1 training loss: 0.0248 validation accuracy: 0.811600\n",
      "Iter-1 training loss: 0.0189 validation accuracy: 0.811000\n",
      "Iter-1 training loss: 0.0224 validation accuracy: 0.811400\n",
      "Iter-1 training loss: 0.0167 validation accuracy: 0.811800\n",
      "Iter-1 training loss: 0.0272 validation accuracy: 0.811800\n",
      "Iter-1 training loss: 0.0204 validation accuracy: 0.813200\n",
      "Iter-1 training loss: 0.0241 validation accuracy: 0.813800\n",
      "Iter-1 training loss: 0.0385 validation accuracy: 0.813200\n",
      "Iter-1 training loss: 0.0319 validation accuracy: 0.814200\n",
      "Iter-1 training loss: 0.0348 validation accuracy: 0.816200\n",
      "Iter-1 training loss: 0.0340 validation accuracy: 0.817600\n",
      "Iter-1 training loss: 0.0295 validation accuracy: 0.819200\n",
      "Iter-1 training loss: 0.0109 validation accuracy: 0.820800\n",
      "Iter-1 training loss: 0.0201 validation accuracy: 0.824000\n",
      "Iter-1 training loss: 0.0149 validation accuracy: 0.825400\n",
      "Iter-1 training loss: 0.0245 validation accuracy: 0.829400\n",
      "Iter-1 training loss: 0.0155 validation accuracy: 0.830800\n",
      "Iter-1 training loss: 0.0166 validation accuracy: 0.831000\n",
      "Iter-1 training loss: 0.0208 validation accuracy: 0.832800\n",
      "Iter-1 training loss: 0.0343 validation accuracy: 0.835200\n",
      "Iter-1 training loss: 0.0275 validation accuracy: 0.835200\n",
      "Iter-1 training loss: 0.0110 validation accuracy: 0.836400\n",
      "Iter-1 training loss: 0.0237 validation accuracy: 0.837800\n",
      "Iter-1 training loss: 0.0184 validation accuracy: 0.837800\n",
      "Iter-1 training loss: 0.0148 validation accuracy: 0.837400\n",
      "Iter-1 training loss: 0.0174 validation accuracy: 0.836800\n",
      "Iter-1 training loss: 0.0107 validation accuracy: 0.836000\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.836600\n",
      "Iter-1 training loss: 0.0146 validation accuracy: 0.835800\n",
      "Iter-1 training loss: 0.0117 validation accuracy: 0.837200\n",
      "Iter-1 training loss: 0.0271 validation accuracy: 0.836800\n",
      "Iter-1 training loss: 0.0153 validation accuracy: 0.836800\n",
      "Iter-1 training loss: 0.0182 validation accuracy: 0.835400\n",
      "Iter-1 training loss: 0.0185 validation accuracy: 0.835000\n",
      "Iter-1 training loss: 0.0195 validation accuracy: 0.836400\n",
      "Iter-1 training loss: 0.0201 validation accuracy: 0.835400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 0.0256 validation accuracy: 0.835200\n",
      "Iter-1 training loss: 0.0257 validation accuracy: 0.835800\n",
      "Iter-1 training loss: 0.0251 validation accuracy: 0.836600\n",
      "Iter-1 training loss: 0.0228 validation accuracy: 0.836800\n",
      "Iter-1 training loss: 0.0243 validation accuracy: 0.836600\n",
      "Iter-1 training loss: 0.0086 validation accuracy: 0.838600\n",
      "Iter-1 training loss: 0.0256 validation accuracy: 0.839600\n",
      "Iter-1 training loss: 0.0167 validation accuracy: 0.841600\n",
      "Iter-1 training loss: 0.0120 validation accuracy: 0.842200\n",
      "Iter-1 training loss: 0.0160 validation accuracy: 0.843800\n",
      "Iter-1 training loss: 0.0107 validation accuracy: 0.845800\n",
      "Iter-1 training loss: 0.0189 validation accuracy: 0.847800\n",
      "Iter-1 training loss: 0.0336 validation accuracy: 0.848800\n",
      "Iter-1 training loss: 0.0216 validation accuracy: 0.849600\n",
      "Iter-1 training loss: 0.0214 validation accuracy: 0.852600\n",
      "Iter-1 training loss: 0.0232 validation accuracy: 0.856400\n",
      "Iter-1 training loss: 0.0060 validation accuracy: 0.857600\n",
      "Iter-1 training loss: 0.0156 validation accuracy: 0.858600\n",
      "Iter-1 training loss: 0.0142 validation accuracy: 0.858600\n",
      "Iter-1 training loss: 0.0116 validation accuracy: 0.859200\n",
      "Iter-1 training loss: 0.0141 validation accuracy: 0.861000\n",
      "Iter-1 training loss: 0.0163 validation accuracy: 0.862800\n",
      "Iter-1 training loss: 0.0165 validation accuracy: 0.864800\n",
      "Iter-1 training loss: 0.0265 validation accuracy: 0.866800\n",
      "Iter-1 training loss: 0.0231 validation accuracy: 0.867200\n",
      "Iter-1 training loss: 0.0143 validation accuracy: 0.868200\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.868000\n",
      "Iter-1 training loss: 0.0102 validation accuracy: 0.868200\n",
      "Iter-1 training loss: 0.0089 validation accuracy: 0.868200\n",
      "Iter-1 training loss: 0.0148 validation accuracy: 0.868400\n",
      "Iter-1 training loss: 0.0223 validation accuracy: 0.868800\n",
      "Iter-1 training loss: 0.0112 validation accuracy: 0.868800\n",
      "Iter-1 training loss: 0.0100 validation accuracy: 0.870800\n",
      "Iter-1 training loss: 0.0078 validation accuracy: 0.871400\n",
      "Iter-1 training loss: 0.0230 validation accuracy: 0.871400\n",
      "Iter-1 training loss: 0.0150 validation accuracy: 0.872600\n",
      "Iter-1 training loss: 0.0274 validation accuracy: 0.873600\n",
      "Iter-1 training loss: 0.0221 validation accuracy: 0.874000\n",
      "Iter-1 training loss: 0.0110 validation accuracy: 0.873600\n",
      "Iter-1 training loss: 0.0126 validation accuracy: 0.873800\n",
      "Iter-1 training loss: 0.0117 validation accuracy: 0.874400\n",
      "Iter-1 training loss: 0.0199 validation accuracy: 0.874400\n",
      "Iter-1 training loss: 0.0153 validation accuracy: 0.874800\n",
      "Iter-1 training loss: 0.0299 validation accuracy: 0.875200\n",
      "Iter-1 training loss: 0.0139 validation accuracy: 0.875400\n",
      "Iter-1 training loss: 0.0130 validation accuracy: 0.876200\n",
      "Iter-1 training loss: 0.0169 validation accuracy: 0.877200\n",
      "Iter-1 training loss: 0.0135 validation accuracy: 0.877200\n",
      "Iter-1 training loss: 0.0171 validation accuracy: 0.877800\n",
      "Iter-1 training loss: 0.0222 validation accuracy: 0.878400\n",
      "Iter-1 training loss: 0.0126 validation accuracy: 0.879400\n",
      "Iter-1 training loss: 0.0109 validation accuracy: 0.879200\n",
      "Iter-1 training loss: 0.0279 validation accuracy: 0.880800\n",
      "Iter-1 training loss: 0.0140 validation accuracy: 0.881000\n",
      "Iter-1 training loss: 0.0088 validation accuracy: 0.881600\n",
      "Iter-1 training loss: 0.0102 validation accuracy: 0.882000\n",
      "Iter-1 training loss: 0.0086 validation accuracy: 0.882000\n",
      "Iter-1 training loss: 0.0167 validation accuracy: 0.883400\n",
      "Iter-1 training loss: 0.0160 validation accuracy: 0.884400\n",
      "Iter-1 training loss: 0.0142 validation accuracy: 0.883800\n",
      "Iter-1 training loss: 0.0145 validation accuracy: 0.885600\n",
      "Iter-1 training loss: 0.0116 validation accuracy: 0.885800\n",
      "Iter-1 training loss: 0.0101 validation accuracy: 0.885600\n",
      "Iter-1 training loss: 0.0219 validation accuracy: 0.885200\n",
      "Iter-1 training loss: 0.0162 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0199 validation accuracy: 0.887400\n",
      "Iter-1 training loss: 0.0138 validation accuracy: 0.888400\n",
      "Iter-1 training loss: 0.0259 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0200 validation accuracy: 0.890800\n",
      "Iter-1 training loss: 0.0141 validation accuracy: 0.891400\n",
      "Iter-1 training loss: 0.0127 validation accuracy: 0.892400\n",
      "Iter-1 training loss: 0.0135 validation accuracy: 0.892400\n",
      "Iter-1 training loss: 0.0118 validation accuracy: 0.891800\n",
      "Iter-1 training loss: 0.0088 validation accuracy: 0.892600\n",
      "Iter-1 training loss: 0.0149 validation accuracy: 0.894200\n",
      "Iter-1 training loss: 0.0275 validation accuracy: 0.893400\n",
      "Iter-1 training loss: 0.0078 validation accuracy: 0.893600\n",
      "Iter-1 training loss: 0.0252 validation accuracy: 0.892800\n",
      "Iter-1 training loss: 0.0151 validation accuracy: 0.892400\n",
      "Iter-1 training loss: 0.0160 validation accuracy: 0.891800\n",
      "Iter-1 training loss: 0.0121 validation accuracy: 0.891800\n",
      "Iter-1 training loss: 0.0153 validation accuracy: 0.892200\n",
      "Iter-1 training loss: 0.0090 validation accuracy: 0.892200\n",
      "Iter-1 training loss: 0.0147 validation accuracy: 0.891400\n",
      "Iter-1 training loss: 0.0164 validation accuracy: 0.891000\n",
      "Iter-1 training loss: 0.0096 validation accuracy: 0.890200\n",
      "Iter-1 training loss: 0.0057 validation accuracy: 0.890600\n",
      "Iter-1 training loss: 0.0232 validation accuracy: 0.890200\n",
      "Iter-1 training loss: 0.0083 validation accuracy: 0.891000\n",
      "Iter-1 training loss: 0.0233 validation accuracy: 0.891200\n",
      "Iter-1 training loss: 0.0081 validation accuracy: 0.891000\n",
      "Iter-1 training loss: 0.0099 validation accuracy: 0.890800\n",
      "Iter-1 training loss: 0.0222 validation accuracy: 0.890200\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0103 validation accuracy: 0.890000\n",
      "Iter-1 training loss: 0.0100 validation accuracy: 0.889600\n",
      "Iter-1 training loss: 0.0264 validation accuracy: 0.889800\n",
      "Iter-1 training loss: 0.0051 validation accuracy: 0.889800\n",
      "Iter-1 training loss: 0.0277 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0135 validation accuracy: 0.889600\n",
      "Iter-1 training loss: 0.0073 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0260 validation accuracy: 0.888600\n",
      "Iter-1 training loss: 0.0081 validation accuracy: 0.888200\n",
      "Iter-1 training loss: 0.0121 validation accuracy: 0.888600\n",
      "Iter-1 training loss: 0.0099 validation accuracy: 0.888800\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.889200\n",
      "Iter-1 training loss: 0.0202 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0177 validation accuracy: 0.890600\n",
      "Iter-1 training loss: 0.0108 validation accuracy: 0.890400\n",
      "Iter-1 training loss: 0.0234 validation accuracy: 0.890200\n",
      "Iter-1 training loss: 0.0086 validation accuracy: 0.890400\n",
      "Iter-1 training loss: 0.0049 validation accuracy: 0.890400\n",
      "Iter-1 training loss: 0.0095 validation accuracy: 0.889600\n",
      "Iter-1 training loss: 0.0079 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0190 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0193 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.888800\n",
      "Iter-1 training loss: 0.0224 validation accuracy: 0.888400\n",
      "Iter-1 training loss: 0.0064 validation accuracy: 0.888800\n",
      "Iter-1 training loss: 0.0187 validation accuracy: 0.889200\n",
      "Iter-1 training loss: 0.0272 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0186 validation accuracy: 0.889200\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.889800\n",
      "Iter-1 training loss: 0.0125 validation accuracy: 0.889800\n",
      "Iter-1 training loss: 0.0062 validation accuracy: 0.889600\n",
      "Iter-1 training loss: 0.0156 validation accuracy: 0.890600\n",
      "Iter-1 training loss: 0.0212 validation accuracy: 0.890800\n",
      "Iter-1 training loss: 0.0187 validation accuracy: 0.890600\n",
      "Iter-1 training loss: 0.0197 validation accuracy: 0.889200\n",
      "Iter-1 training loss: 0.0027 validation accuracy: 0.889000\n",
      "Iter-1 training loss: 0.0028 validation accuracy: 0.888400\n",
      "Iter-1 training loss: 0.0221 validation accuracy: 0.888200\n",
      "Iter-1 training loss: 0.0157 validation accuracy: 0.888000\n",
      "Iter-1 training loss: 0.0176 validation accuracy: 0.888000\n",
      "Iter-1 training loss: 0.0176 validation accuracy: 0.887600\n",
      "Iter-1 training loss: 0.0170 validation accuracy: 0.888200\n",
      "Iter-1 training loss: 0.0163 validation accuracy: 0.887400\n",
      "Iter-1 training loss: 0.0086 validation accuracy: 0.887400\n",
      "Iter-1 training loss: 0.0142 validation accuracy: 0.887200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 0.0150 validation accuracy: 0.887200\n",
      "Iter-1 training loss: 0.0117 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0111 validation accuracy: 0.885800\n",
      "Iter-1 training loss: 0.0056 validation accuracy: 0.885200\n",
      "Iter-1 training loss: 0.0128 validation accuracy: 0.885000\n",
      "Iter-1 training loss: 0.0141 validation accuracy: 0.885600\n",
      "Iter-1 training loss: 0.0143 validation accuracy: 0.885800\n",
      "Iter-1 training loss: 0.0113 validation accuracy: 0.887000\n",
      "Iter-1 training loss: 0.0140 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0144 validation accuracy: 0.886200\n",
      "Iter-1 training loss: 0.0088 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0096 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0128 validation accuracy: 0.887000\n",
      "Iter-1 training loss: 0.0064 validation accuracy: 0.886400\n",
      "Iter-1 training loss: 0.0175 validation accuracy: 0.886800\n",
      "Iter-1 training loss: 0.0180 validation accuracy: 0.886200\n",
      "Iter-1 training loss: 0.0139 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0093 validation accuracy: 0.885800\n",
      "Iter-1 training loss: 0.0062 validation accuracy: 0.886200\n",
      "Iter-1 training loss: 0.0074 validation accuracy: 0.886200\n",
      "Iter-1 training loss: 0.0186 validation accuracy: 0.885200\n",
      "Iter-1 training loss: 0.0034 validation accuracy: 0.885000\n",
      "Iter-1 training loss: 0.0035 validation accuracy: 0.884000\n",
      "Iter-1 training loss: 0.0113 validation accuracy: 0.884800\n",
      "Iter-1 training loss: 0.0130 validation accuracy: 0.883600\n",
      "Iter-1 training loss: 0.0139 validation accuracy: 0.883400\n",
      "Iter-1 training loss: 0.0085 validation accuracy: 0.883800\n",
      "Iter-1 training loss: 0.0075 validation accuracy: 0.883400\n",
      "Iter-1 training loss: 0.0134 validation accuracy: 0.883000\n",
      "Iter-1 training loss: 0.0082 validation accuracy: 0.882600\n",
      "Iter-1 training loss: 0.0103 validation accuracy: 0.882400\n",
      "Iter-1 training loss: 0.0081 validation accuracy: 0.883000\n",
      "Iter-1 training loss: 0.0131 validation accuracy: 0.883600\n",
      "Iter-1 training loss: 0.0124 validation accuracy: 0.884400\n",
      "Iter-1 training loss: 0.0031 validation accuracy: 0.884600\n",
      "Iter-1 training loss: 0.0103 validation accuracy: 0.885600\n",
      "Iter-1 training loss: 0.0079 validation accuracy: 0.886000\n",
      "Iter-1 training loss: 0.0074 validation accuracy: 0.885600\n",
      "Iter-1 training loss: 0.0117 validation accuracy: 0.885800\n",
      "Iter-1 training loss: 0.0093 validation accuracy: 0.886000\n",
      "Iter-1 training loss: 0.0144 validation accuracy: 0.886200\n",
      "Iter-1 training loss: 0.0097 validation accuracy: 0.886200\n",
      "Iter-1 training loss: 0.0151 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0149 validation accuracy: 0.886000\n",
      "Iter-1 training loss: 0.0115 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0114 validation accuracy: 0.886800\n",
      "Iter-1 training loss: 0.0205 validation accuracy: 0.886600\n",
      "Iter-1 training loss: 0.0095 validation accuracy: 0.886000\n",
      "Iter-1 training loss: 0.0252 validation accuracy: 0.886400\n",
      "Iter-1 training loss: 0.0155 validation accuracy: 0.886800\n",
      "Iter-1 training loss: 0.0063 validation accuracy: 0.887000\n",
      "Iter-1 training loss: 0.0017 validation accuracy: 0.887800\n",
      "Iter-1 training loss: 0.0069 validation accuracy: 0.889200\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.890000\n",
      "Iter-1 training loss: 0.0231 validation accuracy: 0.891200\n",
      "Iter-1 training loss: 0.0114 validation accuracy: 0.891600\n",
      "Iter-1 training loss: 0.0104 validation accuracy: 0.892800\n",
      "Iter-1 training loss: 0.0143 validation accuracy: 0.893400\n",
      "Iter-1 training loss: 0.0209 validation accuracy: 0.893400\n",
      "Iter-1 training loss: 0.0142 validation accuracy: 0.893000\n",
      "Iter-1 training loss: 0.0178 validation accuracy: 0.893400\n",
      "Iter-1 training loss: 0.0180 validation accuracy: 0.893200\n",
      "Iter-1 training loss: 0.0094 validation accuracy: 0.892600\n",
      "Iter-1 training loss: 0.0129 validation accuracy: 0.892400\n",
      "Iter-1 training loss: 0.0162 validation accuracy: 0.892200\n",
      "Iter-1 training loss: 0.0088 validation accuracy: 0.892800\n",
      "Iter-1 training loss: 0.0093 validation accuracy: 0.893200\n",
      "Iter-1 training loss: 0.0063 validation accuracy: 0.893600\n",
      "Iter-1 training loss: 0.0181 validation accuracy: 0.894400\n",
      "Iter-1 training loss: 0.0220 validation accuracy: 0.895200\n",
      "Iter-1 training loss: 0.0109 validation accuracy: 0.895200\n",
      "Iter-1 training loss: 0.0086 validation accuracy: 0.895000\n",
      "Iter-1 training loss: 0.0116 validation accuracy: 0.894800\n",
      "Iter-1 training loss: 0.0209 validation accuracy: 0.894400\n",
      "Iter-1 training loss: 0.0221 validation accuracy: 0.895800\n",
      "Iter-1 training loss: 0.0141 validation accuracy: 0.896000\n",
      "Iter-1 training loss: 0.0111 validation accuracy: 0.896800\n",
      "Iter-1 training loss: 0.0111 validation accuracy: 0.896800\n",
      "Iter-1 training loss: 0.0066 validation accuracy: 0.896600\n",
      "Iter-1 training loss: 0.0201 validation accuracy: 0.897400\n",
      "Iter-1 training loss: 0.0061 validation accuracy: 0.897200\n",
      "Iter-1 training loss: 0.0139 validation accuracy: 0.897400\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.897600\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.898600\n",
      "Iter-1 training loss: 0.0165 validation accuracy: 0.898400\n",
      "Iter-1 training loss: 0.0109 validation accuracy: 0.898800\n",
      "Iter-1 training loss: 0.0187 validation accuracy: 0.898600\n",
      "Iter-1 training loss: 0.0026 validation accuracy: 0.898600\n",
      "Iter-1 training loss: 0.0119 validation accuracy: 0.898800\n",
      "Iter-1 training loss: 0.0053 validation accuracy: 0.899600\n",
      "Iter-1 training loss: 0.0050 validation accuracy: 0.900400\n",
      "Iter-1 training loss: 0.0088 validation accuracy: 0.901400\n",
      "Iter-1 training loss: 0.0119 validation accuracy: 0.902400\n",
      "Iter-1 training loss: 0.0104 validation accuracy: 0.902600\n",
      "Iter-1 training loss: 0.0159 validation accuracy: 0.903400\n",
      "Iter-1 training loss: 0.0063 validation accuracy: 0.903200\n",
      "Iter-1 training loss: 0.0152 validation accuracy: 0.902200\n",
      "Iter-1 training loss: 0.0162 validation accuracy: 0.902400\n",
      "Iter-1 training loss: 0.0089 validation accuracy: 0.903600\n",
      "Iter-1 training loss: 0.0080 validation accuracy: 0.903200\n",
      "Iter-1 training loss: 0.0153 validation accuracy: 0.902800\n",
      "Iter-1 training loss: 0.0107 validation accuracy: 0.902200\n",
      "Iter-1 training loss: 0.0082 validation accuracy: 0.900800\n",
      "Iter-1 training loss: 0.0038 validation accuracy: 0.901200\n",
      "Iter-1 training loss: 0.0077 validation accuracy: 0.900800\n",
      "Iter-1 training loss: 0.0162 validation accuracy: 0.900400\n",
      "Iter-1 training loss: 0.0112 validation accuracy: 0.900600\n",
      "Iter-1 training loss: 0.0116 validation accuracy: 0.900000\n",
      "Iter-1 training loss: 0.0113 validation accuracy: 0.899400\n",
      "Iter-1 training loss: 0.0086 validation accuracy: 0.899000\n",
      "Iter-1 training loss: 0.0077 validation accuracy: 0.897800\n",
      "Iter-1 training loss: 0.0127 validation accuracy: 0.897800\n",
      "Iter-1 training loss: 0.0072 validation accuracy: 0.897600\n",
      "Iter-1 training loss: 0.0133 validation accuracy: 0.896800\n",
      "Iter-1 training loss: 0.0158 validation accuracy: 0.895600\n",
      "Iter-1 training loss: 0.0093 validation accuracy: 0.894800\n",
      "Iter-1 training loss: 0.0144 validation accuracy: 0.893800\n",
      "Iter-1 training loss: 0.0150 validation accuracy: 0.893200\n",
      "Iter-1 training loss: 0.0127 validation accuracy: 0.891800\n",
      "Iter-1 training loss: 0.0068 validation accuracy: 0.891200\n",
      "Iter-1 training loss: 0.0062 validation accuracy: 0.890800\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.891000\n",
      "Iter-1 training loss: 0.0161 validation accuracy: 0.889400\n",
      "Iter-1 training loss: 0.0058 validation accuracy: 0.888000\n",
      "Iter-1 training loss: 0.0062 validation accuracy: 0.887400\n",
      "Iter-1 training loss: 0.0133 validation accuracy: 0.887800\n",
      "Iter-1 training loss: 0.0209 validation accuracy: 0.888000\n",
      "Iter-1 training loss: 0.0096 validation accuracy: 0.888200\n",
      "Iter-1 training loss: 0.0124 validation accuracy: 0.888200\n",
      "Iter-1 training loss: 0.0217 validation accuracy: 0.888200\n",
      "Iter-1 training loss: 0.0164 validation accuracy: 0.888400\n",
      "Iter-1 training loss: 0.0084 validation accuracy: 0.889600\n",
      "Iter-1 training loss: 0.0024 validation accuracy: 0.890000\n",
      "Iter-1 training loss: 0.0082 validation accuracy: 0.890800\n",
      "Iter-1 training loss: 0.0070 validation accuracy: 0.891000\n",
      "Iter-1 training loss: 0.0110 validation accuracy: 0.891800\n",
      "Iter-1 training loss: 0.0168 validation accuracy: 0.892200\n",
      "Iter-1 training loss: 0.0128 validation accuracy: 0.893600\n",
      "Iter-1 training loss: 0.0038 validation accuracy: 0.894200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 0.0158 validation accuracy: 0.895600\n",
      "Iter-1 training loss: 0.0146 validation accuracy: 0.896800\n",
      "Iter-1 training loss: 0.0165 validation accuracy: 0.897800\n",
      "Iter-1 training loss: 0.0139 validation accuracy: 0.898000\n",
      "Iter-1 training loss: 0.0081 validation accuracy: 0.899000\n",
      "Iter-1 training loss: 0.0066 validation accuracy: 0.899800\n",
      "Iter-1 training loss: 0.0139 validation accuracy: 0.900400\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.901200\n",
      "Iter-1 training loss: 0.0099 validation accuracy: 0.902000\n",
      "Iter-1 training loss: 0.0136 validation accuracy: 0.901600\n",
      "Iter-1 training loss: 0.0101 validation accuracy: 0.902400\n",
      "Iter-1 training loss: 0.0040 validation accuracy: 0.903200\n",
      "Iter-1 training loss: 0.0244 validation accuracy: 0.904000\n",
      "Iter-1 training loss: 0.0114 validation accuracy: 0.904800\n",
      "Iter-1 training loss: 0.0137 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0175 validation accuracy: 0.905200\n",
      "Iter-1 training loss: 0.0094 validation accuracy: 0.905400\n",
      "Iter-1 training loss: 0.0110 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0130 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0028 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0054 validation accuracy: 0.906200\n",
      "Iter-1 training loss: 0.0090 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0033 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0052 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0020 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0052 validation accuracy: 0.906400\n",
      "Iter-1 training loss: 0.0049 validation accuracy: 0.906200\n",
      "Iter-1 training loss: 0.0106 validation accuracy: 0.906400\n",
      "Iter-1 training loss: 0.0136 validation accuracy: 0.906600\n",
      "Iter-1 training loss: 0.0110 validation accuracy: 0.906400\n",
      "Iter-1 training loss: 0.0110 validation accuracy: 0.906200\n",
      "Iter-1 training loss: 0.0054 validation accuracy: 0.905800\n",
      "Iter-1 training loss: 0.0101 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0088 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0123 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0108 validation accuracy: 0.905400\n",
      "Iter-1 training loss: 0.0027 validation accuracy: 0.905800\n",
      "Iter-1 training loss: 0.0081 validation accuracy: 0.905200\n",
      "Iter-1 training loss: 0.0183 validation accuracy: 0.905000\n",
      "Iter-1 training loss: 0.0056 validation accuracy: 0.905400\n",
      "Iter-1 training loss: 0.0112 validation accuracy: 0.905600\n",
      "Iter-1 training loss: 0.0071 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0042 validation accuracy: 0.906000\n",
      "Iter-1 training loss: 0.0140 validation accuracy: 0.906200\n",
      "Iter-1 training loss: 0.0085 validation accuracy: 0.906600\n",
      "Iter-1 training loss: 0.0168 validation accuracy: 0.906600\n",
      "Iter-1 training loss: 0.0079 validation accuracy: 0.906800\n",
      "Iter-1 training loss: 0.0038 validation accuracy: 0.907600\n",
      "Iter-1 training loss: 0.0080 validation accuracy: 0.907200\n",
      "Iter-1 training loss: 0.0064 validation accuracy: 0.907600\n",
      "Iter-1 training loss: 0.0123 validation accuracy: 0.908000\n",
      "Iter-1 training loss: 0.0055 validation accuracy: 0.908200\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # print loss for train, valid, and test\n",
    "num_hidden_units = 8\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "net = CNN(C=C, D=D, H=num_layers, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "net = adam(nn=net, X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = net.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEACAYAAABGYoqtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+SEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc5a219b400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
