{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        #         dX = dout @ W.T # vanilla Backprop\n",
    "        #         dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y = np.tanh(y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = np.tanh(y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def onehot(self, labels):\n",
    "        # y = np.zeros([labels.size, np.max(labels) + 1])\n",
    "        y = np.zeros([labels.size, self.C])\n",
    "        y[range(labels.size), labels] = 1.\n",
    "        return y\n",
    "\n",
    "    def squared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        data_loss = 0.5 * np.sum((y_pred - self.onehot(y_train))**2)/ m\n",
    "        return data_loss\n",
    "\n",
    "    def dsquared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        grad_y = (y_pred - self.onehot(y_train))/ m\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.squared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        dy = self.dsquared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        \n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-0.4521, acc-0.0800, valid loss-0.4519, acc-0.0822, test loss-0.4517, acc-0.0857\n",
      "Iter-20, train loss-0.4516, acc-0.0900, valid loss-0.4518, acc-0.0816, test loss-0.4517, acc-0.0866\n",
      "Iter-30, train loss-0.4510, acc-0.1100, valid loss-0.4517, acc-0.0830, test loss-0.4516, acc-0.0880\n",
      "Iter-40, train loss-0.4518, acc-0.1300, valid loss-0.4516, acc-0.0836, test loss-0.4515, acc-0.0872\n",
      "Iter-50, train loss-0.4513, acc-0.1100, valid loss-0.4517, acc-0.0796, test loss-0.4516, acc-0.0864\n",
      "Iter-60, train loss-0.4531, acc-0.0800, valid loss-0.4515, acc-0.0828, test loss-0.4514, acc-0.0882\n",
      "Iter-70, train loss-0.4519, acc-0.0800, valid loss-0.4516, acc-0.0834, test loss-0.4515, acc-0.0877\n",
      "Iter-80, train loss-0.4516, acc-0.1000, valid loss-0.4514, acc-0.0874, test loss-0.4513, acc-0.0893\n",
      "Iter-90, train loss-0.4524, acc-0.0900, valid loss-0.4513, acc-0.0866, test loss-0.4513, acc-0.0881\n",
      "Iter-100, train loss-0.4515, acc-0.1000, valid loss-0.4511, acc-0.0894, test loss-0.4510, acc-0.0881\n",
      "Iter-110, train loss-0.4495, acc-0.1200, valid loss-0.4508, acc-0.0954, test loss-0.4507, acc-0.0923\n",
      "Iter-120, train loss-0.4498, acc-0.1100, valid loss-0.4505, acc-0.1016, test loss-0.4504, acc-0.0979\n",
      "Iter-130, train loss-0.4484, acc-0.1900, valid loss-0.4501, acc-0.1048, test loss-0.4500, acc-0.1034\n",
      "Iter-140, train loss-0.4489, acc-0.1500, valid loss-0.4499, acc-0.1082, test loss-0.4498, acc-0.1052\n",
      "Iter-150, train loss-0.4474, acc-0.1700, valid loss-0.4497, acc-0.1102, test loss-0.4496, acc-0.1075\n",
      "Iter-160, train loss-0.4478, acc-0.1500, valid loss-0.4492, acc-0.1144, test loss-0.4492, acc-0.1119\n",
      "Iter-170, train loss-0.4494, acc-0.0600, valid loss-0.4488, acc-0.1182, test loss-0.4487, acc-0.1182\n",
      "Iter-180, train loss-0.4474, acc-0.1000, valid loss-0.4484, acc-0.1232, test loss-0.4484, acc-0.1232\n",
      "Iter-190, train loss-0.4488, acc-0.1300, valid loss-0.4481, acc-0.1304, test loss-0.4481, acc-0.1288\n",
      "Iter-200, train loss-0.4488, acc-0.1200, valid loss-0.4478, acc-0.1366, test loss-0.4478, acc-0.1336\n",
      "Iter-210, train loss-0.4469, acc-0.1200, valid loss-0.4473, acc-0.1466, test loss-0.4473, acc-0.1409\n",
      "Iter-220, train loss-0.4491, acc-0.1600, valid loss-0.4469, acc-0.1502, test loss-0.4469, acc-0.1446\n",
      "Iter-230, train loss-0.4489, acc-0.1300, valid loss-0.4461, acc-0.1616, test loss-0.4461, acc-0.1539\n",
      "Iter-240, train loss-0.4454, acc-0.1900, valid loss-0.4453, acc-0.1744, test loss-0.4453, acc-0.1694\n",
      "Iter-250, train loss-0.4460, acc-0.1700, valid loss-0.4444, acc-0.1824, test loss-0.4444, acc-0.1812\n",
      "Iter-260, train loss-0.4445, acc-0.2000, valid loss-0.4434, acc-0.1970, test loss-0.4434, acc-0.1940\n",
      "Iter-270, train loss-0.4444, acc-0.2200, valid loss-0.4427, acc-0.2052, test loss-0.4427, acc-0.2022\n",
      "Iter-280, train loss-0.4438, acc-0.1800, valid loss-0.4417, acc-0.2172, test loss-0.4417, acc-0.2164\n",
      "Iter-290, train loss-0.4416, acc-0.2100, valid loss-0.4408, acc-0.2336, test loss-0.4408, acc-0.2285\n",
      "Iter-300, train loss-0.4363, acc-0.2800, valid loss-0.4396, acc-0.2482, test loss-0.4396, acc-0.2437\n",
      "Iter-310, train loss-0.4362, acc-0.2300, valid loss-0.4384, acc-0.2596, test loss-0.4385, acc-0.2551\n",
      "Iter-320, train loss-0.4352, acc-0.3300, valid loss-0.4374, acc-0.2718, test loss-0.4374, acc-0.2651\n",
      "Iter-330, train loss-0.4347, acc-0.3000, valid loss-0.4361, acc-0.2810, test loss-0.4361, acc-0.2763\n",
      "Iter-340, train loss-0.4385, acc-0.2300, valid loss-0.4347, acc-0.2882, test loss-0.4347, acc-0.2847\n",
      "Iter-350, train loss-0.4320, acc-0.3000, valid loss-0.4334, acc-0.2934, test loss-0.4334, acc-0.2918\n",
      "Iter-360, train loss-0.4282, acc-0.3700, valid loss-0.4316, acc-0.3006, test loss-0.4316, acc-0.2981\n",
      "Iter-370, train loss-0.4297, acc-0.2800, valid loss-0.4301, acc-0.3076, test loss-0.4300, acc-0.3042\n",
      "Iter-380, train loss-0.4305, acc-0.2400, valid loss-0.4280, acc-0.3166, test loss-0.4279, acc-0.3109\n",
      "Iter-390, train loss-0.4279, acc-0.2600, valid loss-0.4260, acc-0.3160, test loss-0.4260, acc-0.3132\n",
      "Iter-400, train loss-0.4248, acc-0.3100, valid loss-0.4240, acc-0.3180, test loss-0.4239, acc-0.3153\n",
      "Iter-410, train loss-0.4265, acc-0.2500, valid loss-0.4221, acc-0.3226, test loss-0.4219, acc-0.3213\n",
      "Iter-420, train loss-0.4215, acc-0.3600, valid loss-0.4200, acc-0.3258, test loss-0.4198, acc-0.3252\n",
      "Iter-430, train loss-0.4221, acc-0.2800, valid loss-0.4179, acc-0.3296, test loss-0.4177, acc-0.3291\n",
      "Iter-440, train loss-0.4191, acc-0.2900, valid loss-0.4158, acc-0.3370, test loss-0.4156, acc-0.3369\n",
      "Iter-450, train loss-0.4095, acc-0.3600, valid loss-0.4134, acc-0.3474, test loss-0.4132, acc-0.3437\n",
      "Iter-460, train loss-0.4077, acc-0.3000, valid loss-0.4111, acc-0.3604, test loss-0.4108, acc-0.3520\n",
      "Iter-470, train loss-0.4123, acc-0.3300, valid loss-0.4082, acc-0.3688, test loss-0.4079, acc-0.3642\n",
      "Iter-480, train loss-0.4108, acc-0.2600, valid loss-0.4058, acc-0.3782, test loss-0.4054, acc-0.3764\n",
      "Iter-490, train loss-0.4094, acc-0.3300, valid loss-0.4037, acc-0.3850, test loss-0.4033, acc-0.3851\n",
      "Iter-500, train loss-0.4054, acc-0.3900, valid loss-0.4013, acc-0.4020, test loss-0.4009, acc-0.4036\n",
      "Iter-510, train loss-0.3959, acc-0.4600, valid loss-0.3990, acc-0.4324, test loss-0.3985, acc-0.4267\n",
      "Iter-520, train loss-0.3938, acc-0.4700, valid loss-0.3966, acc-0.4690, test loss-0.3960, acc-0.4652\n",
      "Iter-530, train loss-0.3941, acc-0.4900, valid loss-0.3944, acc-0.4906, test loss-0.3939, acc-0.4789\n",
      "Iter-540, train loss-0.3963, acc-0.4500, valid loss-0.3923, acc-0.5056, test loss-0.3918, acc-0.4961\n",
      "Iter-550, train loss-0.3884, acc-0.5200, valid loss-0.3900, acc-0.5104, test loss-0.3895, acc-0.5022\n",
      "Iter-560, train loss-0.4017, acc-0.3900, valid loss-0.3878, acc-0.5162, test loss-0.3873, acc-0.5111\n",
      "Iter-570, train loss-0.3902, acc-0.5200, valid loss-0.3855, acc-0.5160, test loss-0.3850, acc-0.5117\n",
      "Iter-580, train loss-0.3806, acc-0.5500, valid loss-0.3831, acc-0.5158, test loss-0.3827, acc-0.5088\n",
      "Iter-590, train loss-0.3779, acc-0.4800, valid loss-0.3807, acc-0.5172, test loss-0.3803, acc-0.5113\n",
      "Iter-600, train loss-0.3753, acc-0.5000, valid loss-0.3788, acc-0.5150, test loss-0.3783, acc-0.5078\n",
      "Iter-610, train loss-0.3756, acc-0.4800, valid loss-0.3769, acc-0.5182, test loss-0.3764, acc-0.5113\n",
      "Iter-620, train loss-0.3886, acc-0.4900, valid loss-0.3749, acc-0.5166, test loss-0.3745, acc-0.5099\n",
      "Iter-630, train loss-0.3612, acc-0.6200, valid loss-0.3734, acc-0.5136, test loss-0.3730, acc-0.5072\n",
      "Iter-640, train loss-0.3673, acc-0.4800, valid loss-0.3720, acc-0.5154, test loss-0.3715, acc-0.5065\n",
      "Iter-650, train loss-0.3730, acc-0.4800, valid loss-0.3701, acc-0.5086, test loss-0.3697, acc-0.5017\n",
      "Iter-660, train loss-0.3580, acc-0.5900, valid loss-0.3683, acc-0.5138, test loss-0.3679, acc-0.5058\n",
      "Iter-670, train loss-0.3567, acc-0.5700, valid loss-0.3668, acc-0.5160, test loss-0.3664, acc-0.5080\n",
      "Iter-680, train loss-0.3751, acc-0.5100, valid loss-0.3649, acc-0.5176, test loss-0.3646, acc-0.5084\n",
      "Iter-690, train loss-0.3673, acc-0.4700, valid loss-0.3631, acc-0.5186, test loss-0.3629, acc-0.5086\n",
      "Iter-700, train loss-0.3541, acc-0.5300, valid loss-0.3618, acc-0.5250, test loss-0.3616, acc-0.5161\n",
      "Iter-710, train loss-0.3651, acc-0.5400, valid loss-0.3602, acc-0.5260, test loss-0.3601, acc-0.5172\n",
      "Iter-720, train loss-0.3575, acc-0.5700, valid loss-0.3584, acc-0.5260, test loss-0.3584, acc-0.5170\n",
      "Iter-730, train loss-0.3607, acc-0.5400, valid loss-0.3569, acc-0.5260, test loss-0.3570, acc-0.5161\n",
      "Iter-740, train loss-0.3379, acc-0.5700, valid loss-0.3554, acc-0.5272, test loss-0.3555, acc-0.5176\n",
      "Iter-750, train loss-0.3453, acc-0.5400, valid loss-0.3540, acc-0.5256, test loss-0.3542, acc-0.5167\n",
      "Iter-760, train loss-0.3749, acc-0.4200, valid loss-0.3526, acc-0.5268, test loss-0.3527, acc-0.5176\n",
      "Iter-770, train loss-0.3367, acc-0.5700, valid loss-0.3513, acc-0.5248, test loss-0.3515, acc-0.5159\n",
      "Iter-780, train loss-0.3449, acc-0.4900, valid loss-0.3500, acc-0.5284, test loss-0.3502, acc-0.5175\n",
      "Iter-790, train loss-0.3241, acc-0.6300, valid loss-0.3486, acc-0.5284, test loss-0.3489, acc-0.5182\n",
      "Iter-800, train loss-0.3571, acc-0.4800, valid loss-0.3473, acc-0.5290, test loss-0.3477, acc-0.5192\n",
      "Iter-810, train loss-0.3385, acc-0.5400, valid loss-0.3462, acc-0.5324, test loss-0.3467, acc-0.5199\n",
      "Iter-820, train loss-0.3261, acc-0.5300, valid loss-0.3448, acc-0.5352, test loss-0.3453, acc-0.5232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-0.3558, acc-0.5000, valid loss-0.3438, acc-0.5358, test loss-0.3443, acc-0.5234\n",
      "Iter-840, train loss-0.3258, acc-0.6100, valid loss-0.3426, acc-0.5348, test loss-0.3432, acc-0.5216\n",
      "Iter-850, train loss-0.3467, acc-0.4900, valid loss-0.3414, acc-0.5376, test loss-0.3422, acc-0.5209\n",
      "Iter-860, train loss-0.3304, acc-0.5600, valid loss-0.3400, acc-0.5422, test loss-0.3409, acc-0.5251\n",
      "Iter-870, train loss-0.3482, acc-0.4800, valid loss-0.3389, acc-0.5436, test loss-0.3397, acc-0.5271\n",
      "Iter-880, train loss-0.3488, acc-0.5300, valid loss-0.3372, acc-0.5476, test loss-0.3381, acc-0.5321\n",
      "Iter-890, train loss-0.3325, acc-0.5800, valid loss-0.3359, acc-0.5510, test loss-0.3370, acc-0.5348\n",
      "Iter-900, train loss-0.3340, acc-0.5900, valid loss-0.3345, acc-0.5540, test loss-0.3355, acc-0.5394\n",
      "Iter-910, train loss-0.3215, acc-0.6000, valid loss-0.3332, acc-0.5566, test loss-0.3343, acc-0.5427\n",
      "Iter-920, train loss-0.3270, acc-0.5800, valid loss-0.3322, acc-0.5554, test loss-0.3334, acc-0.5436\n",
      "Iter-930, train loss-0.3245, acc-0.5800, valid loss-0.3311, acc-0.5578, test loss-0.3324, acc-0.5436\n",
      "Iter-940, train loss-0.3323, acc-0.5300, valid loss-0.3296, acc-0.5602, test loss-0.3310, acc-0.5492\n",
      "Iter-950, train loss-0.3326, acc-0.5800, valid loss-0.3283, acc-0.5616, test loss-0.3297, acc-0.5516\n",
      "Iter-960, train loss-0.3319, acc-0.5300, valid loss-0.3270, acc-0.5662, test loss-0.3286, acc-0.5553\n",
      "Iter-970, train loss-0.3494, acc-0.5000, valid loss-0.3256, acc-0.5692, test loss-0.3271, acc-0.5601\n",
      "Iter-980, train loss-0.3257, acc-0.5700, valid loss-0.3244, acc-0.5706, test loss-0.3259, acc-0.5634\n",
      "Iter-990, train loss-0.3310, acc-0.5800, valid loss-0.3230, acc-0.5728, test loss-0.3243, acc-0.5657\n",
      "Iter-1000, train loss-0.3214, acc-0.6100, valid loss-0.3217, acc-0.5732, test loss-0.3231, acc-0.5660\n",
      "Iter-1010, train loss-0.3252, acc-0.5700, valid loss-0.3206, acc-0.5730, test loss-0.3220, acc-0.5672\n",
      "Iter-1020, train loss-0.3298, acc-0.5700, valid loss-0.3193, acc-0.5788, test loss-0.3208, acc-0.5723\n",
      "Iter-1030, train loss-0.3167, acc-0.6200, valid loss-0.3180, acc-0.5758, test loss-0.3194, acc-0.5736\n",
      "Iter-1040, train loss-0.3374, acc-0.5500, valid loss-0.3159, acc-0.5906, test loss-0.3177, acc-0.5850\n",
      "Iter-1050, train loss-0.3087, acc-0.6000, valid loss-0.3144, acc-0.5966, test loss-0.3164, acc-0.5897\n",
      "Iter-1060, train loss-0.3506, acc-0.4700, valid loss-0.3133, acc-0.5978, test loss-0.3152, acc-0.5905\n",
      "Iter-1070, train loss-0.3158, acc-0.6300, valid loss-0.3120, acc-0.5996, test loss-0.3140, acc-0.5913\n",
      "Iter-1080, train loss-0.3281, acc-0.5500, valid loss-0.3108, acc-0.6042, test loss-0.3130, acc-0.5954\n",
      "Iter-1090, train loss-0.3361, acc-0.4900, valid loss-0.3095, acc-0.6072, test loss-0.3119, acc-0.5977\n",
      "Iter-1100, train loss-0.3347, acc-0.5600, valid loss-0.3084, acc-0.6110, test loss-0.3109, acc-0.6011\n",
      "Iter-1110, train loss-0.3053, acc-0.6800, valid loss-0.3072, acc-0.6128, test loss-0.3098, acc-0.6009\n",
      "Iter-1120, train loss-0.3128, acc-0.6100, valid loss-0.3062, acc-0.6122, test loss-0.3088, acc-0.6018\n",
      "Iter-1130, train loss-0.3117, acc-0.6100, valid loss-0.3048, acc-0.6186, test loss-0.3076, acc-0.6041\n",
      "Iter-1140, train loss-0.3270, acc-0.5300, valid loss-0.3033, acc-0.6190, test loss-0.3062, acc-0.6082\n",
      "Iter-1150, train loss-0.3106, acc-0.5600, valid loss-0.3023, acc-0.6210, test loss-0.3053, acc-0.6117\n",
      "Iter-1160, train loss-0.3061, acc-0.6100, valid loss-0.3008, acc-0.6222, test loss-0.3039, acc-0.6120\n",
      "Iter-1170, train loss-0.2901, acc-0.6500, valid loss-0.2996, acc-0.6242, test loss-0.3026, acc-0.6158\n",
      "Iter-1180, train loss-0.3390, acc-0.5400, valid loss-0.2980, acc-0.6280, test loss-0.3010, acc-0.6203\n",
      "Iter-1190, train loss-0.2894, acc-0.6500, valid loss-0.2966, acc-0.6302, test loss-0.2998, acc-0.6211\n",
      "Iter-1200, train loss-0.2825, acc-0.6600, valid loss-0.2950, acc-0.6370, test loss-0.2983, acc-0.6256\n",
      "Iter-1210, train loss-0.3062, acc-0.6100, valid loss-0.2938, acc-0.6388, test loss-0.2971, acc-0.6290\n",
      "Iter-1220, train loss-0.2888, acc-0.6500, valid loss-0.2925, acc-0.6402, test loss-0.2959, acc-0.6299\n",
      "Iter-1230, train loss-0.3061, acc-0.6300, valid loss-0.2908, acc-0.6448, test loss-0.2943, acc-0.6343\n",
      "Iter-1240, train loss-0.3256, acc-0.5800, valid loss-0.2896, acc-0.6472, test loss-0.2933, acc-0.6357\n",
      "Iter-1250, train loss-0.3093, acc-0.6000, valid loss-0.2885, acc-0.6502, test loss-0.2924, acc-0.6363\n",
      "Iter-1260, train loss-0.2558, acc-0.7300, valid loss-0.2871, acc-0.6516, test loss-0.2911, acc-0.6378\n",
      "Iter-1270, train loss-0.2868, acc-0.6000, valid loss-0.2860, acc-0.6518, test loss-0.2900, acc-0.6379\n",
      "Iter-1280, train loss-0.2980, acc-0.6000, valid loss-0.2846, acc-0.6542, test loss-0.2886, acc-0.6407\n",
      "Iter-1290, train loss-0.2877, acc-0.6400, valid loss-0.2831, acc-0.6580, test loss-0.2870, acc-0.6447\n",
      "Iter-1300, train loss-0.2659, acc-0.7100, valid loss-0.2813, acc-0.6592, test loss-0.2850, acc-0.6466\n",
      "Iter-1310, train loss-0.2627, acc-0.7700, valid loss-0.2799, acc-0.6616, test loss-0.2837, acc-0.6491\n",
      "Iter-1320, train loss-0.2674, acc-0.7200, valid loss-0.2783, acc-0.6648, test loss-0.2820, acc-0.6527\n",
      "Iter-1330, train loss-0.3017, acc-0.5800, valid loss-0.2770, acc-0.6656, test loss-0.2806, acc-0.6528\n",
      "Iter-1340, train loss-0.2781, acc-0.6600, valid loss-0.2761, acc-0.6660, test loss-0.2798, acc-0.6541\n",
      "Iter-1350, train loss-0.2757, acc-0.6700, valid loss-0.2746, acc-0.6684, test loss-0.2784, acc-0.6534\n",
      "Iter-1360, train loss-0.3030, acc-0.5900, valid loss-0.2732, acc-0.6712, test loss-0.2771, acc-0.6567\n",
      "Iter-1370, train loss-0.2906, acc-0.6100, valid loss-0.2720, acc-0.6724, test loss-0.2758, acc-0.6590\n",
      "Iter-1380, train loss-0.2777, acc-0.6900, valid loss-0.2704, acc-0.6750, test loss-0.2744, acc-0.6614\n",
      "Iter-1390, train loss-0.2809, acc-0.6800, valid loss-0.2693, acc-0.6764, test loss-0.2734, acc-0.6620\n",
      "Iter-1400, train loss-0.2967, acc-0.6300, valid loss-0.2678, acc-0.6766, test loss-0.2719, acc-0.6626\n",
      "Iter-1410, train loss-0.2774, acc-0.6500, valid loss-0.2665, acc-0.6784, test loss-0.2707, acc-0.6635\n",
      "Iter-1420, train loss-0.2951, acc-0.6000, valid loss-0.2652, acc-0.6806, test loss-0.2695, acc-0.6648\n",
      "Iter-1430, train loss-0.2817, acc-0.6400, valid loss-0.2640, acc-0.6826, test loss-0.2681, acc-0.6672\n",
      "Iter-1440, train loss-0.2620, acc-0.7300, valid loss-0.2628, acc-0.6818, test loss-0.2669, acc-0.6649\n",
      "Iter-1450, train loss-0.2895, acc-0.6000, valid loss-0.2615, acc-0.6826, test loss-0.2656, acc-0.6684\n",
      "Iter-1460, train loss-0.2675, acc-0.6900, valid loss-0.2603, acc-0.6846, test loss-0.2647, acc-0.6703\n",
      "Iter-1470, train loss-0.2820, acc-0.6000, valid loss-0.2592, acc-0.6840, test loss-0.2634, acc-0.6694\n",
      "Iter-1480, train loss-0.2617, acc-0.6900, valid loss-0.2575, acc-0.6872, test loss-0.2618, acc-0.6726\n",
      "Iter-1490, train loss-0.2627, acc-0.6900, valid loss-0.2565, acc-0.6882, test loss-0.2608, acc-0.6724\n",
      "Iter-1500, train loss-0.2404, acc-0.7400, valid loss-0.2553, acc-0.6896, test loss-0.2596, acc-0.6746\n",
      "Iter-1510, train loss-0.2190, acc-0.8200, valid loss-0.2540, acc-0.6938, test loss-0.2584, acc-0.6778\n",
      "Iter-1520, train loss-0.2747, acc-0.6400, valid loss-0.2529, acc-0.6928, test loss-0.2574, acc-0.6787\n",
      "Iter-1530, train loss-0.2528, acc-0.6900, valid loss-0.2518, acc-0.6962, test loss-0.2563, acc-0.6799\n",
      "Iter-1540, train loss-0.2415, acc-0.7200, valid loss-0.2506, acc-0.6974, test loss-0.2551, acc-0.6818\n",
      "Iter-1550, train loss-0.2393, acc-0.7000, valid loss-0.2496, acc-0.6980, test loss-0.2540, acc-0.6832\n",
      "Iter-1560, train loss-0.2766, acc-0.6700, valid loss-0.2483, acc-0.7006, test loss-0.2527, acc-0.6839\n",
      "Iter-1570, train loss-0.2493, acc-0.6600, valid loss-0.2470, acc-0.7018, test loss-0.2513, acc-0.6839\n",
      "Iter-1580, train loss-0.2379, acc-0.7600, valid loss-0.2457, acc-0.7006, test loss-0.2503, acc-0.6840\n",
      "Iter-1590, train loss-0.2337, acc-0.7000, valid loss-0.2445, acc-0.7016, test loss-0.2490, acc-0.6857\n",
      "Iter-1600, train loss-0.2535, acc-0.6900, valid loss-0.2434, acc-0.7036, test loss-0.2478, acc-0.6868\n",
      "Iter-1610, train loss-0.2585, acc-0.6900, valid loss-0.2421, acc-0.7046, test loss-0.2464, acc-0.6882\n",
      "Iter-1620, train loss-0.2597, acc-0.6500, valid loss-0.2407, acc-0.7056, test loss-0.2451, acc-0.6889\n",
      "Iter-1630, train loss-0.2316, acc-0.7100, valid loss-0.2397, acc-0.7062, test loss-0.2440, acc-0.6893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.2790, acc-0.6000, valid loss-0.2388, acc-0.7082, test loss-0.2430, acc-0.6914\n",
      "Iter-1650, train loss-0.2260, acc-0.7200, valid loss-0.2380, acc-0.7102, test loss-0.2422, acc-0.6925\n",
      "Iter-1660, train loss-0.2342, acc-0.7300, valid loss-0.2372, acc-0.7096, test loss-0.2412, acc-0.6936\n",
      "Iter-1670, train loss-0.2366, acc-0.7500, valid loss-0.2360, acc-0.7110, test loss-0.2400, acc-0.6954\n",
      "Iter-1680, train loss-0.2386, acc-0.7300, valid loss-0.2353, acc-0.7116, test loss-0.2394, acc-0.6947\n",
      "Iter-1690, train loss-0.2801, acc-0.6300, valid loss-0.2342, acc-0.7138, test loss-0.2381, acc-0.6959\n",
      "Iter-1700, train loss-0.2566, acc-0.6700, valid loss-0.2331, acc-0.7152, test loss-0.2369, acc-0.6983\n",
      "Iter-1710, train loss-0.2367, acc-0.7000, valid loss-0.2320, acc-0.7170, test loss-0.2358, acc-0.6990\n",
      "Iter-1720, train loss-0.2731, acc-0.6200, valid loss-0.2311, acc-0.7156, test loss-0.2349, acc-0.6987\n",
      "Iter-1730, train loss-0.2371, acc-0.6900, valid loss-0.2299, acc-0.7194, test loss-0.2338, acc-0.7016\n",
      "Iter-1740, train loss-0.2295, acc-0.6800, valid loss-0.2291, acc-0.7210, test loss-0.2328, acc-0.7033\n",
      "Iter-1750, train loss-0.2530, acc-0.6400, valid loss-0.2282, acc-0.7222, test loss-0.2319, acc-0.7046\n",
      "Iter-1760, train loss-0.2379, acc-0.6900, valid loss-0.2273, acc-0.7226, test loss-0.2310, acc-0.7052\n",
      "Iter-1770, train loss-0.2509, acc-0.6500, valid loss-0.2263, acc-0.7238, test loss-0.2299, acc-0.7073\n",
      "Iter-1780, train loss-0.2414, acc-0.6800, valid loss-0.2254, acc-0.7254, test loss-0.2289, acc-0.7104\n",
      "Iter-1790, train loss-0.2293, acc-0.7400, valid loss-0.2247, acc-0.7266, test loss-0.2283, acc-0.7105\n",
      "Iter-1800, train loss-0.2246, acc-0.6900, valid loss-0.2240, acc-0.7280, test loss-0.2276, acc-0.7101\n",
      "Iter-1810, train loss-0.2368, acc-0.6900, valid loss-0.2230, acc-0.7284, test loss-0.2267, acc-0.7108\n",
      "Iter-1820, train loss-0.2446, acc-0.6500, valid loss-0.2224, acc-0.7280, test loss-0.2259, acc-0.7109\n",
      "Iter-1830, train loss-0.2449, acc-0.6600, valid loss-0.2215, acc-0.7280, test loss-0.2249, acc-0.7121\n",
      "Iter-1840, train loss-0.2094, acc-0.7800, valid loss-0.2207, acc-0.7298, test loss-0.2240, acc-0.7141\n",
      "Iter-1850, train loss-0.2366, acc-0.6800, valid loss-0.2197, acc-0.7290, test loss-0.2230, acc-0.7145\n",
      "Iter-1860, train loss-0.2621, acc-0.6500, valid loss-0.2189, acc-0.7296, test loss-0.2222, acc-0.7142\n",
      "Iter-1870, train loss-0.2337, acc-0.6600, valid loss-0.2184, acc-0.7312, test loss-0.2216, acc-0.7149\n",
      "Iter-1880, train loss-0.2329, acc-0.7000, valid loss-0.2174, acc-0.7320, test loss-0.2206, acc-0.7153\n",
      "Iter-1890, train loss-0.2366, acc-0.6900, valid loss-0.2168, acc-0.7312, test loss-0.2200, acc-0.7165\n",
      "Iter-1900, train loss-0.2371, acc-0.7200, valid loss-0.2159, acc-0.7312, test loss-0.2191, acc-0.7170\n",
      "Iter-1910, train loss-0.2301, acc-0.7100, valid loss-0.2150, acc-0.7332, test loss-0.2181, acc-0.7204\n",
      "Iter-1920, train loss-0.2303, acc-0.6600, valid loss-0.2142, acc-0.7338, test loss-0.2172, acc-0.7221\n",
      "Iter-1930, train loss-0.2462, acc-0.7000, valid loss-0.2133, acc-0.7366, test loss-0.2162, acc-0.7235\n",
      "Iter-1940, train loss-0.2012, acc-0.7500, valid loss-0.2125, acc-0.7382, test loss-0.2155, acc-0.7251\n",
      "Iter-1950, train loss-0.2022, acc-0.7500, valid loss-0.2120, acc-0.7376, test loss-0.2151, acc-0.7247\n",
      "Iter-1960, train loss-0.2075, acc-0.7400, valid loss-0.2113, acc-0.7392, test loss-0.2141, acc-0.7268\n",
      "Iter-1970, train loss-0.2735, acc-0.6100, valid loss-0.2105, acc-0.7406, test loss-0.2133, acc-0.7282\n",
      "Iter-1980, train loss-0.2268, acc-0.7000, valid loss-0.2100, acc-0.7410, test loss-0.2127, acc-0.7283\n",
      "Iter-1990, train loss-0.2215, acc-0.7300, valid loss-0.2092, acc-0.7412, test loss-0.2118, acc-0.7305\n",
      "Iter-2000, train loss-0.2272, acc-0.6900, valid loss-0.2086, acc-0.7420, test loss-0.2109, acc-0.7328\n",
      "Iter-2010, train loss-0.2186, acc-0.7000, valid loss-0.2081, acc-0.7430, test loss-0.2103, acc-0.7324\n",
      "Iter-2020, train loss-0.2340, acc-0.7200, valid loss-0.2075, acc-0.7414, test loss-0.2097, acc-0.7320\n",
      "Iter-2030, train loss-0.2121, acc-0.7300, valid loss-0.2069, acc-0.7414, test loss-0.2091, acc-0.7330\n",
      "Iter-2040, train loss-0.1943, acc-0.7500, valid loss-0.2060, acc-0.7432, test loss-0.2083, acc-0.7326\n",
      "Iter-2050, train loss-0.1995, acc-0.7400, valid loss-0.2055, acc-0.7442, test loss-0.2078, acc-0.7327\n",
      "Iter-2060, train loss-0.2272, acc-0.6700, valid loss-0.2048, acc-0.7446, test loss-0.2072, acc-0.7336\n",
      "Iter-2070, train loss-0.1757, acc-0.8100, valid loss-0.2041, acc-0.7462, test loss-0.2064, acc-0.7349\n",
      "Iter-2080, train loss-0.1814, acc-0.7500, valid loss-0.2031, acc-0.7462, test loss-0.2054, acc-0.7361\n",
      "Iter-2090, train loss-0.2137, acc-0.6800, valid loss-0.2025, acc-0.7466, test loss-0.2046, acc-0.7382\n",
      "Iter-2100, train loss-0.2073, acc-0.7500, valid loss-0.2018, acc-0.7470, test loss-0.2039, acc-0.7394\n",
      "Iter-2110, train loss-0.1945, acc-0.7300, valid loss-0.2011, acc-0.7476, test loss-0.2033, acc-0.7401\n",
      "Iter-2120, train loss-0.2464, acc-0.7200, valid loss-0.2006, acc-0.7486, test loss-0.2028, acc-0.7408\n",
      "Iter-2130, train loss-0.2242, acc-0.7000, valid loss-0.2000, acc-0.7502, test loss-0.2022, acc-0.7426\n",
      "Iter-2140, train loss-0.2473, acc-0.6400, valid loss-0.1993, acc-0.7510, test loss-0.2015, acc-0.7436\n",
      "Iter-2150, train loss-0.2151, acc-0.7300, valid loss-0.1987, acc-0.7532, test loss-0.2009, acc-0.7439\n",
      "Iter-2160, train loss-0.2072, acc-0.7400, valid loss-0.1980, acc-0.7526, test loss-0.2002, acc-0.7455\n",
      "Iter-2170, train loss-0.1831, acc-0.7400, valid loss-0.1974, acc-0.7540, test loss-0.1995, acc-0.7461\n",
      "Iter-2180, train loss-0.2161, acc-0.7300, valid loss-0.1967, acc-0.7558, test loss-0.1988, acc-0.7474\n",
      "Iter-2190, train loss-0.2132, acc-0.6800, valid loss-0.1963, acc-0.7570, test loss-0.1981, acc-0.7492\n",
      "Iter-2200, train loss-0.2081, acc-0.7000, valid loss-0.1957, acc-0.7568, test loss-0.1974, acc-0.7500\n",
      "Iter-2210, train loss-0.2299, acc-0.6600, valid loss-0.1955, acc-0.7548, test loss-0.1974, acc-0.7480\n",
      "Iter-2220, train loss-0.1881, acc-0.7400, valid loss-0.1951, acc-0.7558, test loss-0.1972, acc-0.7477\n",
      "Iter-2230, train loss-0.2207, acc-0.7600, valid loss-0.1946, acc-0.7570, test loss-0.1969, acc-0.7471\n",
      "Iter-2240, train loss-0.2120, acc-0.7300, valid loss-0.1941, acc-0.7592, test loss-0.1961, acc-0.7497\n",
      "Iter-2250, train loss-0.1973, acc-0.7800, valid loss-0.1934, acc-0.7600, test loss-0.1954, acc-0.7498\n",
      "Iter-2260, train loss-0.1869, acc-0.7500, valid loss-0.1933, acc-0.7580, test loss-0.1951, acc-0.7501\n",
      "Iter-2270, train loss-0.2091, acc-0.7400, valid loss-0.1927, acc-0.7586, test loss-0.1947, acc-0.7512\n",
      "Iter-2280, train loss-0.1779, acc-0.7600, valid loss-0.1922, acc-0.7600, test loss-0.1942, acc-0.7512\n",
      "Iter-2290, train loss-0.1795, acc-0.7500, valid loss-0.1916, acc-0.7604, test loss-0.1936, acc-0.7516\n",
      "Iter-2300, train loss-0.1888, acc-0.7300, valid loss-0.1912, acc-0.7626, test loss-0.1931, acc-0.7523\n",
      "Iter-2310, train loss-0.2159, acc-0.7200, valid loss-0.1909, acc-0.7620, test loss-0.1929, acc-0.7524\n",
      "Iter-2320, train loss-0.2023, acc-0.7200, valid loss-0.1905, acc-0.7616, test loss-0.1923, acc-0.7549\n",
      "Iter-2330, train loss-0.1856, acc-0.7800, valid loss-0.1900, acc-0.7642, test loss-0.1920, acc-0.7541\n",
      "Iter-2340, train loss-0.2280, acc-0.7000, valid loss-0.1895, acc-0.7652, test loss-0.1915, acc-0.7552\n",
      "Iter-2350, train loss-0.2022, acc-0.7300, valid loss-0.1892, acc-0.7652, test loss-0.1912, acc-0.7550\n",
      "Iter-2360, train loss-0.2227, acc-0.7000, valid loss-0.1888, acc-0.7644, test loss-0.1908, acc-0.7548\n",
      "Iter-2370, train loss-0.2024, acc-0.7400, valid loss-0.1885, acc-0.7664, test loss-0.1902, acc-0.7562\n",
      "Iter-2380, train loss-0.2043, acc-0.7100, valid loss-0.1879, acc-0.7670, test loss-0.1896, acc-0.7568\n",
      "Iter-2390, train loss-0.1951, acc-0.7500, valid loss-0.1877, acc-0.7650, test loss-0.1897, acc-0.7534\n",
      "Iter-2400, train loss-0.1597, acc-0.8400, valid loss-0.1872, acc-0.7656, test loss-0.1891, acc-0.7539\n",
      "Iter-2410, train loss-0.1872, acc-0.7600, valid loss-0.1868, acc-0.7672, test loss-0.1887, acc-0.7552\n",
      "Iter-2420, train loss-0.1975, acc-0.7500, valid loss-0.1867, acc-0.7692, test loss-0.1887, acc-0.7558\n",
      "Iter-2430, train loss-0.2302, acc-0.7300, valid loss-0.1863, acc-0.7708, test loss-0.1882, acc-0.7582\n",
      "Iter-2440, train loss-0.2143, acc-0.6900, valid loss-0.1858, acc-0.7702, test loss-0.1878, acc-0.7569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.1843, acc-0.7800, valid loss-0.1859, acc-0.7706, test loss-0.1878, acc-0.7585\n",
      "Iter-2460, train loss-0.2034, acc-0.7600, valid loss-0.1858, acc-0.7702, test loss-0.1878, acc-0.7566\n",
      "Iter-2470, train loss-0.1827, acc-0.7800, valid loss-0.1856, acc-0.7670, test loss-0.1878, acc-0.7535\n",
      "Iter-2480, train loss-0.1949, acc-0.7000, valid loss-0.1856, acc-0.7680, test loss-0.1878, acc-0.7555\n",
      "Iter-2490, train loss-0.2030, acc-0.7200, valid loss-0.1852, acc-0.7706, test loss-0.1872, acc-0.7576\n",
      "Iter-2500, train loss-0.1948, acc-0.7600, valid loss-0.1848, acc-0.7716, test loss-0.1868, acc-0.7589\n",
      "Iter-2510, train loss-0.1717, acc-0.8000, valid loss-0.1850, acc-0.7716, test loss-0.1870, acc-0.7582\n",
      "Iter-2520, train loss-0.2067, acc-0.7400, valid loss-0.1851, acc-0.7732, test loss-0.1872, acc-0.7585\n",
      "Iter-2530, train loss-0.1956, acc-0.7300, valid loss-0.1850, acc-0.7730, test loss-0.1870, acc-0.7593\n",
      "Iter-2540, train loss-0.1760, acc-0.8000, valid loss-0.1849, acc-0.7736, test loss-0.1869, acc-0.7603\n",
      "Iter-2550, train loss-0.2007, acc-0.7200, valid loss-0.1847, acc-0.7740, test loss-0.1868, acc-0.7586\n",
      "Iter-2560, train loss-0.2055, acc-0.7400, valid loss-0.1847, acc-0.7728, test loss-0.1866, acc-0.7587\n",
      "Iter-2570, train loss-0.1827, acc-0.7600, valid loss-0.1845, acc-0.7730, test loss-0.1862, acc-0.7604\n",
      "Iter-2580, train loss-0.2151, acc-0.7200, valid loss-0.1843, acc-0.7730, test loss-0.1860, acc-0.7619\n",
      "Iter-2590, train loss-0.2476, acc-0.6500, valid loss-0.1843, acc-0.7718, test loss-0.1858, acc-0.7623\n",
      "Iter-2600, train loss-0.2101, acc-0.7300, valid loss-0.1840, acc-0.7734, test loss-0.1854, acc-0.7641\n",
      "Iter-2610, train loss-0.2409, acc-0.6300, valid loss-0.1837, acc-0.7744, test loss-0.1851, acc-0.7632\n",
      "Iter-2620, train loss-0.1738, acc-0.8600, valid loss-0.1836, acc-0.7730, test loss-0.1849, acc-0.7633\n",
      "Iter-2630, train loss-0.1871, acc-0.7800, valid loss-0.1835, acc-0.7712, test loss-0.1849, acc-0.7618\n",
      "Iter-2640, train loss-0.1820, acc-0.7900, valid loss-0.1836, acc-0.7710, test loss-0.1850, acc-0.7618\n",
      "Iter-2650, train loss-0.1756, acc-0.7800, valid loss-0.1835, acc-0.7706, test loss-0.1850, acc-0.7617\n",
      "Iter-2660, train loss-0.1909, acc-0.7300, valid loss-0.1833, acc-0.7718, test loss-0.1846, acc-0.7617\n",
      "Iter-2670, train loss-0.1923, acc-0.7500, valid loss-0.1831, acc-0.7726, test loss-0.1846, acc-0.7611\n",
      "Iter-2680, train loss-0.1678, acc-0.7900, valid loss-0.1827, acc-0.7738, test loss-0.1841, acc-0.7630\n",
      "Iter-2690, train loss-0.1876, acc-0.7500, valid loss-0.1825, acc-0.7730, test loss-0.1841, acc-0.7627\n",
      "Iter-2700, train loss-0.2122, acc-0.7400, valid loss-0.1826, acc-0.7718, test loss-0.1842, acc-0.7626\n",
      "Iter-2710, train loss-0.2294, acc-0.7000, valid loss-0.1824, acc-0.7728, test loss-0.1840, acc-0.7635\n",
      "Iter-2720, train loss-0.1964, acc-0.7500, valid loss-0.1821, acc-0.7714, test loss-0.1838, acc-0.7627\n",
      "Iter-2730, train loss-0.1609, acc-0.7900, valid loss-0.1821, acc-0.7700, test loss-0.1837, acc-0.7626\n",
      "Iter-2740, train loss-0.1959, acc-0.7600, valid loss-0.1821, acc-0.7724, test loss-0.1835, acc-0.7644\n",
      "Iter-2750, train loss-0.1790, acc-0.8100, valid loss-0.1819, acc-0.7740, test loss-0.1835, acc-0.7642\n",
      "Iter-2760, train loss-0.2172, acc-0.7000, valid loss-0.1820, acc-0.7728, test loss-0.1838, acc-0.7638\n",
      "Iter-2770, train loss-0.1867, acc-0.7500, valid loss-0.1819, acc-0.7726, test loss-0.1837, acc-0.7645\n",
      "Iter-2780, train loss-0.1702, acc-0.8000, valid loss-0.1818, acc-0.7706, test loss-0.1838, acc-0.7640\n",
      "Iter-2790, train loss-0.1838, acc-0.7700, valid loss-0.1815, acc-0.7714, test loss-0.1833, acc-0.7657\n",
      "Iter-2800, train loss-0.2059, acc-0.7100, valid loss-0.1813, acc-0.7720, test loss-0.1832, acc-0.7653\n",
      "Iter-2810, train loss-0.1891, acc-0.7800, valid loss-0.1813, acc-0.7736, test loss-0.1831, acc-0.7674\n",
      "Iter-2820, train loss-0.1805, acc-0.7800, valid loss-0.1813, acc-0.7736, test loss-0.1831, acc-0.7668\n",
      "Iter-2830, train loss-0.1863, acc-0.7500, valid loss-0.1814, acc-0.7744, test loss-0.1831, acc-0.7676\n",
      "Iter-2840, train loss-0.1823, acc-0.7800, valid loss-0.1815, acc-0.7752, test loss-0.1830, acc-0.7685\n",
      "Iter-2850, train loss-0.1942, acc-0.7500, valid loss-0.1813, acc-0.7748, test loss-0.1828, acc-0.7697\n",
      "Iter-2860, train loss-0.1693, acc-0.8200, valid loss-0.1816, acc-0.7742, test loss-0.1829, acc-0.7692\n",
      "Iter-2870, train loss-0.1793, acc-0.7500, valid loss-0.1814, acc-0.7740, test loss-0.1830, acc-0.7684\n",
      "Iter-2880, train loss-0.2065, acc-0.7500, valid loss-0.1811, acc-0.7746, test loss-0.1826, acc-0.7686\n",
      "Iter-2890, train loss-0.1985, acc-0.7000, valid loss-0.1814, acc-0.7754, test loss-0.1829, acc-0.7689\n",
      "Iter-2900, train loss-0.1775, acc-0.7800, valid loss-0.1812, acc-0.7746, test loss-0.1829, acc-0.7684\n",
      "Iter-2910, train loss-0.1604, acc-0.8000, valid loss-0.1810, acc-0.7752, test loss-0.1826, acc-0.7682\n",
      "Iter-2920, train loss-0.1971, acc-0.7500, valid loss-0.1811, acc-0.7758, test loss-0.1829, acc-0.7681\n",
      "Iter-2930, train loss-0.2184, acc-0.7100, valid loss-0.1810, acc-0.7756, test loss-0.1828, acc-0.7680\n",
      "Iter-2940, train loss-0.1589, acc-0.8500, valid loss-0.1811, acc-0.7758, test loss-0.1828, acc-0.7683\n",
      "Iter-2950, train loss-0.1252, acc-0.8400, valid loss-0.1808, acc-0.7766, test loss-0.1823, acc-0.7691\n",
      "Iter-2960, train loss-0.1753, acc-0.8000, valid loss-0.1810, acc-0.7768, test loss-0.1826, acc-0.7686\n",
      "Iter-2970, train loss-0.1824, acc-0.7500, valid loss-0.1809, acc-0.7770, test loss-0.1827, acc-0.7682\n",
      "Iter-2980, train loss-0.1646, acc-0.7900, valid loss-0.1809, acc-0.7752, test loss-0.1828, acc-0.7682\n",
      "Iter-2990, train loss-0.1898, acc-0.7600, valid loss-0.1809, acc-0.7760, test loss-0.1828, acc-0.7684\n",
      "Iter-3000, train loss-0.1967, acc-0.7500, valid loss-0.1808, acc-0.7776, test loss-0.1828, acc-0.7695\n",
      "Iter-3010, train loss-0.1895, acc-0.7500, valid loss-0.1810, acc-0.7758, test loss-0.1830, acc-0.7684\n",
      "Iter-3020, train loss-0.1959, acc-0.7300, valid loss-0.1812, acc-0.7756, test loss-0.1833, acc-0.7682\n",
      "Iter-3030, train loss-0.1626, acc-0.8300, valid loss-0.1815, acc-0.7740, test loss-0.1837, acc-0.7670\n",
      "Iter-3040, train loss-0.1731, acc-0.8100, valid loss-0.1815, acc-0.7738, test loss-0.1836, acc-0.7681\n",
      "Iter-3050, train loss-0.1817, acc-0.8000, valid loss-0.1818, acc-0.7736, test loss-0.1839, acc-0.7678\n",
      "Iter-3060, train loss-0.1907, acc-0.7600, valid loss-0.1817, acc-0.7736, test loss-0.1838, acc-0.7685\n",
      "Iter-3070, train loss-0.1810, acc-0.7800, valid loss-0.1820, acc-0.7728, test loss-0.1841, acc-0.7680\n",
      "Iter-3080, train loss-0.1554, acc-0.8000, valid loss-0.1823, acc-0.7728, test loss-0.1844, acc-0.7683\n",
      "Iter-3090, train loss-0.1788, acc-0.8100, valid loss-0.1824, acc-0.7726, test loss-0.1847, acc-0.7675\n",
      "Iter-3100, train loss-0.2248, acc-0.6600, valid loss-0.1827, acc-0.7726, test loss-0.1851, acc-0.7677\n",
      "Iter-3110, train loss-0.1947, acc-0.7700, valid loss-0.1829, acc-0.7732, test loss-0.1852, acc-0.7670\n",
      "Iter-3120, train loss-0.1787, acc-0.7700, valid loss-0.1829, acc-0.7714, test loss-0.1853, acc-0.7673\n",
      "Iter-3130, train loss-0.2011, acc-0.7700, valid loss-0.1828, acc-0.7706, test loss-0.1853, acc-0.7662\n",
      "Iter-3140, train loss-0.1784, acc-0.7900, valid loss-0.1829, acc-0.7714, test loss-0.1853, acc-0.7671\n",
      "Iter-3150, train loss-0.2240, acc-0.6700, valid loss-0.1828, acc-0.7714, test loss-0.1852, acc-0.7669\n",
      "Iter-3160, train loss-0.1695, acc-0.7900, valid loss-0.1832, acc-0.7724, test loss-0.1856, acc-0.7672\n",
      "Iter-3170, train loss-0.1941, acc-0.7700, valid loss-0.1834, acc-0.7714, test loss-0.1858, acc-0.7667\n",
      "Iter-3180, train loss-0.1663, acc-0.7900, valid loss-0.1833, acc-0.7716, test loss-0.1860, acc-0.7653\n",
      "Iter-3190, train loss-0.1830, acc-0.7900, valid loss-0.1833, acc-0.7710, test loss-0.1861, acc-0.7652\n",
      "Iter-3200, train loss-0.1825, acc-0.7900, valid loss-0.1833, acc-0.7716, test loss-0.1861, acc-0.7653\n",
      "Iter-3210, train loss-0.2141, acc-0.7400, valid loss-0.1833, acc-0.7722, test loss-0.1860, acc-0.7651\n",
      "Iter-3220, train loss-0.1643, acc-0.8000, valid loss-0.1835, acc-0.7712, test loss-0.1864, acc-0.7652\n",
      "Iter-3230, train loss-0.2175, acc-0.6700, valid loss-0.1835, acc-0.7712, test loss-0.1865, acc-0.7650\n",
      "Iter-3240, train loss-0.1709, acc-0.7800, valid loss-0.1834, acc-0.7716, test loss-0.1865, acc-0.7645\n",
      "Iter-3250, train loss-0.1978, acc-0.7700, valid loss-0.1832, acc-0.7704, test loss-0.1864, acc-0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.1775, acc-0.7500, valid loss-0.1833, acc-0.7700, test loss-0.1864, acc-0.7627\n",
      "Iter-3270, train loss-0.1773, acc-0.7900, valid loss-0.1836, acc-0.7706, test loss-0.1868, acc-0.7620\n",
      "Iter-3280, train loss-0.1956, acc-0.7500, valid loss-0.1835, acc-0.7708, test loss-0.1866, acc-0.7625\n",
      "Iter-3290, train loss-0.1886, acc-0.7800, valid loss-0.1836, acc-0.7714, test loss-0.1867, acc-0.7627\n",
      "Iter-3300, train loss-0.2090, acc-0.7000, valid loss-0.1837, acc-0.7724, test loss-0.1869, acc-0.7618\n",
      "Iter-3310, train loss-0.2212, acc-0.7300, valid loss-0.1838, acc-0.7712, test loss-0.1870, acc-0.7619\n",
      "Iter-3320, train loss-0.1773, acc-0.8200, valid loss-0.1840, acc-0.7712, test loss-0.1870, acc-0.7630\n",
      "Iter-3330, train loss-0.1988, acc-0.7500, valid loss-0.1844, acc-0.7714, test loss-0.1875, acc-0.7629\n",
      "Iter-3340, train loss-0.1857, acc-0.7800, valid loss-0.1843, acc-0.7714, test loss-0.1875, acc-0.7616\n",
      "Iter-3350, train loss-0.1941, acc-0.7600, valid loss-0.1846, acc-0.7704, test loss-0.1878, acc-0.7631\n",
      "Iter-3360, train loss-0.1613, acc-0.8600, valid loss-0.1847, acc-0.7706, test loss-0.1878, acc-0.7627\n",
      "Iter-3370, train loss-0.1969, acc-0.7300, valid loss-0.1850, acc-0.7698, test loss-0.1880, acc-0.7624\n",
      "Iter-3380, train loss-0.1763, acc-0.7800, valid loss-0.1852, acc-0.7712, test loss-0.1883, acc-0.7617\n",
      "Iter-3390, train loss-0.1803, acc-0.7900, valid loss-0.1854, acc-0.7694, test loss-0.1884, acc-0.7617\n",
      "Iter-3400, train loss-0.1944, acc-0.7700, valid loss-0.1858, acc-0.7710, test loss-0.1889, acc-0.7615\n",
      "Iter-3410, train loss-0.1932, acc-0.7200, valid loss-0.1862, acc-0.7712, test loss-0.1894, acc-0.7610\n",
      "Iter-3420, train loss-0.1859, acc-0.7800, valid loss-0.1862, acc-0.7718, test loss-0.1895, acc-0.7607\n",
      "Iter-3430, train loss-0.1924, acc-0.7700, valid loss-0.1866, acc-0.7714, test loss-0.1902, acc-0.7599\n",
      "Iter-3440, train loss-0.2220, acc-0.7100, valid loss-0.1871, acc-0.7698, test loss-0.1907, acc-0.7590\n",
      "Iter-3450, train loss-0.2276, acc-0.6700, valid loss-0.1872, acc-0.7708, test loss-0.1908, acc-0.7587\n",
      "Iter-3460, train loss-0.1916, acc-0.7400, valid loss-0.1877, acc-0.7676, test loss-0.1916, acc-0.7571\n",
      "Iter-3470, train loss-0.1801, acc-0.7900, valid loss-0.1881, acc-0.7656, test loss-0.1919, acc-0.7574\n",
      "Iter-3480, train loss-0.1882, acc-0.7600, valid loss-0.1886, acc-0.7640, test loss-0.1924, acc-0.7561\n",
      "Iter-3490, train loss-0.2234, acc-0.7100, valid loss-0.1889, acc-0.7634, test loss-0.1930, acc-0.7562\n",
      "Iter-3500, train loss-0.1929, acc-0.7600, valid loss-0.1888, acc-0.7640, test loss-0.1931, acc-0.7565\n",
      "Iter-3510, train loss-0.1897, acc-0.7500, valid loss-0.1893, acc-0.7654, test loss-0.1934, acc-0.7567\n",
      "Iter-3520, train loss-0.2442, acc-0.6900, valid loss-0.1896, acc-0.7656, test loss-0.1938, acc-0.7571\n",
      "Iter-3530, train loss-0.1766, acc-0.7900, valid loss-0.1903, acc-0.7634, test loss-0.1946, acc-0.7556\n",
      "Iter-3540, train loss-0.1973, acc-0.7300, valid loss-0.1904, acc-0.7646, test loss-0.1948, acc-0.7563\n",
      "Iter-3550, train loss-0.2261, acc-0.6700, valid loss-0.1904, acc-0.7640, test loss-0.1948, acc-0.7557\n",
      "Iter-3560, train loss-0.1809, acc-0.7500, valid loss-0.1909, acc-0.7646, test loss-0.1953, acc-0.7542\n",
      "Iter-3570, train loss-0.2149, acc-0.7300, valid loss-0.1912, acc-0.7652, test loss-0.1955, acc-0.7538\n",
      "Iter-3580, train loss-0.2032, acc-0.7600, valid loss-0.1911, acc-0.7638, test loss-0.1956, acc-0.7531\n",
      "Iter-3590, train loss-0.2012, acc-0.7600, valid loss-0.1917, acc-0.7638, test loss-0.1962, acc-0.7526\n",
      "Iter-3600, train loss-0.2199, acc-0.7100, valid loss-0.1919, acc-0.7644, test loss-0.1966, acc-0.7509\n",
      "Iter-3610, train loss-0.2095, acc-0.7100, valid loss-0.1924, acc-0.7658, test loss-0.1969, acc-0.7524\n",
      "Iter-3620, train loss-0.1656, acc-0.8100, valid loss-0.1929, acc-0.7656, test loss-0.1974, acc-0.7524\n",
      "Iter-3630, train loss-0.2063, acc-0.7300, valid loss-0.1930, acc-0.7668, test loss-0.1977, acc-0.7524\n",
      "Iter-3640, train loss-0.1896, acc-0.7600, valid loss-0.1935, acc-0.7658, test loss-0.1982, acc-0.7527\n",
      "Iter-3650, train loss-0.1854, acc-0.7900, valid loss-0.1936, acc-0.7658, test loss-0.1983, acc-0.7530\n",
      "Iter-3660, train loss-0.1792, acc-0.8100, valid loss-0.1939, acc-0.7646, test loss-0.1988, acc-0.7518\n",
      "Iter-3670, train loss-0.2652, acc-0.6500, valid loss-0.1945, acc-0.7630, test loss-0.1995, acc-0.7511\n",
      "Iter-3680, train loss-0.2035, acc-0.7700, valid loss-0.1949, acc-0.7622, test loss-0.2000, acc-0.7503\n",
      "Iter-3690, train loss-0.2177, acc-0.7300, valid loss-0.1955, acc-0.7622, test loss-0.2005, acc-0.7500\n",
      "Iter-3700, train loss-0.2130, acc-0.7300, valid loss-0.1956, acc-0.7640, test loss-0.2006, acc-0.7506\n",
      "Iter-3710, train loss-0.2233, acc-0.7200, valid loss-0.1959, acc-0.7636, test loss-0.2008, acc-0.7513\n",
      "Iter-3720, train loss-0.2279, acc-0.6700, valid loss-0.1964, acc-0.7634, test loss-0.2013, acc-0.7512\n",
      "Iter-3730, train loss-0.2212, acc-0.6900, valid loss-0.1966, acc-0.7638, test loss-0.2014, acc-0.7512\n",
      "Iter-3740, train loss-0.2146, acc-0.7600, valid loss-0.1970, acc-0.7628, test loss-0.2017, acc-0.7503\n",
      "Iter-3750, train loss-0.2006, acc-0.7600, valid loss-0.1974, acc-0.7624, test loss-0.2023, acc-0.7499\n",
      "Iter-3760, train loss-0.1957, acc-0.7600, valid loss-0.1977, acc-0.7620, test loss-0.2025, acc-0.7492\n",
      "Iter-3770, train loss-0.2258, acc-0.6700, valid loss-0.1979, acc-0.7614, test loss-0.2026, acc-0.7499\n",
      "Iter-3780, train loss-0.2153, acc-0.7500, valid loss-0.1985, acc-0.7602, test loss-0.2034, acc-0.7492\n",
      "Iter-3790, train loss-0.1965, acc-0.7200, valid loss-0.1989, acc-0.7594, test loss-0.2037, acc-0.7487\n",
      "Iter-3800, train loss-0.2364, acc-0.6900, valid loss-0.1998, acc-0.7596, test loss-0.2045, acc-0.7481\n",
      "Iter-3810, train loss-0.2020, acc-0.7700, valid loss-0.2003, acc-0.7586, test loss-0.2049, acc-0.7475\n",
      "Iter-3820, train loss-0.2263, acc-0.7100, valid loss-0.2007, acc-0.7602, test loss-0.2053, acc-0.7469\n",
      "Iter-3830, train loss-0.2419, acc-0.6700, valid loss-0.2018, acc-0.7588, test loss-0.2062, acc-0.7471\n",
      "Iter-3840, train loss-0.1774, acc-0.8200, valid loss-0.2022, acc-0.7596, test loss-0.2066, acc-0.7442\n",
      "Iter-3850, train loss-0.2003, acc-0.7500, valid loss-0.2026, acc-0.7590, test loss-0.2069, acc-0.7452\n",
      "Iter-3860, train loss-0.2064, acc-0.7300, valid loss-0.2029, acc-0.7578, test loss-0.2073, acc-0.7445\n",
      "Iter-3870, train loss-0.2294, acc-0.7300, valid loss-0.2027, acc-0.7590, test loss-0.2071, acc-0.7445\n",
      "Iter-3880, train loss-0.2174, acc-0.7200, valid loss-0.2031, acc-0.7580, test loss-0.2075, acc-0.7451\n",
      "Iter-3890, train loss-0.1890, acc-0.7800, valid loss-0.2036, acc-0.7570, test loss-0.2079, acc-0.7448\n",
      "Iter-3900, train loss-0.2135, acc-0.7300, valid loss-0.2036, acc-0.7564, test loss-0.2079, acc-0.7447\n",
      "Iter-3910, train loss-0.1858, acc-0.7600, valid loss-0.2039, acc-0.7526, test loss-0.2084, acc-0.7440\n",
      "Iter-3920, train loss-0.1703, acc-0.7900, valid loss-0.2047, acc-0.7522, test loss-0.2092, acc-0.7431\n",
      "Iter-3930, train loss-0.2058, acc-0.7600, valid loss-0.2046, acc-0.7526, test loss-0.2092, acc-0.7426\n",
      "Iter-3940, train loss-0.2045, acc-0.7700, valid loss-0.2050, acc-0.7512, test loss-0.2096, acc-0.7418\n",
      "Iter-3950, train loss-0.2204, acc-0.7300, valid loss-0.2055, acc-0.7520, test loss-0.2100, acc-0.7419\n",
      "Iter-3960, train loss-0.2121, acc-0.7500, valid loss-0.2065, acc-0.7536, test loss-0.2108, acc-0.7427\n",
      "Iter-3970, train loss-0.2083, acc-0.7500, valid loss-0.2064, acc-0.7502, test loss-0.2106, acc-0.7415\n",
      "Iter-3980, train loss-0.2230, acc-0.7000, valid loss-0.2069, acc-0.7484, test loss-0.2112, acc-0.7410\n",
      "Iter-3990, train loss-0.2451, acc-0.6400, valid loss-0.2073, acc-0.7488, test loss-0.2114, acc-0.7395\n",
      "Iter-4000, train loss-0.2392, acc-0.6500, valid loss-0.2078, acc-0.7490, test loss-0.2119, acc-0.7403\n",
      "Iter-4010, train loss-0.2237, acc-0.7800, valid loss-0.2084, acc-0.7466, test loss-0.2126, acc-0.7370\n",
      "Iter-4020, train loss-0.2297, acc-0.7100, valid loss-0.2088, acc-0.7452, test loss-0.2130, acc-0.7353\n",
      "Iter-4030, train loss-0.2206, acc-0.6900, valid loss-0.2091, acc-0.7434, test loss-0.2137, acc-0.7341\n",
      "Iter-4040, train loss-0.1694, acc-0.8300, valid loss-0.2105, acc-0.7418, test loss-0.2149, acc-0.7329\n",
      "Iter-4050, train loss-0.2484, acc-0.6600, valid loss-0.2102, acc-0.7442, test loss-0.2147, acc-0.7334\n",
      "Iter-4060, train loss-0.2355, acc-0.7100, valid loss-0.2104, acc-0.7452, test loss-0.2147, acc-0.7351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.2233, acc-0.7400, valid loss-0.2109, acc-0.7438, test loss-0.2155, acc-0.7339\n",
      "Iter-4080, train loss-0.1723, acc-0.8000, valid loss-0.2109, acc-0.7446, test loss-0.2154, acc-0.7354\n",
      "Iter-4090, train loss-0.1706, acc-0.8200, valid loss-0.2110, acc-0.7428, test loss-0.2157, acc-0.7337\n",
      "Iter-4100, train loss-0.1686, acc-0.8300, valid loss-0.2117, acc-0.7410, test loss-0.2165, acc-0.7325\n",
      "Iter-4110, train loss-0.1929, acc-0.7800, valid loss-0.2123, acc-0.7422, test loss-0.2166, acc-0.7334\n",
      "Iter-4120, train loss-0.2652, acc-0.6200, valid loss-0.2125, acc-0.7406, test loss-0.2170, acc-0.7317\n",
      "Iter-4130, train loss-0.2006, acc-0.7200, valid loss-0.2126, acc-0.7398, test loss-0.2173, acc-0.7296\n",
      "Iter-4140, train loss-0.2132, acc-0.7300, valid loss-0.2121, acc-0.7400, test loss-0.2165, acc-0.7305\n",
      "Iter-4150, train loss-0.2299, acc-0.6800, valid loss-0.2122, acc-0.7388, test loss-0.2168, acc-0.7279\n",
      "Iter-4160, train loss-0.2211, acc-0.7300, valid loss-0.2121, acc-0.7390, test loss-0.2166, acc-0.7289\n",
      "Iter-4170, train loss-0.2123, acc-0.7500, valid loss-0.2125, acc-0.7364, test loss-0.2170, acc-0.7260\n",
      "Iter-4180, train loss-0.2362, acc-0.7200, valid loss-0.2126, acc-0.7366, test loss-0.2170, acc-0.7259\n",
      "Iter-4190, train loss-0.2215, acc-0.7000, valid loss-0.2129, acc-0.7372, test loss-0.2174, acc-0.7256\n",
      "Iter-4200, train loss-0.2306, acc-0.6700, valid loss-0.2128, acc-0.7382, test loss-0.2173, acc-0.7269\n",
      "Iter-4210, train loss-0.1999, acc-0.7600, valid loss-0.2129, acc-0.7386, test loss-0.2175, acc-0.7275\n",
      "Iter-4220, train loss-0.1919, acc-0.7600, valid loss-0.2127, acc-0.7390, test loss-0.2172, acc-0.7283\n",
      "Iter-4230, train loss-0.2309, acc-0.7200, valid loss-0.2132, acc-0.7388, test loss-0.2178, acc-0.7258\n",
      "Iter-4240, train loss-0.2719, acc-0.6400, valid loss-0.2132, acc-0.7384, test loss-0.2178, acc-0.7245\n",
      "Iter-4250, train loss-0.1916, acc-0.7800, valid loss-0.2132, acc-0.7362, test loss-0.2177, acc-0.7253\n",
      "Iter-4260, train loss-0.1777, acc-0.7600, valid loss-0.2133, acc-0.7356, test loss-0.2180, acc-0.7231\n",
      "Iter-4270, train loss-0.2421, acc-0.7000, valid loss-0.2136, acc-0.7352, test loss-0.2183, acc-0.7232\n",
      "Iter-4280, train loss-0.2196, acc-0.6500, valid loss-0.2134, acc-0.7346, test loss-0.2181, acc-0.7207\n",
      "Iter-4290, train loss-0.2335, acc-0.6800, valid loss-0.2139, acc-0.7378, test loss-0.2186, acc-0.7238\n",
      "Iter-4300, train loss-0.2119, acc-0.7500, valid loss-0.2136, acc-0.7358, test loss-0.2183, acc-0.7222\n",
      "Iter-4310, train loss-0.2283, acc-0.7100, valid loss-0.2136, acc-0.7374, test loss-0.2182, acc-0.7252\n",
      "Iter-4320, train loss-0.2083, acc-0.7300, valid loss-0.2136, acc-0.7370, test loss-0.2183, acc-0.7241\n",
      "Iter-4330, train loss-0.2052, acc-0.7200, valid loss-0.2141, acc-0.7366, test loss-0.2188, acc-0.7237\n",
      "Iter-4340, train loss-0.2696, acc-0.6000, valid loss-0.2143, acc-0.7360, test loss-0.2190, acc-0.7240\n",
      "Iter-4350, train loss-0.1997, acc-0.7700, valid loss-0.2142, acc-0.7378, test loss-0.2186, acc-0.7261\n",
      "Iter-4360, train loss-0.2328, acc-0.7400, valid loss-0.2143, acc-0.7374, test loss-0.2186, acc-0.7274\n",
      "Iter-4370, train loss-0.2187, acc-0.7300, valid loss-0.2148, acc-0.7400, test loss-0.2193, acc-0.7278\n",
      "Iter-4380, train loss-0.2169, acc-0.7600, valid loss-0.2149, acc-0.7386, test loss-0.2196, acc-0.7274\n",
      "Iter-4390, train loss-0.2043, acc-0.7300, valid loss-0.2150, acc-0.7378, test loss-0.2198, acc-0.7263\n",
      "Iter-4400, train loss-0.1954, acc-0.7400, valid loss-0.2156, acc-0.7382, test loss-0.2202, acc-0.7263\n",
      "Iter-4410, train loss-0.2205, acc-0.7600, valid loss-0.2165, acc-0.7380, test loss-0.2209, acc-0.7258\n",
      "Iter-4420, train loss-0.2576, acc-0.6500, valid loss-0.2166, acc-0.7384, test loss-0.2210, acc-0.7268\n",
      "Iter-4430, train loss-0.2559, acc-0.6400, valid loss-0.2162, acc-0.7368, test loss-0.2205, acc-0.7261\n",
      "Iter-4440, train loss-0.2252, acc-0.7300, valid loss-0.2168, acc-0.7370, test loss-0.2211, acc-0.7263\n",
      "Iter-4450, train loss-0.2327, acc-0.7200, valid loss-0.2170, acc-0.7370, test loss-0.2213, acc-0.7264\n",
      "Iter-4460, train loss-0.2225, acc-0.7100, valid loss-0.2173, acc-0.7358, test loss-0.2216, acc-0.7257\n",
      "Iter-4470, train loss-0.2216, acc-0.7100, valid loss-0.2181, acc-0.7354, test loss-0.2224, acc-0.7248\n",
      "Iter-4480, train loss-0.2426, acc-0.6800, valid loss-0.2188, acc-0.7326, test loss-0.2231, acc-0.7221\n",
      "Iter-4490, train loss-0.2446, acc-0.6800, valid loss-0.2192, acc-0.7334, test loss-0.2235, acc-0.7220\n",
      "Iter-4500, train loss-0.2079, acc-0.7600, valid loss-0.2191, acc-0.7306, test loss-0.2235, acc-0.7196\n",
      "Iter-4510, train loss-0.2153, acc-0.7300, valid loss-0.2192, acc-0.7334, test loss-0.2236, acc-0.7219\n",
      "Iter-4520, train loss-0.2302, acc-0.6800, valid loss-0.2196, acc-0.7340, test loss-0.2240, acc-0.7230\n",
      "Iter-4530, train loss-0.2082, acc-0.7300, valid loss-0.2200, acc-0.7322, test loss-0.2245, acc-0.7220\n",
      "Iter-4540, train loss-0.1990, acc-0.7700, valid loss-0.2202, acc-0.7312, test loss-0.2246, acc-0.7218\n",
      "Iter-4550, train loss-0.2586, acc-0.7000, valid loss-0.2195, acc-0.7354, test loss-0.2238, acc-0.7237\n",
      "Iter-4560, train loss-0.1980, acc-0.7500, valid loss-0.2200, acc-0.7344, test loss-0.2244, acc-0.7242\n",
      "Iter-4570, train loss-0.2278, acc-0.7100, valid loss-0.2203, acc-0.7340, test loss-0.2248, acc-0.7214\n",
      "Iter-4580, train loss-0.2193, acc-0.7100, valid loss-0.2210, acc-0.7328, test loss-0.2255, acc-0.7220\n",
      "Iter-4590, train loss-0.2709, acc-0.6300, valid loss-0.2210, acc-0.7324, test loss-0.2256, acc-0.7214\n",
      "Iter-4600, train loss-0.2585, acc-0.6700, valid loss-0.2212, acc-0.7322, test loss-0.2258, acc-0.7189\n",
      "Iter-4610, train loss-0.2515, acc-0.6700, valid loss-0.2212, acc-0.7324, test loss-0.2259, acc-0.7194\n",
      "Iter-4620, train loss-0.2310, acc-0.6800, valid loss-0.2211, acc-0.7328, test loss-0.2258, acc-0.7204\n",
      "Iter-4630, train loss-0.2280, acc-0.7300, valid loss-0.2211, acc-0.7342, test loss-0.2259, acc-0.7194\n",
      "Iter-4640, train loss-0.2241, acc-0.7200, valid loss-0.2214, acc-0.7350, test loss-0.2262, acc-0.7208\n",
      "Iter-4650, train loss-0.2374, acc-0.7100, valid loss-0.2209, acc-0.7346, test loss-0.2259, acc-0.7202\n",
      "Iter-4660, train loss-0.2349, acc-0.7100, valid loss-0.2209, acc-0.7332, test loss-0.2257, acc-0.7211\n",
      "Iter-4670, train loss-0.2746, acc-0.6300, valid loss-0.2210, acc-0.7340, test loss-0.2258, acc-0.7244\n",
      "Iter-4680, train loss-0.2448, acc-0.6800, valid loss-0.2210, acc-0.7356, test loss-0.2256, acc-0.7250\n",
      "Iter-4690, train loss-0.1991, acc-0.7600, valid loss-0.2209, acc-0.7356, test loss-0.2255, acc-0.7244\n",
      "Iter-4700, train loss-0.2044, acc-0.7600, valid loss-0.2210, acc-0.7370, test loss-0.2255, acc-0.7250\n",
      "Iter-4710, train loss-0.2324, acc-0.7000, valid loss-0.2214, acc-0.7384, test loss-0.2259, acc-0.7269\n",
      "Iter-4720, train loss-0.2195, acc-0.7100, valid loss-0.2217, acc-0.7370, test loss-0.2263, acc-0.7263\n",
      "Iter-4730, train loss-0.2277, acc-0.7700, valid loss-0.2220, acc-0.7380, test loss-0.2264, acc-0.7270\n",
      "Iter-4740, train loss-0.2194, acc-0.7400, valid loss-0.2223, acc-0.7386, test loss-0.2269, acc-0.7273\n",
      "Iter-4750, train loss-0.2672, acc-0.6300, valid loss-0.2226, acc-0.7390, test loss-0.2273, acc-0.7275\n",
      "Iter-4760, train loss-0.2342, acc-0.7100, valid loss-0.2233, acc-0.7380, test loss-0.2281, acc-0.7265\n",
      "Iter-4770, train loss-0.2201, acc-0.7700, valid loss-0.2230, acc-0.7376, test loss-0.2280, acc-0.7245\n",
      "Iter-4780, train loss-0.1988, acc-0.7800, valid loss-0.2231, acc-0.7352, test loss-0.2282, acc-0.7229\n",
      "Iter-4790, train loss-0.2454, acc-0.7300, valid loss-0.2234, acc-0.7338, test loss-0.2284, acc-0.7230\n",
      "Iter-4800, train loss-0.2377, acc-0.6800, valid loss-0.2231, acc-0.7332, test loss-0.2283, acc-0.7188\n",
      "Iter-4810, train loss-0.2517, acc-0.7100, valid loss-0.2235, acc-0.7314, test loss-0.2287, acc-0.7197\n",
      "Iter-4820, train loss-0.2347, acc-0.7500, valid loss-0.2236, acc-0.7316, test loss-0.2289, acc-0.7158\n",
      "Iter-4830, train loss-0.2554, acc-0.7300, valid loss-0.2235, acc-0.7300, test loss-0.2290, acc-0.7152\n",
      "Iter-4840, train loss-0.2182, acc-0.7200, valid loss-0.2239, acc-0.7298, test loss-0.2292, acc-0.7156\n",
      "Iter-4850, train loss-0.2409, acc-0.7200, valid loss-0.2237, acc-0.7306, test loss-0.2290, acc-0.7175\n",
      "Iter-4860, train loss-0.2180, acc-0.7300, valid loss-0.2236, acc-0.7304, test loss-0.2290, acc-0.7171\n",
      "Iter-4870, train loss-0.2097, acc-0.6900, valid loss-0.2235, acc-0.7302, test loss-0.2288, acc-0.7161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.2770, acc-0.6700, valid loss-0.2232, acc-0.7284, test loss-0.2286, acc-0.7166\n",
      "Iter-4890, train loss-0.2470, acc-0.7300, valid loss-0.2234, acc-0.7300, test loss-0.2289, acc-0.7166\n",
      "Iter-4900, train loss-0.2378, acc-0.7200, valid loss-0.2233, acc-0.7300, test loss-0.2286, acc-0.7181\n",
      "Iter-4910, train loss-0.2046, acc-0.7800, valid loss-0.2234, acc-0.7304, test loss-0.2287, acc-0.7178\n",
      "Iter-4920, train loss-0.2328, acc-0.7000, valid loss-0.2235, acc-0.7304, test loss-0.2287, acc-0.7195\n",
      "Iter-4930, train loss-0.2427, acc-0.6800, valid loss-0.2232, acc-0.7290, test loss-0.2284, acc-0.7182\n",
      "Iter-4940, train loss-0.2226, acc-0.7300, valid loss-0.2238, acc-0.7286, test loss-0.2291, acc-0.7154\n",
      "Iter-4950, train loss-0.2333, acc-0.7200, valid loss-0.2236, acc-0.7308, test loss-0.2287, acc-0.7174\n",
      "Iter-4960, train loss-0.2274, acc-0.7400, valid loss-0.2236, acc-0.7296, test loss-0.2288, acc-0.7171\n",
      "Iter-4970, train loss-0.2397, acc-0.6700, valid loss-0.2237, acc-0.7296, test loss-0.2288, acc-0.7174\n",
      "Iter-4980, train loss-0.2461, acc-0.6700, valid loss-0.2241, acc-0.7334, test loss-0.2292, acc-0.7186\n",
      "Iter-4990, train loss-0.2119, acc-0.7600, valid loss-0.2236, acc-0.7300, test loss-0.2287, acc-0.7185\n",
      "Iter-5000, train loss-0.2685, acc-0.6900, valid loss-0.2236, acc-0.7312, test loss-0.2288, acc-0.7186\n",
      "Iter-5010, train loss-0.2168, acc-0.7100, valid loss-0.2239, acc-0.7314, test loss-0.2290, acc-0.7185\n",
      "Iter-5020, train loss-0.2419, acc-0.6600, valid loss-0.2236, acc-0.7306, test loss-0.2289, acc-0.7179\n",
      "Iter-5030, train loss-0.2215, acc-0.7200, valid loss-0.2238, acc-0.7308, test loss-0.2290, acc-0.7176\n",
      "Iter-5040, train loss-0.2041, acc-0.7600, valid loss-0.2244, acc-0.7346, test loss-0.2294, acc-0.7218\n",
      "Iter-5050, train loss-0.2080, acc-0.7600, valid loss-0.2252, acc-0.7334, test loss-0.2303, acc-0.7209\n",
      "Iter-5060, train loss-0.2125, acc-0.7800, valid loss-0.2252, acc-0.7306, test loss-0.2304, acc-0.7178\n",
      "Iter-5070, train loss-0.2315, acc-0.7500, valid loss-0.2248, acc-0.7320, test loss-0.2300, acc-0.7192\n",
      "Iter-5080, train loss-0.2429, acc-0.7000, valid loss-0.2249, acc-0.7332, test loss-0.2301, acc-0.7196\n",
      "Iter-5090, train loss-0.2765, acc-0.6500, valid loss-0.2254, acc-0.7332, test loss-0.2307, acc-0.7192\n",
      "Iter-5100, train loss-0.2310, acc-0.6800, valid loss-0.2251, acc-0.7330, test loss-0.2304, acc-0.7217\n",
      "Iter-5110, train loss-0.2444, acc-0.6700, valid loss-0.2249, acc-0.7346, test loss-0.2302, acc-0.7215\n",
      "Iter-5120, train loss-0.2392, acc-0.6700, valid loss-0.2250, acc-0.7350, test loss-0.2302, acc-0.7232\n",
      "Iter-5130, train loss-0.2565, acc-0.6900, valid loss-0.2250, acc-0.7328, test loss-0.2302, acc-0.7205\n",
      "Iter-5140, train loss-0.2119, acc-0.7400, valid loss-0.2246, acc-0.7330, test loss-0.2299, acc-0.7212\n",
      "Iter-5150, train loss-0.2479, acc-0.6900, valid loss-0.2251, acc-0.7288, test loss-0.2304, acc-0.7172\n",
      "Iter-5160, train loss-0.2173, acc-0.6900, valid loss-0.2260, acc-0.7286, test loss-0.2313, acc-0.7170\n",
      "Iter-5170, train loss-0.2554, acc-0.6900, valid loss-0.2260, acc-0.7282, test loss-0.2312, acc-0.7172\n",
      "Iter-5180, train loss-0.2520, acc-0.6600, valid loss-0.2257, acc-0.7266, test loss-0.2311, acc-0.7156\n",
      "Iter-5190, train loss-0.2354, acc-0.6800, valid loss-0.2262, acc-0.7254, test loss-0.2317, acc-0.7156\n",
      "Iter-5200, train loss-0.2707, acc-0.6800, valid loss-0.2263, acc-0.7280, test loss-0.2318, acc-0.7185\n",
      "Iter-5210, train loss-0.2563, acc-0.6800, valid loss-0.2263, acc-0.7274, test loss-0.2319, acc-0.7166\n",
      "Iter-5220, train loss-0.2040, acc-0.7500, valid loss-0.2264, acc-0.7276, test loss-0.2319, acc-0.7170\n",
      "Iter-5230, train loss-0.2203, acc-0.7500, valid loss-0.2266, acc-0.7258, test loss-0.2322, acc-0.7155\n",
      "Iter-5240, train loss-0.2346, acc-0.7200, valid loss-0.2269, acc-0.7278, test loss-0.2323, acc-0.7176\n",
      "Iter-5250, train loss-0.2262, acc-0.7300, valid loss-0.2278, acc-0.7290, test loss-0.2332, acc-0.7175\n",
      "Iter-5260, train loss-0.2198, acc-0.8100, valid loss-0.2279, acc-0.7278, test loss-0.2334, acc-0.7171\n",
      "Iter-5270, train loss-0.2267, acc-0.6800, valid loss-0.2278, acc-0.7280, test loss-0.2334, acc-0.7168\n",
      "Iter-5280, train loss-0.2231, acc-0.7400, valid loss-0.2276, acc-0.7282, test loss-0.2333, acc-0.7176\n",
      "Iter-5290, train loss-0.2410, acc-0.7500, valid loss-0.2276, acc-0.7280, test loss-0.2334, acc-0.7164\n",
      "Iter-5300, train loss-0.2004, acc-0.7800, valid loss-0.2275, acc-0.7264, test loss-0.2333, acc-0.7158\n",
      "Iter-5310, train loss-0.2290, acc-0.7200, valid loss-0.2275, acc-0.7268, test loss-0.2333, acc-0.7168\n",
      "Iter-5320, train loss-0.2561, acc-0.6400, valid loss-0.2279, acc-0.7276, test loss-0.2337, acc-0.7182\n",
      "Iter-5330, train loss-0.2454, acc-0.7100, valid loss-0.2280, acc-0.7280, test loss-0.2337, acc-0.7185\n",
      "Iter-5340, train loss-0.2184, acc-0.7600, valid loss-0.2281, acc-0.7274, test loss-0.2338, acc-0.7169\n",
      "Iter-5350, train loss-0.2763, acc-0.6100, valid loss-0.2284, acc-0.7296, test loss-0.2340, acc-0.7172\n",
      "Iter-5360, train loss-0.2106, acc-0.7800, valid loss-0.2285, acc-0.7284, test loss-0.2343, acc-0.7165\n",
      "Iter-5370, train loss-0.2333, acc-0.7400, valid loss-0.2290, acc-0.7290, test loss-0.2347, acc-0.7162\n",
      "Iter-5380, train loss-0.2366, acc-0.7100, valid loss-0.2287, acc-0.7294, test loss-0.2345, acc-0.7160\n",
      "Iter-5390, train loss-0.2050, acc-0.7600, valid loss-0.2284, acc-0.7292, test loss-0.2342, acc-0.7154\n",
      "Iter-5400, train loss-0.2134, acc-0.8000, valid loss-0.2287, acc-0.7286, test loss-0.2344, acc-0.7165\n",
      "Iter-5410, train loss-0.2341, acc-0.7400, valid loss-0.2290, acc-0.7268, test loss-0.2347, acc-0.7155\n",
      "Iter-5420, train loss-0.2602, acc-0.6900, valid loss-0.2288, acc-0.7260, test loss-0.2345, acc-0.7158\n",
      "Iter-5430, train loss-0.2735, acc-0.6800, valid loss-0.2293, acc-0.7252, test loss-0.2349, acc-0.7141\n",
      "Iter-5440, train loss-0.2430, acc-0.7200, valid loss-0.2296, acc-0.7256, test loss-0.2352, acc-0.7143\n",
      "Iter-5450, train loss-0.2583, acc-0.6600, valid loss-0.2303, acc-0.7260, test loss-0.2357, acc-0.7150\n",
      "Iter-5460, train loss-0.2616, acc-0.6700, valid loss-0.2301, acc-0.7266, test loss-0.2357, acc-0.7157\n",
      "Iter-5470, train loss-0.2231, acc-0.7800, valid loss-0.2310, acc-0.7256, test loss-0.2366, acc-0.7157\n",
      "Iter-5480, train loss-0.2162, acc-0.7400, valid loss-0.2307, acc-0.7254, test loss-0.2364, acc-0.7153\n",
      "Iter-5490, train loss-0.2163, acc-0.7600, valid loss-0.2311, acc-0.7262, test loss-0.2368, acc-0.7148\n",
      "Iter-5500, train loss-0.2338, acc-0.7300, valid loss-0.2310, acc-0.7264, test loss-0.2368, acc-0.7148\n",
      "Iter-5510, train loss-0.2379, acc-0.7200, valid loss-0.2307, acc-0.7250, test loss-0.2366, acc-0.7146\n",
      "Iter-5520, train loss-0.2538, acc-0.7200, valid loss-0.2310, acc-0.7238, test loss-0.2369, acc-0.7143\n",
      "Iter-5530, train loss-0.2413, acc-0.6800, valid loss-0.2316, acc-0.7232, test loss-0.2377, acc-0.7139\n",
      "Iter-5540, train loss-0.2219, acc-0.7600, valid loss-0.2316, acc-0.7234, test loss-0.2378, acc-0.7135\n",
      "Iter-5550, train loss-0.2105, acc-0.7900, valid loss-0.2318, acc-0.7232, test loss-0.2379, acc-0.7131\n",
      "Iter-5560, train loss-0.2618, acc-0.6800, valid loss-0.2322, acc-0.7218, test loss-0.2380, acc-0.7132\n",
      "Iter-5570, train loss-0.2553, acc-0.7000, valid loss-0.2328, acc-0.7228, test loss-0.2386, acc-0.7123\n",
      "Iter-5580, train loss-0.2612, acc-0.6900, valid loss-0.2329, acc-0.7222, test loss-0.2389, acc-0.7106\n",
      "Iter-5590, train loss-0.2334, acc-0.7400, valid loss-0.2330, acc-0.7246, test loss-0.2389, acc-0.7119\n",
      "Iter-5600, train loss-0.2532, acc-0.6700, valid loss-0.2327, acc-0.7240, test loss-0.2385, acc-0.7134\n",
      "Iter-5610, train loss-0.2509, acc-0.7100, valid loss-0.2330, acc-0.7250, test loss-0.2387, acc-0.7113\n",
      "Iter-5620, train loss-0.2958, acc-0.6100, valid loss-0.2328, acc-0.7244, test loss-0.2386, acc-0.7116\n",
      "Iter-5630, train loss-0.2083, acc-0.7900, valid loss-0.2332, acc-0.7240, test loss-0.2391, acc-0.7099\n",
      "Iter-5640, train loss-0.2544, acc-0.6800, valid loss-0.2337, acc-0.7230, test loss-0.2397, acc-0.7087\n",
      "Iter-5650, train loss-0.2350, acc-0.6900, valid loss-0.2341, acc-0.7224, test loss-0.2401, acc-0.7099\n",
      "Iter-5660, train loss-0.2545, acc-0.7000, valid loss-0.2341, acc-0.7234, test loss-0.2402, acc-0.7099\n",
      "Iter-5670, train loss-0.2241, acc-0.7700, valid loss-0.2344, acc-0.7228, test loss-0.2406, acc-0.7097\n",
      "Iter-5680, train loss-0.2787, acc-0.6900, valid loss-0.2349, acc-0.7216, test loss-0.2410, acc-0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-0.2841, acc-0.6400, valid loss-0.2351, acc-0.7206, test loss-0.2410, acc-0.7073\n",
      "Iter-5700, train loss-0.2345, acc-0.7500, valid loss-0.2349, acc-0.7202, test loss-0.2409, acc-0.7074\n",
      "Iter-5710, train loss-0.2416, acc-0.6900, valid loss-0.2349, acc-0.7194, test loss-0.2408, acc-0.7068\n",
      "Iter-5720, train loss-0.1985, acc-0.7500, valid loss-0.2348, acc-0.7190, test loss-0.2407, acc-0.7063\n",
      "Iter-5730, train loss-0.2540, acc-0.7100, valid loss-0.2350, acc-0.7186, test loss-0.2408, acc-0.7062\n",
      "Iter-5740, train loss-0.2157, acc-0.7600, valid loss-0.2346, acc-0.7180, test loss-0.2404, acc-0.7060\n",
      "Iter-5750, train loss-0.2321, acc-0.7200, valid loss-0.2348, acc-0.7196, test loss-0.2406, acc-0.7065\n",
      "Iter-5760, train loss-0.2847, acc-0.6600, valid loss-0.2346, acc-0.7196, test loss-0.2404, acc-0.7067\n",
      "Iter-5770, train loss-0.2546, acc-0.6700, valid loss-0.2350, acc-0.7190, test loss-0.2407, acc-0.7066\n",
      "Iter-5780, train loss-0.2631, acc-0.6900, valid loss-0.2356, acc-0.7180, test loss-0.2413, acc-0.7059\n",
      "Iter-5790, train loss-0.2454, acc-0.7400, valid loss-0.2359, acc-0.7188, test loss-0.2416, acc-0.7072\n",
      "Iter-5800, train loss-0.2650, acc-0.6500, valid loss-0.2360, acc-0.7194, test loss-0.2419, acc-0.7061\n",
      "Iter-5810, train loss-0.2324, acc-0.7300, valid loss-0.2358, acc-0.7204, test loss-0.2417, acc-0.7058\n",
      "Iter-5820, train loss-0.2352, acc-0.7500, valid loss-0.2359, acc-0.7204, test loss-0.2417, acc-0.7058\n",
      "Iter-5830, train loss-0.2773, acc-0.6200, valid loss-0.2360, acc-0.7190, test loss-0.2417, acc-0.7051\n",
      "Iter-5840, train loss-0.2450, acc-0.7100, valid loss-0.2360, acc-0.7188, test loss-0.2416, acc-0.7059\n",
      "Iter-5850, train loss-0.2470, acc-0.7300, valid loss-0.2364, acc-0.7190, test loss-0.2420, acc-0.7069\n",
      "Iter-5860, train loss-0.2906, acc-0.6300, valid loss-0.2367, acc-0.7198, test loss-0.2425, acc-0.7072\n",
      "Iter-5870, train loss-0.2774, acc-0.5900, valid loss-0.2367, acc-0.7194, test loss-0.2426, acc-0.7068\n",
      "Iter-5880, train loss-0.2487, acc-0.6600, valid loss-0.2367, acc-0.7188, test loss-0.2426, acc-0.7068\n",
      "Iter-5890, train loss-0.2456, acc-0.6900, valid loss-0.2369, acc-0.7180, test loss-0.2427, acc-0.7063\n",
      "Iter-5900, train loss-0.2493, acc-0.6900, valid loss-0.2374, acc-0.7182, test loss-0.2432, acc-0.7054\n",
      "Iter-5910, train loss-0.2245, acc-0.7500, valid loss-0.2380, acc-0.7154, test loss-0.2436, acc-0.7036\n",
      "Iter-5920, train loss-0.2578, acc-0.6400, valid loss-0.2386, acc-0.7162, test loss-0.2442, acc-0.7041\n",
      "Iter-5930, train loss-0.2691, acc-0.6400, valid loss-0.2390, acc-0.7152, test loss-0.2447, acc-0.7037\n",
      "Iter-5940, train loss-0.2427, acc-0.7200, valid loss-0.2391, acc-0.7156, test loss-0.2447, acc-0.7040\n",
      "Iter-5950, train loss-0.3127, acc-0.6100, valid loss-0.2391, acc-0.7156, test loss-0.2446, acc-0.7049\n",
      "Iter-5960, train loss-0.2196, acc-0.7200, valid loss-0.2399, acc-0.7136, test loss-0.2452, acc-0.7046\n",
      "Iter-5970, train loss-0.2622, acc-0.6700, valid loss-0.2398, acc-0.7160, test loss-0.2452, acc-0.7064\n",
      "Iter-5980, train loss-0.2632, acc-0.6800, valid loss-0.2398, acc-0.7162, test loss-0.2450, acc-0.7066\n",
      "Iter-5990, train loss-0.2219, acc-0.7700, valid loss-0.2402, acc-0.7146, test loss-0.2453, acc-0.7061\n",
      "Iter-6000, train loss-0.2558, acc-0.7000, valid loss-0.2399, acc-0.7154, test loss-0.2451, acc-0.7062\n",
      "Iter-6010, train loss-0.3165, acc-0.6300, valid loss-0.2403, acc-0.7138, test loss-0.2454, acc-0.7055\n",
      "Iter-6020, train loss-0.2485, acc-0.6800, valid loss-0.2402, acc-0.7148, test loss-0.2455, acc-0.7065\n",
      "Iter-6030, train loss-0.2612, acc-0.6500, valid loss-0.2403, acc-0.7138, test loss-0.2456, acc-0.7049\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
