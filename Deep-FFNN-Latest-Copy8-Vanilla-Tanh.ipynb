{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # vanilla Backprop\n",
    "#         dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward include in dcross_entropy\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_prob = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.2957 valid loss: 2.2889, valid accuracy: 0.1218\n",
      "Iter-20 train loss: 2.3067 valid loss: 2.2858, valid accuracy: 0.1278\n",
      "Iter-30 train loss: 2.2777 valid loss: 2.2829, valid accuracy: 0.1356\n",
      "Iter-40 train loss: 2.3021 valid loss: 2.2799, valid accuracy: 0.1424\n",
      "Iter-50 train loss: 2.2783 valid loss: 2.2771, valid accuracy: 0.1492\n",
      "Iter-60 train loss: 2.2632 valid loss: 2.2742, valid accuracy: 0.1542\n",
      "Iter-70 train loss: 2.2405 valid loss: 2.2714, valid accuracy: 0.1600\n",
      "Iter-80 train loss: 2.2952 valid loss: 2.2685, valid accuracy: 0.1676\n",
      "Iter-90 train loss: 2.2804 valid loss: 2.2657, valid accuracy: 0.1776\n",
      "Iter-100 train loss: 2.2326 valid loss: 2.2629, valid accuracy: 0.1836\n",
      "Iter-110 train loss: 2.2565 valid loss: 2.2601, valid accuracy: 0.1898\n",
      "Iter-120 train loss: 2.2466 valid loss: 2.2571, valid accuracy: 0.1970\n",
      "Iter-130 train loss: 2.2573 valid loss: 2.2542, valid accuracy: 0.2056\n",
      "Iter-140 train loss: 2.2800 valid loss: 2.2515, valid accuracy: 0.2144\n",
      "Iter-150 train loss: 2.2460 valid loss: 2.2487, valid accuracy: 0.2224\n",
      "Iter-160 train loss: 2.2821 valid loss: 2.2460, valid accuracy: 0.2288\n",
      "Iter-170 train loss: 2.2576 valid loss: 2.2434, valid accuracy: 0.2376\n",
      "Iter-180 train loss: 2.2388 valid loss: 2.2405, valid accuracy: 0.2454\n",
      "Iter-190 train loss: 2.2283 valid loss: 2.2376, valid accuracy: 0.2552\n",
      "Iter-200 train loss: 2.2573 valid loss: 2.2348, valid accuracy: 0.2648\n",
      "Iter-210 train loss: 2.2300 valid loss: 2.2320, valid accuracy: 0.2728\n",
      "Iter-220 train loss: 2.2435 valid loss: 2.2292, valid accuracy: 0.2802\n",
      "Iter-230 train loss: 2.2109 valid loss: 2.2263, valid accuracy: 0.2888\n",
      "Iter-240 train loss: 2.2566 valid loss: 2.2235, valid accuracy: 0.2996\n",
      "Iter-250 train loss: 2.2105 valid loss: 2.2208, valid accuracy: 0.3086\n",
      "Iter-260 train loss: 2.2453 valid loss: 2.2180, valid accuracy: 0.3170\n",
      "Iter-270 train loss: 2.2195 valid loss: 2.2152, valid accuracy: 0.3242\n",
      "Iter-280 train loss: 2.2074 valid loss: 2.2123, valid accuracy: 0.3306\n",
      "Iter-290 train loss: 2.2182 valid loss: 2.2095, valid accuracy: 0.3392\n",
      "Iter-300 train loss: 2.2071 valid loss: 2.2068, valid accuracy: 0.3440\n",
      "Iter-310 train loss: 2.1937 valid loss: 2.2040, valid accuracy: 0.3520\n",
      "Iter-320 train loss: 2.1952 valid loss: 2.2013, valid accuracy: 0.3586\n",
      "Iter-330 train loss: 2.1942 valid loss: 2.1986, valid accuracy: 0.3650\n",
      "Iter-340 train loss: 2.1876 valid loss: 2.1958, valid accuracy: 0.3710\n",
      "Iter-350 train loss: 2.1657 valid loss: 2.1930, valid accuracy: 0.3756\n",
      "Iter-360 train loss: 2.1950 valid loss: 2.1902, valid accuracy: 0.3796\n",
      "Iter-370 train loss: 2.2094 valid loss: 2.1875, valid accuracy: 0.3858\n",
      "Iter-380 train loss: 2.2229 valid loss: 2.1847, valid accuracy: 0.3916\n",
      "Iter-390 train loss: 2.1648 valid loss: 2.1820, valid accuracy: 0.3978\n",
      "Iter-400 train loss: 2.1822 valid loss: 2.1792, valid accuracy: 0.4034\n",
      "Iter-410 train loss: 2.1600 valid loss: 2.1765, valid accuracy: 0.4078\n",
      "Iter-420 train loss: 2.1773 valid loss: 2.1738, valid accuracy: 0.4126\n",
      "Iter-430 train loss: 2.1919 valid loss: 2.1713, valid accuracy: 0.4152\n",
      "Iter-440 train loss: 2.1494 valid loss: 2.1685, valid accuracy: 0.4188\n",
      "Iter-450 train loss: 2.1782 valid loss: 2.1659, valid accuracy: 0.4216\n",
      "Iter-460 train loss: 2.1913 valid loss: 2.1633, valid accuracy: 0.4256\n",
      "Iter-470 train loss: 2.1845 valid loss: 2.1607, valid accuracy: 0.4302\n",
      "Iter-480 train loss: 2.1685 valid loss: 2.1579, valid accuracy: 0.4342\n",
      "Iter-490 train loss: 2.1562 valid loss: 2.1552, valid accuracy: 0.4392\n",
      "Iter-500 train loss: 2.1510 valid loss: 2.1525, valid accuracy: 0.4424\n",
      "Iter-510 train loss: 2.1224 valid loss: 2.1498, valid accuracy: 0.4450\n",
      "Iter-520 train loss: 2.1330 valid loss: 2.1471, valid accuracy: 0.4490\n",
      "Iter-530 train loss: 2.1635 valid loss: 2.1444, valid accuracy: 0.4522\n",
      "Iter-540 train loss: 2.1398 valid loss: 2.1415, valid accuracy: 0.4546\n",
      "Iter-550 train loss: 2.1256 valid loss: 2.1388, valid accuracy: 0.4584\n",
      "Iter-560 train loss: 2.1634 valid loss: 2.1362, valid accuracy: 0.4624\n",
      "Iter-570 train loss: 2.1533 valid loss: 2.1335, valid accuracy: 0.4650\n",
      "Iter-580 train loss: 2.1429 valid loss: 2.1308, valid accuracy: 0.4692\n",
      "Iter-590 train loss: 2.1525 valid loss: 2.1280, valid accuracy: 0.4740\n",
      "Iter-600 train loss: 2.1145 valid loss: 2.1251, valid accuracy: 0.4782\n",
      "Iter-610 train loss: 2.1592 valid loss: 2.1225, valid accuracy: 0.4796\n",
      "Iter-620 train loss: 2.1066 valid loss: 2.1198, valid accuracy: 0.4820\n",
      "Iter-630 train loss: 2.1391 valid loss: 2.1170, valid accuracy: 0.4846\n",
      "Iter-640 train loss: 2.1164 valid loss: 2.1144, valid accuracy: 0.4882\n",
      "Iter-650 train loss: 2.1184 valid loss: 2.1117, valid accuracy: 0.4912\n",
      "Iter-660 train loss: 2.1066 valid loss: 2.1090, valid accuracy: 0.4932\n",
      "Iter-670 train loss: 2.0975 valid loss: 2.1061, valid accuracy: 0.4956\n",
      "Iter-680 train loss: 2.1200 valid loss: 2.1035, valid accuracy: 0.4980\n",
      "Iter-690 train loss: 2.1484 valid loss: 2.1008, valid accuracy: 0.4998\n",
      "Iter-700 train loss: 2.1182 valid loss: 2.0981, valid accuracy: 0.5016\n",
      "Iter-710 train loss: 2.1012 valid loss: 2.0953, valid accuracy: 0.5032\n",
      "Iter-720 train loss: 2.1006 valid loss: 2.0926, valid accuracy: 0.5046\n",
      "Iter-730 train loss: 2.1129 valid loss: 2.0898, valid accuracy: 0.5076\n",
      "Iter-740 train loss: 2.1211 valid loss: 2.0871, valid accuracy: 0.5096\n",
      "Iter-750 train loss: 2.0816 valid loss: 2.0843, valid accuracy: 0.5120\n",
      "Iter-760 train loss: 2.0682 valid loss: 2.0816, valid accuracy: 0.5126\n",
      "Iter-770 train loss: 2.1120 valid loss: 2.0788, valid accuracy: 0.5130\n",
      "Iter-780 train loss: 2.1042 valid loss: 2.0761, valid accuracy: 0.5158\n",
      "Iter-790 train loss: 2.0379 valid loss: 2.0733, valid accuracy: 0.5188\n",
      "Iter-800 train loss: 2.0723 valid loss: 2.0705, valid accuracy: 0.5222\n",
      "Iter-810 train loss: 2.0643 valid loss: 2.0678, valid accuracy: 0.5252\n",
      "Iter-820 train loss: 2.0565 valid loss: 2.0651, valid accuracy: 0.5266\n",
      "Iter-830 train loss: 2.0573 valid loss: 2.0623, valid accuracy: 0.5286\n",
      "Iter-840 train loss: 2.0617 valid loss: 2.0596, valid accuracy: 0.5296\n",
      "Iter-850 train loss: 2.0981 valid loss: 2.0570, valid accuracy: 0.5318\n",
      "Iter-860 train loss: 2.0309 valid loss: 2.0542, valid accuracy: 0.5324\n",
      "Iter-870 train loss: 2.0311 valid loss: 2.0513, valid accuracy: 0.5338\n",
      "Iter-880 train loss: 2.0419 valid loss: 2.0485, valid accuracy: 0.5362\n",
      "Iter-890 train loss: 2.0436 valid loss: 2.0457, valid accuracy: 0.5364\n",
      "Iter-900 train loss: 2.0611 valid loss: 2.0430, valid accuracy: 0.5390\n",
      "Iter-910 train loss: 2.0376 valid loss: 2.0403, valid accuracy: 0.5404\n",
      "Iter-920 train loss: 2.0435 valid loss: 2.0375, valid accuracy: 0.5428\n",
      "Iter-930 train loss: 2.0229 valid loss: 2.0347, valid accuracy: 0.5450\n",
      "Iter-940 train loss: 2.0566 valid loss: 2.0319, valid accuracy: 0.5476\n",
      "Iter-950 train loss: 2.0236 valid loss: 2.0289, valid accuracy: 0.5480\n",
      "Iter-960 train loss: 2.0474 valid loss: 2.0262, valid accuracy: 0.5482\n",
      "Iter-970 train loss: 2.0685 valid loss: 2.0235, valid accuracy: 0.5514\n",
      "Iter-980 train loss: 2.0043 valid loss: 2.0205, valid accuracy: 0.5534\n",
      "Iter-990 train loss: 2.0329 valid loss: 2.0176, valid accuracy: 0.5550\n",
      "Iter-1000 train loss: 2.0012 valid loss: 2.0148, valid accuracy: 0.5560\n",
      "Iter-1010 train loss: 1.9644 valid loss: 2.0120, valid accuracy: 0.5566\n",
      "Iter-1020 train loss: 1.9992 valid loss: 2.0090, valid accuracy: 0.5574\n",
      "Iter-1030 train loss: 2.0229 valid loss: 2.0063, valid accuracy: 0.5600\n",
      "Iter-1040 train loss: 1.9884 valid loss: 2.0034, valid accuracy: 0.5606\n",
      "Iter-1050 train loss: 1.9948 valid loss: 2.0006, valid accuracy: 0.5614\n",
      "Iter-1060 train loss: 2.0156 valid loss: 1.9978, valid accuracy: 0.5626\n",
      "Iter-1070 train loss: 2.0251 valid loss: 1.9950, valid accuracy: 0.5636\n",
      "Iter-1080 train loss: 2.0171 valid loss: 1.9922, valid accuracy: 0.5650\n",
      "Iter-1090 train loss: 1.9929 valid loss: 1.9893, valid accuracy: 0.5664\n",
      "Iter-1100 train loss: 1.9797 valid loss: 1.9865, valid accuracy: 0.5678\n",
      "Iter-1110 train loss: 1.9938 valid loss: 1.9837, valid accuracy: 0.5694\n",
      "Iter-1120 train loss: 1.9215 valid loss: 1.9807, valid accuracy: 0.5708\n",
      "Iter-1130 train loss: 1.9637 valid loss: 1.9778, valid accuracy: 0.5728\n",
      "Iter-1140 train loss: 1.9763 valid loss: 1.9749, valid accuracy: 0.5724\n",
      "Iter-1150 train loss: 1.9717 valid loss: 1.9721, valid accuracy: 0.5732\n",
      "Iter-1160 train loss: 2.0190 valid loss: 1.9692, valid accuracy: 0.5752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 1.9941 valid loss: 1.9664, valid accuracy: 0.5752\n",
      "Iter-1180 train loss: 2.0257 valid loss: 1.9637, valid accuracy: 0.5770\n",
      "Iter-1190 train loss: 1.9478 valid loss: 1.9608, valid accuracy: 0.5776\n",
      "Iter-1200 train loss: 1.9399 valid loss: 1.9580, valid accuracy: 0.5782\n",
      "Iter-1210 train loss: 1.9699 valid loss: 1.9553, valid accuracy: 0.5786\n",
      "Iter-1220 train loss: 1.9354 valid loss: 1.9524, valid accuracy: 0.5788\n",
      "Iter-1230 train loss: 1.9082 valid loss: 1.9495, valid accuracy: 0.5794\n",
      "Iter-1240 train loss: 1.9829 valid loss: 1.9466, valid accuracy: 0.5792\n",
      "Iter-1250 train loss: 1.9499 valid loss: 1.9438, valid accuracy: 0.5802\n",
      "Iter-1260 train loss: 1.9274 valid loss: 1.9409, valid accuracy: 0.5808\n",
      "Iter-1270 train loss: 2.0122 valid loss: 1.9382, valid accuracy: 0.5808\n",
      "Iter-1280 train loss: 1.9460 valid loss: 1.9353, valid accuracy: 0.5822\n",
      "Iter-1290 train loss: 1.8948 valid loss: 1.9324, valid accuracy: 0.5822\n",
      "Iter-1300 train loss: 1.9766 valid loss: 1.9297, valid accuracy: 0.5822\n",
      "Iter-1310 train loss: 1.8747 valid loss: 1.9267, valid accuracy: 0.5830\n",
      "Iter-1320 train loss: 1.9672 valid loss: 1.9239, valid accuracy: 0.5842\n",
      "Iter-1330 train loss: 1.9100 valid loss: 1.9211, valid accuracy: 0.5850\n",
      "Iter-1340 train loss: 1.9610 valid loss: 1.9183, valid accuracy: 0.5860\n",
      "Iter-1350 train loss: 1.8562 valid loss: 1.9153, valid accuracy: 0.5864\n",
      "Iter-1360 train loss: 1.8919 valid loss: 1.9125, valid accuracy: 0.5874\n",
      "Iter-1370 train loss: 1.8624 valid loss: 1.9096, valid accuracy: 0.5866\n",
      "Iter-1380 train loss: 1.9086 valid loss: 1.9067, valid accuracy: 0.5880\n",
      "Iter-1390 train loss: 1.9487 valid loss: 1.9038, valid accuracy: 0.5886\n",
      "Iter-1400 train loss: 1.9171 valid loss: 1.9009, valid accuracy: 0.5882\n",
      "Iter-1410 train loss: 1.9357 valid loss: 1.8981, valid accuracy: 0.5884\n",
      "Iter-1420 train loss: 1.9126 valid loss: 1.8952, valid accuracy: 0.5890\n",
      "Iter-1430 train loss: 1.8771 valid loss: 1.8923, valid accuracy: 0.5898\n",
      "Iter-1440 train loss: 1.9181 valid loss: 1.8895, valid accuracy: 0.5912\n",
      "Iter-1450 train loss: 1.9082 valid loss: 1.8867, valid accuracy: 0.5922\n",
      "Iter-1460 train loss: 1.9225 valid loss: 1.8838, valid accuracy: 0.5936\n",
      "Iter-1470 train loss: 1.8675 valid loss: 1.8809, valid accuracy: 0.5940\n",
      "Iter-1480 train loss: 1.9000 valid loss: 1.8780, valid accuracy: 0.5944\n",
      "Iter-1490 train loss: 1.9267 valid loss: 1.8751, valid accuracy: 0.5958\n",
      "Iter-1500 train loss: 1.9480 valid loss: 1.8722, valid accuracy: 0.5956\n",
      "Iter-1510 train loss: 1.8946 valid loss: 1.8694, valid accuracy: 0.5962\n",
      "Iter-1520 train loss: 1.9117 valid loss: 1.8666, valid accuracy: 0.5962\n",
      "Iter-1530 train loss: 1.8234 valid loss: 1.8637, valid accuracy: 0.5960\n",
      "Iter-1540 train loss: 1.9147 valid loss: 1.8609, valid accuracy: 0.5962\n",
      "Iter-1550 train loss: 1.8650 valid loss: 1.8580, valid accuracy: 0.5962\n",
      "Iter-1560 train loss: 1.8796 valid loss: 1.8552, valid accuracy: 0.5956\n",
      "Iter-1570 train loss: 1.9406 valid loss: 1.8525, valid accuracy: 0.5958\n",
      "Iter-1580 train loss: 1.8917 valid loss: 1.8497, valid accuracy: 0.5974\n",
      "Iter-1590 train loss: 1.8247 valid loss: 1.8468, valid accuracy: 0.5980\n",
      "Iter-1600 train loss: 1.8757 valid loss: 1.8441, valid accuracy: 0.5984\n",
      "Iter-1610 train loss: 1.8500 valid loss: 1.8413, valid accuracy: 0.5994\n",
      "Iter-1620 train loss: 1.8991 valid loss: 1.8385, valid accuracy: 0.5992\n",
      "Iter-1630 train loss: 1.8758 valid loss: 1.8357, valid accuracy: 0.6000\n",
      "Iter-1640 train loss: 1.8565 valid loss: 1.8328, valid accuracy: 0.6000\n",
      "Iter-1650 train loss: 1.8067 valid loss: 1.8299, valid accuracy: 0.6014\n",
      "Iter-1660 train loss: 1.8624 valid loss: 1.8270, valid accuracy: 0.6010\n",
      "Iter-1670 train loss: 1.7960 valid loss: 1.8240, valid accuracy: 0.6006\n",
      "Iter-1680 train loss: 1.8508 valid loss: 1.8212, valid accuracy: 0.6008\n",
      "Iter-1690 train loss: 1.8157 valid loss: 1.8184, valid accuracy: 0.6022\n",
      "Iter-1700 train loss: 1.8446 valid loss: 1.8156, valid accuracy: 0.6026\n",
      "Iter-1710 train loss: 1.7809 valid loss: 1.8128, valid accuracy: 0.6030\n",
      "Iter-1720 train loss: 1.7693 valid loss: 1.8099, valid accuracy: 0.6040\n",
      "Iter-1730 train loss: 1.8404 valid loss: 1.8070, valid accuracy: 0.6048\n",
      "Iter-1740 train loss: 1.7767 valid loss: 1.8044, valid accuracy: 0.6048\n",
      "Iter-1750 train loss: 1.7490 valid loss: 1.8015, valid accuracy: 0.6058\n",
      "Iter-1760 train loss: 1.7846 valid loss: 1.7986, valid accuracy: 0.6066\n",
      "Iter-1770 train loss: 1.8336 valid loss: 1.7957, valid accuracy: 0.6066\n",
      "Iter-1780 train loss: 1.7749 valid loss: 1.7929, valid accuracy: 0.6064\n",
      "Iter-1790 train loss: 1.9234 valid loss: 1.7902, valid accuracy: 0.6084\n",
      "Iter-1800 train loss: 1.7055 valid loss: 1.7872, valid accuracy: 0.6094\n",
      "Iter-1810 train loss: 1.7834 valid loss: 1.7844, valid accuracy: 0.6086\n",
      "Iter-1820 train loss: 1.7994 valid loss: 1.7814, valid accuracy: 0.6100\n",
      "Iter-1830 train loss: 1.7511 valid loss: 1.7786, valid accuracy: 0.6102\n",
      "Iter-1840 train loss: 1.8230 valid loss: 1.7758, valid accuracy: 0.6108\n",
      "Iter-1850 train loss: 1.8549 valid loss: 1.7730, valid accuracy: 0.6108\n",
      "Iter-1860 train loss: 1.8235 valid loss: 1.7703, valid accuracy: 0.6112\n",
      "Iter-1870 train loss: 1.7999 valid loss: 1.7675, valid accuracy: 0.6112\n",
      "Iter-1880 train loss: 1.7559 valid loss: 1.7647, valid accuracy: 0.6130\n",
      "Iter-1890 train loss: 1.8034 valid loss: 1.7619, valid accuracy: 0.6120\n",
      "Iter-1900 train loss: 1.7464 valid loss: 1.7591, valid accuracy: 0.6122\n",
      "Iter-1910 train loss: 1.8199 valid loss: 1.7564, valid accuracy: 0.6134\n",
      "Iter-1920 train loss: 1.7368 valid loss: 1.7536, valid accuracy: 0.6138\n",
      "Iter-1930 train loss: 1.7017 valid loss: 1.7508, valid accuracy: 0.6140\n",
      "Iter-1940 train loss: 1.7464 valid loss: 1.7479, valid accuracy: 0.6138\n",
      "Iter-1950 train loss: 1.7584 valid loss: 1.7450, valid accuracy: 0.6144\n",
      "Iter-1960 train loss: 1.6981 valid loss: 1.7421, valid accuracy: 0.6144\n",
      "Iter-1970 train loss: 1.7691 valid loss: 1.7393, valid accuracy: 0.6152\n",
      "Iter-1980 train loss: 1.6155 valid loss: 1.7365, valid accuracy: 0.6150\n",
      "Iter-1990 train loss: 1.7053 valid loss: 1.7337, valid accuracy: 0.6164\n",
      "Iter-2000 train loss: 1.7802 valid loss: 1.7308, valid accuracy: 0.6160\n",
      "Iter-2010 train loss: 1.7949 valid loss: 1.7281, valid accuracy: 0.6166\n",
      "Iter-2020 train loss: 1.7773 valid loss: 1.7253, valid accuracy: 0.6170\n",
      "Iter-2030 train loss: 1.7306 valid loss: 1.7225, valid accuracy: 0.6170\n",
      "Iter-2040 train loss: 1.7641 valid loss: 1.7197, valid accuracy: 0.6174\n",
      "Iter-2050 train loss: 1.6921 valid loss: 1.7170, valid accuracy: 0.6178\n",
      "Iter-2060 train loss: 1.7505 valid loss: 1.7142, valid accuracy: 0.6180\n",
      "Iter-2070 train loss: 1.7156 valid loss: 1.7114, valid accuracy: 0.6184\n",
      "Iter-2080 train loss: 1.7990 valid loss: 1.7086, valid accuracy: 0.6186\n",
      "Iter-2090 train loss: 1.6625 valid loss: 1.7058, valid accuracy: 0.6180\n",
      "Iter-2100 train loss: 1.6664 valid loss: 1.7030, valid accuracy: 0.6184\n",
      "Iter-2110 train loss: 1.7160 valid loss: 1.7003, valid accuracy: 0.6196\n",
      "Iter-2120 train loss: 1.6848 valid loss: 1.6976, valid accuracy: 0.6202\n",
      "Iter-2130 train loss: 1.7868 valid loss: 1.6948, valid accuracy: 0.6202\n",
      "Iter-2140 train loss: 1.7122 valid loss: 1.6922, valid accuracy: 0.6208\n",
      "Iter-2150 train loss: 1.7105 valid loss: 1.6895, valid accuracy: 0.6216\n",
      "Iter-2160 train loss: 1.7305 valid loss: 1.6867, valid accuracy: 0.6216\n",
      "Iter-2170 train loss: 1.6922 valid loss: 1.6840, valid accuracy: 0.6232\n",
      "Iter-2180 train loss: 1.6921 valid loss: 1.6813, valid accuracy: 0.6232\n",
      "Iter-2190 train loss: 1.6462 valid loss: 1.6786, valid accuracy: 0.6230\n",
      "Iter-2200 train loss: 1.6855 valid loss: 1.6758, valid accuracy: 0.6232\n",
      "Iter-2210 train loss: 1.6722 valid loss: 1.6730, valid accuracy: 0.6230\n",
      "Iter-2220 train loss: 1.7171 valid loss: 1.6704, valid accuracy: 0.6248\n",
      "Iter-2230 train loss: 1.7163 valid loss: 1.6677, valid accuracy: 0.6250\n",
      "Iter-2240 train loss: 1.6617 valid loss: 1.6650, valid accuracy: 0.6246\n",
      "Iter-2250 train loss: 1.7209 valid loss: 1.6622, valid accuracy: 0.6254\n",
      "Iter-2260 train loss: 1.6450 valid loss: 1.6595, valid accuracy: 0.6252\n",
      "Iter-2270 train loss: 1.7124 valid loss: 1.6567, valid accuracy: 0.6256\n",
      "Iter-2280 train loss: 1.6293 valid loss: 1.6542, valid accuracy: 0.6264\n",
      "Iter-2290 train loss: 1.7055 valid loss: 1.6514, valid accuracy: 0.6268\n",
      "Iter-2300 train loss: 1.6309 valid loss: 1.6488, valid accuracy: 0.6270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 1.6960 valid loss: 1.6463, valid accuracy: 0.6274\n",
      "Iter-2320 train loss: 1.6532 valid loss: 1.6437, valid accuracy: 0.6274\n",
      "Iter-2330 train loss: 1.7049 valid loss: 1.6410, valid accuracy: 0.6276\n",
      "Iter-2340 train loss: 1.6249 valid loss: 1.6383, valid accuracy: 0.6274\n",
      "Iter-2350 train loss: 1.6310 valid loss: 1.6357, valid accuracy: 0.6282\n",
      "Iter-2360 train loss: 1.6148 valid loss: 1.6331, valid accuracy: 0.6280\n",
      "Iter-2370 train loss: 1.5903 valid loss: 1.6305, valid accuracy: 0.6284\n",
      "Iter-2380 train loss: 1.6522 valid loss: 1.6279, valid accuracy: 0.6280\n",
      "Iter-2390 train loss: 1.6623 valid loss: 1.6252, valid accuracy: 0.6282\n",
      "Iter-2400 train loss: 1.7074 valid loss: 1.6226, valid accuracy: 0.6280\n",
      "Iter-2410 train loss: 1.6295 valid loss: 1.6199, valid accuracy: 0.6284\n",
      "Iter-2420 train loss: 1.5734 valid loss: 1.6173, valid accuracy: 0.6288\n",
      "Iter-2430 train loss: 1.6861 valid loss: 1.6148, valid accuracy: 0.6286\n",
      "Iter-2440 train loss: 1.6356 valid loss: 1.6121, valid accuracy: 0.6294\n",
      "Iter-2450 train loss: 1.6242 valid loss: 1.6095, valid accuracy: 0.6300\n",
      "Iter-2460 train loss: 1.6194 valid loss: 1.6069, valid accuracy: 0.6306\n",
      "Iter-2470 train loss: 1.4641 valid loss: 1.6043, valid accuracy: 0.6316\n",
      "Iter-2480 train loss: 1.5770 valid loss: 1.6017, valid accuracy: 0.6320\n",
      "Iter-2490 train loss: 1.6202 valid loss: 1.5992, valid accuracy: 0.6324\n",
      "Iter-2500 train loss: 1.6456 valid loss: 1.5967, valid accuracy: 0.6330\n",
      "Iter-2510 train loss: 1.6137 valid loss: 1.5941, valid accuracy: 0.6336\n",
      "Iter-2520 train loss: 1.6094 valid loss: 1.5916, valid accuracy: 0.6338\n",
      "Iter-2530 train loss: 1.5944 valid loss: 1.5890, valid accuracy: 0.6338\n",
      "Iter-2540 train loss: 1.6770 valid loss: 1.5865, valid accuracy: 0.6344\n",
      "Iter-2550 train loss: 1.6668 valid loss: 1.5840, valid accuracy: 0.6346\n",
      "Iter-2560 train loss: 1.5516 valid loss: 1.5815, valid accuracy: 0.6352\n",
      "Iter-2570 train loss: 1.5978 valid loss: 1.5790, valid accuracy: 0.6352\n",
      "Iter-2580 train loss: 1.5920 valid loss: 1.5765, valid accuracy: 0.6356\n",
      "Iter-2590 train loss: 1.5833 valid loss: 1.5739, valid accuracy: 0.6358\n",
      "Iter-2600 train loss: 1.5248 valid loss: 1.5714, valid accuracy: 0.6360\n",
      "Iter-2610 train loss: 1.6022 valid loss: 1.5689, valid accuracy: 0.6362\n",
      "Iter-2620 train loss: 1.5936 valid loss: 1.5664, valid accuracy: 0.6362\n",
      "Iter-2630 train loss: 1.5362 valid loss: 1.5639, valid accuracy: 0.6366\n",
      "Iter-2640 train loss: 1.5173 valid loss: 1.5614, valid accuracy: 0.6368\n",
      "Iter-2650 train loss: 1.5986 valid loss: 1.5588, valid accuracy: 0.6370\n",
      "Iter-2660 train loss: 1.5689 valid loss: 1.5564, valid accuracy: 0.6374\n",
      "Iter-2670 train loss: 1.6452 valid loss: 1.5539, valid accuracy: 0.6376\n",
      "Iter-2680 train loss: 1.5715 valid loss: 1.5515, valid accuracy: 0.6380\n",
      "Iter-2690 train loss: 1.5802 valid loss: 1.5489, valid accuracy: 0.6384\n",
      "Iter-2700 train loss: 1.5370 valid loss: 1.5464, valid accuracy: 0.6382\n",
      "Iter-2710 train loss: 1.5689 valid loss: 1.5440, valid accuracy: 0.6388\n",
      "Iter-2720 train loss: 1.5810 valid loss: 1.5414, valid accuracy: 0.6398\n",
      "Iter-2730 train loss: 1.4884 valid loss: 1.5389, valid accuracy: 0.6396\n",
      "Iter-2740 train loss: 1.5376 valid loss: 1.5364, valid accuracy: 0.6400\n",
      "Iter-2750 train loss: 1.5988 valid loss: 1.5341, valid accuracy: 0.6402\n",
      "Iter-2760 train loss: 1.5801 valid loss: 1.5317, valid accuracy: 0.6408\n",
      "Iter-2770 train loss: 1.4579 valid loss: 1.5292, valid accuracy: 0.6410\n",
      "Iter-2780 train loss: 1.4621 valid loss: 1.5268, valid accuracy: 0.6408\n",
      "Iter-2790 train loss: 1.6512 valid loss: 1.5244, valid accuracy: 0.6420\n",
      "Iter-2800 train loss: 1.5005 valid loss: 1.5219, valid accuracy: 0.6430\n",
      "Iter-2810 train loss: 1.4644 valid loss: 1.5195, valid accuracy: 0.6432\n",
      "Iter-2820 train loss: 1.5904 valid loss: 1.5170, valid accuracy: 0.6452\n",
      "Iter-2830 train loss: 1.5196 valid loss: 1.5147, valid accuracy: 0.6464\n",
      "Iter-2840 train loss: 1.5282 valid loss: 1.5123, valid accuracy: 0.6466\n",
      "Iter-2850 train loss: 1.5515 valid loss: 1.5099, valid accuracy: 0.6474\n",
      "Iter-2860 train loss: 1.5536 valid loss: 1.5077, valid accuracy: 0.6480\n",
      "Iter-2870 train loss: 1.3972 valid loss: 1.5053, valid accuracy: 0.6488\n",
      "Iter-2880 train loss: 1.3878 valid loss: 1.5028, valid accuracy: 0.6484\n",
      "Iter-2890 train loss: 1.5740 valid loss: 1.5006, valid accuracy: 0.6484\n",
      "Iter-2900 train loss: 1.4420 valid loss: 1.4981, valid accuracy: 0.6492\n",
      "Iter-2910 train loss: 1.4626 valid loss: 1.4957, valid accuracy: 0.6502\n",
      "Iter-2920 train loss: 1.5925 valid loss: 1.4934, valid accuracy: 0.6510\n",
      "Iter-2930 train loss: 1.5848 valid loss: 1.4911, valid accuracy: 0.6518\n",
      "Iter-2940 train loss: 1.6564 valid loss: 1.4887, valid accuracy: 0.6522\n",
      "Iter-2950 train loss: 1.4663 valid loss: 1.4863, valid accuracy: 0.6524\n",
      "Iter-2960 train loss: 1.4126 valid loss: 1.4841, valid accuracy: 0.6530\n",
      "Iter-2970 train loss: 1.4808 valid loss: 1.4818, valid accuracy: 0.6538\n",
      "Iter-2980 train loss: 1.5500 valid loss: 1.4795, valid accuracy: 0.6540\n",
      "Iter-2990 train loss: 1.4299 valid loss: 1.4771, valid accuracy: 0.6538\n",
      "Iter-3000 train loss: 1.4396 valid loss: 1.4749, valid accuracy: 0.6544\n",
      "Iter-3010 train loss: 1.4566 valid loss: 1.4726, valid accuracy: 0.6544\n",
      "Iter-3020 train loss: 1.4603 valid loss: 1.4703, valid accuracy: 0.6546\n",
      "Iter-3030 train loss: 1.4684 valid loss: 1.4681, valid accuracy: 0.6556\n",
      "Iter-3040 train loss: 1.4880 valid loss: 1.4658, valid accuracy: 0.6554\n",
      "Iter-3050 train loss: 1.4140 valid loss: 1.4636, valid accuracy: 0.6562\n",
      "Iter-3060 train loss: 1.5219 valid loss: 1.4613, valid accuracy: 0.6560\n",
      "Iter-3070 train loss: 1.5132 valid loss: 1.4590, valid accuracy: 0.6562\n",
      "Iter-3080 train loss: 1.4554 valid loss: 1.4568, valid accuracy: 0.6574\n",
      "Iter-3090 train loss: 1.3743 valid loss: 1.4546, valid accuracy: 0.6574\n",
      "Iter-3100 train loss: 1.4656 valid loss: 1.4524, valid accuracy: 0.6588\n",
      "Iter-3110 train loss: 1.4430 valid loss: 1.4501, valid accuracy: 0.6590\n",
      "Iter-3120 train loss: 1.5103 valid loss: 1.4479, valid accuracy: 0.6592\n",
      "Iter-3130 train loss: 1.4135 valid loss: 1.4457, valid accuracy: 0.6596\n",
      "Iter-3140 train loss: 1.3647 valid loss: 1.4435, valid accuracy: 0.6598\n",
      "Iter-3150 train loss: 1.4744 valid loss: 1.4412, valid accuracy: 0.6602\n",
      "Iter-3160 train loss: 1.5551 valid loss: 1.4390, valid accuracy: 0.6600\n",
      "Iter-3170 train loss: 1.6026 valid loss: 1.4369, valid accuracy: 0.6606\n",
      "Iter-3180 train loss: 1.4859 valid loss: 1.4346, valid accuracy: 0.6608\n",
      "Iter-3190 train loss: 1.4662 valid loss: 1.4324, valid accuracy: 0.6608\n",
      "Iter-3200 train loss: 1.4848 valid loss: 1.4303, valid accuracy: 0.6606\n",
      "Iter-3210 train loss: 1.4739 valid loss: 1.4282, valid accuracy: 0.6608\n",
      "Iter-3220 train loss: 1.3819 valid loss: 1.4259, valid accuracy: 0.6610\n",
      "Iter-3230 train loss: 1.5636 valid loss: 1.4238, valid accuracy: 0.6624\n",
      "Iter-3240 train loss: 1.4097 valid loss: 1.4215, valid accuracy: 0.6622\n",
      "Iter-3250 train loss: 1.4399 valid loss: 1.4193, valid accuracy: 0.6630\n",
      "Iter-3260 train loss: 1.4507 valid loss: 1.4172, valid accuracy: 0.6638\n",
      "Iter-3270 train loss: 1.3115 valid loss: 1.4150, valid accuracy: 0.6642\n",
      "Iter-3280 train loss: 1.3726 valid loss: 1.4129, valid accuracy: 0.6644\n",
      "Iter-3290 train loss: 1.5061 valid loss: 1.4108, valid accuracy: 0.6644\n",
      "Iter-3300 train loss: 1.3791 valid loss: 1.4087, valid accuracy: 0.6648\n",
      "Iter-3310 train loss: 1.4307 valid loss: 1.4066, valid accuracy: 0.6652\n",
      "Iter-3320 train loss: 1.3769 valid loss: 1.4045, valid accuracy: 0.6652\n",
      "Iter-3330 train loss: 1.3247 valid loss: 1.4023, valid accuracy: 0.6660\n",
      "Iter-3340 train loss: 1.4669 valid loss: 1.4003, valid accuracy: 0.6668\n",
      "Iter-3350 train loss: 1.4852 valid loss: 1.3983, valid accuracy: 0.6670\n",
      "Iter-3360 train loss: 1.3568 valid loss: 1.3963, valid accuracy: 0.6676\n",
      "Iter-3370 train loss: 1.4672 valid loss: 1.3943, valid accuracy: 0.6678\n",
      "Iter-3380 train loss: 1.3803 valid loss: 1.3923, valid accuracy: 0.6690\n",
      "Iter-3390 train loss: 1.4048 valid loss: 1.3903, valid accuracy: 0.6708\n",
      "Iter-3400 train loss: 1.3330 valid loss: 1.3883, valid accuracy: 0.6706\n",
      "Iter-3410 train loss: 1.3689 valid loss: 1.3863, valid accuracy: 0.6706\n",
      "Iter-3420 train loss: 1.4158 valid loss: 1.3842, valid accuracy: 0.6718\n",
      "Iter-3430 train loss: 1.4258 valid loss: 1.3822, valid accuracy: 0.6726\n",
      "Iter-3440 train loss: 1.3789 valid loss: 1.3801, valid accuracy: 0.6722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 1.4242 valid loss: 1.3781, valid accuracy: 0.6730\n",
      "Iter-3460 train loss: 1.4321 valid loss: 1.3761, valid accuracy: 0.6728\n",
      "Iter-3470 train loss: 1.3528 valid loss: 1.3741, valid accuracy: 0.6732\n",
      "Iter-3480 train loss: 1.2436 valid loss: 1.3721, valid accuracy: 0.6730\n",
      "Iter-3490 train loss: 1.3748 valid loss: 1.3701, valid accuracy: 0.6732\n",
      "Iter-3500 train loss: 1.4648 valid loss: 1.3681, valid accuracy: 0.6742\n",
      "Iter-3510 train loss: 1.5095 valid loss: 1.3660, valid accuracy: 0.6746\n",
      "Iter-3520 train loss: 1.3750 valid loss: 1.3640, valid accuracy: 0.6756\n",
      "Iter-3530 train loss: 1.3333 valid loss: 1.3620, valid accuracy: 0.6752\n",
      "Iter-3540 train loss: 1.4141 valid loss: 1.3601, valid accuracy: 0.6756\n",
      "Iter-3550 train loss: 1.4074 valid loss: 1.3583, valid accuracy: 0.6756\n",
      "Iter-3560 train loss: 1.3940 valid loss: 1.3564, valid accuracy: 0.6764\n",
      "Iter-3570 train loss: 1.4152 valid loss: 1.3545, valid accuracy: 0.6770\n",
      "Iter-3580 train loss: 1.3476 valid loss: 1.3526, valid accuracy: 0.6778\n",
      "Iter-3590 train loss: 1.3983 valid loss: 1.3506, valid accuracy: 0.6786\n",
      "Iter-3600 train loss: 1.3985 valid loss: 1.3486, valid accuracy: 0.6786\n",
      "Iter-3610 train loss: 1.4285 valid loss: 1.3468, valid accuracy: 0.6796\n",
      "Iter-3620 train loss: 1.3770 valid loss: 1.3448, valid accuracy: 0.6802\n",
      "Iter-3630 train loss: 1.2522 valid loss: 1.3428, valid accuracy: 0.6802\n",
      "Iter-3640 train loss: 1.2229 valid loss: 1.3410, valid accuracy: 0.6806\n",
      "Iter-3650 train loss: 1.4162 valid loss: 1.3392, valid accuracy: 0.6802\n",
      "Iter-3660 train loss: 1.3928 valid loss: 1.3372, valid accuracy: 0.6810\n",
      "Iter-3670 train loss: 1.4274 valid loss: 1.3353, valid accuracy: 0.6820\n",
      "Iter-3680 train loss: 1.3539 valid loss: 1.3334, valid accuracy: 0.6826\n",
      "Iter-3690 train loss: 1.4518 valid loss: 1.3315, valid accuracy: 0.6836\n",
      "Iter-3700 train loss: 1.2772 valid loss: 1.3295, valid accuracy: 0.6838\n",
      "Iter-3710 train loss: 1.3523 valid loss: 1.3276, valid accuracy: 0.6848\n",
      "Iter-3720 train loss: 1.3229 valid loss: 1.3258, valid accuracy: 0.6850\n",
      "Iter-3730 train loss: 1.4399 valid loss: 1.3239, valid accuracy: 0.6854\n",
      "Iter-3740 train loss: 1.4473 valid loss: 1.3220, valid accuracy: 0.6854\n",
      "Iter-3750 train loss: 1.3383 valid loss: 1.3201, valid accuracy: 0.6856\n",
      "Iter-3760 train loss: 1.4281 valid loss: 1.3183, valid accuracy: 0.6860\n",
      "Iter-3770 train loss: 1.2785 valid loss: 1.3166, valid accuracy: 0.6860\n",
      "Iter-3780 train loss: 1.1796 valid loss: 1.3147, valid accuracy: 0.6866\n",
      "Iter-3790 train loss: 1.3061 valid loss: 1.3129, valid accuracy: 0.6876\n",
      "Iter-3800 train loss: 1.4632 valid loss: 1.3109, valid accuracy: 0.6876\n",
      "Iter-3810 train loss: 1.3281 valid loss: 1.3092, valid accuracy: 0.6880\n",
      "Iter-3820 train loss: 1.3934 valid loss: 1.3073, valid accuracy: 0.6892\n",
      "Iter-3830 train loss: 1.3617 valid loss: 1.3055, valid accuracy: 0.6892\n",
      "Iter-3840 train loss: 1.3664 valid loss: 1.3038, valid accuracy: 0.6894\n",
      "Iter-3850 train loss: 1.2812 valid loss: 1.3021, valid accuracy: 0.6894\n",
      "Iter-3860 train loss: 1.3250 valid loss: 1.3003, valid accuracy: 0.6898\n",
      "Iter-3870 train loss: 1.3568 valid loss: 1.2985, valid accuracy: 0.6900\n",
      "Iter-3880 train loss: 1.2755 valid loss: 1.2967, valid accuracy: 0.6900\n",
      "Iter-3890 train loss: 1.3368 valid loss: 1.2949, valid accuracy: 0.6902\n",
      "Iter-3900 train loss: 1.2478 valid loss: 1.2931, valid accuracy: 0.6910\n",
      "Iter-3910 train loss: 1.2206 valid loss: 1.2913, valid accuracy: 0.6918\n",
      "Iter-3920 train loss: 1.2821 valid loss: 1.2896, valid accuracy: 0.6922\n",
      "Iter-3930 train loss: 1.2469 valid loss: 1.2878, valid accuracy: 0.6928\n",
      "Iter-3940 train loss: 1.4226 valid loss: 1.2861, valid accuracy: 0.6926\n",
      "Iter-3950 train loss: 1.2754 valid loss: 1.2844, valid accuracy: 0.6926\n",
      "Iter-3960 train loss: 1.3519 valid loss: 1.2826, valid accuracy: 0.6930\n",
      "Iter-3970 train loss: 1.4459 valid loss: 1.2809, valid accuracy: 0.6932\n",
      "Iter-3980 train loss: 1.3449 valid loss: 1.2792, valid accuracy: 0.6938\n",
      "Iter-3990 train loss: 1.3325 valid loss: 1.2775, valid accuracy: 0.6944\n",
      "Iter-4000 train loss: 1.2882 valid loss: 1.2758, valid accuracy: 0.6948\n",
      "Iter-4010 train loss: 1.2410 valid loss: 1.2741, valid accuracy: 0.6948\n",
      "Iter-4020 train loss: 1.2302 valid loss: 1.2724, valid accuracy: 0.6960\n",
      "Iter-4030 train loss: 1.1443 valid loss: 1.2707, valid accuracy: 0.6958\n",
      "Iter-4040 train loss: 1.3300 valid loss: 1.2689, valid accuracy: 0.6962\n",
      "Iter-4050 train loss: 1.2968 valid loss: 1.2673, valid accuracy: 0.6974\n",
      "Iter-4060 train loss: 1.2553 valid loss: 1.2655, valid accuracy: 0.6972\n",
      "Iter-4070 train loss: 1.2974 valid loss: 1.2638, valid accuracy: 0.6980\n",
      "Iter-4080 train loss: 1.2306 valid loss: 1.2622, valid accuracy: 0.6980\n",
      "Iter-4090 train loss: 1.1423 valid loss: 1.2605, valid accuracy: 0.6982\n",
      "Iter-4100 train loss: 1.2658 valid loss: 1.2588, valid accuracy: 0.6986\n",
      "Iter-4110 train loss: 1.3523 valid loss: 1.2571, valid accuracy: 0.6992\n",
      "Iter-4120 train loss: 1.2896 valid loss: 1.2554, valid accuracy: 0.6994\n",
      "Iter-4130 train loss: 1.4057 valid loss: 1.2538, valid accuracy: 0.6996\n",
      "Iter-4140 train loss: 1.2007 valid loss: 1.2521, valid accuracy: 0.6998\n",
      "Iter-4150 train loss: 1.1423 valid loss: 1.2504, valid accuracy: 0.6996\n",
      "Iter-4160 train loss: 1.2456 valid loss: 1.2487, valid accuracy: 0.7000\n",
      "Iter-4170 train loss: 1.3177 valid loss: 1.2471, valid accuracy: 0.7006\n",
      "Iter-4180 train loss: 1.3075 valid loss: 1.2456, valid accuracy: 0.7008\n",
      "Iter-4190 train loss: 1.2804 valid loss: 1.2440, valid accuracy: 0.7012\n",
      "Iter-4200 train loss: 1.1592 valid loss: 1.2423, valid accuracy: 0.7012\n",
      "Iter-4210 train loss: 1.2408 valid loss: 1.2407, valid accuracy: 0.7004\n",
      "Iter-4220 train loss: 1.3845 valid loss: 1.2391, valid accuracy: 0.7008\n",
      "Iter-4230 train loss: 1.1506 valid loss: 1.2374, valid accuracy: 0.7018\n",
      "Iter-4240 train loss: 1.3274 valid loss: 1.2358, valid accuracy: 0.7026\n",
      "Iter-4250 train loss: 1.3450 valid loss: 1.2342, valid accuracy: 0.7036\n",
      "Iter-4260 train loss: 1.1843 valid loss: 1.2326, valid accuracy: 0.7034\n",
      "Iter-4270 train loss: 1.1738 valid loss: 1.2310, valid accuracy: 0.7042\n",
      "Iter-4280 train loss: 1.1204 valid loss: 1.2294, valid accuracy: 0.7042\n",
      "Iter-4290 train loss: 1.2301 valid loss: 1.2279, valid accuracy: 0.7050\n",
      "Iter-4300 train loss: 1.2354 valid loss: 1.2263, valid accuracy: 0.7056\n",
      "Iter-4310 train loss: 1.1644 valid loss: 1.2246, valid accuracy: 0.7058\n",
      "Iter-4320 train loss: 1.3415 valid loss: 1.2230, valid accuracy: 0.7058\n",
      "Iter-4330 train loss: 1.2249 valid loss: 1.2213, valid accuracy: 0.7062\n",
      "Iter-4340 train loss: 1.2658 valid loss: 1.2197, valid accuracy: 0.7064\n",
      "Iter-4350 train loss: 1.2360 valid loss: 1.2181, valid accuracy: 0.7064\n",
      "Iter-4360 train loss: 1.2946 valid loss: 1.2166, valid accuracy: 0.7070\n",
      "Iter-4370 train loss: 1.0894 valid loss: 1.2149, valid accuracy: 0.7070\n",
      "Iter-4380 train loss: 1.1986 valid loss: 1.2133, valid accuracy: 0.7072\n",
      "Iter-4390 train loss: 1.2224 valid loss: 1.2117, valid accuracy: 0.7082\n",
      "Iter-4400 train loss: 1.2234 valid loss: 1.2102, valid accuracy: 0.7088\n",
      "Iter-4410 train loss: 1.2447 valid loss: 1.2087, valid accuracy: 0.7088\n",
      "Iter-4420 train loss: 1.0679 valid loss: 1.2072, valid accuracy: 0.7092\n",
      "Iter-4430 train loss: 1.2471 valid loss: 1.2057, valid accuracy: 0.7098\n",
      "Iter-4440 train loss: 1.3082 valid loss: 1.2041, valid accuracy: 0.7098\n",
      "Iter-4450 train loss: 1.1764 valid loss: 1.2026, valid accuracy: 0.7108\n",
      "Iter-4460 train loss: 1.3089 valid loss: 1.2011, valid accuracy: 0.7112\n",
      "Iter-4470 train loss: 1.1595 valid loss: 1.1996, valid accuracy: 0.7122\n",
      "Iter-4480 train loss: 1.1164 valid loss: 1.1981, valid accuracy: 0.7130\n",
      "Iter-4490 train loss: 1.1592 valid loss: 1.1966, valid accuracy: 0.7136\n",
      "Iter-4500 train loss: 1.2203 valid loss: 1.1951, valid accuracy: 0.7146\n",
      "Iter-4510 train loss: 1.3022 valid loss: 1.1936, valid accuracy: 0.7156\n",
      "Iter-4520 train loss: 1.1276 valid loss: 1.1921, valid accuracy: 0.7154\n",
      "Iter-4530 train loss: 1.3812 valid loss: 1.1906, valid accuracy: 0.7154\n",
      "Iter-4540 train loss: 1.1329 valid loss: 1.1891, valid accuracy: 0.7158\n",
      "Iter-4550 train loss: 1.1652 valid loss: 1.1876, valid accuracy: 0.7156\n",
      "Iter-4560 train loss: 1.1086 valid loss: 1.1861, valid accuracy: 0.7160\n",
      "Iter-4570 train loss: 1.1310 valid loss: 1.1847, valid accuracy: 0.7170\n",
      "Iter-4580 train loss: 1.1457 valid loss: 1.1831, valid accuracy: 0.7168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 1.2027 valid loss: 1.1817, valid accuracy: 0.7174\n",
      "Iter-4600 train loss: 1.1818 valid loss: 1.1803, valid accuracy: 0.7180\n",
      "Iter-4610 train loss: 1.2222 valid loss: 1.1787, valid accuracy: 0.7180\n",
      "Iter-4620 train loss: 1.2072 valid loss: 1.1773, valid accuracy: 0.7186\n",
      "Iter-4630 train loss: 1.1297 valid loss: 1.1759, valid accuracy: 0.7190\n",
      "Iter-4640 train loss: 1.1183 valid loss: 1.1745, valid accuracy: 0.7196\n",
      "Iter-4650 train loss: 1.1320 valid loss: 1.1731, valid accuracy: 0.7198\n",
      "Iter-4660 train loss: 1.1746 valid loss: 1.1717, valid accuracy: 0.7208\n",
      "Iter-4670 train loss: 1.2234 valid loss: 1.1703, valid accuracy: 0.7210\n",
      "Iter-4680 train loss: 1.1216 valid loss: 1.1689, valid accuracy: 0.7216\n",
      "Iter-4690 train loss: 1.2800 valid loss: 1.1675, valid accuracy: 0.7218\n",
      "Iter-4700 train loss: 1.1717 valid loss: 1.1660, valid accuracy: 0.7214\n",
      "Iter-4710 train loss: 1.2463 valid loss: 1.1646, valid accuracy: 0.7222\n",
      "Iter-4720 train loss: 1.2279 valid loss: 1.1632, valid accuracy: 0.7226\n",
      "Iter-4730 train loss: 1.2836 valid loss: 1.1618, valid accuracy: 0.7232\n",
      "Iter-4740 train loss: 1.1067 valid loss: 1.1604, valid accuracy: 0.7230\n",
      "Iter-4750 train loss: 1.0399 valid loss: 1.1590, valid accuracy: 0.7228\n",
      "Iter-4760 train loss: 1.1905 valid loss: 1.1575, valid accuracy: 0.7238\n",
      "Iter-4770 train loss: 1.2459 valid loss: 1.1561, valid accuracy: 0.7240\n",
      "Iter-4780 train loss: 1.1791 valid loss: 1.1547, valid accuracy: 0.7236\n",
      "Iter-4790 train loss: 1.1543 valid loss: 1.1533, valid accuracy: 0.7230\n",
      "Iter-4800 train loss: 1.2342 valid loss: 1.1519, valid accuracy: 0.7232\n",
      "Iter-4810 train loss: 1.0779 valid loss: 1.1505, valid accuracy: 0.7236\n",
      "Iter-4820 train loss: 1.1226 valid loss: 1.1491, valid accuracy: 0.7244\n",
      "Iter-4830 train loss: 1.0370 valid loss: 1.1478, valid accuracy: 0.7252\n",
      "Iter-4840 train loss: 1.0807 valid loss: 1.1463, valid accuracy: 0.7252\n",
      "Iter-4850 train loss: 1.2285 valid loss: 1.1449, valid accuracy: 0.7258\n",
      "Iter-4860 train loss: 1.0665 valid loss: 1.1435, valid accuracy: 0.7268\n",
      "Iter-4870 train loss: 1.1659 valid loss: 1.1421, valid accuracy: 0.7276\n",
      "Iter-4880 train loss: 1.0731 valid loss: 1.1406, valid accuracy: 0.7280\n",
      "Iter-4890 train loss: 1.1266 valid loss: 1.1393, valid accuracy: 0.7284\n",
      "Iter-4900 train loss: 1.1703 valid loss: 1.1380, valid accuracy: 0.7286\n",
      "Iter-4910 train loss: 1.1920 valid loss: 1.1366, valid accuracy: 0.7294\n",
      "Iter-4920 train loss: 1.1632 valid loss: 1.1352, valid accuracy: 0.7294\n",
      "Iter-4930 train loss: 1.1368 valid loss: 1.1339, valid accuracy: 0.7300\n",
      "Iter-4940 train loss: 1.1041 valid loss: 1.1326, valid accuracy: 0.7306\n",
      "Iter-4950 train loss: 1.3145 valid loss: 1.1313, valid accuracy: 0.7312\n",
      "Iter-4960 train loss: 1.1100 valid loss: 1.1300, valid accuracy: 0.7318\n",
      "Iter-4970 train loss: 1.1621 valid loss: 1.1286, valid accuracy: 0.7320\n",
      "Iter-4980 train loss: 1.1419 valid loss: 1.1272, valid accuracy: 0.7332\n",
      "Iter-4990 train loss: 1.0219 valid loss: 1.1259, valid accuracy: 0.7332\n",
      "Iter-5000 train loss: 1.0168 valid loss: 1.1245, valid accuracy: 0.7340\n",
      "Iter-5010 train loss: 1.2411 valid loss: 1.1232, valid accuracy: 0.7342\n",
      "Iter-5020 train loss: 1.2042 valid loss: 1.1218, valid accuracy: 0.7352\n",
      "Iter-5030 train loss: 1.2437 valid loss: 1.1205, valid accuracy: 0.7348\n",
      "Iter-5040 train loss: 1.0526 valid loss: 1.1192, valid accuracy: 0.7350\n",
      "Iter-5050 train loss: 1.1704 valid loss: 1.1179, valid accuracy: 0.7352\n",
      "Iter-5060 train loss: 1.0768 valid loss: 1.1166, valid accuracy: 0.7352\n",
      "Iter-5070 train loss: 1.1004 valid loss: 1.1153, valid accuracy: 0.7358\n",
      "Iter-5080 train loss: 1.2413 valid loss: 1.1139, valid accuracy: 0.7364\n",
      "Iter-5090 train loss: 1.2577 valid loss: 1.1127, valid accuracy: 0.7368\n",
      "Iter-5100 train loss: 1.0858 valid loss: 1.1114, valid accuracy: 0.7378\n",
      "Iter-5110 train loss: 1.1158 valid loss: 1.1100, valid accuracy: 0.7374\n",
      "Iter-5120 train loss: 1.1665 valid loss: 1.1087, valid accuracy: 0.7384\n",
      "Iter-5130 train loss: 1.1586 valid loss: 1.1075, valid accuracy: 0.7384\n",
      "Iter-5140 train loss: 1.0514 valid loss: 1.1062, valid accuracy: 0.7384\n",
      "Iter-5150 train loss: 0.9944 valid loss: 1.1049, valid accuracy: 0.7386\n",
      "Iter-5160 train loss: 1.0719 valid loss: 1.1036, valid accuracy: 0.7390\n",
      "Iter-5170 train loss: 1.0525 valid loss: 1.1023, valid accuracy: 0.7394\n",
      "Iter-5180 train loss: 0.9515 valid loss: 1.1010, valid accuracy: 0.7400\n",
      "Iter-5190 train loss: 1.1258 valid loss: 1.0998, valid accuracy: 0.7400\n",
      "Iter-5200 train loss: 1.0719 valid loss: 1.0985, valid accuracy: 0.7404\n",
      "Iter-5210 train loss: 1.0134 valid loss: 1.0973, valid accuracy: 0.7402\n",
      "Iter-5220 train loss: 1.1549 valid loss: 1.0961, valid accuracy: 0.7408\n",
      "Iter-5230 train loss: 1.1074 valid loss: 1.0948, valid accuracy: 0.7410\n",
      "Iter-5240 train loss: 1.1331 valid loss: 1.0935, valid accuracy: 0.7410\n",
      "Iter-5250 train loss: 0.9414 valid loss: 1.0923, valid accuracy: 0.7408\n",
      "Iter-5260 train loss: 1.0915 valid loss: 1.0911, valid accuracy: 0.7408\n",
      "Iter-5270 train loss: 1.1093 valid loss: 1.0898, valid accuracy: 0.7414\n",
      "Iter-5280 train loss: 0.9793 valid loss: 1.0885, valid accuracy: 0.7418\n",
      "Iter-5290 train loss: 1.0756 valid loss: 1.0873, valid accuracy: 0.7420\n",
      "Iter-5300 train loss: 1.2061 valid loss: 1.0861, valid accuracy: 0.7420\n",
      "Iter-5310 train loss: 1.2153 valid loss: 1.0849, valid accuracy: 0.7422\n",
      "Iter-5320 train loss: 1.0375 valid loss: 1.0836, valid accuracy: 0.7430\n",
      "Iter-5330 train loss: 1.1159 valid loss: 1.0824, valid accuracy: 0.7436\n",
      "Iter-5340 train loss: 1.1142 valid loss: 1.0812, valid accuracy: 0.7446\n",
      "Iter-5350 train loss: 1.2494 valid loss: 1.0800, valid accuracy: 0.7446\n",
      "Iter-5360 train loss: 1.0418 valid loss: 1.0787, valid accuracy: 0.7446\n",
      "Iter-5370 train loss: 1.0359 valid loss: 1.0775, valid accuracy: 0.7446\n",
      "Iter-5380 train loss: 0.9781 valid loss: 1.0763, valid accuracy: 0.7440\n",
      "Iter-5390 train loss: 1.1516 valid loss: 1.0751, valid accuracy: 0.7442\n",
      "Iter-5400 train loss: 1.1019 valid loss: 1.0740, valid accuracy: 0.7444\n",
      "Iter-5410 train loss: 1.3024 valid loss: 1.0728, valid accuracy: 0.7446\n",
      "Iter-5420 train loss: 1.1028 valid loss: 1.0715, valid accuracy: 0.7444\n",
      "Iter-5430 train loss: 1.0653 valid loss: 1.0703, valid accuracy: 0.7448\n",
      "Iter-5440 train loss: 1.2638 valid loss: 1.0691, valid accuracy: 0.7456\n",
      "Iter-5450 train loss: 0.9769 valid loss: 1.0679, valid accuracy: 0.7456\n",
      "Iter-5460 train loss: 1.0669 valid loss: 1.0667, valid accuracy: 0.7466\n",
      "Iter-5470 train loss: 0.8985 valid loss: 1.0655, valid accuracy: 0.7474\n",
      "Iter-5480 train loss: 1.0271 valid loss: 1.0644, valid accuracy: 0.7468\n",
      "Iter-5490 train loss: 1.1691 valid loss: 1.0632, valid accuracy: 0.7466\n",
      "Iter-5500 train loss: 1.0296 valid loss: 1.0620, valid accuracy: 0.7464\n",
      "Iter-5510 train loss: 1.0158 valid loss: 1.0608, valid accuracy: 0.7472\n",
      "Iter-5520 train loss: 1.1153 valid loss: 1.0596, valid accuracy: 0.7476\n",
      "Iter-5530 train loss: 1.1285 valid loss: 1.0584, valid accuracy: 0.7484\n",
      "Iter-5540 train loss: 1.0768 valid loss: 1.0572, valid accuracy: 0.7490\n",
      "Iter-5550 train loss: 1.2312 valid loss: 1.0560, valid accuracy: 0.7496\n",
      "Iter-5560 train loss: 1.0910 valid loss: 1.0548, valid accuracy: 0.7498\n",
      "Iter-5570 train loss: 0.9996 valid loss: 1.0536, valid accuracy: 0.7504\n",
      "Iter-5580 train loss: 1.0269 valid loss: 1.0524, valid accuracy: 0.7506\n",
      "Iter-5590 train loss: 1.1090 valid loss: 1.0512, valid accuracy: 0.7510\n",
      "Iter-5600 train loss: 1.1462 valid loss: 1.0501, valid accuracy: 0.7520\n",
      "Iter-5610 train loss: 1.1411 valid loss: 1.0489, valid accuracy: 0.7518\n",
      "Iter-5620 train loss: 1.0247 valid loss: 1.0477, valid accuracy: 0.7520\n",
      "Iter-5630 train loss: 0.9609 valid loss: 1.0465, valid accuracy: 0.7518\n",
      "Iter-5640 train loss: 1.1207 valid loss: 1.0453, valid accuracy: 0.7520\n",
      "Iter-5650 train loss: 1.0866 valid loss: 1.0441, valid accuracy: 0.7524\n",
      "Iter-5660 train loss: 1.1507 valid loss: 1.0429, valid accuracy: 0.7526\n",
      "Iter-5670 train loss: 0.9759 valid loss: 1.0418, valid accuracy: 0.7526\n",
      "Iter-5680 train loss: 1.0264 valid loss: 1.0406, valid accuracy: 0.7532\n",
      "Iter-5690 train loss: 1.0736 valid loss: 1.0394, valid accuracy: 0.7542\n",
      "Iter-5700 train loss: 0.9179 valid loss: 1.0383, valid accuracy: 0.7546\n",
      "Iter-5710 train loss: 0.9497 valid loss: 1.0372, valid accuracy: 0.7556\n",
      "Iter-5720 train loss: 1.2048 valid loss: 1.0360, valid accuracy: 0.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 1.2062 valid loss: 1.0349, valid accuracy: 0.7570\n",
      "Iter-5740 train loss: 0.9522 valid loss: 1.0337, valid accuracy: 0.7574\n",
      "Iter-5750 train loss: 1.0707 valid loss: 1.0327, valid accuracy: 0.7578\n",
      "Iter-5760 train loss: 1.0131 valid loss: 1.0316, valid accuracy: 0.7580\n",
      "Iter-5770 train loss: 0.8913 valid loss: 1.0305, valid accuracy: 0.7590\n",
      "Iter-5780 train loss: 0.9841 valid loss: 1.0293, valid accuracy: 0.7596\n",
      "Iter-5790 train loss: 1.0314 valid loss: 1.0282, valid accuracy: 0.7600\n",
      "Iter-5800 train loss: 0.9429 valid loss: 1.0270, valid accuracy: 0.7602\n",
      "Iter-5810 train loss: 1.0353 valid loss: 1.0258, valid accuracy: 0.7606\n",
      "Iter-5820 train loss: 0.9655 valid loss: 1.0247, valid accuracy: 0.7604\n",
      "Iter-5830 train loss: 1.0411 valid loss: 1.0236, valid accuracy: 0.7616\n",
      "Iter-5840 train loss: 1.0041 valid loss: 1.0225, valid accuracy: 0.7616\n",
      "Iter-5850 train loss: 0.9357 valid loss: 1.0214, valid accuracy: 0.7620\n",
      "Iter-5860 train loss: 1.1138 valid loss: 1.0202, valid accuracy: 0.7624\n",
      "Iter-5870 train loss: 1.0708 valid loss: 1.0191, valid accuracy: 0.7622\n",
      "Iter-5880 train loss: 1.0676 valid loss: 1.0180, valid accuracy: 0.7624\n",
      "Iter-5890 train loss: 1.1276 valid loss: 1.0169, valid accuracy: 0.7626\n",
      "Iter-5900 train loss: 1.1425 valid loss: 1.0158, valid accuracy: 0.7636\n",
      "Iter-5910 train loss: 0.9866 valid loss: 1.0147, valid accuracy: 0.7632\n",
      "Iter-5920 train loss: 0.9956 valid loss: 1.0136, valid accuracy: 0.7636\n",
      "Iter-5930 train loss: 1.0533 valid loss: 1.0125, valid accuracy: 0.7636\n",
      "Iter-5940 train loss: 1.0116 valid loss: 1.0114, valid accuracy: 0.7646\n",
      "Iter-5950 train loss: 1.1229 valid loss: 1.0104, valid accuracy: 0.7648\n",
      "Iter-5960 train loss: 0.9227 valid loss: 1.0093, valid accuracy: 0.7648\n",
      "Iter-5970 train loss: 1.0685 valid loss: 1.0082, valid accuracy: 0.7654\n",
      "Iter-5980 train loss: 1.0470 valid loss: 1.0072, valid accuracy: 0.7652\n",
      "Iter-5990 train loss: 1.0993 valid loss: 1.0061, valid accuracy: 0.7656\n",
      "Iter-6000 train loss: 1.0357 valid loss: 1.0050, valid accuracy: 0.7660\n",
      "Iter-6010 train loss: 1.0677 valid loss: 1.0039, valid accuracy: 0.7664\n",
      "Iter-6020 train loss: 0.8947 valid loss: 1.0028, valid accuracy: 0.7664\n",
      "Iter-6030 train loss: 0.9563 valid loss: 1.0018, valid accuracy: 0.7672\n",
      "Iter-6040 train loss: 0.9861 valid loss: 1.0007, valid accuracy: 0.7676\n",
      "Iter-6050 train loss: 1.1328 valid loss: 0.9996, valid accuracy: 0.7680\n",
      "Iter-6060 train loss: 1.0600 valid loss: 0.9985, valid accuracy: 0.7678\n",
      "Iter-6070 train loss: 0.8919 valid loss: 0.9975, valid accuracy: 0.7682\n",
      "Iter-6080 train loss: 1.0292 valid loss: 0.9965, valid accuracy: 0.7682\n",
      "Iter-6090 train loss: 1.1694 valid loss: 0.9954, valid accuracy: 0.7686\n",
      "Iter-6100 train loss: 1.0397 valid loss: 0.9944, valid accuracy: 0.7690\n",
      "Iter-6110 train loss: 1.1775 valid loss: 0.9933, valid accuracy: 0.7684\n",
      "Iter-6120 train loss: 1.1499 valid loss: 0.9923, valid accuracy: 0.7678\n",
      "Iter-6130 train loss: 1.0161 valid loss: 0.9912, valid accuracy: 0.7682\n",
      "Iter-6140 train loss: 1.0399 valid loss: 0.9902, valid accuracy: 0.7690\n",
      "Iter-6150 train loss: 0.9186 valid loss: 0.9892, valid accuracy: 0.7694\n",
      "Iter-6160 train loss: 0.8959 valid loss: 0.9881, valid accuracy: 0.7694\n",
      "Iter-6170 train loss: 0.8751 valid loss: 0.9871, valid accuracy: 0.7696\n",
      "Iter-6180 train loss: 0.9825 valid loss: 0.9861, valid accuracy: 0.7696\n",
      "Iter-6190 train loss: 0.9958 valid loss: 0.9851, valid accuracy: 0.7700\n",
      "Iter-6200 train loss: 1.0030 valid loss: 0.9842, valid accuracy: 0.7708\n",
      "Iter-6210 train loss: 1.0678 valid loss: 0.9832, valid accuracy: 0.7714\n",
      "Iter-6220 train loss: 1.1488 valid loss: 0.9821, valid accuracy: 0.7718\n",
      "Iter-6230 train loss: 1.0192 valid loss: 0.9811, valid accuracy: 0.7724\n",
      "Iter-6240 train loss: 0.9158 valid loss: 0.9801, valid accuracy: 0.7724\n",
      "Iter-6250 train loss: 1.0927 valid loss: 0.9791, valid accuracy: 0.7726\n",
      "Iter-6260 train loss: 0.9120 valid loss: 0.9780, valid accuracy: 0.7728\n",
      "Iter-6270 train loss: 1.1004 valid loss: 0.9770, valid accuracy: 0.7728\n",
      "Iter-6280 train loss: 0.8607 valid loss: 0.9760, valid accuracy: 0.7730\n",
      "Iter-6290 train loss: 1.1096 valid loss: 0.9751, valid accuracy: 0.7730\n",
      "Iter-6300 train loss: 0.9352 valid loss: 0.9740, valid accuracy: 0.7730\n",
      "Iter-6310 train loss: 0.9424 valid loss: 0.9730, valid accuracy: 0.7732\n",
      "Iter-6320 train loss: 1.1028 valid loss: 0.9720, valid accuracy: 0.7736\n",
      "Iter-6330 train loss: 0.8516 valid loss: 0.9710, valid accuracy: 0.7740\n",
      "Iter-6340 train loss: 0.9747 valid loss: 0.9700, valid accuracy: 0.7754\n",
      "Iter-6350 train loss: 0.9167 valid loss: 0.9690, valid accuracy: 0.7758\n",
      "Iter-6360 train loss: 1.0303 valid loss: 0.9680, valid accuracy: 0.7758\n",
      "Iter-6370 train loss: 0.9144 valid loss: 0.9669, valid accuracy: 0.7758\n",
      "Iter-6380 train loss: 1.0481 valid loss: 0.9659, valid accuracy: 0.7766\n",
      "Iter-6390 train loss: 1.0887 valid loss: 0.9649, valid accuracy: 0.7772\n",
      "Iter-6400 train loss: 1.0381 valid loss: 0.9639, valid accuracy: 0.7778\n",
      "Iter-6410 train loss: 0.9380 valid loss: 0.9629, valid accuracy: 0.7784\n",
      "Iter-6420 train loss: 1.0548 valid loss: 0.9620, valid accuracy: 0.7788\n",
      "Iter-6430 train loss: 0.8705 valid loss: 0.9609, valid accuracy: 0.7790\n",
      "Iter-6440 train loss: 0.9172 valid loss: 0.9599, valid accuracy: 0.7806\n",
      "Iter-6450 train loss: 0.9970 valid loss: 0.9589, valid accuracy: 0.7806\n",
      "Iter-6460 train loss: 0.9616 valid loss: 0.9580, valid accuracy: 0.7810\n",
      "Iter-6470 train loss: 1.0884 valid loss: 0.9570, valid accuracy: 0.7808\n",
      "Iter-6480 train loss: 0.8538 valid loss: 0.9559, valid accuracy: 0.7816\n",
      "Iter-6490 train loss: 1.0342 valid loss: 0.9549, valid accuracy: 0.7818\n",
      "Iter-6500 train loss: 1.0201 valid loss: 0.9539, valid accuracy: 0.7820\n",
      "Iter-6510 train loss: 1.1298 valid loss: 0.9529, valid accuracy: 0.7822\n",
      "Iter-6520 train loss: 0.9648 valid loss: 0.9519, valid accuracy: 0.7822\n",
      "Iter-6530 train loss: 1.0043 valid loss: 0.9509, valid accuracy: 0.7824\n",
      "Iter-6540 train loss: 1.2021 valid loss: 0.9499, valid accuracy: 0.7830\n",
      "Iter-6550 train loss: 1.0469 valid loss: 0.9489, valid accuracy: 0.7832\n",
      "Iter-6560 train loss: 1.1214 valid loss: 0.9480, valid accuracy: 0.7836\n",
      "Iter-6570 train loss: 0.8222 valid loss: 0.9470, valid accuracy: 0.7836\n",
      "Iter-6580 train loss: 0.9665 valid loss: 0.9461, valid accuracy: 0.7838\n",
      "Iter-6590 train loss: 0.8734 valid loss: 0.9451, valid accuracy: 0.7852\n",
      "Iter-6600 train loss: 0.8731 valid loss: 0.9441, valid accuracy: 0.7850\n",
      "Iter-6610 train loss: 0.9950 valid loss: 0.9432, valid accuracy: 0.7848\n",
      "Iter-6620 train loss: 0.9791 valid loss: 0.9422, valid accuracy: 0.7848\n",
      "Iter-6630 train loss: 0.9011 valid loss: 0.9413, valid accuracy: 0.7848\n",
      "Iter-6640 train loss: 0.9881 valid loss: 0.9403, valid accuracy: 0.7852\n",
      "Iter-6650 train loss: 0.9400 valid loss: 0.9393, valid accuracy: 0.7856\n",
      "Iter-6660 train loss: 0.8756 valid loss: 0.9383, valid accuracy: 0.7858\n",
      "Iter-6670 train loss: 1.0970 valid loss: 0.9373, valid accuracy: 0.7864\n",
      "Iter-6680 train loss: 1.0041 valid loss: 0.9364, valid accuracy: 0.7862\n",
      "Iter-6690 train loss: 0.7916 valid loss: 0.9354, valid accuracy: 0.7864\n",
      "Iter-6700 train loss: 0.9199 valid loss: 0.9345, valid accuracy: 0.7860\n",
      "Iter-6710 train loss: 1.0393 valid loss: 0.9335, valid accuracy: 0.7868\n",
      "Iter-6720 train loss: 0.7697 valid loss: 0.9326, valid accuracy: 0.7876\n",
      "Iter-6730 train loss: 0.9213 valid loss: 0.9316, valid accuracy: 0.7876\n",
      "Iter-6740 train loss: 0.8912 valid loss: 0.9306, valid accuracy: 0.7878\n",
      "Iter-6750 train loss: 1.0010 valid loss: 0.9297, valid accuracy: 0.7880\n",
      "Iter-6760 train loss: 1.0184 valid loss: 0.9287, valid accuracy: 0.7890\n",
      "Iter-6770 train loss: 0.8894 valid loss: 0.9278, valid accuracy: 0.7894\n",
      "Iter-6780 train loss: 1.0544 valid loss: 0.9269, valid accuracy: 0.7896\n",
      "Iter-6790 train loss: 0.8640 valid loss: 0.9260, valid accuracy: 0.7898\n",
      "Iter-6800 train loss: 0.9204 valid loss: 0.9250, valid accuracy: 0.7902\n",
      "Iter-6810 train loss: 1.0274 valid loss: 0.9240, valid accuracy: 0.7902\n",
      "Iter-6820 train loss: 0.8868 valid loss: 0.9231, valid accuracy: 0.7900\n",
      "Iter-6830 train loss: 0.9534 valid loss: 0.9221, valid accuracy: 0.7904\n",
      "Iter-6840 train loss: 1.0389 valid loss: 0.9213, valid accuracy: 0.7904\n",
      "Iter-6850 train loss: 0.9644 valid loss: 0.9203, valid accuracy: 0.7908\n",
      "Iter-6860 train loss: 0.8759 valid loss: 0.9194, valid accuracy: 0.7914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 0.8697 valid loss: 0.9185, valid accuracy: 0.7916\n",
      "Iter-6880 train loss: 1.0834 valid loss: 0.9175, valid accuracy: 0.7916\n",
      "Iter-6890 train loss: 0.8235 valid loss: 0.9167, valid accuracy: 0.7920\n",
      "Iter-6900 train loss: 0.9203 valid loss: 0.9157, valid accuracy: 0.7920\n",
      "Iter-6910 train loss: 1.0455 valid loss: 0.9148, valid accuracy: 0.7922\n",
      "Iter-6920 train loss: 0.8940 valid loss: 0.9139, valid accuracy: 0.7914\n",
      "Iter-6930 train loss: 1.0712 valid loss: 0.9129, valid accuracy: 0.7918\n",
      "Iter-6940 train loss: 0.8535 valid loss: 0.9121, valid accuracy: 0.7918\n",
      "Iter-6950 train loss: 0.8210 valid loss: 0.9111, valid accuracy: 0.7926\n",
      "Iter-6960 train loss: 0.9145 valid loss: 0.9103, valid accuracy: 0.7928\n",
      "Iter-6970 train loss: 0.9724 valid loss: 0.9093, valid accuracy: 0.7930\n",
      "Iter-6980 train loss: 1.0111 valid loss: 0.9084, valid accuracy: 0.7928\n",
      "Iter-6990 train loss: 1.0004 valid loss: 0.9076, valid accuracy: 0.7934\n",
      "Iter-7000 train loss: 0.9602 valid loss: 0.9067, valid accuracy: 0.7944\n",
      "Iter-7010 train loss: 1.0339 valid loss: 0.9057, valid accuracy: 0.7948\n",
      "Iter-7020 train loss: 0.9444 valid loss: 0.9048, valid accuracy: 0.7950\n",
      "Iter-7030 train loss: 0.9412 valid loss: 0.9039, valid accuracy: 0.7950\n",
      "Iter-7040 train loss: 1.1574 valid loss: 0.9030, valid accuracy: 0.7948\n",
      "Iter-7050 train loss: 0.8561 valid loss: 0.9021, valid accuracy: 0.7948\n",
      "Iter-7060 train loss: 0.8638 valid loss: 0.9012, valid accuracy: 0.7946\n",
      "Iter-7070 train loss: 0.8699 valid loss: 0.9003, valid accuracy: 0.7946\n",
      "Iter-7080 train loss: 0.8894 valid loss: 0.8995, valid accuracy: 0.7948\n",
      "Iter-7090 train loss: 0.8556 valid loss: 0.8986, valid accuracy: 0.7948\n",
      "Iter-7100 train loss: 0.7679 valid loss: 0.8978, valid accuracy: 0.7946\n",
      "Iter-7110 train loss: 0.8878 valid loss: 0.8969, valid accuracy: 0.7950\n",
      "Iter-7120 train loss: 0.8890 valid loss: 0.8960, valid accuracy: 0.7948\n",
      "Iter-7130 train loss: 0.8185 valid loss: 0.8951, valid accuracy: 0.7952\n",
      "Iter-7140 train loss: 1.0253 valid loss: 0.8942, valid accuracy: 0.7952\n",
      "Iter-7150 train loss: 1.0736 valid loss: 0.8934, valid accuracy: 0.7956\n",
      "Iter-7160 train loss: 0.9574 valid loss: 0.8925, valid accuracy: 0.7962\n",
      "Iter-7170 train loss: 0.9136 valid loss: 0.8917, valid accuracy: 0.7964\n",
      "Iter-7180 train loss: 1.0560 valid loss: 0.8909, valid accuracy: 0.7980\n",
      "Iter-7190 train loss: 0.8048 valid loss: 0.8900, valid accuracy: 0.7984\n",
      "Iter-7200 train loss: 1.0169 valid loss: 0.8891, valid accuracy: 0.7982\n",
      "Iter-7210 train loss: 0.8377 valid loss: 0.8883, valid accuracy: 0.7986\n",
      "Iter-7220 train loss: 0.8898 valid loss: 0.8875, valid accuracy: 0.7988\n",
      "Iter-7230 train loss: 0.7440 valid loss: 0.8866, valid accuracy: 0.7994\n",
      "Iter-7240 train loss: 0.9311 valid loss: 0.8857, valid accuracy: 0.7994\n",
      "Iter-7250 train loss: 0.9369 valid loss: 0.8848, valid accuracy: 0.7998\n",
      "Iter-7260 train loss: 0.8994 valid loss: 0.8840, valid accuracy: 0.7998\n",
      "Iter-7270 train loss: 0.8703 valid loss: 0.8831, valid accuracy: 0.8002\n",
      "Iter-7280 train loss: 0.9220 valid loss: 0.8823, valid accuracy: 0.8000\n",
      "Iter-7290 train loss: 1.0942 valid loss: 0.8815, valid accuracy: 0.8004\n",
      "Iter-7300 train loss: 0.9573 valid loss: 0.8806, valid accuracy: 0.8004\n",
      "Iter-7310 train loss: 0.9512 valid loss: 0.8797, valid accuracy: 0.8012\n",
      "Iter-7320 train loss: 1.0634 valid loss: 0.8789, valid accuracy: 0.8016\n",
      "Iter-7330 train loss: 0.8668 valid loss: 0.8780, valid accuracy: 0.8012\n",
      "Iter-7340 train loss: 0.7943 valid loss: 0.8772, valid accuracy: 0.8020\n",
      "Iter-7350 train loss: 0.8900 valid loss: 0.8763, valid accuracy: 0.8018\n",
      "Iter-7360 train loss: 0.8525 valid loss: 0.8754, valid accuracy: 0.8022\n",
      "Iter-7370 train loss: 0.8768 valid loss: 0.8746, valid accuracy: 0.8026\n",
      "Iter-7380 train loss: 1.0172 valid loss: 0.8738, valid accuracy: 0.8028\n",
      "Iter-7390 train loss: 0.7716 valid loss: 0.8729, valid accuracy: 0.8030\n",
      "Iter-7400 train loss: 0.8046 valid loss: 0.8720, valid accuracy: 0.8038\n",
      "Iter-7410 train loss: 0.9466 valid loss: 0.8712, valid accuracy: 0.8038\n",
      "Iter-7420 train loss: 0.8939 valid loss: 0.8704, valid accuracy: 0.8050\n",
      "Iter-7430 train loss: 0.9019 valid loss: 0.8696, valid accuracy: 0.8046\n",
      "Iter-7440 train loss: 0.8003 valid loss: 0.8688, valid accuracy: 0.8050\n",
      "Iter-7450 train loss: 0.9431 valid loss: 0.8679, valid accuracy: 0.8056\n",
      "Iter-7460 train loss: 0.7722 valid loss: 0.8671, valid accuracy: 0.8062\n",
      "Iter-7470 train loss: 0.8981 valid loss: 0.8662, valid accuracy: 0.8056\n",
      "Iter-7480 train loss: 0.9078 valid loss: 0.8654, valid accuracy: 0.8064\n",
      "Iter-7490 train loss: 0.9040 valid loss: 0.8647, valid accuracy: 0.8070\n",
      "Iter-7500 train loss: 0.7383 valid loss: 0.8639, valid accuracy: 0.8072\n",
      "Iter-7510 train loss: 0.8609 valid loss: 0.8632, valid accuracy: 0.8068\n",
      "Iter-7520 train loss: 0.7920 valid loss: 0.8623, valid accuracy: 0.8070\n",
      "Iter-7530 train loss: 0.9189 valid loss: 0.8614, valid accuracy: 0.8076\n",
      "Iter-7540 train loss: 1.1488 valid loss: 0.8605, valid accuracy: 0.8074\n",
      "Iter-7550 train loss: 0.8088 valid loss: 0.8597, valid accuracy: 0.8078\n",
      "Iter-7560 train loss: 0.8041 valid loss: 0.8589, valid accuracy: 0.8084\n",
      "Iter-7570 train loss: 0.8359 valid loss: 0.8580, valid accuracy: 0.8088\n",
      "Iter-7580 train loss: 0.8750 valid loss: 0.8573, valid accuracy: 0.8088\n",
      "Iter-7590 train loss: 0.8611 valid loss: 0.8565, valid accuracy: 0.8090\n",
      "Iter-7600 train loss: 0.9677 valid loss: 0.8557, valid accuracy: 0.8092\n",
      "Iter-7610 train loss: 0.7421 valid loss: 0.8550, valid accuracy: 0.8092\n",
      "Iter-7620 train loss: 0.7394 valid loss: 0.8542, valid accuracy: 0.8096\n",
      "Iter-7630 train loss: 0.8146 valid loss: 0.8533, valid accuracy: 0.8100\n",
      "Iter-7640 train loss: 0.8208 valid loss: 0.8525, valid accuracy: 0.8102\n",
      "Iter-7650 train loss: 0.8916 valid loss: 0.8517, valid accuracy: 0.8102\n",
      "Iter-7660 train loss: 0.9572 valid loss: 0.8509, valid accuracy: 0.8110\n",
      "Iter-7670 train loss: 0.8237 valid loss: 0.8501, valid accuracy: 0.8118\n",
      "Iter-7680 train loss: 0.9118 valid loss: 0.8492, valid accuracy: 0.8118\n",
      "Iter-7690 train loss: 0.8641 valid loss: 0.8484, valid accuracy: 0.8128\n",
      "Iter-7700 train loss: 0.8291 valid loss: 0.8476, valid accuracy: 0.8134\n",
      "Iter-7710 train loss: 0.8333 valid loss: 0.8467, valid accuracy: 0.8138\n",
      "Iter-7720 train loss: 0.9173 valid loss: 0.8459, valid accuracy: 0.8142\n",
      "Iter-7730 train loss: 0.7633 valid loss: 0.8451, valid accuracy: 0.8142\n",
      "Iter-7740 train loss: 0.8080 valid loss: 0.8443, valid accuracy: 0.8146\n",
      "Iter-7750 train loss: 0.8755 valid loss: 0.8435, valid accuracy: 0.8152\n",
      "Iter-7760 train loss: 0.8099 valid loss: 0.8427, valid accuracy: 0.8158\n",
      "Iter-7770 train loss: 0.8222 valid loss: 0.8420, valid accuracy: 0.8154\n",
      "Iter-7780 train loss: 0.8911 valid loss: 0.8412, valid accuracy: 0.8156\n",
      "Iter-7790 train loss: 0.8254 valid loss: 0.8404, valid accuracy: 0.8160\n",
      "Iter-7800 train loss: 0.7777 valid loss: 0.8397, valid accuracy: 0.8160\n",
      "Iter-7810 train loss: 0.8128 valid loss: 0.8389, valid accuracy: 0.8160\n",
      "Iter-7820 train loss: 0.8491 valid loss: 0.8381, valid accuracy: 0.8162\n",
      "Iter-7830 train loss: 0.7864 valid loss: 0.8374, valid accuracy: 0.8166\n",
      "Iter-7840 train loss: 0.8324 valid loss: 0.8366, valid accuracy: 0.8170\n",
      "Iter-7850 train loss: 0.8642 valid loss: 0.8358, valid accuracy: 0.8186\n",
      "Iter-7860 train loss: 0.8734 valid loss: 0.8350, valid accuracy: 0.8186\n",
      "Iter-7870 train loss: 0.7604 valid loss: 0.8341, valid accuracy: 0.8186\n",
      "Iter-7880 train loss: 0.8722 valid loss: 0.8333, valid accuracy: 0.8192\n",
      "Iter-7890 train loss: 0.7823 valid loss: 0.8325, valid accuracy: 0.8194\n",
      "Iter-7900 train loss: 0.7403 valid loss: 0.8318, valid accuracy: 0.8194\n",
      "Iter-7910 train loss: 0.7274 valid loss: 0.8310, valid accuracy: 0.8194\n",
      "Iter-7920 train loss: 0.8176 valid loss: 0.8302, valid accuracy: 0.8198\n",
      "Iter-7930 train loss: 0.7689 valid loss: 0.8293, valid accuracy: 0.8200\n",
      "Iter-7940 train loss: 0.9105 valid loss: 0.8286, valid accuracy: 0.8200\n",
      "Iter-7950 train loss: 0.9796 valid loss: 0.8278, valid accuracy: 0.8196\n",
      "Iter-7960 train loss: 0.9970 valid loss: 0.8271, valid accuracy: 0.8202\n",
      "Iter-7970 train loss: 0.9026 valid loss: 0.8263, valid accuracy: 0.8202\n",
      "Iter-7980 train loss: 0.8580 valid loss: 0.8256, valid accuracy: 0.8198\n",
      "Iter-7990 train loss: 0.8539 valid loss: 0.8248, valid accuracy: 0.8202\n",
      "Iter-8000 train loss: 0.7869 valid loss: 0.8241, valid accuracy: 0.8206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 0.7535 valid loss: 0.8233, valid accuracy: 0.8202\n",
      "Iter-8020 train loss: 0.9395 valid loss: 0.8225, valid accuracy: 0.8200\n",
      "Iter-8030 train loss: 1.0033 valid loss: 0.8218, valid accuracy: 0.8202\n",
      "Iter-8040 train loss: 0.7778 valid loss: 0.8210, valid accuracy: 0.8202\n",
      "Iter-8050 train loss: 0.9007 valid loss: 0.8203, valid accuracy: 0.8206\n",
      "Iter-8060 train loss: 0.9216 valid loss: 0.8195, valid accuracy: 0.8208\n",
      "Iter-8070 train loss: 0.8014 valid loss: 0.8188, valid accuracy: 0.8206\n",
      "Iter-8080 train loss: 0.7222 valid loss: 0.8180, valid accuracy: 0.8206\n",
      "Iter-8090 train loss: 0.7300 valid loss: 0.8172, valid accuracy: 0.8208\n",
      "Iter-8100 train loss: 0.9657 valid loss: 0.8165, valid accuracy: 0.8216\n",
      "Iter-8110 train loss: 0.9094 valid loss: 0.8157, valid accuracy: 0.8216\n",
      "Iter-8120 train loss: 1.0399 valid loss: 0.8150, valid accuracy: 0.8220\n",
      "Iter-8130 train loss: 0.8624 valid loss: 0.8143, valid accuracy: 0.8226\n",
      "Iter-8140 train loss: 0.9508 valid loss: 0.8136, valid accuracy: 0.8218\n",
      "Iter-8150 train loss: 0.8798 valid loss: 0.8128, valid accuracy: 0.8222\n",
      "Iter-8160 train loss: 0.8122 valid loss: 0.8120, valid accuracy: 0.8218\n",
      "Iter-8170 train loss: 0.7554 valid loss: 0.8113, valid accuracy: 0.8216\n",
      "Iter-8180 train loss: 0.7806 valid loss: 0.8105, valid accuracy: 0.8226\n",
      "Iter-8190 train loss: 0.9491 valid loss: 0.8098, valid accuracy: 0.8228\n",
      "Iter-8200 train loss: 0.8658 valid loss: 0.8091, valid accuracy: 0.8226\n",
      "Iter-8210 train loss: 0.9154 valid loss: 0.8084, valid accuracy: 0.8228\n",
      "Iter-8220 train loss: 0.8194 valid loss: 0.8076, valid accuracy: 0.8228\n",
      "Iter-8230 train loss: 0.8354 valid loss: 0.8069, valid accuracy: 0.8230\n",
      "Iter-8240 train loss: 0.9574 valid loss: 0.8062, valid accuracy: 0.8236\n",
      "Iter-8250 train loss: 0.8119 valid loss: 0.8055, valid accuracy: 0.8234\n",
      "Iter-8260 train loss: 0.7980 valid loss: 0.8047, valid accuracy: 0.8236\n",
      "Iter-8270 train loss: 0.6940 valid loss: 0.8040, valid accuracy: 0.8232\n",
      "Iter-8280 train loss: 0.8607 valid loss: 0.8033, valid accuracy: 0.8240\n",
      "Iter-8290 train loss: 0.8725 valid loss: 0.8025, valid accuracy: 0.8234\n",
      "Iter-8300 train loss: 0.8535 valid loss: 0.8018, valid accuracy: 0.8240\n",
      "Iter-8310 train loss: 0.8059 valid loss: 0.8010, valid accuracy: 0.8238\n",
      "Iter-8320 train loss: 0.8762 valid loss: 0.8003, valid accuracy: 0.8244\n",
      "Iter-8330 train loss: 0.7893 valid loss: 0.7996, valid accuracy: 0.8250\n",
      "Iter-8340 train loss: 0.9146 valid loss: 0.7990, valid accuracy: 0.8246\n",
      "Iter-8350 train loss: 0.9073 valid loss: 0.7982, valid accuracy: 0.8246\n",
      "Iter-8360 train loss: 0.8034 valid loss: 0.7975, valid accuracy: 0.8244\n",
      "Iter-8370 train loss: 0.8699 valid loss: 0.7968, valid accuracy: 0.8250\n",
      "Iter-8380 train loss: 0.8171 valid loss: 0.7961, valid accuracy: 0.8250\n",
      "Iter-8390 train loss: 0.7078 valid loss: 0.7954, valid accuracy: 0.8244\n",
      "Iter-8400 train loss: 0.9044 valid loss: 0.7947, valid accuracy: 0.8252\n",
      "Iter-8410 train loss: 0.7991 valid loss: 0.7940, valid accuracy: 0.8258\n",
      "Iter-8420 train loss: 0.8050 valid loss: 0.7932, valid accuracy: 0.8254\n",
      "Iter-8430 train loss: 0.8767 valid loss: 0.7925, valid accuracy: 0.8260\n",
      "Iter-8440 train loss: 0.8448 valid loss: 0.7918, valid accuracy: 0.8264\n",
      "Iter-8450 train loss: 1.0501 valid loss: 0.7911, valid accuracy: 0.8266\n",
      "Iter-8460 train loss: 0.6538 valid loss: 0.7904, valid accuracy: 0.8272\n",
      "Iter-8470 train loss: 0.7222 valid loss: 0.7897, valid accuracy: 0.8276\n",
      "Iter-8480 train loss: 0.7177 valid loss: 0.7889, valid accuracy: 0.8268\n",
      "Iter-8490 train loss: 0.8612 valid loss: 0.7882, valid accuracy: 0.8272\n",
      "Iter-8500 train loss: 0.8387 valid loss: 0.7875, valid accuracy: 0.8274\n",
      "Iter-8510 train loss: 0.8034 valid loss: 0.7868, valid accuracy: 0.8276\n",
      "Iter-8520 train loss: 0.7091 valid loss: 0.7861, valid accuracy: 0.8280\n",
      "Iter-8530 train loss: 0.6831 valid loss: 0.7854, valid accuracy: 0.8282\n",
      "Iter-8540 train loss: 0.8076 valid loss: 0.7846, valid accuracy: 0.8286\n",
      "Iter-8550 train loss: 0.8894 valid loss: 0.7839, valid accuracy: 0.8282\n",
      "Iter-8560 train loss: 0.7151 valid loss: 0.7832, valid accuracy: 0.8284\n",
      "Iter-8570 train loss: 0.8011 valid loss: 0.7825, valid accuracy: 0.8284\n",
      "Iter-8580 train loss: 0.6317 valid loss: 0.7819, valid accuracy: 0.8286\n",
      "Iter-8590 train loss: 0.7747 valid loss: 0.7812, valid accuracy: 0.8288\n",
      "Iter-8600 train loss: 0.9279 valid loss: 0.7806, valid accuracy: 0.8286\n",
      "Iter-8610 train loss: 0.7430 valid loss: 0.7798, valid accuracy: 0.8288\n",
      "Iter-8620 train loss: 0.7387 valid loss: 0.7792, valid accuracy: 0.8296\n",
      "Iter-8630 train loss: 0.7785 valid loss: 0.7785, valid accuracy: 0.8290\n",
      "Iter-8640 train loss: 0.9211 valid loss: 0.7778, valid accuracy: 0.8288\n",
      "Iter-8650 train loss: 0.7167 valid loss: 0.7771, valid accuracy: 0.8294\n",
      "Iter-8660 train loss: 0.8553 valid loss: 0.7764, valid accuracy: 0.8292\n",
      "Iter-8670 train loss: 0.7218 valid loss: 0.7757, valid accuracy: 0.8292\n",
      "Iter-8680 train loss: 0.7258 valid loss: 0.7751, valid accuracy: 0.8298\n",
      "Iter-8690 train loss: 0.7086 valid loss: 0.7744, valid accuracy: 0.8298\n",
      "Iter-8700 train loss: 0.7114 valid loss: 0.7737, valid accuracy: 0.8298\n",
      "Iter-8710 train loss: 0.8923 valid loss: 0.7730, valid accuracy: 0.8302\n",
      "Iter-8720 train loss: 0.6571 valid loss: 0.7723, valid accuracy: 0.8308\n",
      "Iter-8730 train loss: 0.9369 valid loss: 0.7716, valid accuracy: 0.8306\n",
      "Iter-8740 train loss: 0.7451 valid loss: 0.7710, valid accuracy: 0.8308\n",
      "Iter-8750 train loss: 0.7943 valid loss: 0.7703, valid accuracy: 0.8310\n",
      "Iter-8760 train loss: 0.7101 valid loss: 0.7696, valid accuracy: 0.8312\n",
      "Iter-8770 train loss: 0.8019 valid loss: 0.7690, valid accuracy: 0.8314\n",
      "Iter-8780 train loss: 0.8395 valid loss: 0.7683, valid accuracy: 0.8312\n",
      "Iter-8790 train loss: 0.7019 valid loss: 0.7676, valid accuracy: 0.8312\n",
      "Iter-8800 train loss: 0.7294 valid loss: 0.7670, valid accuracy: 0.8316\n",
      "Iter-8810 train loss: 0.9589 valid loss: 0.7663, valid accuracy: 0.8320\n",
      "Iter-8820 train loss: 0.7599 valid loss: 0.7656, valid accuracy: 0.8322\n",
      "Iter-8830 train loss: 0.9494 valid loss: 0.7650, valid accuracy: 0.8326\n",
      "Iter-8840 train loss: 0.9284 valid loss: 0.7644, valid accuracy: 0.8332\n",
      "Iter-8850 train loss: 0.6975 valid loss: 0.7636, valid accuracy: 0.8338\n",
      "Iter-8860 train loss: 0.7234 valid loss: 0.7630, valid accuracy: 0.8336\n",
      "Iter-8870 train loss: 0.9529 valid loss: 0.7623, valid accuracy: 0.8346\n",
      "Iter-8880 train loss: 0.6827 valid loss: 0.7616, valid accuracy: 0.8344\n",
      "Iter-8890 train loss: 0.7318 valid loss: 0.7610, valid accuracy: 0.8346\n",
      "Iter-8900 train loss: 0.7721 valid loss: 0.7604, valid accuracy: 0.8344\n",
      "Iter-8910 train loss: 0.7021 valid loss: 0.7597, valid accuracy: 0.8352\n",
      "Iter-8920 train loss: 0.7930 valid loss: 0.7590, valid accuracy: 0.8352\n",
      "Iter-8930 train loss: 0.6284 valid loss: 0.7584, valid accuracy: 0.8358\n",
      "Iter-8940 train loss: 0.6673 valid loss: 0.7577, valid accuracy: 0.8358\n",
      "Iter-8950 train loss: 0.7475 valid loss: 0.7571, valid accuracy: 0.8356\n",
      "Iter-8960 train loss: 0.8787 valid loss: 0.7565, valid accuracy: 0.8364\n",
      "Iter-8970 train loss: 0.8374 valid loss: 0.7558, valid accuracy: 0.8362\n",
      "Iter-8980 train loss: 0.7598 valid loss: 0.7552, valid accuracy: 0.8364\n",
      "Iter-8990 train loss: 0.6864 valid loss: 0.7545, valid accuracy: 0.8364\n",
      "Iter-9000 train loss: 0.6727 valid loss: 0.7539, valid accuracy: 0.8364\n",
      "Iter-9010 train loss: 0.8943 valid loss: 0.7533, valid accuracy: 0.8370\n",
      "Iter-9020 train loss: 0.7270 valid loss: 0.7526, valid accuracy: 0.8370\n",
      "Iter-9030 train loss: 0.8390 valid loss: 0.7520, valid accuracy: 0.8368\n",
      "Iter-9040 train loss: 0.7786 valid loss: 0.7513, valid accuracy: 0.8370\n",
      "Iter-9050 train loss: 0.5944 valid loss: 0.7506, valid accuracy: 0.8368\n",
      "Iter-9060 train loss: 0.9265 valid loss: 0.7500, valid accuracy: 0.8364\n",
      "Iter-9070 train loss: 0.6439 valid loss: 0.7494, valid accuracy: 0.8366\n",
      "Iter-9080 train loss: 0.6938 valid loss: 0.7487, valid accuracy: 0.8364\n",
      "Iter-9090 train loss: 0.7051 valid loss: 0.7481, valid accuracy: 0.8370\n",
      "Iter-9100 train loss: 0.9221 valid loss: 0.7475, valid accuracy: 0.8370\n",
      "Iter-9110 train loss: 0.6415 valid loss: 0.7469, valid accuracy: 0.8370\n",
      "Iter-9120 train loss: 0.8189 valid loss: 0.7462, valid accuracy: 0.8366\n",
      "Iter-9130 train loss: 0.8144 valid loss: 0.7456, valid accuracy: 0.8370\n",
      "Iter-9140 train loss: 0.7225 valid loss: 0.7449, valid accuracy: 0.8370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 0.8259 valid loss: 0.7443, valid accuracy: 0.8374\n",
      "Iter-9160 train loss: 0.8290 valid loss: 0.7437, valid accuracy: 0.8374\n",
      "Iter-9170 train loss: 0.9413 valid loss: 0.7431, valid accuracy: 0.8374\n",
      "Iter-9180 train loss: 0.8234 valid loss: 0.7426, valid accuracy: 0.8374\n",
      "Iter-9190 train loss: 0.6386 valid loss: 0.7420, valid accuracy: 0.8376\n",
      "Iter-9200 train loss: 0.6487 valid loss: 0.7414, valid accuracy: 0.8378\n",
      "Iter-9210 train loss: 0.8677 valid loss: 0.7408, valid accuracy: 0.8380\n",
      "Iter-9220 train loss: 0.7443 valid loss: 0.7401, valid accuracy: 0.8380\n",
      "Iter-9230 train loss: 0.9077 valid loss: 0.7396, valid accuracy: 0.8384\n",
      "Iter-9240 train loss: 0.7423 valid loss: 0.7389, valid accuracy: 0.8378\n",
      "Iter-9250 train loss: 0.9006 valid loss: 0.7383, valid accuracy: 0.8386\n",
      "Iter-9260 train loss: 0.6967 valid loss: 0.7377, valid accuracy: 0.8384\n",
      "Iter-9270 train loss: 0.7671 valid loss: 0.7372, valid accuracy: 0.8384\n",
      "Iter-9280 train loss: 0.6890 valid loss: 0.7366, valid accuracy: 0.8384\n",
      "Iter-9290 train loss: 0.6364 valid loss: 0.7360, valid accuracy: 0.8384\n",
      "Iter-9300 train loss: 0.7237 valid loss: 0.7354, valid accuracy: 0.8388\n",
      "Iter-9310 train loss: 0.7336 valid loss: 0.7348, valid accuracy: 0.8390\n",
      "Iter-9320 train loss: 0.5602 valid loss: 0.7342, valid accuracy: 0.8388\n",
      "Iter-9330 train loss: 0.5831 valid loss: 0.7336, valid accuracy: 0.8390\n",
      "Iter-9340 train loss: 0.8239 valid loss: 0.7330, valid accuracy: 0.8392\n",
      "Iter-9350 train loss: 0.7719 valid loss: 0.7324, valid accuracy: 0.8394\n",
      "Iter-9360 train loss: 0.8722 valid loss: 0.7319, valid accuracy: 0.8392\n",
      "Iter-9370 train loss: 0.7047 valid loss: 0.7313, valid accuracy: 0.8398\n",
      "Iter-9380 train loss: 0.6126 valid loss: 0.7307, valid accuracy: 0.8402\n",
      "Iter-9390 train loss: 0.8024 valid loss: 0.7301, valid accuracy: 0.8400\n",
      "Iter-9400 train loss: 0.7143 valid loss: 0.7295, valid accuracy: 0.8402\n",
      "Iter-9410 train loss: 0.6302 valid loss: 0.7289, valid accuracy: 0.8406\n",
      "Iter-9420 train loss: 0.8924 valid loss: 0.7284, valid accuracy: 0.8408\n",
      "Iter-9430 train loss: 0.7050 valid loss: 0.7278, valid accuracy: 0.8410\n",
      "Iter-9440 train loss: 0.7113 valid loss: 0.7272, valid accuracy: 0.8412\n",
      "Iter-9450 train loss: 0.5415 valid loss: 0.7266, valid accuracy: 0.8414\n",
      "Iter-9460 train loss: 0.6906 valid loss: 0.7260, valid accuracy: 0.8414\n",
      "Iter-9470 train loss: 0.8311 valid loss: 0.7254, valid accuracy: 0.8418\n",
      "Iter-9480 train loss: 0.6761 valid loss: 0.7248, valid accuracy: 0.8418\n",
      "Iter-9490 train loss: 0.6423 valid loss: 0.7242, valid accuracy: 0.8418\n",
      "Iter-9500 train loss: 0.7216 valid loss: 0.7236, valid accuracy: 0.8420\n",
      "Iter-9510 train loss: 0.7728 valid loss: 0.7230, valid accuracy: 0.8418\n",
      "Iter-9520 train loss: 0.7820 valid loss: 0.7225, valid accuracy: 0.8420\n",
      "Iter-9530 train loss: 0.6512 valid loss: 0.7219, valid accuracy: 0.8420\n",
      "Iter-9540 train loss: 0.6186 valid loss: 0.7213, valid accuracy: 0.8420\n",
      "Iter-9550 train loss: 0.7051 valid loss: 0.7208, valid accuracy: 0.8420\n",
      "Iter-9560 train loss: 0.7284 valid loss: 0.7202, valid accuracy: 0.8416\n",
      "Iter-9570 train loss: 0.7804 valid loss: 0.7196, valid accuracy: 0.8420\n",
      "Iter-9580 train loss: 0.8014 valid loss: 0.7190, valid accuracy: 0.8422\n",
      "Iter-9590 train loss: 0.6369 valid loss: 0.7185, valid accuracy: 0.8422\n",
      "Iter-9600 train loss: 0.8809 valid loss: 0.7179, valid accuracy: 0.8428\n",
      "Iter-9610 train loss: 0.9197 valid loss: 0.7173, valid accuracy: 0.8424\n",
      "Iter-9620 train loss: 0.7508 valid loss: 0.7168, valid accuracy: 0.8426\n",
      "Iter-9630 train loss: 0.7184 valid loss: 0.7162, valid accuracy: 0.8428\n",
      "Iter-9640 train loss: 0.7066 valid loss: 0.7156, valid accuracy: 0.8430\n",
      "Iter-9650 train loss: 0.7769 valid loss: 0.7151, valid accuracy: 0.8428\n",
      "Iter-9660 train loss: 0.7223 valid loss: 0.7145, valid accuracy: 0.8430\n",
      "Iter-9670 train loss: 0.8550 valid loss: 0.7140, valid accuracy: 0.8432\n",
      "Iter-9680 train loss: 0.7255 valid loss: 0.7134, valid accuracy: 0.8432\n",
      "Iter-9690 train loss: 0.7902 valid loss: 0.7128, valid accuracy: 0.8428\n",
      "Iter-9700 train loss: 0.6249 valid loss: 0.7123, valid accuracy: 0.8434\n",
      "Iter-9710 train loss: 0.7393 valid loss: 0.7117, valid accuracy: 0.8434\n",
      "Iter-9720 train loss: 0.7045 valid loss: 0.7112, valid accuracy: 0.8438\n",
      "Iter-9730 train loss: 0.6860 valid loss: 0.7106, valid accuracy: 0.8436\n",
      "Iter-9740 train loss: 0.5898 valid loss: 0.7100, valid accuracy: 0.8436\n",
      "Iter-9750 train loss: 0.8095 valid loss: 0.7095, valid accuracy: 0.8438\n",
      "Iter-9760 train loss: 0.7649 valid loss: 0.7089, valid accuracy: 0.8442\n",
      "Iter-9770 train loss: 0.7794 valid loss: 0.7083, valid accuracy: 0.8448\n",
      "Iter-9780 train loss: 0.9212 valid loss: 0.7079, valid accuracy: 0.8446\n",
      "Iter-9790 train loss: 0.6824 valid loss: 0.7073, valid accuracy: 0.8444\n",
      "Iter-9800 train loss: 0.7700 valid loss: 0.7068, valid accuracy: 0.8446\n",
      "Iter-9810 train loss: 0.7372 valid loss: 0.7061, valid accuracy: 0.8452\n",
      "Iter-9820 train loss: 0.8160 valid loss: 0.7056, valid accuracy: 0.8452\n",
      "Iter-9830 train loss: 0.7208 valid loss: 0.7050, valid accuracy: 0.8456\n",
      "Iter-9840 train loss: 0.6004 valid loss: 0.7044, valid accuracy: 0.8454\n",
      "Iter-9850 train loss: 0.8937 valid loss: 0.7038, valid accuracy: 0.8452\n",
      "Iter-9860 train loss: 0.7486 valid loss: 0.7033, valid accuracy: 0.8456\n",
      "Iter-9870 train loss: 0.6314 valid loss: 0.7028, valid accuracy: 0.8460\n",
      "Iter-9880 train loss: 0.6202 valid loss: 0.7021, valid accuracy: 0.8462\n",
      "Iter-9890 train loss: 0.7545 valid loss: 0.7016, valid accuracy: 0.8462\n",
      "Iter-9900 train loss: 0.5992 valid loss: 0.7010, valid accuracy: 0.8460\n",
      "Iter-9910 train loss: 0.6534 valid loss: 0.7005, valid accuracy: 0.8464\n",
      "Iter-9920 train loss: 0.8201 valid loss: 0.6999, valid accuracy: 0.8466\n",
      "Iter-9930 train loss: 0.6658 valid loss: 0.6993, valid accuracy: 0.8464\n",
      "Iter-9940 train loss: 0.7083 valid loss: 0.6988, valid accuracy: 0.8462\n",
      "Iter-9950 train loss: 0.7348 valid loss: 0.6982, valid accuracy: 0.8462\n",
      "Iter-9960 train loss: 0.5637 valid loss: 0.6976, valid accuracy: 0.8470\n",
      "Iter-9970 train loss: 0.6608 valid loss: 0.6971, valid accuracy: 0.8466\n",
      "Iter-9980 train loss: 0.6392 valid loss: 0.6965, valid accuracy: 0.8462\n",
      "Iter-9990 train loss: 0.7264 valid loss: 0.6959, valid accuracy: 0.8462\n",
      "Iter-10000 train loss: 0.7081 valid loss: 0.6953, valid accuracy: 0.8466\n",
      "Last iteration - Test accuracy mean: 0.8415, std: 0.0000, loss: 0.7018\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2B0D75bBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94c4f7b128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n85QxTodxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94c4f7b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
