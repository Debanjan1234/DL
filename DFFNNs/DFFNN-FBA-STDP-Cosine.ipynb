{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = l.elu_fwd(X=y)\n",
    "#         y = l.sigmoid(X=y)\n",
    "#         y = np.tanh(y)\n",
    "#         a, b = 1.7519, 2/3\n",
    "#         y = a * np.tanh(b*y) # LeCun Tanh\n",
    "#         y = np.tanh(y/2)\n",
    "#         y = np.arctan(y)\n",
    "#         y = y / (1 + np.absolute(y)) # softsign\n",
    "#         y = ((((y**2) + 1)**0.5 + 1)/ 2) + y # bent identity\n",
    "#         y = 1 - np.exp(-1 * np.exp(y)) # log log\n",
    "#         y = np.exp(-1 * (y**2)) # Gaussian\n",
    "#         y = np.absolute(y)\n",
    "#         y = np.sin(y)\n",
    "        y = np.cos(y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = l.elu_fwd(X=y)\n",
    "#             y = l.sigmoid(X=y)\n",
    "#             y = np.tanh(y)\n",
    "#             a, b = 1.7519, 2/3\n",
    "#             y = a * np.tanh(b*y)\n",
    "#             y = np.tanh(y/2)\n",
    "#             y = np.arctan(y)\n",
    "#             y = y / (1 + np.absolute(y)) # softsign\n",
    "#             y = ((((y**2) + 1)**0.5 + 1)/ 2) + y # bent identity\n",
    "#             y = 1 - np.exp(-1 * np.exp(y))\n",
    "#             y = np.exp(-1 * (y**2)) # Gaussian\n",
    "#             y = np.absolute(y)\n",
    "#             y = np.sin(y)\n",
    "            y = np.cos(y)\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.4604 valid loss: 2.5069, valid accuracy: 0.1002\n",
      "Iter-200 train loss: 2.3790 valid loss: 2.4336, valid accuracy: 0.1002\n",
      "Iter-300 train loss: 2.5417 valid loss: 2.3882, valid accuracy: 0.1002\n",
      "Iter-400 train loss: 2.3363 valid loss: 2.3591, valid accuracy: 0.1004\n",
      "Iter-500 train loss: 2.3396 valid loss: 2.3424, valid accuracy: 0.1156\n",
      "Iter-600 train loss: 2.3264 valid loss: 2.3288, valid accuracy: 0.1100\n",
      "Iter-700 train loss: 2.2828 valid loss: 2.3198, valid accuracy: 0.1100\n",
      "Iter-800 train loss: 2.3168 valid loss: 2.3143, valid accuracy: 0.1100\n",
      "Iter-900 train loss: 2.3410 valid loss: 2.3092, valid accuracy: 0.1196\n",
      "Iter-1000 train loss: 2.2802 valid loss: 2.3059, valid accuracy: 0.1140\n",
      "Iter-1100 train loss: 2.2909 valid loss: 2.3038, valid accuracy: 0.1126\n",
      "Iter-1200 train loss: 2.2912 valid loss: 2.3027, valid accuracy: 0.1126\n",
      "Iter-1300 train loss: 2.2901 valid loss: 2.3018, valid accuracy: 0.1126\n",
      "Iter-1400 train loss: 2.3158 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-1500 train loss: 2.2844 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-1600 train loss: 2.2842 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.3004 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3061 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3077 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.3094 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3019 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3071 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3003 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.3028 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.2906 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.2887 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.3110 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.2942 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.2959 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.3033 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3048 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.2973 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.2984 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.3010 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.3054 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.3097 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3070 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.3023 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.2978 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3010 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3097 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3046 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.3056 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.2974 valid loss: 2.3004, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.2907 valid loss: 2.3003, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.2972 valid loss: 2.3004, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.2955 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.2999 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.2978 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3033 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3075 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.2973 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3091 valid loss: 2.3015, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3109 valid loss: 2.3012, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.2945 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.2961 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3023 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.3056 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3026 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3079 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3034 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.2973 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.3086 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.3036 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.2927 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.3039 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.3077 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.2977 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.2946 valid loss: 2.3010, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3063 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.2921 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.3032 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.3108 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.2946 valid loss: 2.3010, valid accuracy: 0.1128\n",
      "Iter-7600 train loss: 2.2975 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.2899 valid loss: 2.3007, valid accuracy: 0.1128\n",
      "Iter-7800 train loss: 2.3087 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.2968 valid loss: 2.3007, valid accuracy: 0.1128\n",
      "Iter-8000 train loss: 2.2993 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3029 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.3032 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.3012 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.2906 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.3063 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.2922 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.2951 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.2947 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.2996 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.2984 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.3117 valid loss: 2.3006, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.2882 valid loss: 2.3005, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.3073 valid loss: 2.3007, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2926 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.3131 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.2974 valid loss: 2.3008, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.2931 valid loss: 2.3009, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.2893 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.3013 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.2886 valid loss: 2.3011, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.3012\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fb4ef83c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmVlmDg0zM8vMoWFmZpk5NMzMLLP/D9770+dWH1lAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0fb4ef8cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
