{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = l.elu_fwd(X=y)\n",
    "#         y = l.sigmoid(X=y)\n",
    "#         y = np.tanh(y)\n",
    "#         a, b = 1.7519, 2/3\n",
    "#         y = a * np.tanh(b*y) # LeCun Tanh\n",
    "#         y = np.tanh(y/2)\n",
    "#         y = np.arctan(y)\n",
    "#         y = y / (1 + np.absolute(y)) # softsign\n",
    "#         y = ((((y**2) + 1)**0.5 + 1)/ 2) + y # bent identity\n",
    "#         y = 1 - np.exp(-1 * np.exp(y)) # log log\n",
    "        y = np.exp(-1 * (y**2)) # Gaussian\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = l.elu_fwd(X=y)\n",
    "#             y = l.sigmoid(X=y)\n",
    "#             y = np.tanh(y)\n",
    "#             a, b = 1.7519, 2/3\n",
    "#             y = a * np.tanh(b*y)\n",
    "#             y = np.tanh(y/2)\n",
    "#             y = np.arctan(y)\n",
    "#             y = y / (1 + np.absolute(y)) # softsign\n",
    "#             y = ((((y**2) + 1)**0.5 + 1)/ 2) + y # bent identity\n",
    "#             y = 1 - np.exp(-1 * np.exp(y))\n",
    "            y = np.exp(-1 * (y**2)) # Gaussian\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        ys, ys_prev = self.ys, self.ys_prev # temporal diff instead of differentiable function\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3845 valid loss: 2.4352, valid accuracy: 0.0986\n",
      "Iter-200 train loss: 2.3642 valid loss: 2.3806, valid accuracy: 0.0986\n",
      "Iter-300 train loss: 2.3655 valid loss: 2.3494, valid accuracy: 0.0986\n",
      "Iter-400 train loss: 2.3472 valid loss: 2.3314, valid accuracy: 0.0986\n",
      "Iter-500 train loss: 2.3445 valid loss: 2.3189, valid accuracy: 0.0986\n",
      "Iter-600 train loss: 2.3265 valid loss: 2.3110, valid accuracy: 0.0992\n",
      "Iter-700 train loss: 2.3134 valid loss: 2.3067, valid accuracy: 0.0996\n",
      "Iter-800 train loss: 2.3060 valid loss: 2.3037, valid accuracy: 0.1000\n",
      "Iter-900 train loss: 2.3226 valid loss: 2.3021, valid accuracy: 0.1026\n",
      "Iter-1000 train loss: 2.3038 valid loss: 2.3010, valid accuracy: 0.1016\n",
      "Iter-1100 train loss: 2.3150 valid loss: 2.3003, valid accuracy: 0.1014\n",
      "Iter-1200 train loss: 2.3029 valid loss: 2.2999, valid accuracy: 0.1588\n",
      "Iter-1300 train loss: 2.2979 valid loss: 2.2998, valid accuracy: 0.1514\n",
      "Iter-1400 train loss: 2.2929 valid loss: 2.2995, valid accuracy: 0.1128\n",
      "Iter-1500 train loss: 2.2942 valid loss: 2.2995, valid accuracy: 0.1164\n",
      "Iter-1600 train loss: 2.2984 valid loss: 2.2995, valid accuracy: 0.1128\n",
      "Iter-1700 train loss: 2.3054 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.2853 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.2996 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-2000 train loss: 2.2917 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-2100 train loss: 2.3042 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.3091 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.3075 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.2841 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.3041 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.3001 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.3049 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.2869 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.3014 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.2994 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3037 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.3002 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.2999 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.2993 valid loss: 2.2996, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.3061 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.2881 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.3075 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.2967 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.3035 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.3009 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.2990 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3110 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-4300 train loss: 2.2841 valid loss: 2.2996, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.3037 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.2988 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.2936 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.3017 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.3013 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.3088 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.2921 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.2886 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.3048 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.3062 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3038 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.3155 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.3004 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.2993 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3079 valid loss: 2.2997, valid accuracy: 0.1134\n",
      "Iter-5900 train loss: 2.3015 valid loss: 2.2996, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.2934 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3055 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3116 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.3036 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.3130 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.2983 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.2972 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.2905 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.2876 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.2956 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.2902 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.3058 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.3155 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.3003 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.2884 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.3028 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.3044 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.3086 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.2936 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.2901 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.3060 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.2909 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.2988 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.3032 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.3037 valid loss: 2.2992, valid accuracy: 0.1128\n",
      "Iter-8500 train loss: 2.3076 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.3024 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.3213 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.3050 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.3013 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.2946 valid loss: 2.2991, valid accuracy: 0.1130\n",
      "Iter-9100 train loss: 2.3093 valid loss: 2.2991, valid accuracy: 0.1134\n",
      "Iter-9200 train loss: 2.3012 valid loss: 2.2991, valid accuracy: 0.1144\n",
      "Iter-9300 train loss: 2.2946 valid loss: 2.2990, valid accuracy: 0.1128\n",
      "Iter-9400 train loss: 2.3033 valid loss: 2.2990, valid accuracy: 0.1128\n",
      "Iter-9500 train loss: 2.2909 valid loss: 2.2988, valid accuracy: 0.1128\n",
      "Iter-9600 train loss: 2.2907 valid loss: 2.2987, valid accuracy: 0.1130\n",
      "Iter-9700 train loss: 2.2973 valid loss: 2.2987, valid accuracy: 0.1128\n",
      "Iter-9800 train loss: 2.2914 valid loss: 2.2987, valid accuracy: 0.1072\n",
      "Iter-9900 train loss: 2.3029 valid loss: 2.2991, valid accuracy: 0.1128\n",
      "Iter-10000 train loss: 2.3001 valid loss: 2.2989, valid accuracy: 0.1128\n",
      "Last iteration - Test accuracy mean: 0.1137, std: 0.0000, loss: 2.2990\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa399bc4518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa399bc41d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
