{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        # self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "                 b=np.zeros((1, H)))\n",
    "        #         low, high = (-1. / np.sqrt(D / 2.)), (+1. / np.sqrt(D / 2.))\n",
    "        #         m = dict(W=np.random.uniform(size=(D, H), low=low, high=high),\n",
    "        #                  b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m = dict(W=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "                 b=np.zeros((1, H)))\n",
    "        #         low, high = (-1. / np.sqrt(H / 2.)), (+1. / np.sqrt(H / 2.))\n",
    "        #         m = dict(W=np.random.uniform(size=(H, H), low=low, high=high),\n",
    "        #                  b=np.zeros((1, H)))\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "                 b=np.zeros((1, C)))\n",
    "        #         low, high = (-1. / np.sqrt(H / 2.)), (+1. / np.sqrt(H / 2.))\n",
    "        #         m = dict(W=np.random.uniform(size=(H, C), low=low, high=high),\n",
    "        #                  b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, h = cache\n",
    "\n",
    "        dW = h.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        dX = dout @ W.T # Backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = selu_forward(X=y)\n",
    "        X = y.copy() # pass the previous output to the next layer\n",
    "        caches.append((fc_cache, nl_cache)) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = selu_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            nl_caches.append(nl_cache)\n",
    "        caches.append((fc_caches, nl_caches)) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Input layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dX.copy() # pass it to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = selu_backward(dout=dy, cache=nl_caches[layer])\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = selu_backward(dout=dy, cache=nl_cache)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Momentums\n",
    "        M, R = [], []\n",
    "\n",
    "        # Input layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers momentum\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        # Output layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    " \n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "                # M[0][key] = l.exp_running_avg(M[0][key], grads[0][key], beta1)\n",
    "                # R[0][key] = l.exp_running_avg(R[0][key], grads[0][key]**2, beta2)\n",
    "                # m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                # r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "                # self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "                    # M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grads[1][layer][key], beta1)\n",
    "                    # R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grads[1][layer][key]**2, beta2)\n",
    "                    # m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    # r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "                    # self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                # M[2][key] = l.exp_running_avg(M[2][key], grads[2][key], beta1)\n",
    "                # R[2][key] = l.exp_running_avg(R[2][key], grads[2][key]**2, beta2)\n",
    "                # m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                # r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "                # self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.6073 valid loss: 2.6481, valid accuracy: 0.1144\n",
      "Iter-20 train loss: 2.7863 valid loss: 2.5849, valid accuracy: 0.1204\n",
      "Iter-30 train loss: 2.4829 valid loss: 2.5300, valid accuracy: 0.1286\n",
      "Iter-40 train loss: 2.4650 valid loss: 2.4687, valid accuracy: 0.1354\n",
      "Iter-50 train loss: 2.4562 valid loss: 2.4154, valid accuracy: 0.1452\n",
      "Iter-60 train loss: 2.3232 valid loss: 2.3638, valid accuracy: 0.1564\n",
      "Iter-70 train loss: 2.0872 valid loss: 2.3142, valid accuracy: 0.1652\n",
      "Iter-80 train loss: 2.1284 valid loss: 2.2612, valid accuracy: 0.1762\n",
      "Iter-90 train loss: 2.2015 valid loss: 2.2126, valid accuracy: 0.1896\n",
      "Iter-100 train loss: 2.2306 valid loss: 2.1652, valid accuracy: 0.2022\n",
      "Iter-110 train loss: 2.1185 valid loss: 2.1239, valid accuracy: 0.2140\n",
      "Iter-120 train loss: 2.0098 valid loss: 2.0813, valid accuracy: 0.2280\n",
      "Iter-130 train loss: 1.9960 valid loss: 2.0421, valid accuracy: 0.2436\n",
      "Iter-140 train loss: 1.9394 valid loss: 2.0021, valid accuracy: 0.2610\n",
      "Iter-150 train loss: 1.9126 valid loss: 1.9633, valid accuracy: 0.2810\n",
      "Iter-160 train loss: 1.9533 valid loss: 1.9307, valid accuracy: 0.2970\n",
      "Iter-170 train loss: 1.8260 valid loss: 1.8993, valid accuracy: 0.3136\n",
      "Iter-180 train loss: 1.9216 valid loss: 1.8654, valid accuracy: 0.3336\n",
      "Iter-190 train loss: 1.8628 valid loss: 1.8347, valid accuracy: 0.3466\n",
      "Iter-200 train loss: 1.8734 valid loss: 1.8044, valid accuracy: 0.3722\n",
      "Iter-210 train loss: 1.7024 valid loss: 1.7749, valid accuracy: 0.3920\n",
      "Iter-220 train loss: 1.7720 valid loss: 1.7492, valid accuracy: 0.4074\n",
      "Iter-230 train loss: 1.5233 valid loss: 1.7226, valid accuracy: 0.4250\n",
      "Iter-240 train loss: 1.6449 valid loss: 1.6984, valid accuracy: 0.4392\n",
      "Iter-250 train loss: 1.7779 valid loss: 1.6734, valid accuracy: 0.4530\n",
      "Iter-260 train loss: 1.6405 valid loss: 1.6486, valid accuracy: 0.4686\n",
      "Iter-270 train loss: 1.4909 valid loss: 1.6262, valid accuracy: 0.4796\n",
      "Iter-280 train loss: 1.5804 valid loss: 1.6040, valid accuracy: 0.4922\n",
      "Iter-290 train loss: 1.5233 valid loss: 1.5838, valid accuracy: 0.5052\n",
      "Iter-300 train loss: 1.5786 valid loss: 1.5640, valid accuracy: 0.5148\n",
      "Iter-310 train loss: 1.4576 valid loss: 1.5440, valid accuracy: 0.5246\n",
      "Iter-320 train loss: 1.4671 valid loss: 1.5241, valid accuracy: 0.5382\n",
      "Iter-330 train loss: 1.5544 valid loss: 1.5062, valid accuracy: 0.5452\n",
      "Iter-340 train loss: 1.4645 valid loss: 1.4882, valid accuracy: 0.5532\n",
      "Iter-350 train loss: 1.3031 valid loss: 1.4716, valid accuracy: 0.5576\n",
      "Iter-360 train loss: 1.5642 valid loss: 1.4551, valid accuracy: 0.5632\n",
      "Iter-370 train loss: 1.4866 valid loss: 1.4391, valid accuracy: 0.5714\n",
      "Iter-380 train loss: 1.2791 valid loss: 1.4232, valid accuracy: 0.5792\n",
      "Iter-390 train loss: 1.4049 valid loss: 1.4083, valid accuracy: 0.5858\n",
      "Iter-400 train loss: 1.3912 valid loss: 1.3936, valid accuracy: 0.5940\n",
      "Iter-410 train loss: 1.3338 valid loss: 1.3790, valid accuracy: 0.6008\n",
      "Iter-420 train loss: 1.4619 valid loss: 1.3652, valid accuracy: 0.6070\n",
      "Iter-430 train loss: 1.3642 valid loss: 1.3506, valid accuracy: 0.6160\n",
      "Iter-440 train loss: 1.4157 valid loss: 1.3372, valid accuracy: 0.6222\n",
      "Iter-450 train loss: 1.3902 valid loss: 1.3235, valid accuracy: 0.6300\n",
      "Iter-460 train loss: 1.3250 valid loss: 1.3101, valid accuracy: 0.6376\n",
      "Iter-470 train loss: 1.3605 valid loss: 1.2985, valid accuracy: 0.6420\n",
      "Iter-480 train loss: 1.3896 valid loss: 1.2861, valid accuracy: 0.6470\n",
      "Iter-490 train loss: 1.4173 valid loss: 1.2736, valid accuracy: 0.6526\n",
      "Iter-500 train loss: 1.2603 valid loss: 1.2619, valid accuracy: 0.6570\n",
      "Iter-510 train loss: 1.3063 valid loss: 1.2508, valid accuracy: 0.6616\n",
      "Iter-520 train loss: 1.2463 valid loss: 1.2399, valid accuracy: 0.6660\n",
      "Iter-530 train loss: 1.1641 valid loss: 1.2284, valid accuracy: 0.6688\n",
      "Iter-540 train loss: 1.0024 valid loss: 1.2175, valid accuracy: 0.6724\n",
      "Iter-550 train loss: 1.2644 valid loss: 1.2077, valid accuracy: 0.6788\n",
      "Iter-560 train loss: 1.3301 valid loss: 1.1970, valid accuracy: 0.6798\n",
      "Iter-570 train loss: 1.1562 valid loss: 1.1876, valid accuracy: 0.6822\n",
      "Iter-580 train loss: 1.1379 valid loss: 1.1783, valid accuracy: 0.6866\n",
      "Iter-590 train loss: 1.1485 valid loss: 1.1688, valid accuracy: 0.6900\n",
      "Iter-600 train loss: 1.2724 valid loss: 1.1598, valid accuracy: 0.6954\n",
      "Iter-610 train loss: 1.0934 valid loss: 1.1504, valid accuracy: 0.6990\n",
      "Iter-620 train loss: 1.1688 valid loss: 1.1416, valid accuracy: 0.7018\n",
      "Iter-630 train loss: 1.2020 valid loss: 1.1329, valid accuracy: 0.7034\n",
      "Iter-640 train loss: 1.0129 valid loss: 1.1244, valid accuracy: 0.7058\n",
      "Iter-650 train loss: 1.1846 valid loss: 1.1163, valid accuracy: 0.7072\n",
      "Iter-660 train loss: 1.1662 valid loss: 1.1080, valid accuracy: 0.7114\n",
      "Iter-670 train loss: 1.1308 valid loss: 1.0995, valid accuracy: 0.7154\n",
      "Iter-680 train loss: 0.9904 valid loss: 1.0917, valid accuracy: 0.7182\n",
      "Iter-690 train loss: 0.9891 valid loss: 1.0843, valid accuracy: 0.7220\n",
      "Iter-700 train loss: 0.9431 valid loss: 1.0766, valid accuracy: 0.7232\n",
      "Iter-710 train loss: 1.0024 valid loss: 1.0693, valid accuracy: 0.7252\n",
      "Iter-720 train loss: 0.9226 valid loss: 1.0619, valid accuracy: 0.7280\n",
      "Iter-730 train loss: 1.0165 valid loss: 1.0544, valid accuracy: 0.7308\n",
      "Iter-740 train loss: 1.0648 valid loss: 1.0471, valid accuracy: 0.7332\n",
      "Iter-750 train loss: 0.9844 valid loss: 1.0398, valid accuracy: 0.7368\n",
      "Iter-760 train loss: 0.9820 valid loss: 1.0333, valid accuracy: 0.7386\n",
      "Iter-770 train loss: 1.1462 valid loss: 1.0261, valid accuracy: 0.7392\n",
      "Iter-780 train loss: 1.2053 valid loss: 1.0196, valid accuracy: 0.7414\n",
      "Iter-790 train loss: 1.0872 valid loss: 1.0136, valid accuracy: 0.7436\n",
      "Iter-800 train loss: 0.9141 valid loss: 1.0072, valid accuracy: 0.7458\n",
      "Iter-810 train loss: 0.9350 valid loss: 1.0012, valid accuracy: 0.7450\n",
      "Iter-820 train loss: 1.0822 valid loss: 0.9953, valid accuracy: 0.7464\n",
      "Iter-830 train loss: 0.8918 valid loss: 0.9894, valid accuracy: 0.7496\n",
      "Iter-840 train loss: 0.8524 valid loss: 0.9831, valid accuracy: 0.7526\n",
      "Iter-850 train loss: 0.9227 valid loss: 0.9775, valid accuracy: 0.7548\n",
      "Iter-860 train loss: 0.9260 valid loss: 0.9714, valid accuracy: 0.7566\n",
      "Iter-870 train loss: 0.8913 valid loss: 0.9658, valid accuracy: 0.7564\n",
      "Iter-880 train loss: 0.9652 valid loss: 0.9606, valid accuracy: 0.7590\n",
      "Iter-890 train loss: 0.9102 valid loss: 0.9551, valid accuracy: 0.7606\n",
      "Iter-900 train loss: 0.8954 valid loss: 0.9496, valid accuracy: 0.7622\n",
      "Iter-910 train loss: 0.8604 valid loss: 0.9443, valid accuracy: 0.7638\n",
      "Iter-920 train loss: 1.0674 valid loss: 0.9393, valid accuracy: 0.7642\n",
      "Iter-930 train loss: 1.0078 valid loss: 0.9342, valid accuracy: 0.7656\n",
      "Iter-940 train loss: 0.9747 valid loss: 0.9291, valid accuracy: 0.7672\n",
      "Iter-950 train loss: 0.8958 valid loss: 0.9242, valid accuracy: 0.7690\n",
      "Iter-960 train loss: 0.8765 valid loss: 0.9192, valid accuracy: 0.7692\n",
      "Iter-970 train loss: 0.9730 valid loss: 0.9144, valid accuracy: 0.7698\n",
      "Iter-980 train loss: 0.8810 valid loss: 0.9097, valid accuracy: 0.7718\n",
      "Iter-990 train loss: 0.8634 valid loss: 0.9053, valid accuracy: 0.7720\n",
      "Iter-1000 train loss: 0.8205 valid loss: 0.9007, valid accuracy: 0.7726\n",
      "Iter-1010 train loss: 0.7784 valid loss: 0.8963, valid accuracy: 0.7738\n",
      "Iter-1020 train loss: 0.9882 valid loss: 0.8920, valid accuracy: 0.7756\n",
      "Iter-1030 train loss: 1.0971 valid loss: 0.8878, valid accuracy: 0.7784\n",
      "Iter-1040 train loss: 0.7772 valid loss: 0.8831, valid accuracy: 0.7786\n",
      "Iter-1050 train loss: 0.7712 valid loss: 0.8788, valid accuracy: 0.7814\n",
      "Iter-1060 train loss: 0.9761 valid loss: 0.8745, valid accuracy: 0.7828\n",
      "Iter-1070 train loss: 0.7947 valid loss: 0.8706, valid accuracy: 0.7836\n",
      "Iter-1080 train loss: 0.8625 valid loss: 0.8666, valid accuracy: 0.7836\n",
      "Iter-1090 train loss: 1.0089 valid loss: 0.8627, valid accuracy: 0.7846\n",
      "Iter-1100 train loss: 0.9834 valid loss: 0.8591, valid accuracy: 0.7864\n",
      "Iter-1110 train loss: 0.8184 valid loss: 0.8553, valid accuracy: 0.7878\n",
      "Iter-1120 train loss: 0.7441 valid loss: 0.8514, valid accuracy: 0.7882\n",
      "Iter-1130 train loss: 0.8539 valid loss: 0.8472, valid accuracy: 0.7902\n",
      "Iter-1140 train loss: 0.8882 valid loss: 0.8436, valid accuracy: 0.7914\n",
      "Iter-1150 train loss: 0.7793 valid loss: 0.8399, valid accuracy: 0.7924\n",
      "Iter-1160 train loss: 0.8676 valid loss: 0.8362, valid accuracy: 0.7920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 0.8119 valid loss: 0.8324, valid accuracy: 0.7940\n",
      "Iter-1180 train loss: 0.9357 valid loss: 0.8290, valid accuracy: 0.7952\n",
      "Iter-1190 train loss: 0.8385 valid loss: 0.8257, valid accuracy: 0.7958\n",
      "Iter-1200 train loss: 0.7439 valid loss: 0.8221, valid accuracy: 0.7946\n",
      "Iter-1210 train loss: 0.7976 valid loss: 0.8186, valid accuracy: 0.7950\n",
      "Iter-1220 train loss: 0.9054 valid loss: 0.8152, valid accuracy: 0.7954\n",
      "Iter-1230 train loss: 0.8145 valid loss: 0.8115, valid accuracy: 0.7968\n",
      "Iter-1240 train loss: 1.0351 valid loss: 0.8082, valid accuracy: 0.7996\n",
      "Iter-1250 train loss: 0.8505 valid loss: 0.8047, valid accuracy: 0.7994\n",
      "Iter-1260 train loss: 0.8587 valid loss: 0.8018, valid accuracy: 0.7998\n",
      "Iter-1270 train loss: 0.7799 valid loss: 0.7988, valid accuracy: 0.8008\n",
      "Iter-1280 train loss: 0.8107 valid loss: 0.7960, valid accuracy: 0.8012\n",
      "Iter-1290 train loss: 0.9527 valid loss: 0.7926, valid accuracy: 0.8008\n",
      "Iter-1300 train loss: 0.7139 valid loss: 0.7892, valid accuracy: 0.8012\n",
      "Iter-1310 train loss: 0.9051 valid loss: 0.7863, valid accuracy: 0.8024\n",
      "Iter-1320 train loss: 0.9571 valid loss: 0.7833, valid accuracy: 0.8024\n",
      "Iter-1330 train loss: 0.6765 valid loss: 0.7802, valid accuracy: 0.8028\n",
      "Iter-1340 train loss: 0.6746 valid loss: 0.7772, valid accuracy: 0.8044\n",
      "Iter-1350 train loss: 0.7719 valid loss: 0.7743, valid accuracy: 0.8054\n",
      "Iter-1360 train loss: 0.8290 valid loss: 0.7714, valid accuracy: 0.8060\n",
      "Iter-1370 train loss: 0.8254 valid loss: 0.7685, valid accuracy: 0.8054\n",
      "Iter-1380 train loss: 0.7118 valid loss: 0.7659, valid accuracy: 0.8060\n",
      "Iter-1390 train loss: 0.8676 valid loss: 0.7630, valid accuracy: 0.8064\n",
      "Iter-1400 train loss: 0.8563 valid loss: 0.7604, valid accuracy: 0.8074\n",
      "Iter-1410 train loss: 0.9191 valid loss: 0.7577, valid accuracy: 0.8080\n",
      "Iter-1420 train loss: 0.7412 valid loss: 0.7553, valid accuracy: 0.8076\n",
      "Iter-1430 train loss: 1.1109 valid loss: 0.7528, valid accuracy: 0.8074\n",
      "Iter-1440 train loss: 0.7501 valid loss: 0.7502, valid accuracy: 0.8090\n",
      "Iter-1450 train loss: 0.7451 valid loss: 0.7475, valid accuracy: 0.8086\n",
      "Iter-1460 train loss: 0.9102 valid loss: 0.7449, valid accuracy: 0.8096\n",
      "Iter-1470 train loss: 0.8146 valid loss: 0.7426, valid accuracy: 0.8104\n",
      "Iter-1480 train loss: 0.8265 valid loss: 0.7403, valid accuracy: 0.8108\n",
      "Iter-1490 train loss: 0.6593 valid loss: 0.7377, valid accuracy: 0.8114\n",
      "Iter-1500 train loss: 0.8135 valid loss: 0.7352, valid accuracy: 0.8112\n",
      "Iter-1510 train loss: 0.6830 valid loss: 0.7327, valid accuracy: 0.8126\n",
      "Iter-1520 train loss: 0.6034 valid loss: 0.7305, valid accuracy: 0.8134\n",
      "Iter-1530 train loss: 0.7170 valid loss: 0.7283, valid accuracy: 0.8144\n",
      "Iter-1540 train loss: 0.6930 valid loss: 0.7261, valid accuracy: 0.8136\n",
      "Iter-1550 train loss: 0.8374 valid loss: 0.7236, valid accuracy: 0.8156\n",
      "Iter-1560 train loss: 0.7644 valid loss: 0.7210, valid accuracy: 0.8164\n",
      "Iter-1570 train loss: 0.7557 valid loss: 0.7189, valid accuracy: 0.8162\n",
      "Iter-1580 train loss: 0.7464 valid loss: 0.7165, valid accuracy: 0.8176\n",
      "Iter-1590 train loss: 0.8431 valid loss: 0.7142, valid accuracy: 0.8184\n",
      "Iter-1600 train loss: 0.7583 valid loss: 0.7122, valid accuracy: 0.8172\n",
      "Iter-1610 train loss: 0.6392 valid loss: 0.7101, valid accuracy: 0.8180\n",
      "Iter-1620 train loss: 0.8711 valid loss: 0.7077, valid accuracy: 0.8192\n",
      "Iter-1630 train loss: 0.8097 valid loss: 0.7057, valid accuracy: 0.8200\n",
      "Iter-1640 train loss: 0.6184 valid loss: 0.7037, valid accuracy: 0.8204\n",
      "Iter-1650 train loss: 0.7661 valid loss: 0.7015, valid accuracy: 0.8210\n",
      "Iter-1660 train loss: 0.7744 valid loss: 0.6993, valid accuracy: 0.8218\n",
      "Iter-1670 train loss: 0.6712 valid loss: 0.6974, valid accuracy: 0.8204\n",
      "Iter-1680 train loss: 0.7164 valid loss: 0.6956, valid accuracy: 0.8214\n",
      "Iter-1690 train loss: 0.6103 valid loss: 0.6938, valid accuracy: 0.8210\n",
      "Iter-1700 train loss: 0.7379 valid loss: 0.6916, valid accuracy: 0.8218\n",
      "Iter-1710 train loss: 0.6172 valid loss: 0.6897, valid accuracy: 0.8232\n",
      "Iter-1720 train loss: 0.6992 valid loss: 0.6877, valid accuracy: 0.8232\n",
      "Iter-1730 train loss: 0.5581 valid loss: 0.6860, valid accuracy: 0.8226\n",
      "Iter-1740 train loss: 0.8558 valid loss: 0.6844, valid accuracy: 0.8234\n",
      "Iter-1750 train loss: 0.5910 valid loss: 0.6822, valid accuracy: 0.8228\n",
      "Iter-1760 train loss: 0.7414 valid loss: 0.6802, valid accuracy: 0.8244\n",
      "Iter-1770 train loss: 0.6724 valid loss: 0.6782, valid accuracy: 0.8250\n",
      "Iter-1780 train loss: 0.5884 valid loss: 0.6765, valid accuracy: 0.8256\n",
      "Iter-1790 train loss: 0.7356 valid loss: 0.6751, valid accuracy: 0.8258\n",
      "Iter-1800 train loss: 0.6823 valid loss: 0.6736, valid accuracy: 0.8268\n",
      "Iter-1810 train loss: 0.5505 valid loss: 0.6715, valid accuracy: 0.8272\n",
      "Iter-1820 train loss: 0.8457 valid loss: 0.6696, valid accuracy: 0.8280\n",
      "Iter-1830 train loss: 0.6380 valid loss: 0.6678, valid accuracy: 0.8284\n",
      "Iter-1840 train loss: 0.6356 valid loss: 0.6660, valid accuracy: 0.8286\n",
      "Iter-1850 train loss: 0.6613 valid loss: 0.6641, valid accuracy: 0.8288\n",
      "Iter-1860 train loss: 0.5374 valid loss: 0.6620, valid accuracy: 0.8290\n",
      "Iter-1870 train loss: 0.8732 valid loss: 0.6602, valid accuracy: 0.8290\n",
      "Iter-1880 train loss: 0.7632 valid loss: 0.6586, valid accuracy: 0.8302\n",
      "Iter-1890 train loss: 0.6634 valid loss: 0.6570, valid accuracy: 0.8300\n",
      "Iter-1900 train loss: 0.5632 valid loss: 0.6556, valid accuracy: 0.8302\n",
      "Iter-1910 train loss: 0.5571 valid loss: 0.6541, valid accuracy: 0.8302\n",
      "Iter-1920 train loss: 0.6833 valid loss: 0.6523, valid accuracy: 0.8314\n",
      "Iter-1930 train loss: 0.6774 valid loss: 0.6503, valid accuracy: 0.8318\n",
      "Iter-1940 train loss: 0.8195 valid loss: 0.6490, valid accuracy: 0.8328\n",
      "Iter-1950 train loss: 0.6611 valid loss: 0.6472, valid accuracy: 0.8328\n",
      "Iter-1960 train loss: 0.7986 valid loss: 0.6454, valid accuracy: 0.8340\n",
      "Iter-1970 train loss: 0.6274 valid loss: 0.6439, valid accuracy: 0.8338\n",
      "Iter-1980 train loss: 0.7571 valid loss: 0.6421, valid accuracy: 0.8346\n",
      "Iter-1990 train loss: 0.7312 valid loss: 0.6403, valid accuracy: 0.8352\n",
      "Iter-2000 train loss: 0.8608 valid loss: 0.6389, valid accuracy: 0.8358\n",
      "Iter-2010 train loss: 0.6078 valid loss: 0.6373, valid accuracy: 0.8364\n",
      "Iter-2020 train loss: 0.5642 valid loss: 0.6359, valid accuracy: 0.8370\n",
      "Iter-2030 train loss: 0.6270 valid loss: 0.6344, valid accuracy: 0.8358\n",
      "Iter-2040 train loss: 0.7183 valid loss: 0.6328, valid accuracy: 0.8370\n",
      "Iter-2050 train loss: 0.6271 valid loss: 0.6312, valid accuracy: 0.8388\n",
      "Iter-2060 train loss: 0.6747 valid loss: 0.6297, valid accuracy: 0.8388\n",
      "Iter-2070 train loss: 0.5339 valid loss: 0.6284, valid accuracy: 0.8394\n",
      "Iter-2080 train loss: 0.7249 valid loss: 0.6269, valid accuracy: 0.8398\n",
      "Iter-2090 train loss: 0.6076 valid loss: 0.6254, valid accuracy: 0.8396\n",
      "Iter-2100 train loss: 0.5827 valid loss: 0.6240, valid accuracy: 0.8404\n",
      "Iter-2110 train loss: 0.6075 valid loss: 0.6225, valid accuracy: 0.8400\n",
      "Iter-2120 train loss: 0.6409 valid loss: 0.6210, valid accuracy: 0.8412\n",
      "Iter-2130 train loss: 0.6374 valid loss: 0.6196, valid accuracy: 0.8412\n",
      "Iter-2140 train loss: 0.5579 valid loss: 0.6182, valid accuracy: 0.8424\n",
      "Iter-2150 train loss: 0.7322 valid loss: 0.6168, valid accuracy: 0.8426\n",
      "Iter-2160 train loss: 0.8201 valid loss: 0.6154, valid accuracy: 0.8432\n",
      "Iter-2170 train loss: 0.6370 valid loss: 0.6140, valid accuracy: 0.8428\n",
      "Iter-2180 train loss: 0.5169 valid loss: 0.6126, valid accuracy: 0.8424\n",
      "Iter-2190 train loss: 0.6406 valid loss: 0.6112, valid accuracy: 0.8436\n",
      "Iter-2200 train loss: 0.6295 valid loss: 0.6097, valid accuracy: 0.8438\n",
      "Iter-2210 train loss: 0.5457 valid loss: 0.6085, valid accuracy: 0.8432\n",
      "Iter-2220 train loss: 0.5262 valid loss: 0.6071, valid accuracy: 0.8434\n",
      "Iter-2230 train loss: 0.5709 valid loss: 0.6057, valid accuracy: 0.8442\n",
      "Iter-2240 train loss: 0.6121 valid loss: 0.6045, valid accuracy: 0.8444\n",
      "Iter-2250 train loss: 0.7897 valid loss: 0.6034, valid accuracy: 0.8452\n",
      "Iter-2260 train loss: 0.8570 valid loss: 0.6021, valid accuracy: 0.8452\n",
      "Iter-2270 train loss: 0.7529 valid loss: 0.6008, valid accuracy: 0.8456\n",
      "Iter-2280 train loss: 0.4341 valid loss: 0.5995, valid accuracy: 0.8456\n",
      "Iter-2290 train loss: 0.5540 valid loss: 0.5982, valid accuracy: 0.8460\n",
      "Iter-2300 train loss: 0.4950 valid loss: 0.5971, valid accuracy: 0.8460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 0.5729 valid loss: 0.5959, valid accuracy: 0.8454\n",
      "Iter-2320 train loss: 0.4991 valid loss: 0.5949, valid accuracy: 0.8468\n",
      "Iter-2330 train loss: 0.6844 valid loss: 0.5936, valid accuracy: 0.8462\n",
      "Iter-2340 train loss: 0.5224 valid loss: 0.5924, valid accuracy: 0.8466\n",
      "Iter-2350 train loss: 0.7404 valid loss: 0.5912, valid accuracy: 0.8474\n",
      "Iter-2360 train loss: 0.4724 valid loss: 0.5899, valid accuracy: 0.8476\n",
      "Iter-2370 train loss: 0.7419 valid loss: 0.5888, valid accuracy: 0.8478\n",
      "Iter-2380 train loss: 0.6788 valid loss: 0.5876, valid accuracy: 0.8486\n",
      "Iter-2390 train loss: 0.8313 valid loss: 0.5866, valid accuracy: 0.8484\n",
      "Iter-2400 train loss: 0.4749 valid loss: 0.5854, valid accuracy: 0.8490\n",
      "Iter-2410 train loss: 0.5285 valid loss: 0.5842, valid accuracy: 0.8480\n",
      "Iter-2420 train loss: 0.5824 valid loss: 0.5831, valid accuracy: 0.8494\n",
      "Iter-2430 train loss: 0.7806 valid loss: 0.5820, valid accuracy: 0.8496\n",
      "Iter-2440 train loss: 0.5002 valid loss: 0.5808, valid accuracy: 0.8500\n",
      "Iter-2450 train loss: 0.5580 valid loss: 0.5797, valid accuracy: 0.8494\n",
      "Iter-2460 train loss: 0.7571 valid loss: 0.5786, valid accuracy: 0.8504\n",
      "Iter-2470 train loss: 0.6542 valid loss: 0.5775, valid accuracy: 0.8502\n",
      "Iter-2480 train loss: 0.6403 valid loss: 0.5766, valid accuracy: 0.8500\n",
      "Iter-2490 train loss: 0.6932 valid loss: 0.5755, valid accuracy: 0.8502\n",
      "Iter-2500 train loss: 0.7006 valid loss: 0.5743, valid accuracy: 0.8500\n",
      "Iter-2510 train loss: 0.5613 valid loss: 0.5734, valid accuracy: 0.8506\n",
      "Iter-2520 train loss: 0.4811 valid loss: 0.5724, valid accuracy: 0.8510\n",
      "Iter-2530 train loss: 0.5637 valid loss: 0.5713, valid accuracy: 0.8508\n",
      "Iter-2540 train loss: 0.5681 valid loss: 0.5702, valid accuracy: 0.8504\n",
      "Iter-2550 train loss: 0.4941 valid loss: 0.5690, valid accuracy: 0.8510\n",
      "Iter-2560 train loss: 0.5772 valid loss: 0.5680, valid accuracy: 0.8510\n",
      "Iter-2570 train loss: 1.0102 valid loss: 0.5670, valid accuracy: 0.8520\n",
      "Iter-2580 train loss: 0.6270 valid loss: 0.5662, valid accuracy: 0.8516\n",
      "Iter-2590 train loss: 0.4751 valid loss: 0.5651, valid accuracy: 0.8520\n",
      "Iter-2600 train loss: 0.4826 valid loss: 0.5642, valid accuracy: 0.8520\n",
      "Iter-2610 train loss: 0.5601 valid loss: 0.5632, valid accuracy: 0.8530\n",
      "Iter-2620 train loss: 0.5751 valid loss: 0.5623, valid accuracy: 0.8530\n",
      "Iter-2630 train loss: 0.5826 valid loss: 0.5612, valid accuracy: 0.8528\n",
      "Iter-2640 train loss: 0.5790 valid loss: 0.5600, valid accuracy: 0.8540\n",
      "Iter-2650 train loss: 0.6526 valid loss: 0.5590, valid accuracy: 0.8536\n",
      "Iter-2660 train loss: 0.5680 valid loss: 0.5584, valid accuracy: 0.8542\n",
      "Iter-2670 train loss: 0.5843 valid loss: 0.5574, valid accuracy: 0.8546\n",
      "Iter-2680 train loss: 0.6100 valid loss: 0.5566, valid accuracy: 0.8544\n",
      "Iter-2690 train loss: 0.7519 valid loss: 0.5557, valid accuracy: 0.8544\n",
      "Iter-2700 train loss: 0.4790 valid loss: 0.5547, valid accuracy: 0.8546\n",
      "Iter-2710 train loss: 0.6193 valid loss: 0.5536, valid accuracy: 0.8550\n",
      "Iter-2720 train loss: 0.7379 valid loss: 0.5528, valid accuracy: 0.8558\n",
      "Iter-2730 train loss: 0.5372 valid loss: 0.5520, valid accuracy: 0.8554\n",
      "Iter-2740 train loss: 0.4334 valid loss: 0.5513, valid accuracy: 0.8558\n",
      "Iter-2750 train loss: 0.5307 valid loss: 0.5502, valid accuracy: 0.8560\n",
      "Iter-2760 train loss: 0.4809 valid loss: 0.5491, valid accuracy: 0.8564\n",
      "Iter-2770 train loss: 0.5113 valid loss: 0.5483, valid accuracy: 0.8564\n",
      "Iter-2780 train loss: 0.7063 valid loss: 0.5474, valid accuracy: 0.8562\n",
      "Iter-2790 train loss: 0.6380 valid loss: 0.5467, valid accuracy: 0.8572\n",
      "Iter-2800 train loss: 0.5966 valid loss: 0.5457, valid accuracy: 0.8568\n",
      "Iter-2810 train loss: 0.5302 valid loss: 0.5449, valid accuracy: 0.8568\n",
      "Iter-2820 train loss: 0.5014 valid loss: 0.5441, valid accuracy: 0.8570\n",
      "Iter-2830 train loss: 0.6471 valid loss: 0.5431, valid accuracy: 0.8572\n",
      "Iter-2840 train loss: 0.4607 valid loss: 0.5422, valid accuracy: 0.8574\n",
      "Iter-2850 train loss: 0.6303 valid loss: 0.5414, valid accuracy: 0.8574\n",
      "Iter-2860 train loss: 0.5212 valid loss: 0.5406, valid accuracy: 0.8572\n",
      "Iter-2870 train loss: 0.8333 valid loss: 0.5397, valid accuracy: 0.8576\n",
      "Iter-2880 train loss: 0.4919 valid loss: 0.5389, valid accuracy: 0.8586\n",
      "Iter-2890 train loss: 0.4884 valid loss: 0.5383, valid accuracy: 0.8580\n",
      "Iter-2900 train loss: 0.5573 valid loss: 0.5375, valid accuracy: 0.8592\n",
      "Iter-2910 train loss: 0.5453 valid loss: 0.5369, valid accuracy: 0.8586\n",
      "Iter-2920 train loss: 0.5196 valid loss: 0.5362, valid accuracy: 0.8586\n",
      "Iter-2930 train loss: 0.6191 valid loss: 0.5354, valid accuracy: 0.8598\n",
      "Iter-2940 train loss: 0.4581 valid loss: 0.5345, valid accuracy: 0.8606\n",
      "Iter-2950 train loss: 0.5052 valid loss: 0.5338, valid accuracy: 0.8610\n",
      "Iter-2960 train loss: 0.5242 valid loss: 0.5329, valid accuracy: 0.8600\n",
      "Iter-2970 train loss: 0.5656 valid loss: 0.5322, valid accuracy: 0.8606\n",
      "Iter-2980 train loss: 0.5212 valid loss: 0.5314, valid accuracy: 0.8612\n",
      "Iter-2990 train loss: 0.6759 valid loss: 0.5307, valid accuracy: 0.8602\n",
      "Iter-3000 train loss: 0.7043 valid loss: 0.5301, valid accuracy: 0.8596\n",
      "Iter-3010 train loss: 0.5071 valid loss: 0.5291, valid accuracy: 0.8614\n",
      "Iter-3020 train loss: 0.6465 valid loss: 0.5284, valid accuracy: 0.8616\n",
      "Iter-3030 train loss: 0.5418 valid loss: 0.5276, valid accuracy: 0.8610\n",
      "Iter-3040 train loss: 0.4255 valid loss: 0.5270, valid accuracy: 0.8616\n",
      "Iter-3050 train loss: 0.4957 valid loss: 0.5262, valid accuracy: 0.8614\n",
      "Iter-3060 train loss: 0.5771 valid loss: 0.5256, valid accuracy: 0.8622\n",
      "Iter-3070 train loss: 0.4671 valid loss: 0.5250, valid accuracy: 0.8618\n",
      "Iter-3080 train loss: 0.6202 valid loss: 0.5240, valid accuracy: 0.8618\n",
      "Iter-3090 train loss: 0.6524 valid loss: 0.5231, valid accuracy: 0.8614\n",
      "Iter-3100 train loss: 0.4705 valid loss: 0.5225, valid accuracy: 0.8616\n",
      "Iter-3110 train loss: 0.4386 valid loss: 0.5218, valid accuracy: 0.8632\n",
      "Iter-3120 train loss: 0.5806 valid loss: 0.5211, valid accuracy: 0.8628\n",
      "Iter-3130 train loss: 0.6767 valid loss: 0.5205, valid accuracy: 0.8640\n",
      "Iter-3140 train loss: 0.4882 valid loss: 0.5198, valid accuracy: 0.8632\n",
      "Iter-3150 train loss: 0.4838 valid loss: 0.5191, valid accuracy: 0.8638\n",
      "Iter-3160 train loss: 0.6556 valid loss: 0.5182, valid accuracy: 0.8632\n",
      "Iter-3170 train loss: 0.4592 valid loss: 0.5175, valid accuracy: 0.8630\n",
      "Iter-3180 train loss: 0.4404 valid loss: 0.5167, valid accuracy: 0.8628\n",
      "Iter-3190 train loss: 0.5662 valid loss: 0.5161, valid accuracy: 0.8640\n",
      "Iter-3200 train loss: 0.6404 valid loss: 0.5154, valid accuracy: 0.8630\n",
      "Iter-3210 train loss: 0.4863 valid loss: 0.5147, valid accuracy: 0.8632\n",
      "Iter-3220 train loss: 0.4928 valid loss: 0.5141, valid accuracy: 0.8638\n",
      "Iter-3230 train loss: 0.5620 valid loss: 0.5134, valid accuracy: 0.8644\n",
      "Iter-3240 train loss: 0.5675 valid loss: 0.5129, valid accuracy: 0.8640\n",
      "Iter-3250 train loss: 0.5094 valid loss: 0.5123, valid accuracy: 0.8644\n",
      "Iter-3260 train loss: 0.5439 valid loss: 0.5115, valid accuracy: 0.8642\n",
      "Iter-3270 train loss: 0.5018 valid loss: 0.5108, valid accuracy: 0.8648\n",
      "Iter-3280 train loss: 0.3511 valid loss: 0.5102, valid accuracy: 0.8638\n",
      "Iter-3290 train loss: 0.5415 valid loss: 0.5095, valid accuracy: 0.8644\n",
      "Iter-3300 train loss: 0.6437 valid loss: 0.5089, valid accuracy: 0.8654\n",
      "Iter-3310 train loss: 0.4717 valid loss: 0.5081, valid accuracy: 0.8646\n",
      "Iter-3320 train loss: 0.3474 valid loss: 0.5075, valid accuracy: 0.8652\n",
      "Iter-3330 train loss: 0.5497 valid loss: 0.5070, valid accuracy: 0.8662\n",
      "Iter-3340 train loss: 0.4019 valid loss: 0.5064, valid accuracy: 0.8656\n",
      "Iter-3350 train loss: 0.6318 valid loss: 0.5056, valid accuracy: 0.8662\n",
      "Iter-3360 train loss: 0.5581 valid loss: 0.5051, valid accuracy: 0.8656\n",
      "Iter-3370 train loss: 0.5382 valid loss: 0.5045, valid accuracy: 0.8660\n",
      "Iter-3380 train loss: 0.6413 valid loss: 0.5038, valid accuracy: 0.8664\n",
      "Iter-3390 train loss: 0.5748 valid loss: 0.5032, valid accuracy: 0.8662\n",
      "Iter-3400 train loss: 0.4537 valid loss: 0.5025, valid accuracy: 0.8666\n",
      "Iter-3410 train loss: 0.7090 valid loss: 0.5019, valid accuracy: 0.8668\n",
      "Iter-3420 train loss: 0.4707 valid loss: 0.5012, valid accuracy: 0.8666\n",
      "Iter-3430 train loss: 0.5195 valid loss: 0.5005, valid accuracy: 0.8670\n",
      "Iter-3440 train loss: 0.5966 valid loss: 0.4998, valid accuracy: 0.8672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 0.5933 valid loss: 0.4992, valid accuracy: 0.8672\n",
      "Iter-3460 train loss: 0.9209 valid loss: 0.4987, valid accuracy: 0.8678\n",
      "Iter-3470 train loss: 0.3888 valid loss: 0.4982, valid accuracy: 0.8672\n",
      "Iter-3480 train loss: 0.5227 valid loss: 0.4975, valid accuracy: 0.8676\n",
      "Iter-3490 train loss: 0.5533 valid loss: 0.4971, valid accuracy: 0.8684\n",
      "Iter-3500 train loss: 0.2860 valid loss: 0.4965, valid accuracy: 0.8686\n",
      "Iter-3510 train loss: 0.6362 valid loss: 0.4958, valid accuracy: 0.8688\n",
      "Iter-3520 train loss: 0.6141 valid loss: 0.4953, valid accuracy: 0.8686\n",
      "Iter-3530 train loss: 0.3891 valid loss: 0.4948, valid accuracy: 0.8690\n",
      "Iter-3540 train loss: 0.5801 valid loss: 0.4943, valid accuracy: 0.8694\n",
      "Iter-3550 train loss: 0.5411 valid loss: 0.4936, valid accuracy: 0.8698\n",
      "Iter-3560 train loss: 0.5781 valid loss: 0.4931, valid accuracy: 0.8690\n",
      "Iter-3570 train loss: 0.4227 valid loss: 0.4925, valid accuracy: 0.8696\n",
      "Iter-3580 train loss: 0.5635 valid loss: 0.4919, valid accuracy: 0.8702\n",
      "Iter-3590 train loss: 0.4527 valid loss: 0.4913, valid accuracy: 0.8706\n",
      "Iter-3600 train loss: 0.5312 valid loss: 0.4907, valid accuracy: 0.8706\n",
      "Iter-3610 train loss: 0.5066 valid loss: 0.4900, valid accuracy: 0.8700\n",
      "Iter-3620 train loss: 0.5568 valid loss: 0.4896, valid accuracy: 0.8710\n",
      "Iter-3630 train loss: 0.4760 valid loss: 0.4889, valid accuracy: 0.8700\n",
      "Iter-3640 train loss: 0.3876 valid loss: 0.4884, valid accuracy: 0.8702\n",
      "Iter-3650 train loss: 0.3070 valid loss: 0.4879, valid accuracy: 0.8710\n",
      "Iter-3660 train loss: 0.5414 valid loss: 0.4875, valid accuracy: 0.8708\n",
      "Iter-3670 train loss: 0.4688 valid loss: 0.4869, valid accuracy: 0.8710\n",
      "Iter-3680 train loss: 0.7576 valid loss: 0.4864, valid accuracy: 0.8704\n",
      "Iter-3690 train loss: 0.3679 valid loss: 0.4860, valid accuracy: 0.8708\n",
      "Iter-3700 train loss: 0.5148 valid loss: 0.4856, valid accuracy: 0.8712\n",
      "Iter-3710 train loss: 0.4071 valid loss: 0.4850, valid accuracy: 0.8718\n",
      "Iter-3720 train loss: 0.6860 valid loss: 0.4845, valid accuracy: 0.8712\n",
      "Iter-3730 train loss: 0.4449 valid loss: 0.4842, valid accuracy: 0.8704\n",
      "Iter-3740 train loss: 0.5375 valid loss: 0.4835, valid accuracy: 0.8708\n",
      "Iter-3750 train loss: 0.3232 valid loss: 0.4828, valid accuracy: 0.8716\n",
      "Iter-3760 train loss: 0.5396 valid loss: 0.4821, valid accuracy: 0.8714\n",
      "Iter-3770 train loss: 0.3530 valid loss: 0.4818, valid accuracy: 0.8710\n",
      "Iter-3780 train loss: 0.5204 valid loss: 0.4813, valid accuracy: 0.8716\n",
      "Iter-3790 train loss: 0.3532 valid loss: 0.4810, valid accuracy: 0.8722\n",
      "Iter-3800 train loss: 0.4714 valid loss: 0.4802, valid accuracy: 0.8726\n",
      "Iter-3810 train loss: 0.5635 valid loss: 0.4796, valid accuracy: 0.8720\n",
      "Iter-3820 train loss: 0.4964 valid loss: 0.4790, valid accuracy: 0.8722\n",
      "Iter-3830 train loss: 0.4739 valid loss: 0.4785, valid accuracy: 0.8724\n",
      "Iter-3840 train loss: 0.5822 valid loss: 0.4782, valid accuracy: 0.8728\n",
      "Iter-3850 train loss: 0.7375 valid loss: 0.4778, valid accuracy: 0.8738\n",
      "Iter-3860 train loss: 0.3879 valid loss: 0.4773, valid accuracy: 0.8734\n",
      "Iter-3870 train loss: 0.4067 valid loss: 0.4769, valid accuracy: 0.8734\n",
      "Iter-3880 train loss: 0.5249 valid loss: 0.4763, valid accuracy: 0.8740\n",
      "Iter-3890 train loss: 0.4678 valid loss: 0.4759, valid accuracy: 0.8744\n",
      "Iter-3900 train loss: 0.5599 valid loss: 0.4754, valid accuracy: 0.8740\n",
      "Iter-3910 train loss: 0.3409 valid loss: 0.4751, valid accuracy: 0.8742\n",
      "Iter-3920 train loss: 0.6203 valid loss: 0.4745, valid accuracy: 0.8744\n",
      "Iter-3930 train loss: 0.7157 valid loss: 0.4738, valid accuracy: 0.8748\n",
      "Iter-3940 train loss: 0.4287 valid loss: 0.4733, valid accuracy: 0.8750\n",
      "Iter-3950 train loss: 0.4478 valid loss: 0.4730, valid accuracy: 0.8744\n",
      "Iter-3960 train loss: 0.4315 valid loss: 0.4725, valid accuracy: 0.8744\n",
      "Iter-3970 train loss: 0.4093 valid loss: 0.4719, valid accuracy: 0.8744\n",
      "Iter-3980 train loss: 0.3824 valid loss: 0.4715, valid accuracy: 0.8742\n",
      "Iter-3990 train loss: 0.3896 valid loss: 0.4711, valid accuracy: 0.8746\n",
      "Iter-4000 train loss: 0.5302 valid loss: 0.4705, valid accuracy: 0.8746\n",
      "Iter-4010 train loss: 0.4829 valid loss: 0.4701, valid accuracy: 0.8746\n",
      "Iter-4020 train loss: 0.5867 valid loss: 0.4696, valid accuracy: 0.8740\n",
      "Iter-4030 train loss: 0.5418 valid loss: 0.4690, valid accuracy: 0.8750\n",
      "Iter-4040 train loss: 0.4755 valid loss: 0.4684, valid accuracy: 0.8748\n",
      "Iter-4050 train loss: 0.4709 valid loss: 0.4679, valid accuracy: 0.8754\n",
      "Iter-4060 train loss: 0.4182 valid loss: 0.4674, valid accuracy: 0.8756\n",
      "Iter-4070 train loss: 0.5680 valid loss: 0.4670, valid accuracy: 0.8758\n",
      "Iter-4080 train loss: 0.3311 valid loss: 0.4665, valid accuracy: 0.8756\n",
      "Iter-4090 train loss: 0.6090 valid loss: 0.4661, valid accuracy: 0.8748\n",
      "Iter-4100 train loss: 0.2704 valid loss: 0.4658, valid accuracy: 0.8748\n",
      "Iter-4110 train loss: 0.4187 valid loss: 0.4654, valid accuracy: 0.8752\n",
      "Iter-4120 train loss: 0.5152 valid loss: 0.4648, valid accuracy: 0.8756\n",
      "Iter-4130 train loss: 0.5572 valid loss: 0.4644, valid accuracy: 0.8756\n",
      "Iter-4140 train loss: 0.4278 valid loss: 0.4639, valid accuracy: 0.8758\n",
      "Iter-4150 train loss: 0.4057 valid loss: 0.4633, valid accuracy: 0.8764\n",
      "Iter-4160 train loss: 0.4607 valid loss: 0.4629, valid accuracy: 0.8762\n",
      "Iter-4170 train loss: 0.6538 valid loss: 0.4625, valid accuracy: 0.8758\n",
      "Iter-4180 train loss: 0.4075 valid loss: 0.4620, valid accuracy: 0.8758\n",
      "Iter-4190 train loss: 0.5368 valid loss: 0.4616, valid accuracy: 0.8758\n",
      "Iter-4200 train loss: 0.6060 valid loss: 0.4612, valid accuracy: 0.8758\n",
      "Iter-4210 train loss: 0.5283 valid loss: 0.4609, valid accuracy: 0.8758\n",
      "Iter-4220 train loss: 0.3282 valid loss: 0.4604, valid accuracy: 0.8758\n",
      "Iter-4230 train loss: 0.3967 valid loss: 0.4602, valid accuracy: 0.8766\n",
      "Iter-4240 train loss: 0.3553 valid loss: 0.4596, valid accuracy: 0.8774\n",
      "Iter-4250 train loss: 0.3219 valid loss: 0.4592, valid accuracy: 0.8768\n",
      "Iter-4260 train loss: 0.5120 valid loss: 0.4587, valid accuracy: 0.8770\n",
      "Iter-4270 train loss: 0.5480 valid loss: 0.4583, valid accuracy: 0.8766\n",
      "Iter-4280 train loss: 0.5224 valid loss: 0.4579, valid accuracy: 0.8764\n",
      "Iter-4290 train loss: 0.4997 valid loss: 0.4574, valid accuracy: 0.8768\n",
      "Iter-4300 train loss: 0.5542 valid loss: 0.4569, valid accuracy: 0.8772\n",
      "Iter-4310 train loss: 0.3040 valid loss: 0.4564, valid accuracy: 0.8776\n",
      "Iter-4320 train loss: 0.3332 valid loss: 0.4560, valid accuracy: 0.8778\n",
      "Iter-4330 train loss: 0.4299 valid loss: 0.4557, valid accuracy: 0.8778\n",
      "Iter-4340 train loss: 0.3913 valid loss: 0.4552, valid accuracy: 0.8782\n",
      "Iter-4350 train loss: 0.5044 valid loss: 0.4548, valid accuracy: 0.8776\n",
      "Iter-4360 train loss: 0.3195 valid loss: 0.4546, valid accuracy: 0.8776\n",
      "Iter-4370 train loss: 0.3719 valid loss: 0.4542, valid accuracy: 0.8782\n",
      "Iter-4380 train loss: 0.4268 valid loss: 0.4538, valid accuracy: 0.8786\n",
      "Iter-4390 train loss: 0.4012 valid loss: 0.4534, valid accuracy: 0.8786\n",
      "Iter-4400 train loss: 0.5536 valid loss: 0.4532, valid accuracy: 0.8780\n",
      "Iter-4410 train loss: 0.6091 valid loss: 0.4525, valid accuracy: 0.8786\n",
      "Iter-4420 train loss: 0.3969 valid loss: 0.4521, valid accuracy: 0.8788\n",
      "Iter-4430 train loss: 0.3996 valid loss: 0.4518, valid accuracy: 0.8780\n",
      "Iter-4440 train loss: 0.4215 valid loss: 0.4515, valid accuracy: 0.8784\n",
      "Iter-4450 train loss: 0.2733 valid loss: 0.4511, valid accuracy: 0.8782\n",
      "Iter-4460 train loss: 0.4835 valid loss: 0.4506, valid accuracy: 0.8786\n",
      "Iter-4470 train loss: 0.4010 valid loss: 0.4503, valid accuracy: 0.8784\n",
      "Iter-4480 train loss: 0.5538 valid loss: 0.4499, valid accuracy: 0.8784\n",
      "Iter-4490 train loss: 0.5297 valid loss: 0.4496, valid accuracy: 0.8788\n",
      "Iter-4500 train loss: 0.5053 valid loss: 0.4493, valid accuracy: 0.8788\n",
      "Iter-4510 train loss: 0.4303 valid loss: 0.4490, valid accuracy: 0.8788\n",
      "Iter-4520 train loss: 0.3584 valid loss: 0.4484, valid accuracy: 0.8790\n",
      "Iter-4530 train loss: 0.4624 valid loss: 0.4481, valid accuracy: 0.8794\n",
      "Iter-4540 train loss: 0.4616 valid loss: 0.4476, valid accuracy: 0.8790\n",
      "Iter-4550 train loss: 0.4087 valid loss: 0.4472, valid accuracy: 0.8796\n",
      "Iter-4560 train loss: 0.3994 valid loss: 0.4469, valid accuracy: 0.8794\n",
      "Iter-4570 train loss: 0.4229 valid loss: 0.4466, valid accuracy: 0.8798\n",
      "Iter-4580 train loss: 0.4638 valid loss: 0.4463, valid accuracy: 0.8790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 0.5484 valid loss: 0.4461, valid accuracy: 0.8792\n",
      "Iter-4600 train loss: 0.4817 valid loss: 0.4458, valid accuracy: 0.8790\n",
      "Iter-4610 train loss: 0.5832 valid loss: 0.4455, valid accuracy: 0.8792\n",
      "Iter-4620 train loss: 0.4654 valid loss: 0.4451, valid accuracy: 0.8784\n",
      "Iter-4630 train loss: 0.4614 valid loss: 0.4446, valid accuracy: 0.8796\n",
      "Iter-4640 train loss: 0.7067 valid loss: 0.4442, valid accuracy: 0.8794\n",
      "Iter-4650 train loss: 0.3707 valid loss: 0.4441, valid accuracy: 0.8794\n",
      "Iter-4660 train loss: 0.3053 valid loss: 0.4438, valid accuracy: 0.8792\n",
      "Iter-4670 train loss: 0.3453 valid loss: 0.4436, valid accuracy: 0.8790\n",
      "Iter-4680 train loss: 0.4491 valid loss: 0.4433, valid accuracy: 0.8796\n",
      "Iter-4690 train loss: 0.4298 valid loss: 0.4428, valid accuracy: 0.8800\n",
      "Iter-4700 train loss: 0.3987 valid loss: 0.4424, valid accuracy: 0.8798\n",
      "Iter-4710 train loss: 0.4139 valid loss: 0.4420, valid accuracy: 0.8800\n",
      "Iter-4720 train loss: 0.4347 valid loss: 0.4416, valid accuracy: 0.8796\n",
      "Iter-4730 train loss: 0.4276 valid loss: 0.4412, valid accuracy: 0.8798\n",
      "Iter-4740 train loss: 0.6386 valid loss: 0.4409, valid accuracy: 0.8794\n",
      "Iter-4750 train loss: 0.3873 valid loss: 0.4405, valid accuracy: 0.8800\n",
      "Iter-4760 train loss: 0.4348 valid loss: 0.4401, valid accuracy: 0.8800\n",
      "Iter-4770 train loss: 0.3696 valid loss: 0.4398, valid accuracy: 0.8794\n",
      "Iter-4780 train loss: 0.3737 valid loss: 0.4394, valid accuracy: 0.8794\n",
      "Iter-4790 train loss: 0.3906 valid loss: 0.4390, valid accuracy: 0.8802\n",
      "Iter-4800 train loss: 0.4156 valid loss: 0.4386, valid accuracy: 0.8806\n",
      "Iter-4810 train loss: 0.2929 valid loss: 0.4383, valid accuracy: 0.8800\n",
      "Iter-4820 train loss: 0.3461 valid loss: 0.4378, valid accuracy: 0.8806\n",
      "Iter-4830 train loss: 0.6647 valid loss: 0.4377, valid accuracy: 0.8798\n",
      "Iter-4840 train loss: 0.3759 valid loss: 0.4374, valid accuracy: 0.8804\n",
      "Iter-4850 train loss: 0.3594 valid loss: 0.4370, valid accuracy: 0.8804\n",
      "Iter-4860 train loss: 0.3996 valid loss: 0.4367, valid accuracy: 0.8800\n",
      "Iter-4870 train loss: 0.5212 valid loss: 0.4364, valid accuracy: 0.8798\n",
      "Iter-4880 train loss: 0.4335 valid loss: 0.4359, valid accuracy: 0.8802\n",
      "Iter-4890 train loss: 0.4078 valid loss: 0.4357, valid accuracy: 0.8804\n",
      "Iter-4900 train loss: 0.5034 valid loss: 0.4353, valid accuracy: 0.8802\n",
      "Iter-4910 train loss: 0.4494 valid loss: 0.4350, valid accuracy: 0.8806\n",
      "Iter-4920 train loss: 0.5764 valid loss: 0.4346, valid accuracy: 0.8806\n",
      "Iter-4930 train loss: 0.5148 valid loss: 0.4344, valid accuracy: 0.8808\n",
      "Iter-4940 train loss: 0.4823 valid loss: 0.4340, valid accuracy: 0.8808\n",
      "Iter-4950 train loss: 0.4723 valid loss: 0.4337, valid accuracy: 0.8804\n",
      "Iter-4960 train loss: 0.2733 valid loss: 0.4334, valid accuracy: 0.8818\n",
      "Iter-4970 train loss: 0.4846 valid loss: 0.4332, valid accuracy: 0.8814\n",
      "Iter-4980 train loss: 0.5035 valid loss: 0.4328, valid accuracy: 0.8810\n",
      "Iter-4990 train loss: 0.4216 valid loss: 0.4325, valid accuracy: 0.8812\n",
      "Iter-5000 train loss: 0.5833 valid loss: 0.4321, valid accuracy: 0.8810\n",
      "Iter-5010 train loss: 0.3273 valid loss: 0.4319, valid accuracy: 0.8808\n",
      "Iter-5020 train loss: 0.2845 valid loss: 0.4316, valid accuracy: 0.8808\n",
      "Iter-5030 train loss: 0.3677 valid loss: 0.4312, valid accuracy: 0.8816\n",
      "Iter-5040 train loss: 0.5344 valid loss: 0.4309, valid accuracy: 0.8810\n",
      "Iter-5050 train loss: 0.3564 valid loss: 0.4306, valid accuracy: 0.8814\n",
      "Iter-5060 train loss: 0.5817 valid loss: 0.4303, valid accuracy: 0.8810\n",
      "Iter-5070 train loss: 0.6090 valid loss: 0.4301, valid accuracy: 0.8812\n",
      "Iter-5080 train loss: 0.3574 valid loss: 0.4298, valid accuracy: 0.8814\n",
      "Iter-5090 train loss: 0.4223 valid loss: 0.4295, valid accuracy: 0.8812\n",
      "Iter-5100 train loss: 0.3867 valid loss: 0.4293, valid accuracy: 0.8814\n",
      "Iter-5110 train loss: 0.3264 valid loss: 0.4290, valid accuracy: 0.8814\n",
      "Iter-5120 train loss: 0.6078 valid loss: 0.4289, valid accuracy: 0.8812\n",
      "Iter-5130 train loss: 0.4914 valid loss: 0.4286, valid accuracy: 0.8810\n",
      "Iter-5140 train loss: 0.4705 valid loss: 0.4283, valid accuracy: 0.8810\n",
      "Iter-5150 train loss: 0.4815 valid loss: 0.4279, valid accuracy: 0.8812\n",
      "Iter-5160 train loss: 0.3489 valid loss: 0.4275, valid accuracy: 0.8814\n",
      "Iter-5170 train loss: 0.4979 valid loss: 0.4272, valid accuracy: 0.8806\n",
      "Iter-5180 train loss: 0.4266 valid loss: 0.4270, valid accuracy: 0.8816\n",
      "Iter-5190 train loss: 0.4309 valid loss: 0.4269, valid accuracy: 0.8812\n",
      "Iter-5200 train loss: 0.4087 valid loss: 0.4267, valid accuracy: 0.8808\n",
      "Iter-5210 train loss: 0.4799 valid loss: 0.4264, valid accuracy: 0.8814\n",
      "Iter-5220 train loss: 0.2579 valid loss: 0.4263, valid accuracy: 0.8814\n",
      "Iter-5230 train loss: 0.3988 valid loss: 0.4261, valid accuracy: 0.8818\n",
      "Iter-5240 train loss: 0.4993 valid loss: 0.4258, valid accuracy: 0.8814\n",
      "Iter-5250 train loss: 0.4736 valid loss: 0.4253, valid accuracy: 0.8820\n",
      "Iter-5260 train loss: 0.4308 valid loss: 0.4251, valid accuracy: 0.8816\n",
      "Iter-5270 train loss: 0.3969 valid loss: 0.4247, valid accuracy: 0.8820\n",
      "Iter-5280 train loss: 0.7811 valid loss: 0.4245, valid accuracy: 0.8826\n",
      "Iter-5290 train loss: 0.4799 valid loss: 0.4243, valid accuracy: 0.8834\n",
      "Iter-5300 train loss: 0.3248 valid loss: 0.4240, valid accuracy: 0.8830\n",
      "Iter-5310 train loss: 0.4047 valid loss: 0.4236, valid accuracy: 0.8828\n",
      "Iter-5320 train loss: 0.6025 valid loss: 0.4232, valid accuracy: 0.8828\n",
      "Iter-5330 train loss: 0.4986 valid loss: 0.4230, valid accuracy: 0.8828\n",
      "Iter-5340 train loss: 0.2629 valid loss: 0.4228, valid accuracy: 0.8830\n",
      "Iter-5350 train loss: 0.3948 valid loss: 0.4226, valid accuracy: 0.8828\n",
      "Iter-5360 train loss: 0.3163 valid loss: 0.4223, valid accuracy: 0.8834\n",
      "Iter-5370 train loss: 0.4813 valid loss: 0.4220, valid accuracy: 0.8832\n",
      "Iter-5380 train loss: 0.3476 valid loss: 0.4217, valid accuracy: 0.8830\n",
      "Iter-5390 train loss: 0.4097 valid loss: 0.4214, valid accuracy: 0.8834\n",
      "Iter-5400 train loss: 0.3916 valid loss: 0.4211, valid accuracy: 0.8836\n",
      "Iter-5410 train loss: 0.3273 valid loss: 0.4208, valid accuracy: 0.8836\n",
      "Iter-5420 train loss: 0.5040 valid loss: 0.4208, valid accuracy: 0.8832\n",
      "Iter-5430 train loss: 0.2799 valid loss: 0.4205, valid accuracy: 0.8834\n",
      "Iter-5440 train loss: 0.4022 valid loss: 0.4200, valid accuracy: 0.8832\n",
      "Iter-5450 train loss: 0.4916 valid loss: 0.4197, valid accuracy: 0.8834\n",
      "Iter-5460 train loss: 0.4691 valid loss: 0.4194, valid accuracy: 0.8832\n",
      "Iter-5470 train loss: 0.5635 valid loss: 0.4191, valid accuracy: 0.8836\n",
      "Iter-5480 train loss: 0.3284 valid loss: 0.4189, valid accuracy: 0.8838\n",
      "Iter-5490 train loss: 0.3834 valid loss: 0.4187, valid accuracy: 0.8832\n",
      "Iter-5500 train loss: 0.3826 valid loss: 0.4185, valid accuracy: 0.8838\n",
      "Iter-5510 train loss: 0.3762 valid loss: 0.4181, valid accuracy: 0.8838\n",
      "Iter-5520 train loss: 0.3998 valid loss: 0.4178, valid accuracy: 0.8840\n",
      "Iter-5530 train loss: 0.5198 valid loss: 0.4174, valid accuracy: 0.8836\n",
      "Iter-5540 train loss: 0.2103 valid loss: 0.4172, valid accuracy: 0.8838\n",
      "Iter-5550 train loss: 0.4116 valid loss: 0.4168, valid accuracy: 0.8840\n",
      "Iter-5560 train loss: 0.5036 valid loss: 0.4165, valid accuracy: 0.8844\n",
      "Iter-5570 train loss: 0.3802 valid loss: 0.4162, valid accuracy: 0.8838\n",
      "Iter-5580 train loss: 0.3101 valid loss: 0.4159, valid accuracy: 0.8842\n",
      "Iter-5590 train loss: 0.4017 valid loss: 0.4156, valid accuracy: 0.8842\n",
      "Iter-5600 train loss: 0.4526 valid loss: 0.4153, valid accuracy: 0.8844\n",
      "Iter-5610 train loss: 0.3832 valid loss: 0.4151, valid accuracy: 0.8850\n",
      "Iter-5620 train loss: 0.4408 valid loss: 0.4148, valid accuracy: 0.8842\n",
      "Iter-5630 train loss: 0.3799 valid loss: 0.4146, valid accuracy: 0.8844\n",
      "Iter-5640 train loss: 0.3187 valid loss: 0.4146, valid accuracy: 0.8858\n",
      "Iter-5650 train loss: 0.3682 valid loss: 0.4143, valid accuracy: 0.8854\n",
      "Iter-5660 train loss: 0.6133 valid loss: 0.4140, valid accuracy: 0.8848\n",
      "Iter-5670 train loss: 0.5504 valid loss: 0.4137, valid accuracy: 0.8848\n",
      "Iter-5680 train loss: 0.4102 valid loss: 0.4135, valid accuracy: 0.8846\n",
      "Iter-5690 train loss: 0.3565 valid loss: 0.4131, valid accuracy: 0.8846\n",
      "Iter-5700 train loss: 0.7106 valid loss: 0.4129, valid accuracy: 0.8850\n",
      "Iter-5710 train loss: 0.4491 valid loss: 0.4127, valid accuracy: 0.8848\n",
      "Iter-5720 train loss: 0.4887 valid loss: 0.4124, valid accuracy: 0.8846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 0.3220 valid loss: 0.4122, valid accuracy: 0.8844\n",
      "Iter-5740 train loss: 0.4081 valid loss: 0.4120, valid accuracy: 0.8840\n",
      "Iter-5750 train loss: 0.4421 valid loss: 0.4118, valid accuracy: 0.8846\n",
      "Iter-5760 train loss: 0.4515 valid loss: 0.4115, valid accuracy: 0.8846\n",
      "Iter-5770 train loss: 0.4402 valid loss: 0.4112, valid accuracy: 0.8846\n",
      "Iter-5780 train loss: 0.4011 valid loss: 0.4110, valid accuracy: 0.8850\n",
      "Iter-5790 train loss: 0.5483 valid loss: 0.4109, valid accuracy: 0.8852\n",
      "Iter-5800 train loss: 0.4238 valid loss: 0.4106, valid accuracy: 0.8850\n",
      "Iter-5810 train loss: 0.3420 valid loss: 0.4101, valid accuracy: 0.8856\n",
      "Iter-5820 train loss: 0.3881 valid loss: 0.4099, valid accuracy: 0.8850\n",
      "Iter-5830 train loss: 0.3729 valid loss: 0.4096, valid accuracy: 0.8856\n",
      "Iter-5840 train loss: 0.4456 valid loss: 0.4093, valid accuracy: 0.8858\n",
      "Iter-5850 train loss: 0.3548 valid loss: 0.4092, valid accuracy: 0.8852\n",
      "Iter-5860 train loss: 0.3884 valid loss: 0.4090, valid accuracy: 0.8856\n",
      "Iter-5870 train loss: 0.2145 valid loss: 0.4088, valid accuracy: 0.8858\n",
      "Iter-5880 train loss: 0.2650 valid loss: 0.4085, valid accuracy: 0.8862\n",
      "Iter-5890 train loss: 0.3795 valid loss: 0.4083, valid accuracy: 0.8860\n",
      "Iter-5900 train loss: 0.3317 valid loss: 0.4080, valid accuracy: 0.8862\n",
      "Iter-5910 train loss: 0.5859 valid loss: 0.4077, valid accuracy: 0.8862\n",
      "Iter-5920 train loss: 0.4254 valid loss: 0.4075, valid accuracy: 0.8862\n",
      "Iter-5930 train loss: 0.5975 valid loss: 0.4071, valid accuracy: 0.8860\n",
      "Iter-5940 train loss: 0.4271 valid loss: 0.4068, valid accuracy: 0.8864\n",
      "Iter-5950 train loss: 0.5653 valid loss: 0.4066, valid accuracy: 0.8868\n",
      "Iter-5960 train loss: 0.5559 valid loss: 0.4064, valid accuracy: 0.8864\n",
      "Iter-5970 train loss: 0.3635 valid loss: 0.4061, valid accuracy: 0.8866\n",
      "Iter-5980 train loss: 0.3696 valid loss: 0.4058, valid accuracy: 0.8866\n",
      "Iter-5990 train loss: 0.4919 valid loss: 0.4056, valid accuracy: 0.8876\n",
      "Iter-6000 train loss: 0.6624 valid loss: 0.4052, valid accuracy: 0.8870\n",
      "Iter-6010 train loss: 0.4731 valid loss: 0.4051, valid accuracy: 0.8872\n",
      "Iter-6020 train loss: 0.4369 valid loss: 0.4048, valid accuracy: 0.8872\n",
      "Iter-6030 train loss: 0.4633 valid loss: 0.4046, valid accuracy: 0.8876\n",
      "Iter-6040 train loss: 0.4963 valid loss: 0.4044, valid accuracy: 0.8874\n",
      "Iter-6050 train loss: 0.4785 valid loss: 0.4042, valid accuracy: 0.8868\n",
      "Iter-6060 train loss: 0.3176 valid loss: 0.4040, valid accuracy: 0.8872\n",
      "Iter-6070 train loss: 0.4893 valid loss: 0.4036, valid accuracy: 0.8872\n",
      "Iter-6080 train loss: 0.4054 valid loss: 0.4033, valid accuracy: 0.8874\n",
      "Iter-6090 train loss: 0.4555 valid loss: 0.4031, valid accuracy: 0.8874\n",
      "Iter-6100 train loss: 0.3708 valid loss: 0.4028, valid accuracy: 0.8878\n",
      "Iter-6110 train loss: 0.3792 valid loss: 0.4027, valid accuracy: 0.8876\n",
      "Iter-6120 train loss: 0.3634 valid loss: 0.4024, valid accuracy: 0.8874\n",
      "Iter-6130 train loss: 0.3488 valid loss: 0.4021, valid accuracy: 0.8872\n",
      "Iter-6140 train loss: 0.6455 valid loss: 0.4019, valid accuracy: 0.8878\n",
      "Iter-6150 train loss: 0.4002 valid loss: 0.4017, valid accuracy: 0.8878\n",
      "Iter-6160 train loss: 0.4652 valid loss: 0.4014, valid accuracy: 0.8882\n",
      "Iter-6170 train loss: 0.3480 valid loss: 0.4011, valid accuracy: 0.8882\n",
      "Iter-6180 train loss: 0.4053 valid loss: 0.4009, valid accuracy: 0.8880\n",
      "Iter-6190 train loss: 0.5255 valid loss: 0.4008, valid accuracy: 0.8868\n",
      "Iter-6200 train loss: 0.4183 valid loss: 0.4006, valid accuracy: 0.8864\n",
      "Iter-6210 train loss: 0.3103 valid loss: 0.4003, valid accuracy: 0.8880\n",
      "Iter-6220 train loss: 0.3927 valid loss: 0.4002, valid accuracy: 0.8876\n",
      "Iter-6230 train loss: 0.4442 valid loss: 0.4000, valid accuracy: 0.8882\n",
      "Iter-6240 train loss: 0.2425 valid loss: 0.3997, valid accuracy: 0.8878\n",
      "Iter-6250 train loss: 0.3626 valid loss: 0.3995, valid accuracy: 0.8890\n",
      "Iter-6260 train loss: 0.2796 valid loss: 0.3993, valid accuracy: 0.8884\n",
      "Iter-6270 train loss: 0.4957 valid loss: 0.3991, valid accuracy: 0.8888\n",
      "Iter-6280 train loss: 0.5726 valid loss: 0.3989, valid accuracy: 0.8890\n",
      "Iter-6290 train loss: 0.5941 valid loss: 0.3988, valid accuracy: 0.8884\n",
      "Iter-6300 train loss: 0.4558 valid loss: 0.3986, valid accuracy: 0.8890\n",
      "Iter-6310 train loss: 0.5678 valid loss: 0.3984, valid accuracy: 0.8892\n",
      "Iter-6320 train loss: 0.4820 valid loss: 0.3984, valid accuracy: 0.8890\n",
      "Iter-6330 train loss: 0.3987 valid loss: 0.3982, valid accuracy: 0.8884\n",
      "Iter-6340 train loss: 0.3268 valid loss: 0.3980, valid accuracy: 0.8888\n",
      "Iter-6350 train loss: 0.5930 valid loss: 0.3978, valid accuracy: 0.8890\n",
      "Iter-6360 train loss: 0.4126 valid loss: 0.3976, valid accuracy: 0.8894\n",
      "Iter-6370 train loss: 0.4094 valid loss: 0.3975, valid accuracy: 0.8890\n",
      "Iter-6380 train loss: 0.5171 valid loss: 0.3973, valid accuracy: 0.8890\n",
      "Iter-6390 train loss: 0.5396 valid loss: 0.3971, valid accuracy: 0.8886\n",
      "Iter-6400 train loss: 0.5049 valid loss: 0.3969, valid accuracy: 0.8890\n",
      "Iter-6410 train loss: 0.5897 valid loss: 0.3967, valid accuracy: 0.8884\n",
      "Iter-6420 train loss: 0.3242 valid loss: 0.3964, valid accuracy: 0.8882\n",
      "Iter-6430 train loss: 0.3416 valid loss: 0.3963, valid accuracy: 0.8886\n",
      "Iter-6440 train loss: 0.4546 valid loss: 0.3962, valid accuracy: 0.8884\n",
      "Iter-6450 train loss: 0.5023 valid loss: 0.3960, valid accuracy: 0.8880\n",
      "Iter-6460 train loss: 0.2767 valid loss: 0.3957, valid accuracy: 0.8882\n",
      "Iter-6470 train loss: 0.4190 valid loss: 0.3956, valid accuracy: 0.8880\n",
      "Iter-6480 train loss: 0.4069 valid loss: 0.3954, valid accuracy: 0.8884\n",
      "Iter-6490 train loss: 0.4079 valid loss: 0.3952, valid accuracy: 0.8884\n",
      "Iter-6500 train loss: 0.4770 valid loss: 0.3950, valid accuracy: 0.8882\n",
      "Iter-6510 train loss: 0.3917 valid loss: 0.3949, valid accuracy: 0.8886\n",
      "Iter-6520 train loss: 0.5025 valid loss: 0.3947, valid accuracy: 0.8882\n",
      "Iter-6530 train loss: 0.6458 valid loss: 0.3946, valid accuracy: 0.8882\n",
      "Iter-6540 train loss: 0.4752 valid loss: 0.3945, valid accuracy: 0.8876\n",
      "Iter-6550 train loss: 0.3564 valid loss: 0.3943, valid accuracy: 0.8876\n",
      "Iter-6560 train loss: 0.4045 valid loss: 0.3941, valid accuracy: 0.8878\n",
      "Iter-6570 train loss: 0.4273 valid loss: 0.3939, valid accuracy: 0.8880\n",
      "Iter-6580 train loss: 0.4637 valid loss: 0.3935, valid accuracy: 0.8882\n",
      "Iter-6590 train loss: 0.4367 valid loss: 0.3934, valid accuracy: 0.8876\n",
      "Iter-6600 train loss: 0.3951 valid loss: 0.3932, valid accuracy: 0.8876\n",
      "Iter-6610 train loss: 0.2575 valid loss: 0.3931, valid accuracy: 0.8880\n",
      "Iter-6620 train loss: 0.5638 valid loss: 0.3929, valid accuracy: 0.8880\n",
      "Iter-6630 train loss: 0.4588 valid loss: 0.3927, valid accuracy: 0.8882\n",
      "Iter-6640 train loss: 0.4785 valid loss: 0.3925, valid accuracy: 0.8882\n",
      "Iter-6650 train loss: 0.3716 valid loss: 0.3924, valid accuracy: 0.8886\n",
      "Iter-6660 train loss: 0.3606 valid loss: 0.3921, valid accuracy: 0.8888\n",
      "Iter-6670 train loss: 0.3799 valid loss: 0.3921, valid accuracy: 0.8886\n",
      "Iter-6680 train loss: 0.3788 valid loss: 0.3921, valid accuracy: 0.8882\n",
      "Iter-6690 train loss: 0.5441 valid loss: 0.3920, valid accuracy: 0.8882\n",
      "Iter-6700 train loss: 0.4810 valid loss: 0.3919, valid accuracy: 0.8886\n",
      "Iter-6710 train loss: 0.5127 valid loss: 0.3915, valid accuracy: 0.8890\n",
      "Iter-6720 train loss: 0.3633 valid loss: 0.3912, valid accuracy: 0.8898\n",
      "Iter-6730 train loss: 0.3449 valid loss: 0.3910, valid accuracy: 0.8898\n",
      "Iter-6740 train loss: 0.5589 valid loss: 0.3910, valid accuracy: 0.8896\n",
      "Iter-6750 train loss: 0.4734 valid loss: 0.3908, valid accuracy: 0.8894\n",
      "Iter-6760 train loss: 0.3117 valid loss: 0.3905, valid accuracy: 0.8892\n",
      "Iter-6770 train loss: 0.3745 valid loss: 0.3905, valid accuracy: 0.8892\n",
      "Iter-6780 train loss: 0.4135 valid loss: 0.3904, valid accuracy: 0.8892\n",
      "Iter-6790 train loss: 0.3208 valid loss: 0.3901, valid accuracy: 0.8900\n",
      "Iter-6800 train loss: 0.5547 valid loss: 0.3899, valid accuracy: 0.8898\n",
      "Iter-6810 train loss: 0.4190 valid loss: 0.3895, valid accuracy: 0.8900\n",
      "Iter-6820 train loss: 0.5906 valid loss: 0.3892, valid accuracy: 0.8906\n",
      "Iter-6830 train loss: 0.2994 valid loss: 0.3890, valid accuracy: 0.8900\n",
      "Iter-6840 train loss: 0.3746 valid loss: 0.3889, valid accuracy: 0.8896\n",
      "Iter-6850 train loss: 0.4230 valid loss: 0.3890, valid accuracy: 0.8896\n",
      "Iter-6860 train loss: 0.3338 valid loss: 0.3888, valid accuracy: 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 0.3111 valid loss: 0.3885, valid accuracy: 0.8894\n",
      "Iter-6880 train loss: 0.3106 valid loss: 0.3882, valid accuracy: 0.8900\n",
      "Iter-6890 train loss: 0.4594 valid loss: 0.3880, valid accuracy: 0.8896\n",
      "Iter-6900 train loss: 0.4039 valid loss: 0.3878, valid accuracy: 0.8900\n",
      "Iter-6910 train loss: 0.3406 valid loss: 0.3878, valid accuracy: 0.8894\n",
      "Iter-6920 train loss: 0.4725 valid loss: 0.3876, valid accuracy: 0.8894\n",
      "Iter-6930 train loss: 0.5584 valid loss: 0.3874, valid accuracy: 0.8888\n",
      "Iter-6940 train loss: 0.3744 valid loss: 0.3873, valid accuracy: 0.8888\n",
      "Iter-6950 train loss: 0.4664 valid loss: 0.3872, valid accuracy: 0.8890\n",
      "Iter-6960 train loss: 0.1519 valid loss: 0.3871, valid accuracy: 0.8890\n",
      "Iter-6970 train loss: 0.3444 valid loss: 0.3869, valid accuracy: 0.8890\n",
      "Iter-6980 train loss: 0.4307 valid loss: 0.3869, valid accuracy: 0.8888\n",
      "Iter-6990 train loss: 0.5783 valid loss: 0.3867, valid accuracy: 0.8890\n",
      "Iter-7000 train loss: 0.2574 valid loss: 0.3866, valid accuracy: 0.8888\n",
      "Iter-7010 train loss: 0.3393 valid loss: 0.3862, valid accuracy: 0.8892\n",
      "Iter-7020 train loss: 0.3891 valid loss: 0.3859, valid accuracy: 0.8890\n",
      "Iter-7030 train loss: 0.4221 valid loss: 0.3857, valid accuracy: 0.8896\n",
      "Iter-7040 train loss: 0.4752 valid loss: 0.3855, valid accuracy: 0.8894\n",
      "Iter-7050 train loss: 0.4191 valid loss: 0.3851, valid accuracy: 0.8894\n",
      "Iter-7060 train loss: 0.4501 valid loss: 0.3850, valid accuracy: 0.8900\n",
      "Iter-7070 train loss: 0.4765 valid loss: 0.3849, valid accuracy: 0.8898\n",
      "Iter-7080 train loss: 0.2498 valid loss: 0.3847, valid accuracy: 0.8900\n",
      "Iter-7090 train loss: 0.6090 valid loss: 0.3846, valid accuracy: 0.8904\n",
      "Iter-7100 train loss: 0.4704 valid loss: 0.3844, valid accuracy: 0.8902\n",
      "Iter-7110 train loss: 0.5712 valid loss: 0.3843, valid accuracy: 0.8898\n",
      "Iter-7120 train loss: 0.3244 valid loss: 0.3842, valid accuracy: 0.8902\n",
      "Iter-7130 train loss: 0.3822 valid loss: 0.3839, valid accuracy: 0.8898\n",
      "Iter-7140 train loss: 0.2812 valid loss: 0.3836, valid accuracy: 0.8904\n",
      "Iter-7150 train loss: 0.4630 valid loss: 0.3834, valid accuracy: 0.8908\n",
      "Iter-7160 train loss: 0.4309 valid loss: 0.3832, valid accuracy: 0.8904\n",
      "Iter-7170 train loss: 0.2816 valid loss: 0.3831, valid accuracy: 0.8900\n",
      "Iter-7180 train loss: 0.3723 valid loss: 0.3830, valid accuracy: 0.8902\n",
      "Iter-7190 train loss: 0.4897 valid loss: 0.3828, valid accuracy: 0.8908\n",
      "Iter-7200 train loss: 0.3724 valid loss: 0.3826, valid accuracy: 0.8898\n",
      "Iter-7210 train loss: 0.4179 valid loss: 0.3824, valid accuracy: 0.8904\n",
      "Iter-7220 train loss: 0.4981 valid loss: 0.3823, valid accuracy: 0.8900\n",
      "Iter-7230 train loss: 0.3947 valid loss: 0.3821, valid accuracy: 0.8904\n",
      "Iter-7240 train loss: 0.4910 valid loss: 0.3819, valid accuracy: 0.8908\n",
      "Iter-7250 train loss: 0.4980 valid loss: 0.3818, valid accuracy: 0.8906\n",
      "Iter-7260 train loss: 0.5033 valid loss: 0.3816, valid accuracy: 0.8908\n",
      "Iter-7270 train loss: 0.4182 valid loss: 0.3814, valid accuracy: 0.8908\n",
      "Iter-7280 train loss: 0.4338 valid loss: 0.3814, valid accuracy: 0.8914\n",
      "Iter-7290 train loss: 0.5073 valid loss: 0.3811, valid accuracy: 0.8920\n",
      "Iter-7300 train loss: 0.4044 valid loss: 0.3810, valid accuracy: 0.8920\n",
      "Iter-7310 train loss: 0.4015 valid loss: 0.3807, valid accuracy: 0.8914\n",
      "Iter-7320 train loss: 0.5578 valid loss: 0.3805, valid accuracy: 0.8914\n",
      "Iter-7330 train loss: 0.6113 valid loss: 0.3803, valid accuracy: 0.8914\n",
      "Iter-7340 train loss: 0.4272 valid loss: 0.3802, valid accuracy: 0.8904\n",
      "Iter-7350 train loss: 0.4681 valid loss: 0.3800, valid accuracy: 0.8912\n",
      "Iter-7360 train loss: 0.3952 valid loss: 0.3800, valid accuracy: 0.8908\n",
      "Iter-7370 train loss: 0.4477 valid loss: 0.3799, valid accuracy: 0.8912\n",
      "Iter-7380 train loss: 0.3192 valid loss: 0.3796, valid accuracy: 0.8906\n",
      "Iter-7390 train loss: 0.4959 valid loss: 0.3794, valid accuracy: 0.8906\n",
      "Iter-7400 train loss: 0.4014 valid loss: 0.3793, valid accuracy: 0.8912\n",
      "Iter-7410 train loss: 0.4945 valid loss: 0.3793, valid accuracy: 0.8920\n",
      "Iter-7420 train loss: 0.3036 valid loss: 0.3793, valid accuracy: 0.8922\n",
      "Iter-7430 train loss: 0.3340 valid loss: 0.3792, valid accuracy: 0.8924\n",
      "Iter-7440 train loss: 0.5108 valid loss: 0.3790, valid accuracy: 0.8928\n",
      "Iter-7450 train loss: 0.4144 valid loss: 0.3789, valid accuracy: 0.8926\n",
      "Iter-7460 train loss: 0.2706 valid loss: 0.3787, valid accuracy: 0.8926\n",
      "Iter-7470 train loss: 0.3271 valid loss: 0.3785, valid accuracy: 0.8920\n",
      "Iter-7480 train loss: 0.3944 valid loss: 0.3783, valid accuracy: 0.8924\n",
      "Iter-7490 train loss: 0.6369 valid loss: 0.3783, valid accuracy: 0.8924\n",
      "Iter-7500 train loss: 0.4324 valid loss: 0.3783, valid accuracy: 0.8922\n",
      "Iter-7510 train loss: 0.4475 valid loss: 0.3782, valid accuracy: 0.8916\n",
      "Iter-7520 train loss: 0.4530 valid loss: 0.3782, valid accuracy: 0.8924\n",
      "Iter-7530 train loss: 0.3647 valid loss: 0.3779, valid accuracy: 0.8924\n",
      "Iter-7540 train loss: 0.3820 valid loss: 0.3778, valid accuracy: 0.8918\n",
      "Iter-7550 train loss: 0.6124 valid loss: 0.3776, valid accuracy: 0.8924\n",
      "Iter-7560 train loss: 0.3336 valid loss: 0.3774, valid accuracy: 0.8918\n",
      "Iter-7570 train loss: 0.4609 valid loss: 0.3772, valid accuracy: 0.8918\n",
      "Iter-7580 train loss: 0.3980 valid loss: 0.3770, valid accuracy: 0.8918\n",
      "Iter-7590 train loss: 0.3199 valid loss: 0.3769, valid accuracy: 0.8918\n",
      "Iter-7600 train loss: 0.3182 valid loss: 0.3768, valid accuracy: 0.8916\n",
      "Iter-7610 train loss: 0.1939 valid loss: 0.3765, valid accuracy: 0.8922\n",
      "Iter-7620 train loss: 0.4524 valid loss: 0.3764, valid accuracy: 0.8920\n",
      "Iter-7630 train loss: 0.5720 valid loss: 0.3762, valid accuracy: 0.8922\n",
      "Iter-7640 train loss: 0.3668 valid loss: 0.3761, valid accuracy: 0.8918\n",
      "Iter-7650 train loss: 0.3007 valid loss: 0.3760, valid accuracy: 0.8922\n",
      "Iter-7660 train loss: 0.4559 valid loss: 0.3758, valid accuracy: 0.8928\n",
      "Iter-7670 train loss: 0.6540 valid loss: 0.3757, valid accuracy: 0.8924\n",
      "Iter-7680 train loss: 0.4613 valid loss: 0.3756, valid accuracy: 0.8916\n",
      "Iter-7690 train loss: 0.4173 valid loss: 0.3754, valid accuracy: 0.8918\n",
      "Iter-7700 train loss: 0.4677 valid loss: 0.3751, valid accuracy: 0.8918\n",
      "Iter-7710 train loss: 0.2595 valid loss: 0.3751, valid accuracy: 0.8920\n",
      "Iter-7720 train loss: 0.3985 valid loss: 0.3750, valid accuracy: 0.8912\n",
      "Iter-7730 train loss: 0.2776 valid loss: 0.3748, valid accuracy: 0.8918\n",
      "Iter-7740 train loss: 0.5109 valid loss: 0.3746, valid accuracy: 0.8918\n",
      "Iter-7750 train loss: 0.3126 valid loss: 0.3744, valid accuracy: 0.8914\n",
      "Iter-7760 train loss: 0.4793 valid loss: 0.3744, valid accuracy: 0.8922\n",
      "Iter-7770 train loss: 0.2949 valid loss: 0.3742, valid accuracy: 0.8920\n",
      "Iter-7780 train loss: 0.3537 valid loss: 0.3741, valid accuracy: 0.8920\n",
      "Iter-7790 train loss: 0.3346 valid loss: 0.3740, valid accuracy: 0.8928\n",
      "Iter-7800 train loss: 0.3870 valid loss: 0.3740, valid accuracy: 0.8922\n",
      "Iter-7810 train loss: 0.6596 valid loss: 0.3738, valid accuracy: 0.8922\n",
      "Iter-7820 train loss: 0.2577 valid loss: 0.3737, valid accuracy: 0.8926\n",
      "Iter-7830 train loss: 0.6081 valid loss: 0.3736, valid accuracy: 0.8932\n",
      "Iter-7840 train loss: 0.4966 valid loss: 0.3734, valid accuracy: 0.8928\n",
      "Iter-7850 train loss: 0.3389 valid loss: 0.3734, valid accuracy: 0.8926\n",
      "Iter-7860 train loss: 0.3379 valid loss: 0.3732, valid accuracy: 0.8924\n",
      "Iter-7870 train loss: 0.3010 valid loss: 0.3731, valid accuracy: 0.8924\n",
      "Iter-7880 train loss: 0.2651 valid loss: 0.3729, valid accuracy: 0.8936\n",
      "Iter-7890 train loss: 0.3819 valid loss: 0.3726, valid accuracy: 0.8942\n",
      "Iter-7900 train loss: 0.5631 valid loss: 0.3725, valid accuracy: 0.8934\n",
      "Iter-7910 train loss: 0.5934 valid loss: 0.3725, valid accuracy: 0.8934\n",
      "Iter-7920 train loss: 0.2193 valid loss: 0.3724, valid accuracy: 0.8928\n",
      "Iter-7930 train loss: 0.3630 valid loss: 0.3722, valid accuracy: 0.8930\n",
      "Iter-7940 train loss: 0.4849 valid loss: 0.3721, valid accuracy: 0.8926\n",
      "Iter-7950 train loss: 0.5132 valid loss: 0.3719, valid accuracy: 0.8928\n",
      "Iter-7960 train loss: 0.3674 valid loss: 0.3716, valid accuracy: 0.8934\n",
      "Iter-7970 train loss: 0.3096 valid loss: 0.3715, valid accuracy: 0.8932\n",
      "Iter-7980 train loss: 0.4746 valid loss: 0.3713, valid accuracy: 0.8938\n",
      "Iter-7990 train loss: 0.3801 valid loss: 0.3713, valid accuracy: 0.8936\n",
      "Iter-8000 train loss: 0.3333 valid loss: 0.3711, valid accuracy: 0.8932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 0.2321 valid loss: 0.3710, valid accuracy: 0.8934\n",
      "Iter-8020 train loss: 0.4735 valid loss: 0.3709, valid accuracy: 0.8938\n",
      "Iter-8030 train loss: 0.4338 valid loss: 0.3706, valid accuracy: 0.8942\n",
      "Iter-8040 train loss: 0.2825 valid loss: 0.3705, valid accuracy: 0.8944\n",
      "Iter-8050 train loss: 0.2689 valid loss: 0.3702, valid accuracy: 0.8940\n",
      "Iter-8060 train loss: 0.3327 valid loss: 0.3701, valid accuracy: 0.8940\n",
      "Iter-8070 train loss: 0.4425 valid loss: 0.3699, valid accuracy: 0.8938\n",
      "Iter-8080 train loss: 0.3358 valid loss: 0.3700, valid accuracy: 0.8942\n",
      "Iter-8090 train loss: 0.3032 valid loss: 0.3700, valid accuracy: 0.8940\n",
      "Iter-8100 train loss: 0.4349 valid loss: 0.3698, valid accuracy: 0.8942\n",
      "Iter-8110 train loss: 0.3407 valid loss: 0.3699, valid accuracy: 0.8944\n",
      "Iter-8120 train loss: 0.3331 valid loss: 0.3698, valid accuracy: 0.8944\n",
      "Iter-8130 train loss: 0.3090 valid loss: 0.3697, valid accuracy: 0.8944\n",
      "Iter-8140 train loss: 0.3105 valid loss: 0.3696, valid accuracy: 0.8938\n",
      "Iter-8150 train loss: 0.2345 valid loss: 0.3695, valid accuracy: 0.8944\n",
      "Iter-8160 train loss: 0.3777 valid loss: 0.3693, valid accuracy: 0.8944\n",
      "Iter-8170 train loss: 0.3668 valid loss: 0.3693, valid accuracy: 0.8946\n",
      "Iter-8180 train loss: 0.5301 valid loss: 0.3692, valid accuracy: 0.8944\n",
      "Iter-8190 train loss: 0.5159 valid loss: 0.3690, valid accuracy: 0.8946\n",
      "Iter-8200 train loss: 0.3159 valid loss: 0.3689, valid accuracy: 0.8946\n",
      "Iter-8210 train loss: 0.5862 valid loss: 0.3688, valid accuracy: 0.8950\n",
      "Iter-8220 train loss: 0.3488 valid loss: 0.3685, valid accuracy: 0.8948\n",
      "Iter-8230 train loss: 0.2392 valid loss: 0.3682, valid accuracy: 0.8948\n",
      "Iter-8240 train loss: 0.4455 valid loss: 0.3682, valid accuracy: 0.8944\n",
      "Iter-8250 train loss: 0.4242 valid loss: 0.3680, valid accuracy: 0.8946\n",
      "Iter-8260 train loss: 0.4483 valid loss: 0.3678, valid accuracy: 0.8948\n",
      "Iter-8270 train loss: 0.4563 valid loss: 0.3679, valid accuracy: 0.8948\n",
      "Iter-8280 train loss: 0.4421 valid loss: 0.3677, valid accuracy: 0.8950\n",
      "Iter-8290 train loss: 0.3835 valid loss: 0.3675, valid accuracy: 0.8950\n",
      "Iter-8300 train loss: 0.3431 valid loss: 0.3674, valid accuracy: 0.8948\n",
      "Iter-8310 train loss: 0.3294 valid loss: 0.3672, valid accuracy: 0.8954\n",
      "Iter-8320 train loss: 0.3908 valid loss: 0.3670, valid accuracy: 0.8946\n",
      "Iter-8330 train loss: 0.3408 valid loss: 0.3670, valid accuracy: 0.8954\n",
      "Iter-8340 train loss: 0.5663 valid loss: 0.3669, valid accuracy: 0.8954\n",
      "Iter-8350 train loss: 0.3263 valid loss: 0.3668, valid accuracy: 0.8952\n",
      "Iter-8360 train loss: 0.2638 valid loss: 0.3667, valid accuracy: 0.8948\n",
      "Iter-8370 train loss: 0.2691 valid loss: 0.3665, valid accuracy: 0.8950\n",
      "Iter-8380 train loss: 0.3898 valid loss: 0.3665, valid accuracy: 0.8958\n",
      "Iter-8390 train loss: 0.3352 valid loss: 0.3664, valid accuracy: 0.8962\n",
      "Iter-8400 train loss: 0.4988 valid loss: 0.3662, valid accuracy: 0.8962\n",
      "Iter-8410 train loss: 0.3271 valid loss: 0.3660, valid accuracy: 0.8958\n",
      "Iter-8420 train loss: 0.4712 valid loss: 0.3659, valid accuracy: 0.8956\n",
      "Iter-8430 train loss: 0.5804 valid loss: 0.3658, valid accuracy: 0.8954\n",
      "Iter-8440 train loss: 0.3830 valid loss: 0.3657, valid accuracy: 0.8958\n",
      "Iter-8450 train loss: 0.4910 valid loss: 0.3654, valid accuracy: 0.8960\n",
      "Iter-8460 train loss: 0.4727 valid loss: 0.3651, valid accuracy: 0.8960\n",
      "Iter-8470 train loss: 0.4499 valid loss: 0.3647, valid accuracy: 0.8960\n",
      "Iter-8480 train loss: 0.3386 valid loss: 0.3647, valid accuracy: 0.8956\n",
      "Iter-8490 train loss: 0.3453 valid loss: 0.3646, valid accuracy: 0.8950\n",
      "Iter-8500 train loss: 0.3562 valid loss: 0.3644, valid accuracy: 0.8950\n",
      "Iter-8510 train loss: 0.3641 valid loss: 0.3643, valid accuracy: 0.8950\n",
      "Iter-8520 train loss: 0.4184 valid loss: 0.3643, valid accuracy: 0.8954\n",
      "Iter-8530 train loss: 0.6364 valid loss: 0.3642, valid accuracy: 0.8954\n",
      "Iter-8540 train loss: 0.3221 valid loss: 0.3640, valid accuracy: 0.8954\n",
      "Iter-8550 train loss: 0.3338 valid loss: 0.3639, valid accuracy: 0.8956\n",
      "Iter-8560 train loss: 0.3236 valid loss: 0.3637, valid accuracy: 0.8954\n",
      "Iter-8570 train loss: 0.4018 valid loss: 0.3636, valid accuracy: 0.8956\n",
      "Iter-8580 train loss: 0.2440 valid loss: 0.3635, valid accuracy: 0.8956\n",
      "Iter-8590 train loss: 0.2554 valid loss: 0.3634, valid accuracy: 0.8952\n",
      "Iter-8600 train loss: 0.2833 valid loss: 0.3631, valid accuracy: 0.8956\n",
      "Iter-8610 train loss: 0.4656 valid loss: 0.3630, valid accuracy: 0.8960\n",
      "Iter-8620 train loss: 0.3140 valid loss: 0.3629, valid accuracy: 0.8956\n",
      "Iter-8630 train loss: 0.2785 valid loss: 0.3627, valid accuracy: 0.8958\n",
      "Iter-8640 train loss: 0.3518 valid loss: 0.3627, valid accuracy: 0.8956\n",
      "Iter-8650 train loss: 0.3173 valid loss: 0.3625, valid accuracy: 0.8960\n",
      "Iter-8660 train loss: 0.3167 valid loss: 0.3625, valid accuracy: 0.8962\n",
      "Iter-8670 train loss: 0.3125 valid loss: 0.3623, valid accuracy: 0.8958\n",
      "Iter-8680 train loss: 0.3134 valid loss: 0.3624, valid accuracy: 0.8956\n",
      "Iter-8690 train loss: 0.2168 valid loss: 0.3623, valid accuracy: 0.8962\n",
      "Iter-8700 train loss: 0.3874 valid loss: 0.3622, valid accuracy: 0.8966\n",
      "Iter-8710 train loss: 0.3257 valid loss: 0.3622, valid accuracy: 0.8968\n",
      "Iter-8720 train loss: 0.2968 valid loss: 0.3622, valid accuracy: 0.8968\n",
      "Iter-8730 train loss: 0.6012 valid loss: 0.3621, valid accuracy: 0.8974\n",
      "Iter-8740 train loss: 0.4049 valid loss: 0.3619, valid accuracy: 0.8972\n",
      "Iter-8750 train loss: 0.3423 valid loss: 0.3619, valid accuracy: 0.8966\n",
      "Iter-8760 train loss: 0.3892 valid loss: 0.3617, valid accuracy: 0.8964\n",
      "Iter-8770 train loss: 0.4987 valid loss: 0.3616, valid accuracy: 0.8966\n",
      "Iter-8780 train loss: 0.6333 valid loss: 0.3616, valid accuracy: 0.8958\n",
      "Iter-8790 train loss: 0.3384 valid loss: 0.3615, valid accuracy: 0.8960\n",
      "Iter-8800 train loss: 0.4057 valid loss: 0.3611, valid accuracy: 0.8960\n",
      "Iter-8810 train loss: 0.3834 valid loss: 0.3610, valid accuracy: 0.8960\n",
      "Iter-8820 train loss: 0.2679 valid loss: 0.3608, valid accuracy: 0.8958\n",
      "Iter-8830 train loss: 0.4831 valid loss: 0.3607, valid accuracy: 0.8960\n",
      "Iter-8840 train loss: 0.3560 valid loss: 0.3606, valid accuracy: 0.8964\n",
      "Iter-8850 train loss: 0.2747 valid loss: 0.3604, valid accuracy: 0.8970\n",
      "Iter-8860 train loss: 0.3479 valid loss: 0.3605, valid accuracy: 0.8972\n",
      "Iter-8870 train loss: 0.3594 valid loss: 0.3604, valid accuracy: 0.8976\n",
      "Iter-8880 train loss: 0.3132 valid loss: 0.3601, valid accuracy: 0.8976\n",
      "Iter-8890 train loss: 0.2072 valid loss: 0.3599, valid accuracy: 0.8974\n",
      "Iter-8900 train loss: 0.2785 valid loss: 0.3598, valid accuracy: 0.8970\n",
      "Iter-8910 train loss: 0.2591 valid loss: 0.3597, valid accuracy: 0.8968\n",
      "Iter-8920 train loss: 0.3791 valid loss: 0.3595, valid accuracy: 0.8974\n",
      "Iter-8930 train loss: 0.2665 valid loss: 0.3595, valid accuracy: 0.8974\n",
      "Iter-8940 train loss: 0.3868 valid loss: 0.3594, valid accuracy: 0.8966\n",
      "Iter-8950 train loss: 0.3621 valid loss: 0.3592, valid accuracy: 0.8964\n",
      "Iter-8960 train loss: 0.3665 valid loss: 0.3593, valid accuracy: 0.8964\n",
      "Iter-8970 train loss: 0.2844 valid loss: 0.3592, valid accuracy: 0.8968\n",
      "Iter-8980 train loss: 0.3429 valid loss: 0.3590, valid accuracy: 0.8966\n",
      "Iter-8990 train loss: 0.2969 valid loss: 0.3589, valid accuracy: 0.8964\n",
      "Iter-9000 train loss: 0.3144 valid loss: 0.3588, valid accuracy: 0.8966\n",
      "Iter-9010 train loss: 0.3409 valid loss: 0.3586, valid accuracy: 0.8966\n",
      "Iter-9020 train loss: 0.4272 valid loss: 0.3584, valid accuracy: 0.8968\n",
      "Iter-9030 train loss: 0.2533 valid loss: 0.3582, valid accuracy: 0.8972\n",
      "Iter-9040 train loss: 0.3997 valid loss: 0.3581, valid accuracy: 0.8972\n",
      "Iter-9050 train loss: 0.3697 valid loss: 0.3582, valid accuracy: 0.8970\n",
      "Iter-9060 train loss: 0.3443 valid loss: 0.3580, valid accuracy: 0.8974\n",
      "Iter-9070 train loss: 0.2754 valid loss: 0.3579, valid accuracy: 0.8972\n",
      "Iter-9080 train loss: 0.3184 valid loss: 0.3578, valid accuracy: 0.8972\n",
      "Iter-9090 train loss: 0.2706 valid loss: 0.3577, valid accuracy: 0.8972\n",
      "Iter-9100 train loss: 0.4397 valid loss: 0.3576, valid accuracy: 0.8978\n",
      "Iter-9110 train loss: 0.4612 valid loss: 0.3573, valid accuracy: 0.8980\n",
      "Iter-9120 train loss: 0.4741 valid loss: 0.3571, valid accuracy: 0.8976\n",
      "Iter-9130 train loss: 0.2343 valid loss: 0.3570, valid accuracy: 0.8978\n",
      "Iter-9140 train loss: 0.4541 valid loss: 0.3570, valid accuracy: 0.8976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 0.4560 valid loss: 0.3569, valid accuracy: 0.8978\n",
      "Iter-9160 train loss: 0.3141 valid loss: 0.3568, valid accuracy: 0.8978\n",
      "Iter-9170 train loss: 0.2279 valid loss: 0.3567, valid accuracy: 0.8980\n",
      "Iter-9180 train loss: 0.3092 valid loss: 0.3567, valid accuracy: 0.8978\n",
      "Iter-9190 train loss: 0.3871 valid loss: 0.3565, valid accuracy: 0.8978\n",
      "Iter-9200 train loss: 0.3704 valid loss: 0.3564, valid accuracy: 0.8980\n",
      "Iter-9210 train loss: 0.3309 valid loss: 0.3565, valid accuracy: 0.8982\n",
      "Iter-9220 train loss: 0.3352 valid loss: 0.3564, valid accuracy: 0.8988\n",
      "Iter-9230 train loss: 0.7274 valid loss: 0.3564, valid accuracy: 0.8988\n",
      "Iter-9240 train loss: 0.3400 valid loss: 0.3562, valid accuracy: 0.8980\n",
      "Iter-9250 train loss: 0.3720 valid loss: 0.3562, valid accuracy: 0.8978\n",
      "Iter-9260 train loss: 0.3344 valid loss: 0.3562, valid accuracy: 0.8974\n",
      "Iter-9270 train loss: 0.3163 valid loss: 0.3561, valid accuracy: 0.8980\n",
      "Iter-9280 train loss: 0.3952 valid loss: 0.3560, valid accuracy: 0.8984\n",
      "Iter-9290 train loss: 0.5346 valid loss: 0.3559, valid accuracy: 0.8980\n",
      "Iter-9300 train loss: 0.4171 valid loss: 0.3558, valid accuracy: 0.8978\n",
      "Iter-9310 train loss: 0.3951 valid loss: 0.3559, valid accuracy: 0.8984\n",
      "Iter-9320 train loss: 0.3575 valid loss: 0.3558, valid accuracy: 0.8982\n",
      "Iter-9330 train loss: 0.3110 valid loss: 0.3556, valid accuracy: 0.8984\n",
      "Iter-9340 train loss: 0.5344 valid loss: 0.3555, valid accuracy: 0.8984\n",
      "Iter-9350 train loss: 0.3812 valid loss: 0.3556, valid accuracy: 0.8988\n",
      "Iter-9360 train loss: 0.2733 valid loss: 0.3553, valid accuracy: 0.8992\n",
      "Iter-9370 train loss: 0.3150 valid loss: 0.3552, valid accuracy: 0.8986\n",
      "Iter-9380 train loss: 0.2512 valid loss: 0.3551, valid accuracy: 0.8988\n",
      "Iter-9390 train loss: 0.1139 valid loss: 0.3551, valid accuracy: 0.8988\n",
      "Iter-9400 train loss: 0.2852 valid loss: 0.3550, valid accuracy: 0.8986\n",
      "Iter-9410 train loss: 0.5657 valid loss: 0.3549, valid accuracy: 0.8984\n",
      "Iter-9420 train loss: 0.3658 valid loss: 0.3548, valid accuracy: 0.8984\n",
      "Iter-9430 train loss: 0.2548 valid loss: 0.3547, valid accuracy: 0.8986\n",
      "Iter-9440 train loss: 0.2899 valid loss: 0.3547, valid accuracy: 0.8988\n",
      "Iter-9450 train loss: 0.1536 valid loss: 0.3547, valid accuracy: 0.8988\n",
      "Iter-9460 train loss: 0.3807 valid loss: 0.3546, valid accuracy: 0.8990\n",
      "Iter-9470 train loss: 0.4705 valid loss: 0.3545, valid accuracy: 0.8982\n",
      "Iter-9480 train loss: 0.3014 valid loss: 0.3544, valid accuracy: 0.8982\n",
      "Iter-9490 train loss: 0.3065 valid loss: 0.3544, valid accuracy: 0.8984\n",
      "Iter-9500 train loss: 0.2755 valid loss: 0.3541, valid accuracy: 0.8986\n",
      "Iter-9510 train loss: 0.2687 valid loss: 0.3540, valid accuracy: 0.8990\n",
      "Iter-9520 train loss: 0.3937 valid loss: 0.3539, valid accuracy: 0.8992\n",
      "Iter-9530 train loss: 0.5302 valid loss: 0.3538, valid accuracy: 0.8990\n",
      "Iter-9540 train loss: 0.2056 valid loss: 0.3536, valid accuracy: 0.8992\n",
      "Iter-9550 train loss: 0.3215 valid loss: 0.3535, valid accuracy: 0.8996\n",
      "Iter-9560 train loss: 0.3684 valid loss: 0.3533, valid accuracy: 0.8994\n",
      "Iter-9570 train loss: 0.2930 valid loss: 0.3532, valid accuracy: 0.8998\n",
      "Iter-9580 train loss: 0.3883 valid loss: 0.3531, valid accuracy: 0.8998\n",
      "Iter-9590 train loss: 0.5147 valid loss: 0.3529, valid accuracy: 0.9006\n",
      "Iter-9600 train loss: 0.3516 valid loss: 0.3529, valid accuracy: 0.8996\n",
      "Iter-9610 train loss: 0.3885 valid loss: 0.3528, valid accuracy: 0.9002\n",
      "Iter-9620 train loss: 0.2728 valid loss: 0.3526, valid accuracy: 0.8998\n",
      "Iter-9630 train loss: 0.2753 valid loss: 0.3525, valid accuracy: 0.8994\n",
      "Iter-9640 train loss: 0.1866 valid loss: 0.3523, valid accuracy: 0.8998\n",
      "Iter-9650 train loss: 0.5007 valid loss: 0.3522, valid accuracy: 0.8998\n",
      "Iter-9660 train loss: 0.3582 valid loss: 0.3521, valid accuracy: 0.9004\n",
      "Iter-9670 train loss: 0.2812 valid loss: 0.3521, valid accuracy: 0.9006\n",
      "Iter-9680 train loss: 0.2293 valid loss: 0.3520, valid accuracy: 0.9006\n",
      "Iter-9690 train loss: 0.5245 valid loss: 0.3519, valid accuracy: 0.9008\n",
      "Iter-9700 train loss: 0.2456 valid loss: 0.3518, valid accuracy: 0.9006\n",
      "Iter-9710 train loss: 0.3192 valid loss: 0.3518, valid accuracy: 0.9010\n",
      "Iter-9720 train loss: 0.4134 valid loss: 0.3518, valid accuracy: 0.9004\n",
      "Iter-9730 train loss: 0.3457 valid loss: 0.3517, valid accuracy: 0.9004\n",
      "Iter-9740 train loss: 0.1996 valid loss: 0.3515, valid accuracy: 0.9006\n",
      "Iter-9750 train loss: 0.2291 valid loss: 0.3515, valid accuracy: 0.9002\n",
      "Iter-9760 train loss: 0.2881 valid loss: 0.3514, valid accuracy: 0.9004\n",
      "Iter-9770 train loss: 0.4604 valid loss: 0.3512, valid accuracy: 0.9006\n",
      "Iter-9780 train loss: 0.2599 valid loss: 0.3512, valid accuracy: 0.9006\n",
      "Iter-9790 train loss: 0.3279 valid loss: 0.3510, valid accuracy: 0.9004\n",
      "Iter-9800 train loss: 0.4932 valid loss: 0.3507, valid accuracy: 0.9006\n",
      "Iter-9810 train loss: 0.4268 valid loss: 0.3505, valid accuracy: 0.9006\n",
      "Iter-9820 train loss: 0.5274 valid loss: 0.3505, valid accuracy: 0.9016\n",
      "Iter-9830 train loss: 0.2582 valid loss: 0.3503, valid accuracy: 0.9002\n",
      "Iter-9840 train loss: 0.2093 valid loss: 0.3501, valid accuracy: 0.9002\n",
      "Iter-9850 train loss: 0.3734 valid loss: 0.3500, valid accuracy: 0.9002\n",
      "Iter-9860 train loss: 0.6121 valid loss: 0.3499, valid accuracy: 0.9006\n",
      "Iter-9870 train loss: 0.4101 valid loss: 0.3497, valid accuracy: 0.9008\n",
      "Iter-9880 train loss: 0.6471 valid loss: 0.3498, valid accuracy: 0.9010\n",
      "Iter-9890 train loss: 0.4555 valid loss: 0.3496, valid accuracy: 0.9008\n",
      "Iter-9900 train loss: 0.4093 valid loss: 0.3496, valid accuracy: 0.9002\n",
      "Iter-9910 train loss: 0.3214 valid loss: 0.3494, valid accuracy: 0.9008\n",
      "Iter-9920 train loss: 0.2704 valid loss: 0.3493, valid accuracy: 0.9002\n",
      "Iter-9930 train loss: 0.4400 valid loss: 0.3492, valid accuracy: 0.9002\n",
      "Iter-9940 train loss: 0.2513 valid loss: 0.3492, valid accuracy: 0.9002\n",
      "Iter-9950 train loss: 0.4178 valid loss: 0.3491, valid accuracy: 0.9002\n",
      "Iter-9960 train loss: 0.2897 valid loss: 0.3489, valid accuracy: 0.9002\n",
      "Iter-9970 train loss: 0.3674 valid loss: 0.3488, valid accuracy: 0.9008\n",
      "Iter-9980 train loss: 0.5973 valid loss: 0.3487, valid accuracy: 0.9006\n",
      "Iter-9990 train loss: 0.3413 valid loss: 0.3488, valid accuracy: 0.9016\n",
      "Iter-10000 train loss: 0.4294 valid loss: 0.3487, valid accuracy: 0.9012\n",
      "Last iteration - Test accuracy mean: 0.9039, std: 0.0000, loss: 0.3477\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4cc654828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREQi7P8DCQk7Kf9EGegAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4cc6542b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
