{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000,), (5000,), (10000,))"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape\n",
    "X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype\n",
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBCI-HW1\u001b[0m/                                 \u001b[01;34mDGRUs\u001b[0m/\r\n",
      "\u001b[01;34mBCI-HW2\u001b[0m/                                 \u001b[01;34mDGRUs_old\u001b[0m/\r\n",
      "\u001b[01;34mbin\u001b[0m/                                     environment.yml\r\n",
      "\u001b[01;34mbio-bp-dl\u001b[0m/                               gradient_descent.py\r\n",
      "confusion_mat_cov.ipynb                  \u001b[01;34mimpl\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/                                    \u001b[01;34mimpl_imagernn_karpathy\u001b[0m/\r\n",
      "DCNN.ipynb                               LICENSE\r\n",
      "Deep-FFNN-Tanh-FBA.ipynb                 minimal_net.ipynb\r\n",
      "Deep-FFNN-Tanh-FBA-ITD-Copy1.ipynb       \u001b[01;34mmisc\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD.ipynb             \u001b[01;34mmisc2\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-Vanilla.ipynb             NOTES\r\n",
      "DFFNN-FBA-STDP-Tanh-diff.ipynb           numba-cuda-gpu-example.ipynb\r\n",
      "DFFNN-FBA-STDP-Tanh-diff-no_FBA.ipynb    README.md\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff2.ipynb  \u001b[01;34mrnn-testing-platform\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff.ipynb   \u001b[01;34mtf-based\u001b[0m/\r\n",
      "\u001b[01;34mDFFNNs\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_x.head(), train_x.shape, train_x.dtypes\n",
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "train_y.head(), train_y.shape, train_y.dtypes\n",
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "# test_x.head(), test_x.dtypes, test_x.shape\n",
    "# train_x.shape, train_y.shape, test_x.shape\n",
    "# train_x.head(), train_X.shape\n",
    "# test_x.shape, test_x[:1], \n",
    "# test_x.dtypes, \n",
    "# train_x.dtypes\n",
    "# train_x[:1], \n",
    "# test_x[10:11]\n",
    "train_y.shape, train_y[:10]['Id']\n",
    "test_x.shape, train_y.shape\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "# train_X.shape, train_X.dtype\n",
    "test_X = np.array(test_x)\n",
    "# test_X.dtype\n",
    "\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "# Y_train = np.array(train_Y[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)\n",
    "# X_test.shape, X_train.shape, X_test.dtype, X_train.shape\n",
    "# train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Vocal\n",
      "1 Blues\n",
      "2 International\n",
      "3 Rap\n",
      "4 Folk\n",
      "5 Country\n",
      "6 Reggae\n",
      "7 New_Age\n",
      "8 Latin\n",
      "9 Jazz\n",
      "10 Pop_Rock\n",
      "11 Electronic\n",
      "12 RnB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 12, array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]))"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from collections import Counter\n",
    "# # Count the freq of words in the text/collection of words\n",
    "# word_counts = Counter(text)\n",
    "\n",
    "# # Having counted the frequency of the words in collection, sort them from most to least/top to bottom/descendng\n",
    "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "# # first enumerating for vocab to int\n",
    "# vocab_to_int = {words: ii for ii, words in enumerate(sorted_vocab)}\n",
    "\n",
    "# # into_to_vocab after enumerating through the sorted vocab\n",
    "# int_to_vocab = {ii: words for words, ii in vocab_to_int.items()}\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counted_labels = Counter(train_Y)\n",
    "labels = []\n",
    "for val, key in enumerate(counted_labels):\n",
    "    print(val, key)\n",
    "    labels.append(val)\n",
    "  \n",
    "labels = np.array(labels, dtype=int)\n",
    "labels.size, np.max(labels), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 13)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from impl.layer import onehot\n",
    "\n",
    "labels_onehot = onehot(labels)\n",
    "\n",
    "labels, labels_onehot, counted_labels.keys()\n",
    "key_to_vec = {key: vec for key, vec in zip(counted_labels.keys(), labels_onehot)}\n",
    "key_to_vec, key_to_vec['Vocal']\n",
    "\n",
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_vec[each])\n",
    "    Y_train_vec.append(key_to_vec[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD7CAYAAABKWyniAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPUTCF2IPkfCF2EMkfCH2EAlfiD3k/wOlnI31ivzJxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f725941bf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as mplot\n",
    "\n",
    "mplot.imshow(labels_onehot)\n",
    "mplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13000, 26), (13000, 13), (10400, 26))"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_val.shape, Y_train_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dtype('float64'), array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n",
       "        ..., \n",
       "        [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape, X_train.dtype, X_val.dtype\n",
    "Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 13), (11700, 26), (1300, 26), (1300, 13))"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-379-59b47e4a5a3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n\u001b[0;32m---> 16\u001b[0;31m            n_iter=n_iter, print_after=print_after)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-373-777f2825385b>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(self, train_set, val_set, alpha, mb_size, n_iter, print_after)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ys[0], ys[1] and ys_prev are used for backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mys_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m \u001b[0;31m# for next iteration or epoch learning dW and db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-373-777f2825385b>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, y, y_train)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# softmax is included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dsoftmax is included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/arasdar-DL-git/impl/loss.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(y_pred, y_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mlog_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# to avoid the devision by zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdata_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_like\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "# num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
