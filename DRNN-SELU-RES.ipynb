{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, C, L, p_dropout):\n",
    "        self.L = L\n",
    "        self.H = H\n",
    "        self.D = D\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Input layer\n",
    "        m_in = dict(\n",
    "            Wx=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            bx=np.zeros((1, H))\n",
    "            )\n",
    "\n",
    "        # Hidden layers\n",
    "        m_h = dict(\n",
    "            Wxh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Wx=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Wh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            bxh=np.zeros((1, H)),\n",
    "            bx=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H))\n",
    "            )\n",
    "            \n",
    "        # Output layer\n",
    "        m_out = dict(\n",
    "            Wy=np.random.randn(H, C) / np.sqrt(H / 2.),\n",
    "            by=np.zeros((1, C))\n",
    "            )\n",
    "        \n",
    "        # Model parameters\n",
    "        self.model = []\n",
    "        self.model.append(m_in) # input layer: layer == 0\n",
    "        for _ in range(self.L): # hidden layer: layer == 1:self.L\n",
    "            self.model.append(m_h)\n",
    "        self.model.append(m_out) # output layer: layer == self.L\n",
    "                \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, bxh = m['Wxh'], m['Whh'], m['bxh']\n",
    "        Xh = (X @ Wxh) + (h @ Whh) + bxh\n",
    "        Xh, Xh_nl_cache = l.tanh_forward(Xh)\n",
    "        # Xh, Xh_nl_cache = self.selu_forward(Xh)\n",
    "        \n",
    "        Wx, bx = m['Wx'], m['bx']\n",
    "        X_in = X.copy()\n",
    "        # X = (X @ Wx) + bx\n",
    "        X, X_fc_cache = l.fc_forward(X, Wx, bx)\n",
    "        X, X_nl_cache = l.tanh_forward(X)\n",
    "        # X, X_nl_cache = self.selu_forward(X)\n",
    "        X += X_in\n",
    "        \n",
    "        Wh, bh = m['Wh'], m['bh']\n",
    "        h_in = h.copy()\n",
    "        # h = (h @ Wh) + bh\n",
    "        h, h_fc_cache = l.fc_forward(h, Wh, bh)\n",
    "        h, h_nl_cache = l.tanh_forward(h)\n",
    "        # h, h_nl_cache = self.selu_forward(h)\n",
    "        h += h_in\n",
    "        \n",
    "        h = Xh + h\n",
    "        y = Xh + X\n",
    "\n",
    "        h, h_selu_nl_cache = self.selu_forward(h)\n",
    "        h, h_selu_do_cache = self.alpha_dropout_fwd(h, self.p_dropout)\n",
    "        \n",
    "        y, y_selu_nl_cache = self.selu_forward(y)\n",
    "        y, y_selu_do_cache = self.alpha_dropout_fwd(y, self.p_dropout)\n",
    "        \n",
    "        cache = (X_in, h_in, Wxh, Whh, Xh_nl_cache, X_fc_cache, X_nl_cache, h_fc_cache, h_nl_cache, h_selu_nl_cache, h_selu_do_cache, y_selu_nl_cache, y_selu_do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        (X_in, h_in, Wxh, Whh, Xh_nl_cache, X_fc_cache, X_nl_cache, h_fc_cache, h_nl_cache, h_selu_nl_cache, h_selu_do_cache, y_selu_nl_cache, y_selu_do_cache) = cache\n",
    "         \n",
    "        dh = self.alpha_dropout_bwd(dh, h_selu_do_cache)\n",
    "        dh = self.selu_backward(dh, h_selu_nl_cache)\n",
    "        \n",
    "        dy = self.alpha_dropout_bwd(dy, y_selu_do_cache)\n",
    "        dy = self.selu_backward(dy, y_selu_nl_cache)\n",
    "\n",
    "        dh_out = dh.copy()\n",
    "        # dh = self.selu_backward(dh, h_nl_cache)\n",
    "        dh = l.tanh_backward(dh, h_nl_cache)\n",
    "        dh, dWh, dbh = l.fc_backward(dh, h_fc_cache)\n",
    "        dh += dh_out\n",
    "        #         dWh = h_in.T @ dh # nxh = nx1 @ 1xh\n",
    "        #         dbh = dh * 1.0\n",
    "        #         dh = dh @ Wh.T # 1xn = 1xh @ hxn\n",
    "\n",
    "        dX = dy.copy()\n",
    "        # dX = self.selu_backward(dX, X_nl_cache)\n",
    "        dX = l.tanh_backward(dX, X_nl_cache)\n",
    "        dX, dWx, dbx = l.fc_backward(dX, X_fc_cache)\n",
    "        dX += dy\n",
    "        #         dWx = X_in.T @ dX # nxh = nx1 @ 1xh\n",
    "        #         dbx = dX * 1.0\n",
    "        #         dX = dX @ Wx.T # 1xn = 1xh @ hxn\n",
    "\n",
    "        dXh = dy + dh        \n",
    "        # dXh = self.selu_backward(dXh, Xh_nl_cache)\n",
    "        dXh = l.tanh_backward(dXh, Xh_nl_cache)\n",
    "        dbxh = dXh * 1.0\n",
    "        dWhh = h_in.T @ dXh\n",
    "        dWxh = X_in.T @ dXh\n",
    "        dX += dXh @ Wxh.T # 1xn = 1xh @ hxn\n",
    "        dh += dXh @ Whh.T # 1xn = 1xh @ hxn\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, bxh=dbxh, Wx=dWx, bx=dbx, Wh=dWh, bh=dbh)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        # Input (1), hidden (L), and output layers (1)\n",
    "        caches = []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        # Hidden layers and cells connections\n",
    "        for _ in range(self.L + 2):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        # Hidden layers    \n",
    "        for layer in range(self.L + 2):\n",
    "            if layer == 0: # input layer\n",
    "                ys = []\n",
    "                for X in X_train:\n",
    "                    X_one_hot = np.zeros(self.D)\n",
    "                    X_one_hot[X] = 1.\n",
    "                    X = X_one_hot.reshape(1, -1)\n",
    "                    y, cache = l.fc_forward(X, self.model[layer]['Wx'], self.model[layer]['bx'])\n",
    "                    # print(y.shape, X.shape)\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "            if (layer > 0) and (layer < self.L + 1): # hidden layers\n",
    "                Xs = ys.copy()\n",
    "                ys = []\n",
    "                for X in Xs:\n",
    "                    y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "            if layer == self.L + 1: # output layer\n",
    "                Xs = ys.copy()\n",
    "                ys = []\n",
    "                for X in Xs:\n",
    "                    y, cache = l.fc_forward(X, self.model[layer]['Wy'], self.model[layer]['by'])\n",
    "                    caches[layer].append(cache)\n",
    "                    ys.append(y)\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L + 2):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        for layer in reversed(range(self.L + 2)):\n",
    "            if layer == (self.L + 1):  # Output layer\n",
    "                dXs = []\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dy = dys[t]\n",
    "                    dX, dWy, dby = l.fc_backward(dy, caches[layer][t])\n",
    "                    grads[layer]['Wy'] += dWy\n",
    "                    grads[layer]['by'] += dby\n",
    "                    dXs.append(dX)\n",
    "            if (layer > 0) and (layer < (self.L + 1)): # Middle layers\n",
    "                dys = dXs.copy()\n",
    "                dXs = []\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dy = dys[t]\n",
    "                    dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                    for key in grad[layer].keys():\n",
    "                        grads[layer][key] += grad[layer][key]\n",
    "                    dXs.append(dX)\n",
    "            if layer == 0: # Input-Output layer\n",
    "                dys = dXs.copy()\n",
    "                dXs = []\n",
    "                for t in reversed(range(len(dys))):\n",
    "                    dy = dys[t]\n",
    "                    dX, dWx, dbx = l.fc_backward(dy, caches[layer][t])\n",
    "                    grads[layer]['Wx'] += dWx\n",
    "                    grads[layer]['bx'] += dbx\n",
    "                    dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    #     def test(self, X_seed, h, size):\n",
    "    #         chars = [self.idx2char[X_seed]]\n",
    "    #         idx_list = list(range(self.vocab_size))\n",
    "    #         X = X_seed\n",
    "\n",
    "    #         h_init = h.copy()\n",
    "    #         h = []\n",
    "    #         for _ in range(self.L):\n",
    "    #             h.append(h_init.copy())\n",
    "\n",
    "    #         # Test is different than train since y[t+1] is related to y[t] \n",
    "    #         for _ in range(size):\n",
    "    #             X_one_hot = np.zeros(self.D)\n",
    "    #             X_one_hot[X] = 1.\n",
    "    #             X = X_one_hot.reshape(1, -1)\n",
    "    #             for layer in range(self.L): # start, stop, step\n",
    "    #                 y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "    #                 if layer == self.L-1: # this is the last layer\n",
    "    #                     y_logit = y\n",
    "    #                 else: \n",
    "    #                     X = y # y: output for this layer, X: input for this layer\n",
    "    #             y_prob = l.softmax(y_logit)\n",
    "    #             idx = np.random.choice(idx_list, p=y_prob.ravel())\n",
    "    #             chars.append(self.idx2char[idx])\n",
    "    #             X = idx\n",
    "\n",
    "    #         return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "         \n",
    "        # Hidden layers\n",
    "        for layer in range(nn.L + 2):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L + 2):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                #                 sample = self.test(X_mini[0], state, size=100)\n",
    "                #                 print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 124.9617\n",
      "Iter-2 loss: 80.8457\n",
      "Iter-3 loss: 82.2547\n",
      "Iter-4 loss: 90.4658\n",
      "Iter-5 loss: 113.3202\n",
      "Iter-6 loss: 76.0658\n",
      "Iter-7 loss: 60.8807\n",
      "Iter-8 loss: 83.7895\n",
      "Iter-9 loss: 103.7940\n",
      "Iter-10 loss: 94.5512\n",
      "Iter-11 loss: 75.5367\n",
      "Iter-12 loss: 67.7846\n",
      "Iter-13 loss: 88.8661\n",
      "Iter-14 loss: 92.9699\n",
      "Iter-15 loss: 75.2957\n",
      "Iter-16 loss: 63.8795\n",
      "Iter-17 loss: 68.5055\n",
      "Iter-18 loss: 81.3137\n",
      "Iter-19 loss: 59.5472\n",
      "Iter-20 loss: 68.4197\n",
      "Iter-21 loss: 73.5178\n",
      "Iter-22 loss: 72.3798\n",
      "Iter-23 loss: 93.7848\n",
      "Iter-24 loss: 98.5489\n",
      "Iter-25 loss: 61.6901\n",
      "Iter-26 loss: 68.2607\n",
      "Iter-27 loss: 84.7870\n",
      "Iter-28 loss: 81.7685\n",
      "Iter-29 loss: 74.7123\n",
      "Iter-30 loss: 63.2221\n",
      "Iter-31 loss: 69.6617\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 10 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//100 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "num_output_units = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = RNN(D=num_input_units, H=num_hidden_units, C=num_output_units, L=num_layers, p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
