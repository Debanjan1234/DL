{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3225 valid loss: 2.3122, valid accuracy: 0.0780\n",
      "Iter-200 train loss: 2.3230 valid loss: 2.3113, valid accuracy: 0.0786\n",
      "Iter-300 train loss: 2.3076 valid loss: 2.3104, valid accuracy: 0.0798\n",
      "Iter-400 train loss: 2.3239 valid loss: 2.3096, valid accuracy: 0.0812\n",
      "Iter-500 train loss: 2.3215 valid loss: 2.3086, valid accuracy: 0.0822\n",
      "Iter-600 train loss: 2.3129 valid loss: 2.3077, valid accuracy: 0.0842\n",
      "Iter-700 train loss: 2.3226 valid loss: 2.3069, valid accuracy: 0.0850\n",
      "Iter-800 train loss: 2.3240 valid loss: 2.3060, valid accuracy: 0.0864\n",
      "Iter-900 train loss: 2.3031 valid loss: 2.3051, valid accuracy: 0.0860\n",
      "Iter-1000 train loss: 2.3052 valid loss: 2.3042, valid accuracy: 0.0872\n",
      "Iter-1100 train loss: 2.3056 valid loss: 2.3033, valid accuracy: 0.0876\n",
      "Iter-1200 train loss: 2.3084 valid loss: 2.3025, valid accuracy: 0.0888\n",
      "Iter-1300 train loss: 2.3260 valid loss: 2.3016, valid accuracy: 0.0894\n",
      "Iter-1400 train loss: 2.2976 valid loss: 2.3007, valid accuracy: 0.0916\n",
      "Iter-1500 train loss: 2.2863 valid loss: 2.2998, valid accuracy: 0.0930\n",
      "Iter-1600 train loss: 2.2749 valid loss: 2.2990, valid accuracy: 0.0938\n",
      "Iter-1700 train loss: 2.2642 valid loss: 2.2981, valid accuracy: 0.0946\n",
      "Iter-1800 train loss: 2.2925 valid loss: 2.2973, valid accuracy: 0.0960\n",
      "Iter-1900 train loss: 2.2855 valid loss: 2.2964, valid accuracy: 0.0974\n",
      "Iter-2000 train loss: 2.2877 valid loss: 2.2956, valid accuracy: 0.0982\n",
      "Iter-2100 train loss: 2.2919 valid loss: 2.2947, valid accuracy: 0.1004\n",
      "Iter-2200 train loss: 2.2981 valid loss: 2.2938, valid accuracy: 0.1012\n",
      "Iter-2300 train loss: 2.2934 valid loss: 2.2930, valid accuracy: 0.1036\n",
      "Iter-2400 train loss: 2.2769 valid loss: 2.2921, valid accuracy: 0.1056\n",
      "Iter-2500 train loss: 2.3045 valid loss: 2.2913, valid accuracy: 0.1074\n",
      "Iter-2600 train loss: 2.3150 valid loss: 2.2904, valid accuracy: 0.1088\n",
      "Iter-2700 train loss: 2.2951 valid loss: 2.2896, valid accuracy: 0.1110\n",
      "Iter-2800 train loss: 2.3005 valid loss: 2.2887, valid accuracy: 0.1136\n",
      "Iter-2900 train loss: 2.2866 valid loss: 2.2879, valid accuracy: 0.1160\n",
      "Iter-3000 train loss: 2.2950 valid loss: 2.2870, valid accuracy: 0.1198\n",
      "Iter-3100 train loss: 2.2876 valid loss: 2.2862, valid accuracy: 0.1220\n",
      "Iter-3200 train loss: 2.2737 valid loss: 2.2854, valid accuracy: 0.1256\n",
      "Iter-3300 train loss: 2.2827 valid loss: 2.2845, valid accuracy: 0.1284\n",
      "Iter-3400 train loss: 2.2790 valid loss: 2.2837, valid accuracy: 0.1312\n",
      "Iter-3500 train loss: 2.2935 valid loss: 2.2828, valid accuracy: 0.1364\n",
      "Iter-3600 train loss: 2.3058 valid loss: 2.2820, valid accuracy: 0.1390\n",
      "Iter-3700 train loss: 2.2881 valid loss: 2.2812, valid accuracy: 0.1436\n",
      "Iter-3800 train loss: 2.2881 valid loss: 2.2803, valid accuracy: 0.1466\n",
      "Iter-3900 train loss: 2.2925 valid loss: 2.2795, valid accuracy: 0.1496\n",
      "Iter-4000 train loss: 2.2819 valid loss: 2.2787, valid accuracy: 0.1536\n",
      "Iter-4100 train loss: 2.2742 valid loss: 2.2778, valid accuracy: 0.1540\n",
      "Iter-4200 train loss: 2.2829 valid loss: 2.2770, valid accuracy: 0.1576\n",
      "Iter-4300 train loss: 2.3049 valid loss: 2.2762, valid accuracy: 0.1624\n",
      "Iter-4400 train loss: 2.2803 valid loss: 2.2754, valid accuracy: 0.1668\n",
      "Iter-4500 train loss: 2.3077 valid loss: 2.2746, valid accuracy: 0.1698\n",
      "Iter-4600 train loss: 2.2986 valid loss: 2.2737, valid accuracy: 0.1746\n",
      "Iter-4700 train loss: 2.2540 valid loss: 2.2729, valid accuracy: 0.1786\n",
      "Iter-4800 train loss: 2.2746 valid loss: 2.2721, valid accuracy: 0.1802\n",
      "Iter-4900 train loss: 2.2721 valid loss: 2.2713, valid accuracy: 0.1818\n",
      "Iter-5000 train loss: 2.2631 valid loss: 2.2705, valid accuracy: 0.1840\n",
      "Iter-5100 train loss: 2.2681 valid loss: 2.2696, valid accuracy: 0.1872\n",
      "Iter-5200 train loss: 2.2818 valid loss: 2.2688, valid accuracy: 0.1882\n",
      "Iter-5300 train loss: 2.2442 valid loss: 2.2680, valid accuracy: 0.1896\n",
      "Iter-5400 train loss: 2.2463 valid loss: 2.2672, valid accuracy: 0.1920\n",
      "Iter-5500 train loss: 2.2513 valid loss: 2.2664, valid accuracy: 0.1956\n",
      "Iter-5600 train loss: 2.2652 valid loss: 2.2656, valid accuracy: 0.1968\n",
      "Iter-5700 train loss: 2.2771 valid loss: 2.2648, valid accuracy: 0.2002\n",
      "Iter-5800 train loss: 2.2716 valid loss: 2.2640, valid accuracy: 0.2018\n",
      "Iter-5900 train loss: 2.2544 valid loss: 2.2631, valid accuracy: 0.2050\n",
      "Iter-6000 train loss: 2.2623 valid loss: 2.2623, valid accuracy: 0.2068\n",
      "Iter-6100 train loss: 2.2602 valid loss: 2.2615, valid accuracy: 0.2080\n",
      "Iter-6200 train loss: 2.2673 valid loss: 2.2607, valid accuracy: 0.2108\n",
      "Iter-6300 train loss: 2.2589 valid loss: 2.2600, valid accuracy: 0.2130\n",
      "Iter-6400 train loss: 2.2592 valid loss: 2.2592, valid accuracy: 0.2156\n",
      "Iter-6500 train loss: 2.2732 valid loss: 2.2584, valid accuracy: 0.2160\n",
      "Iter-6600 train loss: 2.2258 valid loss: 2.2576, valid accuracy: 0.2186\n",
      "Iter-6700 train loss: 2.2596 valid loss: 2.2568, valid accuracy: 0.2216\n",
      "Iter-6800 train loss: 2.2320 valid loss: 2.2560, valid accuracy: 0.2238\n",
      "Iter-6900 train loss: 2.2353 valid loss: 2.2552, valid accuracy: 0.2252\n",
      "Iter-7000 train loss: 2.2585 valid loss: 2.2545, valid accuracy: 0.2272\n",
      "Iter-7100 train loss: 2.2433 valid loss: 2.2537, valid accuracy: 0.2298\n",
      "Iter-7200 train loss: 2.2610 valid loss: 2.2529, valid accuracy: 0.2306\n",
      "Iter-7300 train loss: 2.2634 valid loss: 2.2521, valid accuracy: 0.2302\n",
      "Iter-7400 train loss: 2.2665 valid loss: 2.2513, valid accuracy: 0.2318\n",
      "Iter-7500 train loss: 2.2455 valid loss: 2.2505, valid accuracy: 0.2334\n",
      "Iter-7600 train loss: 2.2602 valid loss: 2.2498, valid accuracy: 0.2352\n",
      "Iter-7700 train loss: 2.2436 valid loss: 2.2490, valid accuracy: 0.2366\n",
      "Iter-7800 train loss: 2.2550 valid loss: 2.2483, valid accuracy: 0.2384\n",
      "Iter-7900 train loss: 2.2521 valid loss: 2.2475, valid accuracy: 0.2390\n",
      "Iter-8000 train loss: 2.2364 valid loss: 2.2467, valid accuracy: 0.2408\n",
      "Iter-8100 train loss: 2.2496 valid loss: 2.2459, valid accuracy: 0.2416\n",
      "Iter-8200 train loss: 2.2476 valid loss: 2.2452, valid accuracy: 0.2438\n",
      "Iter-8300 train loss: 2.2543 valid loss: 2.2444, valid accuracy: 0.2458\n",
      "Iter-8400 train loss: 2.2420 valid loss: 2.2437, valid accuracy: 0.2490\n",
      "Iter-8500 train loss: 2.2708 valid loss: 2.2429, valid accuracy: 0.2512\n",
      "Iter-8600 train loss: 2.2248 valid loss: 2.2422, valid accuracy: 0.2534\n",
      "Iter-8700 train loss: 2.2605 valid loss: 2.2414, valid accuracy: 0.2544\n",
      "Iter-8800 train loss: 2.2296 valid loss: 2.2406, valid accuracy: 0.2560\n",
      "Iter-8900 train loss: 2.2625 valid loss: 2.2399, valid accuracy: 0.2574\n",
      "Iter-9000 train loss: 2.2486 valid loss: 2.2391, valid accuracy: 0.2582\n",
      "Iter-9100 train loss: 2.2417 valid loss: 2.2384, valid accuracy: 0.2606\n",
      "Iter-9200 train loss: 2.2357 valid loss: 2.2376, valid accuracy: 0.2620\n",
      "Iter-9300 train loss: 2.2365 valid loss: 2.2369, valid accuracy: 0.2634\n",
      "Iter-9400 train loss: 2.2335 valid loss: 2.2361, valid accuracy: 0.2658\n",
      "Iter-9500 train loss: 2.2431 valid loss: 2.2354, valid accuracy: 0.2680\n",
      "Iter-9600 train loss: 2.2506 valid loss: 2.2346, valid accuracy: 0.2694\n",
      "Iter-9700 train loss: 2.2348 valid loss: 2.2339, valid accuracy: 0.2702\n",
      "Iter-9800 train loss: 2.2180 valid loss: 2.2331, valid accuracy: 0.2706\n",
      "Iter-9900 train loss: 2.2477 valid loss: 2.2324, valid accuracy: 0.2720\n",
      "Iter-10000 train loss: 2.2127 valid loss: 2.2317, valid accuracy: 0.2734\n",
      "Iter-10100 train loss: 2.2303 valid loss: 2.2309, valid accuracy: 0.2740\n",
      "Iter-10200 train loss: 2.2083 valid loss: 2.2302, valid accuracy: 0.2758\n",
      "Iter-10300 train loss: 2.2018 valid loss: 2.2294, valid accuracy: 0.2790\n",
      "Iter-10400 train loss: 2.2232 valid loss: 2.2287, valid accuracy: 0.2788\n",
      "Iter-10500 train loss: 2.2113 valid loss: 2.2279, valid accuracy: 0.2808\n",
      "Iter-10600 train loss: 2.2298 valid loss: 2.2272, valid accuracy: 0.2838\n",
      "Iter-10700 train loss: 2.2455 valid loss: 2.2265, valid accuracy: 0.2842\n",
      "Iter-10800 train loss: 2.2573 valid loss: 2.2257, valid accuracy: 0.2864\n",
      "Iter-10900 train loss: 2.2151 valid loss: 2.2250, valid accuracy: 0.2874\n",
      "Iter-11000 train loss: 2.2188 valid loss: 2.2243, valid accuracy: 0.2888\n",
      "Iter-11100 train loss: 2.2493 valid loss: 2.2235, valid accuracy: 0.2906\n",
      "Iter-11200 train loss: 2.2156 valid loss: 2.2228, valid accuracy: 0.2924\n",
      "Iter-11300 train loss: 2.2191 valid loss: 2.2221, valid accuracy: 0.2942\n",
      "Iter-11400 train loss: 2.2237 valid loss: 2.2214, valid accuracy: 0.2954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.2507 valid loss: 2.2207, valid accuracy: 0.2964\n",
      "Iter-11600 train loss: 2.2147 valid loss: 2.2199, valid accuracy: 0.2976\n",
      "Iter-11700 train loss: 2.1734 valid loss: 2.2192, valid accuracy: 0.2992\n",
      "Iter-11800 train loss: 2.1807 valid loss: 2.2184, valid accuracy: 0.2994\n",
      "Iter-11900 train loss: 2.2230 valid loss: 2.2177, valid accuracy: 0.3006\n",
      "Iter-12000 train loss: 2.1924 valid loss: 2.2170, valid accuracy: 0.3016\n",
      "Iter-12100 train loss: 2.2275 valid loss: 2.2163, valid accuracy: 0.3026\n",
      "Iter-12200 train loss: 2.1893 valid loss: 2.2155, valid accuracy: 0.3038\n",
      "Iter-12300 train loss: 2.2254 valid loss: 2.2148, valid accuracy: 0.3040\n",
      "Iter-12400 train loss: 2.2061 valid loss: 2.2141, valid accuracy: 0.3062\n",
      "Iter-12500 train loss: 2.2358 valid loss: 2.2134, valid accuracy: 0.3066\n",
      "Iter-12600 train loss: 2.2011 valid loss: 2.2127, valid accuracy: 0.3078\n",
      "Iter-12700 train loss: 2.2345 valid loss: 2.2120, valid accuracy: 0.3094\n",
      "Iter-12800 train loss: 2.2300 valid loss: 2.2113, valid accuracy: 0.3100\n",
      "Iter-12900 train loss: 2.2250 valid loss: 2.2106, valid accuracy: 0.3114\n",
      "Iter-13000 train loss: 2.2358 valid loss: 2.2099, valid accuracy: 0.3120\n",
      "Iter-13100 train loss: 2.2293 valid loss: 2.2091, valid accuracy: 0.3138\n",
      "Iter-13200 train loss: 2.2209 valid loss: 2.2084, valid accuracy: 0.3144\n",
      "Iter-13300 train loss: 2.2024 valid loss: 2.2077, valid accuracy: 0.3148\n",
      "Iter-13400 train loss: 2.2359 valid loss: 2.2071, valid accuracy: 0.3154\n",
      "Iter-13500 train loss: 2.2033 valid loss: 2.2064, valid accuracy: 0.3162\n",
      "Iter-13600 train loss: 2.2340 valid loss: 2.2057, valid accuracy: 0.3170\n",
      "Iter-13700 train loss: 2.1990 valid loss: 2.2050, valid accuracy: 0.3176\n",
      "Iter-13800 train loss: 2.1873 valid loss: 2.2043, valid accuracy: 0.3190\n",
      "Iter-13900 train loss: 2.2253 valid loss: 2.2035, valid accuracy: 0.3204\n",
      "Iter-14000 train loss: 2.2086 valid loss: 2.2028, valid accuracy: 0.3208\n",
      "Iter-14100 train loss: 2.2030 valid loss: 2.2021, valid accuracy: 0.3208\n",
      "Iter-14200 train loss: 2.2060 valid loss: 2.2014, valid accuracy: 0.3210\n",
      "Iter-14300 train loss: 2.1889 valid loss: 2.2007, valid accuracy: 0.3218\n",
      "Iter-14400 train loss: 2.2022 valid loss: 2.2000, valid accuracy: 0.3220\n",
      "Iter-14500 train loss: 2.2043 valid loss: 2.1993, valid accuracy: 0.3224\n",
      "Iter-14600 train loss: 2.1980 valid loss: 2.1986, valid accuracy: 0.3226\n",
      "Iter-14700 train loss: 2.2258 valid loss: 2.1980, valid accuracy: 0.3252\n",
      "Iter-14800 train loss: 2.1895 valid loss: 2.1973, valid accuracy: 0.3256\n",
      "Iter-14900 train loss: 2.1502 valid loss: 2.1966, valid accuracy: 0.3248\n",
      "Iter-15000 train loss: 2.1758 valid loss: 2.1959, valid accuracy: 0.3274\n",
      "Iter-15100 train loss: 2.1726 valid loss: 2.1952, valid accuracy: 0.3286\n",
      "Iter-15200 train loss: 2.2047 valid loss: 2.1945, valid accuracy: 0.3298\n",
      "Iter-15300 train loss: 2.1953 valid loss: 2.1938, valid accuracy: 0.3310\n",
      "Iter-15400 train loss: 2.1720 valid loss: 2.1931, valid accuracy: 0.3316\n",
      "Iter-15500 train loss: 2.2175 valid loss: 2.1924, valid accuracy: 0.3318\n",
      "Iter-15600 train loss: 2.1882 valid loss: 2.1917, valid accuracy: 0.3318\n",
      "Iter-15700 train loss: 2.2232 valid loss: 2.1910, valid accuracy: 0.3338\n",
      "Iter-15800 train loss: 2.1946 valid loss: 2.1903, valid accuracy: 0.3340\n",
      "Iter-15900 train loss: 2.1888 valid loss: 2.1897, valid accuracy: 0.3340\n",
      "Iter-16000 train loss: 2.1827 valid loss: 2.1890, valid accuracy: 0.3348\n",
      "Iter-16100 train loss: 2.1880 valid loss: 2.1883, valid accuracy: 0.3362\n",
      "Iter-16200 train loss: 2.2114 valid loss: 2.1876, valid accuracy: 0.3368\n",
      "Iter-16300 train loss: 2.1933 valid loss: 2.1869, valid accuracy: 0.3376\n",
      "Iter-16400 train loss: 2.2032 valid loss: 2.1863, valid accuracy: 0.3380\n",
      "Iter-16500 train loss: 2.1871 valid loss: 2.1856, valid accuracy: 0.3388\n",
      "Iter-16600 train loss: 2.1414 valid loss: 2.1849, valid accuracy: 0.3398\n",
      "Iter-16700 train loss: 2.1820 valid loss: 2.1843, valid accuracy: 0.3398\n",
      "Iter-16800 train loss: 2.1712 valid loss: 2.1836, valid accuracy: 0.3406\n",
      "Iter-16900 train loss: 2.1896 valid loss: 2.1829, valid accuracy: 0.3412\n",
      "Iter-17000 train loss: 2.1693 valid loss: 2.1822, valid accuracy: 0.3414\n",
      "Iter-17100 train loss: 2.1583 valid loss: 2.1816, valid accuracy: 0.3422\n",
      "Iter-17200 train loss: 2.2093 valid loss: 2.1809, valid accuracy: 0.3426\n",
      "Iter-17300 train loss: 2.1751 valid loss: 2.1802, valid accuracy: 0.3440\n",
      "Iter-17400 train loss: 2.1952 valid loss: 2.1796, valid accuracy: 0.3454\n",
      "Iter-17500 train loss: 2.2003 valid loss: 2.1789, valid accuracy: 0.3454\n",
      "Iter-17600 train loss: 2.1907 valid loss: 2.1783, valid accuracy: 0.3458\n",
      "Iter-17700 train loss: 2.1768 valid loss: 2.1776, valid accuracy: 0.3464\n",
      "Iter-17800 train loss: 2.2001 valid loss: 2.1770, valid accuracy: 0.3464\n",
      "Iter-17900 train loss: 2.1837 valid loss: 2.1763, valid accuracy: 0.3468\n",
      "Iter-18000 train loss: 2.1898 valid loss: 2.1756, valid accuracy: 0.3474\n",
      "Iter-18100 train loss: 2.2059 valid loss: 2.1750, valid accuracy: 0.3482\n",
      "Iter-18200 train loss: 2.2212 valid loss: 2.1743, valid accuracy: 0.3496\n",
      "Iter-18300 train loss: 2.1360 valid loss: 2.1737, valid accuracy: 0.3512\n",
      "Iter-18400 train loss: 2.1532 valid loss: 2.1730, valid accuracy: 0.3518\n",
      "Iter-18500 train loss: 2.2081 valid loss: 2.1724, valid accuracy: 0.3526\n",
      "Iter-18600 train loss: 2.1784 valid loss: 2.1717, valid accuracy: 0.3524\n",
      "Iter-18700 train loss: 2.1882 valid loss: 2.1711, valid accuracy: 0.3538\n",
      "Iter-18800 train loss: 2.1321 valid loss: 2.1704, valid accuracy: 0.3540\n",
      "Iter-18900 train loss: 2.1764 valid loss: 2.1698, valid accuracy: 0.3550\n",
      "Iter-19000 train loss: 2.1951 valid loss: 2.1691, valid accuracy: 0.3560\n",
      "Iter-19100 train loss: 2.1928 valid loss: 2.1685, valid accuracy: 0.3564\n",
      "Iter-19200 train loss: 2.1943 valid loss: 2.1678, valid accuracy: 0.3564\n",
      "Iter-19300 train loss: 2.1588 valid loss: 2.1672, valid accuracy: 0.3566\n",
      "Iter-19400 train loss: 2.1026 valid loss: 2.1665, valid accuracy: 0.3570\n",
      "Iter-19500 train loss: 2.1475 valid loss: 2.1659, valid accuracy: 0.3578\n",
      "Iter-19600 train loss: 2.1684 valid loss: 2.1653, valid accuracy: 0.3582\n",
      "Iter-19700 train loss: 2.1840 valid loss: 2.1646, valid accuracy: 0.3584\n",
      "Iter-19800 train loss: 2.1704 valid loss: 2.1640, valid accuracy: 0.3584\n",
      "Iter-19900 train loss: 2.1454 valid loss: 2.1634, valid accuracy: 0.3592\n",
      "Iter-20000 train loss: 2.1563 valid loss: 2.1627, valid accuracy: 0.3594\n",
      "Iter-20100 train loss: 2.1077 valid loss: 2.1621, valid accuracy: 0.3606\n",
      "Iter-20200 train loss: 2.1530 valid loss: 2.1614, valid accuracy: 0.3604\n",
      "Iter-20300 train loss: 2.1518 valid loss: 2.1608, valid accuracy: 0.3604\n",
      "Iter-20400 train loss: 2.1783 valid loss: 2.1602, valid accuracy: 0.3608\n",
      "Iter-20500 train loss: 2.1638 valid loss: 2.1595, valid accuracy: 0.3612\n",
      "Iter-20600 train loss: 2.1435 valid loss: 2.1589, valid accuracy: 0.3614\n",
      "Iter-20700 train loss: 2.1989 valid loss: 2.1583, valid accuracy: 0.3616\n",
      "Iter-20800 train loss: 2.1396 valid loss: 2.1576, valid accuracy: 0.3624\n",
      "Iter-20900 train loss: 2.1207 valid loss: 2.1570, valid accuracy: 0.3630\n",
      "Iter-21000 train loss: 2.1523 valid loss: 2.1563, valid accuracy: 0.3634\n",
      "Iter-21100 train loss: 2.1375 valid loss: 2.1557, valid accuracy: 0.3638\n",
      "Iter-21200 train loss: 2.1803 valid loss: 2.1551, valid accuracy: 0.3650\n",
      "Iter-21300 train loss: 2.0992 valid loss: 2.1544, valid accuracy: 0.3656\n",
      "Iter-21400 train loss: 2.1838 valid loss: 2.1538, valid accuracy: 0.3660\n",
      "Iter-21500 train loss: 2.1176 valid loss: 2.1532, valid accuracy: 0.3666\n",
      "Iter-21600 train loss: 2.1462 valid loss: 2.1526, valid accuracy: 0.3678\n",
      "Iter-21700 train loss: 2.1554 valid loss: 2.1519, valid accuracy: 0.3684\n",
      "Iter-21800 train loss: 2.0949 valid loss: 2.1513, valid accuracy: 0.3684\n",
      "Iter-21900 train loss: 2.1629 valid loss: 2.1507, valid accuracy: 0.3698\n",
      "Iter-22000 train loss: 2.1585 valid loss: 2.1500, valid accuracy: 0.3706\n",
      "Iter-22100 train loss: 2.1206 valid loss: 2.1494, valid accuracy: 0.3704\n",
      "Iter-22200 train loss: 2.1508 valid loss: 2.1488, valid accuracy: 0.3704\n",
      "Iter-22300 train loss: 2.1647 valid loss: 2.1482, valid accuracy: 0.3708\n",
      "Iter-22400 train loss: 2.1463 valid loss: 2.1476, valid accuracy: 0.3714\n",
      "Iter-22500 train loss: 2.1703 valid loss: 2.1470, valid accuracy: 0.3714\n",
      "Iter-22600 train loss: 2.1740 valid loss: 2.1464, valid accuracy: 0.3716\n",
      "Iter-22700 train loss: 2.1702 valid loss: 2.1457, valid accuracy: 0.3716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.1208 valid loss: 2.1451, valid accuracy: 0.3716\n",
      "Iter-22900 train loss: 2.1134 valid loss: 2.1445, valid accuracy: 0.3728\n",
      "Iter-23000 train loss: 2.1234 valid loss: 2.1439, valid accuracy: 0.3732\n",
      "Iter-23100 train loss: 2.2036 valid loss: 2.1433, valid accuracy: 0.3746\n",
      "Iter-23200 train loss: 2.1445 valid loss: 2.1427, valid accuracy: 0.3746\n",
      "Iter-23300 train loss: 2.1534 valid loss: 2.1421, valid accuracy: 0.3754\n",
      "Iter-23400 train loss: 2.1492 valid loss: 2.1415, valid accuracy: 0.3766\n",
      "Iter-23500 train loss: 2.1394 valid loss: 2.1409, valid accuracy: 0.3776\n",
      "Iter-23600 train loss: 2.1278 valid loss: 2.1403, valid accuracy: 0.3780\n",
      "Iter-23700 train loss: 2.1405 valid loss: 2.1397, valid accuracy: 0.3780\n",
      "Iter-23800 train loss: 2.1283 valid loss: 2.1391, valid accuracy: 0.3776\n",
      "Iter-23900 train loss: 2.0992 valid loss: 2.1385, valid accuracy: 0.3772\n",
      "Iter-24000 train loss: 2.1543 valid loss: 2.1379, valid accuracy: 0.3774\n",
      "Iter-24100 train loss: 2.1383 valid loss: 2.1373, valid accuracy: 0.3780\n",
      "Iter-24200 train loss: 2.1379 valid loss: 2.1367, valid accuracy: 0.3782\n",
      "Iter-24300 train loss: 2.1479 valid loss: 2.1361, valid accuracy: 0.3788\n",
      "Iter-24400 train loss: 2.1232 valid loss: 2.1355, valid accuracy: 0.3792\n",
      "Iter-24500 train loss: 2.0920 valid loss: 2.1349, valid accuracy: 0.3796\n",
      "Iter-24600 train loss: 2.1278 valid loss: 2.1343, valid accuracy: 0.3794\n",
      "Iter-24700 train loss: 2.1186 valid loss: 2.1336, valid accuracy: 0.3800\n",
      "Iter-24800 train loss: 2.1498 valid loss: 2.1330, valid accuracy: 0.3800\n",
      "Iter-24900 train loss: 2.1401 valid loss: 2.1324, valid accuracy: 0.3800\n",
      "Iter-25000 train loss: 2.1548 valid loss: 2.1319, valid accuracy: 0.3800\n",
      "Iter-25100 train loss: 2.0928 valid loss: 2.1313, valid accuracy: 0.3804\n",
      "Iter-25200 train loss: 2.1515 valid loss: 2.1307, valid accuracy: 0.3802\n",
      "Iter-25300 train loss: 2.1268 valid loss: 2.1301, valid accuracy: 0.3806\n",
      "Iter-25400 train loss: 2.1155 valid loss: 2.1295, valid accuracy: 0.3810\n",
      "Iter-25500 train loss: 2.1393 valid loss: 2.1289, valid accuracy: 0.3812\n",
      "Iter-25600 train loss: 2.1566 valid loss: 2.1283, valid accuracy: 0.3816\n",
      "Iter-25700 train loss: 2.1343 valid loss: 2.1277, valid accuracy: 0.3820\n",
      "Iter-25800 train loss: 2.1124 valid loss: 2.1271, valid accuracy: 0.3826\n",
      "Iter-25900 train loss: 2.1223 valid loss: 2.1265, valid accuracy: 0.3826\n",
      "Iter-26000 train loss: 2.1545 valid loss: 2.1259, valid accuracy: 0.3832\n",
      "Iter-26100 train loss: 2.1380 valid loss: 2.1253, valid accuracy: 0.3830\n",
      "Iter-26200 train loss: 2.1339 valid loss: 2.1248, valid accuracy: 0.3842\n",
      "Iter-26300 train loss: 2.1694 valid loss: 2.1242, valid accuracy: 0.3842\n",
      "Iter-26400 train loss: 2.1491 valid loss: 2.1236, valid accuracy: 0.3842\n",
      "Iter-26500 train loss: 2.1848 valid loss: 2.1230, valid accuracy: 0.3842\n",
      "Iter-26600 train loss: 2.1289 valid loss: 2.1224, valid accuracy: 0.3836\n",
      "Iter-26700 train loss: 2.1215 valid loss: 2.1218, valid accuracy: 0.3836\n",
      "Iter-26800 train loss: 2.1177 valid loss: 2.1213, valid accuracy: 0.3842\n",
      "Iter-26900 train loss: 2.1458 valid loss: 2.1207, valid accuracy: 0.3848\n",
      "Iter-27000 train loss: 2.1465 valid loss: 2.1201, valid accuracy: 0.3852\n",
      "Iter-27100 train loss: 2.0865 valid loss: 2.1195, valid accuracy: 0.3854\n",
      "Iter-27200 train loss: 2.1497 valid loss: 2.1190, valid accuracy: 0.3868\n",
      "Iter-27300 train loss: 2.1335 valid loss: 2.1184, valid accuracy: 0.3868\n",
      "Iter-27400 train loss: 2.0786 valid loss: 2.1178, valid accuracy: 0.3876\n",
      "Iter-27500 train loss: 2.0680 valid loss: 2.1172, valid accuracy: 0.3874\n",
      "Iter-27600 train loss: 2.0967 valid loss: 2.1167, valid accuracy: 0.3874\n",
      "Iter-27700 train loss: 2.1340 valid loss: 2.1161, valid accuracy: 0.3874\n",
      "Iter-27800 train loss: 2.1455 valid loss: 2.1155, valid accuracy: 0.3878\n",
      "Iter-27900 train loss: 2.1054 valid loss: 2.1149, valid accuracy: 0.3878\n",
      "Iter-28000 train loss: 2.1435 valid loss: 2.1143, valid accuracy: 0.3886\n",
      "Iter-28100 train loss: 2.1675 valid loss: 2.1138, valid accuracy: 0.3886\n",
      "Iter-28200 train loss: 2.1028 valid loss: 2.1132, valid accuracy: 0.3892\n",
      "Iter-28300 train loss: 2.1189 valid loss: 2.1126, valid accuracy: 0.3900\n",
      "Iter-28400 train loss: 2.0956 valid loss: 2.1120, valid accuracy: 0.3902\n",
      "Iter-28500 train loss: 2.1345 valid loss: 2.1115, valid accuracy: 0.3902\n",
      "Iter-28600 train loss: 2.1737 valid loss: 2.1109, valid accuracy: 0.3906\n",
      "Iter-28700 train loss: 2.0734 valid loss: 2.1103, valid accuracy: 0.3906\n",
      "Iter-28800 train loss: 2.0910 valid loss: 2.1097, valid accuracy: 0.3914\n",
      "Iter-28900 train loss: 2.0867 valid loss: 2.1092, valid accuracy: 0.3918\n",
      "Iter-29000 train loss: 2.1176 valid loss: 2.1086, valid accuracy: 0.3922\n",
      "Iter-29100 train loss: 2.1886 valid loss: 2.1080, valid accuracy: 0.3930\n",
      "Iter-29200 train loss: 2.0620 valid loss: 2.1075, valid accuracy: 0.3930\n",
      "Iter-29300 train loss: 2.1773 valid loss: 2.1069, valid accuracy: 0.3930\n",
      "Iter-29400 train loss: 2.0840 valid loss: 2.1063, valid accuracy: 0.3930\n",
      "Iter-29500 train loss: 2.1013 valid loss: 2.1058, valid accuracy: 0.3932\n",
      "Iter-29600 train loss: 2.0622 valid loss: 2.1052, valid accuracy: 0.3932\n",
      "Iter-29700 train loss: 2.1039 valid loss: 2.1047, valid accuracy: 0.3936\n",
      "Iter-29800 train loss: 2.0930 valid loss: 2.1041, valid accuracy: 0.3928\n",
      "Iter-29900 train loss: 2.1394 valid loss: 2.1036, valid accuracy: 0.3938\n",
      "Iter-30000 train loss: 2.1716 valid loss: 2.1030, valid accuracy: 0.3938\n",
      "Iter-30100 train loss: 2.1208 valid loss: 2.1025, valid accuracy: 0.3938\n",
      "Iter-30200 train loss: 2.1212 valid loss: 2.1019, valid accuracy: 0.3942\n",
      "Iter-30300 train loss: 2.1551 valid loss: 2.1014, valid accuracy: 0.3944\n",
      "Iter-30400 train loss: 2.0840 valid loss: 2.1008, valid accuracy: 0.3950\n",
      "Iter-30500 train loss: 2.0751 valid loss: 2.1002, valid accuracy: 0.3948\n",
      "Iter-30600 train loss: 2.0352 valid loss: 2.0997, valid accuracy: 0.3948\n",
      "Iter-30700 train loss: 2.1101 valid loss: 2.0991, valid accuracy: 0.3940\n",
      "Iter-30800 train loss: 2.1032 valid loss: 2.0986, valid accuracy: 0.3950\n",
      "Iter-30900 train loss: 2.1438 valid loss: 2.0980, valid accuracy: 0.3952\n",
      "Iter-31000 train loss: 2.1334 valid loss: 2.0975, valid accuracy: 0.3950\n",
      "Iter-31100 train loss: 2.1691 valid loss: 2.0969, valid accuracy: 0.3954\n",
      "Iter-31200 train loss: 2.0806 valid loss: 2.0964, valid accuracy: 0.3958\n",
      "Iter-31300 train loss: 2.0665 valid loss: 2.0959, valid accuracy: 0.3962\n",
      "Iter-31400 train loss: 2.1313 valid loss: 2.0953, valid accuracy: 0.3960\n",
      "Iter-31500 train loss: 2.0750 valid loss: 2.0948, valid accuracy: 0.3954\n",
      "Iter-31600 train loss: 2.1068 valid loss: 2.0942, valid accuracy: 0.3960\n",
      "Iter-31700 train loss: 2.0918 valid loss: 2.0937, valid accuracy: 0.3960\n",
      "Iter-31800 train loss: 2.0667 valid loss: 2.0931, valid accuracy: 0.3960\n",
      "Iter-31900 train loss: 2.0762 valid loss: 2.0926, valid accuracy: 0.3964\n",
      "Iter-32000 train loss: 2.0651 valid loss: 2.0920, valid accuracy: 0.3964\n",
      "Iter-32100 train loss: 2.1066 valid loss: 2.0915, valid accuracy: 0.3966\n",
      "Iter-32200 train loss: 2.0395 valid loss: 2.0909, valid accuracy: 0.3962\n",
      "Iter-32300 train loss: 2.1389 valid loss: 2.0904, valid accuracy: 0.3966\n",
      "Iter-32400 train loss: 2.1087 valid loss: 2.0899, valid accuracy: 0.3970\n",
      "Iter-32500 train loss: 2.0631 valid loss: 2.0893, valid accuracy: 0.3970\n",
      "Iter-32600 train loss: 2.0914 valid loss: 2.0888, valid accuracy: 0.3966\n",
      "Iter-32700 train loss: 2.1416 valid loss: 2.0883, valid accuracy: 0.3972\n",
      "Iter-32800 train loss: 2.0710 valid loss: 2.0877, valid accuracy: 0.3978\n",
      "Iter-32900 train loss: 2.0337 valid loss: 2.0872, valid accuracy: 0.3980\n",
      "Iter-33000 train loss: 2.1076 valid loss: 2.0866, valid accuracy: 0.3984\n",
      "Iter-33100 train loss: 2.1156 valid loss: 2.0861, valid accuracy: 0.3978\n",
      "Iter-33200 train loss: 2.0542 valid loss: 2.0856, valid accuracy: 0.3980\n",
      "Iter-33300 train loss: 2.1375 valid loss: 2.0851, valid accuracy: 0.3984\n",
      "Iter-33400 train loss: 2.1173 valid loss: 2.0845, valid accuracy: 0.3984\n",
      "Iter-33500 train loss: 2.1194 valid loss: 2.0840, valid accuracy: 0.3986\n",
      "Iter-33600 train loss: 2.0565 valid loss: 2.0835, valid accuracy: 0.3994\n",
      "Iter-33700 train loss: 2.0467 valid loss: 2.0830, valid accuracy: 0.4002\n",
      "Iter-33800 train loss: 2.0910 valid loss: 2.0824, valid accuracy: 0.3996\n",
      "Iter-33900 train loss: 2.0608 valid loss: 2.0819, valid accuracy: 0.4000\n",
      "Iter-34000 train loss: 2.1052 valid loss: 2.0814, valid accuracy: 0.4008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.1510 valid loss: 2.0808, valid accuracy: 0.4004\n",
      "Iter-34200 train loss: 2.0582 valid loss: 2.0803, valid accuracy: 0.4002\n",
      "Iter-34300 train loss: 2.1457 valid loss: 2.0797, valid accuracy: 0.4004\n",
      "Iter-34400 train loss: 2.1362 valid loss: 2.0792, valid accuracy: 0.4004\n",
      "Iter-34500 train loss: 2.1441 valid loss: 2.0787, valid accuracy: 0.4008\n",
      "Iter-34600 train loss: 2.1001 valid loss: 2.0782, valid accuracy: 0.4014\n",
      "Iter-34700 train loss: 2.1189 valid loss: 2.0777, valid accuracy: 0.4016\n",
      "Iter-34800 train loss: 2.0263 valid loss: 2.0771, valid accuracy: 0.4022\n",
      "Iter-34900 train loss: 2.1581 valid loss: 2.0766, valid accuracy: 0.4028\n",
      "Iter-35000 train loss: 2.1361 valid loss: 2.0761, valid accuracy: 0.4034\n",
      "Iter-35100 train loss: 2.0773 valid loss: 2.0756, valid accuracy: 0.4032\n",
      "Iter-35200 train loss: 2.0991 valid loss: 2.0751, valid accuracy: 0.4034\n",
      "Iter-35300 train loss: 2.0935 valid loss: 2.0746, valid accuracy: 0.4030\n",
      "Iter-35400 train loss: 2.1405 valid loss: 2.0740, valid accuracy: 0.4034\n",
      "Iter-35500 train loss: 2.0617 valid loss: 2.0735, valid accuracy: 0.4030\n",
      "Iter-35600 train loss: 2.0373 valid loss: 2.0730, valid accuracy: 0.4034\n",
      "Iter-35700 train loss: 2.0094 valid loss: 2.0725, valid accuracy: 0.4034\n",
      "Iter-35800 train loss: 2.1430 valid loss: 2.0719, valid accuracy: 0.4038\n",
      "Iter-35900 train loss: 2.1537 valid loss: 2.0714, valid accuracy: 0.4040\n",
      "Iter-36000 train loss: 2.0968 valid loss: 2.0709, valid accuracy: 0.4042\n",
      "Iter-36100 train loss: 2.1186 valid loss: 2.0704, valid accuracy: 0.4040\n",
      "Iter-36200 train loss: 2.1161 valid loss: 2.0699, valid accuracy: 0.4040\n",
      "Iter-36300 train loss: 2.1167 valid loss: 2.0693, valid accuracy: 0.4042\n",
      "Iter-36400 train loss: 2.0398 valid loss: 2.0688, valid accuracy: 0.4046\n",
      "Iter-36500 train loss: 2.1128 valid loss: 2.0683, valid accuracy: 0.4042\n",
      "Iter-36600 train loss: 2.0371 valid loss: 2.0678, valid accuracy: 0.4042\n",
      "Iter-36700 train loss: 2.0717 valid loss: 2.0673, valid accuracy: 0.4044\n",
      "Iter-36800 train loss: 2.0341 valid loss: 2.0668, valid accuracy: 0.4050\n",
      "Iter-36900 train loss: 2.0885 valid loss: 2.0663, valid accuracy: 0.4054\n",
      "Iter-37000 train loss: 2.0661 valid loss: 2.0658, valid accuracy: 0.4054\n",
      "Iter-37100 train loss: 2.0462 valid loss: 2.0653, valid accuracy: 0.4058\n",
      "Iter-37200 train loss: 2.1271 valid loss: 2.0648, valid accuracy: 0.4062\n",
      "Iter-37300 train loss: 2.0970 valid loss: 2.0642, valid accuracy: 0.4066\n",
      "Iter-37400 train loss: 2.0652 valid loss: 2.0637, valid accuracy: 0.4076\n",
      "Iter-37500 train loss: 2.0972 valid loss: 2.0632, valid accuracy: 0.4078\n",
      "Iter-37600 train loss: 2.0442 valid loss: 2.0627, valid accuracy: 0.4078\n",
      "Iter-37700 train loss: 2.0418 valid loss: 2.0622, valid accuracy: 0.4080\n",
      "Iter-37800 train loss: 2.0547 valid loss: 2.0617, valid accuracy: 0.4076\n",
      "Iter-37900 train loss: 2.0473 valid loss: 2.0612, valid accuracy: 0.4084\n",
      "Iter-38000 train loss: 2.0761 valid loss: 2.0607, valid accuracy: 0.4086\n",
      "Iter-38100 train loss: 2.1407 valid loss: 2.0602, valid accuracy: 0.4088\n",
      "Iter-38200 train loss: 2.1151 valid loss: 2.0597, valid accuracy: 0.4090\n",
      "Iter-38300 train loss: 2.0662 valid loss: 2.0592, valid accuracy: 0.4092\n",
      "Iter-38400 train loss: 2.1197 valid loss: 2.0587, valid accuracy: 0.4096\n",
      "Iter-38500 train loss: 2.0947 valid loss: 2.0582, valid accuracy: 0.4104\n",
      "Iter-38600 train loss: 2.0954 valid loss: 2.0577, valid accuracy: 0.4108\n",
      "Iter-38700 train loss: 2.0765 valid loss: 2.0572, valid accuracy: 0.4114\n",
      "Iter-38800 train loss: 2.0615 valid loss: 2.0567, valid accuracy: 0.4108\n",
      "Iter-38900 train loss: 1.9980 valid loss: 2.0562, valid accuracy: 0.4108\n",
      "Iter-39000 train loss: 2.0214 valid loss: 2.0557, valid accuracy: 0.4102\n",
      "Iter-39100 train loss: 2.0912 valid loss: 2.0552, valid accuracy: 0.4106\n",
      "Iter-39200 train loss: 2.0887 valid loss: 2.0547, valid accuracy: 0.4112\n",
      "Iter-39300 train loss: 2.0767 valid loss: 2.0542, valid accuracy: 0.4114\n",
      "Iter-39400 train loss: 2.0630 valid loss: 2.0537, valid accuracy: 0.4110\n",
      "Iter-39500 train loss: 2.0573 valid loss: 2.0533, valid accuracy: 0.4112\n",
      "Iter-39600 train loss: 2.0530 valid loss: 2.0528, valid accuracy: 0.4114\n",
      "Iter-39700 train loss: 2.0505 valid loss: 2.0523, valid accuracy: 0.4118\n",
      "Iter-39800 train loss: 2.0560 valid loss: 2.0518, valid accuracy: 0.4122\n",
      "Iter-39900 train loss: 2.0210 valid loss: 2.0513, valid accuracy: 0.4124\n",
      "Iter-40000 train loss: 2.0458 valid loss: 2.0508, valid accuracy: 0.4120\n",
      "Iter-40100 train loss: 2.0548 valid loss: 2.0503, valid accuracy: 0.4126\n",
      "Iter-40200 train loss: 2.0803 valid loss: 2.0498, valid accuracy: 0.4134\n",
      "Iter-40300 train loss: 2.1384 valid loss: 2.0493, valid accuracy: 0.4132\n",
      "Iter-40400 train loss: 2.0190 valid loss: 2.0489, valid accuracy: 0.4130\n",
      "Iter-40500 train loss: 2.0421 valid loss: 2.0484, valid accuracy: 0.4134\n",
      "Iter-40600 train loss: 2.0624 valid loss: 2.0479, valid accuracy: 0.4134\n",
      "Iter-40700 train loss: 2.0526 valid loss: 2.0474, valid accuracy: 0.4128\n",
      "Iter-40800 train loss: 2.1099 valid loss: 2.0469, valid accuracy: 0.4136\n",
      "Iter-40900 train loss: 2.0203 valid loss: 2.0464, valid accuracy: 0.4136\n",
      "Iter-41000 train loss: 2.0638 valid loss: 2.0459, valid accuracy: 0.4134\n",
      "Iter-41100 train loss: 2.0107 valid loss: 2.0454, valid accuracy: 0.4140\n",
      "Iter-41200 train loss: 2.0508 valid loss: 2.0449, valid accuracy: 0.4144\n",
      "Iter-41300 train loss: 2.1163 valid loss: 2.0444, valid accuracy: 0.4144\n",
      "Iter-41400 train loss: 2.0723 valid loss: 2.0440, valid accuracy: 0.4146\n",
      "Iter-41500 train loss: 2.1078 valid loss: 2.0435, valid accuracy: 0.4148\n",
      "Iter-41600 train loss: 2.1276 valid loss: 2.0430, valid accuracy: 0.4146\n",
      "Iter-41700 train loss: 2.0808 valid loss: 2.0425, valid accuracy: 0.4146\n",
      "Iter-41800 train loss: 2.0657 valid loss: 2.0421, valid accuracy: 0.4152\n",
      "Iter-41900 train loss: 2.0559 valid loss: 2.0416, valid accuracy: 0.4152\n",
      "Iter-42000 train loss: 2.0307 valid loss: 2.0411, valid accuracy: 0.4156\n",
      "Iter-42100 train loss: 1.9799 valid loss: 2.0406, valid accuracy: 0.4162\n",
      "Iter-42200 train loss: 2.0080 valid loss: 2.0401, valid accuracy: 0.4156\n",
      "Iter-42300 train loss: 2.0833 valid loss: 2.0396, valid accuracy: 0.4160\n",
      "Iter-42400 train loss: 2.1138 valid loss: 2.0392, valid accuracy: 0.4160\n",
      "Iter-42500 train loss: 2.0187 valid loss: 2.0387, valid accuracy: 0.4168\n",
      "Iter-42600 train loss: 2.0682 valid loss: 2.0382, valid accuracy: 0.4168\n",
      "Iter-42700 train loss: 2.0442 valid loss: 2.0378, valid accuracy: 0.4168\n",
      "Iter-42800 train loss: 2.0495 valid loss: 2.0373, valid accuracy: 0.4164\n",
      "Iter-42900 train loss: 2.0349 valid loss: 2.0368, valid accuracy: 0.4164\n",
      "Iter-43000 train loss: 1.9780 valid loss: 2.0364, valid accuracy: 0.4168\n",
      "Iter-43100 train loss: 2.0431 valid loss: 2.0359, valid accuracy: 0.4168\n",
      "Iter-43200 train loss: 2.0696 valid loss: 2.0354, valid accuracy: 0.4166\n",
      "Iter-43300 train loss: 2.0275 valid loss: 2.0349, valid accuracy: 0.4166\n",
      "Iter-43400 train loss: 2.0432 valid loss: 2.0345, valid accuracy: 0.4172\n",
      "Iter-43500 train loss: 2.0518 valid loss: 2.0340, valid accuracy: 0.4178\n",
      "Iter-43600 train loss: 2.1005 valid loss: 2.0335, valid accuracy: 0.4180\n",
      "Iter-43700 train loss: 2.0205 valid loss: 2.0330, valid accuracy: 0.4182\n",
      "Iter-43800 train loss: 2.0350 valid loss: 2.0325, valid accuracy: 0.4184\n",
      "Iter-43900 train loss: 2.0770 valid loss: 2.0321, valid accuracy: 0.4186\n",
      "Iter-44000 train loss: 1.9595 valid loss: 2.0316, valid accuracy: 0.4194\n",
      "Iter-44100 train loss: 2.0285 valid loss: 2.0311, valid accuracy: 0.4190\n",
      "Iter-44200 train loss: 2.0775 valid loss: 2.0307, valid accuracy: 0.4194\n",
      "Iter-44300 train loss: 1.9860 valid loss: 2.0302, valid accuracy: 0.4196\n",
      "Iter-44400 train loss: 2.0271 valid loss: 2.0297, valid accuracy: 0.4200\n",
      "Iter-44500 train loss: 2.0340 valid loss: 2.0293, valid accuracy: 0.4202\n",
      "Iter-44600 train loss: 2.1238 valid loss: 2.0288, valid accuracy: 0.4208\n",
      "Iter-44700 train loss: 2.0700 valid loss: 2.0283, valid accuracy: 0.4206\n",
      "Iter-44800 train loss: 2.0395 valid loss: 2.0279, valid accuracy: 0.4202\n",
      "Iter-44900 train loss: 2.0478 valid loss: 2.0274, valid accuracy: 0.4212\n",
      "Iter-45000 train loss: 2.0177 valid loss: 2.0270, valid accuracy: 0.4216\n",
      "Iter-45100 train loss: 1.9852 valid loss: 2.0265, valid accuracy: 0.4216\n",
      "Iter-45200 train loss: 1.9985 valid loss: 2.0260, valid accuracy: 0.4216\n",
      "Iter-45300 train loss: 2.0509 valid loss: 2.0256, valid accuracy: 0.4220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 2.0199 valid loss: 2.0251, valid accuracy: 0.4218\n",
      "Iter-45500 train loss: 2.0047 valid loss: 2.0247, valid accuracy: 0.4220\n",
      "Iter-45600 train loss: 1.9664 valid loss: 2.0242, valid accuracy: 0.4220\n",
      "Iter-45700 train loss: 2.0389 valid loss: 2.0237, valid accuracy: 0.4228\n",
      "Iter-45800 train loss: 2.0481 valid loss: 2.0233, valid accuracy: 0.4222\n",
      "Iter-45900 train loss: 1.9823 valid loss: 2.0228, valid accuracy: 0.4226\n",
      "Iter-46000 train loss: 2.0144 valid loss: 2.0224, valid accuracy: 0.4226\n",
      "Iter-46100 train loss: 2.0110 valid loss: 2.0220, valid accuracy: 0.4220\n",
      "Iter-46200 train loss: 2.0102 valid loss: 2.0215, valid accuracy: 0.4224\n",
      "Iter-46300 train loss: 2.0583 valid loss: 2.0210, valid accuracy: 0.4226\n",
      "Iter-46400 train loss: 2.0477 valid loss: 2.0206, valid accuracy: 0.4220\n",
      "Iter-46500 train loss: 2.0284 valid loss: 2.0201, valid accuracy: 0.4226\n",
      "Iter-46600 train loss: 2.0086 valid loss: 2.0197, valid accuracy: 0.4230\n",
      "Iter-46700 train loss: 2.0902 valid loss: 2.0192, valid accuracy: 0.4230\n",
      "Iter-46800 train loss: 2.1035 valid loss: 2.0188, valid accuracy: 0.4224\n",
      "Iter-46900 train loss: 1.9730 valid loss: 2.0183, valid accuracy: 0.4230\n",
      "Iter-47000 train loss: 1.9464 valid loss: 2.0179, valid accuracy: 0.4232\n",
      "Iter-47100 train loss: 2.0914 valid loss: 2.0175, valid accuracy: 0.4228\n",
      "Iter-47200 train loss: 2.0114 valid loss: 2.0170, valid accuracy: 0.4234\n",
      "Iter-47300 train loss: 1.9944 valid loss: 2.0166, valid accuracy: 0.4230\n",
      "Iter-47400 train loss: 1.9970 valid loss: 2.0161, valid accuracy: 0.4236\n",
      "Iter-47500 train loss: 2.0410 valid loss: 2.0156, valid accuracy: 0.4238\n",
      "Iter-47600 train loss: 2.0568 valid loss: 2.0152, valid accuracy: 0.4238\n",
      "Iter-47700 train loss: 2.0601 valid loss: 2.0147, valid accuracy: 0.4238\n",
      "Iter-47800 train loss: 1.9414 valid loss: 2.0142, valid accuracy: 0.4240\n",
      "Iter-47900 train loss: 2.0601 valid loss: 2.0138, valid accuracy: 0.4238\n",
      "Iter-48000 train loss: 1.9774 valid loss: 2.0133, valid accuracy: 0.4238\n",
      "Iter-48100 train loss: 2.0177 valid loss: 2.0129, valid accuracy: 0.4240\n",
      "Iter-48200 train loss: 1.9453 valid loss: 2.0125, valid accuracy: 0.4244\n",
      "Iter-48300 train loss: 2.0156 valid loss: 2.0121, valid accuracy: 0.4242\n",
      "Iter-48400 train loss: 2.0179 valid loss: 2.0116, valid accuracy: 0.4246\n",
      "Iter-48500 train loss: 2.0582 valid loss: 2.0112, valid accuracy: 0.4246\n",
      "Iter-48600 train loss: 2.0012 valid loss: 2.0108, valid accuracy: 0.4254\n",
      "Iter-48700 train loss: 2.0169 valid loss: 2.0103, valid accuracy: 0.4252\n",
      "Iter-48800 train loss: 1.9953 valid loss: 2.0099, valid accuracy: 0.4256\n",
      "Iter-48900 train loss: 2.0331 valid loss: 2.0094, valid accuracy: 0.4258\n",
      "Iter-49000 train loss: 2.0447 valid loss: 2.0090, valid accuracy: 0.4262\n",
      "Iter-49100 train loss: 1.9566 valid loss: 2.0085, valid accuracy: 0.4264\n",
      "Iter-49200 train loss: 2.0560 valid loss: 2.0081, valid accuracy: 0.4264\n",
      "Iter-49300 train loss: 2.0134 valid loss: 2.0076, valid accuracy: 0.4262\n",
      "Iter-49400 train loss: 1.9436 valid loss: 2.0072, valid accuracy: 0.4260\n",
      "Iter-49500 train loss: 2.0500 valid loss: 2.0068, valid accuracy: 0.4264\n",
      "Iter-49600 train loss: 1.9650 valid loss: 2.0063, valid accuracy: 0.4264\n",
      "Iter-49700 train loss: 2.0743 valid loss: 2.0059, valid accuracy: 0.4264\n",
      "Iter-49800 train loss: 2.0019 valid loss: 2.0054, valid accuracy: 0.4266\n",
      "Iter-49900 train loss: 2.0450 valid loss: 2.0050, valid accuracy: 0.4270\n",
      "Iter-50000 train loss: 2.0214 valid loss: 2.0045, valid accuracy: 0.4272\n",
      "Iter-50100 train loss: 2.0518 valid loss: 2.0041, valid accuracy: 0.4272\n",
      "Iter-50200 train loss: 1.9885 valid loss: 2.0037, valid accuracy: 0.4274\n",
      "Iter-50300 train loss: 2.0415 valid loss: 2.0033, valid accuracy: 0.4274\n",
      "Iter-50400 train loss: 1.9883 valid loss: 2.0028, valid accuracy: 0.4280\n",
      "Iter-50500 train loss: 2.0571 valid loss: 2.0024, valid accuracy: 0.4280\n",
      "Iter-50600 train loss: 2.0064 valid loss: 2.0019, valid accuracy: 0.4280\n",
      "Iter-50700 train loss: 2.0125 valid loss: 2.0015, valid accuracy: 0.4282\n",
      "Iter-50800 train loss: 2.0038 valid loss: 2.0011, valid accuracy: 0.4282\n",
      "Iter-50900 train loss: 1.9818 valid loss: 2.0007, valid accuracy: 0.4280\n",
      "Iter-51000 train loss: 2.0881 valid loss: 2.0002, valid accuracy: 0.4276\n",
      "Iter-51100 train loss: 1.9930 valid loss: 1.9998, valid accuracy: 0.4284\n",
      "Iter-51200 train loss: 1.9671 valid loss: 1.9994, valid accuracy: 0.4288\n",
      "Iter-51300 train loss: 2.0476 valid loss: 1.9989, valid accuracy: 0.4278\n",
      "Iter-51400 train loss: 1.9837 valid loss: 1.9985, valid accuracy: 0.4276\n",
      "Iter-51500 train loss: 1.9246 valid loss: 1.9981, valid accuracy: 0.4276\n",
      "Iter-51600 train loss: 1.9844 valid loss: 1.9976, valid accuracy: 0.4280\n",
      "Iter-51700 train loss: 2.0372 valid loss: 1.9972, valid accuracy: 0.4282\n",
      "Iter-51800 train loss: 2.0343 valid loss: 1.9968, valid accuracy: 0.4284\n",
      "Iter-51900 train loss: 2.0973 valid loss: 1.9964, valid accuracy: 0.4288\n",
      "Iter-52000 train loss: 1.9711 valid loss: 1.9959, valid accuracy: 0.4288\n",
      "Iter-52100 train loss: 1.9691 valid loss: 1.9955, valid accuracy: 0.4292\n",
      "Iter-52200 train loss: 1.9777 valid loss: 1.9951, valid accuracy: 0.4282\n",
      "Iter-52300 train loss: 2.0250 valid loss: 1.9946, valid accuracy: 0.4284\n",
      "Iter-52400 train loss: 2.0301 valid loss: 1.9942, valid accuracy: 0.4284\n",
      "Iter-52500 train loss: 1.9722 valid loss: 1.9938, valid accuracy: 0.4284\n",
      "Iter-52600 train loss: 2.0807 valid loss: 1.9934, valid accuracy: 0.4292\n",
      "Iter-52700 train loss: 1.9476 valid loss: 1.9930, valid accuracy: 0.4290\n",
      "Iter-52800 train loss: 1.9686 valid loss: 1.9925, valid accuracy: 0.4294\n",
      "Iter-52900 train loss: 2.0061 valid loss: 1.9921, valid accuracy: 0.4298\n",
      "Iter-53000 train loss: 1.9738 valid loss: 1.9917, valid accuracy: 0.4304\n",
      "Iter-53100 train loss: 2.0472 valid loss: 1.9913, valid accuracy: 0.4300\n",
      "Iter-53200 train loss: 1.9353 valid loss: 1.9909, valid accuracy: 0.4300\n",
      "Iter-53300 train loss: 1.9584 valid loss: 1.9904, valid accuracy: 0.4298\n",
      "Iter-53400 train loss: 2.0124 valid loss: 1.9900, valid accuracy: 0.4296\n",
      "Iter-53500 train loss: 1.9796 valid loss: 1.9896, valid accuracy: 0.4300\n",
      "Iter-53600 train loss: 2.0156 valid loss: 1.9892, valid accuracy: 0.4304\n",
      "Iter-53700 train loss: 2.0037 valid loss: 1.9888, valid accuracy: 0.4308\n",
      "Iter-53800 train loss: 2.0104 valid loss: 1.9884, valid accuracy: 0.4306\n",
      "Iter-53900 train loss: 1.9918 valid loss: 1.9879, valid accuracy: 0.4304\n",
      "Iter-54000 train loss: 2.0019 valid loss: 1.9875, valid accuracy: 0.4298\n",
      "Iter-54100 train loss: 2.0178 valid loss: 1.9871, valid accuracy: 0.4306\n",
      "Iter-54200 train loss: 1.8855 valid loss: 1.9867, valid accuracy: 0.4304\n",
      "Iter-54300 train loss: 2.0365 valid loss: 1.9863, valid accuracy: 0.4298\n",
      "Iter-54400 train loss: 2.0437 valid loss: 1.9859, valid accuracy: 0.4296\n",
      "Iter-54500 train loss: 2.0697 valid loss: 1.9855, valid accuracy: 0.4298\n",
      "Iter-54600 train loss: 1.9576 valid loss: 1.9851, valid accuracy: 0.4298\n",
      "Iter-54700 train loss: 1.9887 valid loss: 1.9847, valid accuracy: 0.4302\n",
      "Iter-54800 train loss: 2.0518 valid loss: 1.9843, valid accuracy: 0.4304\n",
      "Iter-54900 train loss: 1.8993 valid loss: 1.9839, valid accuracy: 0.4306\n",
      "Iter-55000 train loss: 1.9568 valid loss: 1.9834, valid accuracy: 0.4310\n",
      "Iter-55100 train loss: 2.0255 valid loss: 1.9830, valid accuracy: 0.4306\n",
      "Iter-55200 train loss: 1.9566 valid loss: 1.9826, valid accuracy: 0.4312\n",
      "Iter-55300 train loss: 1.9981 valid loss: 1.9822, valid accuracy: 0.4310\n",
      "Iter-55400 train loss: 1.9249 valid loss: 1.9818, valid accuracy: 0.4314\n",
      "Iter-55500 train loss: 1.9582 valid loss: 1.9814, valid accuracy: 0.4314\n",
      "Iter-55600 train loss: 1.9407 valid loss: 1.9810, valid accuracy: 0.4314\n",
      "Iter-55700 train loss: 1.9177 valid loss: 1.9805, valid accuracy: 0.4318\n",
      "Iter-55800 train loss: 1.9930 valid loss: 1.9801, valid accuracy: 0.4322\n",
      "Iter-55900 train loss: 1.9911 valid loss: 1.9797, valid accuracy: 0.4322\n",
      "Iter-56000 train loss: 2.1103 valid loss: 1.9793, valid accuracy: 0.4324\n",
      "Iter-56100 train loss: 2.0231 valid loss: 1.9789, valid accuracy: 0.4330\n",
      "Iter-56200 train loss: 1.9322 valid loss: 1.9784, valid accuracy: 0.4328\n",
      "Iter-56300 train loss: 2.0732 valid loss: 1.9780, valid accuracy: 0.4322\n",
      "Iter-56400 train loss: 2.0430 valid loss: 1.9776, valid accuracy: 0.4324\n",
      "Iter-56500 train loss: 1.9781 valid loss: 1.9772, valid accuracy: 0.4324\n",
      "Iter-56600 train loss: 1.9030 valid loss: 1.9768, valid accuracy: 0.4324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 1.9706 valid loss: 1.9764, valid accuracy: 0.4316\n",
      "Iter-56800 train loss: 2.0013 valid loss: 1.9760, valid accuracy: 0.4328\n",
      "Iter-56900 train loss: 1.9436 valid loss: 1.9756, valid accuracy: 0.4330\n",
      "Iter-57000 train loss: 2.0298 valid loss: 1.9752, valid accuracy: 0.4336\n",
      "Iter-57100 train loss: 2.0452 valid loss: 1.9748, valid accuracy: 0.4330\n",
      "Iter-57200 train loss: 2.0251 valid loss: 1.9744, valid accuracy: 0.4334\n",
      "Iter-57300 train loss: 1.9925 valid loss: 1.9740, valid accuracy: 0.4338\n",
      "Iter-57400 train loss: 2.0097 valid loss: 1.9736, valid accuracy: 0.4342\n",
      "Iter-57500 train loss: 2.0003 valid loss: 1.9733, valid accuracy: 0.4340\n",
      "Iter-57600 train loss: 2.0236 valid loss: 1.9729, valid accuracy: 0.4336\n",
      "Iter-57700 train loss: 1.9268 valid loss: 1.9725, valid accuracy: 0.4336\n",
      "Iter-57800 train loss: 1.9591 valid loss: 1.9721, valid accuracy: 0.4340\n",
      "Iter-57900 train loss: 1.9890 valid loss: 1.9717, valid accuracy: 0.4342\n",
      "Iter-58000 train loss: 1.9647 valid loss: 1.9713, valid accuracy: 0.4340\n",
      "Iter-58100 train loss: 1.8794 valid loss: 1.9708, valid accuracy: 0.4346\n",
      "Iter-58200 train loss: 1.9624 valid loss: 1.9704, valid accuracy: 0.4346\n",
      "Iter-58300 train loss: 2.0124 valid loss: 1.9700, valid accuracy: 0.4346\n",
      "Iter-58400 train loss: 1.9054 valid loss: 1.9696, valid accuracy: 0.4352\n",
      "Iter-58500 train loss: 1.9674 valid loss: 1.9692, valid accuracy: 0.4350\n",
      "Iter-58600 train loss: 2.0236 valid loss: 1.9688, valid accuracy: 0.4354\n",
      "Iter-58700 train loss: 2.0010 valid loss: 1.9684, valid accuracy: 0.4352\n",
      "Iter-58800 train loss: 1.9971 valid loss: 1.9680, valid accuracy: 0.4352\n",
      "Iter-58900 train loss: 2.0151 valid loss: 1.9676, valid accuracy: 0.4354\n",
      "Iter-59000 train loss: 1.9736 valid loss: 1.9672, valid accuracy: 0.4350\n",
      "Iter-59100 train loss: 1.9538 valid loss: 1.9668, valid accuracy: 0.4354\n",
      "Iter-59200 train loss: 2.0325 valid loss: 1.9664, valid accuracy: 0.4362\n",
      "Iter-59300 train loss: 1.9876 valid loss: 1.9661, valid accuracy: 0.4362\n",
      "Iter-59400 train loss: 2.0105 valid loss: 1.9656, valid accuracy: 0.4362\n",
      "Iter-59500 train loss: 1.9424 valid loss: 1.9653, valid accuracy: 0.4366\n",
      "Iter-59600 train loss: 1.9467 valid loss: 1.9649, valid accuracy: 0.4368\n",
      "Iter-59700 train loss: 1.9947 valid loss: 1.9645, valid accuracy: 0.4370\n",
      "Iter-59800 train loss: 1.9321 valid loss: 1.9640, valid accuracy: 0.4368\n",
      "Iter-59900 train loss: 1.9771 valid loss: 1.9636, valid accuracy: 0.4368\n",
      "Iter-60000 train loss: 1.9341 valid loss: 1.9633, valid accuracy: 0.4370\n",
      "Iter-60100 train loss: 1.9344 valid loss: 1.9629, valid accuracy: 0.4362\n",
      "Iter-60200 train loss: 1.8748 valid loss: 1.9625, valid accuracy: 0.4366\n",
      "Iter-60300 train loss: 2.0443 valid loss: 1.9621, valid accuracy: 0.4372\n",
      "Iter-60400 train loss: 2.0303 valid loss: 1.9617, valid accuracy: 0.4370\n",
      "Iter-60500 train loss: 1.9820 valid loss: 1.9613, valid accuracy: 0.4372\n",
      "Iter-60600 train loss: 1.9611 valid loss: 1.9609, valid accuracy: 0.4374\n",
      "Iter-60700 train loss: 1.9711 valid loss: 1.9605, valid accuracy: 0.4378\n",
      "Iter-60800 train loss: 1.9064 valid loss: 1.9602, valid accuracy: 0.4372\n",
      "Iter-60900 train loss: 1.9386 valid loss: 1.9598, valid accuracy: 0.4368\n",
      "Iter-61000 train loss: 1.9990 valid loss: 1.9594, valid accuracy: 0.4374\n",
      "Iter-61100 train loss: 1.9496 valid loss: 1.9590, valid accuracy: 0.4376\n",
      "Iter-61200 train loss: 1.9169 valid loss: 1.9586, valid accuracy: 0.4382\n",
      "Iter-61300 train loss: 1.8990 valid loss: 1.9582, valid accuracy: 0.4384\n",
      "Iter-61400 train loss: 2.0505 valid loss: 1.9578, valid accuracy: 0.4384\n",
      "Iter-61500 train loss: 1.9645 valid loss: 1.9574, valid accuracy: 0.4382\n",
      "Iter-61600 train loss: 1.9782 valid loss: 1.9571, valid accuracy: 0.4392\n",
      "Iter-61700 train loss: 1.9042 valid loss: 1.9567, valid accuracy: 0.4390\n",
      "Iter-61800 train loss: 1.9236 valid loss: 1.9563, valid accuracy: 0.4394\n",
      "Iter-61900 train loss: 1.9611 valid loss: 1.9559, valid accuracy: 0.4394\n",
      "Iter-62000 train loss: 1.9976 valid loss: 1.9555, valid accuracy: 0.4392\n",
      "Iter-62100 train loss: 1.9593 valid loss: 1.9551, valid accuracy: 0.4398\n",
      "Iter-62200 train loss: 2.0449 valid loss: 1.9547, valid accuracy: 0.4398\n",
      "Iter-62300 train loss: 2.0648 valid loss: 1.9543, valid accuracy: 0.4400\n",
      "Iter-62400 train loss: 1.9329 valid loss: 1.9540, valid accuracy: 0.4404\n",
      "Iter-62500 train loss: 1.9386 valid loss: 1.9536, valid accuracy: 0.4404\n",
      "Iter-62600 train loss: 1.9357 valid loss: 1.9532, valid accuracy: 0.4400\n",
      "Iter-62700 train loss: 1.9802 valid loss: 1.9529, valid accuracy: 0.4402\n",
      "Iter-62800 train loss: 1.9187 valid loss: 1.9525, valid accuracy: 0.4406\n",
      "Iter-62900 train loss: 1.9620 valid loss: 1.9521, valid accuracy: 0.4410\n",
      "Iter-63000 train loss: 1.9425 valid loss: 1.9517, valid accuracy: 0.4406\n",
      "Iter-63100 train loss: 1.9423 valid loss: 1.9513, valid accuracy: 0.4404\n",
      "Iter-63200 train loss: 1.9569 valid loss: 1.9510, valid accuracy: 0.4406\n",
      "Iter-63300 train loss: 1.8601 valid loss: 1.9506, valid accuracy: 0.4406\n",
      "Iter-63400 train loss: 2.0477 valid loss: 1.9502, valid accuracy: 0.4404\n",
      "Iter-63500 train loss: 2.0124 valid loss: 1.9499, valid accuracy: 0.4408\n",
      "Iter-63600 train loss: 1.9438 valid loss: 1.9495, valid accuracy: 0.4406\n",
      "Iter-63700 train loss: 1.9605 valid loss: 1.9491, valid accuracy: 0.4404\n",
      "Iter-63800 train loss: 1.8926 valid loss: 1.9487, valid accuracy: 0.4414\n",
      "Iter-63900 train loss: 1.9987 valid loss: 1.9483, valid accuracy: 0.4412\n",
      "Iter-64000 train loss: 1.9520 valid loss: 1.9479, valid accuracy: 0.4414\n",
      "Iter-64100 train loss: 2.0634 valid loss: 1.9476, valid accuracy: 0.4414\n",
      "Iter-64200 train loss: 1.9785 valid loss: 1.9472, valid accuracy: 0.4410\n",
      "Iter-64300 train loss: 1.9430 valid loss: 1.9468, valid accuracy: 0.4414\n",
      "Iter-64400 train loss: 1.8867 valid loss: 1.9464, valid accuracy: 0.4412\n",
      "Iter-64500 train loss: 1.9963 valid loss: 1.9461, valid accuracy: 0.4414\n",
      "Iter-64600 train loss: 1.9688 valid loss: 1.9457, valid accuracy: 0.4420\n",
      "Iter-64700 train loss: 2.0336 valid loss: 1.9453, valid accuracy: 0.4418\n",
      "Iter-64800 train loss: 2.0283 valid loss: 1.9449, valid accuracy: 0.4416\n",
      "Iter-64900 train loss: 2.0041 valid loss: 1.9446, valid accuracy: 0.4420\n",
      "Iter-65000 train loss: 1.9845 valid loss: 1.9442, valid accuracy: 0.4416\n",
      "Iter-65100 train loss: 2.0187 valid loss: 1.9438, valid accuracy: 0.4416\n",
      "Iter-65200 train loss: 1.9775 valid loss: 1.9434, valid accuracy: 0.4418\n",
      "Iter-65300 train loss: 1.9988 valid loss: 1.9431, valid accuracy: 0.4416\n",
      "Iter-65400 train loss: 1.9454 valid loss: 1.9427, valid accuracy: 0.4422\n",
      "Iter-65500 train loss: 1.9692 valid loss: 1.9423, valid accuracy: 0.4420\n",
      "Iter-65600 train loss: 1.9300 valid loss: 1.9419, valid accuracy: 0.4422\n",
      "Iter-65700 train loss: 1.9574 valid loss: 1.9416, valid accuracy: 0.4430\n",
      "Iter-65800 train loss: 1.9016 valid loss: 1.9412, valid accuracy: 0.4432\n",
      "Iter-65900 train loss: 2.0396 valid loss: 1.9408, valid accuracy: 0.4428\n",
      "Iter-66000 train loss: 1.9980 valid loss: 1.9405, valid accuracy: 0.4424\n",
      "Iter-66100 train loss: 1.9489 valid loss: 1.9401, valid accuracy: 0.4414\n",
      "Iter-66200 train loss: 1.9227 valid loss: 1.9398, valid accuracy: 0.4422\n",
      "Iter-66300 train loss: 1.9655 valid loss: 1.9394, valid accuracy: 0.4424\n",
      "Iter-66400 train loss: 1.9098 valid loss: 1.9390, valid accuracy: 0.4424\n",
      "Iter-66500 train loss: 1.9543 valid loss: 1.9387, valid accuracy: 0.4422\n",
      "Iter-66600 train loss: 1.9172 valid loss: 1.9383, valid accuracy: 0.4422\n",
      "Iter-66700 train loss: 2.0222 valid loss: 1.9379, valid accuracy: 0.4424\n",
      "Iter-66800 train loss: 1.9812 valid loss: 1.9376, valid accuracy: 0.4426\n",
      "Iter-66900 train loss: 1.9430 valid loss: 1.9372, valid accuracy: 0.4432\n",
      "Iter-67000 train loss: 1.9602 valid loss: 1.9369, valid accuracy: 0.4428\n",
      "Iter-67100 train loss: 1.8972 valid loss: 1.9365, valid accuracy: 0.4432\n",
      "Iter-67200 train loss: 1.9033 valid loss: 1.9361, valid accuracy: 0.4432\n",
      "Iter-67300 train loss: 1.9928 valid loss: 1.9358, valid accuracy: 0.4434\n",
      "Iter-67400 train loss: 1.9138 valid loss: 1.9354, valid accuracy: 0.4436\n",
      "Iter-67500 train loss: 1.9549 valid loss: 1.9350, valid accuracy: 0.4438\n",
      "Iter-67600 train loss: 1.9738 valid loss: 1.9347, valid accuracy: 0.4434\n",
      "Iter-67700 train loss: 1.9565 valid loss: 1.9343, valid accuracy: 0.4434\n",
      "Iter-67800 train loss: 1.8909 valid loss: 1.9339, valid accuracy: 0.4438\n",
      "Iter-67900 train loss: 1.9334 valid loss: 1.9336, valid accuracy: 0.4436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 2.0025 valid loss: 1.9332, valid accuracy: 0.4442\n",
      "Iter-68100 train loss: 1.9361 valid loss: 1.9329, valid accuracy: 0.4440\n",
      "Iter-68200 train loss: 1.9620 valid loss: 1.9325, valid accuracy: 0.4442\n",
      "Iter-68300 train loss: 1.9550 valid loss: 1.9322, valid accuracy: 0.4440\n",
      "Iter-68400 train loss: 1.9510 valid loss: 1.9318, valid accuracy: 0.4444\n",
      "Iter-68500 train loss: 1.8848 valid loss: 1.9314, valid accuracy: 0.4444\n",
      "Iter-68600 train loss: 2.0139 valid loss: 1.9311, valid accuracy: 0.4440\n",
      "Iter-68700 train loss: 1.8877 valid loss: 1.9307, valid accuracy: 0.4446\n",
      "Iter-68800 train loss: 1.9961 valid loss: 1.9304, valid accuracy: 0.4444\n",
      "Iter-68900 train loss: 1.9468 valid loss: 1.9300, valid accuracy: 0.4446\n",
      "Iter-69000 train loss: 1.9130 valid loss: 1.9297, valid accuracy: 0.4452\n",
      "Iter-69100 train loss: 1.9564 valid loss: 1.9293, valid accuracy: 0.4456\n",
      "Iter-69200 train loss: 1.9337 valid loss: 1.9290, valid accuracy: 0.4450\n",
      "Iter-69300 train loss: 1.8674 valid loss: 1.9286, valid accuracy: 0.4452\n",
      "Iter-69400 train loss: 1.9610 valid loss: 1.9282, valid accuracy: 0.4454\n",
      "Iter-69500 train loss: 1.8695 valid loss: 1.9279, valid accuracy: 0.4458\n",
      "Iter-69600 train loss: 1.9444 valid loss: 1.9275, valid accuracy: 0.4456\n",
      "Iter-69700 train loss: 1.8656 valid loss: 1.9272, valid accuracy: 0.4466\n",
      "Iter-69800 train loss: 1.8733 valid loss: 1.9268, valid accuracy: 0.4456\n",
      "Iter-69900 train loss: 1.8481 valid loss: 1.9264, valid accuracy: 0.4464\n",
      "Iter-70000 train loss: 1.9816 valid loss: 1.9261, valid accuracy: 0.4464\n",
      "Iter-70100 train loss: 2.0612 valid loss: 1.9257, valid accuracy: 0.4470\n",
      "Iter-70200 train loss: 1.9699 valid loss: 1.9253, valid accuracy: 0.4472\n",
      "Iter-70300 train loss: 1.8904 valid loss: 1.9250, valid accuracy: 0.4468\n",
      "Iter-70400 train loss: 1.9204 valid loss: 1.9247, valid accuracy: 0.4472\n",
      "Iter-70500 train loss: 1.8555 valid loss: 1.9243, valid accuracy: 0.4472\n",
      "Iter-70600 train loss: 1.9464 valid loss: 1.9240, valid accuracy: 0.4476\n",
      "Iter-70700 train loss: 1.8956 valid loss: 1.9236, valid accuracy: 0.4474\n",
      "Iter-70800 train loss: 1.9175 valid loss: 1.9233, valid accuracy: 0.4472\n",
      "Iter-70900 train loss: 1.9156 valid loss: 1.9229, valid accuracy: 0.4478\n",
      "Iter-71000 train loss: 1.8811 valid loss: 1.9226, valid accuracy: 0.4482\n",
      "Iter-71100 train loss: 1.8919 valid loss: 1.9222, valid accuracy: 0.4480\n",
      "Iter-71200 train loss: 1.9831 valid loss: 1.9219, valid accuracy: 0.4480\n",
      "Iter-71300 train loss: 1.8635 valid loss: 1.9215, valid accuracy: 0.4474\n",
      "Iter-71400 train loss: 1.9543 valid loss: 1.9212, valid accuracy: 0.4474\n",
      "Iter-71500 train loss: 1.8601 valid loss: 1.9208, valid accuracy: 0.4474\n",
      "Iter-71600 train loss: 1.9137 valid loss: 1.9205, valid accuracy: 0.4478\n",
      "Iter-71700 train loss: 1.8886 valid loss: 1.9201, valid accuracy: 0.4478\n",
      "Iter-71800 train loss: 1.9100 valid loss: 1.9198, valid accuracy: 0.4476\n",
      "Iter-71900 train loss: 1.9529 valid loss: 1.9195, valid accuracy: 0.4478\n",
      "Iter-72000 train loss: 1.9373 valid loss: 1.9191, valid accuracy: 0.4482\n",
      "Iter-72100 train loss: 1.8237 valid loss: 1.9188, valid accuracy: 0.4478\n",
      "Iter-72200 train loss: 1.9053 valid loss: 1.9184, valid accuracy: 0.4478\n",
      "Iter-72300 train loss: 1.9201 valid loss: 1.9181, valid accuracy: 0.4484\n",
      "Iter-72400 train loss: 1.9000 valid loss: 1.9178, valid accuracy: 0.4486\n",
      "Iter-72500 train loss: 2.0054 valid loss: 1.9174, valid accuracy: 0.4486\n",
      "Iter-72600 train loss: 1.9927 valid loss: 1.9171, valid accuracy: 0.4490\n",
      "Iter-72700 train loss: 1.8717 valid loss: 1.9168, valid accuracy: 0.4492\n",
      "Iter-72800 train loss: 1.9623 valid loss: 1.9164, valid accuracy: 0.4488\n",
      "Iter-72900 train loss: 1.9761 valid loss: 1.9161, valid accuracy: 0.4488\n",
      "Iter-73000 train loss: 1.8967 valid loss: 1.9157, valid accuracy: 0.4486\n",
      "Iter-73100 train loss: 1.8976 valid loss: 1.9154, valid accuracy: 0.4488\n",
      "Iter-73200 train loss: 1.8539 valid loss: 1.9150, valid accuracy: 0.4488\n",
      "Iter-73300 train loss: 1.9227 valid loss: 1.9147, valid accuracy: 0.4490\n",
      "Iter-73400 train loss: 1.9003 valid loss: 1.9144, valid accuracy: 0.4492\n",
      "Iter-73500 train loss: 1.9335 valid loss: 1.9140, valid accuracy: 0.4492\n",
      "Iter-73600 train loss: 1.8599 valid loss: 1.9137, valid accuracy: 0.4492\n",
      "Iter-73700 train loss: 1.8961 valid loss: 1.9133, valid accuracy: 0.4494\n",
      "Iter-73800 train loss: 1.9948 valid loss: 1.9130, valid accuracy: 0.4494\n",
      "Iter-73900 train loss: 1.8973 valid loss: 1.9127, valid accuracy: 0.4482\n",
      "Iter-74000 train loss: 1.8303 valid loss: 1.9123, valid accuracy: 0.4490\n",
      "Iter-74100 train loss: 1.9241 valid loss: 1.9120, valid accuracy: 0.4496\n",
      "Iter-74200 train loss: 1.9959 valid loss: 1.9116, valid accuracy: 0.4494\n",
      "Iter-74300 train loss: 1.9605 valid loss: 1.9113, valid accuracy: 0.4496\n",
      "Iter-74400 train loss: 2.0258 valid loss: 1.9110, valid accuracy: 0.4492\n",
      "Iter-74500 train loss: 1.9291 valid loss: 1.9106, valid accuracy: 0.4498\n",
      "Iter-74600 train loss: 1.9074 valid loss: 1.9103, valid accuracy: 0.4498\n",
      "Iter-74700 train loss: 1.9360 valid loss: 1.9100, valid accuracy: 0.4500\n",
      "Iter-74800 train loss: 2.0037 valid loss: 1.9096, valid accuracy: 0.4500\n",
      "Iter-74900 train loss: 1.9917 valid loss: 1.9093, valid accuracy: 0.4504\n",
      "Iter-75000 train loss: 2.0094 valid loss: 1.9090, valid accuracy: 0.4506\n",
      "Iter-75100 train loss: 1.8877 valid loss: 1.9086, valid accuracy: 0.4506\n",
      "Iter-75200 train loss: 1.8834 valid loss: 1.9083, valid accuracy: 0.4506\n",
      "Iter-75300 train loss: 1.8584 valid loss: 1.9080, valid accuracy: 0.4506\n",
      "Iter-75400 train loss: 1.9369 valid loss: 1.9076, valid accuracy: 0.4506\n",
      "Iter-75500 train loss: 2.0404 valid loss: 1.9073, valid accuracy: 0.4504\n",
      "Iter-75600 train loss: 1.9458 valid loss: 1.9070, valid accuracy: 0.4510\n",
      "Iter-75700 train loss: 1.9550 valid loss: 1.9066, valid accuracy: 0.4504\n",
      "Iter-75800 train loss: 1.8741 valid loss: 1.9063, valid accuracy: 0.4508\n",
      "Iter-75900 train loss: 1.9701 valid loss: 1.9060, valid accuracy: 0.4512\n",
      "Iter-76000 train loss: 1.8262 valid loss: 1.9057, valid accuracy: 0.4508\n",
      "Iter-76100 train loss: 1.9344 valid loss: 1.9053, valid accuracy: 0.4514\n",
      "Iter-76200 train loss: 1.9561 valid loss: 1.9050, valid accuracy: 0.4514\n",
      "Iter-76300 train loss: 1.9874 valid loss: 1.9047, valid accuracy: 0.4512\n",
      "Iter-76400 train loss: 1.9445 valid loss: 1.9044, valid accuracy: 0.4510\n",
      "Iter-76500 train loss: 1.9077 valid loss: 1.9040, valid accuracy: 0.4512\n",
      "Iter-76600 train loss: 1.9696 valid loss: 1.9037, valid accuracy: 0.4518\n",
      "Iter-76700 train loss: 1.8800 valid loss: 1.9033, valid accuracy: 0.4514\n",
      "Iter-76800 train loss: 1.9440 valid loss: 1.9030, valid accuracy: 0.4512\n",
      "Iter-76900 train loss: 1.8651 valid loss: 1.9027, valid accuracy: 0.4508\n",
      "Iter-77000 train loss: 1.9429 valid loss: 1.9024, valid accuracy: 0.4508\n",
      "Iter-77100 train loss: 1.9443 valid loss: 1.9021, valid accuracy: 0.4506\n",
      "Iter-77200 train loss: 1.8619 valid loss: 1.9017, valid accuracy: 0.4514\n",
      "Iter-77300 train loss: 1.9445 valid loss: 1.9014, valid accuracy: 0.4516\n",
      "Iter-77400 train loss: 2.0613 valid loss: 1.9011, valid accuracy: 0.4512\n",
      "Iter-77500 train loss: 1.9513 valid loss: 1.9008, valid accuracy: 0.4514\n",
      "Iter-77600 train loss: 1.9683 valid loss: 1.9004, valid accuracy: 0.4506\n",
      "Iter-77700 train loss: 1.8501 valid loss: 1.9001, valid accuracy: 0.4506\n",
      "Iter-77800 train loss: 1.9682 valid loss: 1.8998, valid accuracy: 0.4508\n",
      "Iter-77900 train loss: 1.8389 valid loss: 1.8995, valid accuracy: 0.4512\n",
      "Iter-78000 train loss: 1.9398 valid loss: 1.8991, valid accuracy: 0.4512\n",
      "Iter-78100 train loss: 1.8990 valid loss: 1.8988, valid accuracy: 0.4516\n",
      "Iter-78200 train loss: 1.8146 valid loss: 1.8984, valid accuracy: 0.4518\n",
      "Iter-78300 train loss: 1.9048 valid loss: 1.8981, valid accuracy: 0.4518\n",
      "Iter-78400 train loss: 1.8841 valid loss: 1.8978, valid accuracy: 0.4518\n",
      "Iter-78500 train loss: 1.8768 valid loss: 1.8975, valid accuracy: 0.4514\n",
      "Iter-78600 train loss: 1.9966 valid loss: 1.8971, valid accuracy: 0.4514\n",
      "Iter-78700 train loss: 1.8144 valid loss: 1.8968, valid accuracy: 0.4510\n",
      "Iter-78800 train loss: 1.9138 valid loss: 1.8964, valid accuracy: 0.4516\n",
      "Iter-78900 train loss: 1.8946 valid loss: 1.8961, valid accuracy: 0.4516\n",
      "Iter-79000 train loss: 1.8993 valid loss: 1.8958, valid accuracy: 0.4516\n",
      "Iter-79100 train loss: 1.9134 valid loss: 1.8955, valid accuracy: 0.4520\n",
      "Iter-79200 train loss: 1.8104 valid loss: 1.8951, valid accuracy: 0.4518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 1.8110 valid loss: 1.8948, valid accuracy: 0.4520\n",
      "Iter-79400 train loss: 1.8857 valid loss: 1.8945, valid accuracy: 0.4522\n",
      "Iter-79500 train loss: 1.8933 valid loss: 1.8941, valid accuracy: 0.4524\n",
      "Iter-79600 train loss: 1.8466 valid loss: 1.8938, valid accuracy: 0.4522\n",
      "Iter-79700 train loss: 1.9274 valid loss: 1.8935, valid accuracy: 0.4524\n",
      "Iter-79800 train loss: 1.9037 valid loss: 1.8932, valid accuracy: 0.4526\n",
      "Iter-79900 train loss: 1.9490 valid loss: 1.8928, valid accuracy: 0.4522\n",
      "Iter-80000 train loss: 1.8681 valid loss: 1.8925, valid accuracy: 0.4520\n",
      "Iter-80100 train loss: 1.8814 valid loss: 1.8922, valid accuracy: 0.4520\n",
      "Iter-80200 train loss: 1.9760 valid loss: 1.8919, valid accuracy: 0.4524\n",
      "Iter-80300 train loss: 1.9361 valid loss: 1.8915, valid accuracy: 0.4526\n",
      "Iter-80400 train loss: 1.8886 valid loss: 1.8912, valid accuracy: 0.4528\n",
      "Iter-80500 train loss: 1.8087 valid loss: 1.8909, valid accuracy: 0.4542\n",
      "Iter-80600 train loss: 1.8496 valid loss: 1.8905, valid accuracy: 0.4548\n",
      "Iter-80700 train loss: 1.8644 valid loss: 1.8902, valid accuracy: 0.4546\n",
      "Iter-80800 train loss: 1.9126 valid loss: 1.8899, valid accuracy: 0.4556\n",
      "Iter-80900 train loss: 1.8131 valid loss: 1.8896, valid accuracy: 0.4554\n",
      "Iter-81000 train loss: 1.9370 valid loss: 1.8893, valid accuracy: 0.4550\n",
      "Iter-81100 train loss: 1.8986 valid loss: 1.8889, valid accuracy: 0.4552\n",
      "Iter-81200 train loss: 1.8572 valid loss: 1.8886, valid accuracy: 0.4552\n",
      "Iter-81300 train loss: 1.9222 valid loss: 1.8883, valid accuracy: 0.4550\n",
      "Iter-81400 train loss: 1.9610 valid loss: 1.8880, valid accuracy: 0.4544\n",
      "Iter-81500 train loss: 1.9175 valid loss: 1.8877, valid accuracy: 0.4544\n",
      "Iter-81600 train loss: 1.9038 valid loss: 1.8874, valid accuracy: 0.4548\n",
      "Iter-81700 train loss: 1.9789 valid loss: 1.8870, valid accuracy: 0.4542\n",
      "Iter-81800 train loss: 1.8774 valid loss: 1.8867, valid accuracy: 0.4546\n",
      "Iter-81900 train loss: 1.8809 valid loss: 1.8864, valid accuracy: 0.4546\n",
      "Iter-82000 train loss: 1.7396 valid loss: 1.8861, valid accuracy: 0.4552\n",
      "Iter-82100 train loss: 1.9414 valid loss: 1.8858, valid accuracy: 0.4554\n",
      "Iter-82200 train loss: 1.8724 valid loss: 1.8855, valid accuracy: 0.4548\n",
      "Iter-82300 train loss: 1.9582 valid loss: 1.8852, valid accuracy: 0.4552\n",
      "Iter-82400 train loss: 1.8747 valid loss: 1.8849, valid accuracy: 0.4554\n",
      "Iter-82500 train loss: 1.8690 valid loss: 1.8846, valid accuracy: 0.4556\n",
      "Iter-82600 train loss: 1.8350 valid loss: 1.8843, valid accuracy: 0.4554\n",
      "Iter-82700 train loss: 1.8864 valid loss: 1.8840, valid accuracy: 0.4548\n",
      "Iter-82800 train loss: 1.9014 valid loss: 1.8837, valid accuracy: 0.4548\n",
      "Iter-82900 train loss: 1.9764 valid loss: 1.8834, valid accuracy: 0.4552\n",
      "Iter-83000 train loss: 1.8898 valid loss: 1.8831, valid accuracy: 0.4550\n",
      "Iter-83100 train loss: 1.9414 valid loss: 1.8828, valid accuracy: 0.4562\n",
      "Iter-83200 train loss: 1.9045 valid loss: 1.8825, valid accuracy: 0.4560\n",
      "Iter-83300 train loss: 1.8166 valid loss: 1.8822, valid accuracy: 0.4562\n",
      "Iter-83400 train loss: 1.7765 valid loss: 1.8819, valid accuracy: 0.4556\n",
      "Iter-83500 train loss: 1.9085 valid loss: 1.8816, valid accuracy: 0.4556\n",
      "Iter-83600 train loss: 1.7942 valid loss: 1.8812, valid accuracy: 0.4558\n",
      "Iter-83700 train loss: 1.8771 valid loss: 1.8809, valid accuracy: 0.4562\n",
      "Iter-83800 train loss: 1.7575 valid loss: 1.8806, valid accuracy: 0.4564\n",
      "Iter-83900 train loss: 1.9646 valid loss: 1.8803, valid accuracy: 0.4564\n",
      "Iter-84000 train loss: 1.8815 valid loss: 1.8800, valid accuracy: 0.4566\n",
      "Iter-84100 train loss: 1.8728 valid loss: 1.8797, valid accuracy: 0.4566\n",
      "Iter-84200 train loss: 1.9606 valid loss: 1.8794, valid accuracy: 0.4564\n",
      "Iter-84300 train loss: 1.8673 valid loss: 1.8791, valid accuracy: 0.4566\n",
      "Iter-84400 train loss: 1.8862 valid loss: 1.8788, valid accuracy: 0.4562\n",
      "Iter-84500 train loss: 1.9963 valid loss: 1.8785, valid accuracy: 0.4564\n",
      "Iter-84600 train loss: 1.8843 valid loss: 1.8782, valid accuracy: 0.4566\n",
      "Iter-84700 train loss: 1.8820 valid loss: 1.8778, valid accuracy: 0.4566\n",
      "Iter-84800 train loss: 1.8911 valid loss: 1.8775, valid accuracy: 0.4568\n",
      "Iter-84900 train loss: 1.8879 valid loss: 1.8772, valid accuracy: 0.4576\n",
      "Iter-85000 train loss: 1.9075 valid loss: 1.8769, valid accuracy: 0.4578\n",
      "Iter-85100 train loss: 1.7628 valid loss: 1.8766, valid accuracy: 0.4580\n",
      "Iter-85200 train loss: 1.9972 valid loss: 1.8763, valid accuracy: 0.4578\n",
      "Iter-85300 train loss: 1.7759 valid loss: 1.8760, valid accuracy: 0.4576\n",
      "Iter-85400 train loss: 1.9510 valid loss: 1.8757, valid accuracy: 0.4580\n",
      "Iter-85500 train loss: 1.8011 valid loss: 1.8754, valid accuracy: 0.4580\n",
      "Iter-85600 train loss: 1.9023 valid loss: 1.8750, valid accuracy: 0.4578\n",
      "Iter-85700 train loss: 1.8980 valid loss: 1.8747, valid accuracy: 0.4580\n",
      "Iter-85800 train loss: 1.9098 valid loss: 1.8744, valid accuracy: 0.4580\n",
      "Iter-85900 train loss: 1.7961 valid loss: 1.8741, valid accuracy: 0.4580\n",
      "Iter-86000 train loss: 1.9292 valid loss: 1.8738, valid accuracy: 0.4578\n",
      "Iter-86100 train loss: 1.8650 valid loss: 1.8735, valid accuracy: 0.4582\n",
      "Iter-86200 train loss: 1.9098 valid loss: 1.8732, valid accuracy: 0.4578\n",
      "Iter-86300 train loss: 1.8927 valid loss: 1.8729, valid accuracy: 0.4574\n",
      "Iter-86400 train loss: 1.9169 valid loss: 1.8726, valid accuracy: 0.4578\n",
      "Iter-86500 train loss: 1.9202 valid loss: 1.8723, valid accuracy: 0.4578\n",
      "Iter-86600 train loss: 1.8566 valid loss: 1.8720, valid accuracy: 0.4574\n",
      "Iter-86700 train loss: 1.9042 valid loss: 1.8717, valid accuracy: 0.4576\n",
      "Iter-86800 train loss: 1.9221 valid loss: 1.8714, valid accuracy: 0.4576\n",
      "Iter-86900 train loss: 1.9194 valid loss: 1.8711, valid accuracy: 0.4576\n",
      "Iter-87000 train loss: 1.7406 valid loss: 1.8708, valid accuracy: 0.4578\n",
      "Iter-87100 train loss: 1.8745 valid loss: 1.8705, valid accuracy: 0.4580\n",
      "Iter-87200 train loss: 1.9046 valid loss: 1.8702, valid accuracy: 0.4578\n",
      "Iter-87300 train loss: 1.9095 valid loss: 1.8699, valid accuracy: 0.4580\n",
      "Iter-87400 train loss: 1.9596 valid loss: 1.8696, valid accuracy: 0.4580\n",
      "Iter-87500 train loss: 1.8971 valid loss: 1.8693, valid accuracy: 0.4586\n",
      "Iter-87600 train loss: 2.0118 valid loss: 1.8690, valid accuracy: 0.4586\n",
      "Iter-87700 train loss: 1.7839 valid loss: 1.8687, valid accuracy: 0.4586\n",
      "Iter-87800 train loss: 1.8667 valid loss: 1.8684, valid accuracy: 0.4588\n",
      "Iter-87900 train loss: 1.7658 valid loss: 1.8681, valid accuracy: 0.4588\n",
      "Iter-88000 train loss: 1.8166 valid loss: 1.8678, valid accuracy: 0.4586\n",
      "Iter-88100 train loss: 1.8077 valid loss: 1.8675, valid accuracy: 0.4592\n",
      "Iter-88200 train loss: 1.8659 valid loss: 1.8672, valid accuracy: 0.4600\n",
      "Iter-88300 train loss: 1.8367 valid loss: 1.8669, valid accuracy: 0.4602\n",
      "Iter-88400 train loss: 1.8466 valid loss: 1.8666, valid accuracy: 0.4606\n",
      "Iter-88500 train loss: 1.8700 valid loss: 1.8663, valid accuracy: 0.4602\n",
      "Iter-88600 train loss: 1.8394 valid loss: 1.8660, valid accuracy: 0.4598\n",
      "Iter-88700 train loss: 1.8889 valid loss: 1.8657, valid accuracy: 0.4604\n",
      "Iter-88800 train loss: 1.8467 valid loss: 1.8654, valid accuracy: 0.4602\n",
      "Iter-88900 train loss: 1.9411 valid loss: 1.8651, valid accuracy: 0.4602\n",
      "Iter-89000 train loss: 1.9683 valid loss: 1.8648, valid accuracy: 0.4596\n",
      "Iter-89100 train loss: 1.8542 valid loss: 1.8645, valid accuracy: 0.4594\n",
      "Iter-89200 train loss: 1.8155 valid loss: 1.8642, valid accuracy: 0.4594\n",
      "Iter-89300 train loss: 1.8679 valid loss: 1.8640, valid accuracy: 0.4598\n",
      "Iter-89400 train loss: 1.9389 valid loss: 1.8637, valid accuracy: 0.4594\n",
      "Iter-89500 train loss: 1.8058 valid loss: 1.8634, valid accuracy: 0.4608\n",
      "Iter-89600 train loss: 1.7796 valid loss: 1.8631, valid accuracy: 0.4604\n",
      "Iter-89700 train loss: 1.8521 valid loss: 1.8628, valid accuracy: 0.4608\n",
      "Iter-89800 train loss: 1.8522 valid loss: 1.8625, valid accuracy: 0.4606\n",
      "Iter-89900 train loss: 1.8040 valid loss: 1.8622, valid accuracy: 0.4602\n",
      "Iter-90000 train loss: 1.8541 valid loss: 1.8619, valid accuracy: 0.4604\n",
      "Iter-90100 train loss: 1.7037 valid loss: 1.8616, valid accuracy: 0.4608\n",
      "Iter-90200 train loss: 1.8481 valid loss: 1.8613, valid accuracy: 0.4612\n",
      "Iter-90300 train loss: 1.8892 valid loss: 1.8610, valid accuracy: 0.4606\n",
      "Iter-90400 train loss: 1.8613 valid loss: 1.8607, valid accuracy: 0.4612\n",
      "Iter-90500 train loss: 1.9666 valid loss: 1.8604, valid accuracy: 0.4608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 1.9003 valid loss: 1.8601, valid accuracy: 0.4606\n",
      "Iter-90700 train loss: 2.0386 valid loss: 1.8598, valid accuracy: 0.4608\n",
      "Iter-90800 train loss: 1.9734 valid loss: 1.8595, valid accuracy: 0.4612\n",
      "Iter-90900 train loss: 1.8602 valid loss: 1.8593, valid accuracy: 0.4616\n",
      "Iter-91000 train loss: 1.9350 valid loss: 1.8590, valid accuracy: 0.4604\n",
      "Iter-91100 train loss: 1.6784 valid loss: 1.8587, valid accuracy: 0.4610\n",
      "Iter-91200 train loss: 1.8622 valid loss: 1.8584, valid accuracy: 0.4606\n",
      "Iter-91300 train loss: 1.8269 valid loss: 1.8581, valid accuracy: 0.4608\n",
      "Iter-91400 train loss: 1.8780 valid loss: 1.8578, valid accuracy: 0.4612\n",
      "Iter-91500 train loss: 1.8652 valid loss: 1.8575, valid accuracy: 0.4622\n",
      "Iter-91600 train loss: 1.9312 valid loss: 1.8572, valid accuracy: 0.4626\n",
      "Iter-91700 train loss: 1.8286 valid loss: 1.8569, valid accuracy: 0.4622\n",
      "Iter-91800 train loss: 1.8511 valid loss: 1.8566, valid accuracy: 0.4626\n",
      "Iter-91900 train loss: 1.8932 valid loss: 1.8564, valid accuracy: 0.4624\n",
      "Iter-92000 train loss: 1.9325 valid loss: 1.8561, valid accuracy: 0.4628\n",
      "Iter-92100 train loss: 1.8566 valid loss: 1.8558, valid accuracy: 0.4622\n",
      "Iter-92200 train loss: 1.9263 valid loss: 1.8555, valid accuracy: 0.4626\n",
      "Iter-92300 train loss: 1.8649 valid loss: 1.8552, valid accuracy: 0.4628\n",
      "Iter-92400 train loss: 1.9274 valid loss: 1.8549, valid accuracy: 0.4626\n",
      "Iter-92500 train loss: 1.8278 valid loss: 1.8546, valid accuracy: 0.4626\n",
      "Iter-92600 train loss: 1.8633 valid loss: 1.8543, valid accuracy: 0.4630\n",
      "Iter-92700 train loss: 1.9406 valid loss: 1.8540, valid accuracy: 0.4624\n",
      "Iter-92800 train loss: 1.8711 valid loss: 1.8537, valid accuracy: 0.4626\n",
      "Iter-92900 train loss: 1.8486 valid loss: 1.8534, valid accuracy: 0.4626\n",
      "Iter-93000 train loss: 1.8879 valid loss: 1.8531, valid accuracy: 0.4628\n",
      "Iter-93100 train loss: 1.8696 valid loss: 1.8529, valid accuracy: 0.4626\n",
      "Iter-93200 train loss: 1.8216 valid loss: 1.8525, valid accuracy: 0.4630\n",
      "Iter-93300 train loss: 1.8613 valid loss: 1.8522, valid accuracy: 0.4634\n",
      "Iter-93400 train loss: 1.8342 valid loss: 1.8520, valid accuracy: 0.4630\n",
      "Iter-93500 train loss: 1.7942 valid loss: 1.8517, valid accuracy: 0.4628\n",
      "Iter-93600 train loss: 1.9405 valid loss: 1.8514, valid accuracy: 0.4630\n",
      "Iter-93700 train loss: 1.7950 valid loss: 1.8511, valid accuracy: 0.4634\n",
      "Iter-93800 train loss: 1.9672 valid loss: 1.8509, valid accuracy: 0.4634\n",
      "Iter-93900 train loss: 1.9017 valid loss: 1.8506, valid accuracy: 0.4638\n",
      "Iter-94000 train loss: 1.7723 valid loss: 1.8503, valid accuracy: 0.4638\n",
      "Iter-94100 train loss: 1.8329 valid loss: 1.8500, valid accuracy: 0.4646\n",
      "Iter-94200 train loss: 1.8328 valid loss: 1.8497, valid accuracy: 0.4642\n",
      "Iter-94300 train loss: 1.8236 valid loss: 1.8495, valid accuracy: 0.4644\n",
      "Iter-94400 train loss: 1.9812 valid loss: 1.8492, valid accuracy: 0.4642\n",
      "Iter-94500 train loss: 1.9411 valid loss: 1.8489, valid accuracy: 0.4646\n",
      "Iter-94600 train loss: 1.8298 valid loss: 1.8486, valid accuracy: 0.4650\n",
      "Iter-94700 train loss: 1.9325 valid loss: 1.8484, valid accuracy: 0.4644\n",
      "Iter-94800 train loss: 1.9133 valid loss: 1.8481, valid accuracy: 0.4650\n",
      "Iter-94900 train loss: 1.8478 valid loss: 1.8478, valid accuracy: 0.4650\n",
      "Iter-95000 train loss: 1.8650 valid loss: 1.8475, valid accuracy: 0.4652\n",
      "Iter-95100 train loss: 1.7980 valid loss: 1.8473, valid accuracy: 0.4648\n",
      "Iter-95200 train loss: 1.7952 valid loss: 1.8470, valid accuracy: 0.4648\n",
      "Iter-95300 train loss: 1.7648 valid loss: 1.8467, valid accuracy: 0.4650\n",
      "Iter-95400 train loss: 1.9649 valid loss: 1.8464, valid accuracy: 0.4652\n",
      "Iter-95500 train loss: 1.7897 valid loss: 1.8461, valid accuracy: 0.4648\n",
      "Iter-95600 train loss: 1.8635 valid loss: 1.8459, valid accuracy: 0.4650\n",
      "Iter-95700 train loss: 1.9051 valid loss: 1.8456, valid accuracy: 0.4648\n",
      "Iter-95800 train loss: 1.8387 valid loss: 1.8453, valid accuracy: 0.4648\n",
      "Iter-95900 train loss: 1.8702 valid loss: 1.8450, valid accuracy: 0.4656\n",
      "Iter-96000 train loss: 1.9749 valid loss: 1.8447, valid accuracy: 0.4652\n",
      "Iter-96100 train loss: 1.7895 valid loss: 1.8445, valid accuracy: 0.4646\n",
      "Iter-96200 train loss: 1.7240 valid loss: 1.8442, valid accuracy: 0.4644\n",
      "Iter-96300 train loss: 1.7664 valid loss: 1.8439, valid accuracy: 0.4650\n",
      "Iter-96400 train loss: 1.8699 valid loss: 1.8437, valid accuracy: 0.4652\n",
      "Iter-96500 train loss: 1.9418 valid loss: 1.8434, valid accuracy: 0.4644\n",
      "Iter-96600 train loss: 1.8293 valid loss: 1.8431, valid accuracy: 0.4646\n",
      "Iter-96700 train loss: 1.8052 valid loss: 1.8428, valid accuracy: 0.4652\n",
      "Iter-96800 train loss: 1.8520 valid loss: 1.8426, valid accuracy: 0.4654\n",
      "Iter-96900 train loss: 1.8266 valid loss: 1.8423, valid accuracy: 0.4658\n",
      "Iter-97000 train loss: 1.9369 valid loss: 1.8420, valid accuracy: 0.4652\n",
      "Iter-97100 train loss: 1.9020 valid loss: 1.8417, valid accuracy: 0.4656\n",
      "Iter-97200 train loss: 1.8790 valid loss: 1.8414, valid accuracy: 0.4654\n",
      "Iter-97300 train loss: 1.8837 valid loss: 1.8412, valid accuracy: 0.4660\n",
      "Iter-97400 train loss: 1.8567 valid loss: 1.8409, valid accuracy: 0.4658\n",
      "Iter-97500 train loss: 1.8698 valid loss: 1.8406, valid accuracy: 0.4668\n",
      "Iter-97600 train loss: 1.9053 valid loss: 1.8403, valid accuracy: 0.4666\n",
      "Iter-97700 train loss: 1.8880 valid loss: 1.8400, valid accuracy: 0.4664\n",
      "Iter-97800 train loss: 1.9178 valid loss: 1.8398, valid accuracy: 0.4664\n",
      "Iter-97900 train loss: 1.9058 valid loss: 1.8395, valid accuracy: 0.4664\n",
      "Iter-98000 train loss: 1.8480 valid loss: 1.8392, valid accuracy: 0.4670\n",
      "Iter-98100 train loss: 1.7971 valid loss: 1.8390, valid accuracy: 0.4670\n",
      "Iter-98200 train loss: 1.7100 valid loss: 1.8387, valid accuracy: 0.4662\n",
      "Iter-98300 train loss: 1.9462 valid loss: 1.8384, valid accuracy: 0.4674\n",
      "Iter-98400 train loss: 1.8938 valid loss: 1.8382, valid accuracy: 0.4672\n",
      "Iter-98500 train loss: 1.8699 valid loss: 1.8379, valid accuracy: 0.4670\n",
      "Iter-98600 train loss: 1.8895 valid loss: 1.8376, valid accuracy: 0.4670\n",
      "Iter-98700 train loss: 1.9489 valid loss: 1.8373, valid accuracy: 0.4668\n",
      "Iter-98800 train loss: 1.8626 valid loss: 1.8371, valid accuracy: 0.4662\n",
      "Iter-98900 train loss: 1.8993 valid loss: 1.8368, valid accuracy: 0.4664\n",
      "Iter-99000 train loss: 1.7885 valid loss: 1.8366, valid accuracy: 0.4668\n",
      "Iter-99100 train loss: 1.8548 valid loss: 1.8363, valid accuracy: 0.4668\n",
      "Iter-99200 train loss: 1.8480 valid loss: 1.8360, valid accuracy: 0.4668\n",
      "Iter-99300 train loss: 1.8761 valid loss: 1.8357, valid accuracy: 0.4666\n",
      "Iter-99400 train loss: 1.9428 valid loss: 1.8354, valid accuracy: 0.4670\n",
      "Iter-99500 train loss: 1.8979 valid loss: 1.8352, valid accuracy: 0.4674\n",
      "Iter-99600 train loss: 1.7992 valid loss: 1.8349, valid accuracy: 0.4676\n",
      "Iter-99700 train loss: 1.8573 valid loss: 1.8346, valid accuracy: 0.4672\n",
      "Iter-99800 train loss: 1.9531 valid loss: 1.8344, valid accuracy: 0.4676\n",
      "Iter-99900 train loss: 1.8337 valid loss: 1.8341, valid accuracy: 0.4682\n",
      "Iter-100000 train loss: 1.8677 valid loss: 1.8338, valid accuracy: 0.4674\n",
      "Last iteration - Test accuracy mean: 0.4658, std: 0.0000, loss: 1.8419\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XWYlNXbwPHv2aJ3BUS6WxoJFcSlkVBUShAQAzAQQV/A\nBPyhYisqJiJSolgoISIhIYi0hCDN0p2ysJz3jzPDxE7uTu3O/bmuueaZJ88My9xznnPOfZTWGiGE\nECIm3AUQQggRGSQgCCGEACQgCCGEsJCAIIQQApCAIIQQwkICghBCCMCHgKCUKqGUmq+U2qiU2qCU\netzDvvWVUpeUUncFtphCCCGCLc6HfS4Dg7XWa5VSeYFVSqm5Wust9jsppWKA0cAvQSinEEKIIPNa\nQ9BaH9Rar7UsnwU2A8Vd7DoAmA4cDmgJhRBChIRfbQhKqTJAbWCF0/piQEet9YeAClThhBBChI7P\nAcFyu2g6MNBSU7D3DjDUfvcAlE0IIUQIKV9yGSml4oCfgdla63ddbN9hXQSuBc4BfbXWM5z2k8RJ\nQgiRAVrroP/Q9rWG8DmwyVUwANBal7M8ymJqEY84BwO7feWhNcOHDw97GSLlIZ+FfBbyWXh+hIrX\nXkZKqUZAD2CDUmoNoIFngNKA1lp/4nSI1AKEECIL8hoQtNZLgVhfT6i1vj9TJRJCCBEWMlI5TJKT\nk8NdhIghn4WNfBY28lmEnk+NygG7mFI6lNcTQojsQCmFDkGjsi8jlYUQ2VCZMmXYvXt3uIsh7JQu\nXZpdu3aF7fpSQxAiSll+dYa7GMKOu3+TUNUQpA1BCCEEIAFBCCGEhQQEIYQQgAQEIUQ2d+XKFfLl\ny8e+ffv8Pnb79u3ExETP12TI32nXrqG+ohAiK8mXLx+JiYkkJiYSGxtL7ty5r66bOnWq3+eLiYnh\nzJkzlChRIkPlUSp6cnWGvJcRaLQGpeD4ccifP2SXF0LYyQq9jMqVK8e4ceNo2rSp233S0tKIjfU5\nmYJftm/fTqVKlUhLSwvK+Z1FdS+js85JtIUQwo6r5G7PP/883bp1o3v37iQlJTF58mSWL1/OTTfd\nRP78+SlevDgDBw68+iWelpZGTEwMe/bsAaBnz54MHDiQtm3bkpiYSKNGjXwej5GSkkKHDh0oWLAg\nlStXZvz48Ve3rVixghtuuIGkpCSKFi3K0KFmRoALFy7Qo0cPrr32WvLnz8+NN97I8ePHA/HxBFxY\nAsK8eeG4qhAiu/jhhx+49957OXXqFF27diU+Pp4xY8Zw/Phxli5dyi+//MLHH398dX/n2z5Tp07l\npZde4sSJE5QsWZLnn3/ep+t27dqV8uXLc/DgQb766iuGDBnC4sWLARgwYABDhgzh1KlT/Pvvv3Tq\n1AmA8ePHc+HCBfbv38/x48cZO3YsOXPmDNAnEVhhCQgtW5rnUqWgXbtwlEAI4Y1SgXkEQ+PGjWnb\nti0AOXLk4IYbbqB+/foopShTpgwPPfQQixYturq/cy2jU6dO1KlTh9jYWHr06MHatWu9XnPnzp2s\nXLmS0aNHEx8fT506dejTpw8TJ04EICEhgW3btnH8+HHy5MlD/fr1AYiPj+fo0aNs3boVpRR169Yl\nd+7cgfooAirszeezZkG+fPDVV+EuiRDCntaBeQRDyZIlHV7/888/tG/fnqJFi5KUlMTw4cM5evSo\n2+OLFClydTl37tyc9eH+9YEDB7j22msdft2XLl2alJQUwNQENm7cSOXKlbnxxhuZPXs2APfddx8t\nWrSgS5culCxZkmeeeYYrV6749X5DJewBAUxbwlNPhbsUQoiswvkWUL9+/ahRowY7duzg1KlTjBw5\nMuAN5sWKFePo0aNcuHDh6ro9e/ZQvHhxACpWrMjUqVM5cuQIgwcP5u677yY1NZX4+HheeOEFNm3a\nxJIlS/juu++YPHlyQMsWKBEREABSUiA1NdylEEJkRWfOnCEpKYlcuXKxefNmh/aDzLIGljJlylCv\nXj2eeeYZUlNTWbt2LePHj6dnz54ATJo0iWPHjgGQmJhITEwMMTExLFiwgI0bN6K1Jm/evMTHx0fs\n2IaIKlWOHHDgQLhLIYSIFL6OAXjzzTf54osvSExM5OGHH6Zbt25uz+PvuAL7/adNm8bWrVspUqQI\nXbp0YfTo0dxyyy0AzJo1i6pVq5KUlMSQIUP4+uuviYuLY//+/dx1110kJSVRo0YNWrVqRffu3f0q\nQ6iEZRyCN/PmQfPmISiQEFEsK4xDiDbhHocQkQEBYPp0aNUK0tIgJgYSE4NcOCGijASEyBPugOD1\nlpFSqoRSar5SaqNSaoNS6nEX+9yulFqnlFqjlPpTKdXI7QljfWso6NTJBIT8+SEpCcaOhTDOGyGE\nENme1xqCUqoIUERrvVYplRdYBdyhtd5it09urfV5y3IN4GutdVUX59L0vQG++RpOlMtQgeUHjRCB\nITWEyBPxNQSt9UGt9VrL8llgM1DcaZ/zdi/zAu472a7rCQ/eCNW+zlCBt20zg12k8VkIIQLLr15G\nSqkyQG1ghYttHZVSm4GfgPvdnmTFQJg0G5o/Ax0egvhzfhXYMtaDP/4wz+vX+3W4EEIIN3xuVLbc\nLloI/E9r/aOH/RoDw7XWLV1sszUqJ5yBto9BiRUwfSocrJOB4hvW7KkHD0Lhwhk+jRBRRW4ZRZ5w\n3zKK82UnpVQcMB2Y6CkYAGitlyilyimlCmitXaT0G2GeUoEf+kCN1tCzNSwZBsufAO3/0AhLKhGK\nFIFff4UWLRy3nzhh0mPExJiHEEJEsoULF7Jw4cKQX9enGoJS6kvgqNZ6sJvt5bXW2y3LdYEftdYl\nXeznutvpNTvh7h5wMR/88AWcLerfu7AzZQrcc49ZHjUK/v0XJkwwr1u2hLlzM3xqIbIVqSFEnnDX\nEHzpdtoI6AE0s3QrXa2UaqOU6qeU6mvZ7W6l1N9KqdXAe0AXv0pxsiyM/x323Qj960CV7/19H1d1\n727yIi1fDiNH2oIBmNqDECJ72717NzExMVcTyLVt2/ZqRlJv+zorW7Ys8+fPD1pZI03kDUwr8Qfc\n1RN2N4E578DFwI5Is77dwYOhfn1bbUKIaBOpNYTbbruNhg0bMmLECIf1P/74I/379yclJcVjLqDd\nu3dTrlw5Ll265DVnkLd9y5Yty7hx42jWrFmG3ou/Ir6GEHL7boKP1sKVWOhfG0otCejpp0+Hy5fh\n7bfhrbcCemohRAD07t2bSZMmpVs/adIkevbsGbGJ4bKDyPxkU/PCT5+aGkLnztBiGMReDMipO3eG\nNm1sr//7zwQIIURk6NixI8eOHWPJEtuPwZMnT/Lzzz/Tq1cvwCSSq1u3LklJSZQuXZqRI0e6PV/T\npk35/PPPAbhy5QpPPfUUhQoVokKFCsycOdPncqWmpvLEE09QvHhxSpQowaBBg7h06RIAx44do0OH\nDuTPn5+CBQty6623Xj3u1VdfpUSJEiQmJlK1alUWLFjg1+cRSpEZEKz+ud3UFq7dAn3rQ5E1ATnt\nb7/ZlitUgB49zHJqKmzeDHfcAa7SlW/caBqphRDBkzNnTjp37syXX355dd20adOoWrUq1atXByBv\n3rxMnDiRU6dOMXPmTD766CNmzJjh9dyffPIJs2bNYt26dfz1119Mnz7d53KNGjWKP//8k/Xr17Nu\n3Tr+/PNPRo0aBZhsqyVLluTYsWMcPnyYl19+GYCtW7fywQcfsGrVKk6fPs0vv/xCmTJl/Pg0Qsun\nbqdhda4wfPU91Jpouqeuegh+fx4uZ35O0jVrTPK8r7+G556Dd94Byw8JZsyArl0hzu4Tql4dChQA\nS8pzIbI1NTIwt6z1cP/bKXr37k379u15//33SUhIYOLEifTu3fvq9iZNmlxdrl69Ot26dWPRokXc\nfvvtHs/7zTff8MQTT1CsWDEAnn76aYepNj2ZMmUKH3zwAQULFgRg+PDh9O/fn5EjRxIfH8+BAwfY\nuXMn5cuXp1Ejk84tNjaW1NRU/v77bwoWLEipUqX8+hxCTmsdsgegU1O1TkvTesiQDEzGl3e/puud\nmkeraEosC9AEf+YxcWL6dcePaweg9TXXaH3pktapqVqILM38949cFStW1NOmTdPbt2/XCQkJ+vDh\nw1e3rVixQjdt2lQXKlRIJyUl6Vy5culevXpprbXetWuXjomJ0WlpaVprrZOTk/W4ceO01lpXqVJF\nz5o16+p5/vnnH4d9nZUpU0b/9ttvWmutc+XKpTdt2nR125YtW3SOHDm01lqfOXNGP/nkk7pcuXK6\nfPnyevTo0Vf3mzp1qm7cuLEuUKCAvueee/T+/fvdvmd3/yaW9UH/jg75LaP4eDM4zFKj8s/ZojDt\nW1jwInS9C9o84XfqC3cskx659MILttnclIIOHaCO08Bqrc3tJiFEYPTs2ZMJEyYwadIkWrduTaFC\nha5u6969Ox07diQlJYWTJ0/Sr18/n3pMFS1alL179159vXv3bp/LU6xYMYf9d+/efbWmkTdvXt54\n4w22b9/OjBkzeOutt662FXTr1o3FixdfPXbYsGE+XzPUwtaGEBtrbtc0cp8o2w0FmzrD2L8h1zF4\npAaU/c37YRlQoADMnAn/+x/s3Gm5ujJjHDZutO23ZQvkzQvXXx+UYggRlXr16sW8efP47LPPHG4X\nAZw9e5b8+fMTHx/Pn3/+yZQpUxy2uwsOXbp0YcyYMaSkpHDixAleffVVn8tzzz33MGrUKI4ePcrR\no0f53//+d3X6zJkzZ7J9+3YA8uXLR1xcHDExMWzdupUFCxaQmppKQkICuXLliuheUmEtWUwMPP44\nDBiQgYMvFITvJ8Ks96FjH5MoL8epgJfR2gBdpYp5Vip9Cu558+D8eYQQAVS6dGluvvlmzp8/n65t\nYOzYsTz//PMkJSUxatQounbt6rDd3ZSZDz30EK1bt6ZWrVrUq1ePu+++22MZ7I997rnnqFevHjVr\n1rx6/LPPPgvAtm3baNGiBfny5aNRo0Y8+uij3HrrrVy8eJFhw4ZRqFAhihUrxpEjR3jllVcy/JkE\nW8gHprm7np/TnDrKcRpaDIVKP8PMsbC1QyZO5p01KFy+bGo6779vC2oROM5HCJcidWBaNJOBaRbr\n1sHp02Z5+XIYNMiPgy8mwswPTY2hzSC4uzvkPhKUcoLtSz8uE320PvoINmwITHmEECIQIiYg1Kxp\nMpJa1a9vnu3XebUrGT5cD2eKmbaF6l/h6xzOGeXqFpJV69bQvj3YtWFd9fDDJvleZi1bZqYYFUKI\nzIqYgOCseXPo3x+OHvXzwEu5Ye4bMPVHaDIKunWEfPuDUkarr76yLV+8CEuXwmefmcyqM2dCqVJm\nRLTVDz/YlmvVMrPAZbTmvmKFrWYlhBCZEbEB4brr4MMPISEB8uTJwAlSGsLHq+BgbZMTqc44glVb\nWLbMtlykCDRuDA895LhPrlzmthjAnXeaZ63NjG+VKpkcSxmRqbYXIYSwE7EBwd7IkeDU68w3aTlg\n4Uj4ch7U/xB6N4NrgztY4ORJ99tq14ZVq2yvz561LZ/KYAcpCQhCiECJuIDQpQtUreq47skn4Ysv\nYMwY89oyFsR3h2rCZ8thy53QpwkkD4e4/7wfFwTW8Qxgmx8azPSfYL7gfR3gNnCgZGwVQgROxAWE\nadMg0c0UCNZbR9ZbL365EgcrHoeP10ChTfBIdSj/S4bLmVH2t5fsPf+8LevqfjdNHqdPQ0qK7fWY\nMbBnT2DLJ6JH6dKlUUrJI4IepUuXDuvfRMSMQ/DF55/DAw+Ye++ZvlVScRbcNgAO1IW5b8Kp8Ced\nmjjRlkLjt99g6lQYNgzKl4fFi8Gaz8v6Edp/BuXLO2ZiXbXKpAmpWTM0ZRdCBE/UjUPwRdOm0K6d\nWbamoMvwjGfb2pr0F0eqQb860OR/EHchYGXNiH/+sS1//bXpqWTN6Pvaa7Zt1rxK9iyj5q+qVw9u\nusksu+r2mlk7d8KBA4E/rxAifLJUDcGTO+6wfXn67Zpd0OopKLoKfnkLtnQEIqO1tmFD+PJL047y\n889m3eDB8Oab6WtJV67Y1illejYdPGjGKQT6Y1cKKlaErVsDe14hRHoRU0NQSpVQSs1XSm1USm1Q\nSj3uYp/uSql1lscSpVSN4BTXPeuESdbJbvxysgx8Pd3M0tbsOTPvQpB7I/lqxQozVmHLFtu6Zctc\nf8FPm5Z+XTBng5PxD0JkL77cMroMDNZaVwNuAh5VSlVx2mcH0ERrXQsYBXwa2GJ6V7u2eS5a1HZb\nyW87WpgZ2ra2M72RWg8OSsI8f/33n2P7wPLl8Prr6fd7/33H1/ZBo0ED27l27PB8rQsXTCbaK1c8\nl0vS4AiRvXgNCFrrg1rrtZbls8BmoLjTPsu11tZvzuXO20NJKXNrxX70sF+uxMOKgTB2o0ma91gV\nqD0elJdvxxAbOjT9uqVL4fffXe+/cqV5HjnSNEC706QJ1K0L1apBt26eyyABQYjsxa/0bEqpMkBt\nYIWH3R4EZnvYHjRjx9pqB7fdlsmTnbsOZnwGxVZC2wFQ7yOY/R6kNMh0OYPp1lttX9TOX9i7dsGm\nTZ6PX7fO1mh95IjnXE0SEITIXnwOCEqpvMB0YKClpuBqn6ZAH6Cxu/OMGDHi6nJycjLJycm+FsGr\nhx+2Lbsby+C3/fVh3DKoOdHkRfq3Dcx7xcz1HKGsvZWcv7DLlnW9/86dZrBf+/aOPZjOnLEt//WX\nGQQ3ZQpcuhTY8gohHC1cuJCFCxeG/Lo+9TJSSsUBPwOztdbvutmnJvAt0EZrvd3NPkHrZeTK8eNw\nzTVmzoKAyHHadE+t/QUsfgb+fMzcYopQCQmma6hlTnAHr75qbg21aGFqAc89lz77any8+fLX2tyi\neu01s2wdE3HttaYWIYQIrojpZWTxObDJQzAohQkGPd0Fg3AoUMDMymbfQydTLibCr6/D+MVQYQ48\nXAvK/Rqgkweep9g7dCi0bWt7vXx5xs9//rxpiBZCZG2+dDttBPQAmiml1iilViul2iil+iml+lp2\nex4oAIy17PNnEMvst8qVHV/nz5/JEx6tApPmwLzR0L4/dL0TCkZeh/xLl1zXDuy3W3MhzZuXfrt9\nQLGObzh3zrZ87Jh5rlPH1Bjmzg1A243F4MGwenVgziWE8I3XNgSt9VLA400XrfVDwEOe9okESpme\nOJUre/6i9PFs8M/tsL0VNHwXHrgZ1t8Li16ACwUCUdyQePJJ99vsA4K1C2qbNlCunON+W7dCzpxm\nAN2cOfDii2agoFLw008mYWHFiv6V6+23TXtG3bre971yxXSXzZ3bv2sIIRxlqdQVmTFqFLz0kknn\nUCCQ39eXc8LSofDBJohNhccqw01vhi2baiClpdmWd+82z0uWmC9+K2siPq1tyyNHmnEhdeqYtoma\nNW3dXv3ha3PT2LEZnDNDCOEgagLCs8/C008H8QLnroOZY2H871BmkRm/UHNSxI1fyAil3LcRFLeM\nOLl4ERYsMMvW2oT1+b//PNdEvv7aNqL6/HnbLSlfA4JzHichRMZETUAImaNVYeoM+G4iNHgf+t4A\n5eeGu1SZ9tNPmTt+8WLb8vnz5tbSnDnmddeuJkUHOAYea0C5eDFz1xZC+CbqA8IDDwTpxHtugc/+\ngMXPQtvHoGdLKLImSBfLWvLkMY3P3hqgN2yAceNM+4RzDWXpUtuyDJATIjCiNiBMnWpucXz6qW3S\nmrZtbXMOBIaCTZ3gg41mtrYebeHueyC/3OPwxbJl8OCDZtk5SV/jxulrDtZ9/ZGSYju3/Wx2QkSj\nqA0I3bpBcrK5X33TTXDokMkW+tZb6ZPEZdqVeFj5CLy3DY5cDw81hLaPQt6DAb5Q1mX9le9qrgdf\njRvn/zElSpiZ586fT997SohoE7UBwdl110HevHDDDfDoo+aLIuBS88Lvz8P7WyAtBzxSzaTbjoCM\nqqFw+HD6ddZAcNttULgwtGzp+tjERNuEPOvX24795RfTaO0P56Bz7Jhjjyp30tJM4BAiu5KA4Ean\nTr71gc+Q89eaiXg+XgP59sPjFaHRaxCfvb9tqldPf78/xvIXePasCRgbN7o/3jp/tLULLJhxERMm\n2F6fOOG5DOfPQ44c3sv6zjvmR4K9UaOke6vI3iQguPH222Ze4qA6VQp+/BzGL4Lif8KAilD/A4jN\nnt1qjhyBmTMDd76zlhSL9kGmQAE4edIsHz5sMrzas0/MZ5/e+/hx82xt6F68OH2eJpkdTmR3EhAi\nwdGqZsa2qTOg4iwYUAnqjIOYIE53FiYdOmT82M8/d3x9xx3m2blx+a+/zHPhwibD69atJkjs22fb\nZ9Ei2wxzL78MZcqY5blZv4ewEBkmAcFHCQkwfz688IJ5vWFDEC5y4AaYMhO+nQo1J5vBbbW+zJaB\nISM++cT0CBowwLy29g5z5twOUbmymYa0ZEkzxzTAZi8zpDrPVy2C7+JFyZ4bbhIQfFCqlPljbdrU\npGW4dMncD+/cOUgX3HszTJhvJuip+5lpfK4xGZQPLZ/Z3LFjjm0IvrK2P1SxTP7qbeyCBITQGzw4\nfbuNCC0JCF5Mnpy+O2OcJSVgnz5BvviuZNO+MHOsmbHtUQkMvvL2hX76dHCue/y4mX40q0pN9T6X\n9pkzwfklb39LT4SHTxPkBOxiIZ4gJ9iOHTOTxISGhrLzoelwyH3UZFX9uyvoQM3+kzUcPAhFigTv\n/DExjl+IrlKAe/oTXrkSGjTIuqOnlTK1YOutUVeaNoWFCwPzHo8fN1lqc+Y0bUIzZmTdzy6YIm2C\nHOFCvNNkac8+G8yrKdjZHD5fDDM/gPpjo7LGEPRamRPrNKL2NQpv81JnFe5+kXvq+uvpOGf33msC\nzFmXE+4aBQvCfff5dj4RfBIQMsF+3uYHHzTtCsFnFxhmvQ/1P4RHr7c0Pmf/yY5nzw7t9RITzUC4\nc+ds66ZPty0rZR7W5HxWJUsG5vpKBba767Fj8P33pidWoMrojrWLsbcAkpE2IREcEhAyyZrmolu3\nIDYyu6RgRwtLjWEs1B4PAyrDDZ9k23EM4eJqsJtSpteZlf0ymC/BIUNgyhTH9efPe79H7+zQIf/2\n9+Sdd+Cuu2w1HyHsSUDIpEcftS3HxqbvKx98lhrDhAXw/ZdQ5XsYWB5ufCfbj3wOlfHjHW8ZDR9u\nnr0NXHz9dfOwlycPvPqqf9dfuzZ9wMlOAtVm8M8/GZsbXNh4nUJTeHfvvbY0F927m3vM998P119v\n1l26lL69ISj2NIbJs6HoKmjyEjQeDcufMIn1LiZ6P164/PU+YYJt/IK9IUNsy3v2mFxHvvzy/vdf\nz9vT0sx4C2uKjccfN8+uvjiPHw/wDIC4zjk1daopU8+etnUvvGA+r1GjAnNdd5Mw+aptW9ixQxql\nM0Vr7fEBlADmAxuBDcDjLvapDCwD/gMGeziXjiZ799qWzZ9piB+F/tbc1V0zpKCm2bOa3EfCU45s\n8GjSxPs+H3yg9S23OK6rXdvxbwK0vv9+z383992ndVJS+r8bV0DrS5d8/5t87jlzzPz5rs/p7lpx\ncbb1FSqYZaXcl8v+XJs3e96nYUPP1/ZVuXLpj79wIXPnjBSW706C/fDlltFly5d8NeAm4FGlVBWn\nfY4BA4DXnQ+OZkHJmOqPI9Xgu8nw6QrIfcSkxGg9GBKlw7e/fJmm8/hxx5nh7GkN33xjlhctMjUJ\nd+dcuxZO+ZEAV2vH15eyf98Cl1yNPQnXbHsFCnjuuhupvAYErfVBrfVay/JZYDNQ3Gmfo1rrVZjg\nIVzITJ7/TDtRHn7+GMZuAK3g4Zpwx/1QyEv/QnFVSor3fZ5/Pv26tWvNF1WfPtCli1m3fTvUqAEV\nKjjue+6c5wF1rVu73u4cEBISzK0TZ598YsZJ+Or0afj1V9/399WlS473+p3LnxFap59EKZxOnEjf\n8ywr8KtRWSlVBqgNZMG3Gl4haUPw5kxxmPsmjNkGJ8pB7+ZmFrey84EA/K8Ubtmn6AZbV0v7eRi8\nzevgT+I9a/ZWe/36mfkjXPnnn/Tr3n4bWrXy/ZpWt9/uuWfUxIlmUqpAmj7d9pnu3WvLeOtswQJb\nGhORns+NykqpvMB0YKClppAhI0aMuLqcnJxMcnJyRk+V5cTFRcivmAsF4ffnYNlTUHOSmb3tci7z\nemNnM8ObCIm4ODO2ok2b9D1kfG1k9eUXtvPEPoMH25bXrYPatdOfx9ukQdb9u3a1ZY4F+OknW8ZZ\nV7wlFvTXmTO22heY3GNNm7rumdWsmWl8DmQa9mBYuHAhCxcuDP2FfWlowASOOZhg4Gm/4UijslsX\nL2rdsaPrBslz58LYaKrSNJV+0tx3q2ZQSc1Nb2oSToe9ITeaHlr7t68VaP3ff7bXhw+bdStXpt/P\n3bkmTUpfBudl+0blihW9l+nnn23b7BuVd+5Mf40GDdJfzx9//pm+PDVrmm0nTzqeE7Ru29b/a/gL\ntG7VyiwPHKj10KGZPR9a68hoVAb4HNiktX7Xh30lT6QbCQmmO+o99ziu79fP5HO5+ebwlAsdA1vb\nwxcLYdq3ZrKeJ8pAi6GQz4eb5yLT/G38nDEDfvjBLGttnv/4I2PZQu+91zzbpwWxnjPQ7N/nDTcE\n91qR4t13ze23rMBrQFBKNQJ6AM2UUmuUUquVUm2UUv2UUn0t+xRWSu0FBgHPKqX2WG4xCScdOpjR\nq9b/BN99Bx99ZJYnTQpfua7aXx+mfwWf/AVxF+GRGtCxNxReH+6SZWsPP+zf/nfeaR72HnnEtjxn\njv8pvL/6yrZsfwvorbf8v9Xpy5f86tX+ndOVnTszfn13Nm2y3b6bMQPuvjvj58pqvLYhaK2XAh5T\namqtDwFBzoyS/Vy+bEY3W5UtG76ypHOyLMx5BxYOh3ofw71t4HB1+GMwbG9lahUiYMaP929/+wF0\nuXOnH1C3ZYtt2blB2x37Rm37wXM//uj6uqH000/w4ouOvaS2bYNKlfzrOeWLli1h/34TVCZPNj/a\nfHXmDOTNwj+F5X91GMW6CbPFijm+fuyx4JfFrf/yw5Jh8M5O2NAdWgwzE/bU+wjiz3k/XgSd1mYO\nhrVr02/b3HfXAAAgAElEQVR7/HHP2URfecW/a1lrs/4YOdK//du2Tb9u1qz0DdWeemVZawitW5tn\nb6PDAyUx0b8AEmkkIESg/Pltyxs3wmuvha8sV6XlgLX3wUdrTDK9CnNMO0PLIXCNm3q7CBnnlNzW\nFNbvvef5uGee8X7u33+3Le/Z45j51Rf2t6Jccb69E8iMttaxAPYJCtc73f2cPh2OHvV+rmPHTM3B\nG1djVrJKO4kEhAizZ4/pK715s5mV6vrrIVeucJfKnoJdTeGrH+Cz5aCuQN/60L09VJxlXouwc1Vb\n8IerQXZWGek63bat6eq6bVv6bWfOeE8U6EwpmDfPtuwL69gI53TcnTt7D5wALVpA8eLe9/PVzJmR\nl/pbAkKEKVkSChUyc//az8ZmTZRnNWQIhH0Ix4nyMPcNeHsPbLobmr4AAyrCza9DrmNhLpwIBndj\nE557zgxIc9dbavZs09PGOR04mEFx9er5Xgbr/BD27STOXP0id9WIvWiR79d1lfQvM9q3d0yQGAkk\nIGRRr75qurFGhEu5YW0f+GQlfDsVrvsbHq8Ad/SBYgFu8RNhT/Hs6sv2pZdMw6911sDPPnN97NSp\n/l/vww8d2y4qV/b/HO64+lHl6raQqzkxvPH3NtG4ceFrtLeSgJBFlC9vWy5d2jxH3n1JBSkN4IcJ\n8N42OFoVOneBhxpA7S8gLpP5jQUQ+LQPgWSdTc6+Z1JmuapVhMrXX5vnggX9P1ZrOHDAtuythvHg\ng4GvhfhLAkIWMW0avPyyWbbmRbL2oIhI56+FpUNgzL+m62q1r2FQKWj5f5DfReY1kSVcuGDattwJ\n1D3xJUtMug1P1/LUdhCIH0v2eZ8ycr5582w9Bi9fhsKFfT+2Y0ff564OJAkIWUSuXI6zswH072/y\ntjhz3i+sdCxsaweTZ5lGaIAHG0L3dlBxJigvCXNERPngg9Bc55ZboFcvM/La+ivbmbUtwddGZX8H\n6vnya/3iRdvgOGva8cxO9AOmhuUulXowSUDIQhIT4Y03YPRo8zpPHvOLLCnJcT/r3M41a4a2fF6d\nKA+/vm5phO4MySPg8YrQ6DXI7UO/PxFVrLnd3M0bsWCB+2M3bXKc9tTZ+fPp/9/8/LNjTaBXL8ft\nrtoWcuaEcuXMsvX+/7Bh7q8b6SQgZDFPPpl+KL39H3G/frbpPN95J3Tl8svlXGZMw6cr4ZtpUGiT\n6Z3UsbfJoyQE7lNYO3N3O8fTeIm5cx0DhlImrUywU2Nbp1hdtsw8R1o7oMypnI106eLYG6NatfCV\nxWf768MPX5huqnU+h07d4HxBMw/0391M8BBRyVv6bStfuqyePAm33WZ77ZwHyurGG307V44cUL26\nbd2VK+lTjLuSmGiCgLXR/Ztv4OzZyEl3ITWEbKB9e2je3DEhmdYZy3wZNhcKwrL/szRCj4TrvzWN\n0K2egvw+zF8pokJGu2X6Wts4eND7PvnzQ9++jrPSvfSSmTYzI+ynSy1a1LfR48EiASEbmDzZNmrT\nmbsqqf2gt4iiY2BbW5jyM3y2Aq7EwoM3Qq/mUH0qxHmZVkxka/6kCXfXGO3KMT/HUTpnWbU2cGfE\n5s2OqTOs6TZOnw5MRlh/SECIQg8+aEaVRrwT5WDeq/DWPljVz9xSGlQSWg8y7Q4i6nganezsiy98\nPy6zPYN8uV3kTsuWtjkpwJbG4+mnbXNGhIoEhCjQrp3j608/jbzGLI/ScsDGLjDxV/j0T9Ou0KsF\nPHAz1P0MEs6Eu4QiRKwdJnxhn59o717P+44bl7HyWGU2w6l9QLLeQsrI6OjMkoAQBey//K09lLJU\nQLB3siz89rLpurr4aTOWYVApkyaj1GIgq74xkZVk5v+PdaJPe/YZZcNJehlFAesfX69etslSsmxA\nsLoSB1s7mEeeQ1BzErTvD7GpsOZ+WNcLzgQwNaXI0v7+O9wlsImP970HVahJDSGK+DpzVpZzrjD8\n8SSM/Ru+mwTX7DJTf/a4DWpMkRxKgj/+COz5/B31bC9SgwFIQIgKrvo4Z/kagksKUhrCzx+bhuj1\nPaHWl/BkcVN7KP4ncktJBMLSpaG7VocOobuW0l6+GZRSJYAvgcLAFeBTrfUYF/uNAW4DzgH3aa3T\nTdGhlNLericC7+RJMzmIfdrguXNNiuJbbw3zFJ2hkLgPak0wGVd1LKzrCRt6wMky4S6ZED5SaK0z\nUS/x8So+BIQiQBGt9VqlVF5gFXCH1nqL3T63AY9prdsppRoC72qt0435k4AQmXyt/r7xBjz1VHDL\nElwaSiyHWhNN9tXD1UwtYmNnuJjk/XAhwiY0AcHrLSOt9UHrr32t9VlgM+DcWncHphaB1noFkKSU\n8iPZqwinrl2hUyeznDMnNGzoer8SJUJXpuBQsO8mMyf0m/th+SAz7eegUiZlRsVZEJOB+SGFyCb8\nakNQSpUBagMrnDYVB+x7+qaQPmiICPXVVzB0qFmeMQOaNnW9X2Ya0iJOWgJs6QjTvoN3d8DuJnDr\nizC4hBn4VnQ10t4goo3P3U4tt4umAwMtNYUMGTFixNXl5ORkksM+MbAAkyDs4EEzicf8+b4dc9dd\nmR+QExEuWJLprXwECm6FmhOhSye4nAPW3wsbupvxD0KEzELLI7R8CghKqThMMJiotXY1OV4KUNLu\ndQnLunTsA4KILN5mdMplSTyqtaktWPPAZyvHKsGC/8GCF6HkH1BjspkC9FhlExg2dobzhcJdSpHt\nJVseViNDclVfbxl9DmzSWr/rZvsMoBeAUupG4KTW+lAAyifCwHlinTx5zHO7drBmTejLEx4K9t4M\nsz6At1JgyVAotcRM6HNva9NrKYeHGViEyIJ86WXUCPgd2IC5qaqBZ4DSgNZaf2LZ732gDabbaR+t\ndbo8fdLLKGtp0sRM4zdwILz7ruPYBaXgyy9ts0oVKeJb6uAsL/4cVPrZDHgrsxB2NjfzNmxtB5fy\nhLt0ItuKkG6nAb2YBIQsZd8+2LXLTHR+112uB7MtXQqNG9sCwquv2hqos72cJ6DKD1D9KyixwqTt\n/rsb/NvaJOQTImAkIIgIYf3Sd/dPp5QtIBw+nMUm5gmU3Efg+ulQYyoU2ghb7oQN98DuW03eJSEy\nJULGIQiRUbNnh7sEIXS+EPz1MIz/HT5aC0erQMsh8GQx6NAXys+FmEvhLqUQHkkNQXiV0RqCtTdS\nVMu/A6p+Z2oPBf6FfzrApk6wo4XcVhJ+kBqCyCLatYPOnR3XWUc+g2l8jlonysGyp+Cz5fDRGjhY\nGxqPhqeKwF33mjYIycYqIoTUEIRX3moIVkqZuWG3b4eyZaFQIdPA/OSTtjEO587ZurFGtbwHoOr3\nUPVbKLbKNERv6mQapqW3kkhHGpVFhNixA8qX9x4Q1q6F2rVdb7PeOrLeRvrsMzO3s8A0SFf5Aa7/\nFkr8YbqybuoEW9vDxcRwl05EBAkIIhtxDggbN0K1auEtU0TKdRwq/WTaHMosgl23muDwz+3wX/5w\nl06EjQQEkY04B4StW6FSpfCWKeLlOAWVZprgUPY3k6l1UyfYcoekz4g6EhBENuIcEE6ehGuuMevq\n1oXVlnHtBQva2iKEnYSzUGG2CQ4VfoEDdU221i0d4VSpcJdOBJ0EBJGN2AcE53W//gq1apnuqgUL\nmmAQHw+XZWoC1+IumKBQ5QeTRuNkaTMQbvOdcOR6INr7+mZHoQkIMoRShJ1SpkcS2AJG2bKwbVv4\nyhTRLuey1Q5iLkOpxSY49GhrxjZs6WhuK+270UwZKoSPZByCCIncudOvq1vXPOfMmX5b/fq25SpV\nglOmbOFKHOxqCnPehXd2wfSvzDwO7R+Gp4rCHfdDle/NLSchvJAaggiJZ581yfLsLV4MmzfbAgNA\n3rzmOdbuh+1bb5kaQ9WqwS9n1qZM28KBumZOh2t2QuUZ0OADuLM37GlsRkpvayftDsIlaUMQEWPX\nLkhIgGLF4NgxWLDAjIC2/sk8+yy8/LJZrlAB/v03bEXNenKcggpzTJtDhTlwtogJDFvbmd5LkoAv\nwkkbgogyZcrYlgsWhDvvhDFjbOvss6hWqyYBwS8Xk2BjV/NQaVD8T9Ol9bbHIWkPbG9lAsS/beD8\nteEurQgTCQgiYsXGwoAB4S5FNqRjTa1g300wfxTkS4GKs0wajbaPwpFqpuawrR0crIX0WooeEhBE\nliR3HgPoTHFY/ZB5xF6E0r+b2kPnzhB/weRX2trOpNRIzRvu0oogkl5GIst7/fVwlyAbScsBO1rC\nnHfgvW0wYT4cqQoN34Mni0LPVtDwXZPKW2Q7EhBEllG+vHmOjYU6dczy9Onw1FNmedQoOHXKVnt4\n5x3H4/v3D005s5VjlWD5IPhyHryVAn/1h8Lroc8t8FhlaD3ITP4T91+4SyoCwOstI6XUOKA9cEhr\nXdPF9muAz4HywAXgfq31pkAXVIj27c3oZaXMY+RIx+2lS0OiXXJQa8ptESAXE2HzXeahrkCRNVBx\nNtw6EgpvgN1NTKP0v23geIVwl1ZkgC81hPFAaw/bnwHWaK1rAb2BMR72FSJTYmMhJib9TGz//gvd\nuzuuy5MHevWyvVYK5s4Nfhmjgo6BAzfA78/B50vNoLh1PaHoaujTBB4vD+0egco/Qo7T4S6t8JFP\n4xCUUqWBn9zUEH4GXtFaL7W8/he4SWt9xMW+Mg5BBIxSZnBb48bpt+XLZ+ZnKFEC/vvPJNJ7+GEY\nOxYaNYJly0Jf3uih4bq/Tb6lCnOg+AozU9z2Vuaxv56k1PBb1hmHsA64C1iqlGoAlAJKAOkCghCB\ndOaMbWSzq21WOSxTF0f9/M4ho+BwDfNY9pRJxld6sWlruP0h0811ZzNbgDhVOtwFFhaBCAijgXeV\nUquBDcAaIM3dziNGjLi6nJycTHJycgCKIKKRu2AgIszlXLYvf4B8+6HcPBMgmj8LF/Lbtu9KhtR8\nYS1uZFhoeYRWpm8Zudh3J1BDa50um5bcMhLhohQ8+ii8/z40a2bSYogIoK5A4XUmOFT4BYqtNG0T\n21vB9pYmL5PcXiLSbhkp3AxXVEolAee11peUUg8Bi1wFAyEixdSpcPiwaVNYujTcpYlyOgYO1jGP\npUMh/pyZOrT8XOh4H+Q9ZG4v7Whhno+XR0ZOB4/XGoJSagqQDBQEDgHDgQRAa60/UUrdCEwArgAb\ngQe01qfcnEtqCCIslILHHoP33rOtu3LFMauqiECJ+6Dcr1B2vnnoWBMYdjaDnU3hdMlwlzBEZMY0\nIQJm2jTTG6l4ccf1rhqaW7Y0s7iJSKOh4FZbcCi7AC4UMIFhZzMzL8S567yfJkuSgCBE0NkHhHvv\nhQYN4PrroUULz8f16wcff2yW+/aF++6Dm28OWjGFK+qK6d5a9jcTHEr/buZ5sNYgdjeB/64JdykD\nRAKCEEFnHxCsf5rz50Pz5p6P698fPvrI8Tjp1hpmMZfNwDhrgCjxBxytYpLy7WgOexvBJRdT92UJ\nEhCECLoPPjBtC2D7Yk9Lg7g4OHQI1q83t5CcPf00vPKK43ExMZKFNaLEXoQSy223mIqugf03wO5b\nTffWvTeZLrFZggQEIUKicGHT68j+T/PcOZP6Amy//O1naRs2DEaPNo3Sly+bdV26wDffhK7cwk8J\nZ6HUEii9CMosNPmXDtQ1wSHiA0RkdTsVIqpYg4G9bdtgxQq48Ubbum7dbMsNGrgPCMuWSRtD2KXm\ntSXfAxMgSi41waH5M6Y94kBd0/awpzHsaRR1g+QkIAjhRZcuposqQMOGjtt8aTcYM0baFyJSal7Y\n3to8wBYgSi+Gxq9AsVVw5HoTIHbdaoLEf/nDW+Ygk4AghBfTprnf5qpR2l5qqmmPWLEi8OUSAeYc\nIOL+M3NPl/4dGo6Bu7vDifKw+xYTHPY2gtMlwlvmAJOAIEQG+NIU1rw5xMe73la8OKSkmOWnnoI3\n3ghc2USAXM5page7mwDPQWyq6cVUajHUmArtHjVBZE8jW4A4XC1Lp9qQgCBEJrhLsLd6NRQt6v64\ntm3h00/N8muvQY8etlngRIRKS4B9N5rHsv/j6kC5Ukug1FK46W3IfcRstwaIlAZZqqurBAQhMsDa\n6Pzaa7Z1bdvCkCFm2fnL3bkNwf61UlC7tvtrtWkDc+ZkvKwiWBQcq2weax4wq/IcNu0QpZZCi2Fw\n3QY4XN0WIPY0gnORO5WfBAQhMqBiRdi3z7GGUK2amcZz927fz+NL9vc33pCAkGWcuw623GkeYOaC\nKL7S1CLqjIPbH4Dz19oFiMZwtDKRkrBPAoIQGaB1+rxIAJs2ue6y6qyEl7bIvXuhZLTkbcvOLuey\na4fApNsotMk2HuKWlyDHGdh7s62r6/56kJYjLMWVgWki6rkamOaJUjBlCtxzT/pt1lHOzudKS4Mf\nfoBOnczrS5dMg3Nysm1uBuceS9bXf/8N1av79ZZEVpIvxdxiKrnUBIpr/zFTjloDxN6b4cK1IRmY\nFhPsCwghzIjmu++2LcfFmZxJn39u22fvXnj77fTHuuup1KdP4MspwuBMcdjYBea8C5+sgjcOwMIR\ncCmX6e76RNmQFUVqCCLq/fUXnD5tZlLzRUZqCPbH2qe7cPb993DXXY41BPtle1qb+aJTU30rt8ii\nYi7DlXipIQgRCvXq+R4MvImN9T6XgqdRy5UrO762tiO8+KLr/YcPh+ef9718Igu6ErqmXqkhCOEn\nTzUEX46NizNtCL7sW7Ik7NljXvfuDV9+adtu/19JUmNkd6FJbic1BCGyoAED0q/r0iX05RDZi3Q7\nFSKENmww8yZkhH2NoH9//46NibEl6BPCHa9/mkqpcUqpQ0qp9W62JyqlZiil1iqlNiil7gt4KYXI\nJqpXN1N0Zpa/57DeUipSxHF9kyaZL4vIPnz5rTIeaO1h+6PARq11baAp8KZSSmoeIlu7JgxT9Wak\n+e3nn91v27sXFi3KeHlE9uM1IGitlwAnPO0CWGeRyAcc01q76VQnRNa3b5/JL5QVVKjgfpvzaGlv\n80iL7C8QjcrvA9crpfYD64CBATinEBGrePHQ9eqxrxX07Qu9enne/6OPHF9fd5159qW81ar5VzaR\n/QTi1k5rYI3WuplSqjzwq1Kqptb6rKudR4wYcXU5OTmZZF+yewkhuOUWqFXLseupM+dpOvPnN0HF\n3Whne7FZN41/NrTQ8gitQASEPsArAFrr7UqpnUAV4C9XO9sHBCGEf3L4kfNs1Srb8sqVZhR1+/au\n9y1WzAxyc5U6A2DwYHjrLd+vLTIr2fKwGhmSq/oaEBTu87PuBloAS5VShYFKwI4AlE0I4SRHDteN\ny+++a3oM5bObE75uXduyp/kWwNQskpLcb7fP4FqlCmzZ4lt5RdbiNSAopaZgQlVBpdQeYDiQAGit\n9SfAKOALu26pQ7TWx4NUXiGiRt++UNbHvGaPP+75tVX//jB1Krz6qn9lUQr274eCBU3PpFat/Dte\nZA1eA4LWuruX7Qfw3C1VCJEBH3+c8WNLl3a9fvhw8/Akb14466IF0NOUoCJ7kNQVQggHw4alX/fA\nA6Evhwg9CQhCZEMZ7RarNbR2qu/fdReUKmV7XbNmxsslIpsEBCGEXwq7mSPe1/YOEbkkIAiRDZUr\nF/prFiwY+muKwJKAIEQ2k5oKd9wR3Gt88w0sWZLxhmaZIzoySUAQIpvxZVRyZnXqBI0a2cZE3HYb\ndOvm+/EbNrheb5106M47M1c+kTESEISIcs884zhuoVYtuOEG22tfchzNmpV+7MNtt8HMme6Pcb7F\ntHatmYlOhI9MoSmESOfTT83AuIsXzZSf7ib1KVIEDh0yNYVLlyAhwayfNw8qVTJTgLrq8aS1+ylB\nlTK1jdtvh+4eR0FFk9BMoSnzFggh3LJ+wbvj7vddZlJpr11rAskvv2T8HCJj5JaRECLoKlQwX/S+\nqFULChSABg0cG5+XLw9O2YSNBAQhRIb5egf4iy/MF70/ypd3bHzOm9ek87bXp49/5xSeSUAQQmSY\nrwEhWHmQZJa3wJKAIIRIx9c5o+1TZsfFwYMPpg8SWgduoJz0SQkuCQhCiHQ6dYJdu7zvt2SJbT+l\nTO+kQPvmG/fbSpa0LW/fbls+dizw5YgG0stICJGOUu5TaNsrUiT4ZenUybbs3IW1SRPbcrly8N9/\npmdUqOa8zm6khiCEyDJWrPC8PUcO34OBBI30JCAIIULm5ElYv977fu5UrJh+3Y4dcOWK92OHDs34\ndaOFBAQhRMgkJUGNGo7rBg3K3DnLlvXt1/7o0Y6vq1TJ3HWzI68BQSk1Til1yG7OZOftTyml1iil\nViulNiilLiulfOyjIISIVsWKmefYWO/7Tprk/xd4/fqet//xh+ftv//u3/WyA19qCOPxMGey1voN\nrXUdrXVd4Glgodb6ZKAKKITInl580eRB8kWPHr4FDns33eR+W716ZqCbswkTbMslSvh3PXCf8ymr\n8NrLSGu9RCnlQ38DAO4BpmauSEKIaBAfD9ddBx06wKpVgT9/nItvt6NH4cKF9COey5c33Vbr1cvc\nNQsXhgMHMneOcApYPFNK5QLaAN8G6pxCiOyvSROYP9/3/c+e9W2/4cPTrytY0Pzyz5PH9+uBGV/h\nqkbhjj9zQ0SSQFZwOgBL5HaRECKYrF/mdep43i8x0bZ86ZLnfXPndr2+cWPfyuQ8F0RWFciBad3w\n4XbRiBEjri4nJyeTnJwcwCIIIaKFqxqAO65uH8XGwtKlJnBs3Gh+1dv3VlLKdHNdssT36wRubMNC\nyyO0fA0IyvJwvVGpJOBWoIe3E9kHBCGECKebbzbPO3dm7jw1azq+znxgSLY8rEZm9oQ+8aXb6RRg\nGVBJKbVHKdVHKdVPKdXXbreOwC9a6wvBKqgQQlj17WvmdPbG2rXVG2uX1oyk4njxRXjgAffbJ082\nYyB69/b/3KHmSy8jr5PYaa0nABO87SeEEIHw8ce+7de8Ofz2m/f9Kla0ZVLVOv0vfPvXJUrAvn22\n188/7/nc9tOATojwb8ks3mtWCCHc++IL2L07sOfs39+2vHGj6338vWX0zTfBmzPCHxIQhBDZVkyM\n6wbljMiRwzw/8oh5HjAArr/e//O4OqZkSciXL+NlCxQJCEII4YLzr/zffjMB5ppr4JNPYMwY98f2\n6mWe+/VLv83fMRChJAFBCCF8UKsWpKWZQPHQQ573be0m2c+ECZ4DiSvNmvm3f2bIBDlCCBEi1ppD\npJIaghBCuOFvCoqsntwuixdfCCGCIyEBWrXy7xhfU3QfOuS+62xyMqxebXvt7fZUIElAEEIIJ7t2\nBaYbqHVsg7PrrnPfuLxggcnTZG2HCGWiPGlDEEIIJ6V9TfjvJJDzNHfqBOfOBe58vpAaghBChEFS\nkm05Ls7MD2HvwQdh8eLQlklqCEIIEQbt2sGePXD4MNStC7Nnw6lT4S2TBAQhhPDgvvv8b1z2hVJm\nhHLJkua1/XK4SEAQQggPxo8PdwlCRwKCEEIEwHPPmVHF1pnc3ngDbrstvGXyl9Lu+kUF42JK6VBe\nTwghsgOlFFrrAPZhck16GQkhhAAkIAghhLCQgCCEEAKQgCCEEMLCa0BQSo1TSh1SSq33sE+yUmqN\nUupvpdSCwBZRCCFEKPhSQxgPuJnuAZRSScAHQHutdXWgc4DKlq0tXLgw3EWIGPJZ2MhnYSOfReh5\nDQha6yXACQ+7dAe+1VqnWPY/GqCyZWvyx24jn4WNfBY28lmEXiDaECoBBZRSC5RSK5VSPQNwTiGE\nECEWiJHKcUBdoBmQB/hDKfWH1vrfAJxbCCFEiPg0UlkpVRr4SWtd08W2oUBOrfVIy+vPgNla629d\n7CvDlIUQIgNCMVLZ1xqCsjxc+RF4TykVC+QAGgJvudoxFG9ICCFExngNCEqpKUAyUFAptQcYDiQA\nWmv9idZ6i1LqF2A9kAZ8orXeFMQyCyGECIKQJrcTQggRuUI2Ulkp1UYptUUptdXS7pDlKaVKKKXm\nK6U2KqU2KKUet6zPr5Saq5T6Ryn1i2WshvWYp5VS25RSm5VSrezW11VKrbd8Pu/YrU9QSn1lOeYP\npVSp0L5L/yilYpRSq5VSMyyvo/KzUEolKaW+sby3jUqphlH8WQyyDFpdr5SabCl7VHwWrgb2huq9\nK6V6W/b/RynVy6cCa62D/sAEnn+B0kA8sBaoEoprB/l9FQFqW5bzAv8AVYBXgSGW9UOB0Zbl64E1\nmFt1ZSyfibWWtgKob1meBbS2LD8MjLUsdwW+Cvf79vKZDAImATMsr6PyswC+APpYluOApGj8LIBi\nwA4gwfJ6GtA7Wj4LoDFQG1hvty7o7x3ID2y3/N1dY132Wt4QfSg3YnoeWV8PA4aG+x8rCO/zB6AF\nsAUobFlXBNji6n0DszGN8EWATXbruwEfWpbnAA0ty7HAkXC/Tw/vvwTwK6bNyRoQou6zABKB7S7W\nR+NnUQzYbfmCigNmRNv/EcwPYfuAEMz3fth5H8vrD4Gu3soaqltGxYG9dq/3WdZlG0qpMphfAssx\n/9iHALTWB4HrLLs5fw4plnXFMZ+Jlf3nc/UYrXUacFIpVSAobyLz3gb+D7BvmIrGz6IscFQpNd5y\n++wTpVRuovCz0FrvB94E9mDe1ymt9Tyi8LOwc10Q3/spy3t3dy6PJNtpACil8gLTgYFa67M4fiHi\n4nWmLhfAcwWMUqodcEhrvRbPZcz2nwW2wZofaK3rAucwv/6i8e/iGuAOzK/kYkAepVQPovCz8CBi\n3nuoAkIKYN/QU8KyLstTSsVhgsFErfWPltWHlFKFLduLAIct61OAknaHWz8Hd+sdjlFmrEei1vp4\nEN5KZjUCbldK7QCmAs2UUhOBg1H4WewD9mqt/7K8/hYTIKLx76IFsENrfdzyC/Z74Gai87OwCsV7\nz9B3bqgCwkqgglKqtFIqAXN/a0aIrh1sn2Pu771rt24GcJ9luTdm8J51fTdLz4CyQAXgT0u18ZRS\nqmW+etsAAAEcSURBVIFSSgG9nI7pbVnuDMwP2jvJBK31M1rrUlrrcph/3/la657AT0TfZ3EI2KuU\nqmRZ1RzYSBT+XWBuFd2olMppeQ/NgU1E12fhPLA3FO/9F6ClMr3d8gMtLes8C2HDShtML5xtwLBw\nN/QE6D01wgzGW4vpHbDa8j4LAPMs73cucI3dMU9jeg9sBlrZrb8B2GD5fN61W58D+NqyfjlQJtzv\n24fP5VZsjcpR+VkAtTA/hNYC32F6e0TrZzHc8r7WAxMwPQ2j4rMApgD7gYuY4NgH08Ae9PeOCTrb\ngK1AL1/KKwPThBBCANKoLIQQwkICghBCCEACghBCCAsJCEIIIQAJCEIIISwkIAghhAAkIAghhLCQ\ngCCEEAKA/wc43k9CVfkoYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b6d823dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b7cb3c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
