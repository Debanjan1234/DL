{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # row or hight\n",
    "# fh = 5 \n",
    "# fw = 4 \n",
    "# C = 3\n",
    "# i0 = np.arange(fh) # field hight\n",
    "# i0, i0.shape\n",
    "# i0 = np.repeat(i0, fw) # field width\n",
    "# i0, i0.shape\n",
    "# i0 = np.tile(i0, C) # C\n",
    "# i0.shape, i0\n",
    "# oh = 1\n",
    "# ow = 2\n",
    "# i1 = np.repeat(np.arange(oh), ow)\n",
    "# i1, i1.shape\n",
    "# i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "# i.shape, i1.shape, i0.shape, i1, i0, i[-4:], \n",
    "# # i.astype(int)\n",
    "# i.T.shape, i.T\n",
    "\n",
    "# # for cols or width\n",
    "# j0 = np.tile(np.arange(fw), fh * C)\n",
    "# j1 = np.tile(np.arange(ow), oh)\n",
    "# j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "# j0.shape, j1.shape, j.shape, j0, j1, j[:10]\n",
    "# j.T.shape, j.T\n",
    "\n",
    "# k = np.tile(np.arange(C), fh * fw).reshape(-1, 1).T\n",
    "# k.shape, k\n",
    "\n",
    "# X = np.random.rand(1, 2, 3, 4)\n",
    "# X.shape, X\n",
    "# np.pad(X, ((0, 0), (0, 0), (2, 2), (0, 0)), mode='constant')\n",
    "# # np.pad(X, ((0, 0), (0, 0)), mode='constant') # zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution impl\n",
    "# from impl.im2col import *\n",
    "# or\n",
    "# import impl.im2col as im2col\n",
    "# out_height = int(((H + (2 * pad) - kernel_height) / stride) + 1), \n",
    "# stride == 1, ALWAYS\n",
    "# pad == kernel//2, ALWAYS\n",
    "# kernel == min size ALWAYS, i.e. one past, one pres, one post (if exist), i.e. three or two\n",
    "# kernel == 3 or 2 ALWAYS\n",
    "# input=X, kernel=3or2, padding=kernel//2, stride=1, output=y\n",
    "def get_im2col_indices(X_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    # Input shape\n",
    "    N, C, H, W = X_shape\n",
    "    \n",
    "    # Kernel shape\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C\n",
    "    \n",
    "    # Output shape\n",
    "    assert (H + (2 * padding) - field_height) % stride == 0\n",
    "    assert (W + (2 * padding) - field_width) % stride == 0\n",
    "    out_height = int(((H + (2 * padding) - field_height) / stride) + 1)\n",
    "    out_width = int(((W + (2 * padding) - field_width) / stride) + 1)\n",
    "    out_C = 1 # the output channel/ depth\n",
    "\n",
    "    # Row, Height, i\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, field_C)\n",
    "    i1 = np.repeat(np.arange(out_height), out_width)\n",
    "    i1 = np.tile(i1, out_C)\n",
    "    i1 *= stride\n",
    "    \n",
    "    # Column, Width, j\n",
    "    j0 = np.tile(np.arange(field_width), field_height * field_C)\n",
    "    j1 = np.tile(np.arange(out_width), out_height * out_C)\n",
    "    j1 *= stride\n",
    "    \n",
    "    # Channel, Depth, K\n",
    "    k0 = np.repeat(np.arange(field_C), field_height * field_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 = np.repeat(np.arange(out_C), out_height * out_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 *= stride\n",
    "    \n",
    "    # Indices: i, j, k index\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = k0.reshape(-1, 1) + k1.reshape(1, -1)\n",
    "    \n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(X, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant') # X_NxCxHxW\n",
    "\n",
    "    k, i, j = get_im2col_indices(X.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    X_col = X_padded[:, k, i, j] # X_col_txkxn\n",
    "    \n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "    \n",
    "    X_col = X_col.transpose(1, 2, 0).reshape(kernel_size, -1)\n",
    "    \n",
    "    return X_col\n",
    "\n",
    "def col2im_indices(X_col, X_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = X_shape\n",
    "    H_padded, W_padded = H + (2 * padding), W + (2 * padding)\n",
    "    X_padded = np.zeros((N, C, H_padded, W_padded), dtype=X_col.dtype)\n",
    "    \n",
    "    k, i, j = get_im2col_indices(X_shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "\n",
    "    X_col = X_col.reshape(kernel_size, -1, N).transpose(2, 0, 1) # N, K, H * W\n",
    "    np.add.at(X_padded, (slice(None), k, i, j), X_col) # slice(None)== ':'\n",
    "    \n",
    "    return X_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    \n",
    "    # Input X\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    \n",
    "    # Kernel W\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "    \n",
    "    # Output\n",
    "    h_out = ((h_x + (2 * padding) - h_filter) / stride) + 1\n",
    "    w_out = ((w_x + (2 * padding) - w_filter) / stride) + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filter, -1)\n",
    "\n",
    "    out = (W_col @ X_col) + b\n",
    "    out = out.reshape(n_filter, h_out, w_out, n_x).transpose(3, 0, 1, 2)\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W = W.reshape(n_filter, -1)\n",
    "    dX_col = W.T @ dout\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob in this case. \n",
    "# Is this true in other cases as well? Yes.\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.arange(24).reshape(2,3,4)\n",
    "# a\n",
    "# # np.add.at(a, (slice(None), 0, 1, 2), cols_reshaped) # slice(None)== ':'\n",
    "# # a[:, 0, 1]\n",
    "# # a[0,:,1] \n",
    "# # # or \n",
    "# # a[0,slice(None),1],\n",
    "# # # outputs array([1, 5, 9])\n",
    "\n",
    "# # a[0,None,1] \n",
    "# # a[0, 1]\n",
    "# # # gives array([[4, 5, 6, 7]])\n",
    "\n",
    "# # # Could sb explain the latter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "# # Becoming more familiar with y_train data structure, abstraction, shape, and type\n",
    "# y_train.max(), y_train\n",
    "# data = y_train\n",
    "# data.shape, data[0]\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "# X_train[0, :10, :10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout, lam):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lam = lam\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = selu_dropout_forward(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy_reg(self.model[2], y, y_train, lam=self.lam)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = selu_dropout_backward(dout=dy, cache=h_do_cache)\n",
    "            dh = selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = conv_backward(dout=dh, cache=X_conv_cache)\n",
    "        # dX: TODO: hast not been used but this basically should be 0 \n",
    "        # which means input can be perfectly recontructed!\n",
    "        # dX is the grad_input or delta for input or the calculated error or difference or delta\n",
    "        # Can be used as noise which is because it is unwanted and can be added to data to be calculated again\n",
    "        # when the data is not abundantly available!\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        # else: # self.mode == 'regression'\n",
    "        # return np.round(y_logit)\n",
    "        # y_prob for accuracy & y_logit for loss\n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        # if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Train the model\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Test the model \n",
    "            # (validate the model)\n",
    "            # if val_set:\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            # def accuracy(y_true, y_pred):\n",
    "            # return np.mean(y_pred == y_true)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "\n",
    "            # Update the model\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Print the model \n",
    "            # loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(iter, \n",
    "                                                                                                     loss, \n",
    "                                                                                                     valid_loss, \n",
    "                                                                                                     valid_acc))\n",
    "                \n",
    "        # Test the model\n",
    "        # The test data has NOT been used before and has NOT been seen by the model\n",
    "        # # Kernel dead problem sometimes!\n",
    "        y_pred, _ = nn.test(X_test)\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}'.format(acc.mean(), acc.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 train loss: 3.4985 valid loss: 3.3933, valid accuracy: 0.1154\n",
      "Iter-2 train loss: 3.1041 valid loss: 3.1780, valid accuracy: 0.1310\n",
      "Iter-3 train loss: 3.1126 valid loss: 2.9822, valid accuracy: 0.1492\n",
      "Iter-4 train loss: 3.2935 valid loss: 2.8130, valid accuracy: 0.1674\n",
      "Iter-5 train loss: 3.0202 valid loss: 2.6537, valid accuracy: 0.1888\n",
      "Iter-6 train loss: 2.8332 valid loss: 2.5077, valid accuracy: 0.2142\n",
      "Iter-7 train loss: 2.3694 valid loss: 2.3755, valid accuracy: 0.2454\n",
      "Iter-8 train loss: 2.4355 valid loss: 2.2554, valid accuracy: 0.2788\n",
      "Iter-9 train loss: 2.5073 valid loss: 2.1474, valid accuracy: 0.3152\n",
      "Iter-10 train loss: 1.9928 valid loss: 2.0488, valid accuracy: 0.3470\n",
      "Iter-11 train loss: 2.1001 valid loss: 1.9629, valid accuracy: 0.3728\n",
      "Iter-12 train loss: 1.7956 valid loss: 1.8833, valid accuracy: 0.3986\n",
      "Iter-13 train loss: 2.1597 valid loss: 1.8100, valid accuracy: 0.4224\n",
      "Iter-14 train loss: 2.1831 valid loss: 1.7426, valid accuracy: 0.4448\n",
      "Iter-15 train loss: 2.0772 valid loss: 1.6758, valid accuracy: 0.4658\n",
      "Iter-16 train loss: 1.5257 valid loss: 1.6093, valid accuracy: 0.4888\n",
      "Iter-17 train loss: 1.4509 valid loss: 1.5432, valid accuracy: 0.5104\n",
      "Iter-18 train loss: 1.8136 valid loss: 1.4806, valid accuracy: 0.5328\n",
      "Iter-19 train loss: 1.5375 valid loss: 1.4231, valid accuracy: 0.5506\n",
      "Iter-20 train loss: 1.6480 valid loss: 1.3715, valid accuracy: 0.5710\n",
      "Iter-21 train loss: 1.4649 valid loss: 1.3250, valid accuracy: 0.5860\n",
      "Iter-22 train loss: 1.5608 valid loss: 1.2825, valid accuracy: 0.5992\n",
      "Iter-23 train loss: 1.0857 valid loss: 1.2437, valid accuracy: 0.6118\n",
      "Iter-24 train loss: 1.4950 valid loss: 1.2109, valid accuracy: 0.6238\n",
      "Iter-25 train loss: 1.3563 valid loss: 1.1772, valid accuracy: 0.6372\n",
      "Iter-26 train loss: 1.2848 valid loss: 1.1457, valid accuracy: 0.6462\n",
      "Iter-27 train loss: 1.2103 valid loss: 1.1139, valid accuracy: 0.6554\n",
      "Iter-28 train loss: 1.0788 valid loss: 1.0818, valid accuracy: 0.6648\n",
      "Iter-29 train loss: 1.1651 valid loss: 1.0528, valid accuracy: 0.6746\n",
      "Iter-30 train loss: 0.9220 valid loss: 1.0245, valid accuracy: 0.6822\n",
      "Iter-31 train loss: 1.0626 valid loss: 0.9989, valid accuracy: 0.6870\n",
      "Iter-32 train loss: 0.9829 valid loss: 0.9744, valid accuracy: 0.6936\n",
      "Iter-33 train loss: 1.2011 valid loss: 0.9514, valid accuracy: 0.7026\n",
      "Iter-34 train loss: 0.9105 valid loss: 0.9295, valid accuracy: 0.7092\n",
      "Iter-35 train loss: 1.1751 valid loss: 0.9092, valid accuracy: 0.7158\n",
      "Iter-36 train loss: 1.2853 valid loss: 0.8916, valid accuracy: 0.7200\n",
      "Iter-37 train loss: 1.0039 valid loss: 0.8743, valid accuracy: 0.7254\n",
      "Iter-38 train loss: 1.1695 valid loss: 0.8596, valid accuracy: 0.7280\n",
      "Iter-39 train loss: 1.0471 valid loss: 0.8458, valid accuracy: 0.7318\n",
      "Iter-40 train loss: 0.9537 valid loss: 0.8330, valid accuracy: 0.7358\n",
      "Iter-41 train loss: 1.0157 valid loss: 0.8220, valid accuracy: 0.7380\n",
      "Iter-42 train loss: 0.8660 valid loss: 0.8103, valid accuracy: 0.7420\n",
      "Iter-43 train loss: 0.9461 valid loss: 0.7981, valid accuracy: 0.7450\n",
      "Iter-44 train loss: 0.8454 valid loss: 0.7849, valid accuracy: 0.7470\n",
      "Iter-45 train loss: 0.6594 valid loss: 0.7720, valid accuracy: 0.7508\n",
      "Iter-46 train loss: 0.8536 valid loss: 0.7606, valid accuracy: 0.7544\n",
      "Iter-47 train loss: 0.6810 valid loss: 0.7496, valid accuracy: 0.7584\n",
      "Iter-48 train loss: 0.8565 valid loss: 0.7402, valid accuracy: 0.7642\n",
      "Iter-49 train loss: 0.8378 valid loss: 0.7317, valid accuracy: 0.7656\n",
      "Iter-50 train loss: 0.7920 valid loss: 0.7234, valid accuracy: 0.7696\n",
      "Iter-51 train loss: 0.9544 valid loss: 0.7149, valid accuracy: 0.7718\n",
      "Iter-52 train loss: 0.6682 valid loss: 0.7062, valid accuracy: 0.7756\n",
      "Iter-53 train loss: 0.9003 valid loss: 0.6976, valid accuracy: 0.7794\n",
      "Iter-54 train loss: 0.9684 valid loss: 0.6897, valid accuracy: 0.7838\n",
      "Iter-55 train loss: 0.5836 valid loss: 0.6808, valid accuracy: 0.7862\n",
      "Iter-56 train loss: 0.9178 valid loss: 0.6734, valid accuracy: 0.7890\n",
      "Iter-57 train loss: 0.7356 valid loss: 0.6664, valid accuracy: 0.7898\n",
      "Iter-58 train loss: 0.5643 valid loss: 0.6600, valid accuracy: 0.7940\n",
      "Iter-59 train loss: 0.6958 valid loss: 0.6541, valid accuracy: 0.7962\n",
      "Iter-60 train loss: 0.6054 valid loss: 0.6486, valid accuracy: 0.7962\n",
      "Iter-61 train loss: 0.7263 valid loss: 0.6429, valid accuracy: 0.7974\n",
      "Iter-62 train loss: 0.9120 valid loss: 0.6375, valid accuracy: 0.7990\n",
      "Iter-63 train loss: 0.6792 valid loss: 0.6315, valid accuracy: 0.8016\n",
      "Iter-64 train loss: 0.9970 valid loss: 0.6251, valid accuracy: 0.8040\n",
      "Iter-65 train loss: 0.5541 valid loss: 0.6188, valid accuracy: 0.8076\n",
      "Iter-66 train loss: 0.5647 valid loss: 0.6122, valid accuracy: 0.8102\n",
      "Iter-67 train loss: 0.7935 valid loss: 0.6067, valid accuracy: 0.8102\n",
      "Iter-68 train loss: 1.1224 valid loss: 0.6016, valid accuracy: 0.8104\n",
      "Iter-69 train loss: 0.6084 valid loss: 0.5959, valid accuracy: 0.8120\n",
      "Iter-70 train loss: 0.6029 valid loss: 0.5914, valid accuracy: 0.8134\n",
      "Iter-71 train loss: 0.6789 valid loss: 0.5874, valid accuracy: 0.8156\n",
      "Iter-72 train loss: 0.8723 valid loss: 0.5842, valid accuracy: 0.8164\n",
      "Iter-73 train loss: 0.7186 valid loss: 0.5809, valid accuracy: 0.8184\n",
      "Iter-74 train loss: 0.5273 valid loss: 0.5785, valid accuracy: 0.8202\n",
      "Iter-75 train loss: 0.8973 valid loss: 0.5759, valid accuracy: 0.8210\n",
      "Iter-76 train loss: 0.5760 valid loss: 0.5718, valid accuracy: 0.8212\n",
      "Iter-77 train loss: 0.7303 valid loss: 0.5673, valid accuracy: 0.8232\n",
      "Iter-78 train loss: 0.6976 valid loss: 0.5625, valid accuracy: 0.8252\n",
      "Iter-79 train loss: 0.7944 valid loss: 0.5587, valid accuracy: 0.8268\n",
      "Iter-80 train loss: 0.6833 valid loss: 0.5555, valid accuracy: 0.8282\n",
      "Iter-81 train loss: 0.7007 valid loss: 0.5536, valid accuracy: 0.8294\n",
      "Iter-82 train loss: 0.5081 valid loss: 0.5518, valid accuracy: 0.8294\n",
      "Iter-83 train loss: 0.6409 valid loss: 0.5495, valid accuracy: 0.8300\n",
      "Iter-84 train loss: 0.4442 valid loss: 0.5470, valid accuracy: 0.8304\n",
      "Iter-85 train loss: 0.6615 valid loss: 0.5449, valid accuracy: 0.8304\n",
      "Iter-86 train loss: 0.6412 valid loss: 0.5411, valid accuracy: 0.8326\n",
      "Iter-87 train loss: 0.8327 valid loss: 0.5375, valid accuracy: 0.8332\n",
      "Iter-88 train loss: 0.4002 valid loss: 0.5343, valid accuracy: 0.8340\n",
      "Iter-89 train loss: 0.5643 valid loss: 0.5308, valid accuracy: 0.8340\n",
      "Iter-90 train loss: 0.4803 valid loss: 0.5276, valid accuracy: 0.8356\n",
      "Iter-91 train loss: 0.6509 valid loss: 0.5244, valid accuracy: 0.8352\n",
      "Iter-92 train loss: 0.5008 valid loss: 0.5215, valid accuracy: 0.8362\n",
      "Iter-93 train loss: 0.6604 valid loss: 0.5187, valid accuracy: 0.8368\n",
      "Iter-94 train loss: 0.5935 valid loss: 0.5156, valid accuracy: 0.8394\n",
      "Iter-95 train loss: 0.6497 valid loss: 0.5126, valid accuracy: 0.8414\n",
      "Iter-96 train loss: 0.5710 valid loss: 0.5097, valid accuracy: 0.8428\n",
      "Iter-97 train loss: 0.4417 valid loss: 0.5081, valid accuracy: 0.8434\n",
      "Iter-98 train loss: 0.6116 valid loss: 0.5064, valid accuracy: 0.8440\n",
      "Iter-99 train loss: 0.7307 valid loss: 0.5048, valid accuracy: 0.8442\n",
      "Iter-100 train loss: 0.6982 valid loss: 0.5037, valid accuracy: 0.8452\n",
      "Last iteration - Test accuracy mean: 0.8466, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10 # number of kernels/ filters in each layer\n",
    "num_input_units = D # noise added at the input lavel\n",
    "num_output_units = C # number of classes in this classification problem\n",
    "p_dropout = 0.95 #  keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "lam = 1e-3 # reg at the feedback or loss function or function loss level as noise or loss_reg added.\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = CNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers, lam=lam)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhb5A9EibcwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c495a5a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "# plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c495a54a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
