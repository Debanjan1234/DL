{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = l.conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = self.selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = self.alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            if train:\n",
    "                h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "                dy = self.alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            else:\n",
    "                h_conv_cache, h_nl_cache = h_cache[layer]\n",
    "            dh = self.selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = l.conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8 # constant\n",
    "        smooth_train = 1.0\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache, train=True)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 training loss: 12.6462 validation accuracy: 0.136000\n",
      "Iter-20 training loss: 11.3387 validation accuracy: 0.167000\n",
      "Iter-30 training loss: 9.6542 validation accuracy: 0.199000\n",
      "Iter-40 training loss: 8.0988 validation accuracy: 0.227000\n",
      "Iter-50 training loss: 7.3539 validation accuracy: 0.264400\n",
      "Iter-60 training loss: 7.4128 validation accuracy: 0.311200\n",
      "Iter-70 training loss: 6.4650 validation accuracy: 0.352600\n",
      "Iter-80 training loss: 4.6871 validation accuracy: 0.394000\n",
      "Iter-90 training loss: 4.7632 validation accuracy: 0.443200\n",
      "Iter-100 training loss: 3.4660 validation accuracy: 0.481600\n",
      "Iter-110 training loss: 4.0125 validation accuracy: 0.524600\n",
      "Iter-120 training loss: 4.2635 validation accuracy: 0.564200\n",
      "Iter-130 training loss: 2.3959 validation accuracy: 0.598600\n",
      "Iter-140 training loss: 3.6974 validation accuracy: 0.625200\n",
      "Iter-150 training loss: 2.6924 validation accuracy: 0.646000\n",
      "Iter-160 training loss: 2.6090 validation accuracy: 0.661600\n",
      "Iter-170 training loss: 2.3700 validation accuracy: 0.679000\n",
      "Iter-180 training loss: 2.4233 validation accuracy: 0.693800\n",
      "Iter-190 training loss: 2.4299 validation accuracy: 0.708400\n",
      "Iter-200 training loss: 2.0038 validation accuracy: 0.721000\n",
      "Iter-210 training loss: 2.3480 validation accuracy: 0.732600\n",
      "Iter-220 training loss: 1.8282 validation accuracy: 0.746000\n",
      "Iter-230 training loss: 2.0888 validation accuracy: 0.754600\n",
      "Iter-240 training loss: 1.5150 validation accuracy: 0.764200\n",
      "Iter-250 training loss: 1.1388 validation accuracy: 0.771200\n",
      "Iter-260 training loss: 1.4714 validation accuracy: 0.778400\n",
      "Iter-270 training loss: 0.7712 validation accuracy: 0.784800\n",
      "Iter-280 training loss: 1.4220 validation accuracy: 0.790400\n",
      "Iter-290 training loss: 1.7734 validation accuracy: 0.796200\n",
      "Iter-300 training loss: 1.6831 validation accuracy: 0.803800\n",
      "Iter-310 training loss: 1.6548 validation accuracy: 0.808400\n",
      "Iter-320 training loss: 2.7715 validation accuracy: 0.813200\n",
      "Iter-330 training loss: 2.2453 validation accuracy: 0.816000\n",
      "Iter-340 training loss: 2.3636 validation accuracy: 0.818000\n",
      "Iter-350 training loss: 1.2316 validation accuracy: 0.821200\n",
      "Iter-360 training loss: 1.4399 validation accuracy: 0.823400\n",
      "Iter-370 training loss: 2.3768 validation accuracy: 0.826000\n",
      "Iter-380 training loss: 1.7773 validation accuracy: 0.827600\n",
      "Iter-390 training loss: 1.9608 validation accuracy: 0.830200\n",
      "Iter-400 training loss: 0.9903 validation accuracy: 0.833400\n",
      "Iter-410 training loss: 0.7261 validation accuracy: 0.836800\n",
      "Iter-420 training loss: 1.0049 validation accuracy: 0.838400\n",
      "Iter-430 training loss: 1.2757 validation accuracy: 0.841600\n",
      "Iter-440 training loss: 0.8829 validation accuracy: 0.843800\n",
      "Iter-450 training loss: 1.4664 validation accuracy: 0.847400\n",
      "Iter-460 training loss: 1.7581 validation accuracy: 0.849800\n",
      "Iter-470 training loss: 0.6437 validation accuracy: 0.850600\n",
      "Iter-480 training loss: 1.0539 validation accuracy: 0.852800\n",
      "Iter-490 training loss: 1.3413 validation accuracy: 0.853400\n",
      "Iter-500 training loss: 1.0763 validation accuracy: 0.853400\n",
      "Iter-510 training loss: 0.7988 validation accuracy: 0.855800\n",
      "Iter-520 training loss: 1.2589 validation accuracy: 0.855200\n",
      "Iter-530 training loss: 0.6191 validation accuracy: 0.856400\n",
      "Iter-540 training loss: 1.1745 validation accuracy: 0.857200\n",
      "Iter-550 training loss: 1.5523 validation accuracy: 0.859000\n",
      "Iter-560 training loss: 0.9075 validation accuracy: 0.860800\n",
      "Iter-570 training loss: 0.9166 validation accuracy: 0.860800\n",
      "Iter-580 training loss: 0.6044 validation accuracy: 0.861200\n",
      "Iter-590 training loss: 1.0109 validation accuracy: 0.863400\n",
      "Iter-600 training loss: 0.9748 validation accuracy: 0.864200\n",
      "Iter-610 training loss: 0.8254 validation accuracy: 0.866800\n",
      "Iter-620 training loss: 0.7256 validation accuracy: 0.868400\n",
      "Iter-630 training loss: 0.8757 validation accuracy: 0.869200\n",
      "Iter-640 training loss: 1.4498 validation accuracy: 0.870000\n",
      "Iter-650 training loss: 0.9464 validation accuracy: 0.870200\n",
      "Iter-660 training loss: 0.6094 validation accuracy: 0.871800\n",
      "Iter-670 training loss: 1.0915 validation accuracy: 0.872400\n",
      "Iter-680 training loss: 1.4772 validation accuracy: 0.875800\n",
      "Iter-690 training loss: 0.3452 validation accuracy: 0.875200\n",
      "Iter-700 training loss: 1.0850 validation accuracy: 0.873600\n",
      "Iter-710 training loss: 0.5366 validation accuracy: 0.874200\n",
      "Iter-720 training loss: 1.0252 validation accuracy: 0.874200\n",
      "Iter-730 training loss: 1.5611 validation accuracy: 0.876400\n",
      "Iter-740 training loss: 0.9676 validation accuracy: 0.878400\n",
      "Iter-750 training loss: 1.1123 validation accuracy: 0.880000\n",
      "Iter-760 training loss: 0.9186 validation accuracy: 0.880600\n",
      "Iter-770 training loss: 0.9642 validation accuracy: 0.883000\n",
      "Iter-780 training loss: 0.4784 validation accuracy: 0.882600\n",
      "Iter-790 training loss: 0.5266 validation accuracy: 0.883200\n",
      "Iter-800 training loss: 0.4429 validation accuracy: 0.884000\n",
      "Iter-810 training loss: 0.7795 validation accuracy: 0.883800\n",
      "Iter-820 training loss: 1.0157 validation accuracy: 0.883800\n",
      "Iter-830 training loss: 0.6148 validation accuracy: 0.885800\n",
      "Iter-840 training loss: 0.9064 validation accuracy: 0.886400\n",
      "Iter-850 training loss: 0.6659 validation accuracy: 0.887200\n",
      "Iter-860 training loss: 1.5490 validation accuracy: 0.887000\n",
      "Iter-870 training loss: 0.8516 validation accuracy: 0.886200\n",
      "Iter-880 training loss: 0.7427 validation accuracy: 0.886400\n",
      "Iter-890 training loss: 1.1720 validation accuracy: 0.885600\n",
      "Iter-900 training loss: 1.1140 validation accuracy: 0.886000\n",
      "Iter-910 training loss: 0.6354 validation accuracy: 0.886600\n",
      "Iter-920 training loss: 1.0056 validation accuracy: 0.888600\n",
      "Iter-930 training loss: 0.8705 validation accuracy: 0.889400\n",
      "Iter-940 training loss: 0.3787 validation accuracy: 0.890200\n",
      "Iter-950 training loss: 1.1809 validation accuracy: 0.888400\n",
      "Iter-960 training loss: 1.2313 validation accuracy: 0.890000\n",
      "Iter-970 training loss: 0.2126 validation accuracy: 0.889400\n",
      "Iter-980 training loss: 0.6729 validation accuracy: 0.889600\n",
      "Iter-990 training loss: 0.9441 validation accuracy: 0.888800\n",
      "Iter-1000 training loss: 0.8643 validation accuracy: 0.890000\n",
      "Test Mean accuracy: 0.8869, std: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12210eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 10 # depth \n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 2\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = nn.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
