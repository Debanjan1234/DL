{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = l.conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = self.selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = self.alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            if train:\n",
    "                h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "                dy = self.alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            else:\n",
    "                h_conv_cache, h_nl_cache = h_cache[layer]\n",
    "            dh = self.selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = l.conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8 # constant\n",
    "        smooth_train = 1.0\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache, train=True)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 training loss: 2.2544 validation accuracy: 0.363200\n",
      "Iter-20 training loss: 1.5534 validation accuracy: 0.590200\n",
      "Iter-30 training loss: 0.9062 validation accuracy: 0.673600\n",
      "Iter-40 training loss: 0.6448 validation accuracy: 0.762200\n",
      "Iter-50 training loss: 0.7740 validation accuracy: 0.782600\n",
      "Iter-60 training loss: 0.4196 validation accuracy: 0.813200\n",
      "Iter-70 training loss: 0.6650 validation accuracy: 0.833800\n",
      "Iter-80 training loss: 0.2239 validation accuracy: 0.839400\n",
      "Iter-90 training loss: 0.7938 validation accuracy: 0.843400\n",
      "Iter-100 training loss: 0.6154 validation accuracy: 0.857600\n",
      "Iter-110 training loss: 0.4978 validation accuracy: 0.869200\n",
      "Iter-120 training loss: 0.5324 validation accuracy: 0.868800\n",
      "Iter-130 training loss: 0.6212 validation accuracy: 0.867400\n",
      "Iter-140 training loss: 0.6551 validation accuracy: 0.873200\n",
      "Iter-150 training loss: 0.3482 validation accuracy: 0.881400\n",
      "Iter-160 training loss: 0.8693 validation accuracy: 0.881000\n",
      "Iter-170 training loss: 0.3667 validation accuracy: 0.881200\n",
      "Iter-180 training loss: 0.4371 validation accuracy: 0.885200\n",
      "Iter-190 training loss: 0.2209 validation accuracy: 0.889200\n",
      "Iter-200 training loss: 0.3517 validation accuracy: 0.893000\n",
      "Iter-210 training loss: 0.6655 validation accuracy: 0.891600\n",
      "Iter-220 training loss: 0.6899 validation accuracy: 0.894400\n",
      "Iter-230 training loss: 0.2565 validation accuracy: 0.899400\n",
      "Iter-240 training loss: 0.8181 validation accuracy: 0.901800\n",
      "Iter-250 training loss: 0.2292 validation accuracy: 0.902400\n",
      "Iter-260 training loss: 0.3714 validation accuracy: 0.900200\n",
      "Iter-270 training loss: 0.4091 validation accuracy: 0.897600\n",
      "Iter-280 training loss: 0.4898 validation accuracy: 0.897800\n",
      "Iter-290 training loss: 0.2980 validation accuracy: 0.898800\n",
      "Iter-300 training loss: 0.3173 validation accuracy: 0.900600\n",
      "Iter-310 training loss: 0.3064 validation accuracy: 0.905800\n",
      "Iter-320 training loss: 0.3807 validation accuracy: 0.906000\n",
      "Iter-330 training loss: 0.5175 validation accuracy: 0.904200\n",
      "Iter-340 training loss: 0.3086 validation accuracy: 0.901800\n",
      "Iter-350 training loss: 0.5507 validation accuracy: 0.904200\n",
      "Iter-360 training loss: 0.3879 validation accuracy: 0.906400\n",
      "Iter-370 training loss: 0.2092 validation accuracy: 0.906600\n",
      "Iter-380 training loss: 0.2667 validation accuracy: 0.906800\n",
      "Iter-390 training loss: 0.2961 validation accuracy: 0.909000\n",
      "Iter-400 training loss: 0.3256 validation accuracy: 0.910200\n",
      "Iter-410 training loss: 0.4112 validation accuracy: 0.908200\n",
      "Iter-420 training loss: 0.2897 validation accuracy: 0.908600\n",
      "Iter-430 training loss: 0.6235 validation accuracy: 0.908400\n",
      "Iter-440 training loss: 0.3413 validation accuracy: 0.910600\n",
      "Iter-450 training loss: 0.2765 validation accuracy: 0.915400\n",
      "Iter-460 training loss: 0.4621 validation accuracy: 0.915200\n",
      "Iter-470 training loss: 0.4765 validation accuracy: 0.913400\n",
      "Iter-480 training loss: 0.4898 validation accuracy: 0.915600\n",
      "Iter-490 training loss: 0.5828 validation accuracy: 0.918400\n",
      "Iter-500 training loss: 0.2503 validation accuracy: 0.918200\n",
      "Iter-510 training loss: 0.4186 validation accuracy: 0.917800\n",
      "Iter-520 training loss: 0.3130 validation accuracy: 0.919200\n",
      "Iter-530 training loss: 0.3283 validation accuracy: 0.920200\n",
      "Iter-540 training loss: 0.5273 validation accuracy: 0.923400\n",
      "Iter-550 training loss: 0.2166 validation accuracy: 0.924400\n",
      "Iter-560 training loss: 0.5744 validation accuracy: 0.923000\n",
      "Iter-570 training loss: 0.1827 validation accuracy: 0.922200\n",
      "Iter-580 training loss: 0.2131 validation accuracy: 0.921800\n",
      "Iter-590 training loss: 0.2748 validation accuracy: 0.922400\n",
      "Iter-600 training loss: 0.2704 validation accuracy: 0.923200\n",
      "Iter-610 training loss: 0.4228 validation accuracy: 0.923000\n",
      "Iter-620 training loss: 0.2594 validation accuracy: 0.924600\n",
      "Iter-630 training loss: 0.2491 validation accuracy: 0.924400\n",
      "Iter-640 training loss: 0.1812 validation accuracy: 0.926200\n",
      "Iter-650 training loss: 0.2091 validation accuracy: 0.923800\n",
      "Iter-660 training loss: 0.3953 validation accuracy: 0.923000\n",
      "Iter-670 training loss: 0.3770 validation accuracy: 0.925000\n",
      "Iter-680 training loss: 0.2140 validation accuracy: 0.925800\n",
      "Iter-690 training loss: 0.3306 validation accuracy: 0.926800\n",
      "Iter-700 training loss: 0.6033 validation accuracy: 0.926000\n",
      "Iter-710 training loss: 0.4805 validation accuracy: 0.926800\n",
      "Iter-720 training loss: 0.1357 validation accuracy: 0.927800\n",
      "Iter-730 training loss: 0.4547 validation accuracy: 0.931800\n",
      "Iter-740 training loss: 0.1910 validation accuracy: 0.932000\n",
      "Iter-750 training loss: 0.4203 validation accuracy: 0.929400\n",
      "Iter-760 training loss: 0.5227 validation accuracy: 0.928800\n",
      "Iter-770 training loss: 0.2115 validation accuracy: 0.928400\n",
      "Iter-780 training loss: 0.1321 validation accuracy: 0.927200\n",
      "Iter-790 training loss: 0.2832 validation accuracy: 0.927000\n",
      "Iter-800 training loss: 0.4172 validation accuracy: 0.927000\n",
      "Iter-810 training loss: 0.2015 validation accuracy: 0.928000\n",
      "Iter-820 training loss: 0.0491 validation accuracy: 0.926000\n",
      "Iter-830 training loss: 0.1341 validation accuracy: 0.924200\n",
      "Iter-840 training loss: 0.2649 validation accuracy: 0.927200\n",
      "Iter-850 training loss: 0.2207 validation accuracy: 0.929200\n",
      "Iter-860 training loss: 0.0912 validation accuracy: 0.929200\n",
      "Iter-870 training loss: 0.4336 validation accuracy: 0.927800\n",
      "Iter-880 training loss: 0.3368 validation accuracy: 0.927400\n",
      "Iter-890 training loss: 0.0832 validation accuracy: 0.926800\n",
      "Iter-900 training loss: 0.2159 validation accuracy: 0.928800\n",
      "Iter-910 training loss: 0.4332 validation accuracy: 0.931400\n",
      "Iter-920 training loss: 0.2100 validation accuracy: 0.931800\n",
      "Iter-930 training loss: 0.2034 validation accuracy: 0.934000\n",
      "Iter-940 training loss: 0.3336 validation accuracy: 0.933600\n",
      "Iter-950 training loss: 0.3598 validation accuracy: 0.934000\n",
      "Iter-960 training loss: 0.4954 validation accuracy: 0.933200\n",
      "Iter-970 training loss: 0.2036 validation accuracy: 0.933600\n",
      "Iter-980 training loss: 0.2561 validation accuracy: 0.934600\n",
      "Iter-990 training loss: 0.1931 validation accuracy: 0.934200\n",
      "Iter-1000 training loss: 0.3180 validation accuracy: 0.934200\n",
      "Test Mean accuracy: 0.9319, std: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0SeEkBKCok8IISXE/wdqNjlHVM2zggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121f0b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 10 # depth \n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = nn.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
