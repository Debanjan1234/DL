{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # row or hight\n",
    "# fh = 5 \n",
    "# fw = 4 \n",
    "# C = 3\n",
    "# i0 = np.arange(fh) # field hight\n",
    "# i0, i0.shape\n",
    "# i0 = np.repeat(i0, fw) # field width\n",
    "# i0, i0.shape\n",
    "# i0 = np.tile(i0, C) # C\n",
    "# i0.shape, i0\n",
    "# oh = 1\n",
    "# ow = 2\n",
    "# i1 = np.repeat(np.arange(oh), ow)\n",
    "# i1, i1.shape\n",
    "# i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "# i.shape, i1.shape, i0.shape, i1, i0, i[-4:], \n",
    "# # i.astype(int)\n",
    "# i.T.shape, i.T\n",
    "\n",
    "# # for cols or width\n",
    "# j0 = np.tile(np.arange(fw), fh * C)\n",
    "# j1 = np.tile(np.arange(ow), oh)\n",
    "# j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "# j0.shape, j1.shape, j.shape, j0, j1, j[:10]\n",
    "# j.T.shape, j.T\n",
    "\n",
    "# k = np.tile(np.arange(C), fh * fw).reshape(-1, 1).T\n",
    "# k.shape, k\n",
    "\n",
    "# X = np.random.rand(1, 2, 3, 4)\n",
    "# X.shape, X\n",
    "# np.pad(X, ((0, 0), (0, 0), (2, 2), (0, 0)), mode='constant')\n",
    "# # np.pad(X, ((0, 0), (0, 0)), mode='constant') # zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution impl\n",
    "# from impl.im2col import *\n",
    "# or\n",
    "# import impl.im2col as im2col\n",
    "# out_height = int(((H + (2 * pad) - kernel_height) / stride) + 1), \n",
    "# stride == 1, ALWAYS\n",
    "# pad == kernel//2, ALWAYS\n",
    "# kernel == min size ALWAYS, i.e. one past, one pres, one post (if exist), i.e. three or two\n",
    "# kernel == 3 or 2 ALWAYS\n",
    "# input=X, kernel=3or2, padding=kernel//2, stride=1, output=y\n",
    "def get_im2col_indices(X_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    # Input shape\n",
    "    N, C, H, W = X_shape\n",
    "    \n",
    "    # Kernel shape\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C\n",
    "    \n",
    "    # Output shape\n",
    "    assert (H + (2 * padding) - field_height) % stride == 0\n",
    "    assert (W + (2 * padding) - field_width) % stride == 0\n",
    "    out_height = int(((H + (2 * padding) - field_height) / stride) + 1)\n",
    "    out_width = int(((W + (2 * padding) - field_width) / stride) + 1)\n",
    "    out_C = 1 # the output channel/ depth\n",
    "\n",
    "    # Row, Height, i\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, field_C)\n",
    "    i1 = np.repeat(np.arange(out_height), out_width)\n",
    "    i1 = np.tile(i1, out_C)\n",
    "    i1 *= stride\n",
    "    \n",
    "    # Column, Width, j\n",
    "    j0 = np.tile(np.arange(field_width), field_height * field_C)\n",
    "    j1 = np.tile(np.arange(out_width), out_height * out_C)\n",
    "    j1 *= stride\n",
    "    \n",
    "    # Channel, Depth, K\n",
    "    k0 = np.repeat(np.arange(field_C), field_height * field_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 = np.repeat(np.arange(out_C), out_height * out_width) #.reshape(-1, 1) # out_C = 1\n",
    "    k1 *= stride\n",
    "    \n",
    "    # Indices: i, j, k index\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "    k = k0.reshape(-1, 1) + k1.reshape(1, -1)\n",
    "    \n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(X, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    X_padded = np.pad(X, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant') # X_NxCxHxW\n",
    "\n",
    "    k, i, j = get_im2col_indices(X.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    X_col = X_padded[:, k, i, j] # X_col_txkxn\n",
    "    \n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "    \n",
    "    X_col = X_col.transpose(1, 2, 0).reshape(kernel_size, -1)\n",
    "    \n",
    "    return X_col\n",
    "\n",
    "def col2im_indices(X_col, X_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = X_shape\n",
    "    H_padded, W_padded = H + (2 * padding), W + (2 * padding)\n",
    "    X_padded = np.zeros((N, C, H_padded, W_padded), dtype=X_col.dtype)\n",
    "    \n",
    "    k, i, j = get_im2col_indices(X_shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    # field_height, field_width = kernel_shape\n",
    "    field_C = C # x.shape[1]\n",
    "    kernel_size = field_C * field_height * field_width\n",
    "\n",
    "    X_col = X_col.reshape(kernel_size, -1, N).transpose(2, 0, 1) # N, K, H * W\n",
    "    np.add.at(X_padded, (slice(None), k, i, j), X_col) # slice(None)== ':'\n",
    "    \n",
    "    return X_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    \n",
    "    # Input X\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    \n",
    "    # Kernel W\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "    \n",
    "    # Output\n",
    "    h_out = ((h_x + (2 * padding) - h_filter) / stride) + 1\n",
    "    w_out = ((w_x + (2 * padding) - w_filter) / stride) + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filter, -1)\n",
    "\n",
    "    out = (W_col @ X_col) + b\n",
    "    out = out.reshape(n_filter, h_out, w_out, n_x).transpose(3, 0, 1, 2)\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W = W.reshape(n_filter, -1)\n",
    "    dX_col = W.T @ dout\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob in this case. \n",
    "# Is this true in other cases as well? Yes.\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.arange(24).reshape(2,3,4)\n",
    "# a\n",
    "# # np.add.at(a, (slice(None), 0, 1, 2), cols_reshaped) # slice(None)== ':'\n",
    "# # a[:, 0, 1]\n",
    "# # a[0,:,1] \n",
    "# # # or \n",
    "# # a[0,slice(None),1],\n",
    "# # # outputs array([1, 5, 9])\n",
    "\n",
    "# # a[0,None,1] \n",
    "# # a[0, 1]\n",
    "# # # gives array([[4, 5, 6, 7]])\n",
    "\n",
    "# # # Could sb explain the latter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "\n",
    "# # Becoming more familiar with y_train data structure, abstraction, shape, and type\n",
    "# y_train.max(), y_train\n",
    "# data = y_train\n",
    "# data.shape, data[0]\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "# X_train[0, :10, :10, :10]\n",
    "M, D, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout, lam):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lam = lam\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = selu_dropout_forward(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy_reg(self.model[2], y, y_train, lam=self.lam)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = selu_dropout_backward(dout=dy, cache=h_do_cache)\n",
    "            dh = selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = conv_backward(dout=dh, cache=X_conv_cache)\n",
    "        # dX: TODO: hast not been used but this basically should be 0 \n",
    "        # which means input can be perfectly recontructed!\n",
    "        # dX is the grad_input or delta for input or the calculated error or difference or delta\n",
    "        # Can be used as noise which is because it is unwanted and can be added to data to be calculated again\n",
    "        # when the data is not abundantly available!\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "        # else: # self.mode == 'regression'\n",
    "        # return np.round(y_logit)\n",
    "        # y_prob for accuracy & y_logit for loss\n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        # if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Train the model\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Test the updated model to validate the model\n",
    "            # Avoid overfitting/ memorizing and underfitting lack of model capacity\n",
    "            # if val_set:\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            # def accuracy(y_true, y_pred):\n",
    "            # return np.mean(y_pred == y_true)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "                \n",
    "        # Test the model after training and validation after all the epochs\n",
    "        # The test data has NOT been used before and has NOT been seen by the model\n",
    "        # # Kernel dead problem sometimes!\n",
    "        y_pred, _ = nn.test(X_test)\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}'.format(acc.mean(), acc.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 train loss: 4.3722 valid loss: 4.3737, valid accuracy: 0.1136\n",
      "Iter-2 train loss: 4.0819 valid loss: 4.0278, valid accuracy: 0.1318\n",
      "Iter-3 train loss: 3.0140 valid loss: 3.7176, valid accuracy: 0.1456\n",
      "Iter-4 train loss: 3.5745 valid loss: 3.4768, valid accuracy: 0.1642\n",
      "Iter-5 train loss: 3.6142 valid loss: 3.2792, valid accuracy: 0.1876\n",
      "Iter-6 train loss: 3.5878 valid loss: 3.0841, valid accuracy: 0.2048\n",
      "Iter-7 train loss: 3.1232 valid loss: 2.9012, valid accuracy: 0.2266\n",
      "Iter-8 train loss: 2.6499 valid loss: 2.7489, valid accuracy: 0.2524\n",
      "Iter-9 train loss: 2.7292 valid loss: 2.6181, valid accuracy: 0.2750\n",
      "Iter-10 train loss: 2.6014 valid loss: 2.4987, valid accuracy: 0.2978\n",
      "Iter-11 train loss: 2.4616 valid loss: 2.3876, valid accuracy: 0.3222\n",
      "Iter-12 train loss: 2.0816 valid loss: 2.2785, valid accuracy: 0.3484\n",
      "Iter-13 train loss: 2.2667 valid loss: 2.1709, valid accuracy: 0.3698\n",
      "Iter-14 train loss: 2.1860 valid loss: 2.0750, valid accuracy: 0.3930\n",
      "Iter-15 train loss: 2.1846 valid loss: 1.9792, valid accuracy: 0.4114\n",
      "Iter-16 train loss: 1.9619 valid loss: 1.8817, valid accuracy: 0.4360\n",
      "Iter-17 train loss: 1.8460 valid loss: 1.7871, valid accuracy: 0.4568\n",
      "Iter-18 train loss: 1.9393 valid loss: 1.7027, valid accuracy: 0.4800\n",
      "Iter-19 train loss: 1.6424 valid loss: 1.6261, valid accuracy: 0.5024\n",
      "Iter-20 train loss: 1.8101 valid loss: 1.5573, valid accuracy: 0.5210\n",
      "Iter-21 train loss: 1.6714 valid loss: 1.4926, valid accuracy: 0.5378\n",
      "Iter-22 train loss: 1.5243 valid loss: 1.4347, valid accuracy: 0.5532\n",
      "Iter-23 train loss: 1.3935 valid loss: 1.3823, valid accuracy: 0.5692\n",
      "Iter-24 train loss: 1.8068 valid loss: 1.3348, valid accuracy: 0.5836\n",
      "Iter-25 train loss: 1.5279 valid loss: 1.2928, valid accuracy: 0.5922\n",
      "Iter-26 train loss: 1.1559 valid loss: 1.2544, valid accuracy: 0.6024\n",
      "Iter-27 train loss: 1.6031 valid loss: 1.2183, valid accuracy: 0.6122\n",
      "Iter-28 train loss: 1.1814 valid loss: 1.1819, valid accuracy: 0.6254\n",
      "Iter-29 train loss: 1.0455 valid loss: 1.1487, valid accuracy: 0.6346\n",
      "Iter-30 train loss: 1.2734 valid loss: 1.1183, valid accuracy: 0.6424\n",
      "Iter-31 train loss: 1.1846 valid loss: 1.0913, valid accuracy: 0.6544\n",
      "Iter-32 train loss: 1.0145 valid loss: 1.0673, valid accuracy: 0.6620\n",
      "Iter-33 train loss: 1.5660 valid loss: 1.0475, valid accuracy: 0.6674\n",
      "Iter-34 train loss: 0.6465 valid loss: 1.0290, valid accuracy: 0.6732\n",
      "Iter-35 train loss: 1.1327 valid loss: 1.0130, valid accuracy: 0.6772\n",
      "Iter-36 train loss: 1.3055 valid loss: 0.9976, valid accuracy: 0.6832\n",
      "Iter-37 train loss: 1.0750 valid loss: 0.9795, valid accuracy: 0.6902\n",
      "Iter-38 train loss: 1.2472 valid loss: 0.9607, valid accuracy: 0.6960\n",
      "Iter-39 train loss: 1.0209 valid loss: 0.9400, valid accuracy: 0.7036\n",
      "Iter-40 train loss: 0.8969 valid loss: 0.9176, valid accuracy: 0.7102\n",
      "Iter-41 train loss: 0.9124 valid loss: 0.8958, valid accuracy: 0.7176\n",
      "Iter-42 train loss: 1.2762 valid loss: 0.8764, valid accuracy: 0.7232\n",
      "Iter-43 train loss: 0.9101 valid loss: 0.8567, valid accuracy: 0.7294\n",
      "Iter-44 train loss: 0.9533 valid loss: 0.8378, valid accuracy: 0.7388\n",
      "Iter-45 train loss: 0.9512 valid loss: 0.8218, valid accuracy: 0.7414\n",
      "Iter-46 train loss: 0.8591 valid loss: 0.8092, valid accuracy: 0.7430\n",
      "Iter-47 train loss: 0.7608 valid loss: 0.7992, valid accuracy: 0.7450\n",
      "Iter-48 train loss: 1.1574 valid loss: 0.7911, valid accuracy: 0.7496\n",
      "Iter-49 train loss: 1.4410 valid loss: 0.7843, valid accuracy: 0.7524\n",
      "Iter-50 train loss: 0.4908 valid loss: 0.7766, valid accuracy: 0.7544\n",
      "Iter-51 train loss: 0.7857 valid loss: 0.7699, valid accuracy: 0.7574\n",
      "Iter-52 train loss: 0.7352 valid loss: 0.7613, valid accuracy: 0.7608\n",
      "Iter-53 train loss: 0.8806 valid loss: 0.7517, valid accuracy: 0.7642\n",
      "Iter-54 train loss: 0.7520 valid loss: 0.7417, valid accuracy: 0.7678\n",
      "Iter-55 train loss: 0.7960 valid loss: 0.7319, valid accuracy: 0.7718\n",
      "Iter-56 train loss: 0.9261 valid loss: 0.7220, valid accuracy: 0.7730\n",
      "Iter-57 train loss: 0.8286 valid loss: 0.7122, valid accuracy: 0.7756\n",
      "Iter-58 train loss: 0.7762 valid loss: 0.7044, valid accuracy: 0.7778\n",
      "Iter-59 train loss: 0.6813 valid loss: 0.6969, valid accuracy: 0.7822\n",
      "Iter-60 train loss: 1.1034 valid loss: 0.6887, valid accuracy: 0.7840\n",
      "Iter-61 train loss: 0.6982 valid loss: 0.6807, valid accuracy: 0.7878\n",
      "Iter-62 train loss: 0.5825 valid loss: 0.6736, valid accuracy: 0.7912\n",
      "Iter-63 train loss: 0.6909 valid loss: 0.6678, valid accuracy: 0.7956\n",
      "Iter-64 train loss: 0.6563 valid loss: 0.6633, valid accuracy: 0.7962\n",
      "Iter-65 train loss: 0.9107 valid loss: 0.6590, valid accuracy: 0.7978\n",
      "Iter-66 train loss: 0.9449 valid loss: 0.6537, valid accuracy: 0.8000\n",
      "Iter-67 train loss: 0.6985 valid loss: 0.6500, valid accuracy: 0.8026\n",
      "Iter-68 train loss: 0.7506 valid loss: 0.6477, valid accuracy: 0.8024\n",
      "Iter-69 train loss: 0.5641 valid loss: 0.6456, valid accuracy: 0.8040\n",
      "Iter-70 train loss: 0.8442 valid loss: 0.6424, valid accuracy: 0.8046\n",
      "Iter-71 train loss: 0.8600 valid loss: 0.6404, valid accuracy: 0.8068\n",
      "Iter-72 train loss: 0.7772 valid loss: 0.6353, valid accuracy: 0.8078\n",
      "Iter-73 train loss: 0.6672 valid loss: 0.6288, valid accuracy: 0.8098\n",
      "Iter-74 train loss: 0.4806 valid loss: 0.6202, valid accuracy: 0.8122\n",
      "Iter-75 train loss: 0.6051 valid loss: 0.6140, valid accuracy: 0.8162\n",
      "Iter-76 train loss: 0.8286 valid loss: 0.6079, valid accuracy: 0.8208\n",
      "Iter-77 train loss: 0.5903 valid loss: 0.6018, valid accuracy: 0.8242\n",
      "Iter-78 train loss: 0.7623 valid loss: 0.5963, valid accuracy: 0.8254\n",
      "Iter-79 train loss: 0.5282 valid loss: 0.5912, valid accuracy: 0.8284\n",
      "Iter-80 train loss: 0.7921 valid loss: 0.5861, valid accuracy: 0.8314\n",
      "Iter-81 train loss: 0.6200 valid loss: 0.5807, valid accuracy: 0.8330\n",
      "Iter-82 train loss: 0.8827 valid loss: 0.5747, valid accuracy: 0.8342\n",
      "Iter-83 train loss: 0.7771 valid loss: 0.5686, valid accuracy: 0.8364\n",
      "Iter-84 train loss: 0.8270 valid loss: 0.5631, valid accuracy: 0.8374\n",
      "Iter-85 train loss: 1.0773 valid loss: 0.5580, valid accuracy: 0.8374\n",
      "Iter-86 train loss: 0.4962 valid loss: 0.5534, valid accuracy: 0.8376\n",
      "Iter-87 train loss: 0.5534 valid loss: 0.5501, valid accuracy: 0.8374\n",
      "Iter-88 train loss: 0.8297 valid loss: 0.5479, valid accuracy: 0.8384\n",
      "Iter-89 train loss: 0.4771 valid loss: 0.5465, valid accuracy: 0.8384\n",
      "Iter-90 train loss: 0.4461 valid loss: 0.5454, valid accuracy: 0.8390\n",
      "Iter-91 train loss: 0.4620 valid loss: 0.5444, valid accuracy: 0.8396\n",
      "Iter-92 train loss: 0.6384 valid loss: 0.5427, valid accuracy: 0.8398\n",
      "Iter-93 train loss: 0.5595 valid loss: 0.5408, valid accuracy: 0.8404\n",
      "Iter-94 train loss: 0.7835 valid loss: 0.5390, valid accuracy: 0.8412\n",
      "Iter-95 train loss: 0.5631 valid loss: 0.5377, valid accuracy: 0.8410\n",
      "Iter-96 train loss: 0.8351 valid loss: 0.5359, valid accuracy: 0.8424\n",
      "Iter-97 train loss: 0.8198 valid loss: 0.5322, valid accuracy: 0.8438\n",
      "Iter-98 train loss: 0.5940 valid loss: 0.5270, valid accuracy: 0.8436\n",
      "Iter-99 train loss: 0.6823 valid loss: 0.5220, valid accuracy: 0.8460\n",
      "Iter-100 train loss: 0.5680 valid loss: 0.5184, valid accuracy: 0.8472\n",
      "Iter-101 train loss: 0.5367 valid loss: 0.5148, valid accuracy: 0.8482\n",
      "Iter-102 train loss: 0.4219 valid loss: 0.5115, valid accuracy: 0.8488\n",
      "Iter-103 train loss: 0.6039 valid loss: 0.5086, valid accuracy: 0.8512\n",
      "Iter-104 train loss: 0.6444 valid loss: 0.5062, valid accuracy: 0.8526\n",
      "Iter-105 train loss: 0.4976 valid loss: 0.5048, valid accuracy: 0.8534\n",
      "Iter-106 train loss: 0.6711 valid loss: 0.5051, valid accuracy: 0.8518\n",
      "Iter-107 train loss: 0.5183 valid loss: 0.5058, valid accuracy: 0.8518\n",
      "Iter-108 train loss: 0.6296 valid loss: 0.5049, valid accuracy: 0.8526\n",
      "Iter-109 train loss: 0.6378 valid loss: 0.5023, valid accuracy: 0.8554\n",
      "Iter-110 train loss: 0.6029 valid loss: 0.4991, valid accuracy: 0.8548\n",
      "Iter-111 train loss: 0.6038 valid loss: 0.4970, valid accuracy: 0.8540\n",
      "Iter-112 train loss: 0.5264 valid loss: 0.4941, valid accuracy: 0.8548\n",
      "Iter-113 train loss: 0.7265 valid loss: 0.4904, valid accuracy: 0.8540\n",
      "Iter-114 train loss: 0.5950 valid loss: 0.4872, valid accuracy: 0.8556\n",
      "Iter-115 train loss: 0.4393 valid loss: 0.4846, valid accuracy: 0.8582\n",
      "Iter-116 train loss: 0.4846 valid loss: 0.4837, valid accuracy: 0.8594\n",
      "Iter-117 train loss: 0.6111 valid loss: 0.4834, valid accuracy: 0.8590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-118 train loss: 0.5048 valid loss: 0.4837, valid accuracy: 0.8570\n",
      "Iter-119 train loss: 0.3744 valid loss: 0.4834, valid accuracy: 0.8564\n",
      "Iter-120 train loss: 0.3735 valid loss: 0.4828, valid accuracy: 0.8554\n",
      "Iter-121 train loss: 0.8584 valid loss: 0.4799, valid accuracy: 0.8576\n",
      "Iter-122 train loss: 0.5898 valid loss: 0.4765, valid accuracy: 0.8600\n",
      "Iter-123 train loss: 0.6781 valid loss: 0.4719, valid accuracy: 0.8606\n",
      "Iter-124 train loss: 0.7226 valid loss: 0.4672, valid accuracy: 0.8630\n",
      "Iter-125 train loss: 0.4506 valid loss: 0.4626, valid accuracy: 0.8660\n",
      "Iter-126 train loss: 0.5220 valid loss: 0.4584, valid accuracy: 0.8668\n",
      "Iter-127 train loss: 0.4210 valid loss: 0.4554, valid accuracy: 0.8678\n",
      "Iter-128 train loss: 0.7355 valid loss: 0.4540, valid accuracy: 0.8688\n",
      "Iter-129 train loss: 0.5602 valid loss: 0.4537, valid accuracy: 0.8676\n",
      "Iter-130 train loss: 0.7328 valid loss: 0.4549, valid accuracy: 0.8686\n",
      "Iter-131 train loss: 0.4378 valid loss: 0.4558, valid accuracy: 0.8678\n",
      "Iter-132 train loss: 0.6228 valid loss: 0.4570, valid accuracy: 0.8664\n",
      "Iter-133 train loss: 0.5837 valid loss: 0.4578, valid accuracy: 0.8668\n",
      "Iter-134 train loss: 0.5299 valid loss: 0.4579, valid accuracy: 0.8670\n",
      "Iter-135 train loss: 0.7367 valid loss: 0.4573, valid accuracy: 0.8658\n",
      "Iter-136 train loss: 0.7291 valid loss: 0.4568, valid accuracy: 0.8654\n",
      "Iter-137 train loss: 0.6327 valid loss: 0.4535, valid accuracy: 0.8648\n",
      "Iter-138 train loss: 0.5470 valid loss: 0.4501, valid accuracy: 0.8682\n",
      "Iter-139 train loss: 0.3862 valid loss: 0.4470, valid accuracy: 0.8676\n",
      "Iter-140 train loss: 0.4284 valid loss: 0.4447, valid accuracy: 0.8682\n",
      "Iter-141 train loss: 0.6700 valid loss: 0.4431, valid accuracy: 0.8704\n",
      "Iter-142 train loss: 0.5363 valid loss: 0.4413, valid accuracy: 0.8710\n",
      "Iter-143 train loss: 0.4699 valid loss: 0.4401, valid accuracy: 0.8716\n",
      "Iter-144 train loss: 0.6777 valid loss: 0.4396, valid accuracy: 0.8746\n",
      "Iter-145 train loss: 0.5380 valid loss: 0.4383, valid accuracy: 0.8734\n",
      "Iter-146 train loss: 0.6149 valid loss: 0.4382, valid accuracy: 0.8730\n",
      "Iter-147 train loss: 0.6386 valid loss: 0.4386, valid accuracy: 0.8722\n",
      "Iter-148 train loss: 0.8158 valid loss: 0.4394, valid accuracy: 0.8710\n",
      "Iter-149 train loss: 0.4742 valid loss: 0.4375, valid accuracy: 0.8720\n",
      "Iter-150 train loss: 0.6250 valid loss: 0.4342, valid accuracy: 0.8738\n",
      "Iter-151 train loss: 0.5103 valid loss: 0.4301, valid accuracy: 0.8748\n",
      "Iter-152 train loss: 0.5813 valid loss: 0.4262, valid accuracy: 0.8768\n",
      "Iter-153 train loss: 0.3850 valid loss: 0.4234, valid accuracy: 0.8818\n",
      "Iter-154 train loss: 0.4253 valid loss: 0.4217, valid accuracy: 0.8826\n",
      "Iter-155 train loss: 0.7067 valid loss: 0.4213, valid accuracy: 0.8838\n",
      "Iter-156 train loss: 0.5574 valid loss: 0.4217, valid accuracy: 0.8828\n",
      "Iter-157 train loss: 0.7890 valid loss: 0.4226, valid accuracy: 0.8804\n",
      "Iter-158 train loss: 0.4661 valid loss: 0.4234, valid accuracy: 0.8806\n",
      "Iter-159 train loss: 0.3849 valid loss: 0.4243, valid accuracy: 0.8792\n",
      "Iter-160 train loss: 0.6875 valid loss: 0.4247, valid accuracy: 0.8780\n",
      "Iter-161 train loss: 0.5191 valid loss: 0.4241, valid accuracy: 0.8784\n",
      "Iter-162 train loss: 0.4110 valid loss: 0.4213, valid accuracy: 0.8776\n",
      "Iter-163 train loss: 0.6145 valid loss: 0.4187, valid accuracy: 0.8808\n",
      "Iter-164 train loss: 0.3745 valid loss: 0.4168, valid accuracy: 0.8808\n",
      "Iter-165 train loss: 0.4722 valid loss: 0.4148, valid accuracy: 0.8800\n",
      "Iter-166 train loss: 0.7367 valid loss: 0.4127, valid accuracy: 0.8824\n",
      "Iter-167 train loss: 0.5865 valid loss: 0.4110, valid accuracy: 0.8822\n",
      "Iter-168 train loss: 0.5476 valid loss: 0.4092, valid accuracy: 0.8838\n",
      "Iter-169 train loss: 0.4207 valid loss: 0.4081, valid accuracy: 0.8838\n",
      "Iter-170 train loss: 0.2830 valid loss: 0.4075, valid accuracy: 0.8846\n",
      "Iter-171 train loss: 0.5429 valid loss: 0.4066, valid accuracy: 0.8846\n",
      "Iter-172 train loss: 0.3975 valid loss: 0.4057, valid accuracy: 0.8848\n",
      "Iter-173 train loss: 0.7713 valid loss: 0.4044, valid accuracy: 0.8842\n",
      "Iter-174 train loss: 0.5012 valid loss: 0.4023, valid accuracy: 0.8848\n",
      "Iter-175 train loss: 0.5929 valid loss: 0.4012, valid accuracy: 0.8866\n",
      "Iter-176 train loss: 0.4820 valid loss: 0.4001, valid accuracy: 0.8880\n",
      "Iter-177 train loss: 0.5282 valid loss: 0.4004, valid accuracy: 0.8876\n",
      "Iter-178 train loss: 0.2925 valid loss: 0.4011, valid accuracy: 0.8874\n",
      "Iter-179 train loss: 0.5317 valid loss: 0.4022, valid accuracy: 0.8866\n",
      "Iter-180 train loss: 0.2434 valid loss: 0.4019, valid accuracy: 0.8862\n",
      "Iter-181 train loss: 0.4393 valid loss: 0.4010, valid accuracy: 0.8874\n",
      "Iter-182 train loss: 0.4911 valid loss: 0.3992, valid accuracy: 0.8886\n",
      "Iter-183 train loss: 0.4043 valid loss: 0.3981, valid accuracy: 0.8878\n",
      "Iter-184 train loss: 0.5919 valid loss: 0.3972, valid accuracy: 0.8886\n",
      "Iter-185 train loss: 0.3806 valid loss: 0.3964, valid accuracy: 0.8902\n",
      "Iter-186 train loss: 0.3324 valid loss: 0.3951, valid accuracy: 0.8904\n",
      "Iter-187 train loss: 0.2921 valid loss: 0.3946, valid accuracy: 0.8900\n",
      "Iter-188 train loss: 0.5032 valid loss: 0.3935, valid accuracy: 0.8912\n",
      "Iter-189 train loss: 0.6398 valid loss: 0.3919, valid accuracy: 0.8922\n",
      "Iter-190 train loss: 0.3606 valid loss: 0.3908, valid accuracy: 0.8928\n",
      "Iter-191 train loss: 0.3066 valid loss: 0.3906, valid accuracy: 0.8936\n",
      "Iter-192 train loss: 0.4739 valid loss: 0.3917, valid accuracy: 0.8916\n",
      "Iter-193 train loss: 0.7985 valid loss: 0.3926, valid accuracy: 0.8926\n",
      "Iter-194 train loss: 0.4704 valid loss: 0.3932, valid accuracy: 0.8916\n",
      "Iter-195 train loss: 0.3122 valid loss: 0.3950, valid accuracy: 0.8906\n",
      "Iter-196 train loss: 0.6771 valid loss: 0.3970, valid accuracy: 0.8898\n",
      "Iter-197 train loss: 0.4007 valid loss: 0.3977, valid accuracy: 0.8896\n",
      "Iter-198 train loss: 0.3663 valid loss: 0.3982, valid accuracy: 0.8884\n",
      "Iter-199 train loss: 0.3708 valid loss: 0.3981, valid accuracy: 0.8880\n",
      "Iter-200 train loss: 0.4886 valid loss: 0.3986, valid accuracy: 0.8884\n",
      "Last iteration - Test accuracy mean: 0.8827, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 200 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 10 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10 # number of kernels/ filters in each layer\n",
    "num_input_units = D # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = C # number of classes in this classification problem\n",
    "p_dropout = 0.95 #  layer & unit noise: keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "lam = 1e-3 # output noise: reg at the feedback or loss function or function loss level as noise or loss_reg added.\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = CNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers, lam=lam)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nURSlifP/mBGgNcLnV+UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1addf30748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEzt8uSX9SIkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1ae13015f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
