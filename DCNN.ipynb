{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolution impl\n",
    "# from impl.im2col import *\n",
    "# or\n",
    "# import impl.im2col as im2col\n",
    "# out_height = int(((H + (2 * pad) - kernel_height) / stride) + 1), \n",
    "# stride == 1, ALWAYS\n",
    "# pad == kernel//2, ALWAYS\n",
    "# kernel == min size ALWAYS, i.e. one past, one pres, one post (if exist), i.e. three or two\n",
    "# kernel == 3 or 2 ALWAYS\n",
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + (2 * padding) - field_height) % stride == 0\n",
    "    assert (W + (2 * padding) - field_width) % stride == 0\n",
    "    out_height = int(((H + (2 * padding) - field_height) / stride) + 1)\n",
    "    out_width = int(((W + (2 * padding) - field_width) / stride) + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    # or\n",
    "    # j0 = np.tile(np.arange(field_width), field_height)\n",
    "    # j0 = np.tile(j0, C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "#     print('k.shape, i.shape, j.shape', k.shape, i.shape, j.shape)\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant') # X_NxCxHxW\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + (2 * padding), W + (2 * padding)\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    h_out = ((h_x + (2 * padding) - h_filter) / stride) + 1\n",
    "    w_out = ((w_x + (2 * padding) - w_filter) / stride) + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filters, -1)\n",
    "\n",
    "    out = (W_col @ X_col) + b\n",
    "    out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout_reshaped @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W_reshape = W.reshape(n_filter, -1)\n",
    "    dX_col = W_reshape.T @ dout_reshaped\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db\n",
    "\n",
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob in this case! \n",
    "# Is this true in other cases as well?\n",
    "def alpha_dropout_fwd(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def alpha_dropout_bwd(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "# X_train[0, :10, :10, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "#         self.lam = lam\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy(y, y_train)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            dh = selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 4.1131 validation accuracy: 0.108200\n",
      "Iter-2 training loss: 4.1236 validation accuracy: 0.122600\n",
      "Iter-3 training loss: 4.1847 validation accuracy: 0.131000\n",
      "Iter-4 training loss: 3.6091 validation accuracy: 0.145800\n",
      "Iter-5 training loss: 3.1873 validation accuracy: 0.159400\n",
      "Iter-6 training loss: 3.0863 validation accuracy: 0.178600\n",
      "Iter-7 training loss: 3.0036 validation accuracy: 0.199000\n",
      "Iter-8 training loss: 2.7316 validation accuracy: 0.221800\n",
      "Iter-9 training loss: 2.5724 validation accuracy: 0.252600\n",
      "Iter-10 training loss: 2.4735 validation accuracy: 0.277600\n",
      "Iter-11 training loss: 2.3156 validation accuracy: 0.305800\n",
      "Iter-12 training loss: 2.1340 validation accuracy: 0.330000\n",
      "Iter-13 training loss: 2.2217 validation accuracy: 0.359800\n",
      "Iter-14 training loss: 2.2068 validation accuracy: 0.389800\n",
      "Iter-15 training loss: 1.9780 validation accuracy: 0.418000\n",
      "Iter-16 training loss: 1.9489 validation accuracy: 0.441400\n",
      "Iter-17 training loss: 2.0406 validation accuracy: 0.465600\n",
      "Iter-18 training loss: 1.5640 validation accuracy: 0.488600\n",
      "Iter-19 training loss: 1.8284 validation accuracy: 0.511400\n",
      "Iter-20 training loss: 1.8098 validation accuracy: 0.530000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4034e52e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 20 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10\n",
    "# num_input_units = # noise added at the input lavel\n",
    "p_dropout = 0.95 #  keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "# lam = 1e-3 # reg at the feedback or loss function or function loss level as noise or loss_reg added.\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Kernel dead problem\n",
    "# y_pred = nn.test(X_test)\n",
    "# accs = np.mean(y_pred == y_test)\n",
    "# print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
