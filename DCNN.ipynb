{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = l.conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = l.selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                # h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = l.alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = cross_entropy(y, y_train)\n",
    "        dy = dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            # if train: There is no backward in testing/prediction\n",
    "            h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "            dy = l.alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            dh = l.selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = l.conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 4.3231 validation accuracy: 0.112000\n",
      "Iter-2 training loss: 3.5110 validation accuracy: 0.117800\n",
      "Iter-3 training loss: 3.5172 validation accuracy: 0.134800\n",
      "Iter-4 training loss: 3.0398 validation accuracy: 0.154000\n",
      "Iter-5 training loss: 2.8326 validation accuracy: 0.181400\n",
      "Iter-6 training loss: 3.0769 validation accuracy: 0.208600\n",
      "Iter-7 training loss: 2.6363 validation accuracy: 0.231400\n",
      "Iter-8 training loss: 2.2217 validation accuracy: 0.260000\n",
      "Iter-9 training loss: 2.4490 validation accuracy: 0.284000\n",
      "Iter-10 training loss: 2.4285 validation accuracy: 0.300200\n",
      "Iter-11 training loss: 2.4393 validation accuracy: 0.318000\n",
      "Iter-12 training loss: 1.9129 validation accuracy: 0.336400\n",
      "Iter-13 training loss: 1.9666 validation accuracy: 0.355000\n",
      "Iter-14 training loss: 2.2334 validation accuracy: 0.371400\n",
      "Iter-15 training loss: 1.9586 validation accuracy: 0.385600\n",
      "Iter-16 training loss: 2.1769 validation accuracy: 0.401200\n",
      "Iter-17 training loss: 1.8767 validation accuracy: 0.416600\n",
      "Iter-18 training loss: 1.5113 validation accuracy: 0.430800\n",
      "Iter-19 training loss: 1.9336 validation accuracy: 0.446200\n",
      "Iter-20 training loss: 1.8387 validation accuracy: 0.461000\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 20 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Kernel dead problem\n",
    "# y_pred = nn.test(X_test)\n",
    "# accs = np.mean(y_pred == y_test)\n",
    "# print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 3.3585 validation accuracy: 0.113800\n",
      "Iter-2 training loss: 2.9106 validation accuracy: 0.131400\n",
      "Iter-3 training loss: 2.8719 validation accuracy: 0.155000\n",
      "Iter-4 training loss: 2.9717 validation accuracy: 0.177400\n",
      "Iter-5 training loss: 2.6902 validation accuracy: 0.208400\n",
      "Iter-6 training loss: 2.4088 validation accuracy: 0.239200\n",
      "Iter-7 training loss: 2.4490 validation accuracy: 0.269400\n",
      "Iter-8 training loss: 2.7484 validation accuracy: 0.299400\n",
      "Iter-9 training loss: 2.2952 validation accuracy: 0.319000\n",
      "Iter-10 training loss: 2.3132 validation accuracy: 0.336600\n",
      "Iter-11 training loss: 2.2874 validation accuracy: 0.358600\n",
      "Iter-12 training loss: 2.0311 validation accuracy: 0.383200\n",
      "Iter-13 training loss: 1.8080 validation accuracy: 0.405200\n",
      "Iter-14 training loss: 1.6761 validation accuracy: 0.429000\n",
      "Iter-15 training loss: 1.8254 validation accuracy: 0.448200\n",
      "Iter-16 training loss: 1.8498 validation accuracy: 0.465800\n",
      "Iter-17 training loss: 1.7910 validation accuracy: 0.480000\n",
      "Iter-18 training loss: 1.5846 validation accuracy: 0.491800\n",
      "Iter-19 training loss: 1.9106 validation accuracy: 0.507200\n",
      "Iter-20 training loss: 1.5749 validation accuracy: 0.520200\n",
      "Iter-21 training loss: 1.2810 validation accuracy: 0.530800\n",
      "Iter-22 training loss: 1.5737 validation accuracy: 0.546200\n",
      "Iter-23 training loss: 1.1911 validation accuracy: 0.562600\n",
      "Iter-24 training loss: 1.4310 validation accuracy: 0.579000\n",
      "Iter-25 training loss: 1.5488 validation accuracy: 0.589000\n",
      "Iter-26 training loss: 1.3549 validation accuracy: 0.599800\n",
      "Iter-27 training loss: 1.2281 validation accuracy: 0.608400\n",
      "Iter-28 training loss: 1.5307 validation accuracy: 0.618400\n",
      "Iter-29 training loss: 1.5310 validation accuracy: 0.628600\n",
      "Iter-30 training loss: 1.2365 validation accuracy: 0.637000\n",
      "Iter-31 training loss: 1.3888 validation accuracy: 0.646800\n",
      "Iter-32 training loss: 1.1486 validation accuracy: 0.648600\n",
      "Iter-33 training loss: 1.1457 validation accuracy: 0.656800\n",
      "Iter-34 training loss: 1.1128 validation accuracy: 0.666600\n",
      "Iter-35 training loss: 1.0590 validation accuracy: 0.673800\n",
      "Iter-36 training loss: 1.0721 validation accuracy: 0.681600\n",
      "Iter-37 training loss: 0.9821 validation accuracy: 0.687000\n",
      "Iter-38 training loss: 1.0536 validation accuracy: 0.692000\n",
      "Iter-39 training loss: 1.1406 validation accuracy: 0.699400\n",
      "Iter-40 training loss: 1.1399 validation accuracy: 0.706400\n",
      "Iter-41 training loss: 1.1085 validation accuracy: 0.710600\n",
      "Iter-42 training loss: 0.8718 validation accuracy: 0.715400\n",
      "Iter-43 training loss: 0.8836 validation accuracy: 0.720200\n",
      "Iter-44 training loss: 1.0661 validation accuracy: 0.725200\n",
      "Iter-45 training loss: 0.9361 validation accuracy: 0.729800\n",
      "Iter-46 training loss: 0.8326 validation accuracy: 0.734400\n",
      "Iter-47 training loss: 0.8133 validation accuracy: 0.741000\n",
      "Iter-48 training loss: 0.7813 validation accuracy: 0.747400\n",
      "Iter-49 training loss: 0.8720 validation accuracy: 0.752800\n",
      "Iter-50 training loss: 0.6596 validation accuracy: 0.759800\n",
      "Iter-51 training loss: 0.7657 validation accuracy: 0.761200\n",
      "Iter-52 training loss: 0.7078 validation accuracy: 0.765200\n",
      "Iter-53 training loss: 0.9494 validation accuracy: 0.769600\n",
      "Iter-54 training loss: 0.7999 validation accuracy: 0.772600\n",
      "Iter-55 training loss: 0.7904 validation accuracy: 0.778600\n",
      "Iter-56 training loss: 0.6759 validation accuracy: 0.780800\n",
      "Iter-57 training loss: 0.9161 validation accuracy: 0.783800\n",
      "Iter-58 training loss: 0.6098 validation accuracy: 0.789400\n",
      "Iter-59 training loss: 0.5143 validation accuracy: 0.793000\n",
      "Iter-60 training loss: 0.7952 validation accuracy: 0.794400\n",
      "Iter-61 training loss: 1.0196 validation accuracy: 0.798000\n",
      "Iter-62 training loss: 0.7128 validation accuracy: 0.801000\n",
      "Iter-63 training loss: 0.5511 validation accuracy: 0.801200\n",
      "Iter-64 training loss: 0.5176 validation accuracy: 0.802200\n",
      "Iter-65 training loss: 0.6772 validation accuracy: 0.802000\n",
      "Iter-66 training loss: 0.3905 validation accuracy: 0.801600\n",
      "Iter-67 training loss: 0.5675 validation accuracy: 0.802200\n",
      "Iter-68 training loss: 0.6425 validation accuracy: 0.803000\n",
      "Iter-69 training loss: 0.6750 validation accuracy: 0.803800\n",
      "Iter-70 training loss: 0.7860 validation accuracy: 0.804800\n",
      "Iter-71 training loss: 0.7646 validation accuracy: 0.805800\n",
      "Iter-72 training loss: 0.5595 validation accuracy: 0.805600\n",
      "Iter-73 training loss: 0.7190 validation accuracy: 0.805200\n",
      "Iter-74 training loss: 0.6454 validation accuracy: 0.806200\n",
      "Iter-75 training loss: 0.8454 validation accuracy: 0.807600\n",
      "Iter-76 training loss: 0.6504 validation accuracy: 0.807800\n",
      "Iter-77 training loss: 0.3934 validation accuracy: 0.808400\n",
      "Iter-78 training loss: 0.5491 validation accuracy: 0.808000\n",
      "Iter-79 training loss: 0.8852 validation accuracy: 0.809200\n",
      "Iter-80 training loss: 0.4486 validation accuracy: 0.810600\n",
      "Iter-81 training loss: 0.6239 validation accuracy: 0.812400\n",
      "Iter-82 training loss: 0.5619 validation accuracy: 0.813800\n",
      "Iter-83 training loss: 0.5905 validation accuracy: 0.816000\n",
      "Iter-84 training loss: 0.4926 validation accuracy: 0.818000\n",
      "Iter-85 training loss: 0.5976 validation accuracy: 0.820200\n",
      "Iter-86 training loss: 1.0531 validation accuracy: 0.822400\n",
      "Iter-87 training loss: 0.9413 validation accuracy: 0.823400\n",
      "Iter-88 training loss: 0.5082 validation accuracy: 0.824600\n",
      "Iter-89 training loss: 0.7728 validation accuracy: 0.827200\n",
      "Iter-90 training loss: 0.4761 validation accuracy: 0.829000\n",
      "Iter-91 training loss: 0.7782 validation accuracy: 0.830000\n",
      "Iter-92 training loss: 0.6056 validation accuracy: 0.831600\n",
      "Iter-93 training loss: 0.5615 validation accuracy: 0.833400\n",
      "Iter-94 training loss: 0.7011 validation accuracy: 0.834600\n",
      "Iter-95 training loss: 0.3500 validation accuracy: 0.835600\n",
      "Iter-96 training loss: 0.3779 validation accuracy: 0.838200\n",
      "Iter-97 training loss: 0.5240 validation accuracy: 0.839600\n",
      "Iter-98 training loss: 0.5192 validation accuracy: 0.840600\n",
      "Iter-99 training loss: 0.5166 validation accuracy: 0.842800\n",
      "Iter-100 training loss: 0.3540 validation accuracy: 0.844000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122a76da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Hyper-parameters\n",
    "# n_iter = 100 # number of epochs\n",
    "# alpha = 1e-4 # learning_rate\n",
    "# mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "# num_layers = 10 # depth \n",
    "# print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "# num_hidden_units = 10\n",
    "# p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# # build the model/NN and learn it: running session.\n",
    "# nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "# nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "#            n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # # Kernel dead problem\n",
    "# # y_pred = nn.test(X_test)\n",
    "# # accs = np.mean(y_pred == y_test)\n",
    "# # print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # # Display the learning curve and losses for training, validation, and testing\n",
    "# # %matplotlib inline\n",
    "# # %config InlineBackend.figure_format = 'retina'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(nn.losses['train'], label='Train loss')\n",
    "# # plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
