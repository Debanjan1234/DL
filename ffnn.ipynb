{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import impl.neuralnet as nn\n",
    "from impl.solver import *\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter = 1000 # number of epochs\n",
    "alpha = 1e-3\n",
    "mb_size = 64 # minibatch size usually compatible to the Cache/RAM size\n",
    "n_experiment = 1\n",
    "reg = 1e-5\n",
    "print_after = 100\n",
    "p_dropout = 0.8 # dropout/keep_prob\n",
    "loss = 'cross_ent'\n",
    "nonlin = 'relu'\n",
    "solver = 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import hipsternet.input_data as input_data  # NOT used for MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "M, D, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hipsternet.utils as utils\n",
    "X_train, X_val, X_test = utils.prepro(X_train, X_val, X_test)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # if net_type == 'cnn':\n",
    "# img_shape = (1, 28, 28)\n",
    "# X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_val = X_val.reshape(-1, *img_shape)\n",
    "# X_test = X_test.reshape(-1, *img_shape)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# # ((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))\n",
    "# # In [19]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adagrad': <function hipsternet.solver.adagrad>,\n",
       " 'adam': <function hipsternet.solver.adam>,\n",
       " 'momentum': <function hipsternet.solver.momentum>,\n",
       " 'nesterov': <function hipsternet.solver.nesterov>,\n",
       " 'rmsprop': <function hipsternet.solver.rmsprop>,\n",
       " 'sgd': <function hipsternet.solver.sgd>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solvers = dict(\n",
    "    sgd=sgd,\n",
    "    momentum=momentum,\n",
    "    nesterov=nesterov,\n",
    "    adagrad=adagrad,\n",
    "    rmsprop=rmsprop,\n",
    "    adam=adam\n",
    ")\n",
    "solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<function hipsternet.solver.sgd>, array([ 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver_fun = solvers[solver] # solver functions\n",
    "accs = np.zeros(n_experiment)\n",
    "solver_fun, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Experimenting on sgd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('Experimenting on {}'.format(solver))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Biased_Fully_Parametric_leaky_ReLU as the activation function\n",
      "Experiment-1\n",
      "Iter-100 loss: 2.1331 validation: 0.226200\n",
      "Iter-200 loss: 2.1931 validation: 0.357200\n",
      "Iter-300 loss: 1.7955 validation: 0.465600\n",
      "Iter-400 loss: 1.8817 validation: 0.549200\n",
      "Iter-500 loss: 1.5905 validation: 0.607600\n",
      "Iter-600 loss: 1.5075 validation: 0.666600\n",
      "Iter-700 loss: 1.2493 validation: 0.701200\n",
      "Iter-800 loss: 1.0984 validation: 0.725200\n",
      "Iter-900 loss: 1.1242 validation: 0.744200\n",
      "Iter-1000 loss: 1.2582 validation: 0.757800\n",
      "\n",
      "Test Mean accuracy: 0.7553, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import hipsternet.FFNN as ffnn # Feed Forward Neural Net\n",
    "import hipsternet.NN as nn_\n",
    "\n",
    "for k in range(n_experiment):\n",
    "#     print('PReLU as the activation function')\n",
    "    print('ReLU as the activation function')\n",
    "#     print('Softplus as the activation function')\n",
    "#     print('Sigmoid as the activation function')\n",
    "#     print('ELU as the activation function')\n",
    "#     print('Neural_Biased_Fully_Parametric_leaky_ReLU as the activation function')\n",
    "#     print('Biased_Fully_Parametric_ReLU as the activation function')\n",
    "#     print('Fully_Parametric_ReLU as the activation function')\n",
    "#     print('P_Leaky ReLU as the activation function')\n",
    "#     print('Leaky ReLU as the activation function')\n",
    "#     print('Noisy ReLU as the activation function')\n",
    "#     print('Tanh as the activation function')\n",
    "#     print('Integral Tanh as the activation function')\n",
    "    print('Experiment-{}'.format(k + 1))\n",
    "\n",
    "    # net = nn.FeedForwardNet(D, C, H=128, lam=reg, p_dropout=p_dropout, loss=loss, nonlin=nonlin)\n",
    "    net = ffnn.FFNN(D, C, H=128, lam=reg, p_dropout=p_dropout, loss=loss, nonlin=nonlin)\n",
    "    #     net = nn.ConvNet(D=10, C=C, H=128) # Original one\n",
    "#     net = ConvNet_(C=C, D=10, H=128) # Mine\n",
    "    \n",
    "\n",
    "    net = solver_fun(nn=net, X_train=X_train, y_train=y_train, val_set=(X_val, y_val), \n",
    "                     mb_size=mb_size, alpha=alpha, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "    y_pred = net.predict(X_test)\n",
    "    accs[k] = np.mean(y_pred == y_test)\n",
    "\n",
    "print()\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work: -----------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
