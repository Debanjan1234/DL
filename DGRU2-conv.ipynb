{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3629,),\n",
       " (3629,),\n",
       " array([36, 58, 41, 58, 42,  9,  5, 36, 58, 41]),\n",
       " array([58, 41, 58, 42,  9,  5, 36, 58, 41, 58]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# Looking at the X, y\n",
    "X.shape, y.shape, X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, K, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.), \n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "        # Conv model parameters\n",
    "        # y_nx1 = X_nxt @ W_tx1 + b_1x1\n",
    "        m = dict(\n",
    "            W = np.random.randn(K, 1) / np.sqrt(K / 2.),\n",
    "            b = np.random.randn(1, 1) / np.sqrt(1 / 2.)\n",
    "        )\n",
    "        self.model_conv = []\n",
    "        for _ in range(self.L):\n",
    "            self.model_conv.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        # h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        # dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        dhz = (hh - h_in) * dh\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXh + dXz\n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "        \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches, caches_conv = [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            caches_conv.append([])\n",
    "        \n",
    "        # Embedding, Input layer, 1st layer\n",
    "        Xs = []\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "#             print('X.shape', X.shape)\n",
    "            Xs.append(X)\n",
    "        \n",
    "        for layer in range(self.L):\n",
    "            ys = []\n",
    "#             print('np.array(Xs).shape', np.array(Xs).shape)\n",
    "            Xs = np.array(Xs).reshape(len(Xs), -1)\n",
    "#             print('Xs.shape', Xs.shape)\n",
    "            n = Xs.shape[1] # Xs_txn\n",
    "            pad = np.zeros((self.K//2, n))\n",
    "            Xs_pad = np.row_stack((pad, Xs, pad))\n",
    "#             print('Xs_pad.shape', Xs_pad.shape)\n",
    "\n",
    "            for i in range(0, len(Xs_pad) - kernel_size + 1, 1):\n",
    "                X = Xs_pad[i: i + kernel_size] # X_txn\n",
    "#                 print('X.shape', X.shape)\n",
    "                # y_nx1 = X_nxt @ W_tx1 + b_nx1\n",
    "                y, cache = l.fc_forward(X.T, self.model_conv[layer]['W'], self.model_conv[layer]['b'])\n",
    "                caches_conv[layer].append(cache)\n",
    "#                 print('y.shape', y.shape)\n",
    "                X = y.reshape(1, -1).copy() # X_1xn\n",
    "#                 print('X.shape', X.shape)\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "            Xs = ys.copy()\n",
    "\n",
    "        ys_caches = caches, caches_conv\n",
    "\n",
    "        return ys, ys_caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, ys_caches):\n",
    "        dh, grad, grads, grads_conv = [], [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads_conv.append({key: np.zeros_like(val) for key, val in self.model_conv[layer].items()})\n",
    "        \n",
    "        caches, caches_conv = ys_caches\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            # Convolution RNN forward\n",
    "#             print('dys[0].shape', dys[0].shape)\n",
    "            n = dys[0].reshape(1, -1).shape[1] # y_1xn\n",
    "            t = len(dys)\n",
    "            dXs = np.zeros((t, n))\n",
    "#             print('dXs.shape', dXs.shape)\n",
    "            pad = np.zeros((self.K//2, n))\n",
    "            dXs_pad = np.row_stack((pad, dXs, pad))\n",
    "#             print('dXs_pad.shape', dXs_pad.shape)\n",
    "\n",
    "            for t in reversed(range(len(dys))):\n",
    "#                 print('dys[t].shape', dys[t].shape)\n",
    "                dy = dys[t].reshape(1, -1)\n",
    "#                 print('dy.shape', dy.shape)\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "#                 print('dX.shape', dX.shape)\n",
    "                dy = dX.copy().T # dy_nxt\n",
    "#                 print('dy.shape', dy.shape)\n",
    "                dX, dW, db = l.fc_backward(dy, caches_conv[layer][t])\n",
    "#                 print('dX.shape', dX.shape)\n",
    "                grads_conv[layer]['W'] += dW\n",
    "                grads_conv[layer]['b'] += db\n",
    "                dX = dX.T\n",
    "#                 print('dXs_pad.shape', dXs_pad.shape)\n",
    "#                 print('dXs.shape', dXs.shape)\n",
    "                for i in range(t, t + kernel_size, 1):\n",
    "                    np.add.at(dXs_pad, [i], dX[i-t])\n",
    "            dXs = dXs_pad[kernel_size// 2: -(kernel_size// 2)]\n",
    "#             print('dXs_pad.shape', dXs_pad.shape)\n",
    "#             print('dXs.shape', dXs.shape)\n",
    "            dys = dXs.copy()\n",
    "\n",
    "        return dXs, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    # for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            #             sample = nn.test(X_mini[0], state, 100)\n",
    "            #             print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 282.6360\n",
      "Iter-2 loss: 284.0344\n",
      "Iter-3 loss: 276.7268\n",
      "Iter-4 loss: 274.1087\n",
      "Iter-5 loss: 273.4225\n",
      "Iter-6 loss: 260.9835\n",
      "Iter-7 loss: 245.5704\n",
      "Iter-8 loss: 249.3517\n",
      "Iter-9 loss: 235.2443\n",
      "Iter-10 loss: 234.6258\n",
      "Iter-11 loss: 240.3221\n",
      "Iter-12 loss: 244.5241\n",
      "Iter-13 loss: 230.2429\n",
      "Iter-14 loss: 242.0965\n",
      "Iter-15 loss: 223.0875\n",
      "Iter-16 loss: 233.1580\n",
      "Iter-17 loss: 236.1578\n",
      "Iter-18 loss: 210.5977\n",
      "Iter-19 loss: 211.7050\n",
      "Iter-20 loss: 205.6723\n",
      "Iter-21 loss: 229.5645\n",
      "Iter-22 loss: 220.6475\n",
      "Iter-23 loss: 226.6651\n",
      "Iter-24 loss: 217.6213\n",
      "Iter-25 loss: 217.0106\n",
      "Iter-26 loss: 206.8715\n",
      "Iter-27 loss: 213.8593\n",
      "Iter-28 loss: 187.1068\n",
      "Iter-29 loss: 227.2433\n",
      "Iter-30 loss: 210.3694\n",
      "Iter-31 loss: 203.3778\n",
      "Iter-32 loss: 198.8547\n",
      "Iter-33 loss: 198.0858\n",
      "Iter-34 loss: 193.5455\n",
      "Iter-35 loss: 191.8089\n",
      "Iter-36 loss: 194.1203\n",
      "Iter-37 loss: 201.9494\n",
      "Iter-38 loss: 190.3230\n",
      "Iter-39 loss: 192.7842\n",
      "Iter-40 loss: 175.3320\n",
      "Iter-41 loss: 190.1159\n",
      "Iter-42 loss: 170.6590\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 3 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "kernel_size = 9\n",
    "\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          K=kernel_size)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 333.2342\n",
      "Iter-2 loss: 320.9275\n",
      "Iter-3 loss: 314.6743\n",
      "Iter-4 loss: 310.6572\n",
      "Iter-5 loss: 321.2195\n",
      "Iter-6 loss: 301.3016\n",
      "Iter-7 loss: 302.5433\n",
      "Iter-8 loss: 299.0288\n",
      "Iter-9 loss: 311.8440\n",
      "Iter-10 loss: 283.4220\n",
      "Iter-11 loss: 265.0681\n",
      "Iter-12 loss: 245.0536\n",
      "Iter-13 loss: 237.2402\n",
      "Iter-14 loss: 232.4475\n",
      "Iter-15 loss: 213.5020\n",
      "Iter-16 loss: 200.4844\n",
      "Iter-17 loss: 195.1714\n",
      "Iter-18 loss: 199.4185\n",
      "Iter-19 loss: 197.1483\n",
      "Iter-20 loss: 188.5489\n",
      "Iter-21 loss: 200.2441\n",
      "Iter-22 loss: 189.1315\n",
      "Iter-23 loss: 180.6437\n",
      "Iter-24 loss: 186.0387\n",
      "Iter-25 loss: 181.3103\n",
      "Iter-26 loss: 185.1015\n",
      "Iter-27 loss: 182.2362\n",
      "Iter-28 loss: 179.8170\n",
      "Iter-29 loss: 174.2455\n",
      "Iter-30 loss: 176.9534\n",
      "Iter-31 loss: 165.0314\n",
      "Iter-32 loss: 167.3166\n",
      "Iter-33 loss: 164.0225\n",
      "Iter-34 loss: 159.7611\n",
      "Iter-35 loss: 157.1981\n",
      "Iter-36 loss: 164.7027\n",
      "Iter-37 loss: 153.7945\n",
      "Iter-38 loss: 151.3449\n",
      "Iter-39 loss: 154.3753\n",
      "Iter-40 loss: 144.2587\n",
      "Iter-41 loss: 144.0276\n",
      "Iter-42 loss: 141.8098\n",
      "Iter-43 loss: 135.6928\n",
      "Iter-44 loss: 133.7264\n",
      "Iter-45 loss: 138.1463\n",
      "Iter-46 loss: 140.3082\n",
      "Iter-47 loss: 136.9355\n",
      "Iter-48 loss: 130.2120\n",
      "Iter-49 loss: 135.5385\n",
      "Iter-50 loss: 138.3509\n",
      "Iter-51 loss: 135.7811\n",
      "Iter-52 loss: 138.1243\n",
      "Iter-53 loss: 128.5927\n",
      "Iter-54 loss: 127.1606\n",
      "Iter-55 loss: 128.6770\n",
      "Iter-56 loss: 126.5388\n",
      "Iter-57 loss: 132.9555\n",
      "Iter-58 loss: 127.7071\n",
      "Iter-59 loss: 125.1254\n",
      "Iter-60 loss: 122.3742\n",
      "Iter-61 loss: 120.4697\n",
      "Iter-62 loss: 114.1387\n",
      "Iter-63 loss: 115.1085\n",
      "Iter-64 loss: 105.6029\n",
      "Iter-65 loss: 106.2680\n",
      "Iter-66 loss: 108.4999\n",
      "Iter-67 loss: 103.6506\n",
      "Iter-68 loss: 101.6977\n",
      "Iter-69 loss: 102.9012\n",
      "Iter-70 loss: 105.5632\n",
      "Iter-71 loss: 107.4748\n",
      "Iter-72 loss: 109.0490\n",
      "Iter-73 loss: 110.2072\n",
      "Iter-74 loss: 113.3648\n",
      "Iter-75 loss: 108.4870\n",
      "Iter-76 loss: 105.5969\n",
      "Iter-77 loss: 103.6193\n",
      "Iter-78 loss: 107.7163\n",
      "Iter-79 loss: 103.9728\n",
      "Iter-80 loss: 104.6865\n",
      "Iter-81 loss: 114.1758\n",
      "Iter-82 loss: 106.5741\n",
      "Iter-83 loss: 105.5647\n",
      "Iter-84 loss: 104.1126\n",
      "Iter-85 loss: 105.6108\n",
      "Iter-86 loss: 104.4449\n",
      "Iter-87 loss: 105.6826\n",
      "Iter-88 loss: 103.3542\n",
      "Iter-89 loss: 103.4809\n",
      "Iter-90 loss: 103.7948\n",
      "Iter-91 loss: 101.3808\n",
      "Iter-92 loss: 100.1904\n",
      "Iter-93 loss: 98.6352\n",
      "Iter-94 loss: 96.6546\n",
      "Iter-95 loss: 94.4130\n",
      "Iter-96 loss: 93.7716\n",
      "Iter-97 loss: 94.4440\n",
      "Iter-98 loss: 90.9028\n",
      "Iter-99 loss: 90.7333\n",
      "Iter-100 loss: 89.5854\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzhGhFwRBiHNE6AVBEOIcEXpBEIQ45/8Bqzz2i5gywC8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114091128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Hyper-parameters\n",
    "# time_step = 100 # width, minibatch size and test sample size as well\n",
    "# num_layers = 2 # depth\n",
    "# n_iter = 100 # epochs\n",
    "# alpha = 1e-4 # learning_rate\n",
    "# print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "# num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "# num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "# kernel_size = 9\n",
    "\n",
    "\n",
    "# # Build the network and learning it or optimizing it using SGD\n",
    "# net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "#           K=kernel_size)\n",
    "\n",
    "# # Start learning using BP-SGD-ADAM\n",
    "# adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # # Display the learning curve and losses for training, validation, and testing\n",
    "# # %matplotlib inline\n",
    "# # %config InlineBackend.figure_format = 'retina'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
