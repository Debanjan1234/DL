{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Looking at the X, y\n",
    "# X.shape, y.shape, X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, K, N, char2idx, idx2char):\n",
    "        self.D = D # num_input_dim\n",
    "        self.H = H # num_hidden_units\n",
    "        self.L = L # num_hidden_layers\n",
    "        self.K = K # conv kernel size\n",
    "        self.N = N # number of conv units or neurons\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "                    \n",
    "        # Conv model parameters\n",
    "        # y_DxN = X_DxK @ W_KxN + b_1xN\n",
    "        m = dict(\n",
    "            W = np.random.randn(K, N) / np.sqrt(K / 2.),\n",
    "            b = np.random.randn(1, N)\n",
    "        )\n",
    "        self.model_conv = []\n",
    "        for _ in range(self.L):\n",
    "            self.model_conv.append(m)\n",
    "\n",
    "        # Recurrent Model params\n",
    "        Z = H + (D * N)\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.), \n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        # h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        # dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        dhz = (hh - h_in) * dh\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXh + dXz\n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "        \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches, caches_conv = [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            caches_conv.append([])\n",
    "        \n",
    "        # Embedding, Input layer, 1st layer\n",
    "        Xs = []\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            Xs.append(X)\n",
    "        \n",
    "        for layer in range(self.L):\n",
    "            ys = []\n",
    "            Xs = np.array(Xs).reshape(len(Xs), -1)\n",
    "            n = Xs.shape[1] # Xs_txn\n",
    "            pad = np.zeros((self.K//2, n))\n",
    "            Xs_pad = np.row_stack((pad, Xs, pad))\n",
    "\n",
    "            for i in range(0, len(Xs_pad) - kernel_size + 1, 1):\n",
    "                X = Xs_pad[i: i + kernel_size] # X_txn\n",
    "                # y_DxN = X_DxK @ W_KxN + b_1xN\n",
    "                X_conv = X.T # X_DxK\n",
    "                y, cache = l.fc_forward(X_conv, self.model_conv[layer]['W'], self.model_conv[layer]['b'])\n",
    "                caches_conv[layer].append(cache)\n",
    "                X = y.reshape(1, -1).copy() # X_1xD*N\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "            Xs = ys.copy()\n",
    "\n",
    "        ys_caches = caches, caches_conv\n",
    "\n",
    "        return ys, ys_caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, ys_caches):\n",
    "        dh, grad, grads, grads_conv = [], [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads_conv.append({key: np.zeros_like(val) for key, val in self.model_conv[layer].items()})\n",
    "        \n",
    "        caches, caches_conv = ys_caches\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            # Convolution RNN forward\n",
    "            n = dys[0].reshape(1, -1).shape[1] # y_1xn\n",
    "            t = len(dys)\n",
    "            dXs = np.zeros((t, n))\n",
    "            pad = np.zeros((self.K//2, n))\n",
    "            dXs_pad = np.row_stack((pad, dXs, pad))\n",
    "\n",
    "            for t in reversed(range(len(dys))):\n",
    "                dy = dys[t].reshape(1, -1)\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.reshape((self.D, self.N)).copy() # DxN\n",
    "                dX_conv, dW, db = l.fc_backward(dy, caches_conv[layer][t])\n",
    "                grads_conv[layer]['W'] += dW\n",
    "                grads_conv[layer]['b'] += db\n",
    "                dX = dX_conv.T # X_DxK.T= X_KxD\n",
    "                for i in range(t, t + kernel_size, 1):\n",
    "                    np.add.at(dXs_pad, [i], dX[i-t])\n",
    "            dXs = dXs_pad[kernel_size// 2: -(kernel_size// 2)]\n",
    "            dys = dXs.copy()\n",
    "\n",
    "        grads_all = grads, grads_conv\n",
    "        \n",
    "        return grads_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    M_conv, R_conv = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        M_conv.append({k: np.zeros_like(v) for k, v in nn.model_conv[layer].items()})\n",
    "        R_conv.append({k: np.zeros_like(v) for k, v in nn.model_conv[layer].items()})\n",
    "\n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            # Traing\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            grads_all = nn.train_backward(dys, caches)\n",
    "            grads, grads_conv = grads_all\n",
    "            \n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            # Update the weights & biases or model\n",
    "            for layer in range(nn.L):\n",
    "                # Recurrent model\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                # ConvNet model\n",
    "                for k in grads_conv[layer].keys(): #key, value: items\n",
    "                    M_conv[layer][k] = l.exp_running_avg(M_conv[layer][k], grads_conv[layer][k], beta1)\n",
    "                    R_conv[layer][k] = l.exp_running_avg(R_conv[layer][k], grads_conv[layer][k]**2, beta2)\n",
    "                    m_k_hat = M_conv[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R_conv[layer][k] / (1. - (beta2**(iter)))\n",
    "                    nn.model_conv[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{}, train loss: {:.4f}'.format(iter, loss))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1, train loss: 108.8045\n",
      "Iter-2, train loss: 101.4169\n",
      "Iter-3, train loss: 90.5069\n",
      "Iter-4, train loss: 81.5140\n",
      "Iter-5, train loss: 77.9118\n",
      "Iter-6, train loss: 73.6277\n",
      "Iter-7, train loss: 68.3377\n",
      "Iter-8, train loss: 66.1021\n",
      "Iter-9, train loss: 64.1447\n",
      "Iter-10, train loss: 63.3221\n",
      "Iter-11, train loss: 64.9201\n",
      "Iter-12, train loss: 59.9147\n",
      "Iter-13, train loss: 55.9733\n",
      "Iter-14, train loss: 53.9484\n",
      "Iter-15, train loss: 53.6991\n",
      "Iter-16, train loss: 56.8439\n",
      "Iter-17, train loss: 51.7474\n",
      "Iter-18, train loss: 49.6125\n",
      "Iter-19, train loss: 46.4433\n",
      "Iter-20, train loss: 48.7477\n",
      "Iter-21, train loss: 46.6070\n",
      "Iter-22, train loss: 44.7643\n",
      "Iter-23, train loss: 43.3408\n",
      "Iter-24, train loss: 40.9277\n",
      "Iter-25, train loss: 40.0949\n",
      "Iter-26, train loss: 39.7938\n",
      "Iter-27, train loss: 37.1347\n",
      "Iter-28, train loss: 36.5486\n",
      "Iter-29, train loss: 35.0558\n",
      "Iter-30, train loss: 33.2556\n",
      "Iter-31, train loss: 34.0209\n",
      "Iter-32, train loss: 31.1562\n",
      "Iter-33, train loss: 28.6260\n",
      "Iter-34, train loss: 29.0717\n",
      "Iter-35, train loss: 24.7888\n",
      "Iter-36, train loss: 26.0716\n",
      "Iter-37, train loss: 23.6103\n",
      "Iter-38, train loss: 24.8263\n",
      "Iter-39, train loss: 22.7400\n",
      "Iter-40, train loss: 22.6114\n",
      "Iter-41, train loss: 21.2430\n",
      "Iter-42, train loss: 21.2984\n",
      "Iter-43, train loss: 19.9212\n",
      "Iter-44, train loss: 21.1546\n",
      "Iter-45, train loss: 19.1478\n",
      "Iter-46, train loss: 18.5369\n",
      "Iter-47, train loss: 18.4204\n",
      "Iter-48, train loss: 18.0959\n",
      "Iter-49, train loss: 15.8076\n",
      "Iter-50, train loss: 15.2733\n",
      "Iter-51, train loss: 14.8757\n",
      "Iter-52, train loss: 14.7176\n",
      "Iter-53, train loss: 14.2858\n",
      "Iter-54, train loss: 14.1272\n",
      "Iter-55, train loss: 13.7400\n",
      "Iter-56, train loss: 13.0477\n",
      "Iter-57, train loss: 12.7902\n",
      "Iter-58, train loss: 12.5162\n",
      "Iter-59, train loss: 12.2729\n",
      "Iter-60, train loss: 12.0302\n",
      "Iter-61, train loss: 11.5176\n",
      "Iter-62, train loss: 11.2270\n",
      "Iter-63, train loss: 11.0661\n",
      "Iter-64, train loss: 10.9926\n",
      "Iter-65, train loss: 10.9230\n",
      "Iter-66, train loss: 10.4712\n",
      "Iter-67, train loss: 10.2458\n",
      "Iter-68, train loss: 10.2700\n",
      "Iter-69, train loss: 9.9704\n",
      "Iter-70, train loss: 9.8097\n",
      "Iter-71, train loss: 9.5053\n",
      "Iter-72, train loss: 9.3761\n",
      "Iter-73, train loss: 9.1427\n",
      "Iter-74, train loss: 9.0134\n",
      "Iter-75, train loss: 8.8718\n",
      "Iter-76, train loss: 8.5540\n",
      "Iter-77, train loss: 8.4583\n",
      "Iter-78, train loss: 8.2430\n",
      "Iter-79, train loss: 8.0646\n",
      "Iter-80, train loss: 7.7566\n",
      "Iter-81, train loss: 7.7044\n",
      "Iter-82, train loss: 7.4548\n",
      "Iter-83, train loss: 7.3156\n",
      "Iter-84, train loss: 7.1229\n",
      "Iter-85, train loss: 6.9152\n",
      "Iter-86, train loss: 6.6856\n",
      "Iter-87, train loss: 6.5479\n",
      "Iter-88, train loss: 6.4570\n",
      "Iter-89, train loss: 6.2696\n",
      "Iter-90, train loss: 6.1179\n",
      "Iter-91, train loss: 5.9836\n",
      "Iter-92, train loss: 5.8334\n",
      "Iter-93, train loss: 5.7143\n",
      "Iter-94, train loss: 5.5467\n",
      "Iter-95, train loss: 5.3554\n",
      "Iter-96, train loss: 5.1666\n",
      "Iter-97, train loss: 5.0516\n",
      "Iter-98, train loss: 4.9267\n",
      "Iter-99, train loss: 4.8345\n",
      "Iter-100, train loss: 4.6519\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8FOX9wPHPk5CDkABJCOEIEMJ9RwiC4oWKClrU1gPr\nga1HW631bBustlhF+XnUerZFqxW1UA+qIIjlEOVQLrkJEI6EBEISEnLfyfP7YyabSdjNuZs98n3z\nymufnZ2Z/e6EfOfZZ555HqW1RgghhO/yc3cAQgghXEsSvRBC+DhJ9EII4eMk0QshhI+TRC+EED5O\nEr0QQvg4SfRCCOHjJNELIYSPk0QvhBA+rpO7AwDo0aOHjo2NdXcYQgjhVbZv335aax3V1Hoekehj\nY2PZtm2bu8MQQgivopRKbc560nQjhBA+ThK9EEL4OEn0Qgjh4zyijV4I0XKVlZWkp6dTVlbm7lCE\niwUHBxMTE0NAQECrtpdEL4SXSk9PJywsjNjYWJRS7g5HuIjWmpycHNLT0xk4cGCr9iFNN0J4qbKy\nMiIjIyXJ+zilFJGRkW365iaJXggvJkm+Y2jr79mrE31Gfil/+d9BjmYXuTsUIYTwWF6d6DMLynl1\n7WFScordHYoQHU5OTg7x8fHEx8fTq1cv+vbta3teUVHRrH387Gc/4+DBg81+z7fffpuHHnqotSF3\nWF59Mbb2y4zMby5E+4uMjGTnzp0AzJ07l9DQUB577LF662it0Vrj52e/Tvnuu++6PE7h5TX62mYr\nSfRCeI7Dhw8zcuRIbr31VkaNGkVGRgb33nsvCQkJjBo1ij//+c+2dS+44AJ27txJVVUV3bt3JzEx\nkXHjxnHeeeeRlZXV6PscO3aMqVOnMnbsWKZNm0Z6ejoAixcvZvTo0YwbN46pU6cCsGfPHiZOnEh8\nfDxjx47l6NGjrjsAHsjLa/RGppc8Lzq6p5btY//JAqfuc2SfrvzpR6Nate2BAwdYuHAhCQkJAMyf\nP5+IiAiqqqqYOnUqN9xwAyNHjqy3TX5+PhdffDHz58/nkUce4Z133iExMdHhe9x3333cfffd3Hrr\nrSxYsICHHnqITz75hKeeeop169YRHR1NXl4eAG+++SaPPfYYN998M+Xl5egOVjv0kRp9x/qlCeHp\nBg0aZEvyAIsWLWL8+PGMHz+epKQk9u/ff9Y2nTt3Zvr06QBMmDCBlJSURt9j8+bNzJo1C4A77riD\n9evXAzBlyhTuuOMO3n77bWpqagA4//zzeeaZZ3j++edJS0sjODjYGR/Ta3h3jd5M9DWS50UH19qa\nt6t06dLFVk5OTuaVV15hy5YtdO/endtuu81un/DAwEBb2d/fn6qqqla991tvvcXmzZv54osvGD9+\nPDt27OD222/nvPPOY/ny5Vx11VW88847XHTRRa3avzfy7hp93eVYt8YhhHCsoKCAsLAwunbtSkZG\nBl999ZVT9jt58mQ++ugjAD744ANb4j569CiTJ0/m6aefJjw8nBMnTnD06FEGDx7Mgw8+yDXXXMPu\n3budEoO38IkavbTcCOG5xo8fz8iRIxk+fDgDBgxgypQpTtnvG2+8wc9//nOee+45oqOjbT14Hn74\nYY4dO4bWmiuuuILRo0fzzDPPsGjRIgICAujTpw9z5851SgzeQnlC+3ZCQoJuzcQjB04VcNVf1/Pm\nreOZMaa3CyITwnMlJSUxYsQId4ch2om937dSarvWOsHBJjY+0XTjAecqIYTwWN6d6GubbqSNXggh\nHPLuRG8+So1eCCEc8+5Eb6vRCyGEcMTLE31tG72keiGEcMS7E735KHleCCEc8+5EX1ujl8YbIdqd\nO4Ypbk9LlizhwIEDtue1A7A15vDhw8THx7s6tBbz7humzEep0QvR/nx9mOIlS5bg5+fH8OHD3R1K\nm3l5jd54lEQvhOdw5TDFa9euZdy4ccTHxzN+/HiKi4tZvXo1U6dOZebMmcTFxfHEE0+wcOFCJk6c\nyNixY22Dozka1tje8vXr17NixQoefvhh4uPjbftYvHgx5557LsOGDWPTpk2NHofS0lJmz57NmDFj\nGD9+PN9++y1gf8jkwsJCpk+fzrhx4xg9ejSffPKJE34Tdby8Ri/DFAsBwJeJcGqPc/fZawxMn9+q\nTV01TPELL7zAggULmDRpEkVFRbZRKHft2kVSUhLdunUjNjaW++67j61bt/LSSy/x+uuv8+KLLzoc\n1tjR8hkzZnDDDTdw3XXX2d5fa82WLVtYunQpf/7zn1m5cqXDY/Dqq68SFBTEnj172LdvHzNmzCA5\nOdnukMmff/45sbGxfPnll7Zj4Uw+UaOvkSq9EB7FVcMUT5kyhQcffJDXXnuNgoIC/P39AZg0aRLR\n0dEEBwcTFxfHlVdeCcCYMWNs+3E0rLGj5fb8+Mc/bjQ+qw0bNnDbbbcBMGrUKPr06cPhw4ftDpk8\nduxYVq5cSWJiIhs3bqRbt26N7rulvLpG7+cnHemFAFpd83YVVw1T/MQTTzBz5kyWL1/O5MmTWbNm\nDQBBQUG2dfz8/GzP/fz8Wj3csT21+23LMMqOhkzetm0bK1asIDExkenTp/P44487LW6vrtH7SY1e\nCI/nzGGKjxw5wtixY5kzZw7jx49vUY8dR8MaO1oeFhZGYWFhq2O98MIL+fDDDwFjQLKMjAwGDx5s\nd8jkEydOEBoayu23386jjz7KDz/80Or3tcfLE72R6WXiESE8l3WY4jvuuKNNwxS/+OKLjB49mrFj\nxxIaGsoVV1zR7G3feOMNFixYwNixY/nPf/7Dyy+/3OjyW265hWeffbbexdiWeOCBBygtLWXMmDHc\neuutLFy4kMDAQP79738zatQo4uPjOXToELfddhu7du2yXaB99tlnnVqbBy8fpjirsIxz563h6etG\nc/vkAS6ITAjPJcMUdywddphifxkCQQghmtTsRK+U8ldK7VBKfWE+j1BKrVJKJZuP4ZZ15yilDiul\nDiqlrnRF4GBpupG2GyGEcKglNfoHgSTL80RgjdZ6CLDGfI5SaiQwCxgFXAW8qZTyd0649Ukbvejo\n5Ntsx9DW33OzEr1SKga4Gnjbsvha4D2z/B5wnWX5Yq11udb6GHAYOLdNUTqKy4xeet2Ijig4OJic\nnBxJ9j5Oa01OTo7t5rDWaG4/+r8CvwPCLMuitdYZZvkUEG2W+wLfW9ZLN5c5nb+tRi//0UXHExMT\nQ3p6OtnZ2e4ORbhYcHAwMTExrd6+yUSvlLoGyNJab1dKXWJvHa21Vkq1KNsqpe4F7gXo379/Sza1\nkaYb0ZEFBAQwcOBAd4chvEBzmm6mADOVUinAYuBSpdQHQKZSqjeA+Vg7AtEJoJ9l+xhzWT1a6wVa\n6wStdUJUVFSrgpchEIQQomlNJnqt9RytdYzWOhbjIutarfVtwFJgtrnabOBzs7wUmKWUClJKDQSG\nAFucHjl1NXrJ80II4VhbxrqZD3yklLoLSAVuAtBa71NKfQTsB6qA+7XW1W2O1I7aIRCqpe1GCCEc\nalGi11qvA9aZ5RzgMgfrzQPmtTG2Jvn7ycVYIYRoilffGVs7leAH3x93cyRCCOG5vDrR1zpdVO7u\nEIQQwmP5RKIHSM0pJjZxOV/uyWh6ZSGE6EB8JtHvPVEAwLLdJ90ciRBCeBafSfQr950CoLjcJR18\nhBDCa3n1VIIAEwaEExzgx7JdRk3+eG6JmyMSQgjP4vU1en+lpB+9EEI0wutr9FtScus9l5H8hBCi\nPq+v0TckaV4IIerzuUQvhBCiPq9P9BcM7lHveWqOXIwVQggrr0/0w3qFERrk9ZcahBDCZbw+0Xfy\nV1RW17g7DCGE8Fhen+gD/Pyoku6VQgjhkNcn+k7+0o9eCCEa4/WJPsDf6z+CEEK4lNdnyQB/5e4Q\nhBDCo3l9ou/k5/UfQQghXMrrs2TtdIJWWmv+8c0RcmRCEiGE8P5E72cn0e9My+O5Lw/w2Me73BCR\nEEJ4Fq9P9P7q7ERfWW30wiksq2rvcIQQwuN4f6K38wns5H4hhOiwvD7R+9nJ6oVllYCMZCmEEOCj\nif6xj3cDsD31THuHI4QQHsfrE729Xje5xRVuiEQIITyT1yd6e71uhBBC1PH6RG+v140QQog6Xp/o\npUIvhBCN8/5EL5leCCEa5fWJXppuhBCicV6f6GVMMyGEaJzXp8ne3To3+npVdQ13v7eVnWl57RSR\nEEJ4Fq9P9CN6d2309dTcElYnZfHIf3a2U0RCCOFZvD7RN5cMhyCE6KiaTPRKqWCl1Bal1C6l1D6l\n1FPm8gil1CqlVLL5GG7ZZo5S6rBS6qBS6kpXfoCm1F6q1VpSvRCiY2pOjb4cuFRrPQ6IB65SSk0G\nEoE1WushwBrzOUqpkcAsYBRwFfCmUsrfFcE3h5JeOUKIDq7JRK8NRebTAPNHA9cC75nL3wOuM8vX\nAou11uVa62PAYeBcp0bdAq+tTXbXWwshhEdoVhu9UspfKbUTyAJWaa03A9Fa6wxzlVNAtFnuC6RZ\nNk83lzXc571KqW1KqW3Z2dmt/gBNWfLDCUDa6IUQHVezEr3WulprHQ/EAOcqpUY3eF3TwlyqtV6g\ntU7QWidERUW1ZNOzvP7Tc9q0vRBC+LIW9brRWucBX2O0vWcqpXoDmI9Z5mongH6WzWLMZS4zaWBk\nk+vItVghREfVnF43UUqp7ma5MzANOAAsBWabq80GPjfLS4FZSqkgpdRAYAiwxdmBW0WFBfHrqYMb\nXUdL440QooPq1Ix1egPvmT1n/ICPtNZfKKW+Az5SSt0FpAI3AWit9ymlPgL2A1XA/VrrateE33xp\nuaXuDkEIIdyiyUSvtd4NnNUIrrXOAS5zsM08YF6boxNCCNFmHebOWCGE6Kgk0QshhI+TRC+EED5O\nEr0QQvi4Dpfo3/j6MEeyi5peUQghfESHSvT5pZW88NVBbv7H9+4ORQgh2k2HSvTXvr4BgLJKt3fr\nF0KIduMzib45oxGn5JQAMja9EKJj8ZlEX6s5CV/SvBCiI/GZRF9bSZ86rGeT65ZUGE03y3adJC23\nxJVhCSGE2/lMoq/VNbg5w/cYHli0g5lmu70QQvgqn0n0tU02cVGhLdruTEmlC6IRQgjP4TOJXggh\nhH2S6IUQwsf5ZKIf2btri7eJTVzOn5ftd0E0QgjhXj6Z6G9KiGlynW0puWcte2fjMVeEI4QQbuWT\nib45bvj7d+4OQQgh2kWHTfSNmfzsGp5ats/dYQghhFP4XKJ3xugGpwrKeHdjStt3JIQQHsBnEr11\n5APVnHEQhBCig/CZRG+tyMugZUIIUcdnEn0tqcwLIUR9PpfoW1OZT/x0t/MDEUIID+Ezid5eG33v\nbsHN2nbx1jQXRCSEEJ7BZxK9VW0b/bSR0W3eV3JmIafyy9q8HyGEcBefSfSOWmwGRIa0ab/TXv6W\nyc+tAeBEXinZheVt2p8QQrQ3n0n0VtbulY9MG+q0/U6Zv5aJ81Y7bX9CCNEefCbRO+ps4yfdcIQQ\nHZzPJHp7WtMDp6q6xvmBCCGEG/l0om+N33+6x90hCCGEU/lcoteWy7KtabX59Id0J0YjhBDu53OJ\n3pH+ES3vfSPNOEIIX+BziV5ZLstqXVerH9O3W4v3VS1j5gghfECTiV4p1U8p9bVSar9Sap9S6kFz\neYRSapVSKtl8DLdsM0cpdVgpdVApdaUrP4D9mOvKbcnVabmlbQ9GCCHcrDk1+irgUa31SGAycL9S\naiSQCKzRWg8B1pjPMV+bBYwCrgLeVEr5uyJ4ezTaKWPSA+SXVjh8bcYr67nujY3OeSMhhHChJhO9\n1jpDa/2DWS4EkoC+wLXAe+Zq7wHXmeVrgcVa63Kt9THgMHCuswM/i4Mrr23pRp9TVJfoGw59vD+j\ngJ1pea3fuRBCtJMWtdErpWKBc4DNQLTWOsN86RRQO7BMX8A6Sli6ucy1dNt629hz7/vbbeW9Jwqc\ns1MhhGhnzU70SqlQ4FPgIa11vaynjepuixpMlFL3KqW2KaW2ZWdnt2TTxvfr4B5ZjebVW85p9X4r\npAeOEMJLNSvRK6UCMJL8h1rrJebiTKVUb/P13kCWufwE0M+yeYy5rB6t9QKtdYLWOiEqKqq18Z/F\n2o9eNzj39AwLavV+rd8SDpyS2r0Qwns0p9eNAv4JJGmt/2J5aSkw2yzPBj63LJ+llApSSg0EhgBb\nnBeyw0Abf9nhaDjNs2jzcVu5uLy6TfsSQoj21Jwa/RTgduBSpdRO82cGMB+YppRKBi43n6O13gd8\nBOwHVgL3a63dkhnr9alvWcvSWT7e7viO2XnL9/PlngyHrwshhDt1amoFrfUGHA8OeZmDbeYB89oQ\nl9O0tSZvd58NdvnW+mPAMVLmX+309xJCiLbynTtj7XSe1+Y/Z7Pm+R+On3H6/oUQwpl8J9FbOKrD\n1y4/NzaCQP/Wf/Tr39xkK+eV1L+p6qdvfc9ra5JbvW8hhHC2JptuvIaytsdbFmN/uStq+psOn2bT\nkRw2HclhwoBwzh/cw+nvIYQQLeWTNXp3STtTYivvSMsjv6SSE3kyXo4Qwr18MtHba7rRuv7yhAER\nLo/j4he/Zsr8tS5/HyGEaIxPJnpHrI01c2YMd8o+G+vVk1dS6ZT3EEKItvDpRG/tiNOwS6S/n7Fg\neK8wp72fK7pyCiFEW/l0ordq2HRjTcrjYlo+KUmt19Y2r4fN+9+nMm/5/la/jxBCtJZPJ3ql7I+M\n0LDHjWrDcJc/HK8bqvh3n+62u05hWSVPfrbXvLFKCCHal88lemtzjdbNm2GqxgVTBlrPHa9a+tUX\nl1fVW+9EXinplt46QgjhbD7Tj75enbyFA5xVVbt2bljrCMfzViQR1MmPbw5ls/bRS2y9cmT4BCGE\nq/hMjd5RqnbUdFO7XGsY3tt5F2SbUlBaybsbUziaXXzWa7vT83j0o13U1Mik5EII5/GZRF9LKRy2\n1zhqi3/48qFOj2P/yZaNWV9To7nrvW18+kM6p4vKnR6PEKLj8rlE76i5vbFm+Nquls70xe6mhy3e\nYRkQTQPZheW2shBCOIvPJPqWtNE3XMVZc8y21AtfHbSVy6vqD9mfX1LJ5zvPmphLCCFazGcSfVMc\nJfOGXS1vnzzA6e/9zsamu1U+8dleW3nZrpM8/NFOHly8kyPZRU6PRwjRsfhMorfX3FFvtMoG3S6t\nPW9qyz1Cg3jsimGuCbAJP6TWNeM8szyJtQeMKXhLK2TaQiFE2/hMonekOa0y7dl0Y227t558pKON\nEMJVfCbR28vVCvs1fWtir3+RVjfvzOAk1hu1qh1keq2NyU3ScuWmKiFE6/hMorfHUSVZ6wYXYy2v\ntWftvn5zkv1oj+UUE//nVVz4/Nfkl1Tyj2+OEJu4nGOnz+6HL4QQ9vhkoleOyk3NMdjAD09Oc1JE\n9llr9I5OSr9ZtMNW3pWex3NfHgDqt+kLIURjfC7Ra1rfD73hCJdhwa4dISKnuG6+2Yz8shZtm7hk\nN8mZhbz/XYpzgxJC+ByfS/RW1qTtaI5YTYMeOMraG6fOXRcMdG5w0OLml83HcmzlymrN9FfW8+Tn\n+5wdlhDCx/hcoq+f3Jtez1HbONRP+r27BbcpLmd44+sj9Z5XSVcdIUQz+FyiB0c9cOqWNkyPtgHO\nHGwLMLpv6ycncbVNR07Xe55dWM7iLcfdFI0QwtP4XKJ32NOmkfq9owu21uWT4yLbEpZLPfKfXSz4\n1uiNU1ldwy/e30bikj0yzr0QAvChRO+oR01LlzdnnWW/vqB5QbWTUwVlPLvC6I3zzw3HOF1kXOQt\nq6whu7CcN9cdbrSJSgjh23xm4pHmjFrZVG7XWjdrgu8xbZhj1tXe/y6Vk/mlAFz+l2+4cEgP1ief\n5vxBPRjTtxs70/KYMCDczVEKIdqTz9Toa7Xmfqd6PW2U/eXe4kReab2TW+3UhdU1Nfx19SF+8rdN\n7EzLc7C1EMIX+VyibziQWVO5WjsoN9fPpzi/26WrJGUUApBZUEZZZTXvf59KTY2mrLKav64+dNZQ\nyUII3+AzTTctH1++rqtNW+rtd54f26xhiN3lTEklAE98to+Y8M625a+tTeaNr4/QNbgT6WdK+evq\nZDoH+POLiwe5K1QhhIv4XI0e7LfXN2dcGWj5WDf9I0NatkE7q70pKymjbmpDretOAAdPFVJeadTk\nyyprzt6BEMLr+WSir9UwaTfZjOPjHVNyzSEXfvnBdtu0hW+uq38TVvqZEmITl7P3RD4AW1NyHY6s\nKYTwDk0meqXUO0qpLKXUXsuyCKXUKqVUsvkYbnltjlLqsFLqoFLqSlcF3tD5g3qYj3X93ev1uFFn\nP7enOb1uvFVmQd14Ova+1RSWVdomPPnbN0fYlpLLjX//jlfXJAPGdIfSTVMI79OcGv2/gKsaLEsE\n1mithwBrzOcopUYCs4BR5jZvKqX8nRZtI84dGMHhedOZHBfZouYXR3fJtkbyvOmt37gd1J8Eva5c\nbSbvtzfUXWtYvjuDzAKj1n8os5CM/FKGPbGS979PBWB7aq7MfiWEl2gy0WutvwVyGyy+FnjPLL8H\nXGdZvlhrXa61PgYcBs51UqxN6uTv+OM01gPHNgRCG2urAY28vyfwc9CN1BHrOqk5xl22X+zOILOg\njJ/87Tt++8kuZ4cohHCB1mamaK117Zx4p4Bos9wXSLOsl24ua1dDeoYBcE6/7s1av/78sb7L2nTj\nSJOfX0OR2Td/38kCqqprGDhnOYtkbB0hPFabq6DaqAa3uCqslLpXKbVNKbUtOzu7rWHUc+7ACL79\n7VRuTIixvF+D9zcfG9binXWTVGAnz6vdl1iaWlbtz7SVm/oi8+XeU3Ync6msrqG4vBqt4bkVSc4L\nVAjhVK3NRplKqd4A5mOWufwE0M+yXoy57Cxa6wVa6wStdUJUVFQrw3Csf2RIvaTdMJnZS+jOvMy4\n4fdTnbi39mMd397eKW9LSl0rXvqZUlu59tjd9Pfv+GL3SQCOZBdRUFbpijCFEC3Q2kS/FJhtlmcD\nn1uWz1JKBSmlBgJDgC1tC7GtmpHQHYxY2RY9w+rGrw8N8uz70hp2sWyKsjPEZ+2JdEtKLr/+tzH9\n4WUvfcNP3tzkjBCFEG3QnO6Vi4DvgGFKqXSl1F3AfGCaUioZuNx8jtZ6H/ARsB9YCdyvtfagrhlN\nj4fgiuFt9j7Vbr1MnWpHC8bEqW23byg5qwiAM8UVlFV60H8FITqQJquaWutbHLx0mYP15wHz2hKU\nM50XF0nf7p35zWVD7I7lonFNcvcFC749aisnLtljK5dU2E/qjfVaOufpVYzo3ZUvH7yQa1/fwDn9\nw5k7c5TzghVCOOR5VwydrFtIABsTL232DFGuHrEyZf7VtvJYDx7uuCHr/LYVVfaHSmjqom7tMAy7\n0vP516YUZ4UmhGiCzyf6+hpP4u19z+eieya38zs6x/WWdvcv92TYytaLs831i/e38dQymeBcCFfy\n7KuETlZ7Y2gnP2XpXum2cOjSzIu0gVQSQQHdVDFhlBCmSgmjlDBVQghlBFNBJ2U0S2kUNdqPGhQ1\nKDRGuRo/NIpyAijVQZQQRCmBlBNIke5MEZ3Nx2BKCaK5l6WtTTqpucWNrGnfV/uMbp5/+tEonvhs\nD0Gd/HnympEt3o8QwrEOlejHxXTn3oviuPP8WCqrPWekxnAKOK/raboUH2eAyqSfyqaXyiWaM0Sr\nM3RWFc3aT41W+Km2n7mqtaKIzpzRYeQSRq4O44wOI4eu5OowsnQ4mYSTqY2fYjqftQ/rxdliS7mq\nkeP+wffGTVdPXjOS7MJyarQmumuww/WFEM3ToRK9n5/i8RkjAMgrMZLnZSN6Orx5ytmCqICTO/mJ\n37cM80uDhW+xJWgHPVUeVAABUKn9ydARZBDJLj2IzJpw8nQoeYSSp7tQSAiFOoRCOlOoQ8yaeRDV\n+GHUwrVZj6/9qTF/NP7UEEQlnVU5IZQTTAVBVNJFlRJKKWHmY6gqJYwSwlUR4RTSW+Uy0i+VSAoJ\nUmf3iy/SwWTqcIK+6MNfA4LI1OH8981vmO4XzEkdybJNvTAveze7bX7ivNWAcU3jtTXJpJ0p4fkb\nxrFqfybJWYXcd8lgJ/1WhPB9HSrRW3UPCWTL45cR0SWQMgcXF9vKn2rGqqPw7Yt8GLCEBL9DsKCS\nlwKhXAdAyQjW14zhQE0/5sz+MRe/m8ZJ3YNLR/apd+dqyyg0imrAYWfGhuezZp/fNKGUEqXy6aVy\niSKPaHXG9tPzTB7xKp1efrkE51dye6C52TdwbVAgJ3Ukftv6E9QpjFQdTfW+GoapdI7rno2+60ur\nDgHw/A3juGfhNgDuu2QwL3x1gM92nGRj4qXN/QBCdEgdNtED9DSbBWobB+6b2sZaotYMU8c5328f\n/Pt9dgSto6sqhbUQofrzQfXl3DXrJi778DQpuhdHfjmTRxOXA/CHodNI00b5xRvHMe6p/7UtFpdQ\nFBFCkQ7hmO7dyHqabhTTR+XQV50mRmXTV52mjzrNhKp8ZvrvoZsqgY8/5KsgY4vS537Hx4ERHNfR\nsG4vM/3yOa6j0SVnHL7LG1/X3eiVXVjO8dxiJgyIYGdaHst3n+TxGSO8ct5fIZytQyf6Wp38/ep1\ne2ypGJXN9X7r4bUn+CrI7HuePZAvqs9jU80oXn/8QaY/Y9wgfNfoqzliJnRHunUOaHUsnkGRTyj5\nOpQkPaDeKzP69GLFnlN0o4h7RsHBpN30V1mcF1pAQGmKcZJct55Xa78NPP9HtgZ15ZjuTenHS7nP\nv4ZU3YvytD50odR2fWDm6xvIyC8jZf7VXPfGRgDmTB9BXkkF21PPcPnIaGpqNMUVVYQFe/vxFaJl\nJNE7MDkuotHXQylhuv8WePdNNgRtMBZ2vZDEzEtZXz2GjQ/eyeNmbf310NaP5XPwmas4/7m15BRX\ncOnwnraJQbzVij2nAMgnlBf3AZwPwMGoaFafNpqrLhvclZQjSQxUp3h1Whhr16wj1u8UAce+5ncB\nZpPWP19lXzBk624U/20IjxSHcMy/F+yrZLjKIEVHo4F7Fm5jW+oZdjw5jYXfpfLy6kNse+JyeoQG\n2WLKKixUV544AAAY7klEQVTj+ZUHeea60QQHtMv0CUK0K0n0dqz/3dR6iWDCgNoJtDQT1CH49DO2\nBn1m9IYpHMSLlTfyWc0FbLjzThYnNl5bbynrEMqT4yJsiX5M327sMaf78wWrk+quSaw5XAD05Yju\nS+KpPiytGgrAw+OH8vfVu4lVmbxyRRhLVm8gVp1iWnURF/nv5kb1LXz8ESvNX13VX/7Ab4sjOdip\nF522HSXrhypiVHey80vILiznD//dwwd3T2Le8iQ+33mSKYMjuf6cGDvRCeHdJNHb0S+ibsLvbU9c\nTmigPxz6ik8D5zLBLxkOdePT6gtZUn0hSx54iL5b03jcyc0t1qkPa5uZrz8nhgXfHuN0UTnTx/Ty\nqUTvyNJdJ23ll1cfAoJJ0gP4PngUf682vnV1ufAcHli0gxDKeH5qCF9+s4k4dZIrg4oIKjjEdf4b\nCft6lTEuRxDUvPN7Tqje3FMexenPJjAprxtpqjMBZf2oqurNsj0ZXDuuL35+0r4vfIMk+ib0yNwI\nq+dCxi4mhPeDKS9C/E8Zf7qaSzp3AqW45dz+Lnt/67VEjSYqLIjTReVcPDSKr/aeYld6PpcMi2Ld\nQWNM/97dgsnIb3qCEW/3R8twyrVdZUsIJidsBMtrjF5UZXEDeevEMUBz/dAg0pN3E+eXwfldztC3\nOp2hFan0TdrBT3UVPw0CvppL+ZowBlb0JG3bSEJ6D+WpTRXcf+OVjBh5DoU6SNr3hVeSRO9Ixm5Y\n/Sc4sha694dr34CxN4O/8Yc+so/9ze69KM4pg6QZPeKtz+rTGrra+RYxrFdYh0j0VtYx9A9lFtrK\nldW1R1CRp7qxVQ9na/VwNlR3Jqe4nLLKGvyp5pr+VeSnJzFQZXB1VDGlpw4Rl72VrieW8Xog8Plr\n8DkU63DKooYQNWAkKfSm76AxBPQcCt0HQKdAhPBUkugbKsqGVU/CrsUQ3A2ufBYm3g2dgpreFmw3\nZDXXhUN6sD75tMPXFZZavbY/MMHI3l1tNXqri4dG8c0hY3m3zgHkl/r+JCAfbq6b0nDLsbpJUqot\n9wpU12jbtY9q/EnVkeysCWId8VT27c8HacehEv5wRSyfrPqWe0bWMDQgk4N7d5BQnEv4vmXEluXA\ndmN/WvlD9/6oyMEQORgiB5mPg6FrX/DrYENKCY8jib6W1kZy/2oOVBTDlN/ABQ9D5/Cmt3Vg6a+n\nUFbZ+M1Y79450VLzrKMsjfSWPG/XpLhItqeeYfOxXEb1qUv61m8W5/Tvbvdk4Mv2m6NlAnx7qO6z\nV2td79hYy9YRGmr8gzmo+/PYPnjhhpn8duduxoR045ErhvLgu19zVe9ifj+xEx+sWMtUXcCYokzK\nDq+nM5ZvVJ2CIWJQ/eRfezIIiZQxskW7kEQPkJcGy35jNNP0mwQ/ehV6Dm/zbsfG1E1OvinxUtuY\nL2sfvZjMgnLA6MPfyezRZ68nTcPkXj9BGU+01vTpbvQnHxQVyp3nx/KvTSlMjI2wJfcAf6lV1sou\nLCcksK4bpZ/loPo5OAHUHus9J/JRQAGhfJQRyh0DLuDlqkg+rQhh8Z2TOX/+GkaHlbLsp9G8uGgF\n1/UrZYh/Jnmpu+l2YAVKW8byD+5Wl/gj4owTQkQcRAyEkMa79wrREpLoD66E//4CaqpgxouQcJdL\nvmrXJmKAuKhQ4qJCz1rn3/dMIqvQOAHMGNObZbtO1ktCWkP3EKNd3k8puzV9resmJvf3U8yZPpzn\nvjxAbGQI42K6sSs9v15z0QWDe7DhsOOmI19lnSjdWqe2Nv04Yu9u2+O5JeaJQbG3MISq/hfwRkEh\nf09SHHl2BvGJy/GnmiO/HcWdLy0iTp3ijxODyEtLosvR9QTs/oh6v8ng7mbSj6tL/hFxED4QQnvK\nNwHRIh030VdXwdqnYeNfodcYuPE94+u0G4UFB9h6dbx04zievGYE/n6KyNAgsgrL8fODV2adw2c7\nTjCidxjRXY3rBl0CO9VL+tYUYM0Hs8+P5ZGPdhHRJZDwkADOlFTWe31YdBgHzYuZPcOCbCcdX7ct\n1fEwC7WUg3L9dc6ejL66pi55V+MPkYNYV3MO64A/Xn018eZ9FylPX8ZHq9czQGUyqWseZVmH8T9z\nlID0LbBvCWhLm1KnYOgWA936Qfd+0K2/+Wg+D+sD/h33T1ucrWP+byjNg09+ZjTVTPgZXDUfAjxr\nONzATn62Ccb/9bOJrD2QZXt+94VxAMydOYpJAyOZGBvO/pP5LNlxggGRISSbyVo3yPq1SV0Bf511\nDrPfqT9vu7+l3cKNw/R7jGdXHLCVz5TYHyraUVu/bukRDAjmd99UAOGkzL+N4bUngPlXc83La4is\nyuS966LIPn4A8tOIqsqE/DTjG2lxg7ullT907WM5EfQzTgy1J4VuMRAYcnYMwmd1vERfmAnvXw+n\nDxpt8RNmuzuiJkV3DbbbVz8ksBM/mWDcyTn7/FjOH9yDodFhRIUG8cn2dGbG92H5buOGI63rapzW\n/K81fPHABVzz2gY0sPw3F3D1qxvqTchivXmro3pmeZKtfIflBFlYVtfmbq3pO/N47c0sA7rBkGlM\n/GcFEEfK/Kt5e/1RDp4q5IXrhpJ14ghFp44RF5BLVW4qVWeOE1x8Eo5/B/mfgG4wlmlID8u3gP71\nTwrd+xlNR9I85DM6VqLPOw4LrzWS/a2fwKCp7o7IaZRSDI0OAyC2Rxe2PzkNMNr6X/rfIW6e2M82\nGUh8v+5EdDH6fQ/uGWr7e9bauCGr1j0XDuSt9ccAuHJUtG02KFFn1oLvbeXffbrbVt5xPM9WdtY8\nB9b9aK1tJ58XbhzH5H+kUqP9SJk/m1l/28S21DOkzL+ak3ml7E3P5Yp+2uh0kJ9m/B3kpxnPsw9A\n8iqoajANZGDo2cnfelIIjZZuo16k4yT608lGkq8ogjs+g37nujuidhETHsLBZ6bbnq999GIG9uiC\nUooP755EQmy4ba7X+rM5aeZMH8Fb64+hNfzj9gRizeaEPXOvYMxcYxjld382kZ+9u7XdPo8ns3Zf\nveWtuhPAwDkrbOWF36XYymeKmzdzWC1Lc3+96ycn8kptr2UVltmuOVRW13DtGxvJLiwnZf7V/G1H\nOf+3MpfD8x6hpLKa7alnmDqsp/H1oySn7gSQn17/pJC+FUobXMfwDzTuEbA1DTU4KXSNkZvIPEjH\nSPT5J+C9Hxk9a+5cblx87aCsvX2mDO4BGF0y/3LTOC4d3pOuwQHclBDDbZMH2Gr6Y/p2q7cP6zAA\nU4fVTRoS6O9HhQdN0eiJrEM3nPP0Klv5F+9vs5U3HbHfC8p6Ydf6JSHPcv3gQEbdncHLd2eQbZ4Q\nNiSf5v9WGtccvjmUzfvfp7LuYDbf/PYSUnJKmP3OFnb96QoKQ4bz3M4DvHTjL+uP5FleaDkBHLec\nCNLgyNdQmEH9KzsKwnpZEn9f47pBWG/jp2tvCO0lJ4N24vuJvqwA/n0TlBfBz1dCr9Hujsgj/Xh8\n3aiNz98wzlb+9FfnMchycrhyVDQA14ztzUVD6g+/vPNP0xj5x68AWPfYJVzy4jqg43bhbAlrs9hP\n39psK8daRkN9YNEPtvLONGvTUN1+rKm2ynJisN6fsTop0/bt4+kv9rPZvIP40Y924qcU/9ufyeS4\nSEb36cr1b27ifw9fRFyPLrx7IIjZ519u675bT1UFFJyoS/62x+Nw4gdI+gKq7fTi6hJlJPywaOPE\nENrL6D7apQd06Wm+3tO4ZiBNRa3m24m+uhI+vhOykuDWjyXJt8KEAXU37ux96kqCzT/y13863rZc\nKbhseDQhgXX/nWJ7dLGVP7h7ki1hvXaLMdKkaDnryeCXH2y3la95bYOtbO1JtS2lbggI66TsVZY7\nsU8XVdguKH9/NJee5jWaJz/by9VjjFnErnj5W/4wYwTzViQxb0USB5+5iitf/pa/3TaBEb27svlo\nDvH9uxMUMdDo72+P1kbzT8FJo/ZvfSzKhMJTkLkPirLOvnAM4NfJuIDcJco4CYSaJ4Han3onhx7N\nHrKko/DdRK81LH8EjqyBma/B4MvcHZHXCw2y/9/l2HP1Z+ca3NP4BnDZ8J6EBdff5kfj+tgS/Wf3\nT7HNBnXR0Kh6wxSItlu8Nc1Wrp13F+Dj7em2srWmX6M11ZavB+VVdQn3VEHdsA4fbUsnJaeE6a+s\n59NfncfN5gXpY8/NYOCcFdw2uT/PXDeGV1Ync+7ACM4bFEl2UQX+fqFE9BrdeIWrpgZKc6E42/gp\nyoLi00YX0uJsYyyq4mzIPWKUG15ErhXcrf6J4KyTgeV5UFef72Hku4l+w8vww0K48DEYf4e7o+kw\nDj0z3TaMwD/vnGh3nbDgThSWVRHfr26IiIU/P9dW65/7o5HMXbYf6Fg3brmDtd2/pKKa1JwS2/PV\nSXX98zdamt6yLb+PA6fqrgnsTjdOGh98f5wHLh1izh9g3Aswcd5qW/m2tzfTLyKE5348ho2HT1NR\nXcPUYT3JLiynuLyK2B49jARMMwYILC+qOynYTg61ZfMkkX0QUjYYJxB7/IOa+U0hyhifyAtvRvO+\niJvj0Few5ikYfQNc+oS7o+lQ7LbfAv/3kzG2wdu+n3OZrf04sksgOWbvk4U/P5e9J/O5c8pAW6L/\nfs5lxD1u9Fo58PRVDH9yJQBfP3YJU81rAML1rAn91TXJtvIf/rvXVr7rvboLyr+3dDU9aNm2qrrG\ndr3muR+P4da3jesRDU8Gw574kvKqGlLmX81/th7n2OkSEqcPp6SiCoWic+1YRUGhxo+jJiOr6kqj\nd1FRVoMTQ+23hmyjGam2CanG3mivyhiHqEsUdI4wBj0MCbeUzceGzwM629lX+/G9RJ93HJbcA73G\nwrWv+/xXMm9x88S6G766WJqA1v32EtsInxcNjeKiocYF3hW/uZDd6Xn4+SlW/OZC1h3KIjjAn5nj\n+rB010l6dQ3mZ1NieXdjCn/60Ui+2J3B9mYMZSBc53RRXU3f2tX0yr9+aysP/sOXtvLfvzliK2+1\nXE+ortGUV9VdU/j9p3sASJw+3HaxP2X+1dz+z82sTz7NsedmsHTXSZbtyuDt2Qmk5ZaQVVjOhAHh\nVFbXUFFVY/yf8w8wLviG9Wr6w2gNZfkNTgYNvjmUnIEzKXDyByjJtX+xuZZ/EHTubiT94O5Gufax\nbwKMvbHpmNpAOetmjrZISEjQ27Zta3rFptRUw7+ugVN74Jfrm3eWF16lvKqarIJy+kWEUFVdQ2Zh\nOX27d2bfyXzmLt3H32+bQHhIIAdOFTKidxiP/3cPi7akMfu8ARzOLmLj4RwSBoQ3a3wb4X7WHluv\nzIrnwcU7AWNe5wuf/xowpvtMeMb4NrAx8VKmzF8LwObHL+O6NzaSkV/GnrlXsHx3BolL9pA8bzr7\nTxbw8H92svqRizldVM6iLWk8ePkQyquqSc4sYnSDLsXNUlFiXHAuzTUeS3LNcp7xvCzPKJeZz0vz\njfKw6fDjBa06Pkqp7VrrhCbX86lEv+k1+N8TcN3fIf6Wtu9P+ISi8ipCgzqhtWbtgSwuHW70/V+2\nO4Ppo3txKr+M/+44wUVDo3j6i/1sTz3DhAHh8g3By/UIDeR0kdEsaB2x9e4LBvL2BuOO7yeuHmG7\nw/iP14xk3aFsvj2UzdPXjqJ7SCAPLNrBv++eRN/wzlz8wjpWPnQhsZFduPf97bx04zh6hAbywfep\n/GRCDCGBnTicVcTAHl3w91Nore2OdHoWrVvd8tDxEv2ZFHhjMsRdArcskiYb0SpaaxZ+l8rMcX3Y\nmZ7Hq2uS+dOPRuGnYObrG3ng0sEs353B0dPFDOzRhWOni90dsnAj6wmktgsq1I0fNS6mGx/eM5nR\nf/qKhy8fyq8uGUTCM6v47ZXD+OmkATz9xX5um9yfwT3DWvX+HSvRaw0f/ATSNsP9m43R+YRoB0Xl\nVRSWVRIeEkhhWRXZheXERHTmeE4Je07k06d7Zz7dns7aA1ncNnkA21Nz2Zpyhl9cFEd5VQ3/2pQi\ndxR3cEN6hrLqkYtbtW1zE71vXIzd84nRX37685LkRbsKDepku78gOMDfNijc6L7dbO28Fw+Ncrj9\n3JmjAKip0VTVaAI7+XGmuILD2UUM6xXG2qQsNh/L4dZJA/D3Uzz35QFuSohh3cFsPtmezv1TB/HG\n18ZFzavH9Gb5ngxXflzhAmVVdm4QczKX1eiVUlcBrwD+wNta6/mO1m1Tjb4kF16fCOGxcNf/wM+/\nyU2E8DXVNRp/P0VVdQ1rD2QxbWQ0SikOZRYyNDqMovIqFm85zm2TB7DjeB4r92bw8LShvPi/g/xn\naxpLfjWF+/69nbTc0noXPa+N78PnO42hruN6dOGoNFU5XVyPLqx97JJWbevWphullD9wCJgGpANb\ngVu01vvtrd+mRL/ycdj8N/jFehniQAgXKa2oJrCTH/5+ioqqGjr5KTRw4FQB0V2DiQgJZN/JAiJC\njdnLtqWcoV9ECLnFFRzJLmJk767sOZHPh5tTufuCOM6UVPDUsv3MPm8APUKDeGnVIaaP7sWoPl15\n8X/GjVbXn9OX/+44ARjfir4x75zuF9GZtFwHd8R6ob7dO7Mx8dJWbevuRH8eMFdrfaX5fA6A1vo5\ne+u3OtHnHYfXJsDYm+DaN9oQsRDCE1VU1VBSUUX3kEDKKqvJLjS61pZWVHPsdDEj+3SlqLyKrSm5\nTB3Wk5KKKtYdzGbGmN4Ul1fx+c6T3DyxH1mFZXyxK4M7zh/AkaxiVu47xb0XxbE99QzrD2Xzq0sG\nsXxPBj+knuHRK4bxzw3HOJFXyu+vGs6zK5LILCjjqZmjeOKzvaTllvDyzfE8/t+9nC4q54mrR/Dy\nqkMUV1Tz4/F9WfKDcXJq7jegpb+ewtiY7k2uZ4+7E/0NwFVa67vN57cDk7TWv7a3fqsTffYhWJkI\nM1+VtnkhRIfj8RdjlVL3AvcC9O9/9jR5zRI1FG5f4sSohBDC97hqgOcTQD/L8xhzmY3WeoHWOkFr\nnRAV5bhXghBCiLZxVaLfCgxRSg1USgUCs4ClLnovIYQQjXBJ043Wukop9WvgK4zule9orfc1sZkQ\nQggXcFkbvdZ6BbCiyRWFEEK4lEzCKIQQPk4SvRBC+DhJ9EII4eMk0QshhI/ziGGKlVLZQGobdtED\nON3kWu4lMTqHxOgcEqPzuDPOAVrrJm9E8ohE31ZKqW3NuQ3YnSRG55AYnUNidB5viFOaboQQwsdJ\nohdCCB/nK4m+dVOoty+J0TkkRueQGJ3H4+P0iTZ6IYQQjvlKjV4IIYQDXp3olVJXKaUOKqUOK6US\n3RxLilJqj1Jqp1Jqm7ksQim1SimVbD6GW9afY8Z9UCl1pYtiekcplaWU2mtZ1uKYlFITzM92WCn1\nqlJKuTjGuUqpE+ax3KmUmuHmGPsppb5WSu1XSu1TSj1oLveYY9lIjB5zLJVSwUqpLUqpXWaMT5nL\nPeY4NhGnxxzLFtNae+UPxqiYR4A4IBDYBYx0YzwpQI8Gy54HEs1yIvB/ZnmkGW8QMND8HP4uiOki\nYDywty0xAVuAyYACvgSmuzjGucBjdtZ1V4y9gfFmOQxjPuSRnnQsG4nRY46lub9QsxwAbDbfx2OO\nYxNxesyxbOmPN9fozwUOa62Paq0rgMXAtW6OqaFrgffM8nvAdZbli7XW5VrrY8BhjM/jVFrrb4Hc\ntsSklOoNdNVaf6+N/7kLLdu4KkZH3BVjhtb6B7NcCCQBffGgY9lIjI64I0attS4ynwaYPxoPOo5N\nxOmIW+JsCW9O9H2BNMvzdBr/j+1qGlitlNqujGkSAaK11hlm+RQQbZbdGXtLY+prlhsud7UHlFK7\nzaad2q/ybo9RKRULnINRy/PIY9kgRvCgY6mU8ldK7QSygFVaa488jg7iBA86li3hzYne01ygtY4H\npgP3K6Uusr5ontE9qouTJ8Zk+htGk1w8kAG85N5wDEqpUOBT4CGtdYH1NU85lnZi9KhjqbWuNv9O\nYjBqvaMbvO4Rx9FBnB51LFvCmxN9k/PStiet9QnzMQv4L0ZTTKb59Q3zMctc3Z2xtzSmE2a54XKX\n0Vpnmn9oNcBb1DVruS1GpVQARgL9UGtdOyO9Rx1LezF64rE048oDvgauwsOOo6M4PfVYNoc3J3qP\nmZdWKdVFKRVWWwauAPaa8cw2V5sNfG6WlwKzlFJBSqmBwBCMizbtoUUxmV+pC5RSk80eA3dYtnGJ\n2j960/UYx9JtMZr7/CeQpLX+i+UljzmWjmL0pGOplIpSSnU3y52BacABPOg4NhanJx3LFnPHFWBn\n/QAzMHoXHAH+4MY44jCuuu8C9tXGAkQCa4BkYDUQYdnmD2bcB3HRlXhgEcZXzEqM9sG7WhMTkIDx\nn/oI8DrmjXYujPF9YA+wG+OPqLebY7wAozlhN7DT/JnhSceykRg95lgCY4EdZix7gT+29u/Exb9v\nR3F6zLFs6Y/cGSuEED7Om5tuhBBCNIMkeiGE8HGS6IUQwsdJohdCCB8niV4IIXycJHohhPBxkuiF\nEMLHSaIXQggf9/8kmTyKMPaJYwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bc5da58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "kernel_size = 5\n",
    "num_kernels = 2\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, K=kernel_size, N=num_kernels, char2idx=char_to_idx, \n",
    "          idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
