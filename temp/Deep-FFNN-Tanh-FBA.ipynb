{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#         y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#         y *= 2.0 # uni-var/ std\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#             y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#             y *= 2.0 # uni-var/ std\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= 2.0 # uni-var/ std\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= 2.0 # uni-var/ std\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3217, acc-0.0800, valid loss-2.3247, acc-0.0768, test loss-2.3213, acc-0.0797\n",
      "Iter-20, train loss-2.3187, acc-0.0800, valid loss-2.3243, acc-0.0770, test loss-2.3209, acc-0.0801\n",
      "Iter-30, train loss-2.3276, acc-0.0800, valid loss-2.3240, acc-0.0776, test loss-2.3207, acc-0.0805\n",
      "Iter-40, train loss-2.2811, acc-0.1400, valid loss-2.3236, acc-0.0784, test loss-2.3203, acc-0.0820\n",
      "Iter-50, train loss-2.3104, acc-0.1400, valid loss-2.3232, acc-0.0786, test loss-2.3200, acc-0.0825\n",
      "Iter-60, train loss-2.3267, acc-0.0600, valid loss-2.3229, acc-0.0792, test loss-2.3197, acc-0.0833\n",
      "Iter-70, train loss-2.3178, acc-0.1200, valid loss-2.3226, acc-0.0792, test loss-2.3194, acc-0.0839\n",
      "Iter-80, train loss-2.3024, acc-0.2000, valid loss-2.3223, acc-0.0794, test loss-2.3191, acc-0.0845\n",
      "Iter-90, train loss-2.2876, acc-0.1800, valid loss-2.3220, acc-0.0800, test loss-2.3188, acc-0.0844\n",
      "Iter-100, train loss-2.3051, acc-0.1800, valid loss-2.3217, acc-0.0806, test loss-2.3186, acc-0.0847\n",
      "Iter-110, train loss-2.3600, acc-0.0400, valid loss-2.3214, acc-0.0808, test loss-2.3183, acc-0.0851\n",
      "Iter-120, train loss-2.3219, acc-0.1400, valid loss-2.3211, acc-0.0824, test loss-2.3180, acc-0.0859\n",
      "Iter-130, train loss-2.3185, acc-0.1400, valid loss-2.3208, acc-0.0828, test loss-2.3178, acc-0.0872\n",
      "Iter-140, train loss-2.3250, acc-0.0600, valid loss-2.3205, acc-0.0826, test loss-2.3175, acc-0.0878\n",
      "Iter-150, train loss-2.3083, acc-0.0800, valid loss-2.3202, acc-0.0830, test loss-2.3172, acc-0.0882\n",
      "Iter-160, train loss-2.3175, acc-0.0600, valid loss-2.3199, acc-0.0834, test loss-2.3170, acc-0.0888\n",
      "Iter-170, train loss-2.3152, acc-0.1400, valid loss-2.3196, acc-0.0838, test loss-2.3167, acc-0.0903\n",
      "Iter-180, train loss-2.3101, acc-0.0400, valid loss-2.3193, acc-0.0846, test loss-2.3164, acc-0.0910\n",
      "Iter-190, train loss-2.3191, acc-0.1000, valid loss-2.3189, acc-0.0856, test loss-2.3161, acc-0.0926\n",
      "Iter-200, train loss-2.3129, acc-0.1000, valid loss-2.3186, acc-0.0866, test loss-2.3158, acc-0.0939\n",
      "Iter-210, train loss-2.3347, acc-0.0800, valid loss-2.3183, acc-0.0870, test loss-2.3155, acc-0.0945\n",
      "Iter-220, train loss-2.3229, acc-0.0600, valid loss-2.3179, acc-0.0882, test loss-2.3152, acc-0.0948\n",
      "Iter-230, train loss-2.3371, acc-0.0600, valid loss-2.3177, acc-0.0886, test loss-2.3149, acc-0.0953\n",
      "Iter-240, train loss-2.3175, acc-0.0600, valid loss-2.3173, acc-0.0886, test loss-2.3146, acc-0.0964\n",
      "Iter-250, train loss-2.3151, acc-0.0800, valid loss-2.3169, acc-0.0896, test loss-2.3142, acc-0.0973\n",
      "Iter-260, train loss-2.3167, acc-0.0800, valid loss-2.3166, acc-0.0892, test loss-2.3139, acc-0.0990\n",
      "Iter-270, train loss-2.2958, acc-0.2200, valid loss-2.3163, acc-0.0898, test loss-2.3137, acc-0.0996\n",
      "Iter-280, train loss-2.3194, acc-0.0800, valid loss-2.3160, acc-0.0908, test loss-2.3134, acc-0.1011\n",
      "Iter-290, train loss-2.3152, acc-0.1600, valid loss-2.3157, acc-0.0908, test loss-2.3131, acc-0.1022\n",
      "Iter-300, train loss-2.2675, acc-0.2200, valid loss-2.3153, acc-0.0924, test loss-2.3127, acc-0.1035\n",
      "Iter-310, train loss-2.3143, acc-0.1200, valid loss-2.3150, acc-0.0922, test loss-2.3125, acc-0.1042\n",
      "Iter-320, train loss-2.3043, acc-0.1000, valid loss-2.3146, acc-0.0930, test loss-2.3121, acc-0.1053\n",
      "Iter-330, train loss-2.2726, acc-0.1600, valid loss-2.3143, acc-0.0952, test loss-2.3118, acc-0.1069\n",
      "Iter-340, train loss-2.3196, acc-0.0800, valid loss-2.3140, acc-0.0958, test loss-2.3115, acc-0.1073\n",
      "Iter-350, train loss-2.3061, acc-0.1200, valid loss-2.3136, acc-0.0966, test loss-2.3112, acc-0.1085\n",
      "Iter-360, train loss-2.3041, acc-0.1400, valid loss-2.3133, acc-0.0972, test loss-2.3109, acc-0.1094\n",
      "Iter-370, train loss-2.3085, acc-0.1000, valid loss-2.3130, acc-0.0980, test loss-2.3106, acc-0.1104\n",
      "Iter-380, train loss-2.2968, acc-0.0800, valid loss-2.3126, acc-0.0996, test loss-2.3103, acc-0.1118\n",
      "Iter-390, train loss-2.3257, acc-0.0800, valid loss-2.3123, acc-0.1006, test loss-2.3100, acc-0.1122\n",
      "Iter-400, train loss-2.3209, acc-0.1000, valid loss-2.3120, acc-0.1008, test loss-2.3097, acc-0.1130\n",
      "Iter-410, train loss-2.3305, acc-0.1000, valid loss-2.3117, acc-0.1026, test loss-2.3094, acc-0.1130\n",
      "Iter-420, train loss-2.3171, acc-0.1000, valid loss-2.3114, acc-0.1038, test loss-2.3092, acc-0.1142\n",
      "Iter-430, train loss-2.3021, acc-0.0800, valid loss-2.3111, acc-0.1040, test loss-2.3089, acc-0.1145\n",
      "Iter-440, train loss-2.3186, acc-0.0400, valid loss-2.3108, acc-0.1044, test loss-2.3086, acc-0.1149\n",
      "Iter-450, train loss-2.3141, acc-0.1400, valid loss-2.3104, acc-0.1062, test loss-2.3082, acc-0.1163\n",
      "Iter-460, train loss-2.3205, acc-0.0600, valid loss-2.3100, acc-0.1078, test loss-2.3079, acc-0.1173\n",
      "Iter-470, train loss-2.2985, acc-0.0200, valid loss-2.3096, acc-0.1084, test loss-2.3076, acc-0.1179\n",
      "Iter-480, train loss-2.2634, acc-0.1600, valid loss-2.3093, acc-0.1092, test loss-2.3072, acc-0.1188\n",
      "Iter-490, train loss-2.3094, acc-0.0800, valid loss-2.3089, acc-0.1102, test loss-2.3069, acc-0.1199\n",
      "Iter-500, train loss-2.3061, acc-0.1400, valid loss-2.3086, acc-0.1122, test loss-2.3066, acc-0.1203\n",
      "Iter-510, train loss-2.3032, acc-0.1000, valid loss-2.3082, acc-0.1128, test loss-2.3063, acc-0.1218\n",
      "Iter-520, train loss-2.2956, acc-0.1000, valid loss-2.3079, acc-0.1144, test loss-2.3060, acc-0.1226\n",
      "Iter-530, train loss-2.3032, acc-0.1200, valid loss-2.3076, acc-0.1146, test loss-2.3057, acc-0.1241\n",
      "Iter-540, train loss-2.2925, acc-0.1800, valid loss-2.3072, acc-0.1150, test loss-2.3053, acc-0.1255\n",
      "Iter-550, train loss-2.3299, acc-0.0400, valid loss-2.3069, acc-0.1158, test loss-2.3050, acc-0.1264\n",
      "Iter-560, train loss-2.3306, acc-0.1400, valid loss-2.3066, acc-0.1162, test loss-2.3047, acc-0.1275\n",
      "Iter-570, train loss-2.2947, acc-0.1800, valid loss-2.3062, acc-0.1174, test loss-2.3044, acc-0.1291\n",
      "Iter-580, train loss-2.3268, acc-0.1000, valid loss-2.3058, acc-0.1186, test loss-2.3041, acc-0.1304\n",
      "Iter-590, train loss-2.2987, acc-0.1800, valid loss-2.3055, acc-0.1196, test loss-2.3038, acc-0.1313\n",
      "Iter-600, train loss-2.2874, acc-0.1200, valid loss-2.3051, acc-0.1206, test loss-2.3034, acc-0.1320\n",
      "Iter-610, train loss-2.3049, acc-0.1000, valid loss-2.3047, acc-0.1220, test loss-2.3030, acc-0.1328\n",
      "Iter-620, train loss-2.3054, acc-0.1000, valid loss-2.3044, acc-0.1224, test loss-2.3027, acc-0.1331\n",
      "Iter-630, train loss-2.3046, acc-0.0800, valid loss-2.3040, acc-0.1232, test loss-2.3024, acc-0.1339\n",
      "Iter-640, train loss-2.2910, acc-0.1600, valid loss-2.3036, acc-0.1234, test loss-2.3020, acc-0.1349\n",
      "Iter-650, train loss-2.3026, acc-0.1000, valid loss-2.3033, acc-0.1250, test loss-2.3017, acc-0.1354\n",
      "Iter-660, train loss-2.3005, acc-0.1800, valid loss-2.3030, acc-0.1252, test loss-2.3014, acc-0.1359\n",
      "Iter-670, train loss-2.3097, acc-0.0600, valid loss-2.3027, acc-0.1272, test loss-2.3011, acc-0.1366\n",
      "Iter-680, train loss-2.3096, acc-0.1400, valid loss-2.3023, acc-0.1282, test loss-2.3008, acc-0.1377\n",
      "Iter-690, train loss-2.3030, acc-0.1400, valid loss-2.3019, acc-0.1288, test loss-2.3004, acc-0.1383\n",
      "Iter-700, train loss-2.3020, acc-0.1000, valid loss-2.3015, acc-0.1294, test loss-2.3001, acc-0.1391\n",
      "Iter-710, train loss-2.2750, acc-0.2400, valid loss-2.3012, acc-0.1296, test loss-2.2998, acc-0.1393\n",
      "Iter-720, train loss-2.3001, acc-0.1200, valid loss-2.3008, acc-0.1294, test loss-2.2994, acc-0.1394\n",
      "Iter-730, train loss-2.3154, acc-0.1000, valid loss-2.3004, acc-0.1294, test loss-2.2991, acc-0.1398\n",
      "Iter-740, train loss-2.3226, acc-0.0400, valid loss-2.3001, acc-0.1300, test loss-2.2988, acc-0.1401\n",
      "Iter-750, train loss-2.2915, acc-0.1200, valid loss-2.2997, acc-0.1298, test loss-2.2984, acc-0.1406\n",
      "Iter-760, train loss-2.2981, acc-0.0600, valid loss-2.2993, acc-0.1310, test loss-2.2980, acc-0.1412\n",
      "Iter-770, train loss-2.2991, acc-0.1400, valid loss-2.2990, acc-0.1316, test loss-2.2977, acc-0.1416\n",
      "Iter-780, train loss-2.3310, acc-0.0800, valid loss-2.2986, acc-0.1322, test loss-2.2974, acc-0.1419\n",
      "Iter-790, train loss-2.3098, acc-0.1000, valid loss-2.2983, acc-0.1338, test loss-2.2971, acc-0.1425\n",
      "Iter-800, train loss-2.2750, acc-0.1600, valid loss-2.2979, acc-0.1346, test loss-2.2966, acc-0.1427\n",
      "Iter-810, train loss-2.2764, acc-0.1800, valid loss-2.2975, acc-0.1348, test loss-2.2963, acc-0.1434\n",
      "Iter-820, train loss-2.2724, acc-0.2200, valid loss-2.2970, acc-0.1358, test loss-2.2959, acc-0.1446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.3039, acc-0.1000, valid loss-2.2966, acc-0.1360, test loss-2.2955, acc-0.1452\n",
      "Iter-840, train loss-2.2859, acc-0.1400, valid loss-2.2962, acc-0.1374, test loss-2.2951, acc-0.1453\n",
      "Iter-850, train loss-2.2701, acc-0.2600, valid loss-2.2959, acc-0.1384, test loss-2.2948, acc-0.1456\n",
      "Iter-860, train loss-2.2970, acc-0.1400, valid loss-2.2955, acc-0.1386, test loss-2.2944, acc-0.1458\n",
      "Iter-870, train loss-2.3229, acc-0.1800, valid loss-2.2951, acc-0.1392, test loss-2.2941, acc-0.1460\n",
      "Iter-880, train loss-2.3075, acc-0.1200, valid loss-2.2947, acc-0.1392, test loss-2.2937, acc-0.1467\n",
      "Iter-890, train loss-2.2819, acc-0.2200, valid loss-2.2943, acc-0.1394, test loss-2.2933, acc-0.1471\n",
      "Iter-900, train loss-2.2861, acc-0.1400, valid loss-2.2939, acc-0.1400, test loss-2.2929, acc-0.1472\n",
      "Iter-910, train loss-2.3103, acc-0.1400, valid loss-2.2935, acc-0.1402, test loss-2.2925, acc-0.1479\n",
      "Iter-920, train loss-2.3235, acc-0.1600, valid loss-2.2931, acc-0.1404, test loss-2.2921, acc-0.1481\n",
      "Iter-930, train loss-2.3020, acc-0.2200, valid loss-2.2926, acc-0.1410, test loss-2.2917, acc-0.1481\n",
      "Iter-940, train loss-2.2623, acc-0.1800, valid loss-2.2922, acc-0.1416, test loss-2.2913, acc-0.1487\n",
      "Iter-950, train loss-2.2671, acc-0.2000, valid loss-2.2918, acc-0.1416, test loss-2.2909, acc-0.1491\n",
      "Iter-960, train loss-2.2950, acc-0.1400, valid loss-2.2914, acc-0.1422, test loss-2.2905, acc-0.1492\n",
      "Iter-970, train loss-2.2787, acc-0.1600, valid loss-2.2910, acc-0.1420, test loss-2.2902, acc-0.1495\n",
      "Iter-980, train loss-2.2819, acc-0.2200, valid loss-2.2906, acc-0.1422, test loss-2.2898, acc-0.1505\n",
      "Iter-990, train loss-2.2961, acc-0.1800, valid loss-2.2902, acc-0.1420, test loss-2.2894, acc-0.1507\n",
      "Iter-1000, train loss-2.2656, acc-0.1600, valid loss-2.2897, acc-0.1424, test loss-2.2890, acc-0.1515\n",
      "Iter-1010, train loss-2.2903, acc-0.1600, valid loss-2.2893, acc-0.1430, test loss-2.2886, acc-0.1520\n",
      "Iter-1020, train loss-2.2603, acc-0.1800, valid loss-2.2889, acc-0.1430, test loss-2.2882, acc-0.1526\n",
      "Iter-1030, train loss-2.3032, acc-0.1400, valid loss-2.2885, acc-0.1430, test loss-2.2878, acc-0.1531\n",
      "Iter-1040, train loss-2.3037, acc-0.1400, valid loss-2.2881, acc-0.1450, test loss-2.2874, acc-0.1537\n",
      "Iter-1050, train loss-2.2710, acc-0.2200, valid loss-2.2876, acc-0.1468, test loss-2.2870, acc-0.1541\n",
      "Iter-1060, train loss-2.2712, acc-0.1600, valid loss-2.2872, acc-0.1468, test loss-2.2866, acc-0.1545\n",
      "Iter-1070, train loss-2.2656, acc-0.1800, valid loss-2.2868, acc-0.1470, test loss-2.2862, acc-0.1549\n",
      "Iter-1080, train loss-2.2884, acc-0.1800, valid loss-2.2863, acc-0.1470, test loss-2.2858, acc-0.1555\n",
      "Iter-1090, train loss-2.2756, acc-0.1800, valid loss-2.2858, acc-0.1482, test loss-2.2854, acc-0.1561\n",
      "Iter-1100, train loss-2.2679, acc-0.1800, valid loss-2.2854, acc-0.1484, test loss-2.2850, acc-0.1567\n",
      "Iter-1110, train loss-2.3037, acc-0.1200, valid loss-2.2850, acc-0.1490, test loss-2.2846, acc-0.1572\n",
      "Iter-1120, train loss-2.2837, acc-0.1600, valid loss-2.2846, acc-0.1498, test loss-2.2842, acc-0.1579\n",
      "Iter-1130, train loss-2.2858, acc-0.1400, valid loss-2.2841, acc-0.1506, test loss-2.2837, acc-0.1580\n",
      "Iter-1140, train loss-2.2920, acc-0.2000, valid loss-2.2837, acc-0.1506, test loss-2.2833, acc-0.1587\n",
      "Iter-1150, train loss-2.2925, acc-0.1200, valid loss-2.2833, acc-0.1508, test loss-2.2829, acc-0.1589\n",
      "Iter-1160, train loss-2.2910, acc-0.1200, valid loss-2.2828, acc-0.1514, test loss-2.2824, acc-0.1593\n",
      "Iter-1170, train loss-2.2852, acc-0.1000, valid loss-2.2824, acc-0.1516, test loss-2.2820, acc-0.1598\n",
      "Iter-1180, train loss-2.2696, acc-0.2200, valid loss-2.2820, acc-0.1526, test loss-2.2816, acc-0.1600\n",
      "Iter-1190, train loss-2.3016, acc-0.1400, valid loss-2.2816, acc-0.1532, test loss-2.2813, acc-0.1605\n",
      "Iter-1200, train loss-2.2830, acc-0.1400, valid loss-2.2811, acc-0.1542, test loss-2.2808, acc-0.1607\n",
      "Iter-1210, train loss-2.2931, acc-0.1000, valid loss-2.2806, acc-0.1540, test loss-2.2804, acc-0.1616\n",
      "Iter-1220, train loss-2.2993, acc-0.1400, valid loss-2.2802, acc-0.1546, test loss-2.2800, acc-0.1621\n",
      "Iter-1230, train loss-2.2833, acc-0.2000, valid loss-2.2798, acc-0.1558, test loss-2.2796, acc-0.1628\n",
      "Iter-1240, train loss-2.2947, acc-0.2000, valid loss-2.2794, acc-0.1566, test loss-2.2792, acc-0.1636\n",
      "Iter-1250, train loss-2.2509, acc-0.2000, valid loss-2.2789, acc-0.1564, test loss-2.2787, acc-0.1639\n",
      "Iter-1260, train loss-2.2955, acc-0.1200, valid loss-2.2784, acc-0.1570, test loss-2.2783, acc-0.1647\n",
      "Iter-1270, train loss-2.3026, acc-0.1000, valid loss-2.2779, acc-0.1576, test loss-2.2778, acc-0.1652\n",
      "Iter-1280, train loss-2.2602, acc-0.1600, valid loss-2.2775, acc-0.1586, test loss-2.2774, acc-0.1660\n",
      "Iter-1290, train loss-2.2415, acc-0.2200, valid loss-2.2770, acc-0.1592, test loss-2.2769, acc-0.1663\n",
      "Iter-1300, train loss-2.2615, acc-0.2000, valid loss-2.2765, acc-0.1596, test loss-2.2765, acc-0.1664\n",
      "Iter-1310, train loss-2.2607, acc-0.1600, valid loss-2.2761, acc-0.1596, test loss-2.2760, acc-0.1667\n",
      "Iter-1320, train loss-2.2906, acc-0.1400, valid loss-2.2756, acc-0.1596, test loss-2.2756, acc-0.1673\n",
      "Iter-1330, train loss-2.2500, acc-0.1800, valid loss-2.2751, acc-0.1610, test loss-2.2751, acc-0.1677\n",
      "Iter-1340, train loss-2.3099, acc-0.1200, valid loss-2.2746, acc-0.1614, test loss-2.2746, acc-0.1682\n",
      "Iter-1350, train loss-2.2402, acc-0.2400, valid loss-2.2742, acc-0.1618, test loss-2.2742, acc-0.1684\n",
      "Iter-1360, train loss-2.2791, acc-0.2000, valid loss-2.2737, acc-0.1622, test loss-2.2737, acc-0.1689\n",
      "Iter-1370, train loss-2.2441, acc-0.2600, valid loss-2.2732, acc-0.1628, test loss-2.2733, acc-0.1693\n",
      "Iter-1380, train loss-2.2920, acc-0.1000, valid loss-2.2727, acc-0.1638, test loss-2.2728, acc-0.1696\n",
      "Iter-1390, train loss-2.3144, acc-0.1200, valid loss-2.2722, acc-0.1638, test loss-2.2724, acc-0.1701\n",
      "Iter-1400, train loss-2.2399, acc-0.2400, valid loss-2.2717, acc-0.1646, test loss-2.2719, acc-0.1706\n",
      "Iter-1410, train loss-2.2837, acc-0.1800, valid loss-2.2713, acc-0.1654, test loss-2.2715, acc-0.1709\n",
      "Iter-1420, train loss-2.2808, acc-0.1200, valid loss-2.2708, acc-0.1654, test loss-2.2710, acc-0.1713\n",
      "Iter-1430, train loss-2.2653, acc-0.2400, valid loss-2.2703, acc-0.1660, test loss-2.2705, acc-0.1721\n",
      "Iter-1440, train loss-2.2890, acc-0.1000, valid loss-2.2699, acc-0.1668, test loss-2.2701, acc-0.1721\n",
      "Iter-1450, train loss-2.2515, acc-0.1600, valid loss-2.2693, acc-0.1674, test loss-2.2696, acc-0.1724\n",
      "Iter-1460, train loss-2.2493, acc-0.2800, valid loss-2.2688, acc-0.1680, test loss-2.2691, acc-0.1728\n",
      "Iter-1470, train loss-2.2811, acc-0.1000, valid loss-2.2683, acc-0.1682, test loss-2.2686, acc-0.1727\n",
      "Iter-1480, train loss-2.2477, acc-0.1600, valid loss-2.2678, acc-0.1694, test loss-2.2681, acc-0.1731\n",
      "Iter-1490, train loss-2.2731, acc-0.1800, valid loss-2.2673, acc-0.1702, test loss-2.2676, acc-0.1737\n",
      "Iter-1500, train loss-2.2743, acc-0.1800, valid loss-2.2668, acc-0.1706, test loss-2.2671, acc-0.1743\n",
      "Iter-1510, train loss-2.2599, acc-0.1600, valid loss-2.2663, acc-0.1700, test loss-2.2666, acc-0.1744\n",
      "Iter-1520, train loss-2.2949, acc-0.1600, valid loss-2.2657, acc-0.1702, test loss-2.2661, acc-0.1744\n",
      "Iter-1530, train loss-2.2929, acc-0.1400, valid loss-2.2652, acc-0.1706, test loss-2.2656, acc-0.1747\n",
      "Iter-1540, train loss-2.2517, acc-0.1600, valid loss-2.2647, acc-0.1708, test loss-2.2651, acc-0.1751\n",
      "Iter-1550, train loss-2.2731, acc-0.1600, valid loss-2.2641, acc-0.1710, test loss-2.2646, acc-0.1754\n",
      "Iter-1560, train loss-2.3036, acc-0.1200, valid loss-2.2636, acc-0.1718, test loss-2.2641, acc-0.1762\n",
      "Iter-1570, train loss-2.2561, acc-0.2400, valid loss-2.2631, acc-0.1722, test loss-2.2635, acc-0.1766\n",
      "Iter-1580, train loss-2.2761, acc-0.1600, valid loss-2.2626, acc-0.1724, test loss-2.2630, acc-0.1767\n",
      "Iter-1590, train loss-2.2540, acc-0.1800, valid loss-2.2621, acc-0.1728, test loss-2.2625, acc-0.1771\n",
      "Iter-1600, train loss-2.2585, acc-0.1600, valid loss-2.2615, acc-0.1738, test loss-2.2620, acc-0.1771\n",
      "Iter-1610, train loss-2.2489, acc-0.2000, valid loss-2.2610, acc-0.1742, test loss-2.2615, acc-0.1774\n",
      "Iter-1620, train loss-2.2597, acc-0.2400, valid loss-2.2605, acc-0.1742, test loss-2.2610, acc-0.1779\n",
      "Iter-1630, train loss-2.2645, acc-0.1600, valid loss-2.2599, acc-0.1756, test loss-2.2604, acc-0.1780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.3307, acc-0.0200, valid loss-2.2594, acc-0.1760, test loss-2.2599, acc-0.1779\n",
      "Iter-1650, train loss-2.2376, acc-0.1800, valid loss-2.2588, acc-0.1764, test loss-2.2594, acc-0.1786\n",
      "Iter-1660, train loss-2.2620, acc-0.1800, valid loss-2.2583, acc-0.1768, test loss-2.2589, acc-0.1788\n",
      "Iter-1670, train loss-2.2381, acc-0.2000, valid loss-2.2578, acc-0.1762, test loss-2.2584, acc-0.1789\n",
      "Iter-1680, train loss-2.2558, acc-0.1600, valid loss-2.2572, acc-0.1768, test loss-2.2579, acc-0.1797\n",
      "Iter-1690, train loss-2.2678, acc-0.2200, valid loss-2.2567, acc-0.1770, test loss-2.2574, acc-0.1798\n",
      "Iter-1700, train loss-2.2546, acc-0.2400, valid loss-2.2562, acc-0.1776, test loss-2.2568, acc-0.1805\n",
      "Iter-1710, train loss-2.2578, acc-0.2400, valid loss-2.2556, acc-0.1782, test loss-2.2563, acc-0.1810\n",
      "Iter-1720, train loss-2.2565, acc-0.2000, valid loss-2.2551, acc-0.1788, test loss-2.2558, acc-0.1812\n",
      "Iter-1730, train loss-2.2640, acc-0.1600, valid loss-2.2546, acc-0.1790, test loss-2.2553, acc-0.1813\n",
      "Iter-1740, train loss-2.2189, acc-0.2200, valid loss-2.2540, acc-0.1794, test loss-2.2548, acc-0.1814\n",
      "Iter-1750, train loss-2.2593, acc-0.1600, valid loss-2.2535, acc-0.1802, test loss-2.2542, acc-0.1812\n",
      "Iter-1760, train loss-2.2303, acc-0.1600, valid loss-2.2529, acc-0.1806, test loss-2.2537, acc-0.1817\n",
      "Iter-1770, train loss-2.2244, acc-0.2400, valid loss-2.2523, acc-0.1814, test loss-2.2531, acc-0.1816\n",
      "Iter-1780, train loss-2.2521, acc-0.1600, valid loss-2.2518, acc-0.1818, test loss-2.2526, acc-0.1816\n",
      "Iter-1790, train loss-2.2090, acc-0.2600, valid loss-2.2512, acc-0.1818, test loss-2.2520, acc-0.1820\n",
      "Iter-1800, train loss-2.2262, acc-0.2600, valid loss-2.2507, acc-0.1826, test loss-2.2515, acc-0.1822\n",
      "Iter-1810, train loss-2.2606, acc-0.1600, valid loss-2.2501, acc-0.1830, test loss-2.2510, acc-0.1828\n",
      "Iter-1820, train loss-2.2724, acc-0.1200, valid loss-2.2495, acc-0.1830, test loss-2.2504, acc-0.1829\n",
      "Iter-1830, train loss-2.2429, acc-0.1400, valid loss-2.2490, acc-0.1830, test loss-2.2498, acc-0.1829\n",
      "Iter-1840, train loss-2.2592, acc-0.2000, valid loss-2.2484, acc-0.1832, test loss-2.2493, acc-0.1837\n",
      "Iter-1850, train loss-2.2703, acc-0.1400, valid loss-2.2478, acc-0.1844, test loss-2.2487, acc-0.1838\n",
      "Iter-1860, train loss-2.2788, acc-0.1600, valid loss-2.2473, acc-0.1848, test loss-2.2482, acc-0.1845\n",
      "Iter-1870, train loss-2.2663, acc-0.1600, valid loss-2.2467, acc-0.1852, test loss-2.2476, acc-0.1850\n",
      "Iter-1880, train loss-2.2316, acc-0.2600, valid loss-2.2461, acc-0.1856, test loss-2.2471, acc-0.1853\n",
      "Iter-1890, train loss-2.2806, acc-0.1800, valid loss-2.2455, acc-0.1860, test loss-2.2465, acc-0.1855\n",
      "Iter-1900, train loss-2.2632, acc-0.1400, valid loss-2.2449, acc-0.1862, test loss-2.2459, acc-0.1855\n",
      "Iter-1910, train loss-2.2527, acc-0.1200, valid loss-2.2443, acc-0.1872, test loss-2.2453, acc-0.1857\n",
      "Iter-1920, train loss-2.2779, acc-0.1400, valid loss-2.2438, acc-0.1880, test loss-2.2448, acc-0.1863\n",
      "Iter-1930, train loss-2.2388, acc-0.1600, valid loss-2.2432, acc-0.1884, test loss-2.2442, acc-0.1867\n",
      "Iter-1940, train loss-2.2629, acc-0.1200, valid loss-2.2426, acc-0.1896, test loss-2.2436, acc-0.1872\n",
      "Iter-1950, train loss-2.2722, acc-0.1600, valid loss-2.2420, acc-0.1902, test loss-2.2431, acc-0.1876\n",
      "Iter-1960, train loss-2.2595, acc-0.1600, valid loss-2.2415, acc-0.1904, test loss-2.2426, acc-0.1882\n",
      "Iter-1970, train loss-2.2125, acc-0.2000, valid loss-2.2409, acc-0.1904, test loss-2.2420, acc-0.1884\n",
      "Iter-1980, train loss-2.2341, acc-0.2000, valid loss-2.2404, acc-0.1912, test loss-2.2414, acc-0.1884\n",
      "Iter-1990, train loss-2.2590, acc-0.2000, valid loss-2.2398, acc-0.1918, test loss-2.2409, acc-0.1885\n",
      "Iter-2000, train loss-2.2176, acc-0.3000, valid loss-2.2392, acc-0.1922, test loss-2.2403, acc-0.1883\n",
      "Iter-2010, train loss-2.2690, acc-0.1400, valid loss-2.2386, acc-0.1934, test loss-2.2398, acc-0.1883\n",
      "Iter-2020, train loss-2.1959, acc-0.2400, valid loss-2.2380, acc-0.1928, test loss-2.2391, acc-0.1886\n",
      "Iter-2030, train loss-2.2785, acc-0.0800, valid loss-2.2374, acc-0.1926, test loss-2.2386, acc-0.1887\n",
      "Iter-2040, train loss-2.2369, acc-0.2000, valid loss-2.2369, acc-0.1926, test loss-2.2380, acc-0.1889\n",
      "Iter-2050, train loss-2.2602, acc-0.2000, valid loss-2.2362, acc-0.1928, test loss-2.2374, acc-0.1894\n",
      "Iter-2060, train loss-2.2470, acc-0.2000, valid loss-2.2357, acc-0.1930, test loss-2.2369, acc-0.1896\n",
      "Iter-2070, train loss-2.2064, acc-0.2600, valid loss-2.2350, acc-0.1936, test loss-2.2363, acc-0.1897\n",
      "Iter-2080, train loss-2.1893, acc-0.2400, valid loss-2.2344, acc-0.1928, test loss-2.2357, acc-0.1899\n",
      "Iter-2090, train loss-2.2293, acc-0.1800, valid loss-2.2338, acc-0.1938, test loss-2.2351, acc-0.1906\n",
      "Iter-2100, train loss-2.2598, acc-0.1600, valid loss-2.2332, acc-0.1944, test loss-2.2345, acc-0.1908\n",
      "Iter-2110, train loss-2.2109, acc-0.2200, valid loss-2.2326, acc-0.1956, test loss-2.2339, acc-0.1910\n",
      "Iter-2120, train loss-2.2366, acc-0.1800, valid loss-2.2320, acc-0.1958, test loss-2.2333, acc-0.1910\n",
      "Iter-2130, train loss-2.2374, acc-0.2600, valid loss-2.2314, acc-0.1964, test loss-2.2327, acc-0.1915\n",
      "Iter-2140, train loss-2.2317, acc-0.1400, valid loss-2.2308, acc-0.1968, test loss-2.2321, acc-0.1918\n",
      "Iter-2150, train loss-2.2405, acc-0.1600, valid loss-2.2302, acc-0.1976, test loss-2.2315, acc-0.1919\n",
      "Iter-2160, train loss-2.1864, acc-0.3200, valid loss-2.2296, acc-0.1978, test loss-2.2309, acc-0.1925\n",
      "Iter-2170, train loss-2.2057, acc-0.2000, valid loss-2.2289, acc-0.1984, test loss-2.2303, acc-0.1933\n",
      "Iter-2180, train loss-2.2215, acc-0.2200, valid loss-2.2283, acc-0.1986, test loss-2.2297, acc-0.1935\n",
      "Iter-2190, train loss-2.2285, acc-0.1800, valid loss-2.2277, acc-0.1994, test loss-2.2290, acc-0.1939\n",
      "Iter-2200, train loss-2.2174, acc-0.2200, valid loss-2.2271, acc-0.1996, test loss-2.2285, acc-0.1946\n",
      "Iter-2210, train loss-2.2523, acc-0.1800, valid loss-2.2265, acc-0.1994, test loss-2.2279, acc-0.1944\n",
      "Iter-2220, train loss-2.2326, acc-0.1800, valid loss-2.2259, acc-0.1994, test loss-2.2273, acc-0.1951\n",
      "Iter-2230, train loss-2.1740, acc-0.2600, valid loss-2.2253, acc-0.1998, test loss-2.2267, acc-0.1953\n",
      "Iter-2240, train loss-2.2062, acc-0.2200, valid loss-2.2246, acc-0.2002, test loss-2.2260, acc-0.1952\n",
      "Iter-2250, train loss-2.2423, acc-0.2000, valid loss-2.2239, acc-0.2014, test loss-2.2254, acc-0.1957\n",
      "Iter-2260, train loss-2.2170, acc-0.2000, valid loss-2.2233, acc-0.2014, test loss-2.2248, acc-0.1958\n",
      "Iter-2270, train loss-2.2102, acc-0.2200, valid loss-2.2227, acc-0.2022, test loss-2.2241, acc-0.1961\n",
      "Iter-2280, train loss-2.2279, acc-0.1400, valid loss-2.2220, acc-0.2022, test loss-2.2235, acc-0.1967\n",
      "Iter-2290, train loss-2.2394, acc-0.2000, valid loss-2.2215, acc-0.2024, test loss-2.2229, acc-0.1972\n",
      "Iter-2300, train loss-2.2132, acc-0.1800, valid loss-2.2208, acc-0.2026, test loss-2.2223, acc-0.1985\n",
      "Iter-2310, train loss-2.1950, acc-0.2400, valid loss-2.2202, acc-0.2020, test loss-2.2217, acc-0.1991\n",
      "Iter-2320, train loss-2.1868, acc-0.2200, valid loss-2.2196, acc-0.2024, test loss-2.2211, acc-0.1993\n",
      "Iter-2330, train loss-2.2418, acc-0.1800, valid loss-2.2190, acc-0.2024, test loss-2.2205, acc-0.1997\n",
      "Iter-2340, train loss-2.2354, acc-0.2000, valid loss-2.2183, acc-0.2032, test loss-2.2198, acc-0.2000\n",
      "Iter-2350, train loss-2.2263, acc-0.1400, valid loss-2.2177, acc-0.2040, test loss-2.2192, acc-0.2002\n",
      "Iter-2360, train loss-2.2096, acc-0.2000, valid loss-2.2170, acc-0.2042, test loss-2.2186, acc-0.2005\n",
      "Iter-2370, train loss-2.2136, acc-0.2000, valid loss-2.2164, acc-0.2042, test loss-2.2179, acc-0.2013\n",
      "Iter-2380, train loss-2.1573, acc-0.3000, valid loss-2.2158, acc-0.2044, test loss-2.2173, acc-0.2017\n",
      "Iter-2390, train loss-2.1708, acc-0.2800, valid loss-2.2151, acc-0.2052, test loss-2.2167, acc-0.2022\n",
      "Iter-2400, train loss-2.1967, acc-0.3000, valid loss-2.2144, acc-0.2056, test loss-2.2160, acc-0.2030\n",
      "Iter-2410, train loss-2.1947, acc-0.2200, valid loss-2.2138, acc-0.2060, test loss-2.2154, acc-0.2035\n",
      "Iter-2420, train loss-2.2404, acc-0.1000, valid loss-2.2131, acc-0.2068, test loss-2.2147, acc-0.2039\n",
      "Iter-2430, train loss-2.2117, acc-0.2600, valid loss-2.2125, acc-0.2068, test loss-2.2141, acc-0.2049\n",
      "Iter-2440, train loss-2.1986, acc-0.2600, valid loss-2.2118, acc-0.2076, test loss-2.2134, acc-0.2054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.1864, acc-0.2200, valid loss-2.2111, acc-0.2076, test loss-2.2127, acc-0.2055\n",
      "Iter-2460, train loss-2.2414, acc-0.1400, valid loss-2.2104, acc-0.2082, test loss-2.2120, acc-0.2065\n",
      "Iter-2470, train loss-2.1980, acc-0.2000, valid loss-2.2098, acc-0.2088, test loss-2.2114, acc-0.2071\n",
      "Iter-2480, train loss-2.2772, acc-0.0800, valid loss-2.2091, acc-0.2096, test loss-2.2108, acc-0.2080\n",
      "Iter-2490, train loss-2.2120, acc-0.3000, valid loss-2.2085, acc-0.2108, test loss-2.2101, acc-0.2084\n",
      "Iter-2500, train loss-2.1855, acc-0.2400, valid loss-2.2078, acc-0.2112, test loss-2.2094, acc-0.2097\n",
      "Iter-2510, train loss-2.2432, acc-0.1800, valid loss-2.2071, acc-0.2114, test loss-2.2088, acc-0.2104\n",
      "Iter-2520, train loss-2.1713, acc-0.2600, valid loss-2.2065, acc-0.2112, test loss-2.2081, acc-0.2104\n",
      "Iter-2530, train loss-2.2326, acc-0.1600, valid loss-2.2058, acc-0.2118, test loss-2.2075, acc-0.2119\n",
      "Iter-2540, train loss-2.2334, acc-0.1600, valid loss-2.2052, acc-0.2128, test loss-2.2069, acc-0.2121\n",
      "Iter-2550, train loss-2.2670, acc-0.1000, valid loss-2.2045, acc-0.2128, test loss-2.2062, acc-0.2121\n",
      "Iter-2560, train loss-2.1565, acc-0.2800, valid loss-2.2038, acc-0.2150, test loss-2.2055, acc-0.2129\n",
      "Iter-2570, train loss-2.2611, acc-0.1000, valid loss-2.2032, acc-0.2156, test loss-2.2049, acc-0.2133\n",
      "Iter-2580, train loss-2.2068, acc-0.2000, valid loss-2.2025, acc-0.2158, test loss-2.2043, acc-0.2134\n",
      "Iter-2590, train loss-2.1873, acc-0.1800, valid loss-2.2019, acc-0.2154, test loss-2.2036, acc-0.2135\n",
      "Iter-2600, train loss-2.2001, acc-0.1800, valid loss-2.2012, acc-0.2156, test loss-2.2029, acc-0.2149\n",
      "Iter-2610, train loss-2.1916, acc-0.2200, valid loss-2.2005, acc-0.2162, test loss-2.2022, acc-0.2154\n",
      "Iter-2620, train loss-2.1989, acc-0.2200, valid loss-2.1998, acc-0.2168, test loss-2.2016, acc-0.2162\n",
      "Iter-2630, train loss-2.1915, acc-0.3000, valid loss-2.1991, acc-0.2178, test loss-2.2009, acc-0.2175\n",
      "Iter-2640, train loss-2.1579, acc-0.2600, valid loss-2.1985, acc-0.2190, test loss-2.2003, acc-0.2188\n",
      "Iter-2650, train loss-2.2075, acc-0.2000, valid loss-2.1978, acc-0.2198, test loss-2.1996, acc-0.2192\n",
      "Iter-2660, train loss-2.1941, acc-0.3200, valid loss-2.1971, acc-0.2200, test loss-2.1989, acc-0.2200\n",
      "Iter-2670, train loss-2.2277, acc-0.2000, valid loss-2.1965, acc-0.2210, test loss-2.1983, acc-0.2202\n",
      "Iter-2680, train loss-2.2025, acc-0.1600, valid loss-2.1958, acc-0.2224, test loss-2.1976, acc-0.2207\n",
      "Iter-2690, train loss-2.2132, acc-0.1600, valid loss-2.1951, acc-0.2224, test loss-2.1969, acc-0.2214\n",
      "Iter-2700, train loss-2.1393, acc-0.4200, valid loss-2.1943, acc-0.2230, test loss-2.1962, acc-0.2218\n",
      "Iter-2710, train loss-2.1686, acc-0.2800, valid loss-2.1936, acc-0.2242, test loss-2.1955, acc-0.2230\n",
      "Iter-2720, train loss-2.1956, acc-0.2600, valid loss-2.1929, acc-0.2254, test loss-2.1948, acc-0.2243\n",
      "Iter-2730, train loss-2.1918, acc-0.1600, valid loss-2.1923, acc-0.2258, test loss-2.1941, acc-0.2248\n",
      "Iter-2740, train loss-2.1832, acc-0.2000, valid loss-2.1916, acc-0.2264, test loss-2.1935, acc-0.2256\n",
      "Iter-2750, train loss-2.1868, acc-0.1800, valid loss-2.1910, acc-0.2268, test loss-2.1929, acc-0.2262\n",
      "Iter-2760, train loss-2.2070, acc-0.2000, valid loss-2.1903, acc-0.2274, test loss-2.1922, acc-0.2272\n",
      "Iter-2770, train loss-2.1672, acc-0.2800, valid loss-2.1895, acc-0.2286, test loss-2.1915, acc-0.2281\n",
      "Iter-2780, train loss-2.1705, acc-0.2200, valid loss-2.1888, acc-0.2302, test loss-2.1907, acc-0.2300\n",
      "Iter-2790, train loss-2.2251, acc-0.1800, valid loss-2.1881, acc-0.2306, test loss-2.1900, acc-0.2310\n",
      "Iter-2800, train loss-2.2329, acc-0.2400, valid loss-2.1875, acc-0.2314, test loss-2.1894, acc-0.2328\n",
      "Iter-2810, train loss-2.1848, acc-0.2800, valid loss-2.1867, acc-0.2326, test loss-2.1887, acc-0.2334\n",
      "Iter-2820, train loss-2.1936, acc-0.1600, valid loss-2.1860, acc-0.2334, test loss-2.1880, acc-0.2338\n",
      "Iter-2830, train loss-2.1701, acc-0.2400, valid loss-2.1853, acc-0.2342, test loss-2.1873, acc-0.2350\n",
      "Iter-2840, train loss-2.1977, acc-0.2200, valid loss-2.1846, acc-0.2348, test loss-2.1865, acc-0.2358\n",
      "Iter-2850, train loss-2.1943, acc-0.1600, valid loss-2.1839, acc-0.2362, test loss-2.1858, acc-0.2366\n",
      "Iter-2860, train loss-2.2100, acc-0.1600, valid loss-2.1832, acc-0.2362, test loss-2.1851, acc-0.2372\n",
      "Iter-2870, train loss-2.1914, acc-0.2000, valid loss-2.1825, acc-0.2376, test loss-2.1844, acc-0.2383\n",
      "Iter-2880, train loss-2.2020, acc-0.1800, valid loss-2.1817, acc-0.2382, test loss-2.1836, acc-0.2401\n",
      "Iter-2890, train loss-2.2056, acc-0.2000, valid loss-2.1810, acc-0.2398, test loss-2.1829, acc-0.2410\n",
      "Iter-2900, train loss-2.1809, acc-0.2200, valid loss-2.1802, acc-0.2408, test loss-2.1822, acc-0.2415\n",
      "Iter-2910, train loss-2.1894, acc-0.2400, valid loss-2.1795, acc-0.2424, test loss-2.1815, acc-0.2426\n",
      "Iter-2920, train loss-2.2111, acc-0.2000, valid loss-2.1788, acc-0.2434, test loss-2.1808, acc-0.2431\n",
      "Iter-2930, train loss-2.1603, acc-0.3400, valid loss-2.1781, acc-0.2444, test loss-2.1800, acc-0.2441\n",
      "Iter-2940, train loss-2.1907, acc-0.2000, valid loss-2.1774, acc-0.2444, test loss-2.1793, acc-0.2452\n",
      "Iter-2950, train loss-2.2020, acc-0.2800, valid loss-2.1766, acc-0.2466, test loss-2.1786, acc-0.2462\n",
      "Iter-2960, train loss-2.2109, acc-0.1800, valid loss-2.1759, acc-0.2482, test loss-2.1779, acc-0.2470\n",
      "Iter-2970, train loss-2.1733, acc-0.2200, valid loss-2.1752, acc-0.2492, test loss-2.1772, acc-0.2486\n",
      "Iter-2980, train loss-2.1854, acc-0.2400, valid loss-2.1745, acc-0.2496, test loss-2.1765, acc-0.2495\n",
      "Iter-2990, train loss-2.1535, acc-0.2400, valid loss-2.1737, acc-0.2510, test loss-2.1757, acc-0.2513\n",
      "Iter-3000, train loss-2.1992, acc-0.2200, valid loss-2.1729, acc-0.2524, test loss-2.1750, acc-0.2535\n",
      "Iter-3010, train loss-2.1744, acc-0.2200, valid loss-2.1722, acc-0.2544, test loss-2.1742, acc-0.2561\n",
      "Iter-3020, train loss-2.2196, acc-0.1600, valid loss-2.1714, acc-0.2554, test loss-2.1734, acc-0.2576\n",
      "Iter-3030, train loss-2.1987, acc-0.1400, valid loss-2.1707, acc-0.2562, test loss-2.1727, acc-0.2588\n",
      "Iter-3040, train loss-2.1964, acc-0.2400, valid loss-2.1700, acc-0.2572, test loss-2.1720, acc-0.2603\n",
      "Iter-3050, train loss-2.1944, acc-0.2400, valid loss-2.1692, acc-0.2592, test loss-2.1712, acc-0.2619\n",
      "Iter-3060, train loss-2.2494, acc-0.1200, valid loss-2.1685, acc-0.2592, test loss-2.1706, acc-0.2626\n",
      "Iter-3070, train loss-2.1443, acc-0.3200, valid loss-2.1678, acc-0.2604, test loss-2.1698, acc-0.2637\n",
      "Iter-3080, train loss-2.1641, acc-0.2800, valid loss-2.1671, acc-0.2616, test loss-2.1691, acc-0.2645\n",
      "Iter-3090, train loss-2.1667, acc-0.2800, valid loss-2.1663, acc-0.2640, test loss-2.1684, acc-0.2666\n",
      "Iter-3100, train loss-2.1491, acc-0.2200, valid loss-2.1656, acc-0.2648, test loss-2.1676, acc-0.2669\n",
      "Iter-3110, train loss-2.1883, acc-0.2800, valid loss-2.1649, acc-0.2666, test loss-2.1669, acc-0.2682\n",
      "Iter-3120, train loss-2.1620, acc-0.2000, valid loss-2.1642, acc-0.2684, test loss-2.1662, acc-0.2693\n",
      "Iter-3130, train loss-2.1552, acc-0.2200, valid loss-2.1634, acc-0.2694, test loss-2.1655, acc-0.2707\n",
      "Iter-3140, train loss-2.1615, acc-0.2400, valid loss-2.1627, acc-0.2700, test loss-2.1648, acc-0.2712\n",
      "Iter-3150, train loss-2.1520, acc-0.2400, valid loss-2.1619, acc-0.2710, test loss-2.1640, acc-0.2729\n",
      "Iter-3160, train loss-2.1853, acc-0.1600, valid loss-2.1612, acc-0.2724, test loss-2.1633, acc-0.2742\n",
      "Iter-3170, train loss-2.1558, acc-0.3400, valid loss-2.1605, acc-0.2736, test loss-2.1626, acc-0.2750\n",
      "Iter-3180, train loss-2.1021, acc-0.3200, valid loss-2.1597, acc-0.2752, test loss-2.1618, acc-0.2762\n",
      "Iter-3190, train loss-2.1504, acc-0.3000, valid loss-2.1590, acc-0.2758, test loss-2.1611, acc-0.2761\n",
      "Iter-3200, train loss-2.1503, acc-0.2000, valid loss-2.1582, acc-0.2778, test loss-2.1604, acc-0.2778\n",
      "Iter-3210, train loss-2.1858, acc-0.2200, valid loss-2.1575, acc-0.2784, test loss-2.1596, acc-0.2788\n",
      "Iter-3220, train loss-2.1394, acc-0.2800, valid loss-2.1567, acc-0.2796, test loss-2.1588, acc-0.2794\n",
      "Iter-3230, train loss-2.1904, acc-0.2000, valid loss-2.1559, acc-0.2804, test loss-2.1581, acc-0.2800\n",
      "Iter-3240, train loss-2.1030, acc-0.3800, valid loss-2.1552, acc-0.2812, test loss-2.1573, acc-0.2809\n",
      "Iter-3250, train loss-2.1583, acc-0.3000, valid loss-2.1544, acc-0.2814, test loss-2.1565, acc-0.2815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.1398, acc-0.2800, valid loss-2.1536, acc-0.2820, test loss-2.1557, acc-0.2826\n",
      "Iter-3270, train loss-2.1647, acc-0.2600, valid loss-2.1528, acc-0.2822, test loss-2.1550, acc-0.2827\n",
      "Iter-3280, train loss-2.1478, acc-0.3000, valid loss-2.1520, acc-0.2836, test loss-2.1542, acc-0.2838\n",
      "Iter-3290, train loss-2.1944, acc-0.2400, valid loss-2.1513, acc-0.2842, test loss-2.1535, acc-0.2842\n",
      "Iter-3300, train loss-2.1580, acc-0.2400, valid loss-2.1505, acc-0.2840, test loss-2.1527, acc-0.2857\n",
      "Iter-3310, train loss-2.1696, acc-0.3200, valid loss-2.1498, acc-0.2850, test loss-2.1520, acc-0.2867\n",
      "Iter-3320, train loss-2.0902, acc-0.4400, valid loss-2.1490, acc-0.2852, test loss-2.1513, acc-0.2874\n",
      "Iter-3330, train loss-2.1418, acc-0.3000, valid loss-2.1483, acc-0.2868, test loss-2.1505, acc-0.2875\n",
      "Iter-3340, train loss-2.1601, acc-0.3600, valid loss-2.1475, acc-0.2864, test loss-2.1497, acc-0.2883\n",
      "Iter-3350, train loss-2.1187, acc-0.3600, valid loss-2.1468, acc-0.2878, test loss-2.1490, acc-0.2893\n",
      "Iter-3360, train loss-2.1923, acc-0.2600, valid loss-2.1460, acc-0.2886, test loss-2.1483, acc-0.2895\n",
      "Iter-3370, train loss-2.1763, acc-0.2600, valid loss-2.1453, acc-0.2884, test loss-2.1475, acc-0.2897\n",
      "Iter-3380, train loss-2.1244, acc-0.3400, valid loss-2.1446, acc-0.2886, test loss-2.1468, acc-0.2900\n",
      "Iter-3390, train loss-2.0967, acc-0.3400, valid loss-2.1438, acc-0.2892, test loss-2.1460, acc-0.2904\n",
      "Iter-3400, train loss-2.1953, acc-0.2400, valid loss-2.1430, acc-0.2898, test loss-2.1452, acc-0.2906\n",
      "Iter-3410, train loss-2.1836, acc-0.3000, valid loss-2.1423, acc-0.2902, test loss-2.1445, acc-0.2913\n",
      "Iter-3420, train loss-2.1441, acc-0.2600, valid loss-2.1415, acc-0.2912, test loss-2.1437, acc-0.2919\n",
      "Iter-3430, train loss-2.1452, acc-0.2800, valid loss-2.1408, acc-0.2908, test loss-2.1430, acc-0.2919\n",
      "Iter-3440, train loss-2.1158, acc-0.3400, valid loss-2.1400, acc-0.2912, test loss-2.1422, acc-0.2926\n",
      "Iter-3450, train loss-2.1773, acc-0.2000, valid loss-2.1392, acc-0.2902, test loss-2.1415, acc-0.2929\n",
      "Iter-3460, train loss-2.1779, acc-0.2800, valid loss-2.1384, acc-0.2908, test loss-2.1407, acc-0.2936\n",
      "Iter-3470, train loss-2.1312, acc-0.3400, valid loss-2.1377, acc-0.2902, test loss-2.1399, acc-0.2938\n",
      "Iter-3480, train loss-2.1495, acc-0.3600, valid loss-2.1369, acc-0.2908, test loss-2.1392, acc-0.2942\n",
      "Iter-3490, train loss-2.1350, acc-0.3200, valid loss-2.1361, acc-0.2912, test loss-2.1384, acc-0.2951\n",
      "Iter-3500, train loss-2.1447, acc-0.3400, valid loss-2.1354, acc-0.2914, test loss-2.1377, acc-0.2954\n",
      "Iter-3510, train loss-2.1476, acc-0.3600, valid loss-2.1347, acc-0.2910, test loss-2.1370, acc-0.2952\n",
      "Iter-3520, train loss-2.2080, acc-0.1800, valid loss-2.1339, acc-0.2902, test loss-2.1362, acc-0.2956\n",
      "Iter-3530, train loss-2.1276, acc-0.3000, valid loss-2.1331, acc-0.2906, test loss-2.1354, acc-0.2957\n",
      "Iter-3540, train loss-2.1475, acc-0.2800, valid loss-2.1323, acc-0.2910, test loss-2.1346, acc-0.2964\n",
      "Iter-3550, train loss-2.1430, acc-0.3400, valid loss-2.1315, acc-0.2912, test loss-2.1338, acc-0.2970\n",
      "Iter-3560, train loss-2.1317, acc-0.3600, valid loss-2.1307, acc-0.2916, test loss-2.1330, acc-0.2972\n",
      "Iter-3570, train loss-2.1505, acc-0.3200, valid loss-2.1299, acc-0.2912, test loss-2.1322, acc-0.2981\n",
      "Iter-3580, train loss-2.1265, acc-0.2800, valid loss-2.1291, acc-0.2916, test loss-2.1314, acc-0.2983\n",
      "Iter-3590, train loss-2.0798, acc-0.3800, valid loss-2.1283, acc-0.2924, test loss-2.1306, acc-0.2982\n",
      "Iter-3600, train loss-2.0606, acc-0.4000, valid loss-2.1275, acc-0.2930, test loss-2.1299, acc-0.2987\n",
      "Iter-3610, train loss-2.0837, acc-0.3400, valid loss-2.1267, acc-0.2932, test loss-2.1290, acc-0.2997\n",
      "Iter-3620, train loss-2.0925, acc-0.3400, valid loss-2.1258, acc-0.2924, test loss-2.1282, acc-0.3006\n",
      "Iter-3630, train loss-2.1368, acc-0.2400, valid loss-2.1251, acc-0.2926, test loss-2.1274, acc-0.3010\n",
      "Iter-3640, train loss-2.1203, acc-0.2800, valid loss-2.1243, acc-0.2926, test loss-2.1267, acc-0.3015\n",
      "Iter-3650, train loss-2.1266, acc-0.2200, valid loss-2.1235, acc-0.2932, test loss-2.1259, acc-0.3023\n",
      "Iter-3660, train loss-2.1068, acc-0.3600, valid loss-2.1228, acc-0.2942, test loss-2.1252, acc-0.3030\n",
      "Iter-3670, train loss-2.1868, acc-0.2000, valid loss-2.1220, acc-0.2946, test loss-2.1244, acc-0.3032\n",
      "Iter-3680, train loss-2.0881, acc-0.3200, valid loss-2.1212, acc-0.2950, test loss-2.1236, acc-0.3040\n",
      "Iter-3690, train loss-2.1120, acc-0.2800, valid loss-2.1204, acc-0.2954, test loss-2.1228, acc-0.3042\n",
      "Iter-3700, train loss-2.0530, acc-0.3800, valid loss-2.1196, acc-0.2954, test loss-2.1220, acc-0.3046\n",
      "Iter-3710, train loss-2.0868, acc-0.2800, valid loss-2.1188, acc-0.2964, test loss-2.1213, acc-0.3051\n",
      "Iter-3720, train loss-2.0987, acc-0.3200, valid loss-2.1181, acc-0.2972, test loss-2.1205, acc-0.3060\n",
      "Iter-3730, train loss-2.1191, acc-0.3400, valid loss-2.1172, acc-0.2968, test loss-2.1197, acc-0.3065\n",
      "Iter-3740, train loss-2.0920, acc-0.4000, valid loss-2.1164, acc-0.2970, test loss-2.1188, acc-0.3072\n",
      "Iter-3750, train loss-2.1341, acc-0.3000, valid loss-2.1156, acc-0.2984, test loss-2.1180, acc-0.3077\n",
      "Iter-3760, train loss-2.0787, acc-0.3800, valid loss-2.1148, acc-0.2984, test loss-2.1173, acc-0.3079\n",
      "Iter-3770, train loss-2.1280, acc-0.2800, valid loss-2.1140, acc-0.2984, test loss-2.1165, acc-0.3083\n",
      "Iter-3780, train loss-2.1476, acc-0.2000, valid loss-2.1132, acc-0.2986, test loss-2.1157, acc-0.3086\n",
      "Iter-3790, train loss-2.1640, acc-0.2600, valid loss-2.1124, acc-0.2988, test loss-2.1149, acc-0.3094\n",
      "Iter-3800, train loss-2.0734, acc-0.3600, valid loss-2.1116, acc-0.2990, test loss-2.1141, acc-0.3101\n",
      "Iter-3810, train loss-2.1639, acc-0.2400, valid loss-2.1109, acc-0.2994, test loss-2.1134, acc-0.3108\n",
      "Iter-3820, train loss-2.1471, acc-0.2800, valid loss-2.1101, acc-0.3000, test loss-2.1126, acc-0.3114\n",
      "Iter-3830, train loss-2.1138, acc-0.2600, valid loss-2.1093, acc-0.3004, test loss-2.1119, acc-0.3121\n",
      "Iter-3840, train loss-2.0838, acc-0.3600, valid loss-2.1086, acc-0.3008, test loss-2.1111, acc-0.3123\n",
      "Iter-3850, train loss-2.1088, acc-0.2800, valid loss-2.1078, acc-0.3016, test loss-2.1103, acc-0.3130\n",
      "Iter-3860, train loss-2.0491, acc-0.4200, valid loss-2.1070, acc-0.3014, test loss-2.1095, acc-0.3139\n",
      "Iter-3870, train loss-2.1358, acc-0.2600, valid loss-2.1062, acc-0.3010, test loss-2.1087, acc-0.3145\n",
      "Iter-3880, train loss-2.0963, acc-0.2600, valid loss-2.1054, acc-0.3018, test loss-2.1079, acc-0.3151\n",
      "Iter-3890, train loss-2.1739, acc-0.1800, valid loss-2.1046, acc-0.3024, test loss-2.1071, acc-0.3163\n",
      "Iter-3900, train loss-2.0697, acc-0.4000, valid loss-2.1038, acc-0.3028, test loss-2.1064, acc-0.3164\n",
      "Iter-3910, train loss-2.1417, acc-0.2200, valid loss-2.1031, acc-0.3028, test loss-2.1056, acc-0.3166\n",
      "Iter-3920, train loss-2.1389, acc-0.3400, valid loss-2.1022, acc-0.3036, test loss-2.1048, acc-0.3172\n",
      "Iter-3930, train loss-2.1547, acc-0.1600, valid loss-2.1015, acc-0.3032, test loss-2.1040, acc-0.3171\n",
      "Iter-3940, train loss-2.1234, acc-0.3000, valid loss-2.1007, acc-0.3040, test loss-2.1032, acc-0.3178\n",
      "Iter-3950, train loss-2.1358, acc-0.2600, valid loss-2.0999, acc-0.3044, test loss-2.1025, acc-0.3181\n",
      "Iter-3960, train loss-2.1023, acc-0.3200, valid loss-2.0990, acc-0.3066, test loss-2.1016, acc-0.3184\n",
      "Iter-3970, train loss-2.1371, acc-0.2400, valid loss-2.0982, acc-0.3062, test loss-2.1008, acc-0.3185\n",
      "Iter-3980, train loss-2.1232, acc-0.2400, valid loss-2.0974, acc-0.3066, test loss-2.1000, acc-0.3191\n",
      "Iter-3990, train loss-2.1043, acc-0.4000, valid loss-2.0966, acc-0.3062, test loss-2.0992, acc-0.3192\n",
      "Iter-4000, train loss-2.0159, acc-0.3200, valid loss-2.0958, acc-0.3068, test loss-2.0984, acc-0.3199\n",
      "Iter-4010, train loss-2.1338, acc-0.2800, valid loss-2.0950, acc-0.3066, test loss-2.0975, acc-0.3199\n",
      "Iter-4020, train loss-2.0765, acc-0.3200, valid loss-2.0942, acc-0.3074, test loss-2.0967, acc-0.3203\n",
      "Iter-4030, train loss-2.0677, acc-0.3600, valid loss-2.0933, acc-0.3082, test loss-2.0959, acc-0.3210\n",
      "Iter-4040, train loss-2.0230, acc-0.3800, valid loss-2.0926, acc-0.3094, test loss-2.0952, acc-0.3214\n",
      "Iter-4050, train loss-2.0704, acc-0.3400, valid loss-2.0917, acc-0.3104, test loss-2.0943, acc-0.3213\n",
      "Iter-4060, train loss-2.1286, acc-0.3800, valid loss-2.0909, acc-0.3108, test loss-2.0935, acc-0.3217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.1326, acc-0.3400, valid loss-2.0901, acc-0.3116, test loss-2.0927, acc-0.3224\n",
      "Iter-4080, train loss-2.1464, acc-0.2600, valid loss-2.0893, acc-0.3116, test loss-2.0919, acc-0.3233\n",
      "Iter-4090, train loss-2.1198, acc-0.3000, valid loss-2.0885, acc-0.3122, test loss-2.0911, acc-0.3239\n",
      "Iter-4100, train loss-2.1069, acc-0.2800, valid loss-2.0877, acc-0.3122, test loss-2.0903, acc-0.3240\n",
      "Iter-4110, train loss-2.1026, acc-0.3200, valid loss-2.0869, acc-0.3118, test loss-2.0895, acc-0.3240\n",
      "Iter-4120, train loss-2.1624, acc-0.2000, valid loss-2.0860, acc-0.3120, test loss-2.0886, acc-0.3247\n",
      "Iter-4130, train loss-2.0728, acc-0.4000, valid loss-2.0851, acc-0.3124, test loss-2.0877, acc-0.3253\n",
      "Iter-4140, train loss-2.0701, acc-0.3600, valid loss-2.0844, acc-0.3134, test loss-2.0870, acc-0.3261\n",
      "Iter-4150, train loss-2.0506, acc-0.3800, valid loss-2.0835, acc-0.3142, test loss-2.0862, acc-0.3264\n",
      "Iter-4160, train loss-2.1383, acc-0.2000, valid loss-2.0827, acc-0.3144, test loss-2.0854, acc-0.3271\n",
      "Iter-4170, train loss-2.0640, acc-0.4000, valid loss-2.0819, acc-0.3148, test loss-2.0845, acc-0.3279\n",
      "Iter-4180, train loss-2.0473, acc-0.3600, valid loss-2.0811, acc-0.3146, test loss-2.0837, acc-0.3283\n",
      "Iter-4190, train loss-2.1139, acc-0.3200, valid loss-2.0802, acc-0.3154, test loss-2.0829, acc-0.3292\n",
      "Iter-4200, train loss-2.0474, acc-0.3400, valid loss-2.0794, acc-0.3158, test loss-2.0821, acc-0.3302\n",
      "Iter-4210, train loss-2.0628, acc-0.3800, valid loss-2.0786, acc-0.3164, test loss-2.0813, acc-0.3306\n",
      "Iter-4220, train loss-2.1083, acc-0.4000, valid loss-2.0778, acc-0.3168, test loss-2.0805, acc-0.3306\n",
      "Iter-4230, train loss-2.0762, acc-0.4000, valid loss-2.0770, acc-0.3172, test loss-2.0797, acc-0.3309\n",
      "Iter-4240, train loss-2.1393, acc-0.3000, valid loss-2.0762, acc-0.3176, test loss-2.0789, acc-0.3314\n",
      "Iter-4250, train loss-2.0409, acc-0.3800, valid loss-2.0754, acc-0.3182, test loss-2.0781, acc-0.3322\n",
      "Iter-4260, train loss-2.0542, acc-0.3600, valid loss-2.0745, acc-0.3190, test loss-2.0772, acc-0.3328\n",
      "Iter-4270, train loss-2.1077, acc-0.2800, valid loss-2.0736, acc-0.3188, test loss-2.0763, acc-0.3331\n",
      "Iter-4280, train loss-2.1036, acc-0.3200, valid loss-2.0728, acc-0.3208, test loss-2.0755, acc-0.3336\n",
      "Iter-4290, train loss-2.0884, acc-0.3800, valid loss-2.0720, acc-0.3212, test loss-2.0747, acc-0.3345\n",
      "Iter-4300, train loss-2.0231, acc-0.3600, valid loss-2.0712, acc-0.3208, test loss-2.0739, acc-0.3348\n",
      "Iter-4310, train loss-2.0662, acc-0.3200, valid loss-2.0703, acc-0.3210, test loss-2.0731, acc-0.3345\n",
      "Iter-4320, train loss-2.1459, acc-0.2800, valid loss-2.0695, acc-0.3224, test loss-2.0722, acc-0.3357\n",
      "Iter-4330, train loss-2.0864, acc-0.2200, valid loss-2.0687, acc-0.3230, test loss-2.0714, acc-0.3358\n",
      "Iter-4340, train loss-2.1072, acc-0.3000, valid loss-2.0678, acc-0.3224, test loss-2.0705, acc-0.3357\n",
      "Iter-4350, train loss-2.0253, acc-0.4400, valid loss-2.0670, acc-0.3224, test loss-2.0697, acc-0.3366\n",
      "Iter-4360, train loss-1.9889, acc-0.4600, valid loss-2.0661, acc-0.3232, test loss-2.0689, acc-0.3370\n",
      "Iter-4370, train loss-2.0600, acc-0.3200, valid loss-2.0652, acc-0.3238, test loss-2.0680, acc-0.3367\n",
      "Iter-4380, train loss-2.0333, acc-0.3800, valid loss-2.0644, acc-0.3248, test loss-2.0671, acc-0.3373\n",
      "Iter-4390, train loss-2.0108, acc-0.3800, valid loss-2.0635, acc-0.3248, test loss-2.0663, acc-0.3381\n",
      "Iter-4400, train loss-2.0391, acc-0.3200, valid loss-2.0627, acc-0.3248, test loss-2.0654, acc-0.3381\n",
      "Iter-4410, train loss-2.0787, acc-0.2800, valid loss-2.0619, acc-0.3258, test loss-2.0646, acc-0.3385\n",
      "Iter-4420, train loss-2.1115, acc-0.2600, valid loss-2.0610, acc-0.3258, test loss-2.0638, acc-0.3388\n",
      "Iter-4430, train loss-2.0403, acc-0.3200, valid loss-2.0602, acc-0.3270, test loss-2.0629, acc-0.3392\n",
      "Iter-4440, train loss-2.0358, acc-0.3600, valid loss-2.0593, acc-0.3278, test loss-2.0620, acc-0.3395\n",
      "Iter-4450, train loss-2.1246, acc-0.2400, valid loss-2.0585, acc-0.3282, test loss-2.0612, acc-0.3402\n",
      "Iter-4460, train loss-2.0504, acc-0.4000, valid loss-2.0577, acc-0.3282, test loss-2.0604, acc-0.3403\n",
      "Iter-4470, train loss-2.0162, acc-0.3600, valid loss-2.0568, acc-0.3288, test loss-2.0596, acc-0.3405\n",
      "Iter-4480, train loss-2.0622, acc-0.3400, valid loss-2.0560, acc-0.3288, test loss-2.0588, acc-0.3410\n",
      "Iter-4490, train loss-2.0534, acc-0.4400, valid loss-2.0552, acc-0.3298, test loss-2.0579, acc-0.3416\n",
      "Iter-4500, train loss-2.0395, acc-0.3800, valid loss-2.0544, acc-0.3298, test loss-2.0572, acc-0.3422\n",
      "Iter-4510, train loss-2.0054, acc-0.3400, valid loss-2.0536, acc-0.3302, test loss-2.0563, acc-0.3427\n",
      "Iter-4520, train loss-2.0755, acc-0.3200, valid loss-2.0527, acc-0.3306, test loss-2.0555, acc-0.3439\n",
      "Iter-4530, train loss-2.1125, acc-0.2800, valid loss-2.0519, acc-0.3314, test loss-2.0547, acc-0.3454\n",
      "Iter-4540, train loss-2.0006, acc-0.3800, valid loss-2.0511, acc-0.3328, test loss-2.0538, acc-0.3459\n",
      "Iter-4550, train loss-2.0530, acc-0.3800, valid loss-2.0502, acc-0.3340, test loss-2.0530, acc-0.3466\n",
      "Iter-4560, train loss-2.0500, acc-0.3600, valid loss-2.0494, acc-0.3348, test loss-2.0522, acc-0.3478\n",
      "Iter-4570, train loss-2.1024, acc-0.3400, valid loss-2.0485, acc-0.3350, test loss-2.0513, acc-0.3482\n",
      "Iter-4580, train loss-2.0608, acc-0.3200, valid loss-2.0476, acc-0.3352, test loss-2.0504, acc-0.3488\n",
      "Iter-4590, train loss-2.0294, acc-0.4000, valid loss-2.0467, acc-0.3356, test loss-2.0496, acc-0.3494\n",
      "Iter-4600, train loss-2.0225, acc-0.3800, valid loss-2.0458, acc-0.3364, test loss-2.0487, acc-0.3493\n",
      "Iter-4610, train loss-2.0650, acc-0.3200, valid loss-2.0450, acc-0.3372, test loss-2.0478, acc-0.3499\n",
      "Iter-4620, train loss-2.0923, acc-0.3000, valid loss-2.0442, acc-0.3392, test loss-2.0470, acc-0.3506\n",
      "Iter-4630, train loss-2.0431, acc-0.3200, valid loss-2.0434, acc-0.3402, test loss-2.0462, acc-0.3512\n",
      "Iter-4640, train loss-2.0207, acc-0.4400, valid loss-2.0424, acc-0.3410, test loss-2.0453, acc-0.3515\n",
      "Iter-4650, train loss-2.0567, acc-0.3200, valid loss-2.0416, acc-0.3428, test loss-2.0445, acc-0.3524\n",
      "Iter-4660, train loss-2.0247, acc-0.4000, valid loss-2.0408, acc-0.3440, test loss-2.0436, acc-0.3533\n",
      "Iter-4670, train loss-2.0281, acc-0.4000, valid loss-2.0399, acc-0.3446, test loss-2.0427, acc-0.3534\n",
      "Iter-4680, train loss-2.1337, acc-0.2000, valid loss-2.0391, acc-0.3450, test loss-2.0419, acc-0.3536\n",
      "Iter-4690, train loss-1.9888, acc-0.3800, valid loss-2.0383, acc-0.3464, test loss-2.0411, acc-0.3540\n",
      "Iter-4700, train loss-2.0021, acc-0.3600, valid loss-2.0373, acc-0.3468, test loss-2.0402, acc-0.3547\n",
      "Iter-4710, train loss-2.0629, acc-0.2800, valid loss-2.0365, acc-0.3470, test loss-2.0394, acc-0.3557\n",
      "Iter-4720, train loss-2.0042, acc-0.4200, valid loss-2.0357, acc-0.3480, test loss-2.0385, acc-0.3561\n",
      "Iter-4730, train loss-2.0810, acc-0.2800, valid loss-2.0348, acc-0.3486, test loss-2.0377, acc-0.3575\n",
      "Iter-4740, train loss-2.0553, acc-0.3200, valid loss-2.0340, acc-0.3488, test loss-2.0368, acc-0.3582\n",
      "Iter-4750, train loss-2.0543, acc-0.4000, valid loss-2.0332, acc-0.3496, test loss-2.0361, acc-0.3590\n",
      "Iter-4760, train loss-2.1146, acc-0.3200, valid loss-2.0323, acc-0.3500, test loss-2.0352, acc-0.3594\n",
      "Iter-4770, train loss-1.9206, acc-0.5000, valid loss-2.0314, acc-0.3506, test loss-2.0344, acc-0.3595\n",
      "Iter-4780, train loss-2.1010, acc-0.2200, valid loss-2.0305, acc-0.3514, test loss-2.0335, acc-0.3601\n",
      "Iter-4790, train loss-2.0520, acc-0.3600, valid loss-2.0297, acc-0.3514, test loss-2.0326, acc-0.3604\n",
      "Iter-4800, train loss-2.0844, acc-0.3600, valid loss-2.0288, acc-0.3514, test loss-2.0318, acc-0.3622\n",
      "Iter-4810, train loss-2.0650, acc-0.4000, valid loss-2.0279, acc-0.3524, test loss-2.0309, acc-0.3631\n",
      "Iter-4820, train loss-2.0560, acc-0.3000, valid loss-2.0271, acc-0.3526, test loss-2.0300, acc-0.3633\n",
      "Iter-4830, train loss-2.0896, acc-0.2800, valid loss-2.0262, acc-0.3528, test loss-2.0292, acc-0.3636\n",
      "Iter-4840, train loss-2.0249, acc-0.3600, valid loss-2.0253, acc-0.3524, test loss-2.0283, acc-0.3639\n",
      "Iter-4850, train loss-2.0416, acc-0.3600, valid loss-2.0244, acc-0.3526, test loss-2.0274, acc-0.3639\n",
      "Iter-4860, train loss-2.0024, acc-0.3600, valid loss-2.0235, acc-0.3528, test loss-2.0265, acc-0.3642\n",
      "Iter-4870, train loss-2.0999, acc-0.2800, valid loss-2.0226, acc-0.3536, test loss-2.0256, acc-0.3645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-2.0714, acc-0.3000, valid loss-2.0218, acc-0.3546, test loss-2.0248, acc-0.3648\n",
      "Iter-4890, train loss-2.0141, acc-0.3800, valid loss-2.0209, acc-0.3558, test loss-2.0240, acc-0.3660\n",
      "Iter-4900, train loss-2.0197, acc-0.3400, valid loss-2.0200, acc-0.3564, test loss-2.0230, acc-0.3664\n",
      "Iter-4910, train loss-1.9619, acc-0.4200, valid loss-2.0192, acc-0.3572, test loss-2.0222, acc-0.3670\n",
      "Iter-4920, train loss-1.9857, acc-0.3600, valid loss-2.0184, acc-0.3576, test loss-2.0214, acc-0.3674\n",
      "Iter-4930, train loss-2.0014, acc-0.4000, valid loss-2.0176, acc-0.3578, test loss-2.0206, acc-0.3679\n",
      "Iter-4940, train loss-1.9410, acc-0.4800, valid loss-2.0167, acc-0.3586, test loss-2.0197, acc-0.3683\n",
      "Iter-4950, train loss-2.0762, acc-0.2600, valid loss-2.0158, acc-0.3592, test loss-2.0188, acc-0.3686\n",
      "Iter-4960, train loss-2.0558, acc-0.3400, valid loss-2.0149, acc-0.3594, test loss-2.0179, acc-0.3689\n",
      "Iter-4970, train loss-1.9956, acc-0.3000, valid loss-2.0140, acc-0.3598, test loss-2.0170, acc-0.3701\n",
      "Iter-4980, train loss-1.9660, acc-0.4000, valid loss-2.0132, acc-0.3602, test loss-2.0162, acc-0.3709\n",
      "Iter-4990, train loss-1.9750, acc-0.4800, valid loss-2.0123, acc-0.3608, test loss-2.0153, acc-0.3711\n",
      "Iter-5000, train loss-2.0393, acc-0.4000, valid loss-2.0114, acc-0.3614, test loss-2.0145, acc-0.3718\n",
      "Iter-5010, train loss-2.0687, acc-0.3200, valid loss-2.0106, acc-0.3624, test loss-2.0136, acc-0.3723\n",
      "Iter-5020, train loss-2.0181, acc-0.3400, valid loss-2.0097, acc-0.3628, test loss-2.0127, acc-0.3728\n",
      "Iter-5030, train loss-2.0020, acc-0.4200, valid loss-2.0088, acc-0.3634, test loss-2.0119, acc-0.3731\n",
      "Iter-5040, train loss-1.9958, acc-0.3800, valid loss-2.0080, acc-0.3638, test loss-2.0110, acc-0.3735\n",
      "Iter-5050, train loss-2.0516, acc-0.2600, valid loss-2.0071, acc-0.3648, test loss-2.0101, acc-0.3745\n",
      "Iter-5060, train loss-1.9344, acc-0.4400, valid loss-2.0062, acc-0.3648, test loss-2.0092, acc-0.3753\n",
      "Iter-5070, train loss-1.9755, acc-0.4000, valid loss-2.0053, acc-0.3660, test loss-2.0084, acc-0.3756\n",
      "Iter-5080, train loss-2.0169, acc-0.3600, valid loss-2.0045, acc-0.3662, test loss-2.0075, acc-0.3758\n",
      "Iter-5090, train loss-1.9918, acc-0.4000, valid loss-2.0036, acc-0.3670, test loss-2.0067, acc-0.3768\n",
      "Iter-5100, train loss-2.0267, acc-0.3600, valid loss-2.0027, acc-0.3676, test loss-2.0058, acc-0.3774\n",
      "Iter-5110, train loss-2.0177, acc-0.3600, valid loss-2.0019, acc-0.3690, test loss-2.0049, acc-0.3782\n",
      "Iter-5120, train loss-1.9396, acc-0.4200, valid loss-2.0010, acc-0.3698, test loss-2.0041, acc-0.3790\n",
      "Iter-5130, train loss-2.0442, acc-0.2800, valid loss-2.0001, acc-0.3712, test loss-2.0031, acc-0.3798\n",
      "Iter-5140, train loss-2.0394, acc-0.2400, valid loss-1.9992, acc-0.3726, test loss-2.0022, acc-0.3808\n",
      "Iter-5150, train loss-2.0692, acc-0.2200, valid loss-1.9984, acc-0.3726, test loss-2.0015, acc-0.3809\n",
      "Iter-5160, train loss-1.9922, acc-0.3000, valid loss-1.9976, acc-0.3732, test loss-2.0006, acc-0.3816\n",
      "Iter-5170, train loss-1.9997, acc-0.3000, valid loss-1.9967, acc-0.3742, test loss-1.9997, acc-0.3819\n",
      "Iter-5180, train loss-2.0298, acc-0.4200, valid loss-1.9958, acc-0.3752, test loss-1.9988, acc-0.3828\n",
      "Iter-5190, train loss-2.0286, acc-0.3400, valid loss-1.9949, acc-0.3762, test loss-1.9979, acc-0.3834\n",
      "Iter-5200, train loss-1.9099, acc-0.4200, valid loss-1.9940, acc-0.3776, test loss-1.9970, acc-0.3839\n",
      "Iter-5210, train loss-1.9644, acc-0.4400, valid loss-1.9930, acc-0.3786, test loss-1.9961, acc-0.3845\n",
      "Iter-5220, train loss-2.0006, acc-0.3000, valid loss-1.9921, acc-0.3794, test loss-1.9951, acc-0.3850\n",
      "Iter-5230, train loss-1.9667, acc-0.4400, valid loss-1.9912, acc-0.3804, test loss-1.9943, acc-0.3856\n",
      "Iter-5240, train loss-1.9298, acc-0.5000, valid loss-1.9903, acc-0.3806, test loss-1.9933, acc-0.3864\n",
      "Iter-5250, train loss-1.9879, acc-0.3400, valid loss-1.9894, acc-0.3814, test loss-1.9924, acc-0.3869\n",
      "Iter-5260, train loss-2.0478, acc-0.2600, valid loss-1.9885, acc-0.3824, test loss-1.9915, acc-0.3866\n",
      "Iter-5270, train loss-1.9845, acc-0.3600, valid loss-1.9876, acc-0.3838, test loss-1.9907, acc-0.3870\n",
      "Iter-5280, train loss-1.9877, acc-0.3200, valid loss-1.9867, acc-0.3838, test loss-1.9898, acc-0.3876\n",
      "Iter-5290, train loss-1.9349, acc-0.4600, valid loss-1.9858, acc-0.3848, test loss-1.9889, acc-0.3876\n",
      "Iter-5300, train loss-1.9986, acc-0.3400, valid loss-1.9849, acc-0.3846, test loss-1.9880, acc-0.3878\n",
      "Iter-5310, train loss-1.9861, acc-0.3800, valid loss-1.9840, acc-0.3858, test loss-1.9871, acc-0.3877\n",
      "Iter-5320, train loss-1.9406, acc-0.4800, valid loss-1.9831, acc-0.3868, test loss-1.9862, acc-0.3892\n",
      "Iter-5330, train loss-1.9709, acc-0.4200, valid loss-1.9822, acc-0.3870, test loss-1.9853, acc-0.3901\n",
      "Iter-5340, train loss-1.9507, acc-0.4200, valid loss-1.9813, acc-0.3880, test loss-1.9844, acc-0.3913\n",
      "Iter-5350, train loss-1.9581, acc-0.4200, valid loss-1.9804, acc-0.3884, test loss-1.9835, acc-0.3924\n",
      "Iter-5360, train loss-1.9571, acc-0.4200, valid loss-1.9794, acc-0.3892, test loss-1.9825, acc-0.3925\n",
      "Iter-5370, train loss-2.0145, acc-0.4600, valid loss-1.9784, acc-0.3898, test loss-1.9815, acc-0.3931\n",
      "Iter-5380, train loss-2.0372, acc-0.3600, valid loss-1.9775, acc-0.3902, test loss-1.9807, acc-0.3933\n",
      "Iter-5390, train loss-1.9761, acc-0.4000, valid loss-1.9766, acc-0.3924, test loss-1.9797, acc-0.3940\n",
      "Iter-5400, train loss-2.0089, acc-0.3400, valid loss-1.9757, acc-0.3930, test loss-1.9788, acc-0.3949\n",
      "Iter-5410, train loss-1.9626, acc-0.3600, valid loss-1.9748, acc-0.3940, test loss-1.9779, acc-0.3960\n",
      "Iter-5420, train loss-1.9356, acc-0.4800, valid loss-1.9739, acc-0.3950, test loss-1.9771, acc-0.3968\n",
      "Iter-5430, train loss-1.8957, acc-0.4200, valid loss-1.9730, acc-0.3952, test loss-1.9762, acc-0.3974\n",
      "Iter-5440, train loss-2.0797, acc-0.2200, valid loss-1.9722, acc-0.3960, test loss-1.9753, acc-0.3984\n",
      "Iter-5450, train loss-1.9678, acc-0.3600, valid loss-1.9712, acc-0.3968, test loss-1.9744, acc-0.3991\n",
      "Iter-5460, train loss-2.0078, acc-0.3600, valid loss-1.9703, acc-0.3974, test loss-1.9735, acc-0.3993\n",
      "Iter-5470, train loss-1.9159, acc-0.4600, valid loss-1.9694, acc-0.3976, test loss-1.9726, acc-0.3995\n",
      "Iter-5480, train loss-1.9369, acc-0.4800, valid loss-1.9685, acc-0.3990, test loss-1.9716, acc-0.4000\n",
      "Iter-5490, train loss-1.9491, acc-0.3200, valid loss-1.9676, acc-0.4000, test loss-1.9707, acc-0.4004\n",
      "Iter-5500, train loss-1.9973, acc-0.3400, valid loss-1.9666, acc-0.4008, test loss-1.9698, acc-0.4010\n",
      "Iter-5510, train loss-1.8693, acc-0.5400, valid loss-1.9656, acc-0.4010, test loss-1.9688, acc-0.4009\n",
      "Iter-5520, train loss-1.9752, acc-0.3200, valid loss-1.9647, acc-0.4008, test loss-1.9679, acc-0.4015\n",
      "Iter-5530, train loss-1.9589, acc-0.3800, valid loss-1.9638, acc-0.4018, test loss-1.9670, acc-0.4020\n",
      "Iter-5540, train loss-1.9239, acc-0.5000, valid loss-1.9629, acc-0.4032, test loss-1.9661, acc-0.4025\n",
      "Iter-5550, train loss-1.8431, acc-0.4600, valid loss-1.9620, acc-0.4034, test loss-1.9651, acc-0.4032\n",
      "Iter-5560, train loss-1.9336, acc-0.5000, valid loss-1.9611, acc-0.4036, test loss-1.9643, acc-0.4037\n",
      "Iter-5570, train loss-2.0029, acc-0.4200, valid loss-1.9602, acc-0.4042, test loss-1.9634, acc-0.4044\n",
      "Iter-5580, train loss-1.9703, acc-0.4000, valid loss-1.9592, acc-0.4046, test loss-1.9624, acc-0.4049\n",
      "Iter-5590, train loss-1.8780, acc-0.4800, valid loss-1.9583, acc-0.4048, test loss-1.9614, acc-0.4047\n",
      "Iter-5600, train loss-1.9252, acc-0.4200, valid loss-1.9574, acc-0.4058, test loss-1.9605, acc-0.4052\n",
      "Iter-5610, train loss-1.9469, acc-0.4000, valid loss-1.9564, acc-0.4062, test loss-1.9596, acc-0.4049\n",
      "Iter-5620, train loss-1.9800, acc-0.3800, valid loss-1.9555, acc-0.4082, test loss-1.9587, acc-0.4060\n",
      "Iter-5630, train loss-1.9915, acc-0.4400, valid loss-1.9546, acc-0.4088, test loss-1.9577, acc-0.4064\n",
      "Iter-5640, train loss-2.0033, acc-0.4000, valid loss-1.9537, acc-0.4098, test loss-1.9569, acc-0.4073\n",
      "Iter-5650, train loss-2.0719, acc-0.2400, valid loss-1.9528, acc-0.4112, test loss-1.9559, acc-0.4082\n",
      "Iter-5660, train loss-1.9297, acc-0.4200, valid loss-1.9518, acc-0.4114, test loss-1.9549, acc-0.4083\n",
      "Iter-5670, train loss-2.0080, acc-0.4000, valid loss-1.9509, acc-0.4124, test loss-1.9540, acc-0.4088\n",
      "Iter-5680, train loss-1.9992, acc-0.3200, valid loss-1.9500, acc-0.4134, test loss-1.9531, acc-0.4099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-1.9545, acc-0.4000, valid loss-1.9491, acc-0.4146, test loss-1.9522, acc-0.4105\n",
      "Iter-5700, train loss-1.9369, acc-0.5400, valid loss-1.9481, acc-0.4154, test loss-1.9513, acc-0.4107\n",
      "Iter-5710, train loss-2.0549, acc-0.2400, valid loss-1.9472, acc-0.4158, test loss-1.9504, acc-0.4115\n",
      "Iter-5720, train loss-1.9082, acc-0.4200, valid loss-1.9462, acc-0.4166, test loss-1.9494, acc-0.4124\n",
      "Iter-5730, train loss-2.0293, acc-0.3200, valid loss-1.9453, acc-0.4174, test loss-1.9485, acc-0.4130\n",
      "Iter-5740, train loss-1.9197, acc-0.3600, valid loss-1.9444, acc-0.4178, test loss-1.9475, acc-0.4139\n",
      "Iter-5750, train loss-1.9280, acc-0.4200, valid loss-1.9433, acc-0.4184, test loss-1.9465, acc-0.4139\n",
      "Iter-5760, train loss-1.9832, acc-0.3800, valid loss-1.9424, acc-0.4188, test loss-1.9455, acc-0.4146\n",
      "Iter-5770, train loss-1.9413, acc-0.3600, valid loss-1.9414, acc-0.4194, test loss-1.9446, acc-0.4152\n",
      "Iter-5780, train loss-1.9847, acc-0.3600, valid loss-1.9405, acc-0.4194, test loss-1.9436, acc-0.4160\n",
      "Iter-5790, train loss-1.9548, acc-0.4200, valid loss-1.9396, acc-0.4198, test loss-1.9428, acc-0.4167\n",
      "Iter-5800, train loss-1.8983, acc-0.4200, valid loss-1.9386, acc-0.4204, test loss-1.9418, acc-0.4170\n",
      "Iter-5810, train loss-1.8780, acc-0.4200, valid loss-1.9377, acc-0.4204, test loss-1.9409, acc-0.4175\n",
      "Iter-5820, train loss-1.9330, acc-0.4400, valid loss-1.9368, acc-0.4206, test loss-1.9399, acc-0.4178\n",
      "Iter-5830, train loss-1.9550, acc-0.3400, valid loss-1.9358, acc-0.4216, test loss-1.9390, acc-0.4186\n",
      "Iter-5840, train loss-1.9501, acc-0.4000, valid loss-1.9349, acc-0.4222, test loss-1.9380, acc-0.4189\n",
      "Iter-5850, train loss-1.8946, acc-0.4400, valid loss-1.9340, acc-0.4222, test loss-1.9371, acc-0.4198\n",
      "Iter-5860, train loss-1.9299, acc-0.5000, valid loss-1.9330, acc-0.4230, test loss-1.9362, acc-0.4203\n",
      "Iter-5870, train loss-1.8806, acc-0.4400, valid loss-1.9320, acc-0.4232, test loss-1.9352, acc-0.4208\n",
      "Iter-5880, train loss-1.9303, acc-0.4600, valid loss-1.9311, acc-0.4240, test loss-1.9342, acc-0.4213\n",
      "Iter-5890, train loss-1.9801, acc-0.4400, valid loss-1.9300, acc-0.4238, test loss-1.9332, acc-0.4222\n",
      "Iter-5900, train loss-1.8930, acc-0.5200, valid loss-1.9291, acc-0.4238, test loss-1.9322, acc-0.4220\n",
      "Iter-5910, train loss-1.9423, acc-0.4400, valid loss-1.9282, acc-0.4252, test loss-1.9313, acc-0.4231\n",
      "Iter-5920, train loss-1.8878, acc-0.5000, valid loss-1.9272, acc-0.4256, test loss-1.9303, acc-0.4237\n",
      "Iter-5930, train loss-1.9304, acc-0.4000, valid loss-1.9263, acc-0.4260, test loss-1.9294, acc-0.4247\n",
      "Iter-5940, train loss-1.9936, acc-0.4400, valid loss-1.9253, acc-0.4266, test loss-1.9284, acc-0.4248\n",
      "Iter-5950, train loss-1.9620, acc-0.4400, valid loss-1.9244, acc-0.4280, test loss-1.9275, acc-0.4259\n",
      "Iter-5960, train loss-1.8631, acc-0.4800, valid loss-1.9234, acc-0.4286, test loss-1.9265, acc-0.4272\n",
      "Iter-5970, train loss-1.8692, acc-0.5200, valid loss-1.9225, acc-0.4298, test loss-1.9256, acc-0.4281\n",
      "Iter-5980, train loss-1.8476, acc-0.5000, valid loss-1.9215, acc-0.4302, test loss-1.9246, acc-0.4282\n",
      "Iter-5990, train loss-1.9106, acc-0.4600, valid loss-1.9205, acc-0.4304, test loss-1.9237, acc-0.4292\n",
      "Iter-6000, train loss-1.9623, acc-0.4800, valid loss-1.9196, acc-0.4306, test loss-1.9227, acc-0.4301\n",
      "Iter-6010, train loss-1.8309, acc-0.5400, valid loss-1.9185, acc-0.4312, test loss-1.9216, acc-0.4303\n",
      "Iter-6020, train loss-1.9063, acc-0.4000, valid loss-1.9176, acc-0.4314, test loss-1.9207, acc-0.4307\n",
      "Iter-6030, train loss-1.9130, acc-0.4200, valid loss-1.9166, acc-0.4324, test loss-1.9197, acc-0.4317\n",
      "Iter-6040, train loss-1.8240, acc-0.4800, valid loss-1.9156, acc-0.4326, test loss-1.9187, acc-0.4324\n",
      "Iter-6050, train loss-1.8314, acc-0.5200, valid loss-1.9147, acc-0.4338, test loss-1.9178, acc-0.4334\n",
      "Iter-6060, train loss-1.8209, acc-0.5000, valid loss-1.9138, acc-0.4344, test loss-1.9169, acc-0.4339\n",
      "Iter-6070, train loss-1.8845, acc-0.4400, valid loss-1.9128, acc-0.4350, test loss-1.9159, acc-0.4343\n",
      "Iter-6080, train loss-2.0173, acc-0.4000, valid loss-1.9118, acc-0.4354, test loss-1.9149, acc-0.4351\n",
      "Iter-6090, train loss-1.9478, acc-0.3800, valid loss-1.9109, acc-0.4352, test loss-1.9140, acc-0.4355\n",
      "Iter-6100, train loss-1.8442, acc-0.5000, valid loss-1.9098, acc-0.4360, test loss-1.9129, acc-0.4355\n",
      "Iter-6110, train loss-1.9469, acc-0.3800, valid loss-1.9088, acc-0.4366, test loss-1.9120, acc-0.4366\n",
      "Iter-6120, train loss-1.8872, acc-0.5200, valid loss-1.9078, acc-0.4372, test loss-1.9110, acc-0.4376\n",
      "Iter-6130, train loss-1.8512, acc-0.4400, valid loss-1.9069, acc-0.4382, test loss-1.9100, acc-0.4382\n",
      "Iter-6140, train loss-1.8736, acc-0.5200, valid loss-1.9059, acc-0.4382, test loss-1.9089, acc-0.4383\n",
      "Iter-6150, train loss-1.9034, acc-0.4600, valid loss-1.9049, acc-0.4390, test loss-1.9080, acc-0.4388\n",
      "Iter-6160, train loss-1.8996, acc-0.3200, valid loss-1.9039, acc-0.4396, test loss-1.9070, acc-0.4400\n",
      "Iter-6170, train loss-1.9527, acc-0.4000, valid loss-1.9030, acc-0.4400, test loss-1.9061, acc-0.4404\n",
      "Iter-6180, train loss-1.9948, acc-0.3800, valid loss-1.9020, acc-0.4408, test loss-1.9052, acc-0.4411\n",
      "Iter-6190, train loss-1.9172, acc-0.4600, valid loss-1.9011, acc-0.4410, test loss-1.9042, acc-0.4416\n",
      "Iter-6200, train loss-1.9383, acc-0.3600, valid loss-1.9002, acc-0.4408, test loss-1.9032, acc-0.4419\n",
      "Iter-6210, train loss-1.9040, acc-0.4200, valid loss-1.8991, acc-0.4412, test loss-1.9022, acc-0.4429\n",
      "Iter-6220, train loss-1.9250, acc-0.4000, valid loss-1.8981, acc-0.4420, test loss-1.9012, acc-0.4433\n",
      "Iter-6230, train loss-1.8780, acc-0.4200, valid loss-1.8971, acc-0.4428, test loss-1.9002, acc-0.4437\n",
      "Iter-6240, train loss-1.8869, acc-0.5400, valid loss-1.8961, acc-0.4432, test loss-1.8992, acc-0.4445\n",
      "Iter-6250, train loss-1.8437, acc-0.4800, valid loss-1.8951, acc-0.4436, test loss-1.8982, acc-0.4452\n",
      "Iter-6260, train loss-1.8458, acc-0.5000, valid loss-1.8941, acc-0.4438, test loss-1.8972, acc-0.4458\n",
      "Iter-6270, train loss-1.8383, acc-0.4400, valid loss-1.8931, acc-0.4444, test loss-1.8961, acc-0.4461\n",
      "Iter-6280, train loss-1.9055, acc-0.4200, valid loss-1.8920, acc-0.4454, test loss-1.8951, acc-0.4469\n",
      "Iter-6290, train loss-1.7975, acc-0.5400, valid loss-1.8910, acc-0.4458, test loss-1.8941, acc-0.4473\n",
      "Iter-6300, train loss-1.9787, acc-0.3400, valid loss-1.8900, acc-0.4462, test loss-1.8931, acc-0.4479\n",
      "Iter-6310, train loss-1.8217, acc-0.5400, valid loss-1.8890, acc-0.4462, test loss-1.8921, acc-0.4482\n",
      "Iter-6320, train loss-1.8796, acc-0.4000, valid loss-1.8880, acc-0.4470, test loss-1.8911, acc-0.4488\n",
      "Iter-6330, train loss-1.9652, acc-0.3600, valid loss-1.8870, acc-0.4480, test loss-1.8901, acc-0.4492\n",
      "Iter-6340, train loss-1.8462, acc-0.5200, valid loss-1.8860, acc-0.4488, test loss-1.8891, acc-0.4499\n",
      "Iter-6350, train loss-1.9049, acc-0.5000, valid loss-1.8851, acc-0.4496, test loss-1.8881, acc-0.4500\n",
      "Iter-6360, train loss-1.8214, acc-0.5800, valid loss-1.8840, acc-0.4504, test loss-1.8871, acc-0.4508\n",
      "Iter-6370, train loss-1.8624, acc-0.4600, valid loss-1.8830, acc-0.4518, test loss-1.8860, acc-0.4515\n",
      "Iter-6380, train loss-1.8542, acc-0.5000, valid loss-1.8819, acc-0.4522, test loss-1.8850, acc-0.4522\n",
      "Iter-6390, train loss-1.8074, acc-0.4600, valid loss-1.8809, acc-0.4524, test loss-1.8840, acc-0.4531\n",
      "Iter-6400, train loss-1.8259, acc-0.5000, valid loss-1.8800, acc-0.4532, test loss-1.8830, acc-0.4536\n",
      "Iter-6410, train loss-1.9748, acc-0.3200, valid loss-1.8790, acc-0.4532, test loss-1.8821, acc-0.4544\n",
      "Iter-6420, train loss-1.8705, acc-0.4400, valid loss-1.8780, acc-0.4532, test loss-1.8811, acc-0.4555\n",
      "Iter-6430, train loss-1.8785, acc-0.4200, valid loss-1.8769, acc-0.4540, test loss-1.8800, acc-0.4557\n",
      "Iter-6440, train loss-1.8446, acc-0.4800, valid loss-1.8759, acc-0.4552, test loss-1.8790, acc-0.4566\n",
      "Iter-6450, train loss-1.8957, acc-0.4600, valid loss-1.8749, acc-0.4560, test loss-1.8780, acc-0.4573\n",
      "Iter-6460, train loss-1.8896, acc-0.3600, valid loss-1.8739, acc-0.4558, test loss-1.8770, acc-0.4578\n",
      "Iter-6470, train loss-1.9205, acc-0.4200, valid loss-1.8729, acc-0.4570, test loss-1.8760, acc-0.4582\n",
      "Iter-6480, train loss-1.8913, acc-0.3800, valid loss-1.8719, acc-0.4580, test loss-1.8750, acc-0.4589\n",
      "Iter-6490, train loss-1.8402, acc-0.5600, valid loss-1.8709, acc-0.4590, test loss-1.8739, acc-0.4597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-1.8347, acc-0.4600, valid loss-1.8699, acc-0.4598, test loss-1.8729, acc-0.4603\n",
      "Iter-6510, train loss-1.9549, acc-0.3400, valid loss-1.8688, acc-0.4610, test loss-1.8719, acc-0.4609\n",
      "Iter-6520, train loss-1.9160, acc-0.3400, valid loss-1.8678, acc-0.4616, test loss-1.8708, acc-0.4610\n",
      "Iter-6530, train loss-1.8299, acc-0.6200, valid loss-1.8668, acc-0.4616, test loss-1.8698, acc-0.4620\n",
      "Iter-6540, train loss-1.8454, acc-0.4800, valid loss-1.8658, acc-0.4616, test loss-1.8688, acc-0.4629\n",
      "Iter-6550, train loss-1.8915, acc-0.4000, valid loss-1.8647, acc-0.4618, test loss-1.8677, acc-0.4631\n",
      "Iter-6560, train loss-1.8951, acc-0.4200, valid loss-1.8636, acc-0.4618, test loss-1.8666, acc-0.4634\n",
      "Iter-6570, train loss-1.8640, acc-0.4200, valid loss-1.8625, acc-0.4622, test loss-1.8656, acc-0.4638\n",
      "Iter-6580, train loss-1.8204, acc-0.5400, valid loss-1.8615, acc-0.4624, test loss-1.8646, acc-0.4642\n",
      "Iter-6590, train loss-1.9155, acc-0.3800, valid loss-1.8605, acc-0.4626, test loss-1.8636, acc-0.4644\n",
      "Iter-6600, train loss-1.8346, acc-0.4800, valid loss-1.8595, acc-0.4630, test loss-1.8626, acc-0.4647\n",
      "Iter-6610, train loss-1.8006, acc-0.5800, valid loss-1.8585, acc-0.4636, test loss-1.8615, acc-0.4652\n",
      "Iter-6620, train loss-1.9234, acc-0.3000, valid loss-1.8575, acc-0.4638, test loss-1.8605, acc-0.4650\n",
      "Iter-6630, train loss-1.8935, acc-0.4000, valid loss-1.8565, acc-0.4636, test loss-1.8595, acc-0.4652\n",
      "Iter-6640, train loss-1.8941, acc-0.4800, valid loss-1.8555, acc-0.4636, test loss-1.8585, acc-0.4660\n",
      "Iter-6650, train loss-1.8996, acc-0.4400, valid loss-1.8545, acc-0.4642, test loss-1.8575, acc-0.4662\n",
      "Iter-6660, train loss-1.8282, acc-0.4400, valid loss-1.8535, acc-0.4644, test loss-1.8565, acc-0.4667\n",
      "Iter-6670, train loss-1.8001, acc-0.5000, valid loss-1.8525, acc-0.4646, test loss-1.8554, acc-0.4675\n",
      "Iter-6680, train loss-1.8152, acc-0.6200, valid loss-1.8514, acc-0.4656, test loss-1.8544, acc-0.4679\n",
      "Iter-6690, train loss-1.7969, acc-0.5200, valid loss-1.8504, acc-0.4664, test loss-1.8533, acc-0.4690\n",
      "Iter-6700, train loss-1.8452, acc-0.5200, valid loss-1.8493, acc-0.4666, test loss-1.8523, acc-0.4696\n",
      "Iter-6710, train loss-1.8479, acc-0.4600, valid loss-1.8482, acc-0.4672, test loss-1.8512, acc-0.4697\n",
      "Iter-6720, train loss-1.9019, acc-0.3400, valid loss-1.8471, acc-0.4674, test loss-1.8501, acc-0.4704\n",
      "Iter-6730, train loss-1.9202, acc-0.3800, valid loss-1.8461, acc-0.4676, test loss-1.8491, acc-0.4709\n",
      "Iter-6740, train loss-1.9035, acc-0.4200, valid loss-1.8450, acc-0.4678, test loss-1.8480, acc-0.4711\n",
      "Iter-6750, train loss-1.8854, acc-0.4600, valid loss-1.8440, acc-0.4680, test loss-1.8470, acc-0.4716\n",
      "Iter-6760, train loss-1.8351, acc-0.5600, valid loss-1.8430, acc-0.4690, test loss-1.8460, acc-0.4722\n",
      "Iter-6770, train loss-1.8167, acc-0.5000, valid loss-1.8419, acc-0.4688, test loss-1.8449, acc-0.4727\n",
      "Iter-6780, train loss-1.8930, acc-0.4600, valid loss-1.8409, acc-0.4694, test loss-1.8438, acc-0.4731\n",
      "Iter-6790, train loss-1.7412, acc-0.5400, valid loss-1.8398, acc-0.4700, test loss-1.8427, acc-0.4734\n",
      "Iter-6800, train loss-1.8422, acc-0.4600, valid loss-1.8387, acc-0.4704, test loss-1.8417, acc-0.4739\n",
      "Iter-6810, train loss-1.8196, acc-0.5200, valid loss-1.8377, acc-0.4708, test loss-1.8406, acc-0.4744\n",
      "Iter-6820, train loss-1.8315, acc-0.4400, valid loss-1.8366, acc-0.4712, test loss-1.8395, acc-0.4751\n",
      "Iter-6830, train loss-1.8584, acc-0.3600, valid loss-1.8355, acc-0.4714, test loss-1.8384, acc-0.4756\n",
      "Iter-6840, train loss-1.8467, acc-0.4200, valid loss-1.8344, acc-0.4714, test loss-1.8374, acc-0.4759\n",
      "Iter-6850, train loss-1.8027, acc-0.5200, valid loss-1.8334, acc-0.4718, test loss-1.8363, acc-0.4773\n",
      "Iter-6860, train loss-1.9199, acc-0.3400, valid loss-1.8323, acc-0.4724, test loss-1.8352, acc-0.4776\n",
      "Iter-6870, train loss-1.8149, acc-0.4400, valid loss-1.8312, acc-0.4728, test loss-1.8341, acc-0.4778\n",
      "Iter-6880, train loss-1.8249, acc-0.4000, valid loss-1.8301, acc-0.4734, test loss-1.8330, acc-0.4791\n",
      "Iter-6890, train loss-1.7957, acc-0.5000, valid loss-1.8291, acc-0.4734, test loss-1.8320, acc-0.4798\n",
      "Iter-6900, train loss-1.8322, acc-0.4600, valid loss-1.8280, acc-0.4734, test loss-1.8310, acc-0.4801\n",
      "Iter-6910, train loss-1.8403, acc-0.4800, valid loss-1.8270, acc-0.4736, test loss-1.8299, acc-0.4809\n",
      "Iter-6920, train loss-1.8253, acc-0.4800, valid loss-1.8259, acc-0.4738, test loss-1.8288, acc-0.4816\n",
      "Iter-6930, train loss-1.7519, acc-0.5200, valid loss-1.8248, acc-0.4742, test loss-1.8278, acc-0.4820\n",
      "Iter-6940, train loss-1.7859, acc-0.4800, valid loss-1.8238, acc-0.4758, test loss-1.8267, acc-0.4829\n",
      "Iter-6950, train loss-1.7989, acc-0.4800, valid loss-1.8227, acc-0.4756, test loss-1.8256, acc-0.4830\n",
      "Iter-6960, train loss-1.7698, acc-0.4800, valid loss-1.8217, acc-0.4760, test loss-1.8246, acc-0.4833\n",
      "Iter-6970, train loss-1.7619, acc-0.4200, valid loss-1.8206, acc-0.4760, test loss-1.8234, acc-0.4839\n",
      "Iter-6980, train loss-1.8160, acc-0.4800, valid loss-1.8195, acc-0.4766, test loss-1.8223, acc-0.4840\n",
      "Iter-6990, train loss-1.7302, acc-0.5400, valid loss-1.8184, acc-0.4770, test loss-1.8213, acc-0.4845\n",
      "Iter-7000, train loss-1.7717, acc-0.5000, valid loss-1.8174, acc-0.4778, test loss-1.8202, acc-0.4849\n",
      "Iter-7010, train loss-1.7555, acc-0.6000, valid loss-1.8163, acc-0.4790, test loss-1.8191, acc-0.4852\n",
      "Iter-7020, train loss-1.8515, acc-0.4000, valid loss-1.8152, acc-0.4792, test loss-1.8181, acc-0.4857\n",
      "Iter-7030, train loss-1.8335, acc-0.5000, valid loss-1.8141, acc-0.4796, test loss-1.8170, acc-0.4857\n",
      "Iter-7040, train loss-1.8109, acc-0.4400, valid loss-1.8131, acc-0.4798, test loss-1.8159, acc-0.4857\n",
      "Iter-7050, train loss-1.7790, acc-0.5400, valid loss-1.8120, acc-0.4808, test loss-1.8148, acc-0.4866\n",
      "Iter-7060, train loss-1.8052, acc-0.3800, valid loss-1.8110, acc-0.4808, test loss-1.8138, acc-0.4873\n",
      "Iter-7070, train loss-1.8074, acc-0.5200, valid loss-1.8099, acc-0.4816, test loss-1.8127, acc-0.4878\n",
      "Iter-7080, train loss-1.8437, acc-0.4600, valid loss-1.8089, acc-0.4818, test loss-1.8117, acc-0.4886\n",
      "Iter-7090, train loss-1.7986, acc-0.5800, valid loss-1.8077, acc-0.4826, test loss-1.8105, acc-0.4888\n",
      "Iter-7100, train loss-1.7299, acc-0.4800, valid loss-1.8066, acc-0.4826, test loss-1.8094, acc-0.4895\n",
      "Iter-7110, train loss-1.8987, acc-0.4800, valid loss-1.8056, acc-0.4826, test loss-1.8083, acc-0.4896\n",
      "Iter-7120, train loss-1.7342, acc-0.5400, valid loss-1.8045, acc-0.4826, test loss-1.8072, acc-0.4899\n",
      "Iter-7130, train loss-1.8308, acc-0.4800, valid loss-1.8034, acc-0.4838, test loss-1.8061, acc-0.4910\n",
      "Iter-7140, train loss-1.8322, acc-0.4200, valid loss-1.8023, acc-0.4842, test loss-1.8050, acc-0.4920\n",
      "Iter-7150, train loss-1.8655, acc-0.3200, valid loss-1.8012, acc-0.4844, test loss-1.8040, acc-0.4927\n",
      "Iter-7160, train loss-1.7258, acc-0.6000, valid loss-1.8001, acc-0.4854, test loss-1.8028, acc-0.4929\n",
      "Iter-7170, train loss-1.8291, acc-0.4600, valid loss-1.7990, acc-0.4864, test loss-1.8017, acc-0.4931\n",
      "Iter-7180, train loss-1.8836, acc-0.4400, valid loss-1.7979, acc-0.4866, test loss-1.8006, acc-0.4933\n",
      "Iter-7190, train loss-1.7757, acc-0.5000, valid loss-1.7967, acc-0.4880, test loss-1.7994, acc-0.4943\n",
      "Iter-7200, train loss-1.8165, acc-0.4600, valid loss-1.7956, acc-0.4884, test loss-1.7983, acc-0.4953\n",
      "Iter-7210, train loss-1.8642, acc-0.4200, valid loss-1.7945, acc-0.4882, test loss-1.7972, acc-0.4958\n",
      "Iter-7220, train loss-1.8511, acc-0.3800, valid loss-1.7935, acc-0.4886, test loss-1.7962, acc-0.4961\n",
      "Iter-7230, train loss-1.8723, acc-0.4200, valid loss-1.7924, acc-0.4886, test loss-1.7951, acc-0.4963\n",
      "Iter-7240, train loss-1.8250, acc-0.4000, valid loss-1.7914, acc-0.4890, test loss-1.7940, acc-0.4963\n",
      "Iter-7250, train loss-1.8540, acc-0.3800, valid loss-1.7902, acc-0.4892, test loss-1.7929, acc-0.4966\n",
      "Iter-7260, train loss-1.8418, acc-0.4400, valid loss-1.7891, acc-0.4898, test loss-1.7917, acc-0.4974\n",
      "Iter-7270, train loss-1.8573, acc-0.4400, valid loss-1.7880, acc-0.4902, test loss-1.7907, acc-0.4983\n",
      "Iter-7280, train loss-1.7332, acc-0.5800, valid loss-1.7869, acc-0.4906, test loss-1.7896, acc-0.4985\n",
      "Iter-7290, train loss-1.7840, acc-0.5200, valid loss-1.7857, acc-0.4912, test loss-1.7884, acc-0.4989\n",
      "Iter-7300, train loss-1.7403, acc-0.5200, valid loss-1.7847, acc-0.4916, test loss-1.7873, acc-0.4992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-1.7542, acc-0.4800, valid loss-1.7836, acc-0.4918, test loss-1.7862, acc-0.5000\n",
      "Iter-7320, train loss-1.8533, acc-0.4400, valid loss-1.7824, acc-0.4924, test loss-1.7851, acc-0.5002\n",
      "Iter-7330, train loss-1.7751, acc-0.6000, valid loss-1.7813, acc-0.4928, test loss-1.7840, acc-0.5008\n",
      "Iter-7340, train loss-1.7520, acc-0.4800, valid loss-1.7802, acc-0.4940, test loss-1.7829, acc-0.5011\n",
      "Iter-7350, train loss-1.8049, acc-0.5400, valid loss-1.7792, acc-0.4952, test loss-1.7818, acc-0.5013\n",
      "Iter-7360, train loss-1.8687, acc-0.3400, valid loss-1.7780, acc-0.4958, test loss-1.7807, acc-0.5021\n",
      "Iter-7370, train loss-1.7713, acc-0.5600, valid loss-1.7769, acc-0.4966, test loss-1.7796, acc-0.5029\n",
      "Iter-7380, train loss-1.8606, acc-0.4800, valid loss-1.7758, acc-0.4974, test loss-1.7785, acc-0.5027\n",
      "Iter-7390, train loss-1.7681, acc-0.4600, valid loss-1.7747, acc-0.4976, test loss-1.7773, acc-0.5030\n",
      "Iter-7400, train loss-1.8418, acc-0.4200, valid loss-1.7736, acc-0.4984, test loss-1.7762, acc-0.5039\n",
      "Iter-7410, train loss-1.7916, acc-0.5400, valid loss-1.7725, acc-0.4990, test loss-1.7751, acc-0.5041\n",
      "Iter-7420, train loss-1.7732, acc-0.4800, valid loss-1.7714, acc-0.5002, test loss-1.7740, acc-0.5047\n",
      "Iter-7430, train loss-1.8525, acc-0.3600, valid loss-1.7703, acc-0.5012, test loss-1.7729, acc-0.5050\n",
      "Iter-7440, train loss-1.7531, acc-0.4600, valid loss-1.7691, acc-0.5008, test loss-1.7717, acc-0.5053\n",
      "Iter-7450, train loss-1.9075, acc-0.4400, valid loss-1.7680, acc-0.5014, test loss-1.7706, acc-0.5055\n",
      "Iter-7460, train loss-1.6979, acc-0.5800, valid loss-1.7669, acc-0.5022, test loss-1.7695, acc-0.5059\n",
      "Iter-7470, train loss-1.8119, acc-0.5400, valid loss-1.7658, acc-0.5024, test loss-1.7683, acc-0.5066\n",
      "Iter-7480, train loss-1.7337, acc-0.5600, valid loss-1.7646, acc-0.5028, test loss-1.7672, acc-0.5071\n",
      "Iter-7490, train loss-1.7257, acc-0.5200, valid loss-1.7634, acc-0.5038, test loss-1.7660, acc-0.5081\n",
      "Iter-7500, train loss-1.7684, acc-0.5000, valid loss-1.7623, acc-0.5040, test loss-1.7649, acc-0.5087\n",
      "Iter-7510, train loss-1.8243, acc-0.4400, valid loss-1.7612, acc-0.5042, test loss-1.7637, acc-0.5093\n",
      "Iter-7520, train loss-1.8374, acc-0.4000, valid loss-1.7601, acc-0.5046, test loss-1.7626, acc-0.5099\n",
      "Iter-7530, train loss-1.7479, acc-0.4400, valid loss-1.7589, acc-0.5050, test loss-1.7614, acc-0.5107\n",
      "Iter-7540, train loss-1.7172, acc-0.4800, valid loss-1.7578, acc-0.5046, test loss-1.7603, acc-0.5112\n",
      "Iter-7550, train loss-1.7410, acc-0.5000, valid loss-1.7567, acc-0.5048, test loss-1.7592, acc-0.5114\n",
      "Iter-7560, train loss-1.7262, acc-0.5200, valid loss-1.7555, acc-0.5064, test loss-1.7580, acc-0.5120\n",
      "Iter-7570, train loss-1.7491, acc-0.5200, valid loss-1.7544, acc-0.5066, test loss-1.7569, acc-0.5124\n",
      "Iter-7580, train loss-1.7995, acc-0.5400, valid loss-1.7533, acc-0.5074, test loss-1.7558, acc-0.5126\n",
      "Iter-7590, train loss-1.6982, acc-0.5600, valid loss-1.7522, acc-0.5078, test loss-1.7547, acc-0.5127\n",
      "Iter-7600, train loss-1.7511, acc-0.5000, valid loss-1.7511, acc-0.5074, test loss-1.7536, acc-0.5129\n",
      "Iter-7610, train loss-1.7460, acc-0.5200, valid loss-1.7500, acc-0.5082, test loss-1.7524, acc-0.5132\n",
      "Iter-7620, train loss-1.7631, acc-0.5200, valid loss-1.7488, acc-0.5092, test loss-1.7512, acc-0.5141\n",
      "Iter-7630, train loss-1.7288, acc-0.4800, valid loss-1.7476, acc-0.5102, test loss-1.7500, acc-0.5145\n",
      "Iter-7640, train loss-1.7640, acc-0.4800, valid loss-1.7465, acc-0.5112, test loss-1.7489, acc-0.5148\n",
      "Iter-7650, train loss-1.8383, acc-0.3800, valid loss-1.7454, acc-0.5120, test loss-1.7478, acc-0.5153\n",
      "Iter-7660, train loss-1.7449, acc-0.5400, valid loss-1.7444, acc-0.5122, test loss-1.7467, acc-0.5155\n",
      "Iter-7670, train loss-1.8335, acc-0.4000, valid loss-1.7432, acc-0.5132, test loss-1.7456, acc-0.5161\n",
      "Iter-7680, train loss-1.7451, acc-0.6000, valid loss-1.7421, acc-0.5138, test loss-1.7445, acc-0.5166\n",
      "Iter-7690, train loss-1.7125, acc-0.5400, valid loss-1.7410, acc-0.5144, test loss-1.7433, acc-0.5166\n",
      "Iter-7700, train loss-1.7810, acc-0.5000, valid loss-1.7398, acc-0.5152, test loss-1.7421, acc-0.5174\n",
      "Iter-7710, train loss-1.8212, acc-0.5200, valid loss-1.7386, acc-0.5154, test loss-1.7410, acc-0.5174\n",
      "Iter-7720, train loss-1.6774, acc-0.5600, valid loss-1.7374, acc-0.5158, test loss-1.7398, acc-0.5179\n",
      "Iter-7730, train loss-1.7933, acc-0.3800, valid loss-1.7362, acc-0.5156, test loss-1.7386, acc-0.5187\n",
      "Iter-7740, train loss-1.8298, acc-0.4200, valid loss-1.7351, acc-0.5162, test loss-1.7375, acc-0.5192\n",
      "Iter-7750, train loss-1.7944, acc-0.4000, valid loss-1.7340, acc-0.5170, test loss-1.7364, acc-0.5196\n",
      "Iter-7760, train loss-1.7059, acc-0.5000, valid loss-1.7329, acc-0.5188, test loss-1.7352, acc-0.5202\n",
      "Iter-7770, train loss-1.7499, acc-0.4400, valid loss-1.7317, acc-0.5192, test loss-1.7341, acc-0.5207\n",
      "Iter-7780, train loss-1.8761, acc-0.3000, valid loss-1.7307, acc-0.5194, test loss-1.7330, acc-0.5215\n",
      "Iter-7790, train loss-1.7143, acc-0.6200, valid loss-1.7295, acc-0.5196, test loss-1.7319, acc-0.5222\n",
      "Iter-7800, train loss-1.7317, acc-0.5000, valid loss-1.7284, acc-0.5208, test loss-1.7308, acc-0.5228\n",
      "Iter-7810, train loss-1.7971, acc-0.5000, valid loss-1.7273, acc-0.5212, test loss-1.7296, acc-0.5229\n",
      "Iter-7820, train loss-1.7642, acc-0.3400, valid loss-1.7261, acc-0.5214, test loss-1.7284, acc-0.5232\n",
      "Iter-7830, train loss-1.7470, acc-0.4800, valid loss-1.7250, acc-0.5226, test loss-1.7273, acc-0.5242\n",
      "Iter-7840, train loss-1.6623, acc-0.5600, valid loss-1.7239, acc-0.5226, test loss-1.7262, acc-0.5244\n",
      "Iter-7850, train loss-1.7452, acc-0.4600, valid loss-1.7227, acc-0.5234, test loss-1.7250, acc-0.5247\n",
      "Iter-7860, train loss-1.6706, acc-0.5400, valid loss-1.7216, acc-0.5234, test loss-1.7238, acc-0.5254\n",
      "Iter-7870, train loss-1.8103, acc-0.4200, valid loss-1.7205, acc-0.5236, test loss-1.7227, acc-0.5257\n",
      "Iter-7880, train loss-1.6257, acc-0.6400, valid loss-1.7193, acc-0.5234, test loss-1.7215, acc-0.5260\n",
      "Iter-7890, train loss-1.6978, acc-0.5200, valid loss-1.7182, acc-0.5248, test loss-1.7203, acc-0.5263\n",
      "Iter-7900, train loss-1.6361, acc-0.6200, valid loss-1.7171, acc-0.5250, test loss-1.7192, acc-0.5267\n",
      "Iter-7910, train loss-1.6061, acc-0.6800, valid loss-1.7159, acc-0.5260, test loss-1.7181, acc-0.5269\n",
      "Iter-7920, train loss-1.7176, acc-0.6400, valid loss-1.7147, acc-0.5262, test loss-1.7168, acc-0.5276\n",
      "Iter-7930, train loss-1.6416, acc-0.5800, valid loss-1.7136, acc-0.5264, test loss-1.7157, acc-0.5278\n",
      "Iter-7940, train loss-1.8313, acc-0.4400, valid loss-1.7124, acc-0.5264, test loss-1.7146, acc-0.5280\n",
      "Iter-7950, train loss-1.6543, acc-0.6000, valid loss-1.7113, acc-0.5278, test loss-1.7134, acc-0.5288\n",
      "Iter-7960, train loss-1.8026, acc-0.3600, valid loss-1.7101, acc-0.5286, test loss-1.7123, acc-0.5289\n",
      "Iter-7970, train loss-1.8236, acc-0.5000, valid loss-1.7090, acc-0.5290, test loss-1.7111, acc-0.5290\n",
      "Iter-7980, train loss-1.7284, acc-0.4600, valid loss-1.7078, acc-0.5290, test loss-1.7099, acc-0.5295\n",
      "Iter-7990, train loss-1.6827, acc-0.5800, valid loss-1.7067, acc-0.5298, test loss-1.7088, acc-0.5297\n",
      "Iter-8000, train loss-1.7557, acc-0.4800, valid loss-1.7055, acc-0.5306, test loss-1.7076, acc-0.5296\n",
      "Iter-8010, train loss-1.8145, acc-0.3800, valid loss-1.7043, acc-0.5312, test loss-1.7064, acc-0.5294\n",
      "Iter-8020, train loss-1.7066, acc-0.5000, valid loss-1.7031, acc-0.5318, test loss-1.7052, acc-0.5300\n",
      "Iter-8030, train loss-1.7235, acc-0.4600, valid loss-1.7020, acc-0.5316, test loss-1.7040, acc-0.5302\n",
      "Iter-8040, train loss-1.8174, acc-0.5000, valid loss-1.7008, acc-0.5318, test loss-1.7029, acc-0.5302\n",
      "Iter-8050, train loss-1.7049, acc-0.4800, valid loss-1.6997, acc-0.5316, test loss-1.7018, acc-0.5305\n",
      "Iter-8060, train loss-1.7363, acc-0.4800, valid loss-1.6985, acc-0.5324, test loss-1.7006, acc-0.5308\n",
      "Iter-8070, train loss-1.8275, acc-0.4800, valid loss-1.6974, acc-0.5326, test loss-1.6994, acc-0.5310\n",
      "Iter-8080, train loss-1.6062, acc-0.6200, valid loss-1.6962, acc-0.5332, test loss-1.6983, acc-0.5324\n",
      "Iter-8090, train loss-1.8149, acc-0.4000, valid loss-1.6951, acc-0.5340, test loss-1.6971, acc-0.5330\n",
      "Iter-8100, train loss-1.7272, acc-0.3800, valid loss-1.6939, acc-0.5346, test loss-1.6960, acc-0.5338\n",
      "Iter-8110, train loss-1.7978, acc-0.5600, valid loss-1.6928, acc-0.5348, test loss-1.6948, acc-0.5341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-1.7152, acc-0.5200, valid loss-1.6916, acc-0.5356, test loss-1.6936, acc-0.5345\n",
      "Iter-8130, train loss-1.7496, acc-0.4600, valid loss-1.6904, acc-0.5364, test loss-1.6925, acc-0.5349\n",
      "Iter-8140, train loss-1.6983, acc-0.5400, valid loss-1.6892, acc-0.5364, test loss-1.6913, acc-0.5349\n",
      "Iter-8150, train loss-1.6487, acc-0.5800, valid loss-1.6881, acc-0.5370, test loss-1.6901, acc-0.5354\n",
      "Iter-8160, train loss-1.6912, acc-0.5200, valid loss-1.6869, acc-0.5374, test loss-1.6890, acc-0.5355\n",
      "Iter-8170, train loss-1.7141, acc-0.5600, valid loss-1.6857, acc-0.5384, test loss-1.6878, acc-0.5362\n",
      "Iter-8180, train loss-1.7497, acc-0.5000, valid loss-1.6845, acc-0.5388, test loss-1.6865, acc-0.5371\n",
      "Iter-8190, train loss-1.7634, acc-0.4600, valid loss-1.6833, acc-0.5392, test loss-1.6853, acc-0.5378\n",
      "Iter-8200, train loss-1.6594, acc-0.6200, valid loss-1.6822, acc-0.5392, test loss-1.6842, acc-0.5382\n",
      "Iter-8210, train loss-1.6884, acc-0.6000, valid loss-1.6811, acc-0.5392, test loss-1.6831, acc-0.5382\n",
      "Iter-8220, train loss-1.5919, acc-0.6000, valid loss-1.6799, acc-0.5400, test loss-1.6819, acc-0.5385\n",
      "Iter-8230, train loss-1.6882, acc-0.5400, valid loss-1.6787, acc-0.5402, test loss-1.6807, acc-0.5390\n",
      "Iter-8240, train loss-1.6888, acc-0.5600, valid loss-1.6776, acc-0.5406, test loss-1.6796, acc-0.5392\n",
      "Iter-8250, train loss-1.7223, acc-0.5800, valid loss-1.6764, acc-0.5414, test loss-1.6784, acc-0.5398\n",
      "Iter-8260, train loss-1.6598, acc-0.5600, valid loss-1.6752, acc-0.5422, test loss-1.6772, acc-0.5403\n",
      "Iter-8270, train loss-1.6229, acc-0.5800, valid loss-1.6741, acc-0.5422, test loss-1.6761, acc-0.5402\n",
      "Iter-8280, train loss-1.5424, acc-0.6600, valid loss-1.6729, acc-0.5432, test loss-1.6749, acc-0.5406\n",
      "Iter-8290, train loss-1.5953, acc-0.6400, valid loss-1.6717, acc-0.5432, test loss-1.6737, acc-0.5411\n",
      "Iter-8300, train loss-1.6859, acc-0.5200, valid loss-1.6706, acc-0.5440, test loss-1.6726, acc-0.5418\n",
      "Iter-8310, train loss-1.5322, acc-0.6000, valid loss-1.6695, acc-0.5438, test loss-1.6714, acc-0.5418\n",
      "Iter-8320, train loss-1.7346, acc-0.4600, valid loss-1.6683, acc-0.5440, test loss-1.6702, acc-0.5424\n",
      "Iter-8330, train loss-1.5736, acc-0.6800, valid loss-1.6671, acc-0.5440, test loss-1.6690, acc-0.5425\n",
      "Iter-8340, train loss-1.6113, acc-0.6000, valid loss-1.6660, acc-0.5436, test loss-1.6679, acc-0.5426\n",
      "Iter-8350, train loss-1.7590, acc-0.4400, valid loss-1.6649, acc-0.5438, test loss-1.6668, acc-0.5430\n",
      "Iter-8360, train loss-1.6158, acc-0.5800, valid loss-1.6637, acc-0.5434, test loss-1.6656, acc-0.5442\n",
      "Iter-8370, train loss-1.6956, acc-0.5600, valid loss-1.6625, acc-0.5436, test loss-1.6644, acc-0.5447\n",
      "Iter-8380, train loss-1.5428, acc-0.6600, valid loss-1.6614, acc-0.5436, test loss-1.6633, acc-0.5447\n",
      "Iter-8390, train loss-1.7494, acc-0.5200, valid loss-1.6602, acc-0.5438, test loss-1.6621, acc-0.5449\n",
      "Iter-8400, train loss-1.6935, acc-0.5800, valid loss-1.6591, acc-0.5440, test loss-1.6609, acc-0.5456\n",
      "Iter-8410, train loss-1.6369, acc-0.5200, valid loss-1.6579, acc-0.5446, test loss-1.6597, acc-0.5457\n",
      "Iter-8420, train loss-1.6623, acc-0.4800, valid loss-1.6568, acc-0.5450, test loss-1.6586, acc-0.5458\n",
      "Iter-8430, train loss-1.7250, acc-0.6000, valid loss-1.6556, acc-0.5456, test loss-1.6574, acc-0.5465\n",
      "Iter-8440, train loss-1.6869, acc-0.5800, valid loss-1.6545, acc-0.5454, test loss-1.6563, acc-0.5466\n",
      "Iter-8450, train loss-1.5338, acc-0.7200, valid loss-1.6533, acc-0.5464, test loss-1.6551, acc-0.5469\n",
      "Iter-8460, train loss-1.5791, acc-0.7400, valid loss-1.6522, acc-0.5468, test loss-1.6540, acc-0.5473\n",
      "Iter-8470, train loss-1.5858, acc-0.5400, valid loss-1.6510, acc-0.5468, test loss-1.6528, acc-0.5477\n",
      "Iter-8480, train loss-1.7969, acc-0.4000, valid loss-1.6499, acc-0.5480, test loss-1.6516, acc-0.5478\n",
      "Iter-8490, train loss-1.6732, acc-0.5800, valid loss-1.6488, acc-0.5490, test loss-1.6505, acc-0.5481\n",
      "Iter-8500, train loss-1.6382, acc-0.5400, valid loss-1.6476, acc-0.5490, test loss-1.6493, acc-0.5485\n",
      "Iter-8510, train loss-1.6946, acc-0.4600, valid loss-1.6465, acc-0.5492, test loss-1.6482, acc-0.5487\n",
      "Iter-8520, train loss-1.6437, acc-0.6000, valid loss-1.6454, acc-0.5498, test loss-1.6471, acc-0.5489\n",
      "Iter-8530, train loss-1.6183, acc-0.5400, valid loss-1.6442, acc-0.5508, test loss-1.6459, acc-0.5496\n",
      "Iter-8540, train loss-1.6799, acc-0.4400, valid loss-1.6430, acc-0.5508, test loss-1.6447, acc-0.5499\n",
      "Iter-8550, train loss-1.8201, acc-0.4600, valid loss-1.6419, acc-0.5518, test loss-1.6436, acc-0.5505\n",
      "Iter-8560, train loss-1.7191, acc-0.4400, valid loss-1.6408, acc-0.5524, test loss-1.6424, acc-0.5511\n",
      "Iter-8570, train loss-1.7538, acc-0.4800, valid loss-1.6397, acc-0.5526, test loss-1.6413, acc-0.5515\n",
      "Iter-8580, train loss-1.7118, acc-0.4200, valid loss-1.6386, acc-0.5532, test loss-1.6402, acc-0.5517\n",
      "Iter-8590, train loss-1.6652, acc-0.6400, valid loss-1.6373, acc-0.5544, test loss-1.6389, acc-0.5520\n",
      "Iter-8600, train loss-1.6278, acc-0.6000, valid loss-1.6362, acc-0.5538, test loss-1.6378, acc-0.5523\n",
      "Iter-8610, train loss-1.5533, acc-0.6600, valid loss-1.6349, acc-0.5550, test loss-1.6365, acc-0.5529\n",
      "Iter-8620, train loss-1.5791, acc-0.6000, valid loss-1.6337, acc-0.5548, test loss-1.6353, acc-0.5531\n",
      "Iter-8630, train loss-1.6534, acc-0.6400, valid loss-1.6326, acc-0.5554, test loss-1.6341, acc-0.5536\n",
      "Iter-8640, train loss-1.6566, acc-0.5000, valid loss-1.6315, acc-0.5556, test loss-1.6330, acc-0.5537\n",
      "Iter-8650, train loss-1.6598, acc-0.6600, valid loss-1.6304, acc-0.5558, test loss-1.6319, acc-0.5538\n",
      "Iter-8660, train loss-1.7444, acc-0.5200, valid loss-1.6293, acc-0.5556, test loss-1.6308, acc-0.5540\n",
      "Iter-8670, train loss-1.6441, acc-0.6200, valid loss-1.6282, acc-0.5560, test loss-1.6297, acc-0.5550\n",
      "Iter-8680, train loss-1.7886, acc-0.3800, valid loss-1.6271, acc-0.5562, test loss-1.6285, acc-0.5554\n",
      "Iter-8690, train loss-1.6411, acc-0.5200, valid loss-1.6259, acc-0.5568, test loss-1.6274, acc-0.5557\n",
      "Iter-8700, train loss-1.7441, acc-0.3800, valid loss-1.6248, acc-0.5572, test loss-1.6262, acc-0.5557\n",
      "Iter-8710, train loss-1.6819, acc-0.5400, valid loss-1.6236, acc-0.5574, test loss-1.6251, acc-0.5558\n",
      "Iter-8720, train loss-1.5835, acc-0.5600, valid loss-1.6225, acc-0.5578, test loss-1.6240, acc-0.5560\n",
      "Iter-8730, train loss-1.6613, acc-0.5000, valid loss-1.6214, acc-0.5586, test loss-1.6228, acc-0.5562\n",
      "Iter-8740, train loss-1.6569, acc-0.5400, valid loss-1.6202, acc-0.5588, test loss-1.6217, acc-0.5565\n",
      "Iter-8750, train loss-1.7255, acc-0.4200, valid loss-1.6191, acc-0.5600, test loss-1.6206, acc-0.5569\n",
      "Iter-8760, train loss-1.5460, acc-0.6200, valid loss-1.6179, acc-0.5606, test loss-1.6194, acc-0.5575\n",
      "Iter-8770, train loss-1.7492, acc-0.5200, valid loss-1.6168, acc-0.5608, test loss-1.6182, acc-0.5577\n",
      "Iter-8780, train loss-1.6501, acc-0.5800, valid loss-1.6157, acc-0.5616, test loss-1.6171, acc-0.5583\n",
      "Iter-8790, train loss-1.5856, acc-0.6400, valid loss-1.6145, acc-0.5616, test loss-1.6159, acc-0.5591\n",
      "Iter-8800, train loss-1.7649, acc-0.3800, valid loss-1.6133, acc-0.5618, test loss-1.6147, acc-0.5600\n",
      "Iter-8810, train loss-1.5857, acc-0.5800, valid loss-1.6121, acc-0.5620, test loss-1.6135, acc-0.5604\n",
      "Iter-8820, train loss-1.6548, acc-0.4600, valid loss-1.6110, acc-0.5620, test loss-1.6124, acc-0.5610\n",
      "Iter-8830, train loss-1.7164, acc-0.4400, valid loss-1.6098, acc-0.5626, test loss-1.6112, acc-0.5616\n",
      "Iter-8840, train loss-1.6476, acc-0.5800, valid loss-1.6086, acc-0.5628, test loss-1.6100, acc-0.5616\n",
      "Iter-8850, train loss-1.7110, acc-0.5000, valid loss-1.6075, acc-0.5632, test loss-1.6089, acc-0.5625\n",
      "Iter-8860, train loss-1.4750, acc-0.6200, valid loss-1.6063, acc-0.5634, test loss-1.6077, acc-0.5628\n",
      "Iter-8870, train loss-1.6306, acc-0.6200, valid loss-1.6052, acc-0.5642, test loss-1.6065, acc-0.5632\n",
      "Iter-8880, train loss-1.5066, acc-0.6400, valid loss-1.6040, acc-0.5646, test loss-1.6053, acc-0.5637\n",
      "Iter-8890, train loss-1.6312, acc-0.6000, valid loss-1.6029, acc-0.5644, test loss-1.6042, acc-0.5635\n",
      "Iter-8900, train loss-1.7258, acc-0.5200, valid loss-1.6018, acc-0.5650, test loss-1.6031, acc-0.5641\n",
      "Iter-8910, train loss-1.6452, acc-0.5600, valid loss-1.6007, acc-0.5658, test loss-1.6020, acc-0.5643\n",
      "Iter-8920, train loss-1.6355, acc-0.5600, valid loss-1.5996, acc-0.5664, test loss-1.6009, acc-0.5653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-1.6307, acc-0.5400, valid loss-1.5984, acc-0.5672, test loss-1.5997, acc-0.5654\n",
      "Iter-8940, train loss-1.7159, acc-0.5000, valid loss-1.5973, acc-0.5672, test loss-1.5985, acc-0.5654\n",
      "Iter-8950, train loss-1.6294, acc-0.5400, valid loss-1.5961, acc-0.5672, test loss-1.5974, acc-0.5657\n",
      "Iter-8960, train loss-1.5716, acc-0.5200, valid loss-1.5950, acc-0.5674, test loss-1.5963, acc-0.5659\n",
      "Iter-8970, train loss-1.5858, acc-0.5400, valid loss-1.5939, acc-0.5680, test loss-1.5951, acc-0.5662\n",
      "Iter-8980, train loss-1.6056, acc-0.5200, valid loss-1.5927, acc-0.5688, test loss-1.5940, acc-0.5664\n",
      "Iter-8990, train loss-1.5967, acc-0.5600, valid loss-1.5915, acc-0.5690, test loss-1.5928, acc-0.5665\n",
      "Iter-9000, train loss-1.6291, acc-0.5200, valid loss-1.5904, acc-0.5700, test loss-1.5917, acc-0.5671\n",
      "Iter-9010, train loss-1.5330, acc-0.5200, valid loss-1.5893, acc-0.5702, test loss-1.5905, acc-0.5673\n",
      "Iter-9020, train loss-1.3729, acc-0.8000, valid loss-1.5881, acc-0.5706, test loss-1.5893, acc-0.5681\n",
      "Iter-9030, train loss-1.4770, acc-0.6400, valid loss-1.5870, acc-0.5710, test loss-1.5882, acc-0.5685\n",
      "Iter-9040, train loss-1.6761, acc-0.4800, valid loss-1.5859, acc-0.5714, test loss-1.5870, acc-0.5690\n",
      "Iter-9050, train loss-1.4717, acc-0.6200, valid loss-1.5847, acc-0.5716, test loss-1.5859, acc-0.5692\n",
      "Iter-9060, train loss-1.5581, acc-0.5000, valid loss-1.5836, acc-0.5716, test loss-1.5848, acc-0.5693\n",
      "Iter-9070, train loss-1.7293, acc-0.5200, valid loss-1.5826, acc-0.5716, test loss-1.5837, acc-0.5701\n",
      "Iter-9080, train loss-1.6133, acc-0.5200, valid loss-1.5814, acc-0.5720, test loss-1.5825, acc-0.5706\n",
      "Iter-9090, train loss-1.6401, acc-0.5600, valid loss-1.5803, acc-0.5726, test loss-1.5814, acc-0.5710\n",
      "Iter-9100, train loss-1.5774, acc-0.5200, valid loss-1.5791, acc-0.5732, test loss-1.5803, acc-0.5713\n",
      "Iter-9110, train loss-1.5393, acc-0.6600, valid loss-1.5780, acc-0.5742, test loss-1.5791, acc-0.5714\n",
      "Iter-9120, train loss-1.6391, acc-0.5600, valid loss-1.5768, acc-0.5744, test loss-1.5779, acc-0.5719\n",
      "Iter-9130, train loss-1.4630, acc-0.5800, valid loss-1.5756, acc-0.5746, test loss-1.5767, acc-0.5719\n",
      "Iter-9140, train loss-1.6131, acc-0.5000, valid loss-1.5745, acc-0.5750, test loss-1.5755, acc-0.5722\n",
      "Iter-9150, train loss-1.6124, acc-0.5600, valid loss-1.5734, acc-0.5760, test loss-1.5744, acc-0.5731\n",
      "Iter-9160, train loss-1.4890, acc-0.5800, valid loss-1.5723, acc-0.5764, test loss-1.5733, acc-0.5735\n",
      "Iter-9170, train loss-1.6091, acc-0.5200, valid loss-1.5711, acc-0.5772, test loss-1.5721, acc-0.5739\n",
      "Iter-9180, train loss-1.4919, acc-0.6400, valid loss-1.5699, acc-0.5770, test loss-1.5709, acc-0.5743\n",
      "Iter-9190, train loss-1.4492, acc-0.6600, valid loss-1.5687, acc-0.5772, test loss-1.5697, acc-0.5745\n",
      "Iter-9200, train loss-1.5428, acc-0.5400, valid loss-1.5676, acc-0.5772, test loss-1.5686, acc-0.5750\n",
      "Iter-9210, train loss-1.5841, acc-0.5400, valid loss-1.5665, acc-0.5782, test loss-1.5674, acc-0.5753\n",
      "Iter-9220, train loss-1.6732, acc-0.4000, valid loss-1.5654, acc-0.5784, test loss-1.5663, acc-0.5758\n",
      "Iter-9230, train loss-1.5993, acc-0.5400, valid loss-1.5642, acc-0.5790, test loss-1.5652, acc-0.5762\n",
      "Iter-9240, train loss-1.6560, acc-0.5400, valid loss-1.5631, acc-0.5790, test loss-1.5640, acc-0.5764\n",
      "Iter-9250, train loss-1.6353, acc-0.5200, valid loss-1.5619, acc-0.5794, test loss-1.5629, acc-0.5772\n",
      "Iter-9260, train loss-1.5062, acc-0.5800, valid loss-1.5608, acc-0.5796, test loss-1.5617, acc-0.5774\n",
      "Iter-9270, train loss-1.5595, acc-0.5400, valid loss-1.5597, acc-0.5798, test loss-1.5606, acc-0.5777\n",
      "Iter-9280, train loss-1.5062, acc-0.6800, valid loss-1.5587, acc-0.5800, test loss-1.5595, acc-0.5786\n",
      "Iter-9290, train loss-1.6144, acc-0.4800, valid loss-1.5575, acc-0.5802, test loss-1.5583, acc-0.5789\n",
      "Iter-9300, train loss-1.6543, acc-0.4800, valid loss-1.5564, acc-0.5804, test loss-1.5572, acc-0.5795\n",
      "Iter-9310, train loss-1.5775, acc-0.5400, valid loss-1.5552, acc-0.5806, test loss-1.5560, acc-0.5804\n",
      "Iter-9320, train loss-1.5743, acc-0.5200, valid loss-1.5541, acc-0.5806, test loss-1.5549, acc-0.5808\n",
      "Iter-9330, train loss-1.6177, acc-0.5400, valid loss-1.5530, acc-0.5810, test loss-1.5539, acc-0.5810\n",
      "Iter-9340, train loss-1.6385, acc-0.6400, valid loss-1.5518, acc-0.5814, test loss-1.5527, acc-0.5818\n",
      "Iter-9350, train loss-1.6679, acc-0.4800, valid loss-1.5507, acc-0.5822, test loss-1.5516, acc-0.5823\n",
      "Iter-9360, train loss-1.4556, acc-0.6800, valid loss-1.5496, acc-0.5836, test loss-1.5504, acc-0.5831\n",
      "Iter-9370, train loss-1.4121, acc-0.6400, valid loss-1.5484, acc-0.5838, test loss-1.5493, acc-0.5832\n",
      "Iter-9380, train loss-1.6284, acc-0.5600, valid loss-1.5474, acc-0.5842, test loss-1.5482, acc-0.5832\n",
      "Iter-9390, train loss-1.4852, acc-0.6600, valid loss-1.5464, acc-0.5840, test loss-1.5472, acc-0.5833\n",
      "Iter-9400, train loss-1.3944, acc-0.7200, valid loss-1.5453, acc-0.5842, test loss-1.5461, acc-0.5837\n",
      "Iter-9410, train loss-1.5132, acc-0.6400, valid loss-1.5442, acc-0.5850, test loss-1.5450, acc-0.5843\n",
      "Iter-9420, train loss-1.5278, acc-0.5800, valid loss-1.5431, acc-0.5858, test loss-1.5439, acc-0.5847\n",
      "Iter-9430, train loss-1.6216, acc-0.5800, valid loss-1.5420, acc-0.5862, test loss-1.5428, acc-0.5850\n",
      "Iter-9440, train loss-1.5114, acc-0.6200, valid loss-1.5408, acc-0.5868, test loss-1.5416, acc-0.5855\n",
      "Iter-9450, train loss-1.5288, acc-0.6000, valid loss-1.5398, acc-0.5872, test loss-1.5405, acc-0.5860\n",
      "Iter-9460, train loss-1.5803, acc-0.6000, valid loss-1.5386, acc-0.5886, test loss-1.5393, acc-0.5861\n",
      "Iter-9470, train loss-1.4556, acc-0.6600, valid loss-1.5374, acc-0.5886, test loss-1.5381, acc-0.5866\n",
      "Iter-9480, train loss-1.3462, acc-0.7400, valid loss-1.5363, acc-0.5886, test loss-1.5370, acc-0.5871\n",
      "Iter-9490, train loss-1.6392, acc-0.5200, valid loss-1.5352, acc-0.5894, test loss-1.5359, acc-0.5875\n",
      "Iter-9500, train loss-1.5110, acc-0.6400, valid loss-1.5341, acc-0.5912, test loss-1.5348, acc-0.5888\n",
      "Iter-9510, train loss-1.6094, acc-0.5000, valid loss-1.5329, acc-0.5918, test loss-1.5336, acc-0.5888\n",
      "Iter-9520, train loss-1.5672, acc-0.6000, valid loss-1.5319, acc-0.5922, test loss-1.5326, acc-0.5895\n",
      "Iter-9530, train loss-1.4970, acc-0.6000, valid loss-1.5308, acc-0.5924, test loss-1.5315, acc-0.5898\n",
      "Iter-9540, train loss-1.6334, acc-0.4600, valid loss-1.5297, acc-0.5928, test loss-1.5304, acc-0.5900\n",
      "Iter-9550, train loss-1.4458, acc-0.6600, valid loss-1.5286, acc-0.5930, test loss-1.5293, acc-0.5907\n",
      "Iter-9560, train loss-1.4471, acc-0.6600, valid loss-1.5275, acc-0.5928, test loss-1.5282, acc-0.5909\n",
      "Iter-9570, train loss-1.5704, acc-0.5600, valid loss-1.5264, acc-0.5936, test loss-1.5271, acc-0.5913\n",
      "Iter-9580, train loss-1.4871, acc-0.6000, valid loss-1.5253, acc-0.5940, test loss-1.5260, acc-0.5915\n",
      "Iter-9590, train loss-1.4797, acc-0.6400, valid loss-1.5243, acc-0.5942, test loss-1.5250, acc-0.5918\n",
      "Iter-9600, train loss-1.4681, acc-0.6600, valid loss-1.5232, acc-0.5944, test loss-1.5238, acc-0.5924\n",
      "Iter-9610, train loss-1.4197, acc-0.6600, valid loss-1.5221, acc-0.5952, test loss-1.5227, acc-0.5935\n",
      "Iter-9620, train loss-1.6213, acc-0.5000, valid loss-1.5210, acc-0.5954, test loss-1.5216, acc-0.5941\n",
      "Iter-9630, train loss-1.4667, acc-0.7400, valid loss-1.5200, acc-0.5958, test loss-1.5206, acc-0.5948\n",
      "Iter-9640, train loss-1.5140, acc-0.6000, valid loss-1.5189, acc-0.5958, test loss-1.5195, acc-0.5953\n",
      "Iter-9650, train loss-1.3265, acc-0.7800, valid loss-1.5178, acc-0.5958, test loss-1.5183, acc-0.5956\n",
      "Iter-9660, train loss-1.5573, acc-0.6000, valid loss-1.5168, acc-0.5960, test loss-1.5173, acc-0.5960\n",
      "Iter-9670, train loss-1.5068, acc-0.5400, valid loss-1.5157, acc-0.5966, test loss-1.5162, acc-0.5963\n",
      "Iter-9680, train loss-1.5513, acc-0.5800, valid loss-1.5146, acc-0.5964, test loss-1.5151, acc-0.5966\n",
      "Iter-9690, train loss-1.5741, acc-0.5400, valid loss-1.5135, acc-0.5964, test loss-1.5140, acc-0.5966\n",
      "Iter-9700, train loss-1.5338, acc-0.5600, valid loss-1.5124, acc-0.5970, test loss-1.5129, acc-0.5976\n",
      "Iter-9710, train loss-1.5770, acc-0.5000, valid loss-1.5113, acc-0.5978, test loss-1.5118, acc-0.5980\n",
      "Iter-9720, train loss-1.6106, acc-0.6400, valid loss-1.5103, acc-0.5984, test loss-1.5108, acc-0.5988\n",
      "Iter-9730, train loss-1.4473, acc-0.6800, valid loss-1.5091, acc-0.5982, test loss-1.5097, acc-0.5993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-1.4245, acc-0.6400, valid loss-1.5081, acc-0.5978, test loss-1.5086, acc-0.5997\n",
      "Iter-9750, train loss-1.4604, acc-0.6200, valid loss-1.5070, acc-0.5992, test loss-1.5075, acc-0.6004\n",
      "Iter-9760, train loss-1.5260, acc-0.6000, valid loss-1.5060, acc-0.5998, test loss-1.5065, acc-0.6008\n",
      "Iter-9770, train loss-1.4858, acc-0.6400, valid loss-1.5049, acc-0.6000, test loss-1.5054, acc-0.6015\n",
      "Iter-9780, train loss-1.4512, acc-0.6200, valid loss-1.5038, acc-0.6004, test loss-1.5043, acc-0.6017\n",
      "Iter-9790, train loss-1.5328, acc-0.6200, valid loss-1.5027, acc-0.6008, test loss-1.5032, acc-0.6023\n",
      "Iter-9800, train loss-1.5113, acc-0.6200, valid loss-1.5017, acc-0.6010, test loss-1.5021, acc-0.6027\n",
      "Iter-9810, train loss-1.3344, acc-0.7600, valid loss-1.5006, acc-0.6012, test loss-1.5011, acc-0.6033\n",
      "Iter-9820, train loss-1.4184, acc-0.6800, valid loss-1.4995, acc-0.6016, test loss-1.5000, acc-0.6038\n",
      "Iter-9830, train loss-1.5111, acc-0.5400, valid loss-1.4984, acc-0.6016, test loss-1.4989, acc-0.6042\n",
      "Iter-9840, train loss-1.7158, acc-0.4800, valid loss-1.4974, acc-0.6024, test loss-1.4978, acc-0.6046\n",
      "Iter-9850, train loss-1.5155, acc-0.5800, valid loss-1.4965, acc-0.6030, test loss-1.4969, acc-0.6050\n",
      "Iter-9860, train loss-1.4880, acc-0.6400, valid loss-1.4955, acc-0.6034, test loss-1.4958, acc-0.6055\n",
      "Iter-9870, train loss-1.5625, acc-0.5000, valid loss-1.4944, acc-0.6044, test loss-1.4948, acc-0.6061\n",
      "Iter-9880, train loss-1.4497, acc-0.6800, valid loss-1.4934, acc-0.6046, test loss-1.4937, acc-0.6064\n",
      "Iter-9890, train loss-1.5101, acc-0.6000, valid loss-1.4924, acc-0.6048, test loss-1.4927, acc-0.6069\n",
      "Iter-9900, train loss-1.5771, acc-0.5000, valid loss-1.4913, acc-0.6048, test loss-1.4916, acc-0.6076\n",
      "Iter-9910, train loss-1.6087, acc-0.5800, valid loss-1.4902, acc-0.6054, test loss-1.4905, acc-0.6081\n",
      "Iter-9920, train loss-1.4055, acc-0.6400, valid loss-1.4891, acc-0.6052, test loss-1.4894, acc-0.6078\n",
      "Iter-9930, train loss-1.5064, acc-0.5800, valid loss-1.4881, acc-0.6056, test loss-1.4884, acc-0.6081\n",
      "Iter-9940, train loss-1.4722, acc-0.6600, valid loss-1.4870, acc-0.6060, test loss-1.4873, acc-0.6083\n",
      "Iter-9950, train loss-1.5206, acc-0.6200, valid loss-1.4860, acc-0.6060, test loss-1.4862, acc-0.6083\n",
      "Iter-9960, train loss-1.4682, acc-0.5800, valid loss-1.4849, acc-0.6068, test loss-1.4852, acc-0.6086\n",
      "Iter-9970, train loss-1.4724, acc-0.5600, valid loss-1.4838, acc-0.6078, test loss-1.4841, acc-0.6093\n",
      "Iter-9980, train loss-1.4321, acc-0.6800, valid loss-1.4828, acc-0.6076, test loss-1.4831, acc-0.6095\n",
      "Iter-9990, train loss-1.6322, acc-0.4800, valid loss-1.4818, acc-0.6086, test loss-1.4820, acc-0.6104\n",
      "Iter-10000, train loss-1.4732, acc-0.6400, valid loss-1.4808, acc-0.6094, test loss-1.4810, acc-0.6107\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyRh98gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1191562b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnATjRsBxHCfBuBFwHMdJMP8HLeGoL+ViOgIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119284e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
