{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        #         dX = dout @ W.T # vanilla Backprop\n",
    "        #         dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y = np.tanh(y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = np.tanh(y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def onehot(self, labels):\n",
    "        # y = np.zeros([labels.size, np.max(labels) + 1])\n",
    "        y = np.zeros([labels.size, self.C])\n",
    "        y[range(labels.size), labels] = 1.\n",
    "        return y\n",
    "\n",
    "    def squared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        data_loss = 0.5 * np.sum((y_pred - self.onehot(y_train))**2)/ m\n",
    "        return data_loss\n",
    "\n",
    "    def dsquared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        grad_y = (y_pred - self.onehot(y_train))/ m\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.squared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        dy = self.dsquared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        \n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-0.4521, acc-0.0800, valid loss-0.4519, acc-0.0822, test loss-0.4517, acc-0.0857\n",
      "Iter-20, train loss-0.4516, acc-0.0900, valid loss-0.4518, acc-0.0816, test loss-0.4517, acc-0.0866\n",
      "Iter-30, train loss-0.4510, acc-0.1100, valid loss-0.4517, acc-0.0830, test loss-0.4516, acc-0.0880\n",
      "Iter-40, train loss-0.4518, acc-0.1300, valid loss-0.4516, acc-0.0836, test loss-0.4515, acc-0.0872\n",
      "Iter-50, train loss-0.4513, acc-0.1100, valid loss-0.4517, acc-0.0796, test loss-0.4516, acc-0.0864\n",
      "Iter-60, train loss-0.4531, acc-0.0800, valid loss-0.4515, acc-0.0828, test loss-0.4514, acc-0.0882\n",
      "Iter-70, train loss-0.4519, acc-0.0800, valid loss-0.4516, acc-0.0834, test loss-0.4515, acc-0.0877\n",
      "Iter-80, train loss-0.4516, acc-0.1000, valid loss-0.4514, acc-0.0874, test loss-0.4513, acc-0.0893\n",
      "Iter-90, train loss-0.4524, acc-0.0900, valid loss-0.4513, acc-0.0866, test loss-0.4513, acc-0.0881\n",
      "Iter-100, train loss-0.4515, acc-0.1000, valid loss-0.4511, acc-0.0894, test loss-0.4510, acc-0.0881\n",
      "Iter-110, train loss-0.4495, acc-0.1200, valid loss-0.4508, acc-0.0954, test loss-0.4507, acc-0.0923\n",
      "Iter-120, train loss-0.4498, acc-0.1100, valid loss-0.4505, acc-0.1016, test loss-0.4504, acc-0.0979\n",
      "Iter-130, train loss-0.4484, acc-0.1900, valid loss-0.4501, acc-0.1048, test loss-0.4500, acc-0.1034\n",
      "Iter-140, train loss-0.4489, acc-0.1500, valid loss-0.4499, acc-0.1082, test loss-0.4498, acc-0.1052\n",
      "Iter-150, train loss-0.4474, acc-0.1700, valid loss-0.4497, acc-0.1102, test loss-0.4496, acc-0.1075\n",
      "Iter-160, train loss-0.4478, acc-0.1500, valid loss-0.4492, acc-0.1144, test loss-0.4492, acc-0.1119\n",
      "Iter-170, train loss-0.4494, acc-0.0600, valid loss-0.4488, acc-0.1182, test loss-0.4487, acc-0.1182\n",
      "Iter-180, train loss-0.4474, acc-0.1000, valid loss-0.4484, acc-0.1232, test loss-0.4484, acc-0.1232\n",
      "Iter-190, train loss-0.4488, acc-0.1300, valid loss-0.4481, acc-0.1304, test loss-0.4481, acc-0.1288\n",
      "Iter-200, train loss-0.4488, acc-0.1200, valid loss-0.4478, acc-0.1366, test loss-0.4478, acc-0.1336\n",
      "Iter-210, train loss-0.4469, acc-0.1200, valid loss-0.4473, acc-0.1466, test loss-0.4473, acc-0.1409\n",
      "Iter-220, train loss-0.4491, acc-0.1600, valid loss-0.4469, acc-0.1502, test loss-0.4469, acc-0.1446\n",
      "Iter-230, train loss-0.4489, acc-0.1300, valid loss-0.4461, acc-0.1616, test loss-0.4461, acc-0.1539\n",
      "Iter-240, train loss-0.4454, acc-0.1900, valid loss-0.4453, acc-0.1744, test loss-0.4453, acc-0.1694\n",
      "Iter-250, train loss-0.4460, acc-0.1700, valid loss-0.4444, acc-0.1824, test loss-0.4444, acc-0.1812\n",
      "Iter-260, train loss-0.4445, acc-0.2000, valid loss-0.4434, acc-0.1970, test loss-0.4434, acc-0.1940\n",
      "Iter-270, train loss-0.4444, acc-0.2200, valid loss-0.4427, acc-0.2052, test loss-0.4427, acc-0.2022\n",
      "Iter-280, train loss-0.4438, acc-0.1800, valid loss-0.4417, acc-0.2172, test loss-0.4417, acc-0.2164\n",
      "Iter-290, train loss-0.4416, acc-0.2100, valid loss-0.4408, acc-0.2336, test loss-0.4408, acc-0.2285\n",
      "Iter-300, train loss-0.4363, acc-0.2800, valid loss-0.4396, acc-0.2482, test loss-0.4396, acc-0.2437\n",
      "Iter-310, train loss-0.4362, acc-0.2300, valid loss-0.4384, acc-0.2596, test loss-0.4385, acc-0.2551\n",
      "Iter-320, train loss-0.4352, acc-0.3300, valid loss-0.4374, acc-0.2718, test loss-0.4374, acc-0.2651\n",
      "Iter-330, train loss-0.4347, acc-0.3000, valid loss-0.4361, acc-0.2810, test loss-0.4361, acc-0.2763\n",
      "Iter-340, train loss-0.4385, acc-0.2300, valid loss-0.4347, acc-0.2882, test loss-0.4347, acc-0.2847\n",
      "Iter-350, train loss-0.4320, acc-0.3000, valid loss-0.4334, acc-0.2934, test loss-0.4334, acc-0.2918\n",
      "Iter-360, train loss-0.4282, acc-0.3700, valid loss-0.4316, acc-0.3006, test loss-0.4316, acc-0.2981\n",
      "Iter-370, train loss-0.4297, acc-0.2800, valid loss-0.4301, acc-0.3076, test loss-0.4300, acc-0.3042\n",
      "Iter-380, train loss-0.4305, acc-0.2400, valid loss-0.4280, acc-0.3166, test loss-0.4279, acc-0.3109\n",
      "Iter-390, train loss-0.4279, acc-0.2600, valid loss-0.4260, acc-0.3160, test loss-0.4260, acc-0.3132\n",
      "Iter-400, train loss-0.4248, acc-0.3100, valid loss-0.4240, acc-0.3180, test loss-0.4239, acc-0.3153\n",
      "Iter-410, train loss-0.4265, acc-0.2500, valid loss-0.4221, acc-0.3226, test loss-0.4219, acc-0.3213\n",
      "Iter-420, train loss-0.4215, acc-0.3600, valid loss-0.4200, acc-0.3258, test loss-0.4198, acc-0.3252\n",
      "Iter-430, train loss-0.4221, acc-0.2800, valid loss-0.4179, acc-0.3296, test loss-0.4177, acc-0.3291\n",
      "Iter-440, train loss-0.4191, acc-0.2900, valid loss-0.4158, acc-0.3370, test loss-0.4156, acc-0.3369\n",
      "Iter-450, train loss-0.4095, acc-0.3600, valid loss-0.4134, acc-0.3474, test loss-0.4132, acc-0.3437\n",
      "Iter-460, train loss-0.4077, acc-0.3000, valid loss-0.4111, acc-0.3604, test loss-0.4108, acc-0.3520\n",
      "Iter-470, train loss-0.4123, acc-0.3300, valid loss-0.4082, acc-0.3688, test loss-0.4079, acc-0.3642\n",
      "Iter-480, train loss-0.4108, acc-0.2600, valid loss-0.4058, acc-0.3782, test loss-0.4054, acc-0.3764\n",
      "Iter-490, train loss-0.4094, acc-0.3300, valid loss-0.4037, acc-0.3850, test loss-0.4033, acc-0.3851\n",
      "Iter-500, train loss-0.4054, acc-0.3900, valid loss-0.4013, acc-0.4020, test loss-0.4009, acc-0.4036\n",
      "Iter-510, train loss-0.3959, acc-0.4600, valid loss-0.3990, acc-0.4324, test loss-0.3985, acc-0.4267\n",
      "Iter-520, train loss-0.3938, acc-0.4700, valid loss-0.3966, acc-0.4690, test loss-0.3960, acc-0.4652\n",
      "Iter-530, train loss-0.3941, acc-0.4900, valid loss-0.3944, acc-0.4906, test loss-0.3939, acc-0.4789\n",
      "Iter-540, train loss-0.3963, acc-0.4500, valid loss-0.3923, acc-0.5056, test loss-0.3918, acc-0.4961\n",
      "Iter-550, train loss-0.3884, acc-0.5200, valid loss-0.3900, acc-0.5104, test loss-0.3895, acc-0.5022\n",
      "Iter-560, train loss-0.4017, acc-0.3900, valid loss-0.3878, acc-0.5162, test loss-0.3873, acc-0.5111\n",
      "Iter-570, train loss-0.3902, acc-0.5200, valid loss-0.3855, acc-0.5160, test loss-0.3850, acc-0.5117\n",
      "Iter-580, train loss-0.3806, acc-0.5500, valid loss-0.3831, acc-0.5158, test loss-0.3827, acc-0.5088\n",
      "Iter-590, train loss-0.3779, acc-0.4800, valid loss-0.3807, acc-0.5172, test loss-0.3803, acc-0.5113\n",
      "Iter-600, train loss-0.3753, acc-0.5000, valid loss-0.3788, acc-0.5150, test loss-0.3783, acc-0.5078\n",
      "Iter-610, train loss-0.3756, acc-0.4800, valid loss-0.3769, acc-0.5182, test loss-0.3764, acc-0.5113\n",
      "Iter-620, train loss-0.3886, acc-0.4900, valid loss-0.3749, acc-0.5166, test loss-0.3745, acc-0.5099\n",
      "Iter-630, train loss-0.3612, acc-0.6200, valid loss-0.3734, acc-0.5136, test loss-0.3730, acc-0.5072\n",
      "Iter-640, train loss-0.3673, acc-0.4800, valid loss-0.3720, acc-0.5154, test loss-0.3715, acc-0.5065\n",
      "Iter-650, train loss-0.3730, acc-0.4800, valid loss-0.3701, acc-0.5086, test loss-0.3697, acc-0.5017\n",
      "Iter-660, train loss-0.3580, acc-0.5900, valid loss-0.3683, acc-0.5138, test loss-0.3679, acc-0.5058\n",
      "Iter-670, train loss-0.3567, acc-0.5700, valid loss-0.3668, acc-0.5160, test loss-0.3664, acc-0.5080\n",
      "Iter-680, train loss-0.3751, acc-0.5100, valid loss-0.3649, acc-0.5176, test loss-0.3646, acc-0.5084\n",
      "Iter-690, train loss-0.3673, acc-0.4700, valid loss-0.3631, acc-0.5186, test loss-0.3629, acc-0.5086\n",
      "Iter-700, train loss-0.3541, acc-0.5300, valid loss-0.3618, acc-0.5250, test loss-0.3616, acc-0.5161\n",
      "Iter-710, train loss-0.3651, acc-0.5400, valid loss-0.3602, acc-0.5260, test loss-0.3601, acc-0.5172\n",
      "Iter-720, train loss-0.3575, acc-0.5700, valid loss-0.3584, acc-0.5260, test loss-0.3584, acc-0.5170\n",
      "Iter-730, train loss-0.3607, acc-0.5400, valid loss-0.3569, acc-0.5260, test loss-0.3570, acc-0.5161\n",
      "Iter-740, train loss-0.3379, acc-0.5700, valid loss-0.3554, acc-0.5272, test loss-0.3555, acc-0.5176\n",
      "Iter-750, train loss-0.3453, acc-0.5400, valid loss-0.3540, acc-0.5256, test loss-0.3542, acc-0.5167\n",
      "Iter-760, train loss-0.3749, acc-0.4200, valid loss-0.3526, acc-0.5268, test loss-0.3527, acc-0.5176\n",
      "Iter-770, train loss-0.3367, acc-0.5700, valid loss-0.3513, acc-0.5248, test loss-0.3515, acc-0.5159\n",
      "Iter-780, train loss-0.3449, acc-0.4900, valid loss-0.3500, acc-0.5284, test loss-0.3502, acc-0.5175\n",
      "Iter-790, train loss-0.3241, acc-0.6300, valid loss-0.3486, acc-0.5284, test loss-0.3489, acc-0.5182\n",
      "Iter-800, train loss-0.3571, acc-0.4800, valid loss-0.3473, acc-0.5290, test loss-0.3477, acc-0.5192\n",
      "Iter-810, train loss-0.3385, acc-0.5400, valid loss-0.3462, acc-0.5324, test loss-0.3467, acc-0.5199\n",
      "Iter-820, train loss-0.3261, acc-0.5300, valid loss-0.3448, acc-0.5352, test loss-0.3453, acc-0.5232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-0.3558, acc-0.5000, valid loss-0.3438, acc-0.5358, test loss-0.3443, acc-0.5234\n",
      "Iter-840, train loss-0.3258, acc-0.6100, valid loss-0.3426, acc-0.5348, test loss-0.3432, acc-0.5216\n",
      "Iter-850, train loss-0.3467, acc-0.4900, valid loss-0.3414, acc-0.5376, test loss-0.3422, acc-0.5209\n",
      "Iter-860, train loss-0.3304, acc-0.5600, valid loss-0.3400, acc-0.5422, test loss-0.3409, acc-0.5251\n",
      "Iter-870, train loss-0.3482, acc-0.4800, valid loss-0.3389, acc-0.5436, test loss-0.3397, acc-0.5271\n",
      "Iter-880, train loss-0.3488, acc-0.5300, valid loss-0.3372, acc-0.5476, test loss-0.3381, acc-0.5321\n",
      "Iter-890, train loss-0.3325, acc-0.5800, valid loss-0.3359, acc-0.5510, test loss-0.3370, acc-0.5348\n",
      "Iter-900, train loss-0.3340, acc-0.5900, valid loss-0.3345, acc-0.5540, test loss-0.3355, acc-0.5394\n",
      "Iter-910, train loss-0.3215, acc-0.6000, valid loss-0.3332, acc-0.5566, test loss-0.3343, acc-0.5427\n",
      "Iter-920, train loss-0.3270, acc-0.5800, valid loss-0.3322, acc-0.5554, test loss-0.3334, acc-0.5436\n",
      "Iter-930, train loss-0.3245, acc-0.5800, valid loss-0.3311, acc-0.5578, test loss-0.3324, acc-0.5436\n",
      "Iter-940, train loss-0.3323, acc-0.5300, valid loss-0.3296, acc-0.5602, test loss-0.3310, acc-0.5492\n",
      "Iter-950, train loss-0.3326, acc-0.5800, valid loss-0.3283, acc-0.5616, test loss-0.3297, acc-0.5516\n",
      "Iter-960, train loss-0.3319, acc-0.5300, valid loss-0.3270, acc-0.5662, test loss-0.3286, acc-0.5553\n",
      "Iter-970, train loss-0.3494, acc-0.5000, valid loss-0.3256, acc-0.5692, test loss-0.3271, acc-0.5601\n",
      "Iter-980, train loss-0.3257, acc-0.5700, valid loss-0.3244, acc-0.5706, test loss-0.3259, acc-0.5634\n",
      "Iter-990, train loss-0.3310, acc-0.5800, valid loss-0.3230, acc-0.5728, test loss-0.3243, acc-0.5657\n",
      "Iter-1000, train loss-0.3214, acc-0.6100, valid loss-0.3217, acc-0.5732, test loss-0.3231, acc-0.5660\n",
      "Iter-1010, train loss-0.3252, acc-0.5700, valid loss-0.3206, acc-0.5730, test loss-0.3220, acc-0.5672\n",
      "Iter-1020, train loss-0.3298, acc-0.5700, valid loss-0.3193, acc-0.5788, test loss-0.3208, acc-0.5723\n",
      "Iter-1030, train loss-0.3167, acc-0.6200, valid loss-0.3180, acc-0.5758, test loss-0.3194, acc-0.5736\n",
      "Iter-1040, train loss-0.3374, acc-0.5500, valid loss-0.3159, acc-0.5906, test loss-0.3177, acc-0.5850\n",
      "Iter-1050, train loss-0.3087, acc-0.6000, valid loss-0.3144, acc-0.5966, test loss-0.3164, acc-0.5897\n",
      "Iter-1060, train loss-0.3506, acc-0.4700, valid loss-0.3133, acc-0.5978, test loss-0.3152, acc-0.5905\n",
      "Iter-1070, train loss-0.3158, acc-0.6300, valid loss-0.3120, acc-0.5996, test loss-0.3140, acc-0.5913\n",
      "Iter-1080, train loss-0.3281, acc-0.5500, valid loss-0.3108, acc-0.6042, test loss-0.3130, acc-0.5954\n",
      "Iter-1090, train loss-0.3361, acc-0.4900, valid loss-0.3095, acc-0.6072, test loss-0.3119, acc-0.5977\n",
      "Iter-1100, train loss-0.3347, acc-0.5600, valid loss-0.3084, acc-0.6110, test loss-0.3109, acc-0.6011\n",
      "Iter-1110, train loss-0.3053, acc-0.6800, valid loss-0.3072, acc-0.6128, test loss-0.3098, acc-0.6009\n",
      "Iter-1120, train loss-0.3128, acc-0.6100, valid loss-0.3062, acc-0.6122, test loss-0.3088, acc-0.6018\n",
      "Iter-1130, train loss-0.3117, acc-0.6100, valid loss-0.3048, acc-0.6186, test loss-0.3076, acc-0.6041\n",
      "Iter-1140, train loss-0.3270, acc-0.5300, valid loss-0.3033, acc-0.6190, test loss-0.3062, acc-0.6082\n",
      "Iter-1150, train loss-0.3106, acc-0.5600, valid loss-0.3023, acc-0.6210, test loss-0.3053, acc-0.6117\n",
      "Iter-1160, train loss-0.3061, acc-0.6100, valid loss-0.3008, acc-0.6222, test loss-0.3039, acc-0.6120\n",
      "Iter-1170, train loss-0.2901, acc-0.6500, valid loss-0.2996, acc-0.6242, test loss-0.3026, acc-0.6158\n",
      "Iter-1180, train loss-0.3390, acc-0.5400, valid loss-0.2980, acc-0.6280, test loss-0.3010, acc-0.6203\n",
      "Iter-1190, train loss-0.2894, acc-0.6500, valid loss-0.2966, acc-0.6302, test loss-0.2998, acc-0.6211\n",
      "Iter-1200, train loss-0.2825, acc-0.6600, valid loss-0.2950, acc-0.6370, test loss-0.2983, acc-0.6256\n",
      "Iter-1210, train loss-0.3062, acc-0.6100, valid loss-0.2938, acc-0.6388, test loss-0.2971, acc-0.6290\n",
      "Iter-1220, train loss-0.2888, acc-0.6500, valid loss-0.2925, acc-0.6402, test loss-0.2959, acc-0.6299\n",
      "Iter-1230, train loss-0.3061, acc-0.6300, valid loss-0.2908, acc-0.6448, test loss-0.2943, acc-0.6343\n",
      "Iter-1240, train loss-0.3256, acc-0.5800, valid loss-0.2896, acc-0.6472, test loss-0.2933, acc-0.6357\n",
      "Iter-1250, train loss-0.3093, acc-0.6000, valid loss-0.2885, acc-0.6502, test loss-0.2924, acc-0.6363\n",
      "Iter-1260, train loss-0.2558, acc-0.7300, valid loss-0.2871, acc-0.6516, test loss-0.2911, acc-0.6378\n",
      "Iter-1270, train loss-0.2868, acc-0.6000, valid loss-0.2860, acc-0.6518, test loss-0.2900, acc-0.6379\n",
      "Iter-1280, train loss-0.2980, acc-0.6000, valid loss-0.2846, acc-0.6542, test loss-0.2886, acc-0.6407\n",
      "Iter-1290, train loss-0.2877, acc-0.6400, valid loss-0.2831, acc-0.6580, test loss-0.2870, acc-0.6447\n",
      "Iter-1300, train loss-0.2659, acc-0.7100, valid loss-0.2813, acc-0.6592, test loss-0.2850, acc-0.6466\n",
      "Iter-1310, train loss-0.2627, acc-0.7700, valid loss-0.2799, acc-0.6616, test loss-0.2837, acc-0.6491\n",
      "Iter-1320, train loss-0.2674, acc-0.7200, valid loss-0.2783, acc-0.6648, test loss-0.2820, acc-0.6527\n",
      "Iter-1330, train loss-0.3017, acc-0.5800, valid loss-0.2770, acc-0.6656, test loss-0.2806, acc-0.6528\n",
      "Iter-1340, train loss-0.2781, acc-0.6600, valid loss-0.2761, acc-0.6660, test loss-0.2798, acc-0.6541\n",
      "Iter-1350, train loss-0.2757, acc-0.6700, valid loss-0.2746, acc-0.6684, test loss-0.2784, acc-0.6534\n",
      "Iter-1360, train loss-0.3030, acc-0.5900, valid loss-0.2732, acc-0.6712, test loss-0.2771, acc-0.6567\n",
      "Iter-1370, train loss-0.2906, acc-0.6100, valid loss-0.2720, acc-0.6724, test loss-0.2758, acc-0.6590\n",
      "Iter-1380, train loss-0.2777, acc-0.6900, valid loss-0.2704, acc-0.6750, test loss-0.2744, acc-0.6614\n",
      "Iter-1390, train loss-0.2809, acc-0.6800, valid loss-0.2693, acc-0.6764, test loss-0.2734, acc-0.6620\n",
      "Iter-1400, train loss-0.2967, acc-0.6300, valid loss-0.2678, acc-0.6766, test loss-0.2719, acc-0.6626\n",
      "Iter-1410, train loss-0.2774, acc-0.6500, valid loss-0.2665, acc-0.6784, test loss-0.2707, acc-0.6635\n",
      "Iter-1420, train loss-0.2951, acc-0.6000, valid loss-0.2652, acc-0.6806, test loss-0.2695, acc-0.6648\n",
      "Iter-1430, train loss-0.2817, acc-0.6400, valid loss-0.2640, acc-0.6826, test loss-0.2681, acc-0.6672\n",
      "Iter-1440, train loss-0.2620, acc-0.7300, valid loss-0.2628, acc-0.6818, test loss-0.2669, acc-0.6649\n",
      "Iter-1450, train loss-0.2895, acc-0.6000, valid loss-0.2615, acc-0.6826, test loss-0.2656, acc-0.6684\n",
      "Iter-1460, train loss-0.2675, acc-0.6900, valid loss-0.2603, acc-0.6846, test loss-0.2647, acc-0.6703\n",
      "Iter-1470, train loss-0.2820, acc-0.6000, valid loss-0.2592, acc-0.6840, test loss-0.2634, acc-0.6694\n",
      "Iter-1480, train loss-0.2617, acc-0.6900, valid loss-0.2575, acc-0.6872, test loss-0.2618, acc-0.6726\n",
      "Iter-1490, train loss-0.2627, acc-0.6900, valid loss-0.2565, acc-0.6882, test loss-0.2608, acc-0.6724\n",
      "Iter-1500, train loss-0.2404, acc-0.7400, valid loss-0.2553, acc-0.6896, test loss-0.2596, acc-0.6746\n",
      "Iter-1510, train loss-0.2190, acc-0.8200, valid loss-0.2540, acc-0.6938, test loss-0.2584, acc-0.6778\n",
      "Iter-1520, train loss-0.2747, acc-0.6400, valid loss-0.2529, acc-0.6928, test loss-0.2574, acc-0.6787\n",
      "Iter-1530, train loss-0.2528, acc-0.6900, valid loss-0.2518, acc-0.6962, test loss-0.2563, acc-0.6799\n",
      "Iter-1540, train loss-0.2415, acc-0.7200, valid loss-0.2506, acc-0.6974, test loss-0.2551, acc-0.6818\n",
      "Iter-1550, train loss-0.2393, acc-0.7000, valid loss-0.2496, acc-0.6980, test loss-0.2540, acc-0.6832\n",
      "Iter-1560, train loss-0.2766, acc-0.6700, valid loss-0.2483, acc-0.7006, test loss-0.2527, acc-0.6839\n",
      "Iter-1570, train loss-0.2493, acc-0.6600, valid loss-0.2470, acc-0.7018, test loss-0.2513, acc-0.6839\n",
      "Iter-1580, train loss-0.2379, acc-0.7600, valid loss-0.2457, acc-0.7006, test loss-0.2503, acc-0.6840\n",
      "Iter-1590, train loss-0.2337, acc-0.7000, valid loss-0.2445, acc-0.7016, test loss-0.2490, acc-0.6857\n",
      "Iter-1600, train loss-0.2535, acc-0.6900, valid loss-0.2434, acc-0.7036, test loss-0.2478, acc-0.6868\n",
      "Iter-1610, train loss-0.2585, acc-0.6900, valid loss-0.2421, acc-0.7046, test loss-0.2464, acc-0.6882\n",
      "Iter-1620, train loss-0.2597, acc-0.6500, valid loss-0.2407, acc-0.7056, test loss-0.2451, acc-0.6889\n",
      "Iter-1630, train loss-0.2316, acc-0.7100, valid loss-0.2397, acc-0.7062, test loss-0.2440, acc-0.6893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.2790, acc-0.6000, valid loss-0.2388, acc-0.7082, test loss-0.2430, acc-0.6914\n",
      "Iter-1650, train loss-0.2260, acc-0.7200, valid loss-0.2380, acc-0.7102, test loss-0.2422, acc-0.6925\n",
      "Iter-1660, train loss-0.2342, acc-0.7300, valid loss-0.2372, acc-0.7096, test loss-0.2412, acc-0.6936\n",
      "Iter-1670, train loss-0.2366, acc-0.7500, valid loss-0.2360, acc-0.7110, test loss-0.2400, acc-0.6954\n",
      "Iter-1680, train loss-0.2386, acc-0.7300, valid loss-0.2353, acc-0.7116, test loss-0.2394, acc-0.6947\n",
      "Iter-1690, train loss-0.2801, acc-0.6300, valid loss-0.2342, acc-0.7138, test loss-0.2381, acc-0.6959\n",
      "Iter-1700, train loss-0.2566, acc-0.6700, valid loss-0.2331, acc-0.7152, test loss-0.2369, acc-0.6983\n",
      "Iter-1710, train loss-0.2367, acc-0.7000, valid loss-0.2320, acc-0.7170, test loss-0.2358, acc-0.6990\n",
      "Iter-1720, train loss-0.2731, acc-0.6200, valid loss-0.2311, acc-0.7156, test loss-0.2349, acc-0.6987\n",
      "Iter-1730, train loss-0.2371, acc-0.6900, valid loss-0.2299, acc-0.7194, test loss-0.2338, acc-0.7016\n",
      "Iter-1740, train loss-0.2295, acc-0.6800, valid loss-0.2291, acc-0.7210, test loss-0.2328, acc-0.7033\n",
      "Iter-1750, train loss-0.2530, acc-0.6400, valid loss-0.2282, acc-0.7222, test loss-0.2319, acc-0.7046\n",
      "Iter-1760, train loss-0.2379, acc-0.6900, valid loss-0.2273, acc-0.7226, test loss-0.2310, acc-0.7052\n",
      "Iter-1770, train loss-0.2509, acc-0.6500, valid loss-0.2263, acc-0.7238, test loss-0.2299, acc-0.7073\n",
      "Iter-1780, train loss-0.2414, acc-0.6800, valid loss-0.2254, acc-0.7254, test loss-0.2289, acc-0.7104\n",
      "Iter-1790, train loss-0.2293, acc-0.7400, valid loss-0.2247, acc-0.7266, test loss-0.2283, acc-0.7105\n",
      "Iter-1800, train loss-0.2246, acc-0.6900, valid loss-0.2240, acc-0.7280, test loss-0.2276, acc-0.7101\n",
      "Iter-1810, train loss-0.2368, acc-0.6900, valid loss-0.2230, acc-0.7284, test loss-0.2267, acc-0.7108\n",
      "Iter-1820, train loss-0.2446, acc-0.6500, valid loss-0.2224, acc-0.7280, test loss-0.2259, acc-0.7109\n",
      "Iter-1830, train loss-0.2449, acc-0.6600, valid loss-0.2215, acc-0.7280, test loss-0.2249, acc-0.7121\n",
      "Iter-1840, train loss-0.2094, acc-0.7800, valid loss-0.2207, acc-0.7298, test loss-0.2240, acc-0.7141\n",
      "Iter-1850, train loss-0.2366, acc-0.6800, valid loss-0.2197, acc-0.7290, test loss-0.2230, acc-0.7145\n",
      "Iter-1860, train loss-0.2621, acc-0.6500, valid loss-0.2189, acc-0.7296, test loss-0.2222, acc-0.7142\n",
      "Iter-1870, train loss-0.2337, acc-0.6600, valid loss-0.2184, acc-0.7312, test loss-0.2216, acc-0.7149\n",
      "Iter-1880, train loss-0.2329, acc-0.7000, valid loss-0.2174, acc-0.7320, test loss-0.2206, acc-0.7153\n",
      "Iter-1890, train loss-0.2366, acc-0.6900, valid loss-0.2168, acc-0.7312, test loss-0.2200, acc-0.7165\n",
      "Iter-1900, train loss-0.2371, acc-0.7200, valid loss-0.2159, acc-0.7312, test loss-0.2191, acc-0.7170\n",
      "Iter-1910, train loss-0.2301, acc-0.7100, valid loss-0.2150, acc-0.7332, test loss-0.2181, acc-0.7204\n",
      "Iter-1920, train loss-0.2303, acc-0.6600, valid loss-0.2142, acc-0.7338, test loss-0.2172, acc-0.7221\n",
      "Iter-1930, train loss-0.2462, acc-0.7000, valid loss-0.2133, acc-0.7366, test loss-0.2162, acc-0.7235\n",
      "Iter-1940, train loss-0.2012, acc-0.7500, valid loss-0.2125, acc-0.7382, test loss-0.2155, acc-0.7251\n",
      "Iter-1950, train loss-0.2022, acc-0.7500, valid loss-0.2120, acc-0.7376, test loss-0.2151, acc-0.7247\n",
      "Iter-1960, train loss-0.2075, acc-0.7400, valid loss-0.2113, acc-0.7392, test loss-0.2141, acc-0.7268\n",
      "Iter-1970, train loss-0.2735, acc-0.6100, valid loss-0.2105, acc-0.7406, test loss-0.2133, acc-0.7282\n",
      "Iter-1980, train loss-0.2268, acc-0.7000, valid loss-0.2100, acc-0.7410, test loss-0.2127, acc-0.7283\n",
      "Iter-1990, train loss-0.2215, acc-0.7300, valid loss-0.2092, acc-0.7412, test loss-0.2118, acc-0.7305\n",
      "Iter-2000, train loss-0.2272, acc-0.6900, valid loss-0.2086, acc-0.7420, test loss-0.2109, acc-0.7328\n",
      "Iter-2010, train loss-0.2186, acc-0.7000, valid loss-0.2081, acc-0.7430, test loss-0.2103, acc-0.7324\n",
      "Iter-2020, train loss-0.2340, acc-0.7200, valid loss-0.2075, acc-0.7414, test loss-0.2097, acc-0.7320\n",
      "Iter-2030, train loss-0.2121, acc-0.7300, valid loss-0.2069, acc-0.7414, test loss-0.2091, acc-0.7330\n",
      "Iter-2040, train loss-0.1943, acc-0.7500, valid loss-0.2060, acc-0.7432, test loss-0.2083, acc-0.7326\n",
      "Iter-2050, train loss-0.1995, acc-0.7400, valid loss-0.2055, acc-0.7442, test loss-0.2078, acc-0.7327\n",
      "Iter-2060, train loss-0.2272, acc-0.6700, valid loss-0.2048, acc-0.7446, test loss-0.2072, acc-0.7336\n",
      "Iter-2070, train loss-0.1757, acc-0.8100, valid loss-0.2041, acc-0.7462, test loss-0.2064, acc-0.7349\n",
      "Iter-2080, train loss-0.1814, acc-0.7500, valid loss-0.2031, acc-0.7462, test loss-0.2054, acc-0.7361\n",
      "Iter-2090, train loss-0.2137, acc-0.6800, valid loss-0.2025, acc-0.7466, test loss-0.2046, acc-0.7382\n",
      "Iter-2100, train loss-0.2073, acc-0.7500, valid loss-0.2018, acc-0.7470, test loss-0.2039, acc-0.7394\n",
      "Iter-2110, train loss-0.1945, acc-0.7300, valid loss-0.2011, acc-0.7476, test loss-0.2033, acc-0.7401\n",
      "Iter-2120, train loss-0.2464, acc-0.7200, valid loss-0.2006, acc-0.7486, test loss-0.2028, acc-0.7408\n",
      "Iter-2130, train loss-0.2242, acc-0.7000, valid loss-0.2000, acc-0.7502, test loss-0.2022, acc-0.7426\n",
      "Iter-2140, train loss-0.2473, acc-0.6400, valid loss-0.1993, acc-0.7510, test loss-0.2015, acc-0.7436\n",
      "Iter-2150, train loss-0.2151, acc-0.7300, valid loss-0.1987, acc-0.7532, test loss-0.2009, acc-0.7439\n",
      "Iter-2160, train loss-0.2072, acc-0.7400, valid loss-0.1980, acc-0.7526, test loss-0.2002, acc-0.7455\n",
      "Iter-2170, train loss-0.1831, acc-0.7400, valid loss-0.1974, acc-0.7540, test loss-0.1995, acc-0.7461\n",
      "Iter-2180, train loss-0.2161, acc-0.7300, valid loss-0.1967, acc-0.7558, test loss-0.1988, acc-0.7474\n",
      "Iter-2190, train loss-0.2132, acc-0.6800, valid loss-0.1963, acc-0.7570, test loss-0.1981, acc-0.7492\n",
      "Iter-2200, train loss-0.2081, acc-0.7000, valid loss-0.1957, acc-0.7568, test loss-0.1974, acc-0.7500\n",
      "Iter-2210, train loss-0.2299, acc-0.6600, valid loss-0.1955, acc-0.7548, test loss-0.1974, acc-0.7480\n",
      "Iter-2220, train loss-0.1881, acc-0.7400, valid loss-0.1951, acc-0.7558, test loss-0.1972, acc-0.7477\n",
      "Iter-2230, train loss-0.2207, acc-0.7600, valid loss-0.1946, acc-0.7570, test loss-0.1969, acc-0.7471\n",
      "Iter-2240, train loss-0.2120, acc-0.7300, valid loss-0.1941, acc-0.7592, test loss-0.1961, acc-0.7497\n",
      "Iter-2250, train loss-0.1973, acc-0.7800, valid loss-0.1934, acc-0.7600, test loss-0.1954, acc-0.7498\n",
      "Iter-2260, train loss-0.1869, acc-0.7500, valid loss-0.1933, acc-0.7580, test loss-0.1951, acc-0.7501\n",
      "Iter-2270, train loss-0.2091, acc-0.7400, valid loss-0.1927, acc-0.7586, test loss-0.1947, acc-0.7512\n",
      "Iter-2280, train loss-0.1779, acc-0.7600, valid loss-0.1922, acc-0.7600, test loss-0.1942, acc-0.7512\n",
      "Iter-2290, train loss-0.1795, acc-0.7500, valid loss-0.1916, acc-0.7604, test loss-0.1936, acc-0.7516\n",
      "Iter-2300, train loss-0.1888, acc-0.7300, valid loss-0.1912, acc-0.7626, test loss-0.1931, acc-0.7523\n",
      "Iter-2310, train loss-0.2159, acc-0.7200, valid loss-0.1909, acc-0.7620, test loss-0.1929, acc-0.7524\n",
      "Iter-2320, train loss-0.2023, acc-0.7200, valid loss-0.1905, acc-0.7616, test loss-0.1923, acc-0.7549\n",
      "Iter-2330, train loss-0.1856, acc-0.7800, valid loss-0.1900, acc-0.7642, test loss-0.1920, acc-0.7541\n",
      "Iter-2340, train loss-0.2280, acc-0.7000, valid loss-0.1895, acc-0.7652, test loss-0.1915, acc-0.7552\n",
      "Iter-2350, train loss-0.2022, acc-0.7300, valid loss-0.1892, acc-0.7652, test loss-0.1912, acc-0.7550\n",
      "Iter-2360, train loss-0.2227, acc-0.7000, valid loss-0.1888, acc-0.7644, test loss-0.1908, acc-0.7548\n",
      "Iter-2370, train loss-0.2024, acc-0.7400, valid loss-0.1885, acc-0.7664, test loss-0.1902, acc-0.7562\n",
      "Iter-2380, train loss-0.2043, acc-0.7100, valid loss-0.1879, acc-0.7670, test loss-0.1896, acc-0.7568\n",
      "Iter-2390, train loss-0.1951, acc-0.7500, valid loss-0.1877, acc-0.7650, test loss-0.1897, acc-0.7534\n",
      "Iter-2400, train loss-0.1597, acc-0.8400, valid loss-0.1872, acc-0.7656, test loss-0.1891, acc-0.7539\n",
      "Iter-2410, train loss-0.1872, acc-0.7600, valid loss-0.1868, acc-0.7672, test loss-0.1887, acc-0.7552\n",
      "Iter-2420, train loss-0.1975, acc-0.7500, valid loss-0.1867, acc-0.7692, test loss-0.1887, acc-0.7558\n",
      "Iter-2430, train loss-0.2302, acc-0.7300, valid loss-0.1863, acc-0.7708, test loss-0.1882, acc-0.7582\n",
      "Iter-2440, train loss-0.2143, acc-0.6900, valid loss-0.1858, acc-0.7702, test loss-0.1878, acc-0.7569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.1843, acc-0.7800, valid loss-0.1859, acc-0.7706, test loss-0.1878, acc-0.7585\n",
      "Iter-2460, train loss-0.2034, acc-0.7600, valid loss-0.1858, acc-0.7702, test loss-0.1878, acc-0.7566\n",
      "Iter-2470, train loss-0.1827, acc-0.7800, valid loss-0.1856, acc-0.7670, test loss-0.1878, acc-0.7535\n",
      "Iter-2480, train loss-0.1949, acc-0.7000, valid loss-0.1856, acc-0.7680, test loss-0.1878, acc-0.7555\n",
      "Iter-2490, train loss-0.2030, acc-0.7200, valid loss-0.1852, acc-0.7706, test loss-0.1872, acc-0.7576\n",
      "Iter-2500, train loss-0.1948, acc-0.7600, valid loss-0.1848, acc-0.7716, test loss-0.1868, acc-0.7589\n",
      "Iter-2510, train loss-0.1717, acc-0.8000, valid loss-0.1850, acc-0.7716, test loss-0.1870, acc-0.7582\n",
      "Iter-2520, train loss-0.2067, acc-0.7400, valid loss-0.1851, acc-0.7732, test loss-0.1872, acc-0.7585\n",
      "Iter-2530, train loss-0.1956, acc-0.7300, valid loss-0.1850, acc-0.7730, test loss-0.1870, acc-0.7593\n",
      "Iter-2540, train loss-0.1760, acc-0.8000, valid loss-0.1849, acc-0.7736, test loss-0.1869, acc-0.7603\n",
      "Iter-2550, train loss-0.2007, acc-0.7200, valid loss-0.1847, acc-0.7740, test loss-0.1868, acc-0.7586\n",
      "Iter-2560, train loss-0.2055, acc-0.7400, valid loss-0.1847, acc-0.7728, test loss-0.1866, acc-0.7587\n",
      "Iter-2570, train loss-0.1827, acc-0.7600, valid loss-0.1845, acc-0.7730, test loss-0.1862, acc-0.7604\n",
      "Iter-2580, train loss-0.2151, acc-0.7200, valid loss-0.1843, acc-0.7730, test loss-0.1860, acc-0.7619\n",
      "Iter-2590, train loss-0.2476, acc-0.6500, valid loss-0.1843, acc-0.7718, test loss-0.1858, acc-0.7623\n",
      "Iter-2600, train loss-0.2101, acc-0.7300, valid loss-0.1840, acc-0.7734, test loss-0.1854, acc-0.7641\n",
      "Iter-2610, train loss-0.2409, acc-0.6300, valid loss-0.1837, acc-0.7744, test loss-0.1851, acc-0.7632\n",
      "Iter-2620, train loss-0.1738, acc-0.8600, valid loss-0.1836, acc-0.7730, test loss-0.1849, acc-0.7633\n",
      "Iter-2630, train loss-0.1871, acc-0.7800, valid loss-0.1835, acc-0.7712, test loss-0.1849, acc-0.7618\n",
      "Iter-2640, train loss-0.1820, acc-0.7900, valid loss-0.1836, acc-0.7710, test loss-0.1850, acc-0.7618\n",
      "Iter-2650, train loss-0.1756, acc-0.7800, valid loss-0.1835, acc-0.7706, test loss-0.1850, acc-0.7617\n",
      "Iter-2660, train loss-0.1909, acc-0.7300, valid loss-0.1833, acc-0.7718, test loss-0.1846, acc-0.7617\n",
      "Iter-2670, train loss-0.1923, acc-0.7500, valid loss-0.1831, acc-0.7726, test loss-0.1846, acc-0.7611\n",
      "Iter-2680, train loss-0.1678, acc-0.7900, valid loss-0.1827, acc-0.7738, test loss-0.1841, acc-0.7630\n",
      "Iter-2690, train loss-0.1876, acc-0.7500, valid loss-0.1825, acc-0.7730, test loss-0.1841, acc-0.7627\n",
      "Iter-2700, train loss-0.2122, acc-0.7400, valid loss-0.1826, acc-0.7718, test loss-0.1842, acc-0.7626\n",
      "Iter-2710, train loss-0.2294, acc-0.7000, valid loss-0.1824, acc-0.7728, test loss-0.1840, acc-0.7635\n",
      "Iter-2720, train loss-0.1964, acc-0.7500, valid loss-0.1821, acc-0.7714, test loss-0.1838, acc-0.7627\n",
      "Iter-2730, train loss-0.1609, acc-0.7900, valid loss-0.1821, acc-0.7700, test loss-0.1837, acc-0.7626\n",
      "Iter-2740, train loss-0.1959, acc-0.7600, valid loss-0.1821, acc-0.7724, test loss-0.1835, acc-0.7644\n",
      "Iter-2750, train loss-0.1790, acc-0.8100, valid loss-0.1819, acc-0.7740, test loss-0.1835, acc-0.7642\n",
      "Iter-2760, train loss-0.2172, acc-0.7000, valid loss-0.1820, acc-0.7728, test loss-0.1838, acc-0.7638\n",
      "Iter-2770, train loss-0.1867, acc-0.7500, valid loss-0.1819, acc-0.7726, test loss-0.1837, acc-0.7645\n",
      "Iter-2780, train loss-0.1702, acc-0.8000, valid loss-0.1818, acc-0.7706, test loss-0.1838, acc-0.7640\n",
      "Iter-2790, train loss-0.1838, acc-0.7700, valid loss-0.1815, acc-0.7714, test loss-0.1833, acc-0.7657\n",
      "Iter-2800, train loss-0.2059, acc-0.7100, valid loss-0.1813, acc-0.7720, test loss-0.1832, acc-0.7653\n",
      "Iter-2810, train loss-0.1891, acc-0.7800, valid loss-0.1813, acc-0.7736, test loss-0.1831, acc-0.7674\n",
      "Iter-2820, train loss-0.1805, acc-0.7800, valid loss-0.1813, acc-0.7736, test loss-0.1831, acc-0.7668\n",
      "Iter-2830, train loss-0.1863, acc-0.7500, valid loss-0.1814, acc-0.7744, test loss-0.1831, acc-0.7676\n",
      "Iter-2840, train loss-0.1823, acc-0.7800, valid loss-0.1815, acc-0.7752, test loss-0.1830, acc-0.7685\n",
      "Iter-2850, train loss-0.1942, acc-0.7500, valid loss-0.1813, acc-0.7748, test loss-0.1828, acc-0.7697\n",
      "Iter-2860, train loss-0.1693, acc-0.8200, valid loss-0.1816, acc-0.7742, test loss-0.1829, acc-0.7692\n",
      "Iter-2870, train loss-0.1793, acc-0.7500, valid loss-0.1814, acc-0.7740, test loss-0.1830, acc-0.7684\n",
      "Iter-2880, train loss-0.2065, acc-0.7500, valid loss-0.1811, acc-0.7746, test loss-0.1826, acc-0.7686\n",
      "Iter-2890, train loss-0.1985, acc-0.7000, valid loss-0.1814, acc-0.7754, test loss-0.1829, acc-0.7689\n",
      "Iter-2900, train loss-0.1775, acc-0.7800, valid loss-0.1812, acc-0.7746, test loss-0.1829, acc-0.7684\n",
      "Iter-2910, train loss-0.1604, acc-0.8000, valid loss-0.1810, acc-0.7752, test loss-0.1826, acc-0.7682\n",
      "Iter-2920, train loss-0.1971, acc-0.7500, valid loss-0.1811, acc-0.7758, test loss-0.1829, acc-0.7681\n",
      "Iter-2930, train loss-0.2184, acc-0.7100, valid loss-0.1810, acc-0.7756, test loss-0.1828, acc-0.7680\n",
      "Iter-2940, train loss-0.1589, acc-0.8500, valid loss-0.1811, acc-0.7758, test loss-0.1828, acc-0.7683\n",
      "Iter-2950, train loss-0.1252, acc-0.8400, valid loss-0.1808, acc-0.7766, test loss-0.1823, acc-0.7691\n",
      "Iter-2960, train loss-0.1753, acc-0.8000, valid loss-0.1810, acc-0.7768, test loss-0.1826, acc-0.7686\n",
      "Iter-2970, train loss-0.1824, acc-0.7500, valid loss-0.1809, acc-0.7770, test loss-0.1827, acc-0.7682\n",
      "Iter-2980, train loss-0.1646, acc-0.7900, valid loss-0.1809, acc-0.7752, test loss-0.1828, acc-0.7682\n",
      "Iter-2990, train loss-0.1898, acc-0.7600, valid loss-0.1809, acc-0.7760, test loss-0.1828, acc-0.7684\n",
      "Iter-3000, train loss-0.1967, acc-0.7500, valid loss-0.1808, acc-0.7776, test loss-0.1828, acc-0.7695\n",
      "Iter-3010, train loss-0.1895, acc-0.7500, valid loss-0.1810, acc-0.7758, test loss-0.1830, acc-0.7684\n",
      "Iter-3020, train loss-0.1959, acc-0.7300, valid loss-0.1812, acc-0.7756, test loss-0.1833, acc-0.7682\n",
      "Iter-3030, train loss-0.1626, acc-0.8300, valid loss-0.1815, acc-0.7740, test loss-0.1837, acc-0.7670\n",
      "Iter-3040, train loss-0.1731, acc-0.8100, valid loss-0.1815, acc-0.7738, test loss-0.1836, acc-0.7681\n",
      "Iter-3050, train loss-0.1817, acc-0.8000, valid loss-0.1818, acc-0.7736, test loss-0.1839, acc-0.7678\n",
      "Iter-3060, train loss-0.1907, acc-0.7600, valid loss-0.1817, acc-0.7736, test loss-0.1838, acc-0.7685\n",
      "Iter-3070, train loss-0.1810, acc-0.7800, valid loss-0.1820, acc-0.7728, test loss-0.1841, acc-0.7680\n",
      "Iter-3080, train loss-0.1554, acc-0.8000, valid loss-0.1823, acc-0.7728, test loss-0.1844, acc-0.7683\n",
      "Iter-3090, train loss-0.1788, acc-0.8100, valid loss-0.1824, acc-0.7726, test loss-0.1847, acc-0.7675\n",
      "Iter-3100, train loss-0.2248, acc-0.6600, valid loss-0.1827, acc-0.7726, test loss-0.1851, acc-0.7677\n",
      "Iter-3110, train loss-0.1947, acc-0.7700, valid loss-0.1829, acc-0.7732, test loss-0.1852, acc-0.7670\n",
      "Iter-3120, train loss-0.1787, acc-0.7700, valid loss-0.1829, acc-0.7714, test loss-0.1853, acc-0.7673\n",
      "Iter-3130, train loss-0.2011, acc-0.7700, valid loss-0.1828, acc-0.7706, test loss-0.1853, acc-0.7662\n",
      "Iter-3140, train loss-0.1784, acc-0.7900, valid loss-0.1829, acc-0.7714, test loss-0.1853, acc-0.7671\n",
      "Iter-3150, train loss-0.2240, acc-0.6700, valid loss-0.1828, acc-0.7714, test loss-0.1852, acc-0.7669\n",
      "Iter-3160, train loss-0.1695, acc-0.7900, valid loss-0.1832, acc-0.7724, test loss-0.1856, acc-0.7672\n",
      "Iter-3170, train loss-0.1941, acc-0.7700, valid loss-0.1834, acc-0.7714, test loss-0.1858, acc-0.7667\n",
      "Iter-3180, train loss-0.1663, acc-0.7900, valid loss-0.1833, acc-0.7716, test loss-0.1860, acc-0.7653\n",
      "Iter-3190, train loss-0.1830, acc-0.7900, valid loss-0.1833, acc-0.7710, test loss-0.1861, acc-0.7652\n",
      "Iter-3200, train loss-0.1825, acc-0.7900, valid loss-0.1833, acc-0.7716, test loss-0.1861, acc-0.7653\n",
      "Iter-3210, train loss-0.2141, acc-0.7400, valid loss-0.1833, acc-0.7722, test loss-0.1860, acc-0.7651\n",
      "Iter-3220, train loss-0.1643, acc-0.8000, valid loss-0.1835, acc-0.7712, test loss-0.1864, acc-0.7652\n",
      "Iter-3230, train loss-0.2175, acc-0.6700, valid loss-0.1835, acc-0.7712, test loss-0.1865, acc-0.7650\n",
      "Iter-3240, train loss-0.1709, acc-0.7800, valid loss-0.1834, acc-0.7716, test loss-0.1865, acc-0.7645\n",
      "Iter-3250, train loss-0.1978, acc-0.7700, valid loss-0.1832, acc-0.7704, test loss-0.1864, acc-0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.1775, acc-0.7500, valid loss-0.1833, acc-0.7700, test loss-0.1864, acc-0.7627\n",
      "Iter-3270, train loss-0.1773, acc-0.7900, valid loss-0.1836, acc-0.7706, test loss-0.1868, acc-0.7620\n",
      "Iter-3280, train loss-0.1956, acc-0.7500, valid loss-0.1835, acc-0.7708, test loss-0.1866, acc-0.7625\n",
      "Iter-3290, train loss-0.1886, acc-0.7800, valid loss-0.1836, acc-0.7714, test loss-0.1867, acc-0.7627\n",
      "Iter-3300, train loss-0.2090, acc-0.7000, valid loss-0.1837, acc-0.7724, test loss-0.1869, acc-0.7618\n",
      "Iter-3310, train loss-0.2212, acc-0.7300, valid loss-0.1838, acc-0.7712, test loss-0.1870, acc-0.7619\n",
      "Iter-3320, train loss-0.1773, acc-0.8200, valid loss-0.1840, acc-0.7712, test loss-0.1870, acc-0.7630\n",
      "Iter-3330, train loss-0.1988, acc-0.7500, valid loss-0.1844, acc-0.7714, test loss-0.1875, acc-0.7629\n",
      "Iter-3340, train loss-0.1857, acc-0.7800, valid loss-0.1843, acc-0.7714, test loss-0.1875, acc-0.7616\n",
      "Iter-3350, train loss-0.1941, acc-0.7600, valid loss-0.1846, acc-0.7704, test loss-0.1878, acc-0.7631\n",
      "Iter-3360, train loss-0.1613, acc-0.8600, valid loss-0.1847, acc-0.7706, test loss-0.1878, acc-0.7627\n",
      "Iter-3370, train loss-0.1969, acc-0.7300, valid loss-0.1850, acc-0.7698, test loss-0.1880, acc-0.7624\n",
      "Iter-3380, train loss-0.1763, acc-0.7800, valid loss-0.1852, acc-0.7712, test loss-0.1883, acc-0.7617\n",
      "Iter-3390, train loss-0.1803, acc-0.7900, valid loss-0.1854, acc-0.7694, test loss-0.1884, acc-0.7617\n",
      "Iter-3400, train loss-0.1944, acc-0.7700, valid loss-0.1858, acc-0.7710, test loss-0.1889, acc-0.7615\n",
      "Iter-3410, train loss-0.1932, acc-0.7200, valid loss-0.1862, acc-0.7712, test loss-0.1894, acc-0.7610\n",
      "Iter-3420, train loss-0.1859, acc-0.7800, valid loss-0.1862, acc-0.7718, test loss-0.1895, acc-0.7607\n",
      "Iter-3430, train loss-0.1924, acc-0.7700, valid loss-0.1866, acc-0.7714, test loss-0.1902, acc-0.7599\n",
      "Iter-3440, train loss-0.2220, acc-0.7100, valid loss-0.1871, acc-0.7698, test loss-0.1907, acc-0.7590\n",
      "Iter-3450, train loss-0.2276, acc-0.6700, valid loss-0.1872, acc-0.7708, test loss-0.1908, acc-0.7587\n",
      "Iter-3460, train loss-0.1916, acc-0.7400, valid loss-0.1877, acc-0.7676, test loss-0.1916, acc-0.7571\n",
      "Iter-3470, train loss-0.1801, acc-0.7900, valid loss-0.1881, acc-0.7656, test loss-0.1919, acc-0.7574\n",
      "Iter-3480, train loss-0.1882, acc-0.7600, valid loss-0.1886, acc-0.7640, test loss-0.1924, acc-0.7561\n",
      "Iter-3490, train loss-0.2234, acc-0.7100, valid loss-0.1889, acc-0.7634, test loss-0.1930, acc-0.7562\n",
      "Iter-3500, train loss-0.1929, acc-0.7600, valid loss-0.1888, acc-0.7640, test loss-0.1931, acc-0.7565\n",
      "Iter-3510, train loss-0.1897, acc-0.7500, valid loss-0.1893, acc-0.7654, test loss-0.1934, acc-0.7567\n",
      "Iter-3520, train loss-0.2442, acc-0.6900, valid loss-0.1896, acc-0.7656, test loss-0.1938, acc-0.7571\n",
      "Iter-3530, train loss-0.1766, acc-0.7900, valid loss-0.1903, acc-0.7634, test loss-0.1946, acc-0.7556\n",
      "Iter-3540, train loss-0.1973, acc-0.7300, valid loss-0.1904, acc-0.7646, test loss-0.1948, acc-0.7563\n",
      "Iter-3550, train loss-0.2261, acc-0.6700, valid loss-0.1904, acc-0.7640, test loss-0.1948, acc-0.7557\n",
      "Iter-3560, train loss-0.1809, acc-0.7500, valid loss-0.1909, acc-0.7646, test loss-0.1953, acc-0.7542\n",
      "Iter-3570, train loss-0.2149, acc-0.7300, valid loss-0.1912, acc-0.7652, test loss-0.1955, acc-0.7538\n",
      "Iter-3580, train loss-0.2032, acc-0.7600, valid loss-0.1911, acc-0.7638, test loss-0.1956, acc-0.7531\n",
      "Iter-3590, train loss-0.2012, acc-0.7600, valid loss-0.1917, acc-0.7638, test loss-0.1962, acc-0.7526\n",
      "Iter-3600, train loss-0.2199, acc-0.7100, valid loss-0.1919, acc-0.7644, test loss-0.1966, acc-0.7509\n",
      "Iter-3610, train loss-0.2095, acc-0.7100, valid loss-0.1924, acc-0.7658, test loss-0.1969, acc-0.7524\n",
      "Iter-3620, train loss-0.1656, acc-0.8100, valid loss-0.1929, acc-0.7656, test loss-0.1974, acc-0.7524\n",
      "Iter-3630, train loss-0.2063, acc-0.7300, valid loss-0.1930, acc-0.7668, test loss-0.1977, acc-0.7524\n",
      "Iter-3640, train loss-0.1896, acc-0.7600, valid loss-0.1935, acc-0.7658, test loss-0.1982, acc-0.7527\n",
      "Iter-3650, train loss-0.1854, acc-0.7900, valid loss-0.1936, acc-0.7658, test loss-0.1983, acc-0.7530\n",
      "Iter-3660, train loss-0.1792, acc-0.8100, valid loss-0.1939, acc-0.7646, test loss-0.1988, acc-0.7518\n",
      "Iter-3670, train loss-0.2652, acc-0.6500, valid loss-0.1945, acc-0.7630, test loss-0.1995, acc-0.7511\n",
      "Iter-3680, train loss-0.2035, acc-0.7700, valid loss-0.1949, acc-0.7622, test loss-0.2000, acc-0.7503\n",
      "Iter-3690, train loss-0.2177, acc-0.7300, valid loss-0.1955, acc-0.7622, test loss-0.2005, acc-0.7500\n",
      "Iter-3700, train loss-0.2130, acc-0.7300, valid loss-0.1956, acc-0.7640, test loss-0.2006, acc-0.7506\n",
      "Iter-3710, train loss-0.2233, acc-0.7200, valid loss-0.1959, acc-0.7636, test loss-0.2008, acc-0.7513\n",
      "Iter-3720, train loss-0.2279, acc-0.6700, valid loss-0.1964, acc-0.7634, test loss-0.2013, acc-0.7512\n",
      "Iter-3730, train loss-0.2212, acc-0.6900, valid loss-0.1966, acc-0.7638, test loss-0.2014, acc-0.7512\n",
      "Iter-3740, train loss-0.2146, acc-0.7600, valid loss-0.1970, acc-0.7628, test loss-0.2017, acc-0.7503\n",
      "Iter-3750, train loss-0.2006, acc-0.7600, valid loss-0.1974, acc-0.7624, test loss-0.2023, acc-0.7499\n",
      "Iter-3760, train loss-0.1957, acc-0.7600, valid loss-0.1977, acc-0.7620, test loss-0.2025, acc-0.7492\n",
      "Iter-3770, train loss-0.2258, acc-0.6700, valid loss-0.1979, acc-0.7614, test loss-0.2026, acc-0.7499\n",
      "Iter-3780, train loss-0.2153, acc-0.7500, valid loss-0.1985, acc-0.7602, test loss-0.2034, acc-0.7492\n",
      "Iter-3790, train loss-0.1965, acc-0.7200, valid loss-0.1989, acc-0.7594, test loss-0.2037, acc-0.7487\n",
      "Iter-3800, train loss-0.2364, acc-0.6900, valid loss-0.1998, acc-0.7596, test loss-0.2045, acc-0.7481\n",
      "Iter-3810, train loss-0.2020, acc-0.7700, valid loss-0.2003, acc-0.7586, test loss-0.2049, acc-0.7475\n",
      "Iter-3820, train loss-0.2263, acc-0.7100, valid loss-0.2007, acc-0.7602, test loss-0.2053, acc-0.7469\n",
      "Iter-3830, train loss-0.2419, acc-0.6700, valid loss-0.2018, acc-0.7588, test loss-0.2062, acc-0.7471\n",
      "Iter-3840, train loss-0.1774, acc-0.8200, valid loss-0.2022, acc-0.7596, test loss-0.2066, acc-0.7442\n",
      "Iter-3850, train loss-0.2003, acc-0.7500, valid loss-0.2026, acc-0.7590, test loss-0.2069, acc-0.7452\n",
      "Iter-3860, train loss-0.2064, acc-0.7300, valid loss-0.2029, acc-0.7578, test loss-0.2073, acc-0.7445\n",
      "Iter-3870, train loss-0.2294, acc-0.7300, valid loss-0.2027, acc-0.7590, test loss-0.2071, acc-0.7445\n",
      "Iter-3880, train loss-0.2174, acc-0.7200, valid loss-0.2031, acc-0.7580, test loss-0.2075, acc-0.7451\n",
      "Iter-3890, train loss-0.1890, acc-0.7800, valid loss-0.2036, acc-0.7570, test loss-0.2079, acc-0.7448\n",
      "Iter-3900, train loss-0.2135, acc-0.7300, valid loss-0.2036, acc-0.7564, test loss-0.2079, acc-0.7447\n",
      "Iter-3910, train loss-0.1858, acc-0.7600, valid loss-0.2039, acc-0.7526, test loss-0.2084, acc-0.7440\n",
      "Iter-3920, train loss-0.1703, acc-0.7900, valid loss-0.2047, acc-0.7522, test loss-0.2092, acc-0.7431\n",
      "Iter-3930, train loss-0.2058, acc-0.7600, valid loss-0.2046, acc-0.7526, test loss-0.2092, acc-0.7426\n",
      "Iter-3940, train loss-0.2045, acc-0.7700, valid loss-0.2050, acc-0.7512, test loss-0.2096, acc-0.7418\n",
      "Iter-3950, train loss-0.2204, acc-0.7300, valid loss-0.2055, acc-0.7520, test loss-0.2100, acc-0.7419\n",
      "Iter-3960, train loss-0.2121, acc-0.7500, valid loss-0.2065, acc-0.7536, test loss-0.2108, acc-0.7427\n",
      "Iter-3970, train loss-0.2083, acc-0.7500, valid loss-0.2064, acc-0.7502, test loss-0.2106, acc-0.7415\n",
      "Iter-3980, train loss-0.2230, acc-0.7000, valid loss-0.2069, acc-0.7484, test loss-0.2112, acc-0.7410\n",
      "Iter-3990, train loss-0.2451, acc-0.6400, valid loss-0.2073, acc-0.7488, test loss-0.2114, acc-0.7395\n",
      "Iter-4000, train loss-0.2392, acc-0.6500, valid loss-0.2078, acc-0.7490, test loss-0.2119, acc-0.7403\n",
      "Iter-4010, train loss-0.2237, acc-0.7800, valid loss-0.2084, acc-0.7466, test loss-0.2126, acc-0.7370\n",
      "Iter-4020, train loss-0.2297, acc-0.7100, valid loss-0.2088, acc-0.7452, test loss-0.2130, acc-0.7353\n",
      "Iter-4030, train loss-0.2206, acc-0.6900, valid loss-0.2091, acc-0.7434, test loss-0.2137, acc-0.7341\n",
      "Iter-4040, train loss-0.1694, acc-0.8300, valid loss-0.2105, acc-0.7418, test loss-0.2149, acc-0.7329\n",
      "Iter-4050, train loss-0.2484, acc-0.6600, valid loss-0.2102, acc-0.7442, test loss-0.2147, acc-0.7334\n",
      "Iter-4060, train loss-0.2355, acc-0.7100, valid loss-0.2104, acc-0.7452, test loss-0.2147, acc-0.7351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.2233, acc-0.7400, valid loss-0.2109, acc-0.7438, test loss-0.2155, acc-0.7339\n",
      "Iter-4080, train loss-0.1723, acc-0.8000, valid loss-0.2109, acc-0.7446, test loss-0.2154, acc-0.7354\n",
      "Iter-4090, train loss-0.1706, acc-0.8200, valid loss-0.2110, acc-0.7428, test loss-0.2157, acc-0.7337\n",
      "Iter-4100, train loss-0.1686, acc-0.8300, valid loss-0.2117, acc-0.7410, test loss-0.2165, acc-0.7325\n",
      "Iter-4110, train loss-0.1929, acc-0.7800, valid loss-0.2123, acc-0.7422, test loss-0.2166, acc-0.7334\n",
      "Iter-4120, train loss-0.2652, acc-0.6200, valid loss-0.2125, acc-0.7406, test loss-0.2170, acc-0.7317\n",
      "Iter-4130, train loss-0.2006, acc-0.7200, valid loss-0.2126, acc-0.7398, test loss-0.2173, acc-0.7296\n",
      "Iter-4140, train loss-0.2132, acc-0.7300, valid loss-0.2121, acc-0.7400, test loss-0.2165, acc-0.7305\n",
      "Iter-4150, train loss-0.2299, acc-0.6800, valid loss-0.2122, acc-0.7388, test loss-0.2168, acc-0.7279\n",
      "Iter-4160, train loss-0.2211, acc-0.7300, valid loss-0.2121, acc-0.7390, test loss-0.2166, acc-0.7289\n",
      "Iter-4170, train loss-0.2123, acc-0.7500, valid loss-0.2125, acc-0.7364, test loss-0.2170, acc-0.7260\n",
      "Iter-4180, train loss-0.2362, acc-0.7200, valid loss-0.2126, acc-0.7366, test loss-0.2170, acc-0.7259\n",
      "Iter-4190, train loss-0.2215, acc-0.7000, valid loss-0.2129, acc-0.7372, test loss-0.2174, acc-0.7256\n",
      "Iter-4200, train loss-0.2306, acc-0.6700, valid loss-0.2128, acc-0.7382, test loss-0.2173, acc-0.7269\n",
      "Iter-4210, train loss-0.1999, acc-0.7600, valid loss-0.2129, acc-0.7386, test loss-0.2175, acc-0.7275\n",
      "Iter-4220, train loss-0.1919, acc-0.7600, valid loss-0.2127, acc-0.7390, test loss-0.2172, acc-0.7283\n",
      "Iter-4230, train loss-0.2309, acc-0.7200, valid loss-0.2132, acc-0.7388, test loss-0.2178, acc-0.7258\n",
      "Iter-4240, train loss-0.2719, acc-0.6400, valid loss-0.2132, acc-0.7384, test loss-0.2178, acc-0.7245\n",
      "Iter-4250, train loss-0.1916, acc-0.7800, valid loss-0.2132, acc-0.7362, test loss-0.2177, acc-0.7253\n",
      "Iter-4260, train loss-0.1777, acc-0.7600, valid loss-0.2133, acc-0.7356, test loss-0.2180, acc-0.7231\n",
      "Iter-4270, train loss-0.2421, acc-0.7000, valid loss-0.2136, acc-0.7352, test loss-0.2183, acc-0.7232\n",
      "Iter-4280, train loss-0.2196, acc-0.6500, valid loss-0.2134, acc-0.7346, test loss-0.2181, acc-0.7207\n",
      "Iter-4290, train loss-0.2335, acc-0.6800, valid loss-0.2139, acc-0.7378, test loss-0.2186, acc-0.7238\n",
      "Iter-4300, train loss-0.2119, acc-0.7500, valid loss-0.2136, acc-0.7358, test loss-0.2183, acc-0.7222\n",
      "Iter-4310, train loss-0.2283, acc-0.7100, valid loss-0.2136, acc-0.7374, test loss-0.2182, acc-0.7252\n",
      "Iter-4320, train loss-0.2083, acc-0.7300, valid loss-0.2136, acc-0.7370, test loss-0.2183, acc-0.7241\n",
      "Iter-4330, train loss-0.2052, acc-0.7200, valid loss-0.2141, acc-0.7366, test loss-0.2188, acc-0.7237\n",
      "Iter-4340, train loss-0.2696, acc-0.6000, valid loss-0.2143, acc-0.7360, test loss-0.2190, acc-0.7240\n",
      "Iter-4350, train loss-0.1997, acc-0.7700, valid loss-0.2142, acc-0.7378, test loss-0.2186, acc-0.7261\n",
      "Iter-4360, train loss-0.2328, acc-0.7400, valid loss-0.2143, acc-0.7374, test loss-0.2186, acc-0.7274\n",
      "Iter-4370, train loss-0.2187, acc-0.7300, valid loss-0.2148, acc-0.7400, test loss-0.2193, acc-0.7278\n",
      "Iter-4380, train loss-0.2169, acc-0.7600, valid loss-0.2149, acc-0.7386, test loss-0.2196, acc-0.7274\n",
      "Iter-4390, train loss-0.2043, acc-0.7300, valid loss-0.2150, acc-0.7378, test loss-0.2198, acc-0.7263\n",
      "Iter-4400, train loss-0.1954, acc-0.7400, valid loss-0.2156, acc-0.7382, test loss-0.2202, acc-0.7263\n",
      "Iter-4410, train loss-0.2205, acc-0.7600, valid loss-0.2165, acc-0.7380, test loss-0.2209, acc-0.7258\n",
      "Iter-4420, train loss-0.2576, acc-0.6500, valid loss-0.2166, acc-0.7384, test loss-0.2210, acc-0.7268\n",
      "Iter-4430, train loss-0.2559, acc-0.6400, valid loss-0.2162, acc-0.7368, test loss-0.2205, acc-0.7261\n",
      "Iter-4440, train loss-0.2252, acc-0.7300, valid loss-0.2168, acc-0.7370, test loss-0.2211, acc-0.7263\n",
      "Iter-4450, train loss-0.2327, acc-0.7200, valid loss-0.2170, acc-0.7370, test loss-0.2213, acc-0.7264\n",
      "Iter-4460, train loss-0.2225, acc-0.7100, valid loss-0.2173, acc-0.7358, test loss-0.2216, acc-0.7257\n",
      "Iter-4470, train loss-0.2216, acc-0.7100, valid loss-0.2181, acc-0.7354, test loss-0.2224, acc-0.7248\n",
      "Iter-4480, train loss-0.2426, acc-0.6800, valid loss-0.2188, acc-0.7326, test loss-0.2231, acc-0.7221\n",
      "Iter-4490, train loss-0.2446, acc-0.6800, valid loss-0.2192, acc-0.7334, test loss-0.2235, acc-0.7220\n",
      "Iter-4500, train loss-0.2079, acc-0.7600, valid loss-0.2191, acc-0.7306, test loss-0.2235, acc-0.7196\n",
      "Iter-4510, train loss-0.2153, acc-0.7300, valid loss-0.2192, acc-0.7334, test loss-0.2236, acc-0.7219\n",
      "Iter-4520, train loss-0.2302, acc-0.6800, valid loss-0.2196, acc-0.7340, test loss-0.2240, acc-0.7230\n",
      "Iter-4530, train loss-0.2082, acc-0.7300, valid loss-0.2200, acc-0.7322, test loss-0.2245, acc-0.7220\n",
      "Iter-4540, train loss-0.1990, acc-0.7700, valid loss-0.2202, acc-0.7312, test loss-0.2246, acc-0.7218\n",
      "Iter-4550, train loss-0.2586, acc-0.7000, valid loss-0.2195, acc-0.7354, test loss-0.2238, acc-0.7237\n",
      "Iter-4560, train loss-0.1980, acc-0.7500, valid loss-0.2200, acc-0.7344, test loss-0.2244, acc-0.7242\n",
      "Iter-4570, train loss-0.2278, acc-0.7100, valid loss-0.2203, acc-0.7340, test loss-0.2248, acc-0.7214\n",
      "Iter-4580, train loss-0.2193, acc-0.7100, valid loss-0.2210, acc-0.7328, test loss-0.2255, acc-0.7220\n",
      "Iter-4590, train loss-0.2709, acc-0.6300, valid loss-0.2210, acc-0.7324, test loss-0.2256, acc-0.7214\n",
      "Iter-4600, train loss-0.2585, acc-0.6700, valid loss-0.2212, acc-0.7322, test loss-0.2258, acc-0.7189\n",
      "Iter-4610, train loss-0.2515, acc-0.6700, valid loss-0.2212, acc-0.7324, test loss-0.2259, acc-0.7194\n",
      "Iter-4620, train loss-0.2310, acc-0.6800, valid loss-0.2211, acc-0.7328, test loss-0.2258, acc-0.7204\n",
      "Iter-4630, train loss-0.2280, acc-0.7300, valid loss-0.2211, acc-0.7342, test loss-0.2259, acc-0.7194\n",
      "Iter-4640, train loss-0.2241, acc-0.7200, valid loss-0.2214, acc-0.7350, test loss-0.2262, acc-0.7208\n",
      "Iter-4650, train loss-0.2374, acc-0.7100, valid loss-0.2209, acc-0.7346, test loss-0.2259, acc-0.7202\n",
      "Iter-4660, train loss-0.2349, acc-0.7100, valid loss-0.2209, acc-0.7332, test loss-0.2257, acc-0.7211\n",
      "Iter-4670, train loss-0.2746, acc-0.6300, valid loss-0.2210, acc-0.7340, test loss-0.2258, acc-0.7244\n",
      "Iter-4680, train loss-0.2448, acc-0.6800, valid loss-0.2210, acc-0.7356, test loss-0.2256, acc-0.7250\n",
      "Iter-4690, train loss-0.1991, acc-0.7600, valid loss-0.2209, acc-0.7356, test loss-0.2255, acc-0.7244\n",
      "Iter-4700, train loss-0.2044, acc-0.7600, valid loss-0.2210, acc-0.7370, test loss-0.2255, acc-0.7250\n",
      "Iter-4710, train loss-0.2324, acc-0.7000, valid loss-0.2214, acc-0.7384, test loss-0.2259, acc-0.7269\n",
      "Iter-4720, train loss-0.2195, acc-0.7100, valid loss-0.2217, acc-0.7370, test loss-0.2263, acc-0.7263\n",
      "Iter-4730, train loss-0.2277, acc-0.7700, valid loss-0.2220, acc-0.7380, test loss-0.2264, acc-0.7270\n",
      "Iter-4740, train loss-0.2194, acc-0.7400, valid loss-0.2223, acc-0.7386, test loss-0.2269, acc-0.7273\n",
      "Iter-4750, train loss-0.2672, acc-0.6300, valid loss-0.2226, acc-0.7390, test loss-0.2273, acc-0.7275\n",
      "Iter-4760, train loss-0.2342, acc-0.7100, valid loss-0.2233, acc-0.7380, test loss-0.2281, acc-0.7265\n",
      "Iter-4770, train loss-0.2201, acc-0.7700, valid loss-0.2230, acc-0.7376, test loss-0.2280, acc-0.7245\n",
      "Iter-4780, train loss-0.1988, acc-0.7800, valid loss-0.2231, acc-0.7352, test loss-0.2282, acc-0.7229\n",
      "Iter-4790, train loss-0.2454, acc-0.7300, valid loss-0.2234, acc-0.7338, test loss-0.2284, acc-0.7230\n",
      "Iter-4800, train loss-0.2377, acc-0.6800, valid loss-0.2231, acc-0.7332, test loss-0.2283, acc-0.7188\n",
      "Iter-4810, train loss-0.2517, acc-0.7100, valid loss-0.2235, acc-0.7314, test loss-0.2287, acc-0.7197\n",
      "Iter-4820, train loss-0.2347, acc-0.7500, valid loss-0.2236, acc-0.7316, test loss-0.2289, acc-0.7158\n",
      "Iter-4830, train loss-0.2554, acc-0.7300, valid loss-0.2235, acc-0.7300, test loss-0.2290, acc-0.7152\n",
      "Iter-4840, train loss-0.2182, acc-0.7200, valid loss-0.2239, acc-0.7298, test loss-0.2292, acc-0.7156\n",
      "Iter-4850, train loss-0.2409, acc-0.7200, valid loss-0.2237, acc-0.7306, test loss-0.2290, acc-0.7175\n",
      "Iter-4860, train loss-0.2180, acc-0.7300, valid loss-0.2236, acc-0.7304, test loss-0.2290, acc-0.7171\n",
      "Iter-4870, train loss-0.2097, acc-0.6900, valid loss-0.2235, acc-0.7302, test loss-0.2288, acc-0.7161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.2770, acc-0.6700, valid loss-0.2232, acc-0.7284, test loss-0.2286, acc-0.7166\n",
      "Iter-4890, train loss-0.2470, acc-0.7300, valid loss-0.2234, acc-0.7300, test loss-0.2289, acc-0.7166\n",
      "Iter-4900, train loss-0.2378, acc-0.7200, valid loss-0.2233, acc-0.7300, test loss-0.2286, acc-0.7181\n",
      "Iter-4910, train loss-0.2046, acc-0.7800, valid loss-0.2234, acc-0.7304, test loss-0.2287, acc-0.7178\n",
      "Iter-4920, train loss-0.2328, acc-0.7000, valid loss-0.2235, acc-0.7304, test loss-0.2287, acc-0.7195\n",
      "Iter-4930, train loss-0.2427, acc-0.6800, valid loss-0.2232, acc-0.7290, test loss-0.2284, acc-0.7182\n",
      "Iter-4940, train loss-0.2226, acc-0.7300, valid loss-0.2238, acc-0.7286, test loss-0.2291, acc-0.7154\n",
      "Iter-4950, train loss-0.2333, acc-0.7200, valid loss-0.2236, acc-0.7308, test loss-0.2287, acc-0.7174\n",
      "Iter-4960, train loss-0.2274, acc-0.7400, valid loss-0.2236, acc-0.7296, test loss-0.2288, acc-0.7171\n",
      "Iter-4970, train loss-0.2397, acc-0.6700, valid loss-0.2237, acc-0.7296, test loss-0.2288, acc-0.7174\n",
      "Iter-4980, train loss-0.2461, acc-0.6700, valid loss-0.2241, acc-0.7334, test loss-0.2292, acc-0.7186\n",
      "Iter-4990, train loss-0.2119, acc-0.7600, valid loss-0.2236, acc-0.7300, test loss-0.2287, acc-0.7185\n",
      "Iter-5000, train loss-0.2685, acc-0.6900, valid loss-0.2236, acc-0.7312, test loss-0.2288, acc-0.7186\n",
      "Iter-5010, train loss-0.2168, acc-0.7100, valid loss-0.2239, acc-0.7314, test loss-0.2290, acc-0.7185\n",
      "Iter-5020, train loss-0.2419, acc-0.6600, valid loss-0.2236, acc-0.7306, test loss-0.2289, acc-0.7179\n",
      "Iter-5030, train loss-0.2215, acc-0.7200, valid loss-0.2238, acc-0.7308, test loss-0.2290, acc-0.7176\n",
      "Iter-5040, train loss-0.2041, acc-0.7600, valid loss-0.2244, acc-0.7346, test loss-0.2294, acc-0.7218\n",
      "Iter-5050, train loss-0.2080, acc-0.7600, valid loss-0.2252, acc-0.7334, test loss-0.2303, acc-0.7209\n",
      "Iter-5060, train loss-0.2125, acc-0.7800, valid loss-0.2252, acc-0.7306, test loss-0.2304, acc-0.7178\n",
      "Iter-5070, train loss-0.2315, acc-0.7500, valid loss-0.2248, acc-0.7320, test loss-0.2300, acc-0.7192\n",
      "Iter-5080, train loss-0.2429, acc-0.7000, valid loss-0.2249, acc-0.7332, test loss-0.2301, acc-0.7196\n",
      "Iter-5090, train loss-0.2765, acc-0.6500, valid loss-0.2254, acc-0.7332, test loss-0.2307, acc-0.7192\n",
      "Iter-5100, train loss-0.2310, acc-0.6800, valid loss-0.2251, acc-0.7330, test loss-0.2304, acc-0.7217\n",
      "Iter-5110, train loss-0.2444, acc-0.6700, valid loss-0.2249, acc-0.7346, test loss-0.2302, acc-0.7215\n",
      "Iter-5120, train loss-0.2392, acc-0.6700, valid loss-0.2250, acc-0.7350, test loss-0.2302, acc-0.7232\n",
      "Iter-5130, train loss-0.2565, acc-0.6900, valid loss-0.2250, acc-0.7328, test loss-0.2302, acc-0.7205\n",
      "Iter-5140, train loss-0.2119, acc-0.7400, valid loss-0.2246, acc-0.7330, test loss-0.2299, acc-0.7212\n",
      "Iter-5150, train loss-0.2479, acc-0.6900, valid loss-0.2251, acc-0.7288, test loss-0.2304, acc-0.7172\n",
      "Iter-5160, train loss-0.2173, acc-0.6900, valid loss-0.2260, acc-0.7286, test loss-0.2313, acc-0.7170\n",
      "Iter-5170, train loss-0.2554, acc-0.6900, valid loss-0.2260, acc-0.7282, test loss-0.2312, acc-0.7172\n",
      "Iter-5180, train loss-0.2520, acc-0.6600, valid loss-0.2257, acc-0.7266, test loss-0.2311, acc-0.7156\n",
      "Iter-5190, train loss-0.2354, acc-0.6800, valid loss-0.2262, acc-0.7254, test loss-0.2317, acc-0.7156\n",
      "Iter-5200, train loss-0.2707, acc-0.6800, valid loss-0.2263, acc-0.7280, test loss-0.2318, acc-0.7185\n",
      "Iter-5210, train loss-0.2563, acc-0.6800, valid loss-0.2263, acc-0.7274, test loss-0.2319, acc-0.7166\n",
      "Iter-5220, train loss-0.2040, acc-0.7500, valid loss-0.2264, acc-0.7276, test loss-0.2319, acc-0.7170\n",
      "Iter-5230, train loss-0.2203, acc-0.7500, valid loss-0.2266, acc-0.7258, test loss-0.2322, acc-0.7155\n",
      "Iter-5240, train loss-0.2346, acc-0.7200, valid loss-0.2269, acc-0.7278, test loss-0.2323, acc-0.7176\n",
      "Iter-5250, train loss-0.2262, acc-0.7300, valid loss-0.2278, acc-0.7290, test loss-0.2332, acc-0.7175\n",
      "Iter-5260, train loss-0.2198, acc-0.8100, valid loss-0.2279, acc-0.7278, test loss-0.2334, acc-0.7171\n",
      "Iter-5270, train loss-0.2267, acc-0.6800, valid loss-0.2278, acc-0.7280, test loss-0.2334, acc-0.7168\n",
      "Iter-5280, train loss-0.2231, acc-0.7400, valid loss-0.2276, acc-0.7282, test loss-0.2333, acc-0.7176\n",
      "Iter-5290, train loss-0.2410, acc-0.7500, valid loss-0.2276, acc-0.7280, test loss-0.2334, acc-0.7164\n",
      "Iter-5300, train loss-0.2004, acc-0.7800, valid loss-0.2275, acc-0.7264, test loss-0.2333, acc-0.7158\n",
      "Iter-5310, train loss-0.2290, acc-0.7200, valid loss-0.2275, acc-0.7268, test loss-0.2333, acc-0.7168\n",
      "Iter-5320, train loss-0.2561, acc-0.6400, valid loss-0.2279, acc-0.7276, test loss-0.2337, acc-0.7182\n",
      "Iter-5330, train loss-0.2454, acc-0.7100, valid loss-0.2280, acc-0.7280, test loss-0.2337, acc-0.7185\n",
      "Iter-5340, train loss-0.2184, acc-0.7600, valid loss-0.2281, acc-0.7274, test loss-0.2338, acc-0.7169\n",
      "Iter-5350, train loss-0.2763, acc-0.6100, valid loss-0.2284, acc-0.7296, test loss-0.2340, acc-0.7172\n",
      "Iter-5360, train loss-0.2106, acc-0.7800, valid loss-0.2285, acc-0.7284, test loss-0.2343, acc-0.7165\n",
      "Iter-5370, train loss-0.2333, acc-0.7400, valid loss-0.2290, acc-0.7290, test loss-0.2347, acc-0.7162\n",
      "Iter-5380, train loss-0.2366, acc-0.7100, valid loss-0.2287, acc-0.7294, test loss-0.2345, acc-0.7160\n",
      "Iter-5390, train loss-0.2050, acc-0.7600, valid loss-0.2284, acc-0.7292, test loss-0.2342, acc-0.7154\n",
      "Iter-5400, train loss-0.2134, acc-0.8000, valid loss-0.2287, acc-0.7286, test loss-0.2344, acc-0.7165\n",
      "Iter-5410, train loss-0.2341, acc-0.7400, valid loss-0.2290, acc-0.7268, test loss-0.2347, acc-0.7155\n",
      "Iter-5420, train loss-0.2602, acc-0.6900, valid loss-0.2288, acc-0.7260, test loss-0.2345, acc-0.7158\n",
      "Iter-5430, train loss-0.2735, acc-0.6800, valid loss-0.2293, acc-0.7252, test loss-0.2349, acc-0.7141\n",
      "Iter-5440, train loss-0.2430, acc-0.7200, valid loss-0.2296, acc-0.7256, test loss-0.2352, acc-0.7143\n",
      "Iter-5450, train loss-0.2583, acc-0.6600, valid loss-0.2303, acc-0.7260, test loss-0.2357, acc-0.7150\n",
      "Iter-5460, train loss-0.2616, acc-0.6700, valid loss-0.2301, acc-0.7266, test loss-0.2357, acc-0.7157\n",
      "Iter-5470, train loss-0.2231, acc-0.7800, valid loss-0.2310, acc-0.7256, test loss-0.2366, acc-0.7157\n",
      "Iter-5480, train loss-0.2162, acc-0.7400, valid loss-0.2307, acc-0.7254, test loss-0.2364, acc-0.7153\n",
      "Iter-5490, train loss-0.2163, acc-0.7600, valid loss-0.2311, acc-0.7262, test loss-0.2368, acc-0.7148\n",
      "Iter-5500, train loss-0.2338, acc-0.7300, valid loss-0.2310, acc-0.7264, test loss-0.2368, acc-0.7148\n",
      "Iter-5510, train loss-0.2379, acc-0.7200, valid loss-0.2307, acc-0.7250, test loss-0.2366, acc-0.7146\n",
      "Iter-5520, train loss-0.2538, acc-0.7200, valid loss-0.2310, acc-0.7238, test loss-0.2369, acc-0.7143\n",
      "Iter-5530, train loss-0.2413, acc-0.6800, valid loss-0.2316, acc-0.7232, test loss-0.2377, acc-0.7139\n",
      "Iter-5540, train loss-0.2219, acc-0.7600, valid loss-0.2316, acc-0.7234, test loss-0.2378, acc-0.7135\n",
      "Iter-5550, train loss-0.2105, acc-0.7900, valid loss-0.2318, acc-0.7232, test loss-0.2379, acc-0.7131\n",
      "Iter-5560, train loss-0.2618, acc-0.6800, valid loss-0.2322, acc-0.7218, test loss-0.2380, acc-0.7132\n",
      "Iter-5570, train loss-0.2553, acc-0.7000, valid loss-0.2328, acc-0.7228, test loss-0.2386, acc-0.7123\n",
      "Iter-5580, train loss-0.2612, acc-0.6900, valid loss-0.2329, acc-0.7222, test loss-0.2389, acc-0.7106\n",
      "Iter-5590, train loss-0.2334, acc-0.7400, valid loss-0.2330, acc-0.7246, test loss-0.2389, acc-0.7119\n",
      "Iter-5600, train loss-0.2532, acc-0.6700, valid loss-0.2327, acc-0.7240, test loss-0.2385, acc-0.7134\n",
      "Iter-5610, train loss-0.2509, acc-0.7100, valid loss-0.2330, acc-0.7250, test loss-0.2387, acc-0.7113\n",
      "Iter-5620, train loss-0.2958, acc-0.6100, valid loss-0.2328, acc-0.7244, test loss-0.2386, acc-0.7116\n",
      "Iter-5630, train loss-0.2083, acc-0.7900, valid loss-0.2332, acc-0.7240, test loss-0.2391, acc-0.7099\n",
      "Iter-5640, train loss-0.2544, acc-0.6800, valid loss-0.2337, acc-0.7230, test loss-0.2397, acc-0.7087\n",
      "Iter-5650, train loss-0.2350, acc-0.6900, valid loss-0.2341, acc-0.7224, test loss-0.2401, acc-0.7099\n",
      "Iter-5660, train loss-0.2545, acc-0.7000, valid loss-0.2341, acc-0.7234, test loss-0.2402, acc-0.7099\n",
      "Iter-5670, train loss-0.2241, acc-0.7700, valid loss-0.2344, acc-0.7228, test loss-0.2406, acc-0.7097\n",
      "Iter-5680, train loss-0.2787, acc-0.6900, valid loss-0.2349, acc-0.7216, test loss-0.2410, acc-0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-0.2841, acc-0.6400, valid loss-0.2351, acc-0.7206, test loss-0.2410, acc-0.7073\n",
      "Iter-5700, train loss-0.2345, acc-0.7500, valid loss-0.2349, acc-0.7202, test loss-0.2409, acc-0.7074\n",
      "Iter-5710, train loss-0.2416, acc-0.6900, valid loss-0.2349, acc-0.7194, test loss-0.2408, acc-0.7068\n",
      "Iter-5720, train loss-0.1985, acc-0.7500, valid loss-0.2348, acc-0.7190, test loss-0.2407, acc-0.7063\n",
      "Iter-5730, train loss-0.2540, acc-0.7100, valid loss-0.2350, acc-0.7186, test loss-0.2408, acc-0.7062\n",
      "Iter-5740, train loss-0.2157, acc-0.7600, valid loss-0.2346, acc-0.7180, test loss-0.2404, acc-0.7060\n",
      "Iter-5750, train loss-0.2321, acc-0.7200, valid loss-0.2348, acc-0.7196, test loss-0.2406, acc-0.7065\n",
      "Iter-5760, train loss-0.2847, acc-0.6600, valid loss-0.2346, acc-0.7196, test loss-0.2404, acc-0.7067\n",
      "Iter-5770, train loss-0.2546, acc-0.6700, valid loss-0.2350, acc-0.7190, test loss-0.2407, acc-0.7066\n",
      "Iter-5780, train loss-0.2631, acc-0.6900, valid loss-0.2356, acc-0.7180, test loss-0.2413, acc-0.7059\n",
      "Iter-5790, train loss-0.2454, acc-0.7400, valid loss-0.2359, acc-0.7188, test loss-0.2416, acc-0.7072\n",
      "Iter-5800, train loss-0.2650, acc-0.6500, valid loss-0.2360, acc-0.7194, test loss-0.2419, acc-0.7061\n",
      "Iter-5810, train loss-0.2324, acc-0.7300, valid loss-0.2358, acc-0.7204, test loss-0.2417, acc-0.7058\n",
      "Iter-5820, train loss-0.2352, acc-0.7500, valid loss-0.2359, acc-0.7204, test loss-0.2417, acc-0.7058\n",
      "Iter-5830, train loss-0.2773, acc-0.6200, valid loss-0.2360, acc-0.7190, test loss-0.2417, acc-0.7051\n",
      "Iter-5840, train loss-0.2450, acc-0.7100, valid loss-0.2360, acc-0.7188, test loss-0.2416, acc-0.7059\n",
      "Iter-5850, train loss-0.2470, acc-0.7300, valid loss-0.2364, acc-0.7190, test loss-0.2420, acc-0.7069\n",
      "Iter-5860, train loss-0.2906, acc-0.6300, valid loss-0.2367, acc-0.7198, test loss-0.2425, acc-0.7072\n",
      "Iter-5870, train loss-0.2774, acc-0.5900, valid loss-0.2367, acc-0.7194, test loss-0.2426, acc-0.7068\n",
      "Iter-5880, train loss-0.2487, acc-0.6600, valid loss-0.2367, acc-0.7188, test loss-0.2426, acc-0.7068\n",
      "Iter-5890, train loss-0.2456, acc-0.6900, valid loss-0.2369, acc-0.7180, test loss-0.2427, acc-0.7063\n",
      "Iter-5900, train loss-0.2493, acc-0.6900, valid loss-0.2374, acc-0.7182, test loss-0.2432, acc-0.7054\n",
      "Iter-5910, train loss-0.2245, acc-0.7500, valid loss-0.2380, acc-0.7154, test loss-0.2436, acc-0.7036\n",
      "Iter-5920, train loss-0.2578, acc-0.6400, valid loss-0.2386, acc-0.7162, test loss-0.2442, acc-0.7041\n",
      "Iter-5930, train loss-0.2691, acc-0.6400, valid loss-0.2390, acc-0.7152, test loss-0.2447, acc-0.7037\n",
      "Iter-5940, train loss-0.2427, acc-0.7200, valid loss-0.2391, acc-0.7156, test loss-0.2447, acc-0.7040\n",
      "Iter-5950, train loss-0.3127, acc-0.6100, valid loss-0.2391, acc-0.7156, test loss-0.2446, acc-0.7049\n",
      "Iter-5960, train loss-0.2196, acc-0.7200, valid loss-0.2399, acc-0.7136, test loss-0.2452, acc-0.7046\n",
      "Iter-5970, train loss-0.2622, acc-0.6700, valid loss-0.2398, acc-0.7160, test loss-0.2452, acc-0.7064\n",
      "Iter-5980, train loss-0.2632, acc-0.6800, valid loss-0.2398, acc-0.7162, test loss-0.2450, acc-0.7066\n",
      "Iter-5990, train loss-0.2219, acc-0.7700, valid loss-0.2402, acc-0.7146, test loss-0.2453, acc-0.7061\n",
      "Iter-6000, train loss-0.2558, acc-0.7000, valid loss-0.2399, acc-0.7154, test loss-0.2451, acc-0.7062\n",
      "Iter-6010, train loss-0.3165, acc-0.6300, valid loss-0.2403, acc-0.7138, test loss-0.2454, acc-0.7055\n",
      "Iter-6020, train loss-0.2485, acc-0.6800, valid loss-0.2402, acc-0.7148, test loss-0.2455, acc-0.7065\n",
      "Iter-6030, train loss-0.2612, acc-0.6500, valid loss-0.2403, acc-0.7138, test loss-0.2456, acc-0.7049\n",
      "Iter-6040, train loss-0.2128, acc-0.7600, valid loss-0.2412, acc-0.7106, test loss-0.2463, acc-0.7014\n",
      "Iter-6050, train loss-0.2597, acc-0.6900, valid loss-0.2413, acc-0.7102, test loss-0.2465, acc-0.7017\n",
      "Iter-6060, train loss-0.2584, acc-0.6900, valid loss-0.2416, acc-0.7088, test loss-0.2469, acc-0.6994\n",
      "Iter-6070, train loss-0.2396, acc-0.7200, valid loss-0.2411, acc-0.7120, test loss-0.2464, acc-0.7029\n",
      "Iter-6080, train loss-0.2437, acc-0.7000, valid loss-0.2409, acc-0.7110, test loss-0.2463, acc-0.7014\n",
      "Iter-6090, train loss-0.2382, acc-0.7200, valid loss-0.2412, acc-0.7118, test loss-0.2464, acc-0.7014\n",
      "Iter-6100, train loss-0.2344, acc-0.7300, valid loss-0.2413, acc-0.7098, test loss-0.2465, acc-0.6996\n",
      "Iter-6110, train loss-0.2541, acc-0.6900, valid loss-0.2415, acc-0.7098, test loss-0.2467, acc-0.6991\n",
      "Iter-6120, train loss-0.2350, acc-0.7600, valid loss-0.2415, acc-0.7088, test loss-0.2467, acc-0.6993\n",
      "Iter-6130, train loss-0.2382, acc-0.7300, valid loss-0.2418, acc-0.7060, test loss-0.2470, acc-0.6966\n",
      "Iter-6140, train loss-0.2317, acc-0.7200, valid loss-0.2416, acc-0.7064, test loss-0.2467, acc-0.6980\n",
      "Iter-6150, train loss-0.2414, acc-0.7100, valid loss-0.2413, acc-0.7074, test loss-0.2465, acc-0.6977\n",
      "Iter-6160, train loss-0.2664, acc-0.6700, valid loss-0.2414, acc-0.7084, test loss-0.2466, acc-0.6970\n",
      "Iter-6170, train loss-0.2403, acc-0.7000, valid loss-0.2409, acc-0.7092, test loss-0.2460, acc-0.6993\n",
      "Iter-6180, train loss-0.2709, acc-0.6500, valid loss-0.2407, acc-0.7090, test loss-0.2458, acc-0.6993\n",
      "Iter-6190, train loss-0.2512, acc-0.7500, valid loss-0.2409, acc-0.7064, test loss-0.2460, acc-0.6982\n",
      "Iter-6200, train loss-0.2290, acc-0.6900, valid loss-0.2405, acc-0.7080, test loss-0.2457, acc-0.6976\n",
      "Iter-6210, train loss-0.2191, acc-0.7500, valid loss-0.2404, acc-0.7082, test loss-0.2455, acc-0.6984\n",
      "Iter-6220, train loss-0.2339, acc-0.6900, valid loss-0.2405, acc-0.7070, test loss-0.2458, acc-0.6993\n",
      "Iter-6230, train loss-0.2374, acc-0.7200, valid loss-0.2409, acc-0.7068, test loss-0.2461, acc-0.7002\n",
      "Iter-6240, train loss-0.2568, acc-0.6500, valid loss-0.2412, acc-0.7048, test loss-0.2465, acc-0.6983\n",
      "Iter-6250, train loss-0.2641, acc-0.6200, valid loss-0.2410, acc-0.7062, test loss-0.2463, acc-0.7003\n",
      "Iter-6260, train loss-0.2735, acc-0.6300, valid loss-0.2409, acc-0.7060, test loss-0.2462, acc-0.6985\n",
      "Iter-6270, train loss-0.2508, acc-0.7000, valid loss-0.2413, acc-0.7052, test loss-0.2466, acc-0.6960\n",
      "Iter-6280, train loss-0.2294, acc-0.7500, valid loss-0.2415, acc-0.7054, test loss-0.2468, acc-0.6966\n",
      "Iter-6290, train loss-0.2333, acc-0.7000, valid loss-0.2415, acc-0.7058, test loss-0.2468, acc-0.6978\n",
      "Iter-6300, train loss-0.2632, acc-0.7000, valid loss-0.2416, acc-0.7064, test loss-0.2469, acc-0.6964\n",
      "Iter-6310, train loss-0.2021, acc-0.7400, valid loss-0.2411, acc-0.7062, test loss-0.2465, acc-0.7004\n",
      "Iter-6320, train loss-0.2542, acc-0.6700, valid loss-0.2414, acc-0.7060, test loss-0.2468, acc-0.7004\n",
      "Iter-6330, train loss-0.2605, acc-0.6800, valid loss-0.2412, acc-0.7072, test loss-0.2465, acc-0.7015\n",
      "Iter-6340, train loss-0.2896, acc-0.6600, valid loss-0.2410, acc-0.7076, test loss-0.2464, acc-0.7027\n",
      "Iter-6350, train loss-0.2884, acc-0.6400, valid loss-0.2412, acc-0.7084, test loss-0.2467, acc-0.7028\n",
      "Iter-6360, train loss-0.2515, acc-0.6500, valid loss-0.2411, acc-0.7106, test loss-0.2466, acc-0.7016\n",
      "Iter-6370, train loss-0.2409, acc-0.7500, valid loss-0.2409, acc-0.7072, test loss-0.2465, acc-0.7008\n",
      "Iter-6380, train loss-0.2506, acc-0.7000, valid loss-0.2408, acc-0.7078, test loss-0.2466, acc-0.7008\n",
      "Iter-6390, train loss-0.2869, acc-0.6200, valid loss-0.2407, acc-0.7078, test loss-0.2464, acc-0.7001\n",
      "Iter-6400, train loss-0.2224, acc-0.7700, valid loss-0.2409, acc-0.7060, test loss-0.2466, acc-0.7007\n",
      "Iter-6410, train loss-0.2587, acc-0.7100, valid loss-0.2411, acc-0.7050, test loss-0.2469, acc-0.6998\n",
      "Iter-6420, train loss-0.2332, acc-0.7100, valid loss-0.2411, acc-0.7080, test loss-0.2468, acc-0.6996\n",
      "Iter-6430, train loss-0.2675, acc-0.6500, valid loss-0.2404, acc-0.7082, test loss-0.2462, acc-0.6983\n",
      "Iter-6440, train loss-0.2398, acc-0.7200, valid loss-0.2409, acc-0.7066, test loss-0.2467, acc-0.6984\n",
      "Iter-6450, train loss-0.2409, acc-0.7100, valid loss-0.2409, acc-0.7068, test loss-0.2467, acc-0.6975\n",
      "Iter-6460, train loss-0.2653, acc-0.6700, valid loss-0.2408, acc-0.7070, test loss-0.2467, acc-0.6966\n",
      "Iter-6470, train loss-0.2498, acc-0.6500, valid loss-0.2410, acc-0.7070, test loss-0.2467, acc-0.6970\n",
      "Iter-6480, train loss-0.2339, acc-0.7300, valid loss-0.2408, acc-0.7072, test loss-0.2465, acc-0.6971\n",
      "Iter-6490, train loss-0.2290, acc-0.7200, valid loss-0.2411, acc-0.7062, test loss-0.2467, acc-0.6969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-0.2645, acc-0.6800, valid loss-0.2409, acc-0.7058, test loss-0.2466, acc-0.6966\n",
      "Iter-6510, train loss-0.2501, acc-0.7200, valid loss-0.2411, acc-0.7044, test loss-0.2468, acc-0.6969\n",
      "Iter-6520, train loss-0.2320, acc-0.7300, valid loss-0.2414, acc-0.7036, test loss-0.2471, acc-0.6954\n",
      "Iter-6530, train loss-0.2415, acc-0.7000, valid loss-0.2410, acc-0.7042, test loss-0.2468, acc-0.6974\n",
      "Iter-6540, train loss-0.2603, acc-0.6500, valid loss-0.2408, acc-0.7052, test loss-0.2466, acc-0.6959\n",
      "Iter-6550, train loss-0.2454, acc-0.6600, valid loss-0.2409, acc-0.7052, test loss-0.2467, acc-0.6955\n",
      "Iter-6560, train loss-0.2545, acc-0.7100, valid loss-0.2409, acc-0.7032, test loss-0.2467, acc-0.6960\n",
      "Iter-6570, train loss-0.2143, acc-0.7600, valid loss-0.2414, acc-0.7038, test loss-0.2471, acc-0.6956\n",
      "Iter-6580, train loss-0.2625, acc-0.6800, valid loss-0.2411, acc-0.7038, test loss-0.2469, acc-0.6960\n",
      "Iter-6590, train loss-0.2228, acc-0.7000, valid loss-0.2414, acc-0.7026, test loss-0.2470, acc-0.6953\n",
      "Iter-6600, train loss-0.2769, acc-0.6400, valid loss-0.2413, acc-0.7002, test loss-0.2469, acc-0.6961\n",
      "Iter-6610, train loss-0.2380, acc-0.7100, valid loss-0.2410, acc-0.7026, test loss-0.2467, acc-0.6951\n",
      "Iter-6620, train loss-0.2713, acc-0.7100, valid loss-0.2413, acc-0.6998, test loss-0.2469, acc-0.6958\n",
      "Iter-6630, train loss-0.2644, acc-0.6300, valid loss-0.2413, acc-0.7008, test loss-0.2468, acc-0.6956\n",
      "Iter-6640, train loss-0.2126, acc-0.7500, valid loss-0.2415, acc-0.6996, test loss-0.2471, acc-0.6943\n",
      "Iter-6650, train loss-0.2217, acc-0.7200, valid loss-0.2418, acc-0.6978, test loss-0.2472, acc-0.6942\n",
      "Iter-6660, train loss-0.2856, acc-0.5900, valid loss-0.2418, acc-0.6982, test loss-0.2474, acc-0.6956\n",
      "Iter-6670, train loss-0.2468, acc-0.7000, valid loss-0.2421, acc-0.6974, test loss-0.2476, acc-0.6957\n",
      "Iter-6680, train loss-0.2612, acc-0.7000, valid loss-0.2426, acc-0.6942, test loss-0.2481, acc-0.6948\n",
      "Iter-6690, train loss-0.2377, acc-0.7100, valid loss-0.2426, acc-0.6966, test loss-0.2480, acc-0.6950\n",
      "Iter-6700, train loss-0.2133, acc-0.7300, valid loss-0.2428, acc-0.6950, test loss-0.2482, acc-0.6940\n",
      "Iter-6710, train loss-0.2423, acc-0.7300, valid loss-0.2428, acc-0.6950, test loss-0.2483, acc-0.6928\n",
      "Iter-6720, train loss-0.2407, acc-0.6800, valid loss-0.2424, acc-0.6956, test loss-0.2479, acc-0.6938\n",
      "Iter-6730, train loss-0.2484, acc-0.7000, valid loss-0.2422, acc-0.6956, test loss-0.2479, acc-0.6933\n",
      "Iter-6740, train loss-0.2552, acc-0.6600, valid loss-0.2429, acc-0.6954, test loss-0.2485, acc-0.6926\n",
      "Iter-6750, train loss-0.2872, acc-0.6300, valid loss-0.2429, acc-0.6954, test loss-0.2487, acc-0.6928\n",
      "Iter-6760, train loss-0.2570, acc-0.6500, valid loss-0.2429, acc-0.6948, test loss-0.2487, acc-0.6935\n",
      "Iter-6770, train loss-0.2504, acc-0.6400, valid loss-0.2429, acc-0.6950, test loss-0.2487, acc-0.6927\n",
      "Iter-6780, train loss-0.2579, acc-0.6700, valid loss-0.2431, acc-0.6948, test loss-0.2489, acc-0.6929\n",
      "Iter-6790, train loss-0.2421, acc-0.6600, valid loss-0.2437, acc-0.6936, test loss-0.2496, acc-0.6926\n",
      "Iter-6800, train loss-0.2272, acc-0.6900, valid loss-0.2433, acc-0.6940, test loss-0.2492, acc-0.6939\n",
      "Iter-6810, train loss-0.2539, acc-0.7000, valid loss-0.2430, acc-0.6938, test loss-0.2488, acc-0.6927\n",
      "Iter-6820, train loss-0.2609, acc-0.7100, valid loss-0.2434, acc-0.6942, test loss-0.2491, acc-0.6934\n",
      "Iter-6830, train loss-0.2537, acc-0.6800, valid loss-0.2439, acc-0.6922, test loss-0.2496, acc-0.6930\n",
      "Iter-6840, train loss-0.2095, acc-0.7800, valid loss-0.2437, acc-0.6938, test loss-0.2495, acc-0.6927\n",
      "Iter-6850, train loss-0.2669, acc-0.6900, valid loss-0.2432, acc-0.6944, test loss-0.2492, acc-0.6919\n",
      "Iter-6860, train loss-0.3144, acc-0.5600, valid loss-0.2433, acc-0.6958, test loss-0.2495, acc-0.6922\n",
      "Iter-6870, train loss-0.2570, acc-0.7000, valid loss-0.2437, acc-0.6936, test loss-0.2497, acc-0.6916\n",
      "Iter-6880, train loss-0.2131, acc-0.7600, valid loss-0.2434, acc-0.6944, test loss-0.2495, acc-0.6915\n",
      "Iter-6890, train loss-0.2567, acc-0.6100, valid loss-0.2432, acc-0.6950, test loss-0.2494, acc-0.6915\n",
      "Iter-6900, train loss-0.2701, acc-0.6100, valid loss-0.2431, acc-0.6948, test loss-0.2491, acc-0.6919\n",
      "Iter-6910, train loss-0.2762, acc-0.6300, valid loss-0.2433, acc-0.6938, test loss-0.2492, acc-0.6910\n",
      "Iter-6920, train loss-0.2360, acc-0.7300, valid loss-0.2431, acc-0.6934, test loss-0.2489, acc-0.6919\n",
      "Iter-6930, train loss-0.2561, acc-0.6800, valid loss-0.2433, acc-0.6940, test loss-0.2493, acc-0.6906\n",
      "Iter-6940, train loss-0.2234, acc-0.7300, valid loss-0.2438, acc-0.6926, test loss-0.2497, acc-0.6896\n",
      "Iter-6950, train loss-0.2375, acc-0.6800, valid loss-0.2438, acc-0.6932, test loss-0.2497, acc-0.6903\n",
      "Iter-6960, train loss-0.2905, acc-0.5900, valid loss-0.2441, acc-0.6924, test loss-0.2500, acc-0.6896\n",
      "Iter-6970, train loss-0.2442, acc-0.7300, valid loss-0.2439, acc-0.6936, test loss-0.2499, acc-0.6897\n",
      "Iter-6980, train loss-0.2693, acc-0.6500, valid loss-0.2441, acc-0.6932, test loss-0.2500, acc-0.6889\n",
      "Iter-6990, train loss-0.2508, acc-0.6800, valid loss-0.2439, acc-0.6934, test loss-0.2499, acc-0.6874\n",
      "Iter-7000, train loss-0.2382, acc-0.6800, valid loss-0.2441, acc-0.6938, test loss-0.2500, acc-0.6889\n",
      "Iter-7010, train loss-0.2886, acc-0.6100, valid loss-0.2436, acc-0.6946, test loss-0.2496, acc-0.6876\n",
      "Iter-7020, train loss-0.2491, acc-0.6700, valid loss-0.2437, acc-0.6938, test loss-0.2496, acc-0.6889\n",
      "Iter-7030, train loss-0.2352, acc-0.7000, valid loss-0.2434, acc-0.6948, test loss-0.2495, acc-0.6884\n",
      "Iter-7040, train loss-0.2373, acc-0.7000, valid loss-0.2427, acc-0.6944, test loss-0.2488, acc-0.6856\n",
      "Iter-7050, train loss-0.2841, acc-0.6000, valid loss-0.2427, acc-0.6926, test loss-0.2488, acc-0.6855\n",
      "Iter-7060, train loss-0.2467, acc-0.7200, valid loss-0.2428, acc-0.6940, test loss-0.2489, acc-0.6828\n",
      "Iter-7070, train loss-0.2785, acc-0.5900, valid loss-0.2426, acc-0.6922, test loss-0.2487, acc-0.6804\n",
      "Iter-7080, train loss-0.2654, acc-0.6400, valid loss-0.2431, acc-0.6922, test loss-0.2491, acc-0.6808\n",
      "Iter-7090, train loss-0.2690, acc-0.6800, valid loss-0.2430, acc-0.6914, test loss-0.2491, acc-0.6803\n",
      "Iter-7100, train loss-0.2746, acc-0.6000, valid loss-0.2433, acc-0.6920, test loss-0.2494, acc-0.6809\n",
      "Iter-7110, train loss-0.2318, acc-0.7100, valid loss-0.2434, acc-0.6896, test loss-0.2496, acc-0.6786\n",
      "Iter-7120, train loss-0.2274, acc-0.7000, valid loss-0.2438, acc-0.6906, test loss-0.2499, acc-0.6800\n",
      "Iter-7130, train loss-0.2714, acc-0.6400, valid loss-0.2435, acc-0.6906, test loss-0.2497, acc-0.6777\n",
      "Iter-7140, train loss-0.2545, acc-0.6600, valid loss-0.2441, acc-0.6898, test loss-0.2502, acc-0.6790\n",
      "Iter-7150, train loss-0.2700, acc-0.6300, valid loss-0.2441, acc-0.6902, test loss-0.2502, acc-0.6786\n",
      "Iter-7160, train loss-0.2524, acc-0.6600, valid loss-0.2444, acc-0.6894, test loss-0.2505, acc-0.6767\n",
      "Iter-7170, train loss-0.2648, acc-0.6000, valid loss-0.2445, acc-0.6880, test loss-0.2505, acc-0.6766\n",
      "Iter-7180, train loss-0.2455, acc-0.6700, valid loss-0.2445, acc-0.6868, test loss-0.2504, acc-0.6765\n",
      "Iter-7190, train loss-0.2576, acc-0.6600, valid loss-0.2447, acc-0.6858, test loss-0.2506, acc-0.6747\n",
      "Iter-7200, train loss-0.2696, acc-0.6500, valid loss-0.2445, acc-0.6882, test loss-0.2504, acc-0.6751\n",
      "Iter-7210, train loss-0.2783, acc-0.6400, valid loss-0.2449, acc-0.6838, test loss-0.2508, acc-0.6755\n",
      "Iter-7220, train loss-0.2822, acc-0.6100, valid loss-0.2449, acc-0.6852, test loss-0.2508, acc-0.6743\n",
      "Iter-7230, train loss-0.2399, acc-0.7200, valid loss-0.2447, acc-0.6846, test loss-0.2507, acc-0.6720\n",
      "Iter-7240, train loss-0.2158, acc-0.7300, valid loss-0.2451, acc-0.6832, test loss-0.2510, acc-0.6721\n",
      "Iter-7250, train loss-0.2337, acc-0.7000, valid loss-0.2449, acc-0.6848, test loss-0.2508, acc-0.6717\n",
      "Iter-7260, train loss-0.2869, acc-0.6300, valid loss-0.2445, acc-0.6848, test loss-0.2505, acc-0.6719\n",
      "Iter-7270, train loss-0.2692, acc-0.6700, valid loss-0.2446, acc-0.6856, test loss-0.2507, acc-0.6708\n",
      "Iter-7280, train loss-0.2367, acc-0.6900, valid loss-0.2446, acc-0.6862, test loss-0.2507, acc-0.6700\n",
      "Iter-7290, train loss-0.2527, acc-0.6600, valid loss-0.2448, acc-0.6868, test loss-0.2510, acc-0.6710\n",
      "Iter-7300, train loss-0.2512, acc-0.6300, valid loss-0.2450, acc-0.6868, test loss-0.2512, acc-0.6706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-0.2308, acc-0.6800, valid loss-0.2454, acc-0.6838, test loss-0.2516, acc-0.6685\n",
      "Iter-7320, train loss-0.2579, acc-0.6700, valid loss-0.2456, acc-0.6858, test loss-0.2518, acc-0.6691\n",
      "Iter-7330, train loss-0.2376, acc-0.6900, valid loss-0.2455, acc-0.6854, test loss-0.2518, acc-0.6683\n",
      "Iter-7340, train loss-0.2366, acc-0.6800, valid loss-0.2459, acc-0.6824, test loss-0.2522, acc-0.6681\n",
      "Iter-7350, train loss-0.2694, acc-0.6000, valid loss-0.2459, acc-0.6836, test loss-0.2522, acc-0.6688\n",
      "Iter-7360, train loss-0.2353, acc-0.6600, valid loss-0.2463, acc-0.6838, test loss-0.2524, acc-0.6687\n",
      "Iter-7370, train loss-0.2354, acc-0.6700, valid loss-0.2465, acc-0.6828, test loss-0.2528, acc-0.6676\n",
      "Iter-7380, train loss-0.2564, acc-0.6300, valid loss-0.2467, acc-0.6838, test loss-0.2529, acc-0.6687\n",
      "Iter-7390, train loss-0.2718, acc-0.6800, valid loss-0.2467, acc-0.6802, test loss-0.2529, acc-0.6683\n",
      "Iter-7400, train loss-0.2828, acc-0.6000, valid loss-0.2473, acc-0.6782, test loss-0.2533, acc-0.6658\n",
      "Iter-7410, train loss-0.2510, acc-0.6400, valid loss-0.2476, acc-0.6794, test loss-0.2536, acc-0.6654\n",
      "Iter-7420, train loss-0.2880, acc-0.6100, valid loss-0.2473, acc-0.6800, test loss-0.2534, acc-0.6662\n",
      "Iter-7430, train loss-0.2827, acc-0.6000, valid loss-0.2482, acc-0.6788, test loss-0.2542, acc-0.6637\n",
      "Iter-7440, train loss-0.2466, acc-0.6500, valid loss-0.2485, acc-0.6786, test loss-0.2546, acc-0.6630\n",
      "Iter-7450, train loss-0.2003, acc-0.7900, valid loss-0.2488, acc-0.6766, test loss-0.2550, acc-0.6620\n",
      "Iter-7460, train loss-0.2694, acc-0.6300, valid loss-0.2489, acc-0.6778, test loss-0.2552, acc-0.6614\n",
      "Iter-7470, train loss-0.2165, acc-0.7400, valid loss-0.2491, acc-0.6750, test loss-0.2554, acc-0.6592\n",
      "Iter-7480, train loss-0.2534, acc-0.6400, valid loss-0.2489, acc-0.6774, test loss-0.2552, acc-0.6591\n",
      "Iter-7490, train loss-0.2565, acc-0.6300, valid loss-0.2489, acc-0.6764, test loss-0.2553, acc-0.6602\n",
      "Iter-7500, train loss-0.2846, acc-0.5900, valid loss-0.2489, acc-0.6778, test loss-0.2554, acc-0.6604\n",
      "Iter-7510, train loss-0.2645, acc-0.6700, valid loss-0.2489, acc-0.6762, test loss-0.2556, acc-0.6593\n",
      "Iter-7520, train loss-0.2921, acc-0.6100, valid loss-0.2493, acc-0.6756, test loss-0.2560, acc-0.6597\n",
      "Iter-7530, train loss-0.2795, acc-0.5800, valid loss-0.2495, acc-0.6774, test loss-0.2563, acc-0.6580\n",
      "Iter-7540, train loss-0.2647, acc-0.6000, valid loss-0.2493, acc-0.6768, test loss-0.2562, acc-0.6599\n",
      "Iter-7550, train loss-0.2798, acc-0.6100, valid loss-0.2496, acc-0.6752, test loss-0.2564, acc-0.6594\n",
      "Iter-7560, train loss-0.2463, acc-0.6900, valid loss-0.2496, acc-0.6750, test loss-0.2565, acc-0.6579\n",
      "Iter-7570, train loss-0.2835, acc-0.6400, valid loss-0.2495, acc-0.6754, test loss-0.2565, acc-0.6601\n",
      "Iter-7580, train loss-0.2638, acc-0.6900, valid loss-0.2493, acc-0.6762, test loss-0.2563, acc-0.6584\n",
      "Iter-7590, train loss-0.2788, acc-0.6300, valid loss-0.2493, acc-0.6778, test loss-0.2563, acc-0.6596\n",
      "Iter-7600, train loss-0.2474, acc-0.6800, valid loss-0.2496, acc-0.6764, test loss-0.2566, acc-0.6597\n",
      "Iter-7610, train loss-0.2470, acc-0.6700, valid loss-0.2500, acc-0.6756, test loss-0.2571, acc-0.6607\n",
      "Iter-7620, train loss-0.2329, acc-0.7000, valid loss-0.2500, acc-0.6758, test loss-0.2572, acc-0.6597\n",
      "Iter-7630, train loss-0.2864, acc-0.6500, valid loss-0.2495, acc-0.6778, test loss-0.2568, acc-0.6586\n",
      "Iter-7640, train loss-0.2858, acc-0.5900, valid loss-0.2499, acc-0.6756, test loss-0.2571, acc-0.6578\n",
      "Iter-7650, train loss-0.2724, acc-0.6100, valid loss-0.2496, acc-0.6770, test loss-0.2568, acc-0.6570\n",
      "Iter-7660, train loss-0.2210, acc-0.7500, valid loss-0.2495, acc-0.6774, test loss-0.2568, acc-0.6575\n",
      "Iter-7670, train loss-0.2502, acc-0.6400, valid loss-0.2493, acc-0.6772, test loss-0.2566, acc-0.6569\n",
      "Iter-7680, train loss-0.2744, acc-0.6400, valid loss-0.2488, acc-0.6782, test loss-0.2560, acc-0.6585\n",
      "Iter-7690, train loss-0.2628, acc-0.6200, valid loss-0.2493, acc-0.6770, test loss-0.2567, acc-0.6570\n",
      "Iter-7700, train loss-0.2777, acc-0.6300, valid loss-0.2493, acc-0.6778, test loss-0.2567, acc-0.6582\n",
      "Iter-7710, train loss-0.2715, acc-0.5900, valid loss-0.2496, acc-0.6772, test loss-0.2570, acc-0.6574\n",
      "Iter-7720, train loss-0.2655, acc-0.6400, valid loss-0.2496, acc-0.6762, test loss-0.2571, acc-0.6590\n",
      "Iter-7730, train loss-0.2951, acc-0.5900, valid loss-0.2499, acc-0.6762, test loss-0.2576, acc-0.6591\n",
      "Iter-7740, train loss-0.2347, acc-0.6800, valid loss-0.2502, acc-0.6762, test loss-0.2579, acc-0.6591\n",
      "Iter-7750, train loss-0.2453, acc-0.6000, valid loss-0.2498, acc-0.6768, test loss-0.2575, acc-0.6610\n",
      "Iter-7760, train loss-0.2524, acc-0.6900, valid loss-0.2499, acc-0.6750, test loss-0.2576, acc-0.6607\n",
      "Iter-7770, train loss-0.2719, acc-0.6200, valid loss-0.2502, acc-0.6746, test loss-0.2579, acc-0.6603\n",
      "Iter-7780, train loss-0.2416, acc-0.7100, valid loss-0.2505, acc-0.6738, test loss-0.2584, acc-0.6590\n",
      "Iter-7790, train loss-0.2909, acc-0.6300, valid loss-0.2499, acc-0.6744, test loss-0.2577, acc-0.6605\n",
      "Iter-7800, train loss-0.2561, acc-0.6500, valid loss-0.2506, acc-0.6732, test loss-0.2584, acc-0.6603\n",
      "Iter-7810, train loss-0.2662, acc-0.6200, valid loss-0.2505, acc-0.6742, test loss-0.2584, acc-0.6601\n",
      "Iter-7820, train loss-0.2554, acc-0.6600, valid loss-0.2505, acc-0.6724, test loss-0.2585, acc-0.6600\n",
      "Iter-7830, train loss-0.2805, acc-0.5800, valid loss-0.2504, acc-0.6726, test loss-0.2584, acc-0.6588\n",
      "Iter-7840, train loss-0.2893, acc-0.5800, valid loss-0.2504, acc-0.6716, test loss-0.2584, acc-0.6581\n",
      "Iter-7850, train loss-0.2532, acc-0.6800, valid loss-0.2499, acc-0.6746, test loss-0.2583, acc-0.6583\n",
      "Iter-7860, train loss-0.2575, acc-0.6500, valid loss-0.2502, acc-0.6728, test loss-0.2586, acc-0.6590\n",
      "Iter-7870, train loss-0.2642, acc-0.6600, valid loss-0.2497, acc-0.6744, test loss-0.2580, acc-0.6604\n",
      "Iter-7880, train loss-0.2600, acc-0.6500, valid loss-0.2495, acc-0.6752, test loss-0.2577, acc-0.6614\n",
      "Iter-7890, train loss-0.2782, acc-0.6000, valid loss-0.2489, acc-0.6764, test loss-0.2572, acc-0.6627\n",
      "Iter-7900, train loss-0.2393, acc-0.7000, valid loss-0.2490, acc-0.6766, test loss-0.2573, acc-0.6626\n",
      "Iter-7910, train loss-0.2670, acc-0.6100, valid loss-0.2488, acc-0.6748, test loss-0.2571, acc-0.6625\n",
      "Iter-7920, train loss-0.3004, acc-0.6200, valid loss-0.2486, acc-0.6754, test loss-0.2569, acc-0.6641\n",
      "Iter-7930, train loss-0.2568, acc-0.6600, valid loss-0.2491, acc-0.6772, test loss-0.2574, acc-0.6630\n",
      "Iter-7940, train loss-0.2723, acc-0.6000, valid loss-0.2490, acc-0.6778, test loss-0.2574, acc-0.6645\n",
      "Iter-7950, train loss-0.2942, acc-0.5900, valid loss-0.2490, acc-0.6782, test loss-0.2573, acc-0.6648\n",
      "Iter-7960, train loss-0.2550, acc-0.6500, valid loss-0.2491, acc-0.6774, test loss-0.2575, acc-0.6647\n",
      "Iter-7970, train loss-0.2624, acc-0.6400, valid loss-0.2497, acc-0.6766, test loss-0.2581, acc-0.6650\n",
      "Iter-7980, train loss-0.3001, acc-0.6200, valid loss-0.2499, acc-0.6768, test loss-0.2582, acc-0.6642\n",
      "Iter-7990, train loss-0.2402, acc-0.6500, valid loss-0.2503, acc-0.6772, test loss-0.2583, acc-0.6634\n",
      "Iter-8000, train loss-0.2503, acc-0.6800, valid loss-0.2503, acc-0.6772, test loss-0.2581, acc-0.6627\n",
      "Iter-8010, train loss-0.2902, acc-0.6300, valid loss-0.2504, acc-0.6774, test loss-0.2582, acc-0.6627\n",
      "Iter-8020, train loss-0.2252, acc-0.6900, valid loss-0.2505, acc-0.6778, test loss-0.2585, acc-0.6628\n",
      "Iter-8030, train loss-0.2630, acc-0.6600, valid loss-0.2504, acc-0.6794, test loss-0.2584, acc-0.6620\n",
      "Iter-8040, train loss-0.2908, acc-0.6400, valid loss-0.2499, acc-0.6788, test loss-0.2578, acc-0.6613\n",
      "Iter-8050, train loss-0.2346, acc-0.7000, valid loss-0.2500, acc-0.6780, test loss-0.2579, acc-0.6607\n",
      "Iter-8060, train loss-0.2950, acc-0.6300, valid loss-0.2503, acc-0.6782, test loss-0.2582, acc-0.6610\n",
      "Iter-8070, train loss-0.2676, acc-0.6300, valid loss-0.2500, acc-0.6780, test loss-0.2580, acc-0.6600\n",
      "Iter-8080, train loss-0.2431, acc-0.6800, valid loss-0.2500, acc-0.6776, test loss-0.2580, acc-0.6592\n",
      "Iter-8090, train loss-0.2524, acc-0.6800, valid loss-0.2496, acc-0.6778, test loss-0.2578, acc-0.6583\n",
      "Iter-8100, train loss-0.2389, acc-0.7300, valid loss-0.2498, acc-0.6772, test loss-0.2582, acc-0.6583\n",
      "Iter-8110, train loss-0.2401, acc-0.7200, valid loss-0.2498, acc-0.6744, test loss-0.2583, acc-0.6568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-0.2419, acc-0.6900, valid loss-0.2504, acc-0.6756, test loss-0.2587, acc-0.6565\n",
      "Iter-8130, train loss-0.2235, acc-0.7500, valid loss-0.2508, acc-0.6736, test loss-0.2590, acc-0.6571\n",
      "Iter-8140, train loss-0.3018, acc-0.5600, valid loss-0.2513, acc-0.6756, test loss-0.2595, acc-0.6574\n",
      "Iter-8150, train loss-0.2603, acc-0.6300, valid loss-0.2515, acc-0.6740, test loss-0.2597, acc-0.6555\n",
      "Iter-8160, train loss-0.2170, acc-0.7300, valid loss-0.2514, acc-0.6736, test loss-0.2596, acc-0.6566\n",
      "Iter-8170, train loss-0.2802, acc-0.6400, valid loss-0.2510, acc-0.6752, test loss-0.2593, acc-0.6577\n",
      "Iter-8180, train loss-0.2621, acc-0.6300, valid loss-0.2512, acc-0.6740, test loss-0.2596, acc-0.6567\n",
      "Iter-8190, train loss-0.3162, acc-0.5700, valid loss-0.2512, acc-0.6736, test loss-0.2595, acc-0.6567\n",
      "Iter-8200, train loss-0.2780, acc-0.6100, valid loss-0.2516, acc-0.6746, test loss-0.2600, acc-0.6566\n",
      "Iter-8210, train loss-0.2836, acc-0.6700, valid loss-0.2520, acc-0.6738, test loss-0.2603, acc-0.6569\n",
      "Iter-8220, train loss-0.2584, acc-0.6900, valid loss-0.2521, acc-0.6740, test loss-0.2603, acc-0.6565\n",
      "Iter-8230, train loss-0.2330, acc-0.7300, valid loss-0.2514, acc-0.6740, test loss-0.2597, acc-0.6575\n",
      "Iter-8240, train loss-0.2850, acc-0.6200, valid loss-0.2518, acc-0.6734, test loss-0.2599, acc-0.6572\n",
      "Iter-8250, train loss-0.2134, acc-0.7200, valid loss-0.2522, acc-0.6692, test loss-0.2605, acc-0.6535\n",
      "Iter-8260, train loss-0.2511, acc-0.6600, valid loss-0.2520, acc-0.6714, test loss-0.2603, acc-0.6528\n",
      "Iter-8270, train loss-0.2420, acc-0.6200, valid loss-0.2525, acc-0.6670, test loss-0.2608, acc-0.6477\n",
      "Iter-8280, train loss-0.2697, acc-0.6300, valid loss-0.2523, acc-0.6666, test loss-0.2607, acc-0.6493\n",
      "Iter-8290, train loss-0.2370, acc-0.6700, valid loss-0.2528, acc-0.6674, test loss-0.2610, acc-0.6496\n",
      "Iter-8300, train loss-0.2561, acc-0.7100, valid loss-0.2527, acc-0.6640, test loss-0.2609, acc-0.6502\n",
      "Iter-8310, train loss-0.2887, acc-0.6100, valid loss-0.2529, acc-0.6640, test loss-0.2612, acc-0.6473\n",
      "Iter-8320, train loss-0.2526, acc-0.7000, valid loss-0.2529, acc-0.6630, test loss-0.2611, acc-0.6461\n",
      "Iter-8330, train loss-0.2366, acc-0.7100, valid loss-0.2530, acc-0.6584, test loss-0.2612, acc-0.6441\n",
      "Iter-8340, train loss-0.2785, acc-0.6100, valid loss-0.2533, acc-0.6574, test loss-0.2615, acc-0.6426\n",
      "Iter-8350, train loss-0.3067, acc-0.4800, valid loss-0.2530, acc-0.6596, test loss-0.2610, acc-0.6456\n",
      "Iter-8360, train loss-0.2878, acc-0.5600, valid loss-0.2533, acc-0.6570, test loss-0.2613, acc-0.6432\n",
      "Iter-8370, train loss-0.2506, acc-0.6500, valid loss-0.2532, acc-0.6570, test loss-0.2611, acc-0.6430\n",
      "Iter-8380, train loss-0.2344, acc-0.7100, valid loss-0.2534, acc-0.6538, test loss-0.2616, acc-0.6422\n",
      "Iter-8390, train loss-0.2753, acc-0.6100, valid loss-0.2532, acc-0.6540, test loss-0.2612, acc-0.6417\n",
      "Iter-8400, train loss-0.2603, acc-0.6100, valid loss-0.2531, acc-0.6566, test loss-0.2611, acc-0.6436\n",
      "Iter-8410, train loss-0.2458, acc-0.6500, valid loss-0.2529, acc-0.6552, test loss-0.2609, acc-0.6447\n",
      "Iter-8420, train loss-0.2355, acc-0.6600, valid loss-0.2536, acc-0.6532, test loss-0.2616, acc-0.6411\n",
      "Iter-8430, train loss-0.2655, acc-0.6300, valid loss-0.2542, acc-0.6500, test loss-0.2622, acc-0.6371\n",
      "Iter-8440, train loss-0.2397, acc-0.6900, valid loss-0.2545, acc-0.6476, test loss-0.2624, acc-0.6368\n",
      "Iter-8450, train loss-0.2669, acc-0.6300, valid loss-0.2547, acc-0.6476, test loss-0.2626, acc-0.6355\n",
      "Iter-8460, train loss-0.2478, acc-0.6000, valid loss-0.2549, acc-0.6458, test loss-0.2627, acc-0.6353\n",
      "Iter-8470, train loss-0.2988, acc-0.5700, valid loss-0.2547, acc-0.6484, test loss-0.2626, acc-0.6362\n",
      "Iter-8480, train loss-0.2475, acc-0.6600, valid loss-0.2548, acc-0.6492, test loss-0.2627, acc-0.6365\n",
      "Iter-8490, train loss-0.2374, acc-0.6700, valid loss-0.2543, acc-0.6510, test loss-0.2622, acc-0.6381\n",
      "Iter-8500, train loss-0.2831, acc-0.5900, valid loss-0.2548, acc-0.6486, test loss-0.2626, acc-0.6360\n",
      "Iter-8510, train loss-0.2582, acc-0.6900, valid loss-0.2545, acc-0.6492, test loss-0.2624, acc-0.6357\n",
      "Iter-8520, train loss-0.2909, acc-0.5900, valid loss-0.2543, acc-0.6488, test loss-0.2624, acc-0.6358\n",
      "Iter-8530, train loss-0.2492, acc-0.6700, valid loss-0.2541, acc-0.6518, test loss-0.2621, acc-0.6365\n",
      "Iter-8540, train loss-0.2746, acc-0.6700, valid loss-0.2547, acc-0.6482, test loss-0.2628, acc-0.6348\n",
      "Iter-8550, train loss-0.2950, acc-0.5400, valid loss-0.2545, acc-0.6492, test loss-0.2626, acc-0.6354\n",
      "Iter-8560, train loss-0.2977, acc-0.5400, valid loss-0.2546, acc-0.6496, test loss-0.2626, acc-0.6356\n",
      "Iter-8570, train loss-0.2497, acc-0.6800, valid loss-0.2542, acc-0.6522, test loss-0.2625, acc-0.6381\n",
      "Iter-8580, train loss-0.2647, acc-0.6000, valid loss-0.2542, acc-0.6504, test loss-0.2624, acc-0.6374\n",
      "Iter-8590, train loss-0.2330, acc-0.6800, valid loss-0.2548, acc-0.6490, test loss-0.2629, acc-0.6349\n",
      "Iter-8600, train loss-0.2162, acc-0.7600, valid loss-0.2548, acc-0.6486, test loss-0.2630, acc-0.6353\n",
      "Iter-8610, train loss-0.2618, acc-0.6300, valid loss-0.2543, acc-0.6466, test loss-0.2625, acc-0.6366\n",
      "Iter-8620, train loss-0.2537, acc-0.6500, valid loss-0.2548, acc-0.6426, test loss-0.2629, acc-0.6352\n",
      "Iter-8630, train loss-0.2029, acc-0.7600, valid loss-0.2550, acc-0.6420, test loss-0.2631, acc-0.6343\n",
      "Iter-8640, train loss-0.3019, acc-0.5500, valid loss-0.2544, acc-0.6476, test loss-0.2626, acc-0.6388\n",
      "Iter-8650, train loss-0.2432, acc-0.6900, valid loss-0.2541, acc-0.6510, test loss-0.2622, acc-0.6406\n",
      "Iter-8660, train loss-0.2403, acc-0.6700, valid loss-0.2544, acc-0.6494, test loss-0.2626, acc-0.6396\n",
      "Iter-8670, train loss-0.2681, acc-0.5900, valid loss-0.2544, acc-0.6484, test loss-0.2627, acc-0.6363\n",
      "Iter-8680, train loss-0.2807, acc-0.6300, valid loss-0.2544, acc-0.6516, test loss-0.2625, acc-0.6400\n",
      "Iter-8690, train loss-0.2417, acc-0.6800, valid loss-0.2544, acc-0.6488, test loss-0.2626, acc-0.6379\n",
      "Iter-8700, train loss-0.3032, acc-0.5500, valid loss-0.2541, acc-0.6520, test loss-0.2624, acc-0.6412\n",
      "Iter-8710, train loss-0.2951, acc-0.5500, valid loss-0.2539, acc-0.6544, test loss-0.2621, acc-0.6429\n",
      "Iter-8720, train loss-0.2399, acc-0.6500, valid loss-0.2540, acc-0.6554, test loss-0.2622, acc-0.6427\n",
      "Iter-8730, train loss-0.2650, acc-0.5800, valid loss-0.2537, acc-0.6584, test loss-0.2621, acc-0.6436\n",
      "Iter-8740, train loss-0.2824, acc-0.6100, valid loss-0.2541, acc-0.6572, test loss-0.2624, acc-0.6428\n",
      "Iter-8750, train loss-0.2900, acc-0.5500, valid loss-0.2544, acc-0.6570, test loss-0.2627, acc-0.6410\n",
      "Iter-8760, train loss-0.2837, acc-0.6100, valid loss-0.2542, acc-0.6560, test loss-0.2626, acc-0.6408\n",
      "Iter-8770, train loss-0.2484, acc-0.7000, valid loss-0.2539, acc-0.6578, test loss-0.2624, acc-0.6424\n",
      "Iter-8780, train loss-0.2755, acc-0.6000, valid loss-0.2535, acc-0.6624, test loss-0.2622, acc-0.6482\n",
      "Iter-8790, train loss-0.2278, acc-0.6900, valid loss-0.2535, acc-0.6658, test loss-0.2622, acc-0.6504\n",
      "Iter-8800, train loss-0.2801, acc-0.6200, valid loss-0.2533, acc-0.6632, test loss-0.2621, acc-0.6500\n",
      "Iter-8810, train loss-0.2938, acc-0.6400, valid loss-0.2532, acc-0.6682, test loss-0.2621, acc-0.6544\n",
      "Iter-8820, train loss-0.2455, acc-0.6700, valid loss-0.2533, acc-0.6682, test loss-0.2621, acc-0.6538\n",
      "Iter-8830, train loss-0.2532, acc-0.6400, valid loss-0.2531, acc-0.6712, test loss-0.2619, acc-0.6553\n",
      "Iter-8840, train loss-0.2545, acc-0.6900, valid loss-0.2528, acc-0.6726, test loss-0.2618, acc-0.6560\n",
      "Iter-8850, train loss-0.2210, acc-0.6900, valid loss-0.2529, acc-0.6744, test loss-0.2617, acc-0.6581\n",
      "Iter-8860, train loss-0.2249, acc-0.7200, valid loss-0.2531, acc-0.6716, test loss-0.2619, acc-0.6558\n",
      "Iter-8870, train loss-0.2530, acc-0.6900, valid loss-0.2527, acc-0.6730, test loss-0.2615, acc-0.6578\n",
      "Iter-8880, train loss-0.2218, acc-0.7700, valid loss-0.2527, acc-0.6708, test loss-0.2615, acc-0.6575\n",
      "Iter-8890, train loss-0.2518, acc-0.6900, valid loss-0.2525, acc-0.6732, test loss-0.2612, acc-0.6577\n",
      "Iter-8900, train loss-0.2746, acc-0.6100, valid loss-0.2525, acc-0.6680, test loss-0.2612, acc-0.6555\n",
      "Iter-8910, train loss-0.2442, acc-0.6800, valid loss-0.2525, acc-0.6688, test loss-0.2612, acc-0.6552\n",
      "Iter-8920, train loss-0.3089, acc-0.6200, valid loss-0.2520, acc-0.6684, test loss-0.2608, acc-0.6546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-0.3077, acc-0.5900, valid loss-0.2519, acc-0.6714, test loss-0.2608, acc-0.6569\n",
      "Iter-8940, train loss-0.2766, acc-0.6700, valid loss-0.2520, acc-0.6700, test loss-0.2610, acc-0.6564\n",
      "Iter-8950, train loss-0.2534, acc-0.6800, valid loss-0.2520, acc-0.6718, test loss-0.2610, acc-0.6571\n",
      "Iter-8960, train loss-0.3050, acc-0.6000, valid loss-0.2525, acc-0.6732, test loss-0.2613, acc-0.6568\n",
      "Iter-8970, train loss-0.2394, acc-0.6900, valid loss-0.2528, acc-0.6696, test loss-0.2616, acc-0.6540\n",
      "Iter-8980, train loss-0.2356, acc-0.7000, valid loss-0.2525, acc-0.6726, test loss-0.2614, acc-0.6555\n",
      "Iter-8990, train loss-0.2490, acc-0.6700, valid loss-0.2525, acc-0.6730, test loss-0.2613, acc-0.6554\n",
      "Iter-9000, train loss-0.2593, acc-0.6400, valid loss-0.2528, acc-0.6694, test loss-0.2616, acc-0.6523\n",
      "Iter-9010, train loss-0.3322, acc-0.6000, valid loss-0.2526, acc-0.6720, test loss-0.2613, acc-0.6549\n",
      "Iter-9020, train loss-0.2274, acc-0.6800, valid loss-0.2528, acc-0.6646, test loss-0.2617, acc-0.6459\n",
      "Iter-9030, train loss-0.2483, acc-0.6600, valid loss-0.2526, acc-0.6728, test loss-0.2617, acc-0.6527\n",
      "Iter-9040, train loss-0.2587, acc-0.6400, valid loss-0.2529, acc-0.6676, test loss-0.2620, acc-0.6481\n",
      "Iter-9050, train loss-0.2353, acc-0.7000, valid loss-0.2526, acc-0.6662, test loss-0.2619, acc-0.6470\n",
      "Iter-9060, train loss-0.3072, acc-0.5900, valid loss-0.2526, acc-0.6708, test loss-0.2618, acc-0.6519\n",
      "Iter-9070, train loss-0.2360, acc-0.6900, valid loss-0.2529, acc-0.6610, test loss-0.2621, acc-0.6431\n",
      "Iter-9080, train loss-0.2982, acc-0.5300, valid loss-0.2528, acc-0.6626, test loss-0.2621, acc-0.6456\n",
      "Iter-9090, train loss-0.2505, acc-0.6200, valid loss-0.2527, acc-0.6698, test loss-0.2620, acc-0.6520\n",
      "Iter-9100, train loss-0.2524, acc-0.6400, valid loss-0.2525, acc-0.6732, test loss-0.2618, acc-0.6551\n",
      "Iter-9110, train loss-0.2421, acc-0.7500, valid loss-0.2524, acc-0.6768, test loss-0.2619, acc-0.6593\n",
      "Iter-9120, train loss-0.2297, acc-0.7500, valid loss-0.2523, acc-0.6748, test loss-0.2617, acc-0.6554\n",
      "Iter-9130, train loss-0.2488, acc-0.6600, valid loss-0.2522, acc-0.6738, test loss-0.2616, acc-0.6552\n",
      "Iter-9140, train loss-0.2709, acc-0.6400, valid loss-0.2522, acc-0.6752, test loss-0.2616, acc-0.6543\n",
      "Iter-9150, train loss-0.2396, acc-0.7100, valid loss-0.2524, acc-0.6746, test loss-0.2617, acc-0.6570\n",
      "Iter-9160, train loss-0.2898, acc-0.6100, valid loss-0.2524, acc-0.6750, test loss-0.2617, acc-0.6571\n",
      "Iter-9170, train loss-0.2592, acc-0.6500, valid loss-0.2521, acc-0.6780, test loss-0.2614, acc-0.6591\n",
      "Iter-9180, train loss-0.3017, acc-0.6000, valid loss-0.2522, acc-0.6738, test loss-0.2615, acc-0.6573\n",
      "Iter-9190, train loss-0.2577, acc-0.6100, valid loss-0.2520, acc-0.6746, test loss-0.2612, acc-0.6579\n",
      "Iter-9200, train loss-0.2960, acc-0.5800, valid loss-0.2518, acc-0.6734, test loss-0.2610, acc-0.6570\n",
      "Iter-9210, train loss-0.2930, acc-0.6400, valid loss-0.2517, acc-0.6760, test loss-0.2610, acc-0.6545\n",
      "Iter-9220, train loss-0.2607, acc-0.6700, valid loss-0.2521, acc-0.6746, test loss-0.2612, acc-0.6565\n",
      "Iter-9230, train loss-0.2659, acc-0.6600, valid loss-0.2517, acc-0.6770, test loss-0.2610, acc-0.6576\n",
      "Iter-9240, train loss-0.2247, acc-0.7200, valid loss-0.2516, acc-0.6768, test loss-0.2609, acc-0.6570\n",
      "Iter-9250, train loss-0.2332, acc-0.6900, valid loss-0.2517, acc-0.6756, test loss-0.2610, acc-0.6564\n",
      "Iter-9260, train loss-0.2490, acc-0.6700, valid loss-0.2515, acc-0.6764, test loss-0.2608, acc-0.6522\n",
      "Iter-9270, train loss-0.2661, acc-0.7100, valid loss-0.2512, acc-0.6738, test loss-0.2608, acc-0.6522\n",
      "Iter-9280, train loss-0.2271, acc-0.7200, valid loss-0.2518, acc-0.6696, test loss-0.2612, acc-0.6484\n",
      "Iter-9290, train loss-0.2607, acc-0.6600, valid loss-0.2517, acc-0.6710, test loss-0.2611, acc-0.6523\n",
      "Iter-9300, train loss-0.2714, acc-0.6100, valid loss-0.2515, acc-0.6718, test loss-0.2610, acc-0.6510\n",
      "Iter-9310, train loss-0.2752, acc-0.6100, valid loss-0.2514, acc-0.6720, test loss-0.2609, acc-0.6511\n",
      "Iter-9320, train loss-0.2777, acc-0.6500, valid loss-0.2515, acc-0.6720, test loss-0.2610, acc-0.6473\n",
      "Iter-9330, train loss-0.2743, acc-0.6600, valid loss-0.2515, acc-0.6714, test loss-0.2611, acc-0.6490\n",
      "Iter-9340, train loss-0.2980, acc-0.6200, valid loss-0.2514, acc-0.6716, test loss-0.2608, acc-0.6477\n",
      "Iter-9350, train loss-0.2891, acc-0.5900, valid loss-0.2516, acc-0.6690, test loss-0.2609, acc-0.6497\n",
      "Iter-9360, train loss-0.2705, acc-0.6600, valid loss-0.2510, acc-0.6698, test loss-0.2605, acc-0.6470\n",
      "Iter-9370, train loss-0.2644, acc-0.6500, valid loss-0.2510, acc-0.6696, test loss-0.2605, acc-0.6477\n",
      "Iter-9380, train loss-0.2542, acc-0.6600, valid loss-0.2510, acc-0.6700, test loss-0.2605, acc-0.6478\n",
      "Iter-9390, train loss-0.2424, acc-0.7200, valid loss-0.2506, acc-0.6696, test loss-0.2602, acc-0.6479\n",
      "Iter-9400, train loss-0.2632, acc-0.6600, valid loss-0.2505, acc-0.6696, test loss-0.2601, acc-0.6470\n",
      "Iter-9410, train loss-0.2810, acc-0.6100, valid loss-0.2504, acc-0.6710, test loss-0.2599, acc-0.6454\n",
      "Iter-9420, train loss-0.2784, acc-0.6700, valid loss-0.2501, acc-0.6710, test loss-0.2597, acc-0.6465\n",
      "Iter-9430, train loss-0.2515, acc-0.7300, valid loss-0.2503, acc-0.6692, test loss-0.2598, acc-0.6454\n",
      "Iter-9440, train loss-0.2569, acc-0.6400, valid loss-0.2504, acc-0.6640, test loss-0.2598, acc-0.6446\n",
      "Iter-9450, train loss-0.2735, acc-0.6100, valid loss-0.2502, acc-0.6654, test loss-0.2596, acc-0.6464\n",
      "Iter-9460, train loss-0.2605, acc-0.6400, valid loss-0.2502, acc-0.6682, test loss-0.2598, acc-0.6441\n",
      "Iter-9470, train loss-0.2981, acc-0.5600, valid loss-0.2502, acc-0.6704, test loss-0.2597, acc-0.6439\n",
      "Iter-9480, train loss-0.2386, acc-0.7000, valid loss-0.2502, acc-0.6682, test loss-0.2597, acc-0.6443\n",
      "Iter-9490, train loss-0.2170, acc-0.7100, valid loss-0.2503, acc-0.6674, test loss-0.2598, acc-0.6437\n",
      "Iter-9500, train loss-0.2760, acc-0.6400, valid loss-0.2503, acc-0.6672, test loss-0.2597, acc-0.6431\n",
      "Iter-9510, train loss-0.2636, acc-0.6500, valid loss-0.2506, acc-0.6650, test loss-0.2597, acc-0.6423\n",
      "Iter-9520, train loss-0.2708, acc-0.6400, valid loss-0.2506, acc-0.6668, test loss-0.2597, acc-0.6445\n",
      "Iter-9530, train loss-0.2661, acc-0.6300, valid loss-0.2505, acc-0.6672, test loss-0.2597, acc-0.6437\n",
      "Iter-9540, train loss-0.2342, acc-0.7100, valid loss-0.2506, acc-0.6650, test loss-0.2598, acc-0.6435\n",
      "Iter-9550, train loss-0.2800, acc-0.5900, valid loss-0.2507, acc-0.6640, test loss-0.2599, acc-0.6433\n",
      "Iter-9560, train loss-0.2540, acc-0.6500, valid loss-0.2506, acc-0.6610, test loss-0.2596, acc-0.6423\n",
      "Iter-9570, train loss-0.2704, acc-0.5900, valid loss-0.2503, acc-0.6630, test loss-0.2594, acc-0.6413\n",
      "Iter-9580, train loss-0.2913, acc-0.5900, valid loss-0.2505, acc-0.6572, test loss-0.2594, acc-0.6394\n",
      "Iter-9590, train loss-0.2640, acc-0.6200, valid loss-0.2508, acc-0.6536, test loss-0.2595, acc-0.6367\n",
      "Iter-9600, train loss-0.2569, acc-0.6700, valid loss-0.2507, acc-0.6522, test loss-0.2594, acc-0.6365\n",
      "Iter-9610, train loss-0.2672, acc-0.6500, valid loss-0.2504, acc-0.6588, test loss-0.2592, acc-0.6427\n",
      "Iter-9620, train loss-0.3138, acc-0.5300, valid loss-0.2501, acc-0.6602, test loss-0.2590, acc-0.6407\n",
      "Iter-9630, train loss-0.2630, acc-0.6200, valid loss-0.2499, acc-0.6636, test loss-0.2588, acc-0.6429\n",
      "Iter-9640, train loss-0.2250, acc-0.7300, valid loss-0.2502, acc-0.6618, test loss-0.2590, acc-0.6392\n",
      "Iter-9650, train loss-0.2578, acc-0.6400, valid loss-0.2500, acc-0.6646, test loss-0.2589, acc-0.6414\n",
      "Iter-9660, train loss-0.2795, acc-0.6300, valid loss-0.2501, acc-0.6626, test loss-0.2590, acc-0.6388\n",
      "Iter-9670, train loss-0.2561, acc-0.6400, valid loss-0.2500, acc-0.6638, test loss-0.2591, acc-0.6421\n",
      "Iter-9680, train loss-0.2214, acc-0.7000, valid loss-0.2500, acc-0.6620, test loss-0.2591, acc-0.6404\n",
      "Iter-9690, train loss-0.2724, acc-0.6100, valid loss-0.2500, acc-0.6584, test loss-0.2589, acc-0.6395\n",
      "Iter-9700, train loss-0.2512, acc-0.6700, valid loss-0.2501, acc-0.6592, test loss-0.2591, acc-0.6388\n",
      "Iter-9710, train loss-0.2385, acc-0.7000, valid loss-0.2500, acc-0.6636, test loss-0.2590, acc-0.6386\n",
      "Iter-9720, train loss-0.2556, acc-0.6700, valid loss-0.2504, acc-0.6588, test loss-0.2594, acc-0.6380\n",
      "Iter-9730, train loss-0.2918, acc-0.6200, valid loss-0.2506, acc-0.6564, test loss-0.2594, acc-0.6399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-0.2477, acc-0.6500, valid loss-0.2514, acc-0.6432, test loss-0.2599, acc-0.6273\n",
      "Iter-9750, train loss-0.2400, acc-0.6500, valid loss-0.2514, acc-0.6462, test loss-0.2601, acc-0.6277\n",
      "Iter-9760, train loss-0.2426, acc-0.6700, valid loss-0.2512, acc-0.6470, test loss-0.2598, acc-0.6295\n",
      "Iter-9770, train loss-0.2221, acc-0.7000, valid loss-0.2515, acc-0.6422, test loss-0.2601, acc-0.6245\n",
      "Iter-9780, train loss-0.2742, acc-0.6200, valid loss-0.2513, acc-0.6436, test loss-0.2599, acc-0.6265\n",
      "Iter-9790, train loss-0.2422, acc-0.6700, valid loss-0.2513, acc-0.6436, test loss-0.2600, acc-0.6283\n",
      "Iter-9800, train loss-0.2462, acc-0.6600, valid loss-0.2512, acc-0.6476, test loss-0.2599, acc-0.6324\n",
      "Iter-9810, train loss-0.2311, acc-0.6900, valid loss-0.2514, acc-0.6418, test loss-0.2601, acc-0.6249\n",
      "Iter-9820, train loss-0.2639, acc-0.6100, valid loss-0.2516, acc-0.6400, test loss-0.2602, acc-0.6232\n",
      "Iter-9830, train loss-0.2300, acc-0.7200, valid loss-0.2514, acc-0.6418, test loss-0.2600, acc-0.6256\n",
      "Iter-9840, train loss-0.2901, acc-0.5500, valid loss-0.2510, acc-0.6456, test loss-0.2597, acc-0.6310\n",
      "Iter-9850, train loss-0.2787, acc-0.5700, valid loss-0.2513, acc-0.6438, test loss-0.2597, acc-0.6304\n",
      "Iter-9860, train loss-0.2877, acc-0.5700, valid loss-0.2518, acc-0.6412, test loss-0.2601, acc-0.6261\n",
      "Iter-9870, train loss-0.2666, acc-0.6600, valid loss-0.2518, acc-0.6406, test loss-0.2601, acc-0.6238\n",
      "Iter-9880, train loss-0.2632, acc-0.6300, valid loss-0.2516, acc-0.6410, test loss-0.2599, acc-0.6272\n",
      "Iter-9890, train loss-0.2661, acc-0.6300, valid loss-0.2518, acc-0.6414, test loss-0.2600, acc-0.6258\n",
      "Iter-9900, train loss-0.2956, acc-0.5600, valid loss-0.2514, acc-0.6444, test loss-0.2598, acc-0.6283\n",
      "Iter-9910, train loss-0.2236, acc-0.6800, valid loss-0.2518, acc-0.6408, test loss-0.2599, acc-0.6250\n",
      "Iter-9920, train loss-0.2577, acc-0.5800, valid loss-0.2519, acc-0.6398, test loss-0.2600, acc-0.6253\n",
      "Iter-9930, train loss-0.2557, acc-0.6400, valid loss-0.2520, acc-0.6418, test loss-0.2599, acc-0.6286\n",
      "Iter-9940, train loss-0.2703, acc-0.6200, valid loss-0.2520, acc-0.6432, test loss-0.2599, acc-0.6308\n",
      "Iter-9950, train loss-0.2766, acc-0.5700, valid loss-0.2518, acc-0.6476, test loss-0.2596, acc-0.6341\n",
      "Iter-9960, train loss-0.2714, acc-0.6100, valid loss-0.2519, acc-0.6500, test loss-0.2597, acc-0.6337\n",
      "Iter-9970, train loss-0.2400, acc-0.6800, valid loss-0.2518, acc-0.6516, test loss-0.2596, acc-0.6355\n",
      "Iter-9980, train loss-0.2440, acc-0.6700, valid loss-0.2516, acc-0.6558, test loss-0.2593, acc-0.6366\n",
      "Iter-9990, train loss-0.2546, acc-0.6200, valid loss-0.2518, acc-0.6590, test loss-0.2594, acc-0.6383\n",
      "Iter-10000, train loss-0.2534, acc-0.6700, valid loss-0.2515, acc-0.6610, test loss-0.2592, acc-0.6438\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYGZmFf4Pyy916RBVB8MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113778588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnWd4FdXWgN+dkBACpNFLaAEFRYpIE0sEFcSGCiooIIKi\nfjbUi1zBS7x2rxUrKmJDuYoicBVBxdhFpClKEymhtxBKIAnJ+n7MOTn95CQ5JWW9zzPPzOy9Z+81\nc+bsNbutZUQERVEUpXoSFWkBFEVRlMihSkBRFKUao0pAURSlGqNKQFEUpRqjSkBRFKUao0pAURSl\nGhOQEjDGDDDGrDHGrDPG3OslPskY87ExZqUx5mdjzEnBF1VRFEUJNiUqAWNMFPAC0B84GRhqjGnv\nluw+YLmIdAZGAlOCLaiiKIoSfAJpCfQA1ovIZhEpAGYCl7qlOQlYBCAia4FWxpgGQZVUURRFCTqB\nKIFmQJbT+VZbmDMrgcsBjDE9gBZA82AIqCiKooSOYA0MPwYkG2OWAf8HLAcKg5S3oiiKEiJqBJBm\nG9aXvZ3mtrBiROQQcL393BizEfjbPSNjjBoqUhRFKQMiYkKRbyAtgSVAW2NMS2NMLHA1MNc5gTEm\n0RgTYzu+AfhGRA57y0xEdBNh8uTJEZehomz6LPRZ6LPwv4WSElsCIlJojLkVWIilNKaJyGpjzFgr\nWl4FOgBvGWOKgD+A0aEUWlEURQkOgXQHISKfAye6hU11Ov7ZPV5RFEWp+OiK4QiRnp4eaRFKpFkz\nmDMn9OVUhmcRLvRZONBnER5MqPubXAozRsJZnlI+jIG77oKnnoq0JIpSvTHGIBEcGFYqITNmwE8/\nlT8fd5392Wflz6+8eVQlWrVqhTFGN90wxtCqVauwv4PaEqiiGNs3Q3ketzEwbhw8/bR1XlgINWqU\nL89Nm6B16/LlUZUwxoR89odSefD1PtjCtSWghB/n9zEYdZXWd4pSsVAlUEXo2hUefth3vDGwZInv\nuOPHHefvvAP16nmmiYnxfv033zhaHiUxe7Znvl9/Hdi1iqIEH1UCVYQVK0rua1+92necsxL47jvY\nvz/wsn/7LfC0v/xSvusVRQkuqgQqMcuXw4YNjvPCQuuret8+7+lFYM8euOMOWLgQ3noLCgo80+Xn\nu17jjxUr4K+/XMM+/tiSxRveWgzbt/svIxD27oXMzPLno4SGoqIi6taty9atWyMtiuKGKoFKzKmn\nQr9+jvPCQujbF+6/3/c1994LU6ZA//5w3XXw1VeeaebO9QzzRdeuVn7OXHGFpaC84U0JPPFE4OX5\n4p//hHPOKX8+ikXdunVJSEggISGB6Oho4uPji8Pef//9UucXFRXFoUOHaN5cjQtXNAJaMaxUDoqK\nrL2//nlfX/bBHrD1JUOgYwdKZDl06FDxcZs2bZg2bRrn+NGyhYWFREdHh0O0sFOV7w20JRBxPvzQ\nd8UYGwuvv+44NwaWLnVNs3mztbIXHErAGee8DxzwjL/gAsdxUZGVPjvbESYCJ5zg/x68UVolkJkZ\nHgVhDPz5Z+jLqUp4M2J2//33c/XVVzNs2DASExOZMWMGP//8M7179yY5OZlmzZpxxx13UGjrFyws\nLCQqKootW7YAMHz4cO644w4GDhxIQkICffr0YfPmzT7LHzJkCE2aNCElJYW+ffuyZs2a4vijR48y\nbtw4WrZsSXJyMunp6RTY+jm//fZbevfuTVJSEi1btmTGjBkAnHnmmbz99tvFeTgrObusL7/8Mu3a\ntaNDhw4A3HbbbaSmppKUlETPnj35yWkhTmFhIQ8++CBt27YlMTGRHj16sHPnTm666SYmTJjgcj8X\nXnghL774Yul/iBChSiDCLFvmO66gwHNGj3v/Ozj61L0pAWcOHPD/xe8rbv16//mWBl8Vvb9B62Bj\nq4eUcvLJJ59w7bXXkpOTw1VXXUVMTAxTpkxh//79/PDDDyxYsICpU4tNjGHcfvz333+fhx9+mOzs\nbFJTU7nfTz/mxRdfzIYNG9i5cycdO3Zk+PDhxXF33nknq1atYsmSJezfv59HHnmEqKgoNm7cyIUX\nXsg999zD/v37Wb58OaeccorPMtzlmzdvHr/++iu///47AL169WLVqlXs37+fwYMHM2TIkGJl88QT\nT/Dxxx+zcOFCcnJyeP3114mLi2PkyJHMnDmzOM/du3fzzTffMGzYsACecHhQJRBh3Cvev/6CRx+F\nX391hM2bB/bxNHv6V1/1zMvXYKw9bUYGfPSRbzm8KQFvrYepU2H+fKsV4oupU638nOoAwLcSeO01\nV1lDSbC6voqKPO8v2BgTnC0UnHHGGQwcOBCAmjVr0q1bN7p371688vWGG27gm2++KU7v3poYPHgw\nXbt2JTo6mmuuuYYVK1b4eAaGESNGEB8fT2xsLP/6179YunQpR48epaioiLfeeovnn3+ehg0bYozh\n9NNPJzo6mhkzZjBw4ECuuOIKoqKiSElJoVOnTgHf38SJE0lMTKRmzZoAXHPNNSQmJhIVFcU999zD\nwYMH+cv2VTZt2jQeffRR2rRpA0CnTp1ISkqid+/exMXFFT+H999/n3PPPZfk5OSA5Qg1qgQijHuF\nNHo03HcfdO/uCLvkEpg40TXd2LGeeflSAs5pD3v18uBbCTi1mIu56SYYOBDuvNN7XmBV5ocOWWmd\n8VUh2QeSvd1XsAmWEti3z/P+go39dynvFgpSU1NdzteuXctFF11EkyZNSExMZPLkyezdu9fn9Y0b\nNy4+jo+P57CPl7OoqIjx48eTlpZGUlIS7dq1wxjD3r172bVrFwUFBcWVrzNZWVmkpaWV8e7wGMR+\n4okn6NChA8nJyaSkpJCbm1t8f1lZWV5lAKvr69133wXg3XffdWnFVARUCVQS7BW8vz90Sd1B/hAp\n/fW+lI4/okL0xumAc/hx7z4ZO3Ysp5xyCn///Tc5OTk88MADQTGJ8fbbb/P555+TmZnJgQMH+Ouv\nv4rHKRo1akRsbCwbnOdK20hNTS3+Unendu3a5ObmFp/v3LnTI43z/WVmZvLMM88we/ZssrOzyc7O\npnbt2sX316JFC68ygKUEZs+ezYoVK/j777+5+OKLS3X/oUaVQBkwBj79NPD0a9b4rqTs0yONsRZN\nuf9n7N0jtvEshg2DL7/0nte6ddb+pZcCl81OWb4Y581zPTfGIac3jIE333QcBzp4nJgIkye7xtsn\nrxgD//2vfzlTUy0bSHaC9WVsl7NFC2vtRXXn0KFDJCYmUqtWLVavXu0yHlDefGvWrElycjJHjhzh\nvvvuK66go6KiuO6667jzzjvZtWsXRUVF/PjjjxQWFnLttdeyYMECZs+eTWFhIfv27eM328rELl26\n8NFHH3Hs2DHWrVvHG2+8UaIMMTExpKSkkJ+fz+TJk12UyOjRo5k0aRJ//2151V25ciUHbH2pLVq0\noFOnTowcOZIhQ4YQGxsblOcSLAJSAsaYAcaYNcaYdcaYe73EJxhj5hpjVhhjfjfGXBd0SSsYa9cG\nnnbHjsDSBbqOJpgDtXaC1W2walX583Dn4EHPlcbHjjmOS5rts3Ur/Phj8OWyk5UV2vwjjfsXvy+e\neuop3nzzTRISErj55pu5+uqrfeYTaJ4Ao0aNokmTJjRt2pRTTjmFM844wyX+6aefpkOHDnTr1o16\n9eoxceJERIRWrVoxb948HnvsMVJSUujWrRurbC/oPffcA0CjRo0YM2aMRxeNu3wDBw6kX79+tGvX\njjZt2pCUlESTJk2K4//xj38waNAg+vXrR2JiImPHjuWY00s6cuRIVq1axYgRIwK+77ARgG/LKOAv\noCUQA6wA2rul+SfwqO24PrAPqOElL6ns/O9/VnX55JOOsN27RV5+2fc1X39tXePMZ5+JLFni2nNr\nz7uk7cEHg9VT7Nj69hWZNSu4eR444HrfpbnWGRDp2dP1fM8ex3F8vMi113pe517uffdZ+3nzXONz\nckSefdb7tUeOiDz1lMiuXda1H37oiNuzx5H3aaf5/PlFRCQ31/WdcchW+f8TSsksWrRI2rRpU2I6\nX++DLbzE+rosWyAtgR7AehHZLCIFwEzgUnddAtS1HdcF9onIcaogF11k7Z37z996C26+uXT5DBwI\nQ4e6hgX6JW4bYwoqixbB4MHBzdP+jILRwli82Hdcbq53m0TuPPKId3nmz/c9yP3dd3D33Y5urCFD\nvKcr6R5//hlsH59KNSM/P5/nnnuOG2+8MdKieCUQJdAMyHI632oLc+YF4CRjzHZgJVDle0id//Q1\nvKy7ttvkOe5FFdqvda84nLs4/FFZBkHt00vz80uvCI4ftzZnZevNzpFz+tLkbZfn+HHv+drD7OU7\ny5GXZw2Kl2Yg3V8ZStVl1apVpKSkcODAAW677bZIi+OVYJmN6A8sF5G+xpg04AtjTCcR8ZjzlZGR\nUXycnp5eaf2IOldq3laUx8ZaDlQ6dICOHV3jnnnG2rtPJvD1lemv7IqMfcZcXByUdkJESoo1+HvF\nFY6w2FjHvbsrQtt4XEBcfrllX+nf//ZuHjs/H2rWtMqyf73/85+O+Lg46NXLddGZP8X80UfeW1kV\nbHxQCQEdO3b0OfXVH5mZmWSGySJiIEpgG9DC6by5LcyZUcCjACKywRizEWgP/OqWzkUJVGacvwJ9\nmRU5cACOHrUsbTrzxx/lK7uytAScKe37bJ/9EwwXmd5YudJ3nPPUV1+Dzj//7HruTzH7mKWoKD5x\n/0B+4IEHQlZWIN1BS4C2xpiWxphY4GrA3c7kZuBcAGNMI+AEoBTfZpWD//zHe7izEpgyBSZNso7P\nPtvau8+NL6+PXSezKZUGJ3tkpcKXmennn/dsSYG1ivmpp6zjDRugfn3v14tYebizYgWcf751/PLL\ngct54IClnK+/Hk47zQr717+8r7geP15NVygViEBGj4EBwFpgPTDBFjYWuNF23ARYAPxm24b6yCeA\ncfSKi/PslYcfdoS/+qpjZoq3mS5xcWWfJaOb61ZU5Di+807PePsMIBGRO+7wnc/FF3uGiYgMGBAc\nOe2/88cfizz2mGf4TTfZw5xeDKXa4+t9sIUTii2gMQER+Rw40S1sqtPxDqxxgWrDkSNWl9DRo44w\nXwOF9u6bgwehVq3Qy1aVOXIksHTZ2bYq1gd79niGHToE29w7OsvJrl3W7+5OeVZ3K0ow0RXDZeSR\nR2DUKKhTx1HJ+3KOYo9PTHR1AhNW6tv6kBI3w73JMC4VLrwF4n3bdqmI1K3rP94+BpeS4r0rxo57\nnz5AQgLYDEYGjZtvdkxNdSYchvIUJRDUqUw5sA9a2it5Xyt5nQdyA5nP7h8BUwRxB2DopdDiB0fU\nrzfCaW61S04qJGbhQa0D0P1la7OzoyvMeQNqHLWUw86ucLAZNPwDmv8EW86A/e2g7jbIrQ8Ftct7\nM0HH2TWmcytNCS+bN2+mdevWHD9+nKioKAYOHMjQoUO9Gk9zT6uEF1UCPpgzx6pE3Fa+u2Cv9Eua\nreMcn5dXRoFMEdRfDf/X0XcadwUAkJjFJWvgrY9qkB8VRVZRa7oVWDYvckjgh2Yx7K23jxmdYHGz\n5TCyK4djoTAQR0rPr4F9J5acLkQ8+6xn2CuvOI6zvOi+SFNZZnZdcMEF9OzZ02M235w5c7jpppvY\ntm1biRW2s+mFz0qYDVEaMxJKcFEl4INhw6yVqP6UQKCU6/1O2gQj+kGKl8lWjx6AvESiOc5Q3md2\n25ocOXAKKXvrcWODcfzfnkyaY59eY62maojD6FEiBxm4DdgGI37zzH5dbBP6DE0he+m9FG7rBekZ\n0Pk92HsC1F8Ht7X3LfcTe6DbVPj1ZjiaUta7LxfeunyUwBg5ciSTJk3yUAJ2U8jV6YtdRKq2kgrV\niLO3jQo+E6KgwLIjI2LZowGRY8dEDh8WycvzPRPkjTes/fXXB2kWTFS+0GOKcHNHIQPH1udxwRwX\n6myXWjE7pTHb5SwyS8zwfD73GR3PYYEiiSNXruMNuZ1nAxKyScLPrrKVtJ0wT0jaKFAU8RlG4dj+\n/DPQtBXzP3H06FFJSkqS7777rjgsOztb4uLi5PfffxcRkU8//VS6du0qCQkJ0qJFC8nIyChOu2nT\nJomKipLCwkIREUlPT5dp06aJiEhhYaHcfffdUr9+fUlLS5MXX3zRJa07jz32mKSlpUndunXl5JNP\nltmzZ7vEv/rqq9KhQ4fi+OXLl4uISFZWllx++eXSoEEDqV+/vtx2220iIpKRkSHXXnuti6zGGBdZ\nJ06cKH369JH4+HjZsGGDTJ8+vbiMtLQ0mTp1qosMn3zyiXTp0kUSEhKkbdu2smDBAvnwww+lW7du\nLumeeuopGTRokM/n7ut9sIUHXNeWZgtJpj4Lq6AvvJ1Jk2xPRBxK4MILRerVExkxwvcfOWhKIHmD\nZ+U5+Cqh1l6ZyZVyHW/4vPgSPik+ns5I6c0PEkduueSJIU/u5j8iIFk0k1e40SPREWrJdEZKPIeE\nmCNWcN1tluw1DwiXXetdKUQVRLyirhhbxf1P3HDDDXLDDTcUn7/yyivStWvX4vNvvvlGVq1aJSIi\nv//+uzRu3FjmzJkjIv6VwMsvvywdOnSQbdu2SXZ2tpxzzjl+lcCsWbNk586dIiLywQcfSO3atV3O\nmzdvLkuXLhURkQ0bNsiWLVuksLBQOnfuLHfffbccPXpU8vLy5IcffhARSwkMHz68OH9vsrZs2VJW\nr14thYWFUlBQIJ999pls3LhRRES+/fZbiY+PL1Y2ixcvlsTERPnqq69ERGT79u2ydu1aycvLk3r1\n6smaNWuKy+ratauHEnNGlUCEGTZMipVA7drWcatW1r5XL99/5OnTrf3o0eWoDE591bWSPPMBmV5z\nkN+LbudZuZ1nJYrjYa64iuQrzvGIeIUb5U1GyGK6SxTHpSlbHYqo8XKh1SJPZZCyvgJUxJHc/P8n\nStXa8rOVhe+//16SkpIkLy9PRET69Okjz/oytyoid955p9x1110i4l8J9O3b1+VLeuHChX6VgDtd\nunSRuXPniohI//79ZcqUKR5pfvrpJ2nYsKHXPANRApMnT/Yrw6BBg4rLHTt2bPF9u3PLLbfIpEmT\nRERk1apVkpKSIvn5+T7zjYQS0DEBJ0Qcx/b56PauQH9doKNGWftp08pQaHQ+dHoXLrmRusdg7eMN\nWcMJnCOTXZINYD6jmM7V/Jde/MQeGrCBtmUoMBgY+rGo+KwuBzlIImNxDEwXur9aO6Ehu9iTIXDJ\naDjV5sTj9nbwwz/gCx/za6s5MllKThQi+vTpQ4MGDfjkk0847bTTWLJkCbNnzy6O/+WXX5gwYQKr\nVq0iPz+f/Px8hgRgAGv79u0urilbtmzpN/3bb7/NM888w6ZNmwA4cuSIi1tHby4ks7KyaNmyZZnH\nLtxdZ86fP59///vfrFu3jqKiIo4ePVrsrzgrK4sLL7zQaz4jRoxg2LBhPPjgg7z77rtceeWVxHgz\nWOXEs89Cu3aW06bx4x02uEKFKoEACck42HnjoY9li6Lrd+ex7KsvgN00YTcALdnEFhx/kAUMAOBn\neodAmLJziAQMQjvWsY1mFBDDq9zIV/TjHRxONHbTyDqYC+kLZvNN07ow8lzrGdieA6/+Atu7eylF\niQTDhw/nrbfeYs2aNfTv358GDRoUxw0bNozbb7+dBQsWEBMTw7hx49i3b1+JeTZp0oQsp6lbmzdv\n9pl2y5Yt3HjjjXz99df07m299127dkVsX2ypqak+XUtu2bKFoqIiD0Xg7lpyhxevT84Dwfn5+Qwe\nPJh3332XSy+9lKioKC677LISZQDo2bMnsbGxfPfdd7z33nu8//77Pu/VzrhxltHJVavghBPgrrtK\nvKRcVJ8h/gDYtcszLJCWQKlI3AIZxtpsFd+sR+wKALqyjFS2EM8RFwVQGVjPCeRSmwJiGcWbvMtw\nDIJBiOY4eTjMZmbmXYZsPBfJgE7O7l1v7GE9m5oHIX4P1HCzr11npxWftMmaNlsSNQ9CQoAu2xQP\nRowYwZdffsnrr7/OyJEjXeIOHz5McnIyMTEx/PLLL7z33nsu8SLeWzFXXnklU6ZMYdu2bWRnZ/P4\n44/7LP/IkSNERUVRv359ioqKmD59erF3MIAxY8bw5JNPsmzZMgA2bNhAVlYWPXr0oEmTJkyYMIHc\n3Fzy8vL40eb+rUuXLnz77bdkZWWRk5PDY4895vcZ2Fs59evXJyoqivnz57Nw4cLi+NGjRzN9+nS+\n/vprRITt27ez1sn14PDhw7n11luJjY3l9NNP91tWJFAlYGPZMsuxii+CogRG9INxThX771fzdcbZ\nXJFvKYB4jrCCrmwllaPEB6HAikMR0cSRV6wUbsKxSG3lKxD7RQaxz60Ce73xz0QY3xAm1XIozQwD\n99hc+t3ZGiZHw/VnQLRt8UWUm1OB1ousfO5Kta69Lt2/kLX2O/KosyMwJVPFadmyJaeffjq5ublc\ncsklLnEvvfQS999/P4mJiTz00ENcddVVLvG+3EnecMMN9O/fn86dO3PaaadxhbO9cDc6dOjA3Xff\nTa9evWjcuDF//PGHi3vJwYMHM3HiRIYNG0ZCQgKXXXYZ+/fvJyoqinnz5rF+/XpatGhBamoqH3zw\nAQDnnnsuV111FZ06daJ79+4ejt/dp4PWqVOHKVOmMGTIEFJSUpg5cyaXXurwq9W9e3emT5/OnXfe\nSWJiIunp6WxxshA4fPhwVq1a5XWhXEXA+NLWISnMGAlneaVh0SKHSQcRRwugTRvLVn3fvv6VhF/q\n7ISrLoNUa+J6u4y1zGIwnbBsFFzMXP5HKQ3uVxFiySOPOJew16NGcMO/3vZ90dIx0O31shc691VY\nOQJOnAdHk60uKV9khPp9NT6/mJWqwbFjx2jUqBHLli3zOn7hjKWApLg76MknLc92xhhEJCSLFar9\nmMD554NbK5YLLnAc252VlFoBmEIYcCf0fMEl+JSM5fzmZIvvKe6qtgoAIJ+atGCzS9fXmKK3GZMB\n57OAh5hEz/rT4fzxMO81GhXu4ZHc55g0bxs7aAJtvoIR5/nMv86jW7gybyGrTHt+mWz7grzkRmsL\nhAy3/11Wr2JlDsB/Z8Hqy20nVXhBkVJmXnrpJbp3716iAogU1b4lYIxVwRcVwbl+PghLRa39cG89\n17CHj3BpwUI+4TIA4jjq8QWsgKGIIjxtVnzGBQxkvtdrRqTewPs7H+P8gp/4MaobBxKOwYFWiFul\nfDMv8coNb0DW6dBrincB5j8Lv96EKayBZJTxG+m7CXDmY3C8ptXSeG0JHGzuI7G2BKoyrVu3BuCT\nTz6hc+fOJaaPREugWiuBgwcty56LFlnepM7z/UEZOLGH4T6bqcuXV8IuaxpZY3awg6aAVdHpV6N/\nPmAIQ5jlNe5a3uFdAu9f/ZSBXIhlu+YVxnIrL1CbI1zKHL6iH8eIox9f8QGufdq7aUCjiwfBoaaW\nwb7Pn4XUHyG7JRxuBohthteTcKQ+1PZjkfX3obDkFsivDTedCntPhL/Phc9eVCWgFGNXAiefbHkg\nfPxxaNsWrrhClUBI6NwZfvvNUgLHjzs8SpWZuGyYYLOT88RuyLWm0/XlK77iXL6kH+fxBaoASsc/\neYRHmEgS2eQT6zJo3pwssmzeT//BE/yH8S7XGttIcxSFnmsXgsAVzCKOY4zhdQYzi/01Y+DsB2Dp\njXAwFS64HU71s4Akw/csGqX64a4EGjSw+74InRIIaEUZlmexNcA64F4v8fcAy4FlwO9Y1sqSvKTz\nuVIuEtSpY63aXLRIZOHCcq78bLbYsUKzxXcCIs3IcknUjKwKsEK1am+xHJPLmSXRFEg0BR5xvi48\nne9lMd3lZcZKF5ZJDfLLLcyDTBQosmxB9X5KGN1b6H+n9a7U2SGMOkMq2n9CiSyAgEjHju6vEyIS\nIbMRWNNI/wJaAjHACqC9n/QXAV/6iAvVsys1zz/veMDlVgJnZzgUQFy2RFPgkmAR6VJdDKdV9G0Q\nH0sqmwNObxnY8x3fnC1yEqskiuMygjd9JuzOYklhrzRgl0RTIMnsk8v4SCrSf0KJPHYl4LkhIqFR\nAiV2BxljegGTReQC2/kEm0BeV3gYY2YAi0TEow1ckbqD4uMdTkcWLYKCAuhfFgeZziYQHs2BvLqI\nbfnF44znASZXuTn/in/iOMqJrGU473A3T/tNa4CK8p9QIo+9O8hLDBLBKaLNAGf3HFuBHt4SGmNq\nYXUd/V/5RQstzv+7DRugadNSZhC/F8Y7ltDzQCGI4R3bgOWNTOU1ApyGqFQpjlGLlXRhJV24h6eo\nwyEeYhIt2cxGWnMia4mhgB/oAzwQaXGVak6wR8ouBr4XEZ/eXZ2dVKSnp5Oenh5kEQLDWQnccAM4\nLQAM5GqHAth1Crz8GynsYx/1ARjGDN5nWNBkVSo3h6nLnTznI1aVgOKNTNsWegJRAtvANv3Corkt\nzBtXA34tJLl7KooU7i3wnJxSXJxhsyGx4En46W7qsZe9WEphELOZw6DgCKkoEaJu3brF5hOOHDlC\nzZo1iY6OxhjD1KlTGTp0aJny7d27N7fddhvDhulHkn/SbZud0H0sBKIElgBtjTEtgR1YFb3HG2CM\nSQTOBq4JqoQhoqjI/7lPLrZ18cx7BZaO5d/cz/08BMAJrGU9JwRPSEWJEIcOHSo+btOmDdOmTeOc\nc86JoEThobCwkOjoQBxsVx1KNIsmIoXArcBC4A9gpoisNsaMNcY4d3oPAhaIyNHQiBpc3FsCJY7N\nmUJo8R10ew2AuKUjEEyxAoglLyIK4KmnvIc3bgynnOI4v+MO7+l27ixtV5hS3bDPInGmqKiIBx98\nkLS0NBo2bMjw4cM5ePAgALm5uQwdOpR69eqRnJxM7969ycnJ4Z577mHJkiWMGTOGhIQE/vGPf3iU\nVVhYyODBg2ncuDEpKSn069ePdevWFcfn5uZy++2306JFC5KTkznnnHMosn3BZWZm0rt3b5KSkmjV\nqhUzZ84ErNaHs4XTqVOncp5tZWheXh5RUVG88sortG3bllNsf5pbbrmF1NRUEhMT6dWrF4sXL3aR\n8YEHHiBTFQ/2AAAgAElEQVQtLY3ExER69uzJ7t27GTNmDJMmTXK5n/79+zN16tQyP/uwEKppR942\nwjQdrqhIZOVK/2miolynYDVsWMJ0wQyHl6Zk9hVH7KRh2Kc5JiQ4jt/w4XGydWuR8893nD/rw3Vw\nXp7I8OHhlV83542w/CfKQ6tWrYpdJ9p57LHH5KyzzpKdO3dKXl6ejBo1Sq6//noREXnuuedkyJAh\nkpeXJ4WFhfLrr79Kbm6uiIj06tVL3nvvPZ9lHT9+XN555x3Jzc2VvLw8ueWWW6RXr17F8ddff730\n799fdu/eLUVFRfL9999LUVGRrF+/XurUqSOzZ8+WwsJC2bt3r/z222/FZc6YMaM4j1deeUXOO+88\nERE5duyYGGPkoosukpycHDl27JiIiLzzzjuSk5Mjx48fl0ceeURSU1Pl+PHjIiLy73//W0499VT5\n+++/RURkxYoVkpOTI99++620bt26uJzt27dL7dq1JTs7O+BnDeGfIhqSTH0WFqYX3j7n3x/GBPon\nLXIogOHnCqawOLIhOyNScTgrgR07XONOPtnat2njUALjx4tkZXnPKz9fZMUK32U9/nj47696bfh/\nUYNVUDnwpgRat24tP/74Y/H533//LfHx8SIi8tJLL0l6enqx/2Fn3CvkktixY4dERUVJXl6eFBQU\nSExMjKxfv94j3eTJk2XYsGFe8whECfz8888+ZSgqKpL4+HhZt26diIi0bNlSvvjiC69p09LS5Pvv\nvxcRkSeffFKuuOKKwG7URiSUQJX0J3DsWMlpRALMbKJtjv+e9tR8Zx4iVn9hXQ46PGVFCBGr28cZ\nu78NZ5Pojz8OzW32y9ytoRpjmc/wxfjxcOKJvuOVEBM0XRNcsrKyGDhwICkpKaSkpHDqqacCsH//\nfkaPHs1ZZ53F4MGDadGiBRMnTkQClKGwsJC7776btLQ0kpKS6NChAwD79u1jx44dFBYW0saLv0Vf\nbiYDpXlzVwN/jz76KO3btyc5OZmUlBTy8vKKXVpu27bNqwxg+Q549913AXj33XcrrA8BZ6qkEgga\nCVkQY2mU2BdXcIxaACziHA5TN2Jide8OJ53kO75JE+jaFU4+2TOuYcPSl2fU1JHiRvPmzVm0aBH7\n9+9n//79ZGdnc+TIEVJSUoiNjeWBBx5g9erVfPvtt3z44YfF/fPuDlvcmT59Ol999RXffPMNBw4c\nYM2aNQCICE2aNKFGjRo+3Un+9ddfXvN0dye5c+dOjzTOcn355Ze88MILzJkzh+zsbPbv309cXFyx\nImvevLlPd5IjRoxg1qxZLFu2jK1bt/r0PVyRUCXgi9P/A3dZM2OTMvYWm31+irtcnKwHkyuvhORk\n6/iAj5UWHTvC/PmwfLlnnN2j3caN8P778J//OFZFAxw+bCmGw4e9552UBPv2WdNlDx/2TOc85lWn\njrU/8UTfsoIqkKrK2LFjuffee9m61XLduXv3bv73v/8B8NVXX7F69WpEhDp16lCjRo3iGTeNGjXi\nb7uTDi8cOnSIuLg4kpOTOXz4MBMnTiyOq1GjBiNGjOCOO+5g9+7dFBUV8cMPPyAiDB8+nE8//ZQ5\nc+ZQWFjI3r17+f13y2lTly5dmDVrFnl5eaxZs4Y333zT770dOnSI2NhY6tWrR15eHvfffz95eXnF\n8aNHj+a+++5j48aNAKxYsaJ4ULx169Z06NCBUaNGcdVVV1GjRsV32VIllYDTQH6Z4gHLiQlwwpQ5\nZNsWgb3ILdyDj+k4QaBZM8ucBVgmrr1RuzbExEBsrGec/dqaNaFGDYiOhrg412ud9+BaScfFQUoK\nJCRYaezp7Glq1nSkrVXLEedLVvf8lcqJt6/3e++9l/POO4++ffuSmJjIGWecwXLbl8m2bdu49NJL\nSUhIoFOnTlx00UVceeWVAIwbN4633nqLevXqMWHCBI98R48eTf369WncuDGdO3fmrLPOcol/7rnn\nSEtLo2vXrtSvX59//etfiAhpaWnMmTOHhx9+mJSUFLp3786ff/4JwPjx4ykoKKBhw4bcdNNNHl00\n7vd38cUXc+aZZ5KWlkbbtm1p2LAhDRo4rANMmDCBCy+8sPjeb775ZhclMXLkSFatWsWIESNK85i9\nE1Xg6dgo2IRqsMHbRjkHqAKlpLGwEjtQmyy1BoJbLygOTOBAyAcJx40Tef11h+ze0jhNlPC4n6ys\n0j2jDh2smVQiIh9/bBnS80aHDlb6Bx90lNWggbVv397/M42ODu0zq/xbeP4TSvhYuHChtGvXrkzX\ngtvAcJc3rLrIek8IxVYlWwLlosYxGNuNxodANloW5YbyHgfx87lbTv71L2tvjNXd4w9/X9al/ep+\n9lnHNZddBqVZCyRSurKc6d277NcqSkUmPz+fKVOmMHbs2OBkOOj64OTjhyqvBIyxPIjl5wdYSU6y\n+jl22Hp9ltGVmZ4LpENGSorjuGFDxxiBnW7dPK+x98+XFveZRb7o2dPqqnLGPmPI2+BzSTh3RylK\nVWHlypWkpKRw5MgRbrnllrJnFJML0fnQ4M/gCeeHKq8EALKzLSVQIqk/ABD3vycBOJNv6caycpd/\nySXW3n2MyP6x4PxV3a6dQ9bt28G+2DA/3/J+9pwXO2TZ2da+NC2B/Hzo1CmwtNOmwebNjvxvuQVG\njbKO777b+zX2FcreZHJWdIpSVejcuTOHDx9m0aJF1LIPmpWFibXh/prwf9YXVtcZGcER0AcVf+i6\nHDjPWomyqbtff/VzwVmWCYjff30ZgO85MyhyOA/YHj/uCI+JsfbuXSv28Ohoa3MO80ZZJiD4y8+d\nKLdPBefBZvc4d5l0YFhRyoHAsvUZIXVIW6VbAs5fqfbKqHt3H4nrbqNGm8/Z82Bd2rKBj7ksaHI8\n+KC191Vh2luO3irM88+HZ54JmijlYswYa28MXHEFtGzpWGj22GOuaf1V/g8/7DgukyOfUvD668HJ\nx8n0jKKElLgCa2+KQMJgabxKK4GAuoDstPiBUfM7Ub/wEL/QnSF8GDQ52ra19u7GCe0tgCZNvMeD\n1d9/551BE6VcNLItkDbGWlOwaZOjVXDvvdbefq92JeBNGdjTtGgBn39ecrmJibBggWtYoGMRQ4fC\noCBY9i6j5WRFKTVTHx5OdkYiRf8OT3lVpjvo2muhXj3XPnPb6m3OPTeAP/GQq3g1AxbTg14EspCg\n9LRpA8uchhjczVf7aikESrgGXAPp4rFbMA1Gd5C3PM44A/74o+Rra9Qo30wmZ1q1shRfMKlZs2WJ\nq2iV6kPLmjUZkfeOS1gCORDC2YlVpiUwYwb4Wgj4118wa5afi++PpaFtdewI3g62aIBVEfXp4z9N\neeoCEf+LtoJJSXKKgC+TKc4Vsr/KuXgWva089+teecU6/u471+tWr3adhR8ba/mPdufjj629vfUV\niKLYuNFKd+65Jaf1Rw8n56x5eZsACcr200/ew0eNCk7+ugV/a0aWS8gmp0Vndg6R4BEWTCq1Ejhy\nBL75xjXM3UCandWrfWQSdRyiC8h6Gn6nI+sIn7U094qnvC2BcBGsD9dgfaE74002L/8rv91VoSYU\n9+0v31CVp5SfbTSnLesZw2te4w2h//EqSbXjneeeA3cXxf36lTKTf8XQPAdii+BZgt/57qsF8tNP\nnn/OyuDQ6KOPfE8L/fZb+OQT1zBj4JFHPNN+/z189pln+K+/wvVu62OMsX7niy/2LZc/g3pTp7qu\nrzj/fMexs+JduNAhk/OA8uWXe8/XPg4ycKBruJulAw/cn5EvnBfv3Xgj3Hdf2dZlBOw1T4kIG2jL\nNMZgEI8tHASkBIwxA4wxa4wx64wx9/pIk26MWW6MWWWM+Tq4YnqnsLCcGTS2bJ1k2WbfvMHocmbo\nyRVXOI6dvzp79aqcLYHLLwcnMyounHmmZyVlDHTp4pm2Tx9Xz2d2unWz1iW4U7OmY22CN+zTcL2R\nlub4OGjRwnWQ2fk3Oe88uOAC69hmGRnwbUq7b19r764kfHXLXXSRtW/aNLCvc/tsLLDWdDz8MDzg\nZ7aIrzydpyUrijslVjvGmCjgBaA/cDIw1BjT3i1NIvAicJGIdASGhEDW4HPTqcTbZhD9hpcaqZzY\nTKEXc9ppruc9erjO8fdWWVZmBgxwLJTzh78vfGcC6brxlcZeqbvH+0rv3Crr2tU1zt/XeMOG1qC1\nXZk4T0n2VUk7G+bzJZvdVH5ZPny0JaD4I5Bvzx7AehHZLCIFwEzA3SvtMOAjEdkGICJ7gytmCEjI\nAuCIrauiMyvLneXbbmPKL77oem4fLLXbCho92jFoKVL1fP3Onw82M/J+mTvX9avXnUD670tKc9VV\n3sO9tb5EHMpZBIa4fdLYWxXeKvVduyxHPJ99ZsX/8osjzldlbFeU/sx/2JVJWZRAuVvMSpUmkCmi\nzYAsp/OtWIrBmROAGFs3UB1gioi8Q4h46inYs8d/F0CJ3NidFy3z56ynLQRhTZ57paADdRaB3K+/\nNKEcxC3vjKxgUtJsKSjbV712Byn+CNY6gRrAqUBfoDbwkzHmJxHxcPWTkZFRfJyenk66+8huANxz\nj7X31z/ql47vEx+7i1tsJiROZG0ZM3Lwj384juPiLBeX3gYIFy50nSJYHQhWZeleYX/8sTW2snix\nZQyvZUv/1y9bZvlKcM5r6dJSLirE6uZyH9w96yxrYNwbM2ZA3brWeIe3dQbGwG+/WaY8OnSwDPb9\n859gd4Y1a5a1BgasMaaEBEfroWNHh0tRX5SlJbB4sSWHL8aMCd5qbMUbmbYt9ASiBLYBLZzOm9vC\nnNkK7BWRY8AxY8y3QGfArxIoL2X+iuvwMU98YR3GkE8wLGp36+aYivj669biNW82fc47r9xFVTt8\ntQQus1n2CFSpuvftQ9n8J8fEWF138+c7wjp08K0Ehg1zHDdt6j2N8yC5PX97V5rz5IK4ONcxFOd3\nrDQDw+3aWYppmQ/7iCU909des56d88ePEkzSbZud0NmPCKT2WwK0Nca0NMbEAlcDc93SzAHOMMZE\nG2PigZ6Ar5n5QcPZDj9YJqMDof+amvzfElhJJ45TCktqfjjhhKBkUyXx4ZO71JRnYLi0+JoB5Uzr\n1sEpy7kx3KWL1dIoCXuakjy7gWNxm/v4h322Uln8TkPVm8hQFtxNvVdGSlQCIlII3AosBP4AZorI\namPMWGPMjbY0a4AFwG/Az8CrIhIeY9hl4KEt1hzBLkEYDLbj7StTsejQAZcVwGUl3EqgJHnbO82R\nK0u59mdy882OsOXLwe5W11+eNne+GAOzZ/svY9w4a19YCHYf68Y4ulPPOMO/nHfeaa3rcOfccx2K\npKzjcyW4+w0rkyeX/h0dN67s5VWUKeEBjQmIyOfgupRWRKa6nT8JPBk80bxT2v5bT4T67OXhlMGw\nv3w5xcYGQx4F/C+Ui+Tq3kAwJjQL/QKtJErzXOyVnLO8JVV8xpRcRln9qVeUirCslOfDpkaNilF/\nVLqf4KefynFx/B64oSfbEuDzeF82pQOja1dPA2bVbdZPMHn0UfjxR+9xpVEC4VYUX34J11wDDz0E\nP/wQ3LwHDbImEvjDGGuR3vz5pXv/3K23fv65NVUXHNZiU1MdZfTqZX0pg/XxY8de5rJlribCfeGu\nLHxN3Y0EpXl+K310IpTkHtaZQLr9wkGlUwLlYnxDmtRdQp8s2LSjfP4C7rjDYRJZKT8pKb59D1dk\nJdCvn7XYKykJTj89uHnHxAQ2kSA6OvAKxf58WrRwDevf3zHgbF/o5jxoHhXlKMPb13tammXWwh8D\nB8KECa5hzgol0pRGCdj/++7XXHtt4HmUxrFTKKl0SqDMf/Iax4guhO1PW6fbCtOCJpMdu7vG0nwN\nKP4ZMsTxxwrktw90oK6idi0Fil3+wYMdYYFUYoH4o7YrAV+rvcva4nW3BuuO3UFRpEhKKv01vXu7\njqlUxu6tSihyGZlUixyb96vVtC/VtNB27fzH2y2Xdu1qveSdO2vXULD44AP4z3+s40BMWJflj1wZ\niYqy7tf9y9qOu6c3O7Vrl7wo7correPbbrP29ufubcFaad9zf9d685/tjrtzoWBin2Ul4ugau+su\na+9r9t/558OkSaGTKRxUOiVQni+4rbaFQqU1EeGt2VaagTUlOFSU5nNFpayD075sF9mxd9nY8y+P\nD/VAVoaX9fpgYi/Hvvf3hV/W515R6o1KpwRKTcpfkGFongMn7oN4jlBA4B2Rv/ziMBnsjPOAVkX5\nMas6Z50FmZmRlqLi0quXq/npQFixAl54wXf8L784+vq7d4fp063VzWXF33+lfXvv4aecAuvWWcfl\nmYXlq79+/PiSr7WbKl+82HoHnRVW377w5JPWtF17uPuKcm+OiEpSvuGi0imBUle4t1t9OR+/aXUW\nH6V0E5q7d/futlG/SsNPVBScfXakpai4lOX5dO5sDcr7ont3xziCMXDdda7mOcrTHeSOr5ZA8+aO\nLtny/O/OPNN7uDfzLr7uq0cP6xk7x0dFWT42LrzQcQ/OxiCjo70P8FeUcalKpwRK1/9m+6W+fITu\n2dmM4+lylT18uKtzEjs6S0ipKJx9dsk2lMKJ88prYxw+GLzha9Da+eu/Rg1Xp0C+8FZO/fre09or\nY2cfEiedVLa1D6efbtmxcub220ufTzipdErAl60Tr9Sz2pDyvdWefZmb/aUukbffdsyVtiNSsf50\nSmBUlK+wYJOebhmpC3ffufO5s6e13bsdU2dFrGmm7vaG7JVkrVpWGvuMJ2/98TExjsFh+wC2naef\ndqzC9ubjOjHRdWqn+/RU51X/7dpZZt59PUdf70/v3rBjh3VsN5Nml8tXHrfcYu3PP99KN2eO97xD\nRaVTAqV6uW9rz6NfOE7ziCtTmc4vS0Wa16yUHf0dg4O3r/c4t79Z3br+8yiNyQnnvEtrqsK94rbX\nJf7eBV9llPYjwv2ZOFOegfZgUHWVQIb1K02wreIsj7/OCy90OAc57zxrcEip3OjvGBxeew3WrHEN\ne/11a/X3+vXW+XvvWXtfFef99wc22Pzzzw5rqytX+p4eC6X7WOzb1/e7MHGid9ni4ix5AuXmm70P\nav/+u6dJ/HC3UqumEuj8li2xtfsVLx35pSAqyuHZKSqq+vkDqIro7xgckpI8zXEnJ1vdIvauF/eB\nZ/dKLj7eu79pd3r2dFzbqVPZjNZ5qz/8vQv+ZPPnb8Gd2FjvRiY7dnRMPInULMNgOZUJGz4fVIan\n+mz6QBaQSnoZnDN07lzyIjFFUQLj4ovh6qut4wEDHAssy4O/L+YePaBZM9jm7vkkwOvdueoqOHo0\n8PRgTQv9+GPP8FNPtcY2fZXfsaM1iL03TE56q44ScCL2OOQ9BGBZwDpCAGvlbbzwAtx6qzV/WlGU\n4DDXyQPJOefAkiWhLe/kk2HrVus4GFZor7zScyC6JPr08W5obulS77LY67bWrS33ueHqFqp03UH5\n+UBMLlwxFO6PhfazIcPw4FfwvzfqctaLL9oUgMVvBNDOdEIXfilVgcr8Hgdq+qO0awbcK9WGDR3u\nRsOBv/UYEDmTJwEpAWPMAGPMGmPMOmOMx/pZY8zZxpgDxphlti201jQmJMIpM6l3rACuuhyASd/B\nhVsO8c2e/3NJeiolzyl1XkRSmf88ilIVeOYZ+MvDMa0njRoF1mJf68OF+K+/eg5qh5L//te7j2mA\nv/+GadM8w8eMCalIQADdQcaYKOAFoB+wHVhijJlj8ybmzLci4sPuYHAoKLD2B544TqLNn+/1l8Ab\nTk3NFXTmeW7jDUYHnO9JJ8F331nHqgQUJbLUqROYtVMIzPKo3fibuxVTu9+EcJGU5Pq179wy8eWq\n1G6uIpQE0hLoAawXkc0iUgDMBC71ki7kPVgnnADEHi5WAOCqAAxCV1aUSgG4k57umAmkKJWV888P\nvn+Diop9UZY/Wrb0nMVUGeje3XLPGkoCUQLNgCyn8622MHd6G2NWGGM+NcaERH9t2gTUs9p2awmO\nZ3d3/6ydOjnWBChKZaVbt+B7OquouK/i98amTZ7mHCJNIAO/rVvDnyH21h6sgeGlQAsR6YLVdfRJ\nCenLTGrTjwDoxlKXBWDlWQymXUCKooSbkgaKw0UgU0S3AU7O6GhuCytGRA47Hc83xrxkjEkREQ9X\n7hlObbf09HTS09NLJXDj2L9YkxjHkRyr0zCRAxwvx0xXZwUQzkEiRVGqL5s2+R+TyMzMBDJ5+umS\nzW6Ul0BqzyVAW2NMS2AHcDUw1DmBMaaRiOyyHfcAjDcFAK5KoCw0PVzE2hqtis8Pkliu/JypjH2G\niqJUPkoyOml9HKdz113WorcH3G1LBJESlYCIFBpjbgUWYnUfTROR1caYsVa0vAoMNsbcDBQAR4Gr\nfOdYPhoX7WVXVICOZH3Qv7/DEuFpp0G9enD4sP9rFEWp/Nx/P2zYEGkpAqNdu/B0GQXUjyIinwMn\nuoVNdTp+EXgxuKJ5J6nxN+zPPxn2lD2PuXMtrz4ffQSpqdbmzbGEoihVi+uui7QEgWP3phZqKt2K\n4aQjMRzIPrXkhH4IxjJyRVHCQ5MmgaVLLl8HQbWl0tkOGv6bMM2UzS8AwOefq2tIRaks7NxpOYMp\niS1brL5zpfRUupZAs9zj9MtbWubrnVfmaUtAUSo2jRr5d8hiJzXV1QOZEjiV7rH9nRjNTTH/KfP1\n9q+FgQNLZw9cURSlKmIkjCuljDFSnvKMgf1xhnaFG9lX0KrU169Zo9NAFUWpfBhjEJGQ9F1UmjGB\nZ54BTBG18+FQUcMy5aHdP4qiKK5Umu6gu+4CE32E2CLIJzDPzMHwXqQoilKVqTRKACAm5iB50RCo\nwdKKZjBKURSlolFplIAxULPGQfKjyt6no91BiqIorlQqJRAbfYi86MBrcq30FUVR/FNplABAzehD\n5EUHLrIqAUVRFP9UKiUQG32Y/HKsCFGloCiK4kqFVwIbN8LBg9ZxzejD5EVFB3yte6WvSkBRFMWV\nCr9OoE0buPpqKCqCuuYAh2qU3fBP8+ZBFExRFKUKUOFbAgAHDlj7RJPNwQCUQFqaZ9j48Zb5aEVR\nFMVBpVACdhJr/0VOnWMlprOblHV2y6ZdQYqiKJ4EpASMMQOMMWuMMeuMMff6SdfdGFNgjLk8eCI6\nSCo6TE5hA4/wuXNdz+32gQK1Q64oilJdKVEJGGOigBeA/sDJwFBjTHsf6R4DFgRbyN27rX1y4RGy\nvSiB+PiS89CWgKIoiieBDAz3ANaLyGYAY8xM4FJgjVu624BZQPegSggsW2btk4sOkh2V4BLXq5dl\nEvrWW+GFF2DiRLjmGjjjDCv+wQfh6FG44YZgS6UoilL5CUQJNAOynM63YimGYowxTYFBInKOMcYl\nLpgkFx3izyhHH8/WrQ7/AM8/b212OnSw9pMmhUoaRVGUyk+wBoafBZzHCsrd+fLmm55dOClygP1F\njYrP1U2koihK+QikJbANaOF03twW5sxpwExjjAHqAxcYYwpExG3IFjIyMoqP09PTSU9P91roRx95\nhtU7foR9pmnxuSoBRVGqIpmZmWRmZoalrBI9ixljooG1QD9gB/ALMFREVvtIPx2YJyIfe4kL2LPY\nxRfD//7nGra0Xm1ujH6epbuvB+DQIahTJ6DsFEVRKi0R9SwmIoXGmFuBhVjdR9NEZLUxZqwVLa+6\nXxICOQFIOZ7LPtOQFi2gfXuoXTtUJSmKolQPAjIbISKfAye6hU31kfb6IMjlOaWzxjHqHYX90or1\nf0DDsnmYVBRFUZyoNCuGY5JXUzcfDh48OdKiKIqiVBkqjRIY2+B225HRhV+KoihBosIqAfeK/qSi\n9cXHAY4tK4qiKCVQYZWAO5dtzGFxfJtIi6EoilKlqPD+BOwsalqX+YeuhNxIS6IoilJ1qDQtgXqF\nh9l3zHIUoGMCiqIowaFyKAFTROvDR9l0tFukJVEURalSVFglsMbJRml0rR20yIG/Cy2rcDowrCiK\nEhwqrBJYt85x3CrhR3bEx5JHHAANPF0KKIqiKGWgwioBZ9rF/cL6WpbPyEGDdExAURQlWFQKJdAj\nJpNNNRoDlgMZRVEUJTiUaEU0qIWVwoqo89e+YPglqT49D+xh/Xpo2zZEAiqKolRAQmlFtEK2BL78\n0vV8dzyMjXsc0K4gRVGUYFIhWwLOFX3tS67i8NwPqBV1gGNFiRQVqSJQFKV6EcqWQIVXAjmxhoR8\nMDY3BTo9VFGU6ka16w4qpu9EEvJhY+34SEuiKIpSJQlICRhjBhhj1hhj1hlj7vUSf4kxZqUxZrkx\n5hdjTJ9gCHd26iMADCr8XwkpFUVRlLIQiI/hKGAdlo/h7cAS4GoRWeOUJl5Ecm3HpwAfiEgHL3mV\nqjtoaeMoTt0pxV1BoN1BiqJUPyLdHdQDWC8im0WkAJgJXOqcwK4AbNQBisov2XFO3SmcGftpubNS\nFEVRvBOIEmgGZDmdb7WFuWCMGWSMWQ3MA8rsZzgvz9o3TvoRgJX5Z5Q1K0VRFKUEguZPQEQ+AT4x\nxpwBPASc5y1dRkZG8XF6ejrp6eku8XGWeSA6pk4l5wgcyksIloiKoiiVgszMTDIzM8NSViBjAr2A\nDBEZYDufAIiIPO7nmg1AdxHZ7xZe4piAfTxgSb26nLbvsMt4AOiYgKIo1Y9IjwksAdoaY1oaY2KB\nq4G5bgKmOR2fCsS6K4DS0vHAYV5N7leeLBRFUZQSKLE7SEQKjTG3AguxlMY0EVltjBlrRcurwBXG\nmBFAPnAUuLJcUtXeTY1ceOVQRrmyURRFUfxToVYMi0BUFJgmi8nd3YvkwlyOUcsjjaIoSnUi0t1B\nYWPcOGufFr+YvbFxHgpAURRFCS4VSgksW2btO6VOZVnD2MgKoyiKUg2oMEpg+3b47jvruFl+DluK\n2kRWIEVRlGpAhVEC8+Y5jpsfNGw7dFrkhFEURakmVBgl4Eyzgmy25p8YaTEURVGqPBVGCRTP+jGF\ntH46Z2AAAAucSURBVDuUy4Zj2hJQFEUJNRVGCRRTeycd9sDqos4eUfd6GLFWFEVRykOFUwJ1664F\nDAdIjrQoiqIoVZ4KowTs3UGpNf9gW5yuD1AURQkHFUYJ2GlZYy2b45IiLYaiKEq1oMIpgSbRm9kR\nVT/SYiiKolQLKp4SMNvZTtNIi6EoilItqHhKoGgPOwpbRloMRVGUakHFUwIFB9iR39YlbPt2a68W\nRBVFUYJLhVACe/bA6tXWcdO8XHYcO8klvkmTCAilKIpSDQiaj+Hy0LCh7SD2MKkHC8nKP8lrOhMS\na9qKoijVl4BaAsaYAcaYNcaYdcYYj3W7xphhxpiVtu17Y8wpZREmus5mGubCDtvA8KefusZrd5Ci\nKEpwKVEJGGOigBeA/sDJwFBjTHu3ZH8DZ4lIZ+Ah4LVABRg2zHHcsPYq9sfGcJyYQC9XFEVRykEg\nLYEewHoR2SwiBcBM4FLnBCLys4jk2E5/BpoFKsD77zuOm8StYXvNOoFeqiiKopSTQJRAMyDL6Xwr\n/iv5McB8X5H+unQuPfYNXXOyAxBJURRFCQZBHRg2xpwDjALO8JXmmmsyOOEE6zg9PR1IL447ViuX\nGfW6wT7P68aNg+HDgymtoihKxSQzM5PMzMywlGWkhNFWY0wvIENEBtjOJwAiIo+7pesEfAQMEJEN\nPvKS//5XuPJK5zDH8ctprVh1+Dxe3PUaIvDZZ3DhhTogrChK9cYYg4iEZH5kIN1BS4C2xpiWxphY\n4GpgrpuALbAUwHBfCsAbtWu7njc+foBt+e0CvVxRFEUpJyV2B4lIoTHmVmAhltKYJiKrjTFjrWh5\nFbgfSAFeMsYYoEBEepSUd26u63mzY0fYnudYI6DrAhRFUUJLQGMCIvI5cKJb2FSn4xuAG8olSXQ+\nLQ4fZ0tul3JloyiKogRO2M1G+Orfj6u9kcRjsMvJgmjv3nDzzWESTFEUpRpSYZRA8/jlbI+viTiJ\nlJQEL70UJsEURVGqIRXCgBxA47i1xQvFim0JKYqiKCGlwiiBJjEb2BltuZXcsiXCwiiKolQTKkx3\nUOPoLewwjQGoWTOMAimKolRjItYScFcGTZN/LFYCiqIoSnioMN1BrQ8WsCmvTBaoFUVRlDISdiUw\nbJi1CMylJRB7iFYHYGPOWeEWR1EUpVpTIVoC0c1+oOc22Fx4YsmJFUVRlKARMSUwZYrjuE/MFwDs\nQJ0JK4qihJOIKYFx4xzH7ZIX8m1iC8AQHR0piRRFUaofFaI7qPehTbxfuy8A/fpFWBhFUZRqRAVQ\nAsLoPw+zeJ/lMSaqAkikKIpSXYh4ldsi+SsAluedA6DdQYqiKGEk4krgkrqvsiylDmA5D9CWgKIo\nSviIeJV7++EP2VHLYSdClYCiKEr4CKjKNcYMMMasMcasM8bc6yX+RGPMj8aYY8aYuwIuPSGLQgP/\n3PtxcZB2BymKooSPEpWAMSYKeAHoD5wMDDXGtHdLtg+4DfhPaQo/97wutN8Hf+adXhzWWM0HKYqi\nhI1AWgI9gPUisllECoCZwKXOCURkr4gsBY4HXHLNg3zx0X4ACp28XDZvHnAOiqIoSjkJRAk0A7Kc\nzrfawspFSgurC6g+u13CfZmaVhRFUYJPQI7mg0sGAClH3uWpxGT25TRwib3ttvBLpCiKUpHIzMwk\nMzMzLGUZKeHT2xjTC8gQkQG28wmAiMjjXtJOBg6JyNM+8hKwyvu1ieH3oq6M2rWsOD41Vb2KKYqi\nuGOMQURMKPIOpDtoCdDWGNPSGBMLXA3M9ZPer6CCQTB02wFv545ziTMhuUVFURTFFyW2BMCaIgo8\nh6U0ponIY8aYsVgtgleNMY2AX4G6QBFwGDhJRA675eNSmsG17FatYOPGctyNoihKFSSULYGAlEDQ\nCjNGGtwDe+pAzANHKJB4l/h27WDdurCJoyiKUikIpRII+8DwnkWvwZY+Hgrg5Zdh4MBwS6MoilK9\nCXtLALyX98sv0L172ERRFEWpNER6YDgspKZGWgJFUZTqR4VpCegiMUVRFO9Ui5aAoiiKEn4iqgTm\nzYtk6YqiKEpEu4NEoEsXWLlSu4MURVF8UaW7g+rXj7QEiqIo1ZeIK4GkpEhLoCiKUn2JuBKIjy85\njaIoihIaIjIm8OefsHUrnHce5OTAjh3Q3t1XmaIoigJUMdtBIDoIrCiKUgqq9MCwoiiKEjlUCSiK\nolRjwq4EYmLCXaKiKIrii4CUgDFmgDFmjTFmnTHmXh9pphhj1htjVhhjuvjKa9OmMkqqKIqiBJ0S\nlYAxJgp4AegPnAwMNca0d0tzAZAmIu2AscArvvJr2rRc8lYZwuVEujKgz8KBPgsH+izCQyAtgR7A\nehHZLCIFwEzgUrc0lwJvA4jIYiDR5nJS8YG+4A70WTjQZ+FAn0V4CEQJNAOynM632sL8pdnmJY2i\nKIpSwdDZQYqiKNWYEheLGWN6ARkiMsB2PgEQEXncKc0rwNci8l/b+RrgbBHZ5ZaXLhNTFEUpA5F0\nNL8EaGuMaQnsAK4GhrqlmQv8H/Bfm9I44K4AIHQ3oSiKopSNEpWAiBQaY24FFmJ1H00TkdXGmLFW\ntLwqIp8ZYwYaY/4CjgCjQiu2oiiKEgzCajtIURRFqViEbWA4kAVnlRljTHNjzCJjzB/GmN+NMbfb\nwpONMQuNMWuNMQuMMYlO1/zTtsButTHmfKfwU40xv9me1bORuJ9gYIyJMsYsM8bMtZ1Xy2dhjEk0\nxnxou7c/jDE9q/GzGGeMWWW7jxnGmNjq8iyMMdOMMbuMMb85hQXt3m3Pcqbtmp+MMS0CEkxEQr5h\nKZu/gJZADLACaB+OssO1AY2BLrbjOsBaoD3wODDeFn4v8Jjt+CRgOVaXXCvb87G3zBYD3W3HnwH9\nI31/ZXwm44B3gbm282r5LIA3gVG24xpAYnV8FkBT4G8g1nb+X2BkdXkWwBlAF+A3p7Cg3TtwM/CS\n7fgqYGYgcoWrJRDIgrNKjYjsFJEVtuPDwGqgOdZ9vmVL9hYwyHZ8CdaPdFxENgHrgR7GmMZAXRFZ\nYkv3ttM1lQZjTHNgIPC6U3C1exbGmATgTBGZDmC7xxyq4bOwEQ3UNsbUAGphrSmqFs9CRL4Hst2C\ng3nvznnNAvoFIle4lEAgC86qDMaYVlga/2egkdhmSonITqChLZmvBXbNsJ6Pncr6rJ4B/gE4DzpV\nx2fRmv9v59xZowyiMPy8iBdUJKawilfEVoggYgrBiAiCnSCIt19gG238BRYpbCy0CCiIUbLpJKQW\nERXFWAgpjAmuSDClRTgWM7t+CiYLfm7IzvtUu2eZ/ea8O3DmzDmz8E3S/Xw0dlfSVgrUIiIWgNvA\nJ5JfSxExRYFaVNhVo+/tMRGxDHyX1L/aBHxZrGYkbSdF4es5I/iz8t7zlXhJZ4FmzoxWagvueS1I\n6fwgcCciBkndcyOUuS76SLvVvaSjoW2SLlKgFitQp+8dteR3KwjMA9UixUC29RQ5xX0MjEXERDY3\nW/+jlFO5r9k+D+yuDG9p8jf7emIIOCdpFngInJQ0BnwpUIvPwFxEvMzvx0lBocR1cQqYjYjFvFN9\nChynTC1a1Ol7+zNJG4AdEbG42gS6FQTaF84kbSJdOGt06dnd5B4wExGjFVsDuJpfXwEmKvYLuaK/\nHzgIvMgp4ZKko5IEXK6MWRdExM2I2BMRB0i/9XREXAImKU+LJjAn6VA2DQPvKXBdkI6Bjknakn0Y\nBmYoSwvx+w69Tt8b+TsAzgPTHc2oi5XxM6SOmY/AyFpU5/+zf0PAMqnz6TXwKvvcD0xl358BfZUx\nN0hV/w/A6Yr9CPAuazW61r79oy4n+NUdVKQWwGHSRugN8ITUHVSqFreyX29JRcyNpWgBPAAWgB+k\ngHgN2FmX78Bm4FG2Pwf2dTIvXxYzxpiCcWHYGGMKxkHAGGMKxkHAGGMKxkHAGGMKxkHAGGMKxkHA\nGGMKxkHAGGMKxkHAGGMK5ifvG+CJTx5tHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113778278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
