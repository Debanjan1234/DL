{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#         y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#         y *= 2.0 # uni-var/ std\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y) # non-linearity/ activation\n",
    "#             y -= l.sigmoid(0.0) # zero-centered/ mean\n",
    "#             y *= 2.0 # uni-var/ std\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "#         dy =  dy @ self.W_fixed[2].T # done\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "#             dy =  dy @ self.W_fixed[2].T # done\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "        dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.2534, acc-0.1600, valid loss-2.2816, acc-0.1628, test loss-2.2830, acc-0.1658\n",
      "Iter-20, train loss-2.2842, acc-0.1800, valid loss-2.2815, acc-0.1634, test loss-2.2829, acc-0.1660\n",
      "Iter-30, train loss-2.2901, acc-0.1200, valid loss-2.2813, acc-0.1632, test loss-2.2827, acc-0.1664\n",
      "Iter-40, train loss-2.3006, acc-0.1200, valid loss-2.2812, acc-0.1636, test loss-2.2826, acc-0.1665\n",
      "Iter-50, train loss-2.2786, acc-0.1600, valid loss-2.2811, acc-0.1638, test loss-2.2825, acc-0.1670\n",
      "Iter-60, train loss-2.2676, acc-0.2600, valid loss-2.2809, acc-0.1638, test loss-2.2824, acc-0.1669\n",
      "Iter-70, train loss-2.2843, acc-0.1000, valid loss-2.2808, acc-0.1640, test loss-2.2823, acc-0.1672\n",
      "Iter-80, train loss-2.2771, acc-0.2200, valid loss-2.2807, acc-0.1640, test loss-2.2821, acc-0.1675\n",
      "Iter-90, train loss-2.2723, acc-0.1800, valid loss-2.2806, acc-0.1650, test loss-2.2820, acc-0.1679\n",
      "Iter-100, train loss-2.2542, acc-0.2800, valid loss-2.2805, acc-0.1658, test loss-2.2819, acc-0.1675\n",
      "Iter-110, train loss-2.2789, acc-0.1600, valid loss-2.2803, acc-0.1660, test loss-2.2818, acc-0.1676\n",
      "Iter-120, train loss-2.2925, acc-0.1200, valid loss-2.2802, acc-0.1664, test loss-2.2816, acc-0.1681\n",
      "Iter-130, train loss-2.2920, acc-0.1400, valid loss-2.2801, acc-0.1666, test loss-2.2815, acc-0.1685\n",
      "Iter-140, train loss-2.2566, acc-0.1800, valid loss-2.2799, acc-0.1672, test loss-2.2814, acc-0.1688\n",
      "Iter-150, train loss-2.2643, acc-0.1800, valid loss-2.2798, acc-0.1678, test loss-2.2813, acc-0.1692\n",
      "Iter-160, train loss-2.2801, acc-0.1600, valid loss-2.2797, acc-0.1680, test loss-2.2811, acc-0.1696\n",
      "Iter-170, train loss-2.2953, acc-0.1400, valid loss-2.2796, acc-0.1680, test loss-2.2810, acc-0.1697\n",
      "Iter-180, train loss-2.2571, acc-0.2200, valid loss-2.2795, acc-0.1682, test loss-2.2809, acc-0.1699\n",
      "Iter-190, train loss-2.2591, acc-0.2600, valid loss-2.2793, acc-0.1682, test loss-2.2808, acc-0.1702\n",
      "Iter-200, train loss-2.2854, acc-0.2200, valid loss-2.2792, acc-0.1686, test loss-2.2807, acc-0.1705\n",
      "Iter-210, train loss-2.2661, acc-0.2000, valid loss-2.2791, acc-0.1686, test loss-2.2805, acc-0.1709\n",
      "Iter-220, train loss-2.2932, acc-0.1000, valid loss-2.2790, acc-0.1684, test loss-2.2804, acc-0.1712\n",
      "Iter-230, train loss-2.2732, acc-0.2000, valid loss-2.2788, acc-0.1690, test loss-2.2803, acc-0.1709\n",
      "Iter-240, train loss-2.2649, acc-0.1400, valid loss-2.2787, acc-0.1686, test loss-2.2802, acc-0.1712\n",
      "Iter-250, train loss-2.2627, acc-0.1800, valid loss-2.2786, acc-0.1684, test loss-2.2800, acc-0.1713\n",
      "Iter-260, train loss-2.2901, acc-0.0600, valid loss-2.2785, acc-0.1684, test loss-2.2799, acc-0.1716\n",
      "Iter-270, train loss-2.2695, acc-0.2200, valid loss-2.2783, acc-0.1690, test loss-2.2798, acc-0.1718\n",
      "Iter-280, train loss-2.2739, acc-0.0800, valid loss-2.2782, acc-0.1692, test loss-2.2797, acc-0.1722\n",
      "Iter-290, train loss-2.2852, acc-0.1000, valid loss-2.2781, acc-0.1694, test loss-2.2795, acc-0.1724\n",
      "Iter-300, train loss-2.2742, acc-0.1800, valid loss-2.2780, acc-0.1690, test loss-2.2794, acc-0.1724\n",
      "Iter-310, train loss-2.2788, acc-0.1400, valid loss-2.2778, acc-0.1698, test loss-2.2793, acc-0.1727\n",
      "Iter-320, train loss-2.2819, acc-0.1600, valid loss-2.2777, acc-0.1704, test loss-2.2792, acc-0.1729\n",
      "Iter-330, train loss-2.2771, acc-0.1600, valid loss-2.2776, acc-0.1706, test loss-2.2791, acc-0.1732\n",
      "Iter-340, train loss-2.2882, acc-0.1400, valid loss-2.2774, acc-0.1708, test loss-2.2789, acc-0.1732\n",
      "Iter-350, train loss-2.2778, acc-0.1800, valid loss-2.2773, acc-0.1710, test loss-2.2788, acc-0.1736\n",
      "Iter-360, train loss-2.2638, acc-0.2800, valid loss-2.2772, acc-0.1718, test loss-2.2787, acc-0.1737\n",
      "Iter-370, train loss-2.2735, acc-0.1600, valid loss-2.2771, acc-0.1722, test loss-2.2786, acc-0.1737\n",
      "Iter-380, train loss-2.2724, acc-0.2400, valid loss-2.2769, acc-0.1724, test loss-2.2784, acc-0.1739\n",
      "Iter-390, train loss-2.2913, acc-0.0800, valid loss-2.2768, acc-0.1722, test loss-2.2783, acc-0.1743\n",
      "Iter-400, train loss-2.2629, acc-0.2400, valid loss-2.2767, acc-0.1734, test loss-2.2782, acc-0.1744\n",
      "Iter-410, train loss-2.2626, acc-0.2600, valid loss-2.2766, acc-0.1742, test loss-2.2781, acc-0.1747\n",
      "Iter-420, train loss-2.2647, acc-0.2000, valid loss-2.2765, acc-0.1740, test loss-2.2780, acc-0.1756\n",
      "Iter-430, train loss-2.2746, acc-0.1600, valid loss-2.2763, acc-0.1746, test loss-2.2778, acc-0.1757\n",
      "Iter-440, train loss-2.2810, acc-0.1600, valid loss-2.2762, acc-0.1748, test loss-2.2777, acc-0.1762\n",
      "Iter-450, train loss-2.2756, acc-0.2000, valid loss-2.2761, acc-0.1752, test loss-2.2776, acc-0.1763\n",
      "Iter-460, train loss-2.2686, acc-0.1400, valid loss-2.2760, acc-0.1750, test loss-2.2775, acc-0.1764\n",
      "Iter-470, train loss-2.2895, acc-0.1200, valid loss-2.2758, acc-0.1756, test loss-2.2773, acc-0.1768\n",
      "Iter-480, train loss-2.2647, acc-0.1800, valid loss-2.2757, acc-0.1756, test loss-2.2772, acc-0.1772\n",
      "Iter-490, train loss-2.2857, acc-0.1600, valid loss-2.2756, acc-0.1762, test loss-2.2771, acc-0.1774\n",
      "Iter-500, train loss-2.2831, acc-0.1000, valid loss-2.2755, acc-0.1760, test loss-2.2770, acc-0.1773\n",
      "Iter-510, train loss-2.2746, acc-0.1800, valid loss-2.2753, acc-0.1764, test loss-2.2769, acc-0.1776\n",
      "Iter-520, train loss-2.2751, acc-0.2200, valid loss-2.2752, acc-0.1768, test loss-2.2767, acc-0.1776\n",
      "Iter-530, train loss-2.2511, acc-0.2400, valid loss-2.2751, acc-0.1766, test loss-2.2766, acc-0.1782\n",
      "Iter-540, train loss-2.2783, acc-0.2000, valid loss-2.2750, acc-0.1772, test loss-2.2765, acc-0.1786\n",
      "Iter-550, train loss-2.2535, acc-0.1800, valid loss-2.2749, acc-0.1774, test loss-2.2764, acc-0.1787\n",
      "Iter-560, train loss-2.2862, acc-0.1600, valid loss-2.2747, acc-0.1772, test loss-2.2762, acc-0.1792\n",
      "Iter-570, train loss-2.2683, acc-0.1400, valid loss-2.2746, acc-0.1774, test loss-2.2761, acc-0.1794\n",
      "Iter-580, train loss-2.2744, acc-0.2600, valid loss-2.2745, acc-0.1778, test loss-2.2760, acc-0.1795\n",
      "Iter-590, train loss-2.2677, acc-0.1800, valid loss-2.2744, acc-0.1778, test loss-2.2759, acc-0.1798\n",
      "Iter-600, train loss-2.2716, acc-0.1800, valid loss-2.2742, acc-0.1780, test loss-2.2758, acc-0.1798\n",
      "Iter-610, train loss-2.3025, acc-0.1000, valid loss-2.2741, acc-0.1776, test loss-2.2756, acc-0.1800\n",
      "Iter-620, train loss-2.2596, acc-0.2400, valid loss-2.2740, acc-0.1776, test loss-2.2755, acc-0.1805\n",
      "Iter-630, train loss-2.2730, acc-0.1400, valid loss-2.2739, acc-0.1784, test loss-2.2754, acc-0.1806\n",
      "Iter-640, train loss-2.2652, acc-0.2000, valid loss-2.2738, acc-0.1794, test loss-2.2753, acc-0.1811\n",
      "Iter-650, train loss-2.2776, acc-0.2200, valid loss-2.2736, acc-0.1804, test loss-2.2752, acc-0.1811\n",
      "Iter-660, train loss-2.2659, acc-0.1800, valid loss-2.2735, acc-0.1804, test loss-2.2750, acc-0.1810\n",
      "Iter-670, train loss-2.2835, acc-0.1600, valid loss-2.2734, acc-0.1802, test loss-2.2749, acc-0.1812\n",
      "Iter-680, train loss-2.2641, acc-0.1400, valid loss-2.2733, acc-0.1804, test loss-2.2748, acc-0.1815\n",
      "Iter-690, train loss-2.2967, acc-0.1200, valid loss-2.2731, acc-0.1806, test loss-2.2747, acc-0.1815\n",
      "Iter-700, train loss-2.2606, acc-0.2600, valid loss-2.2730, acc-0.1810, test loss-2.2745, acc-0.1817\n",
      "Iter-710, train loss-2.2863, acc-0.1600, valid loss-2.2729, acc-0.1808, test loss-2.2744, acc-0.1816\n",
      "Iter-720, train loss-2.2713, acc-0.2200, valid loss-2.2728, acc-0.1810, test loss-2.2743, acc-0.1820\n",
      "Iter-730, train loss-2.2638, acc-0.1400, valid loss-2.2727, acc-0.1814, test loss-2.2742, acc-0.1827\n",
      "Iter-740, train loss-2.2510, acc-0.2400, valid loss-2.2725, acc-0.1814, test loss-2.2741, acc-0.1828\n",
      "Iter-750, train loss-2.2756, acc-0.1600, valid loss-2.2724, acc-0.1812, test loss-2.2739, acc-0.1826\n",
      "Iter-760, train loss-2.2620, acc-0.2000, valid loss-2.2723, acc-0.1814, test loss-2.2738, acc-0.1826\n",
      "Iter-770, train loss-2.2403, acc-0.3200, valid loss-2.2722, acc-0.1818, test loss-2.2737, acc-0.1830\n",
      "Iter-780, train loss-2.2707, acc-0.1600, valid loss-2.2720, acc-0.1818, test loss-2.2736, acc-0.1834\n",
      "Iter-790, train loss-2.2640, acc-0.2200, valid loss-2.2719, acc-0.1820, test loss-2.2734, acc-0.1842\n",
      "Iter-800, train loss-2.2685, acc-0.1600, valid loss-2.2718, acc-0.1826, test loss-2.2733, acc-0.1843\n",
      "Iter-810, train loss-2.2732, acc-0.1400, valid loss-2.2717, acc-0.1824, test loss-2.2732, acc-0.1845\n",
      "Iter-820, train loss-2.2781, acc-0.2000, valid loss-2.2715, acc-0.1836, test loss-2.2731, acc-0.1848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.2750, acc-0.1000, valid loss-2.2714, acc-0.1836, test loss-2.2730, acc-0.1846\n",
      "Iter-840, train loss-2.2797, acc-0.1400, valid loss-2.2713, acc-0.1844, test loss-2.2728, acc-0.1850\n",
      "Iter-850, train loss-2.2837, acc-0.1200, valid loss-2.2712, acc-0.1850, test loss-2.2727, acc-0.1859\n",
      "Iter-860, train loss-2.2676, acc-0.2200, valid loss-2.2710, acc-0.1852, test loss-2.2726, acc-0.1858\n",
      "Iter-870, train loss-2.2583, acc-0.2600, valid loss-2.2709, acc-0.1852, test loss-2.2724, acc-0.1859\n",
      "Iter-880, train loss-2.2775, acc-0.1600, valid loss-2.2708, acc-0.1854, test loss-2.2723, acc-0.1863\n",
      "Iter-890, train loss-2.2850, acc-0.1600, valid loss-2.2707, acc-0.1858, test loss-2.2722, acc-0.1862\n",
      "Iter-900, train loss-2.2726, acc-0.1800, valid loss-2.2705, acc-0.1858, test loss-2.2721, acc-0.1861\n",
      "Iter-910, train loss-2.2696, acc-0.1600, valid loss-2.2704, acc-0.1862, test loss-2.2720, acc-0.1868\n",
      "Iter-920, train loss-2.2855, acc-0.2000, valid loss-2.2703, acc-0.1868, test loss-2.2718, acc-0.1869\n",
      "Iter-930, train loss-2.2857, acc-0.0600, valid loss-2.2702, acc-0.1876, test loss-2.2717, acc-0.1872\n",
      "Iter-940, train loss-2.2858, acc-0.1400, valid loss-2.2701, acc-0.1872, test loss-2.2716, acc-0.1873\n",
      "Iter-950, train loss-2.2797, acc-0.1400, valid loss-2.2699, acc-0.1868, test loss-2.2715, acc-0.1874\n",
      "Iter-960, train loss-2.2695, acc-0.1600, valid loss-2.2698, acc-0.1872, test loss-2.2714, acc-0.1878\n",
      "Iter-970, train loss-2.2478, acc-0.3000, valid loss-2.2697, acc-0.1870, test loss-2.2712, acc-0.1877\n",
      "Iter-980, train loss-2.2798, acc-0.1200, valid loss-2.2696, acc-0.1870, test loss-2.2711, acc-0.1878\n",
      "Iter-990, train loss-2.2794, acc-0.2000, valid loss-2.2695, acc-0.1874, test loss-2.2710, acc-0.1880\n",
      "Iter-1000, train loss-2.2565, acc-0.2200, valid loss-2.2693, acc-0.1874, test loss-2.2709, acc-0.1881\n",
      "Iter-1010, train loss-2.2667, acc-0.1800, valid loss-2.2692, acc-0.1874, test loss-2.2708, acc-0.1885\n",
      "Iter-1020, train loss-2.2698, acc-0.2600, valid loss-2.2691, acc-0.1882, test loss-2.2706, acc-0.1886\n",
      "Iter-1030, train loss-2.2852, acc-0.1200, valid loss-2.2690, acc-0.1882, test loss-2.2705, acc-0.1888\n",
      "Iter-1040, train loss-2.2482, acc-0.2400, valid loss-2.2689, acc-0.1882, test loss-2.2704, acc-0.1890\n",
      "Iter-1050, train loss-2.2793, acc-0.1400, valid loss-2.2687, acc-0.1882, test loss-2.2703, acc-0.1892\n",
      "Iter-1060, train loss-2.2549, acc-0.2400, valid loss-2.2686, acc-0.1884, test loss-2.2702, acc-0.1897\n",
      "Iter-1070, train loss-2.2649, acc-0.2600, valid loss-2.2685, acc-0.1882, test loss-2.2700, acc-0.1905\n",
      "Iter-1080, train loss-2.2581, acc-0.1600, valid loss-2.2684, acc-0.1880, test loss-2.2699, acc-0.1909\n",
      "Iter-1090, train loss-2.2752, acc-0.2000, valid loss-2.2682, acc-0.1876, test loss-2.2698, acc-0.1911\n",
      "Iter-1100, train loss-2.2670, acc-0.1200, valid loss-2.2681, acc-0.1876, test loss-2.2697, acc-0.1913\n",
      "Iter-1110, train loss-2.2802, acc-0.1400, valid loss-2.2680, acc-0.1876, test loss-2.2696, acc-0.1920\n",
      "Iter-1120, train loss-2.2687, acc-0.2400, valid loss-2.2679, acc-0.1880, test loss-2.2694, acc-0.1922\n",
      "Iter-1130, train loss-2.2617, acc-0.1600, valid loss-2.2677, acc-0.1888, test loss-2.2693, acc-0.1926\n",
      "Iter-1140, train loss-2.2556, acc-0.2200, valid loss-2.2676, acc-0.1888, test loss-2.2692, acc-0.1927\n",
      "Iter-1150, train loss-2.2861, acc-0.1400, valid loss-2.2675, acc-0.1892, test loss-2.2691, acc-0.1932\n",
      "Iter-1160, train loss-2.2664, acc-0.1600, valid loss-2.2674, acc-0.1894, test loss-2.2690, acc-0.1933\n",
      "Iter-1170, train loss-2.2475, acc-0.2000, valid loss-2.2673, acc-0.1900, test loss-2.2688, acc-0.1939\n",
      "Iter-1180, train loss-2.2588, acc-0.2400, valid loss-2.2671, acc-0.1908, test loss-2.2687, acc-0.1943\n",
      "Iter-1190, train loss-2.2628, acc-0.2000, valid loss-2.2670, acc-0.1910, test loss-2.2686, acc-0.1947\n",
      "Iter-1200, train loss-2.3044, acc-0.1200, valid loss-2.2669, acc-0.1912, test loss-2.2685, acc-0.1948\n",
      "Iter-1210, train loss-2.2710, acc-0.2600, valid loss-2.2668, acc-0.1912, test loss-2.2684, acc-0.1952\n",
      "Iter-1220, train loss-2.2675, acc-0.1200, valid loss-2.2667, acc-0.1928, test loss-2.2682, acc-0.1950\n",
      "Iter-1230, train loss-2.2653, acc-0.1400, valid loss-2.2665, acc-0.1934, test loss-2.2681, acc-0.1956\n",
      "Iter-1240, train loss-2.2661, acc-0.2600, valid loss-2.2664, acc-0.1934, test loss-2.2680, acc-0.1956\n",
      "Iter-1250, train loss-2.3024, acc-0.1400, valid loss-2.2663, acc-0.1938, test loss-2.2679, acc-0.1961\n",
      "Iter-1260, train loss-2.2959, acc-0.1600, valid loss-2.2662, acc-0.1938, test loss-2.2678, acc-0.1963\n",
      "Iter-1270, train loss-2.2700, acc-0.1800, valid loss-2.2660, acc-0.1944, test loss-2.2676, acc-0.1969\n",
      "Iter-1280, train loss-2.2714, acc-0.1600, valid loss-2.2659, acc-0.1948, test loss-2.2675, acc-0.1970\n",
      "Iter-1290, train loss-2.2431, acc-0.3800, valid loss-2.2658, acc-0.1946, test loss-2.2674, acc-0.1976\n",
      "Iter-1300, train loss-2.2604, acc-0.2000, valid loss-2.2657, acc-0.1946, test loss-2.2673, acc-0.1976\n",
      "Iter-1310, train loss-2.2580, acc-0.2800, valid loss-2.2656, acc-0.1950, test loss-2.2672, acc-0.1978\n",
      "Iter-1320, train loss-2.2563, acc-0.2200, valid loss-2.2655, acc-0.1950, test loss-2.2671, acc-0.1981\n",
      "Iter-1330, train loss-2.2804, acc-0.1400, valid loss-2.2653, acc-0.1952, test loss-2.2669, acc-0.1985\n",
      "Iter-1340, train loss-2.2620, acc-0.2400, valid loss-2.2652, acc-0.1956, test loss-2.2668, acc-0.1990\n",
      "Iter-1350, train loss-2.2873, acc-0.1800, valid loss-2.2651, acc-0.1958, test loss-2.2667, acc-0.1988\n",
      "Iter-1360, train loss-2.2541, acc-0.2200, valid loss-2.2649, acc-0.1964, test loss-2.2666, acc-0.1992\n",
      "Iter-1370, train loss-2.2749, acc-0.2200, valid loss-2.2648, acc-0.1962, test loss-2.2664, acc-0.1994\n",
      "Iter-1380, train loss-2.2648, acc-0.2000, valid loss-2.2647, acc-0.1962, test loss-2.2663, acc-0.1994\n",
      "Iter-1390, train loss-2.2507, acc-0.3000, valid loss-2.2646, acc-0.1960, test loss-2.2662, acc-0.1993\n",
      "Iter-1400, train loss-2.2500, acc-0.2800, valid loss-2.2645, acc-0.1962, test loss-2.2661, acc-0.1996\n",
      "Iter-1410, train loss-2.2626, acc-0.1800, valid loss-2.2644, acc-0.1962, test loss-2.2660, acc-0.1997\n",
      "Iter-1420, train loss-2.2490, acc-0.2600, valid loss-2.2642, acc-0.1962, test loss-2.2659, acc-0.1999\n",
      "Iter-1430, train loss-2.2521, acc-0.2400, valid loss-2.2641, acc-0.1962, test loss-2.2658, acc-0.2007\n",
      "Iter-1440, train loss-2.2582, acc-0.2200, valid loss-2.2640, acc-0.1960, test loss-2.2656, acc-0.2009\n",
      "Iter-1450, train loss-2.2581, acc-0.2000, valid loss-2.2639, acc-0.1964, test loss-2.2655, acc-0.2016\n",
      "Iter-1460, train loss-2.2628, acc-0.2200, valid loss-2.2638, acc-0.1960, test loss-2.2654, acc-0.2019\n",
      "Iter-1470, train loss-2.2809, acc-0.1000, valid loss-2.2637, acc-0.1960, test loss-2.2653, acc-0.2022\n",
      "Iter-1480, train loss-2.2588, acc-0.2600, valid loss-2.2635, acc-0.1960, test loss-2.2652, acc-0.2021\n",
      "Iter-1490, train loss-2.2576, acc-0.2400, valid loss-2.2634, acc-0.1970, test loss-2.2650, acc-0.2029\n",
      "Iter-1500, train loss-2.2678, acc-0.1400, valid loss-2.2633, acc-0.1976, test loss-2.2649, acc-0.2033\n",
      "Iter-1510, train loss-2.2823, acc-0.1800, valid loss-2.2632, acc-0.1976, test loss-2.2648, acc-0.2037\n",
      "Iter-1520, train loss-2.2568, acc-0.2200, valid loss-2.2631, acc-0.1982, test loss-2.2647, acc-0.2037\n",
      "Iter-1530, train loss-2.2384, acc-0.2800, valid loss-2.2629, acc-0.1980, test loss-2.2646, acc-0.2038\n",
      "Iter-1540, train loss-2.2514, acc-0.3400, valid loss-2.2628, acc-0.1984, test loss-2.2644, acc-0.2044\n",
      "Iter-1550, train loss-2.2642, acc-0.1800, valid loss-2.2627, acc-0.1988, test loss-2.2643, acc-0.2047\n",
      "Iter-1560, train loss-2.2653, acc-0.1600, valid loss-2.2626, acc-0.1992, test loss-2.2642, acc-0.2054\n",
      "Iter-1570, train loss-2.2826, acc-0.2000, valid loss-2.2625, acc-0.1990, test loss-2.2641, acc-0.2055\n",
      "Iter-1580, train loss-2.2628, acc-0.2600, valid loss-2.2623, acc-0.1996, test loss-2.2640, acc-0.2055\n",
      "Iter-1590, train loss-2.2785, acc-0.2600, valid loss-2.2622, acc-0.2000, test loss-2.2639, acc-0.2057\n",
      "Iter-1600, train loss-2.2651, acc-0.2200, valid loss-2.2621, acc-0.2002, test loss-2.2637, acc-0.2061\n",
      "Iter-1610, train loss-2.2547, acc-0.2400, valid loss-2.2620, acc-0.2002, test loss-2.2636, acc-0.2064\n",
      "Iter-1620, train loss-2.2662, acc-0.2000, valid loss-2.2619, acc-0.2012, test loss-2.2635, acc-0.2067\n",
      "Iter-1630, train loss-2.2710, acc-0.1200, valid loss-2.2618, acc-0.2012, test loss-2.2634, acc-0.2069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.2567, acc-0.1800, valid loss-2.2616, acc-0.2014, test loss-2.2633, acc-0.2069\n",
      "Iter-1650, train loss-2.2917, acc-0.1600, valid loss-2.2615, acc-0.2014, test loss-2.2632, acc-0.2072\n",
      "Iter-1660, train loss-2.2694, acc-0.1200, valid loss-2.2614, acc-0.2018, test loss-2.2630, acc-0.2076\n",
      "Iter-1670, train loss-2.2637, acc-0.2200, valid loss-2.2613, acc-0.2022, test loss-2.2629, acc-0.2078\n",
      "Iter-1680, train loss-2.2577, acc-0.1800, valid loss-2.2611, acc-0.2022, test loss-2.2628, acc-0.2080\n",
      "Iter-1690, train loss-2.2415, acc-0.2000, valid loss-2.2610, acc-0.2024, test loss-2.2627, acc-0.2083\n",
      "Iter-1700, train loss-2.2650, acc-0.2400, valid loss-2.2609, acc-0.2026, test loss-2.2626, acc-0.2087\n",
      "Iter-1710, train loss-2.2690, acc-0.2200, valid loss-2.2608, acc-0.2024, test loss-2.2624, acc-0.2090\n",
      "Iter-1720, train loss-2.2641, acc-0.2200, valid loss-2.2607, acc-0.2028, test loss-2.2623, acc-0.2093\n",
      "Iter-1730, train loss-2.2337, acc-0.4000, valid loss-2.2605, acc-0.2030, test loss-2.2622, acc-0.2095\n",
      "Iter-1740, train loss-2.2666, acc-0.1000, valid loss-2.2604, acc-0.2036, test loss-2.2621, acc-0.2096\n",
      "Iter-1750, train loss-2.2662, acc-0.1000, valid loss-2.2603, acc-0.2044, test loss-2.2620, acc-0.2097\n",
      "Iter-1760, train loss-2.2598, acc-0.2600, valid loss-2.2602, acc-0.2044, test loss-2.2619, acc-0.2097\n",
      "Iter-1770, train loss-2.2635, acc-0.1800, valid loss-2.2601, acc-0.2042, test loss-2.2617, acc-0.2101\n",
      "Iter-1780, train loss-2.2623, acc-0.2000, valid loss-2.2600, acc-0.2042, test loss-2.2616, acc-0.2099\n",
      "Iter-1790, train loss-2.2624, acc-0.2600, valid loss-2.2599, acc-0.2044, test loss-2.2615, acc-0.2101\n",
      "Iter-1800, train loss-2.2397, acc-0.2600, valid loss-2.2597, acc-0.2046, test loss-2.2614, acc-0.2104\n",
      "Iter-1810, train loss-2.2632, acc-0.2400, valid loss-2.2596, acc-0.2046, test loss-2.2613, acc-0.2108\n",
      "Iter-1820, train loss-2.2760, acc-0.0600, valid loss-2.2595, acc-0.2052, test loss-2.2612, acc-0.2111\n",
      "Iter-1830, train loss-2.2628, acc-0.1800, valid loss-2.2594, acc-0.2052, test loss-2.2610, acc-0.2110\n",
      "Iter-1840, train loss-2.2580, acc-0.2600, valid loss-2.2593, acc-0.2052, test loss-2.2609, acc-0.2111\n",
      "Iter-1850, train loss-2.2688, acc-0.1600, valid loss-2.2591, acc-0.2056, test loss-2.2608, acc-0.2112\n",
      "Iter-1860, train loss-2.2522, acc-0.2600, valid loss-2.2590, acc-0.2066, test loss-2.2607, acc-0.2117\n",
      "Iter-1870, train loss-2.2611, acc-0.1400, valid loss-2.2589, acc-0.2066, test loss-2.2606, acc-0.2124\n",
      "Iter-1880, train loss-2.2591, acc-0.2000, valid loss-2.2588, acc-0.2074, test loss-2.2605, acc-0.2124\n",
      "Iter-1890, train loss-2.2587, acc-0.2200, valid loss-2.2587, acc-0.2082, test loss-2.2604, acc-0.2129\n",
      "Iter-1900, train loss-2.2656, acc-0.1400, valid loss-2.2586, acc-0.2088, test loss-2.2602, acc-0.2132\n",
      "Iter-1910, train loss-2.2674, acc-0.2000, valid loss-2.2584, acc-0.2094, test loss-2.2601, acc-0.2134\n",
      "Iter-1920, train loss-2.2899, acc-0.1800, valid loss-2.2583, acc-0.2100, test loss-2.2600, acc-0.2139\n",
      "Iter-1930, train loss-2.2745, acc-0.1600, valid loss-2.2582, acc-0.2100, test loss-2.2599, acc-0.2140\n",
      "Iter-1940, train loss-2.2464, acc-0.2400, valid loss-2.2581, acc-0.2102, test loss-2.2598, acc-0.2142\n",
      "Iter-1950, train loss-2.2505, acc-0.2200, valid loss-2.2580, acc-0.2112, test loss-2.2597, acc-0.2142\n",
      "Iter-1960, train loss-2.2860, acc-0.1400, valid loss-2.2579, acc-0.2112, test loss-2.2595, acc-0.2150\n",
      "Iter-1970, train loss-2.2531, acc-0.2200, valid loss-2.2577, acc-0.2118, test loss-2.2594, acc-0.2148\n",
      "Iter-1980, train loss-2.2575, acc-0.1400, valid loss-2.2576, acc-0.2122, test loss-2.2593, acc-0.2155\n",
      "Iter-1990, train loss-2.2593, acc-0.2000, valid loss-2.2575, acc-0.2128, test loss-2.2592, acc-0.2154\n",
      "Iter-2000, train loss-2.2818, acc-0.1600, valid loss-2.2574, acc-0.2126, test loss-2.2591, acc-0.2155\n",
      "Iter-2010, train loss-2.2656, acc-0.2200, valid loss-2.2573, acc-0.2128, test loss-2.2590, acc-0.2161\n",
      "Iter-2020, train loss-2.2655, acc-0.1800, valid loss-2.2572, acc-0.2130, test loss-2.2588, acc-0.2163\n",
      "Iter-2030, train loss-2.2565, acc-0.1600, valid loss-2.2570, acc-0.2136, test loss-2.2587, acc-0.2165\n",
      "Iter-2040, train loss-2.2545, acc-0.1600, valid loss-2.2569, acc-0.2134, test loss-2.2586, acc-0.2164\n",
      "Iter-2050, train loss-2.2517, acc-0.2400, valid loss-2.2568, acc-0.2142, test loss-2.2585, acc-0.2171\n",
      "Iter-2060, train loss-2.2817, acc-0.2200, valid loss-2.2567, acc-0.2140, test loss-2.2584, acc-0.2174\n",
      "Iter-2070, train loss-2.2370, acc-0.4000, valid loss-2.2566, acc-0.2146, test loss-2.2583, acc-0.2177\n",
      "Iter-2080, train loss-2.2806, acc-0.1200, valid loss-2.2565, acc-0.2152, test loss-2.2582, acc-0.2182\n",
      "Iter-2090, train loss-2.2499, acc-0.2200, valid loss-2.2564, acc-0.2154, test loss-2.2580, acc-0.2186\n",
      "Iter-2100, train loss-2.2620, acc-0.2400, valid loss-2.2562, acc-0.2154, test loss-2.2579, acc-0.2192\n",
      "Iter-2110, train loss-2.2625, acc-0.1400, valid loss-2.2561, acc-0.2158, test loss-2.2578, acc-0.2193\n",
      "Iter-2120, train loss-2.2601, acc-0.1800, valid loss-2.2560, acc-0.2162, test loss-2.2577, acc-0.2197\n",
      "Iter-2130, train loss-2.2459, acc-0.1600, valid loss-2.2559, acc-0.2164, test loss-2.2576, acc-0.2200\n",
      "Iter-2140, train loss-2.2378, acc-0.3200, valid loss-2.2558, acc-0.2166, test loss-2.2575, acc-0.2205\n",
      "Iter-2150, train loss-2.2588, acc-0.2200, valid loss-2.2556, acc-0.2166, test loss-2.2573, acc-0.2209\n",
      "Iter-2160, train loss-2.2595, acc-0.2000, valid loss-2.2555, acc-0.2172, test loss-2.2572, acc-0.2213\n",
      "Iter-2170, train loss-2.2353, acc-0.2600, valid loss-2.2554, acc-0.2176, test loss-2.2571, acc-0.2215\n",
      "Iter-2180, train loss-2.2646, acc-0.2000, valid loss-2.2553, acc-0.2176, test loss-2.2570, acc-0.2216\n",
      "Iter-2190, train loss-2.2428, acc-0.2200, valid loss-2.2552, acc-0.2180, test loss-2.2569, acc-0.2218\n",
      "Iter-2200, train loss-2.2727, acc-0.1400, valid loss-2.2551, acc-0.2182, test loss-2.2567, acc-0.2223\n",
      "Iter-2210, train loss-2.2499, acc-0.2800, valid loss-2.2549, acc-0.2194, test loss-2.2566, acc-0.2228\n",
      "Iter-2220, train loss-2.2731, acc-0.2000, valid loss-2.2548, acc-0.2204, test loss-2.2565, acc-0.2228\n",
      "Iter-2230, train loss-2.2823, acc-0.1200, valid loss-2.2547, acc-0.2202, test loss-2.2564, acc-0.2232\n",
      "Iter-2240, train loss-2.2546, acc-0.3000, valid loss-2.2546, acc-0.2202, test loss-2.2563, acc-0.2235\n",
      "Iter-2250, train loss-2.2536, acc-0.2200, valid loss-2.2545, acc-0.2202, test loss-2.2562, acc-0.2238\n",
      "Iter-2260, train loss-2.2473, acc-0.2400, valid loss-2.2544, acc-0.2202, test loss-2.2560, acc-0.2236\n",
      "Iter-2270, train loss-2.2391, acc-0.2000, valid loss-2.2543, acc-0.2210, test loss-2.2559, acc-0.2241\n",
      "Iter-2280, train loss-2.2648, acc-0.1600, valid loss-2.2541, acc-0.2214, test loss-2.2558, acc-0.2242\n",
      "Iter-2290, train loss-2.2435, acc-0.1800, valid loss-2.2540, acc-0.2218, test loss-2.2557, acc-0.2248\n",
      "Iter-2300, train loss-2.2503, acc-0.2800, valid loss-2.2539, acc-0.2220, test loss-2.2556, acc-0.2246\n",
      "Iter-2310, train loss-2.2590, acc-0.1400, valid loss-2.2538, acc-0.2226, test loss-2.2555, acc-0.2246\n",
      "Iter-2320, train loss-2.2311, acc-0.3000, valid loss-2.2537, acc-0.2232, test loss-2.2553, acc-0.2246\n",
      "Iter-2330, train loss-2.2611, acc-0.2000, valid loss-2.2535, acc-0.2240, test loss-2.2552, acc-0.2258\n",
      "Iter-2340, train loss-2.2502, acc-0.2400, valid loss-2.2534, acc-0.2248, test loss-2.2551, acc-0.2252\n",
      "Iter-2350, train loss-2.2523, acc-0.2800, valid loss-2.2533, acc-0.2244, test loss-2.2550, acc-0.2255\n",
      "Iter-2360, train loss-2.2333, acc-0.3200, valid loss-2.2532, acc-0.2248, test loss-2.2549, acc-0.2257\n",
      "Iter-2370, train loss-2.2460, acc-0.3200, valid loss-2.2531, acc-0.2252, test loss-2.2548, acc-0.2261\n",
      "Iter-2380, train loss-2.2610, acc-0.1800, valid loss-2.2530, acc-0.2254, test loss-2.2547, acc-0.2264\n",
      "Iter-2390, train loss-2.2411, acc-0.2600, valid loss-2.2528, acc-0.2260, test loss-2.2545, acc-0.2270\n",
      "Iter-2400, train loss-2.2649, acc-0.1600, valid loss-2.2527, acc-0.2264, test loss-2.2544, acc-0.2273\n",
      "Iter-2410, train loss-2.2503, acc-0.3000, valid loss-2.2526, acc-0.2268, test loss-2.2543, acc-0.2277\n",
      "Iter-2420, train loss-2.2431, acc-0.2600, valid loss-2.2525, acc-0.2268, test loss-2.2542, acc-0.2277\n",
      "Iter-2430, train loss-2.2357, acc-0.3400, valid loss-2.2524, acc-0.2270, test loss-2.2541, acc-0.2279\n",
      "Iter-2440, train loss-2.2450, acc-0.2000, valid loss-2.2523, acc-0.2270, test loss-2.2540, acc-0.2285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.2754, acc-0.2200, valid loss-2.2521, acc-0.2270, test loss-2.2538, acc-0.2288\n",
      "Iter-2460, train loss-2.2667, acc-0.2000, valid loss-2.2520, acc-0.2272, test loss-2.2537, acc-0.2287\n",
      "Iter-2470, train loss-2.2581, acc-0.1200, valid loss-2.2519, acc-0.2272, test loss-2.2536, acc-0.2289\n",
      "Iter-2480, train loss-2.2641, acc-0.2400, valid loss-2.2518, acc-0.2272, test loss-2.2535, acc-0.2292\n",
      "Iter-2490, train loss-2.2557, acc-0.2000, valid loss-2.2517, acc-0.2274, test loss-2.2534, acc-0.2291\n",
      "Iter-2500, train loss-2.2560, acc-0.2400, valid loss-2.2516, acc-0.2280, test loss-2.2533, acc-0.2293\n",
      "Iter-2510, train loss-2.2538, acc-0.2600, valid loss-2.2515, acc-0.2284, test loss-2.2532, acc-0.2295\n",
      "Iter-2520, train loss-2.2455, acc-0.2000, valid loss-2.2513, acc-0.2284, test loss-2.2530, acc-0.2302\n",
      "Iter-2530, train loss-2.2603, acc-0.2000, valid loss-2.2512, acc-0.2294, test loss-2.2529, acc-0.2308\n",
      "Iter-2540, train loss-2.2469, acc-0.2400, valid loss-2.2511, acc-0.2296, test loss-2.2528, acc-0.2312\n",
      "Iter-2550, train loss-2.2543, acc-0.2200, valid loss-2.2510, acc-0.2300, test loss-2.2527, acc-0.2317\n",
      "Iter-2560, train loss-2.2692, acc-0.1000, valid loss-2.2509, acc-0.2304, test loss-2.2526, acc-0.2319\n",
      "Iter-2570, train loss-2.2633, acc-0.2000, valid loss-2.2507, acc-0.2306, test loss-2.2524, acc-0.2325\n",
      "Iter-2580, train loss-2.2664, acc-0.2000, valid loss-2.2506, acc-0.2308, test loss-2.2523, acc-0.2326\n",
      "Iter-2590, train loss-2.2452, acc-0.1800, valid loss-2.2505, acc-0.2310, test loss-2.2522, acc-0.2330\n",
      "Iter-2600, train loss-2.2512, acc-0.2400, valid loss-2.2504, acc-0.2312, test loss-2.2521, acc-0.2331\n",
      "Iter-2610, train loss-2.2463, acc-0.2600, valid loss-2.2503, acc-0.2318, test loss-2.2520, acc-0.2338\n",
      "Iter-2620, train loss-2.2464, acc-0.3200, valid loss-2.2501, acc-0.2318, test loss-2.2519, acc-0.2341\n",
      "Iter-2630, train loss-2.2520, acc-0.2600, valid loss-2.2500, acc-0.2324, test loss-2.2517, acc-0.2345\n",
      "Iter-2640, train loss-2.2604, acc-0.2200, valid loss-2.2499, acc-0.2328, test loss-2.2516, acc-0.2347\n",
      "Iter-2650, train loss-2.2400, acc-0.3200, valid loss-2.2498, acc-0.2328, test loss-2.2515, acc-0.2348\n",
      "Iter-2660, train loss-2.2551, acc-0.2800, valid loss-2.2497, acc-0.2326, test loss-2.2514, acc-0.2356\n",
      "Iter-2670, train loss-2.2413, acc-0.2600, valid loss-2.2495, acc-0.2334, test loss-2.2512, acc-0.2353\n",
      "Iter-2680, train loss-2.2697, acc-0.2400, valid loss-2.2494, acc-0.2336, test loss-2.2511, acc-0.2357\n",
      "Iter-2690, train loss-2.2300, acc-0.2400, valid loss-2.2493, acc-0.2338, test loss-2.2510, acc-0.2359\n",
      "Iter-2700, train loss-2.2733, acc-0.1600, valid loss-2.2492, acc-0.2334, test loss-2.2509, acc-0.2361\n",
      "Iter-2710, train loss-2.2457, acc-0.2200, valid loss-2.2491, acc-0.2340, test loss-2.2508, acc-0.2361\n",
      "Iter-2720, train loss-2.2645, acc-0.2600, valid loss-2.2490, acc-0.2338, test loss-2.2507, acc-0.2360\n",
      "Iter-2730, train loss-2.2338, acc-0.3000, valid loss-2.2489, acc-0.2338, test loss-2.2506, acc-0.2360\n",
      "Iter-2740, train loss-2.2305, acc-0.2400, valid loss-2.2487, acc-0.2340, test loss-2.2505, acc-0.2367\n",
      "Iter-2750, train loss-2.2501, acc-0.1800, valid loss-2.2486, acc-0.2340, test loss-2.2503, acc-0.2371\n",
      "Iter-2760, train loss-2.2444, acc-0.2200, valid loss-2.2485, acc-0.2340, test loss-2.2502, acc-0.2372\n",
      "Iter-2770, train loss-2.2422, acc-0.3600, valid loss-2.2484, acc-0.2346, test loss-2.2501, acc-0.2375\n",
      "Iter-2780, train loss-2.2248, acc-0.3600, valid loss-2.2483, acc-0.2348, test loss-2.2500, acc-0.2374\n",
      "Iter-2790, train loss-2.2354, acc-0.3200, valid loss-2.2482, acc-0.2358, test loss-2.2499, acc-0.2375\n",
      "Iter-2800, train loss-2.2385, acc-0.3400, valid loss-2.2481, acc-0.2358, test loss-2.2498, acc-0.2378\n",
      "Iter-2810, train loss-2.2551, acc-0.2400, valid loss-2.2480, acc-0.2364, test loss-2.2497, acc-0.2378\n",
      "Iter-2820, train loss-2.2246, acc-0.3400, valid loss-2.2478, acc-0.2360, test loss-2.2495, acc-0.2377\n",
      "Iter-2830, train loss-2.2161, acc-0.3000, valid loss-2.2477, acc-0.2366, test loss-2.2494, acc-0.2381\n",
      "Iter-2840, train loss-2.2643, acc-0.1200, valid loss-2.2476, acc-0.2368, test loss-2.2493, acc-0.2385\n",
      "Iter-2850, train loss-2.2433, acc-0.3000, valid loss-2.2475, acc-0.2368, test loss-2.2492, acc-0.2391\n",
      "Iter-2860, train loss-2.2453, acc-0.2000, valid loss-2.2474, acc-0.2372, test loss-2.2491, acc-0.2390\n",
      "Iter-2870, train loss-2.2536, acc-0.1800, valid loss-2.2473, acc-0.2372, test loss-2.2490, acc-0.2394\n",
      "Iter-2880, train loss-2.2328, acc-0.2200, valid loss-2.2472, acc-0.2374, test loss-2.2489, acc-0.2400\n",
      "Iter-2890, train loss-2.2219, acc-0.4200, valid loss-2.2470, acc-0.2378, test loss-2.2487, acc-0.2401\n",
      "Iter-2900, train loss-2.2397, acc-0.3000, valid loss-2.2469, acc-0.2376, test loss-2.2486, acc-0.2402\n",
      "Iter-2910, train loss-2.2511, acc-0.2600, valid loss-2.2468, acc-0.2380, test loss-2.2485, acc-0.2408\n",
      "Iter-2920, train loss-2.2406, acc-0.2400, valid loss-2.2467, acc-0.2378, test loss-2.2484, acc-0.2409\n",
      "Iter-2930, train loss-2.2537, acc-0.2200, valid loss-2.2466, acc-0.2386, test loss-2.2483, acc-0.2416\n",
      "Iter-2940, train loss-2.2397, acc-0.2200, valid loss-2.2465, acc-0.2390, test loss-2.2482, acc-0.2415\n",
      "Iter-2950, train loss-2.2376, acc-0.2800, valid loss-2.2464, acc-0.2394, test loss-2.2481, acc-0.2422\n",
      "Iter-2960, train loss-2.2479, acc-0.3000, valid loss-2.2462, acc-0.2398, test loss-2.2480, acc-0.2423\n",
      "Iter-2970, train loss-2.2695, acc-0.1800, valid loss-2.2461, acc-0.2400, test loss-2.2478, acc-0.2425\n",
      "Iter-2980, train loss-2.2497, acc-0.3000, valid loss-2.2460, acc-0.2402, test loss-2.2477, acc-0.2431\n",
      "Iter-2990, train loss-2.2493, acc-0.2000, valid loss-2.2459, acc-0.2404, test loss-2.2476, acc-0.2432\n",
      "Iter-3000, train loss-2.2573, acc-0.2400, valid loss-2.2458, acc-0.2402, test loss-2.2475, acc-0.2431\n",
      "Iter-3010, train loss-2.2476, acc-0.2200, valid loss-2.2457, acc-0.2412, test loss-2.2474, acc-0.2434\n",
      "Iter-3020, train loss-2.2371, acc-0.3000, valid loss-2.2456, acc-0.2408, test loss-2.2473, acc-0.2435\n",
      "Iter-3030, train loss-2.2539, acc-0.2400, valid loss-2.2455, acc-0.2410, test loss-2.2472, acc-0.2438\n",
      "Iter-3040, train loss-2.2360, acc-0.3000, valid loss-2.2454, acc-0.2412, test loss-2.2471, acc-0.2445\n",
      "Iter-3050, train loss-2.2175, acc-0.4000, valid loss-2.2452, acc-0.2414, test loss-2.2469, acc-0.2442\n",
      "Iter-3060, train loss-2.2607, acc-0.2400, valid loss-2.2451, acc-0.2414, test loss-2.2468, acc-0.2444\n",
      "Iter-3070, train loss-2.2306, acc-0.3200, valid loss-2.2450, acc-0.2418, test loss-2.2467, acc-0.2456\n",
      "Iter-3080, train loss-2.2610, acc-0.2200, valid loss-2.2449, acc-0.2426, test loss-2.2466, acc-0.2456\n",
      "Iter-3090, train loss-2.2612, acc-0.2000, valid loss-2.2448, acc-0.2426, test loss-2.2465, acc-0.2460\n",
      "Iter-3100, train loss-2.2398, acc-0.3200, valid loss-2.2447, acc-0.2430, test loss-2.2464, acc-0.2459\n",
      "Iter-3110, train loss-2.2338, acc-0.1600, valid loss-2.2445, acc-0.2430, test loss-2.2463, acc-0.2463\n",
      "Iter-3120, train loss-2.2444, acc-0.2600, valid loss-2.2444, acc-0.2432, test loss-2.2462, acc-0.2460\n",
      "Iter-3130, train loss-2.2464, acc-0.2400, valid loss-2.2443, acc-0.2438, test loss-2.2460, acc-0.2464\n",
      "Iter-3140, train loss-2.2459, acc-0.3200, valid loss-2.2442, acc-0.2440, test loss-2.2459, acc-0.2467\n",
      "Iter-3150, train loss-2.2606, acc-0.2600, valid loss-2.2441, acc-0.2442, test loss-2.2458, acc-0.2472\n",
      "Iter-3160, train loss-2.2451, acc-0.2200, valid loss-2.2440, acc-0.2446, test loss-2.2457, acc-0.2474\n",
      "Iter-3170, train loss-2.2604, acc-0.2400, valid loss-2.2438, acc-0.2450, test loss-2.2456, acc-0.2477\n",
      "Iter-3180, train loss-2.2321, acc-0.2200, valid loss-2.2437, acc-0.2452, test loss-2.2454, acc-0.2481\n",
      "Iter-3190, train loss-2.2336, acc-0.3600, valid loss-2.2436, acc-0.2454, test loss-2.2453, acc-0.2484\n",
      "Iter-3200, train loss-2.2728, acc-0.2200, valid loss-2.2435, acc-0.2460, test loss-2.2452, acc-0.2495\n",
      "Iter-3210, train loss-2.2619, acc-0.2000, valid loss-2.2434, acc-0.2466, test loss-2.2451, acc-0.2497\n",
      "Iter-3220, train loss-2.2366, acc-0.3000, valid loss-2.2432, acc-0.2462, test loss-2.2450, acc-0.2497\n",
      "Iter-3230, train loss-2.2277, acc-0.4000, valid loss-2.2431, acc-0.2464, test loss-2.2449, acc-0.2496\n",
      "Iter-3240, train loss-2.2589, acc-0.2200, valid loss-2.2430, acc-0.2470, test loss-2.2447, acc-0.2503\n",
      "Iter-3250, train loss-2.2727, acc-0.0800, valid loss-2.2429, acc-0.2470, test loss-2.2446, acc-0.2504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.2313, acc-0.3200, valid loss-2.2428, acc-0.2474, test loss-2.2445, acc-0.2501\n",
      "Iter-3270, train loss-2.2332, acc-0.2000, valid loss-2.2427, acc-0.2480, test loss-2.2444, acc-0.2504\n",
      "Iter-3280, train loss-2.2411, acc-0.3000, valid loss-2.2426, acc-0.2482, test loss-2.2443, acc-0.2505\n",
      "Iter-3290, train loss-2.2070, acc-0.3800, valid loss-2.2424, acc-0.2484, test loss-2.2442, acc-0.2507\n",
      "Iter-3300, train loss-2.2273, acc-0.3400, valid loss-2.2423, acc-0.2494, test loss-2.2441, acc-0.2511\n",
      "Iter-3310, train loss-2.2582, acc-0.2000, valid loss-2.2422, acc-0.2498, test loss-2.2440, acc-0.2514\n",
      "Iter-3320, train loss-2.2588, acc-0.2400, valid loss-2.2421, acc-0.2496, test loss-2.2438, acc-0.2515\n",
      "Iter-3330, train loss-2.2489, acc-0.2800, valid loss-2.2420, acc-0.2500, test loss-2.2437, acc-0.2515\n",
      "Iter-3340, train loss-2.2448, acc-0.2200, valid loss-2.2419, acc-0.2500, test loss-2.2436, acc-0.2516\n",
      "Iter-3350, train loss-2.2431, acc-0.1600, valid loss-2.2418, acc-0.2504, test loss-2.2435, acc-0.2518\n",
      "Iter-3360, train loss-2.2500, acc-0.2200, valid loss-2.2416, acc-0.2508, test loss-2.2434, acc-0.2525\n",
      "Iter-3370, train loss-2.2455, acc-0.2600, valid loss-2.2415, acc-0.2510, test loss-2.2433, acc-0.2531\n",
      "Iter-3380, train loss-2.2338, acc-0.2000, valid loss-2.2414, acc-0.2514, test loss-2.2432, acc-0.2534\n",
      "Iter-3390, train loss-2.2555, acc-0.2000, valid loss-2.2413, acc-0.2516, test loss-2.2431, acc-0.2532\n",
      "Iter-3400, train loss-2.2478, acc-0.2000, valid loss-2.2412, acc-0.2516, test loss-2.2429, acc-0.2534\n",
      "Iter-3410, train loss-2.2368, acc-0.3400, valid loss-2.2411, acc-0.2520, test loss-2.2428, acc-0.2534\n",
      "Iter-3420, train loss-2.2228, acc-0.2600, valid loss-2.2410, acc-0.2520, test loss-2.2427, acc-0.2539\n",
      "Iter-3430, train loss-2.2454, acc-0.1800, valid loss-2.2408, acc-0.2524, test loss-2.2426, acc-0.2541\n",
      "Iter-3440, train loss-2.2559, acc-0.1600, valid loss-2.2407, acc-0.2526, test loss-2.2425, acc-0.2543\n",
      "Iter-3450, train loss-2.2398, acc-0.3600, valid loss-2.2406, acc-0.2534, test loss-2.2424, acc-0.2554\n",
      "Iter-3460, train loss-2.2170, acc-0.2800, valid loss-2.2405, acc-0.2530, test loss-2.2422, acc-0.2558\n",
      "Iter-3470, train loss-2.2279, acc-0.2200, valid loss-2.2404, acc-0.2532, test loss-2.2421, acc-0.2559\n",
      "Iter-3480, train loss-2.2483, acc-0.3000, valid loss-2.2403, acc-0.2534, test loss-2.2420, acc-0.2561\n",
      "Iter-3490, train loss-2.2494, acc-0.2000, valid loss-2.2401, acc-0.2538, test loss-2.2419, acc-0.2567\n",
      "Iter-3500, train loss-2.2358, acc-0.2600, valid loss-2.2400, acc-0.2538, test loss-2.2418, acc-0.2568\n",
      "Iter-3510, train loss-2.2470, acc-0.2800, valid loss-2.2399, acc-0.2546, test loss-2.2417, acc-0.2579\n",
      "Iter-3520, train loss-2.2604, acc-0.2800, valid loss-2.2398, acc-0.2546, test loss-2.2416, acc-0.2581\n",
      "Iter-3530, train loss-2.2638, acc-0.1800, valid loss-2.2397, acc-0.2542, test loss-2.2415, acc-0.2584\n",
      "Iter-3540, train loss-2.2757, acc-0.1200, valid loss-2.2396, acc-0.2546, test loss-2.2413, acc-0.2592\n",
      "Iter-3550, train loss-2.2389, acc-0.3800, valid loss-2.2394, acc-0.2552, test loss-2.2412, acc-0.2595\n",
      "Iter-3560, train loss-2.2263, acc-0.2400, valid loss-2.2393, acc-0.2558, test loss-2.2411, acc-0.2600\n",
      "Iter-3570, train loss-2.2578, acc-0.2200, valid loss-2.2392, acc-0.2562, test loss-2.2410, acc-0.2604\n",
      "Iter-3580, train loss-2.2535, acc-0.2400, valid loss-2.2391, acc-0.2570, test loss-2.2409, acc-0.2603\n",
      "Iter-3590, train loss-2.2409, acc-0.3000, valid loss-2.2390, acc-0.2572, test loss-2.2408, acc-0.2600\n",
      "Iter-3600, train loss-2.2487, acc-0.2400, valid loss-2.2389, acc-0.2572, test loss-2.2406, acc-0.2604\n",
      "Iter-3610, train loss-2.2460, acc-0.2200, valid loss-2.2387, acc-0.2576, test loss-2.2405, acc-0.2606\n",
      "Iter-3620, train loss-2.2694, acc-0.1800, valid loss-2.2386, acc-0.2580, test loss-2.2404, acc-0.2610\n",
      "Iter-3630, train loss-2.2209, acc-0.3600, valid loss-2.2385, acc-0.2582, test loss-2.2403, acc-0.2615\n",
      "Iter-3640, train loss-2.2452, acc-0.2200, valid loss-2.2384, acc-0.2580, test loss-2.2402, acc-0.2618\n",
      "Iter-3650, train loss-2.2478, acc-0.2200, valid loss-2.2383, acc-0.2582, test loss-2.2401, acc-0.2617\n",
      "Iter-3660, train loss-2.2372, acc-0.2000, valid loss-2.2382, acc-0.2592, test loss-2.2400, acc-0.2620\n",
      "Iter-3670, train loss-2.2664, acc-0.2600, valid loss-2.2381, acc-0.2596, test loss-2.2399, acc-0.2620\n",
      "Iter-3680, train loss-2.2328, acc-0.2400, valid loss-2.2379, acc-0.2600, test loss-2.2397, acc-0.2623\n",
      "Iter-3690, train loss-2.2460, acc-0.3000, valid loss-2.2378, acc-0.2602, test loss-2.2396, acc-0.2628\n",
      "Iter-3700, train loss-2.2386, acc-0.2200, valid loss-2.2377, acc-0.2608, test loss-2.2395, acc-0.2632\n",
      "Iter-3710, train loss-2.2082, acc-0.4000, valid loss-2.2376, acc-0.2610, test loss-2.2394, acc-0.2636\n",
      "Iter-3720, train loss-2.2094, acc-0.3000, valid loss-2.2375, acc-0.2614, test loss-2.2393, acc-0.2640\n",
      "Iter-3730, train loss-2.2411, acc-0.3600, valid loss-2.2374, acc-0.2618, test loss-2.2392, acc-0.2643\n",
      "Iter-3740, train loss-2.2565, acc-0.2200, valid loss-2.2372, acc-0.2620, test loss-2.2391, acc-0.2647\n",
      "Iter-3750, train loss-2.2481, acc-0.2000, valid loss-2.2371, acc-0.2620, test loss-2.2389, acc-0.2648\n",
      "Iter-3760, train loss-2.2356, acc-0.3000, valid loss-2.2370, acc-0.2626, test loss-2.2388, acc-0.2648\n",
      "Iter-3770, train loss-2.2376, acc-0.2400, valid loss-2.2369, acc-0.2632, test loss-2.2387, acc-0.2652\n",
      "Iter-3780, train loss-2.2384, acc-0.3000, valid loss-2.2368, acc-0.2638, test loss-2.2386, acc-0.2655\n",
      "Iter-3790, train loss-2.2429, acc-0.2800, valid loss-2.2367, acc-0.2638, test loss-2.2385, acc-0.2658\n",
      "Iter-3800, train loss-2.2415, acc-0.3400, valid loss-2.2366, acc-0.2636, test loss-2.2384, acc-0.2659\n",
      "Iter-3810, train loss-2.2507, acc-0.2600, valid loss-2.2364, acc-0.2646, test loss-2.2383, acc-0.2664\n",
      "Iter-3820, train loss-2.2397, acc-0.2400, valid loss-2.2363, acc-0.2648, test loss-2.2381, acc-0.2660\n",
      "Iter-3830, train loss-2.2452, acc-0.2000, valid loss-2.2362, acc-0.2650, test loss-2.2380, acc-0.2663\n",
      "Iter-3840, train loss-2.2115, acc-0.2800, valid loss-2.2361, acc-0.2652, test loss-2.2379, acc-0.2666\n",
      "Iter-3850, train loss-2.2278, acc-0.3200, valid loss-2.2360, acc-0.2650, test loss-2.2378, acc-0.2666\n",
      "Iter-3860, train loss-2.2458, acc-0.0800, valid loss-2.2359, acc-0.2652, test loss-2.2377, acc-0.2669\n",
      "Iter-3870, train loss-2.2190, acc-0.3000, valid loss-2.2358, acc-0.2658, test loss-2.2376, acc-0.2672\n",
      "Iter-3880, train loss-2.2306, acc-0.3200, valid loss-2.2356, acc-0.2658, test loss-2.2375, acc-0.2678\n",
      "Iter-3890, train loss-2.2302, acc-0.3600, valid loss-2.2355, acc-0.2660, test loss-2.2373, acc-0.2682\n",
      "Iter-3900, train loss-2.2327, acc-0.2800, valid loss-2.2354, acc-0.2660, test loss-2.2372, acc-0.2682\n",
      "Iter-3910, train loss-2.2267, acc-0.3400, valid loss-2.2353, acc-0.2660, test loss-2.2371, acc-0.2685\n",
      "Iter-3920, train loss-2.2203, acc-0.3200, valid loss-2.2352, acc-0.2664, test loss-2.2370, acc-0.2688\n",
      "Iter-3930, train loss-2.2237, acc-0.3400, valid loss-2.2350, acc-0.2666, test loss-2.2369, acc-0.2691\n",
      "Iter-3940, train loss-2.2291, acc-0.2400, valid loss-2.2349, acc-0.2666, test loss-2.2368, acc-0.2695\n",
      "Iter-3950, train loss-2.2301, acc-0.3000, valid loss-2.2348, acc-0.2670, test loss-2.2366, acc-0.2700\n",
      "Iter-3960, train loss-2.2577, acc-0.2400, valid loss-2.2347, acc-0.2668, test loss-2.2365, acc-0.2699\n",
      "Iter-3970, train loss-2.2320, acc-0.4200, valid loss-2.2346, acc-0.2672, test loss-2.2364, acc-0.2704\n",
      "Iter-3980, train loss-2.2475, acc-0.2400, valid loss-2.2345, acc-0.2678, test loss-2.2363, acc-0.2703\n",
      "Iter-3990, train loss-2.2160, acc-0.3200, valid loss-2.2343, acc-0.2682, test loss-2.2362, acc-0.2712\n",
      "Iter-4000, train loss-2.2457, acc-0.2200, valid loss-2.2342, acc-0.2682, test loss-2.2361, acc-0.2716\n",
      "Iter-4010, train loss-2.2365, acc-0.2800, valid loss-2.2341, acc-0.2688, test loss-2.2360, acc-0.2716\n",
      "Iter-4020, train loss-2.2507, acc-0.1600, valid loss-2.2340, acc-0.2686, test loss-2.2359, acc-0.2720\n",
      "Iter-4030, train loss-2.2352, acc-0.2800, valid loss-2.2339, acc-0.2692, test loss-2.2358, acc-0.2726\n",
      "Iter-4040, train loss-2.2046, acc-0.4200, valid loss-2.2338, acc-0.2696, test loss-2.2356, acc-0.2722\n",
      "Iter-4050, train loss-2.2445, acc-0.2200, valid loss-2.2337, acc-0.2698, test loss-2.2355, acc-0.2725\n",
      "Iter-4060, train loss-2.2193, acc-0.3200, valid loss-2.2336, acc-0.2702, test loss-2.2354, acc-0.2726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.2426, acc-0.2600, valid loss-2.2334, acc-0.2712, test loss-2.2353, acc-0.2730\n",
      "Iter-4080, train loss-2.2336, acc-0.3000, valid loss-2.2333, acc-0.2710, test loss-2.2352, acc-0.2733\n",
      "Iter-4090, train loss-2.2379, acc-0.2600, valid loss-2.2332, acc-0.2712, test loss-2.2351, acc-0.2734\n",
      "Iter-4100, train loss-2.2562, acc-0.2000, valid loss-2.2331, acc-0.2712, test loss-2.2350, acc-0.2733\n",
      "Iter-4110, train loss-2.2423, acc-0.2800, valid loss-2.2330, acc-0.2726, test loss-2.2348, acc-0.2737\n",
      "Iter-4120, train loss-2.2418, acc-0.2200, valid loss-2.2329, acc-0.2732, test loss-2.2347, acc-0.2741\n",
      "Iter-4130, train loss-2.2486, acc-0.3000, valid loss-2.2327, acc-0.2742, test loss-2.2346, acc-0.2736\n",
      "Iter-4140, train loss-2.2315, acc-0.2200, valid loss-2.2326, acc-0.2744, test loss-2.2345, acc-0.2740\n",
      "Iter-4150, train loss-2.2405, acc-0.2600, valid loss-2.2325, acc-0.2746, test loss-2.2344, acc-0.2743\n",
      "Iter-4160, train loss-2.2386, acc-0.1800, valid loss-2.2324, acc-0.2750, test loss-2.2343, acc-0.2746\n",
      "Iter-4170, train loss-2.2252, acc-0.3200, valid loss-2.2323, acc-0.2758, test loss-2.2342, acc-0.2751\n",
      "Iter-4180, train loss-2.2428, acc-0.1800, valid loss-2.2322, acc-0.2762, test loss-2.2341, acc-0.2755\n",
      "Iter-4190, train loss-2.2666, acc-0.2400, valid loss-2.2321, acc-0.2768, test loss-2.2339, acc-0.2754\n",
      "Iter-4200, train loss-2.2333, acc-0.3800, valid loss-2.2320, acc-0.2768, test loss-2.2338, acc-0.2758\n",
      "Iter-4210, train loss-2.2406, acc-0.2600, valid loss-2.2318, acc-0.2774, test loss-2.2337, acc-0.2764\n",
      "Iter-4220, train loss-2.2396, acc-0.3400, valid loss-2.2317, acc-0.2776, test loss-2.2336, acc-0.2768\n",
      "Iter-4230, train loss-2.2420, acc-0.2600, valid loss-2.2316, acc-0.2778, test loss-2.2335, acc-0.2772\n",
      "Iter-4240, train loss-2.2200, acc-0.2200, valid loss-2.2315, acc-0.2780, test loss-2.2334, acc-0.2776\n",
      "Iter-4250, train loss-2.2249, acc-0.2800, valid loss-2.2314, acc-0.2782, test loss-2.2333, acc-0.2778\n",
      "Iter-4260, train loss-2.2198, acc-0.3400, valid loss-2.2313, acc-0.2786, test loss-2.2332, acc-0.2779\n",
      "Iter-4270, train loss-2.2205, acc-0.3200, valid loss-2.2312, acc-0.2784, test loss-2.2330, acc-0.2782\n",
      "Iter-4280, train loss-2.2382, acc-0.2800, valid loss-2.2310, acc-0.2784, test loss-2.2329, acc-0.2784\n",
      "Iter-4290, train loss-2.2432, acc-0.2200, valid loss-2.2309, acc-0.2790, test loss-2.2328, acc-0.2787\n",
      "Iter-4300, train loss-2.2269, acc-0.3400, valid loss-2.2308, acc-0.2790, test loss-2.2327, acc-0.2789\n",
      "Iter-4310, train loss-2.2715, acc-0.1200, valid loss-2.2307, acc-0.2794, test loss-2.2326, acc-0.2790\n",
      "Iter-4320, train loss-2.2442, acc-0.2400, valid loss-2.2306, acc-0.2796, test loss-2.2325, acc-0.2789\n",
      "Iter-4330, train loss-2.2129, acc-0.3200, valid loss-2.2305, acc-0.2800, test loss-2.2324, acc-0.2793\n",
      "Iter-4340, train loss-2.2446, acc-0.2400, valid loss-2.2304, acc-0.2804, test loss-2.2323, acc-0.2798\n",
      "Iter-4350, train loss-2.2258, acc-0.3400, valid loss-2.2303, acc-0.2804, test loss-2.2321, acc-0.2797\n",
      "Iter-4360, train loss-2.2333, acc-0.3400, valid loss-2.2301, acc-0.2804, test loss-2.2320, acc-0.2801\n",
      "Iter-4370, train loss-2.2165, acc-0.3400, valid loss-2.2300, acc-0.2804, test loss-2.2319, acc-0.2802\n",
      "Iter-4380, train loss-2.2341, acc-0.1800, valid loss-2.2299, acc-0.2808, test loss-2.2318, acc-0.2804\n",
      "Iter-4390, train loss-2.2078, acc-0.4200, valid loss-2.2298, acc-0.2814, test loss-2.2317, acc-0.2807\n",
      "Iter-4400, train loss-2.2464, acc-0.2000, valid loss-2.2297, acc-0.2820, test loss-2.2316, acc-0.2810\n",
      "Iter-4410, train loss-2.2098, acc-0.3400, valid loss-2.2296, acc-0.2826, test loss-2.2315, acc-0.2812\n",
      "Iter-4420, train loss-2.2208, acc-0.2400, valid loss-2.2295, acc-0.2826, test loss-2.2314, acc-0.2814\n",
      "Iter-4430, train loss-2.2429, acc-0.2400, valid loss-2.2293, acc-0.2826, test loss-2.2312, acc-0.2814\n",
      "Iter-4440, train loss-2.2186, acc-0.3400, valid loss-2.2292, acc-0.2830, test loss-2.2311, acc-0.2816\n",
      "Iter-4450, train loss-2.2434, acc-0.2000, valid loss-2.2291, acc-0.2830, test loss-2.2310, acc-0.2817\n",
      "Iter-4460, train loss-2.2430, acc-0.2600, valid loss-2.2290, acc-0.2834, test loss-2.2309, acc-0.2818\n",
      "Iter-4470, train loss-2.2460, acc-0.2200, valid loss-2.2289, acc-0.2834, test loss-2.2308, acc-0.2818\n",
      "Iter-4480, train loss-2.2482, acc-0.2200, valid loss-2.2288, acc-0.2834, test loss-2.2307, acc-0.2821\n",
      "Iter-4490, train loss-2.2672, acc-0.1200, valid loss-2.2287, acc-0.2832, test loss-2.2306, acc-0.2822\n",
      "Iter-4500, train loss-2.2378, acc-0.3000, valid loss-2.2286, acc-0.2832, test loss-2.2305, acc-0.2827\n",
      "Iter-4510, train loss-2.2366, acc-0.2800, valid loss-2.2285, acc-0.2834, test loss-2.2304, acc-0.2828\n",
      "Iter-4520, train loss-2.2130, acc-0.3400, valid loss-2.2284, acc-0.2840, test loss-2.2302, acc-0.2834\n",
      "Iter-4530, train loss-2.2279, acc-0.3000, valid loss-2.2282, acc-0.2842, test loss-2.2301, acc-0.2836\n",
      "Iter-4540, train loss-2.2231, acc-0.2000, valid loss-2.2281, acc-0.2842, test loss-2.2300, acc-0.2836\n",
      "Iter-4550, train loss-2.2229, acc-0.2800, valid loss-2.2280, acc-0.2844, test loss-2.2299, acc-0.2837\n",
      "Iter-4560, train loss-2.1990, acc-0.4400, valid loss-2.2279, acc-0.2844, test loss-2.2298, acc-0.2838\n",
      "Iter-4570, train loss-2.2169, acc-0.3200, valid loss-2.2278, acc-0.2842, test loss-2.2297, acc-0.2841\n",
      "Iter-4580, train loss-2.2418, acc-0.2600, valid loss-2.2277, acc-0.2846, test loss-2.2296, acc-0.2840\n",
      "Iter-4590, train loss-2.2426, acc-0.2200, valid loss-2.2276, acc-0.2848, test loss-2.2295, acc-0.2840\n",
      "Iter-4600, train loss-2.2277, acc-0.2800, valid loss-2.2275, acc-0.2848, test loss-2.2294, acc-0.2841\n",
      "Iter-4610, train loss-2.2363, acc-0.2400, valid loss-2.2273, acc-0.2848, test loss-2.2292, acc-0.2846\n",
      "Iter-4620, train loss-2.2254, acc-0.3400, valid loss-2.2272, acc-0.2852, test loss-2.2291, acc-0.2845\n",
      "Iter-4630, train loss-2.2417, acc-0.2400, valid loss-2.2271, acc-0.2854, test loss-2.2290, acc-0.2848\n",
      "Iter-4640, train loss-2.2175, acc-0.3400, valid loss-2.2270, acc-0.2856, test loss-2.2289, acc-0.2848\n",
      "Iter-4650, train loss-2.2233, acc-0.2200, valid loss-2.2269, acc-0.2866, test loss-2.2288, acc-0.2851\n",
      "Iter-4660, train loss-2.2357, acc-0.3200, valid loss-2.2268, acc-0.2870, test loss-2.2287, acc-0.2852\n",
      "Iter-4670, train loss-2.2293, acc-0.3200, valid loss-2.2267, acc-0.2872, test loss-2.2286, acc-0.2854\n",
      "Iter-4680, train loss-2.2369, acc-0.2800, valid loss-2.2266, acc-0.2872, test loss-2.2284, acc-0.2859\n",
      "Iter-4690, train loss-2.2222, acc-0.3800, valid loss-2.2265, acc-0.2880, test loss-2.2283, acc-0.2865\n",
      "Iter-4700, train loss-2.2132, acc-0.2200, valid loss-2.2263, acc-0.2888, test loss-2.2282, acc-0.2869\n",
      "Iter-4710, train loss-2.2253, acc-0.2000, valid loss-2.2262, acc-0.2888, test loss-2.2281, acc-0.2872\n",
      "Iter-4720, train loss-2.1929, acc-0.3600, valid loss-2.2261, acc-0.2884, test loss-2.2280, acc-0.2873\n",
      "Iter-4730, train loss-2.2403, acc-0.2600, valid loss-2.2260, acc-0.2888, test loss-2.2279, acc-0.2875\n",
      "Iter-4740, train loss-2.2403, acc-0.2200, valid loss-2.2259, acc-0.2892, test loss-2.2278, acc-0.2880\n",
      "Iter-4750, train loss-2.2334, acc-0.2400, valid loss-2.2258, acc-0.2896, test loss-2.2277, acc-0.2884\n",
      "Iter-4760, train loss-2.2158, acc-0.3200, valid loss-2.2257, acc-0.2892, test loss-2.2276, acc-0.2890\n",
      "Iter-4770, train loss-2.2248, acc-0.3200, valid loss-2.2256, acc-0.2894, test loss-2.2275, acc-0.2897\n",
      "Iter-4780, train loss-2.2037, acc-0.3600, valid loss-2.2254, acc-0.2894, test loss-2.2274, acc-0.2897\n",
      "Iter-4790, train loss-2.2542, acc-0.2400, valid loss-2.2253, acc-0.2894, test loss-2.2272, acc-0.2900\n",
      "Iter-4800, train loss-2.2274, acc-0.3200, valid loss-2.2252, acc-0.2908, test loss-2.2271, acc-0.2902\n",
      "Iter-4810, train loss-2.2548, acc-0.2600, valid loss-2.2251, acc-0.2910, test loss-2.2270, acc-0.2907\n",
      "Iter-4820, train loss-2.2435, acc-0.3200, valid loss-2.2250, acc-0.2916, test loss-2.2269, acc-0.2907\n",
      "Iter-4830, train loss-2.2495, acc-0.2800, valid loss-2.2249, acc-0.2914, test loss-2.2268, acc-0.2912\n",
      "Iter-4840, train loss-2.2019, acc-0.4200, valid loss-2.2248, acc-0.2910, test loss-2.2267, acc-0.2916\n",
      "Iter-4850, train loss-2.2185, acc-0.3400, valid loss-2.2247, acc-0.2916, test loss-2.2266, acc-0.2918\n",
      "Iter-4860, train loss-2.2171, acc-0.3600, valid loss-2.2246, acc-0.2918, test loss-2.2265, acc-0.2921\n",
      "Iter-4870, train loss-2.2322, acc-0.2200, valid loss-2.2244, acc-0.2920, test loss-2.2264, acc-0.2923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-2.2350, acc-0.2200, valid loss-2.2243, acc-0.2926, test loss-2.2263, acc-0.2924\n",
      "Iter-4890, train loss-2.2256, acc-0.2600, valid loss-2.2242, acc-0.2928, test loss-2.2262, acc-0.2923\n",
      "Iter-4900, train loss-2.2388, acc-0.2600, valid loss-2.2241, acc-0.2928, test loss-2.2261, acc-0.2926\n",
      "Iter-4910, train loss-2.2180, acc-0.3200, valid loss-2.2240, acc-0.2928, test loss-2.2259, acc-0.2926\n",
      "Iter-4920, train loss-2.2136, acc-0.3200, valid loss-2.2239, acc-0.2938, test loss-2.2258, acc-0.2923\n",
      "Iter-4930, train loss-2.2160, acc-0.3600, valid loss-2.2238, acc-0.2938, test loss-2.2257, acc-0.2930\n",
      "Iter-4940, train loss-2.2048, acc-0.3600, valid loss-2.2237, acc-0.2936, test loss-2.2256, acc-0.2933\n",
      "Iter-4950, train loss-2.2046, acc-0.2400, valid loss-2.2236, acc-0.2940, test loss-2.2255, acc-0.2934\n",
      "Iter-4960, train loss-2.2259, acc-0.3200, valid loss-2.2234, acc-0.2940, test loss-2.2254, acc-0.2935\n",
      "Iter-4970, train loss-2.2391, acc-0.2600, valid loss-2.2233, acc-0.2944, test loss-2.2253, acc-0.2941\n",
      "Iter-4980, train loss-2.2328, acc-0.1800, valid loss-2.2232, acc-0.2950, test loss-2.2252, acc-0.2939\n",
      "Iter-4990, train loss-2.2086, acc-0.3600, valid loss-2.2231, acc-0.2958, test loss-2.2250, acc-0.2943\n",
      "Iter-5000, train loss-2.2321, acc-0.2600, valid loss-2.2230, acc-0.2968, test loss-2.2249, acc-0.2942\n",
      "Iter-5010, train loss-2.1957, acc-0.4000, valid loss-2.2229, acc-0.2964, test loss-2.2248, acc-0.2945\n",
      "Iter-5020, train loss-2.2110, acc-0.3600, valid loss-2.2228, acc-0.2966, test loss-2.2247, acc-0.2944\n",
      "Iter-5030, train loss-2.1902, acc-0.4200, valid loss-2.2226, acc-0.2968, test loss-2.2246, acc-0.2947\n",
      "Iter-5040, train loss-2.2451, acc-0.2200, valid loss-2.2225, acc-0.2976, test loss-2.2245, acc-0.2949\n",
      "Iter-5050, train loss-2.2021, acc-0.3600, valid loss-2.2224, acc-0.2970, test loss-2.2244, acc-0.2953\n",
      "Iter-5060, train loss-2.2232, acc-0.2800, valid loss-2.2223, acc-0.2976, test loss-2.2243, acc-0.2954\n",
      "Iter-5070, train loss-2.2085, acc-0.3000, valid loss-2.2222, acc-0.2982, test loss-2.2242, acc-0.2956\n",
      "Iter-5080, train loss-2.2075, acc-0.2600, valid loss-2.2221, acc-0.2984, test loss-2.2241, acc-0.2955\n",
      "Iter-5090, train loss-2.2299, acc-0.3200, valid loss-2.2220, acc-0.2984, test loss-2.2239, acc-0.2958\n",
      "Iter-5100, train loss-2.2167, acc-0.3400, valid loss-2.2219, acc-0.2986, test loss-2.2238, acc-0.2961\n",
      "Iter-5110, train loss-2.2607, acc-0.1400, valid loss-2.2218, acc-0.2986, test loss-2.2237, acc-0.2960\n",
      "Iter-5120, train loss-2.2131, acc-0.2600, valid loss-2.2216, acc-0.2992, test loss-2.2236, acc-0.2964\n",
      "Iter-5130, train loss-2.2309, acc-0.2400, valid loss-2.2215, acc-0.3000, test loss-2.2235, acc-0.2967\n",
      "Iter-5140, train loss-2.2214, acc-0.2600, valid loss-2.2214, acc-0.3000, test loss-2.2234, acc-0.2972\n",
      "Iter-5150, train loss-2.2026, acc-0.3800, valid loss-2.2213, acc-0.3008, test loss-2.2233, acc-0.2973\n",
      "Iter-5160, train loss-2.2128, acc-0.2800, valid loss-2.2212, acc-0.3008, test loss-2.2232, acc-0.2978\n",
      "Iter-5170, train loss-2.2123, acc-0.2600, valid loss-2.2211, acc-0.3014, test loss-2.2231, acc-0.2980\n",
      "Iter-5180, train loss-2.2203, acc-0.3200, valid loss-2.2210, acc-0.3020, test loss-2.2230, acc-0.2981\n",
      "Iter-5190, train loss-2.2240, acc-0.2600, valid loss-2.2209, acc-0.3020, test loss-2.2229, acc-0.2981\n",
      "Iter-5200, train loss-2.2124, acc-0.3000, valid loss-2.2207, acc-0.3026, test loss-2.2227, acc-0.2983\n",
      "Iter-5210, train loss-2.2184, acc-0.3400, valid loss-2.2206, acc-0.3028, test loss-2.2226, acc-0.2980\n",
      "Iter-5220, train loss-2.2561, acc-0.1800, valid loss-2.2205, acc-0.3036, test loss-2.2225, acc-0.2986\n",
      "Iter-5230, train loss-2.2074, acc-0.4000, valid loss-2.2204, acc-0.3034, test loss-2.2224, acc-0.2992\n",
      "Iter-5240, train loss-2.2180, acc-0.2200, valid loss-2.2203, acc-0.3040, test loss-2.2223, acc-0.2995\n",
      "Iter-5250, train loss-2.2011, acc-0.4000, valid loss-2.2202, acc-0.3040, test loss-2.2222, acc-0.2999\n",
      "Iter-5260, train loss-2.2424, acc-0.2600, valid loss-2.2201, acc-0.3046, test loss-2.2221, acc-0.3002\n",
      "Iter-5270, train loss-2.2207, acc-0.3600, valid loss-2.2200, acc-0.3046, test loss-2.2220, acc-0.3008\n",
      "Iter-5280, train loss-2.2381, acc-0.2600, valid loss-2.2198, acc-0.3052, test loss-2.2218, acc-0.3012\n",
      "Iter-5290, train loss-2.2348, acc-0.2600, valid loss-2.2197, acc-0.3052, test loss-2.2217, acc-0.3014\n",
      "Iter-5300, train loss-2.2070, acc-0.4000, valid loss-2.2196, acc-0.3052, test loss-2.2216, acc-0.3015\n",
      "Iter-5310, train loss-2.2359, acc-0.3000, valid loss-2.2195, acc-0.3056, test loss-2.2215, acc-0.3014\n",
      "Iter-5320, train loss-2.2260, acc-0.2800, valid loss-2.2194, acc-0.3064, test loss-2.2214, acc-0.3019\n",
      "Iter-5330, train loss-2.2211, acc-0.3400, valid loss-2.2193, acc-0.3068, test loss-2.2213, acc-0.3020\n",
      "Iter-5340, train loss-2.2397, acc-0.2600, valid loss-2.2192, acc-0.3068, test loss-2.2212, acc-0.3022\n",
      "Iter-5350, train loss-2.1979, acc-0.4000, valid loss-2.2191, acc-0.3070, test loss-2.2211, acc-0.3022\n",
      "Iter-5360, train loss-2.2141, acc-0.3600, valid loss-2.2190, acc-0.3072, test loss-2.2210, acc-0.3027\n",
      "Iter-5370, train loss-2.2105, acc-0.3600, valid loss-2.2188, acc-0.3070, test loss-2.2209, acc-0.3029\n",
      "Iter-5380, train loss-2.2159, acc-0.2600, valid loss-2.2187, acc-0.3072, test loss-2.2208, acc-0.3029\n",
      "Iter-5390, train loss-2.2127, acc-0.3400, valid loss-2.2186, acc-0.3074, test loss-2.2207, acc-0.3028\n",
      "Iter-5400, train loss-2.2108, acc-0.2800, valid loss-2.2185, acc-0.3078, test loss-2.2205, acc-0.3029\n",
      "Iter-5410, train loss-2.2081, acc-0.4200, valid loss-2.2184, acc-0.3076, test loss-2.2204, acc-0.3028\n",
      "Iter-5420, train loss-2.2319, acc-0.2600, valid loss-2.2183, acc-0.3082, test loss-2.2203, acc-0.3033\n",
      "Iter-5430, train loss-2.2462, acc-0.2600, valid loss-2.2182, acc-0.3090, test loss-2.2202, acc-0.3037\n",
      "Iter-5440, train loss-2.2034, acc-0.3600, valid loss-2.2181, acc-0.3090, test loss-2.2201, acc-0.3044\n",
      "Iter-5450, train loss-2.2375, acc-0.2000, valid loss-2.2180, acc-0.3088, test loss-2.2200, acc-0.3046\n",
      "Iter-5460, train loss-2.2004, acc-0.2800, valid loss-2.2179, acc-0.3096, test loss-2.2199, acc-0.3049\n",
      "Iter-5470, train loss-2.2200, acc-0.3000, valid loss-2.2178, acc-0.3104, test loss-2.2198, acc-0.3051\n",
      "Iter-5480, train loss-2.2210, acc-0.2800, valid loss-2.2177, acc-0.3102, test loss-2.2197, acc-0.3053\n",
      "Iter-5490, train loss-2.2480, acc-0.3200, valid loss-2.2176, acc-0.3104, test loss-2.2196, acc-0.3051\n",
      "Iter-5500, train loss-2.2222, acc-0.3800, valid loss-2.2175, acc-0.3106, test loss-2.2195, acc-0.3052\n",
      "Iter-5510, train loss-2.2401, acc-0.2000, valid loss-2.2173, acc-0.3110, test loss-2.2194, acc-0.3054\n",
      "Iter-5520, train loss-2.2034, acc-0.4200, valid loss-2.2172, acc-0.3110, test loss-2.2193, acc-0.3055\n",
      "Iter-5530, train loss-2.2005, acc-0.3800, valid loss-2.2171, acc-0.3106, test loss-2.2191, acc-0.3059\n",
      "Iter-5540, train loss-2.2327, acc-0.2800, valid loss-2.2170, acc-0.3106, test loss-2.2190, acc-0.3062\n",
      "Iter-5550, train loss-2.2174, acc-0.4000, valid loss-2.2169, acc-0.3112, test loss-2.2189, acc-0.3063\n",
      "Iter-5560, train loss-2.2216, acc-0.3200, valid loss-2.2168, acc-0.3112, test loss-2.2188, acc-0.3066\n",
      "Iter-5570, train loss-2.2240, acc-0.3400, valid loss-2.2167, acc-0.3116, test loss-2.2187, acc-0.3070\n",
      "Iter-5580, train loss-2.2234, acc-0.2800, valid loss-2.2166, acc-0.3114, test loss-2.2186, acc-0.3077\n",
      "Iter-5590, train loss-2.2072, acc-0.3400, valid loss-2.2165, acc-0.3114, test loss-2.2185, acc-0.3079\n",
      "Iter-5600, train loss-2.2059, acc-0.4000, valid loss-2.2163, acc-0.3120, test loss-2.2184, acc-0.3080\n",
      "Iter-5610, train loss-2.2293, acc-0.2800, valid loss-2.2162, acc-0.3120, test loss-2.2183, acc-0.3080\n",
      "Iter-5620, train loss-2.2192, acc-0.3600, valid loss-2.2161, acc-0.3120, test loss-2.2182, acc-0.3080\n",
      "Iter-5630, train loss-2.2341, acc-0.2800, valid loss-2.2160, acc-0.3116, test loss-2.2181, acc-0.3083\n",
      "Iter-5640, train loss-2.2076, acc-0.3400, valid loss-2.2159, acc-0.3118, test loss-2.2180, acc-0.3088\n",
      "Iter-5650, train loss-2.2134, acc-0.3400, valid loss-2.2158, acc-0.3128, test loss-2.2179, acc-0.3091\n",
      "Iter-5660, train loss-2.2316, acc-0.2800, valid loss-2.2157, acc-0.3132, test loss-2.2177, acc-0.3096\n",
      "Iter-5670, train loss-2.2042, acc-0.3600, valid loss-2.2156, acc-0.3130, test loss-2.2176, acc-0.3095\n",
      "Iter-5680, train loss-2.2269, acc-0.2400, valid loss-2.2155, acc-0.3132, test loss-2.2175, acc-0.3096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-2.2421, acc-0.2800, valid loss-2.2154, acc-0.3130, test loss-2.2174, acc-0.3098\n",
      "Iter-5700, train loss-2.2136, acc-0.3000, valid loss-2.2152, acc-0.3132, test loss-2.2173, acc-0.3101\n",
      "Iter-5710, train loss-2.1984, acc-0.4000, valid loss-2.2151, acc-0.3138, test loss-2.2172, acc-0.3108\n",
      "Iter-5720, train loss-2.1886, acc-0.4200, valid loss-2.2150, acc-0.3138, test loss-2.2171, acc-0.3109\n",
      "Iter-5730, train loss-2.2133, acc-0.2800, valid loss-2.2149, acc-0.3140, test loss-2.2170, acc-0.3110\n",
      "Iter-5740, train loss-2.1949, acc-0.4600, valid loss-2.2148, acc-0.3142, test loss-2.2168, acc-0.3113\n",
      "Iter-5750, train loss-2.2407, acc-0.2600, valid loss-2.2147, acc-0.3142, test loss-2.2167, acc-0.3113\n",
      "Iter-5760, train loss-2.2116, acc-0.3600, valid loss-2.2146, acc-0.3142, test loss-2.2166, acc-0.3116\n",
      "Iter-5770, train loss-2.1960, acc-0.4400, valid loss-2.2145, acc-0.3142, test loss-2.2165, acc-0.3113\n",
      "Iter-5780, train loss-2.2332, acc-0.3000, valid loss-2.2144, acc-0.3144, test loss-2.2164, acc-0.3113\n",
      "Iter-5790, train loss-2.1984, acc-0.3400, valid loss-2.2142, acc-0.3150, test loss-2.2163, acc-0.3112\n",
      "Iter-5800, train loss-2.2176, acc-0.4000, valid loss-2.2141, acc-0.3154, test loss-2.2162, acc-0.3114\n",
      "Iter-5810, train loss-2.2304, acc-0.2800, valid loss-2.2140, acc-0.3154, test loss-2.2161, acc-0.3117\n",
      "Iter-5820, train loss-2.2370, acc-0.1600, valid loss-2.2139, acc-0.3158, test loss-2.2160, acc-0.3119\n",
      "Iter-5830, train loss-2.2403, acc-0.2400, valid loss-2.2138, acc-0.3158, test loss-2.2159, acc-0.3122\n",
      "Iter-5840, train loss-2.2253, acc-0.2600, valid loss-2.2137, acc-0.3154, test loss-2.2158, acc-0.3123\n",
      "Iter-5850, train loss-2.2222, acc-0.3000, valid loss-2.2136, acc-0.3156, test loss-2.2157, acc-0.3126\n",
      "Iter-5860, train loss-2.2399, acc-0.2800, valid loss-2.2135, acc-0.3156, test loss-2.2156, acc-0.3130\n",
      "Iter-5870, train loss-2.2226, acc-0.2600, valid loss-2.2134, acc-0.3152, test loss-2.2155, acc-0.3134\n",
      "Iter-5880, train loss-2.2285, acc-0.1600, valid loss-2.2133, acc-0.3154, test loss-2.2154, acc-0.3134\n",
      "Iter-5890, train loss-2.2364, acc-0.2200, valid loss-2.2132, acc-0.3156, test loss-2.2153, acc-0.3135\n",
      "Iter-5900, train loss-2.2209, acc-0.3000, valid loss-2.2131, acc-0.3158, test loss-2.2152, acc-0.3137\n",
      "Iter-5910, train loss-2.1814, acc-0.4400, valid loss-2.2130, acc-0.3160, test loss-2.2150, acc-0.3136\n",
      "Iter-5920, train loss-2.2062, acc-0.3400, valid loss-2.2129, acc-0.3160, test loss-2.2149, acc-0.3141\n",
      "Iter-5930, train loss-2.2280, acc-0.3000, valid loss-2.2127, acc-0.3160, test loss-2.2148, acc-0.3146\n",
      "Iter-5940, train loss-2.2284, acc-0.3000, valid loss-2.2126, acc-0.3160, test loss-2.2147, acc-0.3145\n",
      "Iter-5950, train loss-2.2139, acc-0.3800, valid loss-2.2125, acc-0.3164, test loss-2.2146, acc-0.3147\n",
      "Iter-5960, train loss-2.2024, acc-0.3600, valid loss-2.2124, acc-0.3168, test loss-2.2145, acc-0.3145\n",
      "Iter-5970, train loss-2.2200, acc-0.2800, valid loss-2.2123, acc-0.3172, test loss-2.2144, acc-0.3150\n",
      "Iter-5980, train loss-2.2070, acc-0.3600, valid loss-2.2122, acc-0.3168, test loss-2.2143, acc-0.3151\n",
      "Iter-5990, train loss-2.2004, acc-0.3600, valid loss-2.2121, acc-0.3170, test loss-2.2142, acc-0.3151\n",
      "Iter-6000, train loss-2.1837, acc-0.4000, valid loss-2.2120, acc-0.3170, test loss-2.2141, acc-0.3152\n",
      "Iter-6010, train loss-2.1978, acc-0.3400, valid loss-2.2119, acc-0.3168, test loss-2.2140, acc-0.3153\n",
      "Iter-6020, train loss-2.2189, acc-0.2400, valid loss-2.2118, acc-0.3170, test loss-2.2139, acc-0.3154\n",
      "Iter-6030, train loss-2.2310, acc-0.3200, valid loss-2.2117, acc-0.3172, test loss-2.2138, acc-0.3154\n",
      "Iter-6040, train loss-2.2225, acc-0.2200, valid loss-2.2116, acc-0.3178, test loss-2.2136, acc-0.3156\n",
      "Iter-6050, train loss-2.2245, acc-0.2800, valid loss-2.2114, acc-0.3178, test loss-2.2135, acc-0.3164\n",
      "Iter-6060, train loss-2.2005, acc-0.3800, valid loss-2.2113, acc-0.3178, test loss-2.2134, acc-0.3169\n",
      "Iter-6070, train loss-2.1841, acc-0.3600, valid loss-2.2112, acc-0.3180, test loss-2.2133, acc-0.3170\n",
      "Iter-6080, train loss-2.1921, acc-0.4400, valid loss-2.2111, acc-0.3178, test loss-2.2132, acc-0.3173\n",
      "Iter-6090, train loss-2.1741, acc-0.4400, valid loss-2.2110, acc-0.3182, test loss-2.2131, acc-0.3173\n",
      "Iter-6100, train loss-2.2140, acc-0.2600, valid loss-2.2109, acc-0.3186, test loss-2.2130, acc-0.3178\n",
      "Iter-6110, train loss-2.2243, acc-0.3400, valid loss-2.2108, acc-0.3182, test loss-2.2129, acc-0.3179\n",
      "Iter-6120, train loss-2.1926, acc-0.4000, valid loss-2.2107, acc-0.3184, test loss-2.2128, acc-0.3181\n",
      "Iter-6130, train loss-2.2472, acc-0.2600, valid loss-2.2106, acc-0.3190, test loss-2.2127, acc-0.3182\n",
      "Iter-6140, train loss-2.2153, acc-0.3600, valid loss-2.2104, acc-0.3192, test loss-2.2126, acc-0.3183\n",
      "Iter-6150, train loss-2.2265, acc-0.2800, valid loss-2.2103, acc-0.3200, test loss-2.2124, acc-0.3187\n",
      "Iter-6160, train loss-2.1720, acc-0.4400, valid loss-2.2102, acc-0.3200, test loss-2.2124, acc-0.3183\n",
      "Iter-6170, train loss-2.2106, acc-0.4000, valid loss-2.2101, acc-0.3200, test loss-2.2122, acc-0.3183\n",
      "Iter-6180, train loss-2.1962, acc-0.3800, valid loss-2.2100, acc-0.3200, test loss-2.2121, acc-0.3185\n",
      "Iter-6190, train loss-2.2146, acc-0.2600, valid loss-2.2099, acc-0.3202, test loss-2.2120, acc-0.3187\n",
      "Iter-6200, train loss-2.2072, acc-0.3400, valid loss-2.2098, acc-0.3206, test loss-2.2119, acc-0.3189\n",
      "Iter-6210, train loss-2.2136, acc-0.3400, valid loss-2.2097, acc-0.3210, test loss-2.2118, acc-0.3194\n",
      "Iter-6220, train loss-2.2177, acc-0.2800, valid loss-2.2096, acc-0.3212, test loss-2.2117, acc-0.3192\n",
      "Iter-6230, train loss-2.2050, acc-0.3800, valid loss-2.2095, acc-0.3212, test loss-2.2116, acc-0.3193\n",
      "Iter-6240, train loss-2.1999, acc-0.3600, valid loss-2.2094, acc-0.3212, test loss-2.2115, acc-0.3194\n",
      "Iter-6250, train loss-2.1905, acc-0.3400, valid loss-2.2093, acc-0.3214, test loss-2.2114, acc-0.3195\n",
      "Iter-6260, train loss-2.1997, acc-0.3000, valid loss-2.2092, acc-0.3216, test loss-2.2113, acc-0.3198\n",
      "Iter-6270, train loss-2.1985, acc-0.3800, valid loss-2.2090, acc-0.3212, test loss-2.2112, acc-0.3195\n",
      "Iter-6280, train loss-2.2109, acc-0.3200, valid loss-2.2089, acc-0.3208, test loss-2.2111, acc-0.3201\n",
      "Iter-6290, train loss-2.1738, acc-0.4800, valid loss-2.2088, acc-0.3208, test loss-2.2110, acc-0.3205\n",
      "Iter-6300, train loss-2.2193, acc-0.2600, valid loss-2.2087, acc-0.3214, test loss-2.2108, acc-0.3209\n",
      "Iter-6310, train loss-2.1903, acc-0.4600, valid loss-2.2086, acc-0.3220, test loss-2.2107, acc-0.3207\n",
      "Iter-6320, train loss-2.2081, acc-0.3200, valid loss-2.2085, acc-0.3218, test loss-2.2106, acc-0.3213\n",
      "Iter-6330, train loss-2.1985, acc-0.2800, valid loss-2.2084, acc-0.3220, test loss-2.2105, acc-0.3213\n",
      "Iter-6340, train loss-2.2105, acc-0.3400, valid loss-2.2083, acc-0.3222, test loss-2.2104, acc-0.3214\n",
      "Iter-6350, train loss-2.1864, acc-0.3400, valid loss-2.2082, acc-0.3228, test loss-2.2103, acc-0.3213\n",
      "Iter-6360, train loss-2.1955, acc-0.3200, valid loss-2.2081, acc-0.3230, test loss-2.2102, acc-0.3215\n",
      "Iter-6370, train loss-2.1888, acc-0.4600, valid loss-2.2080, acc-0.3232, test loss-2.2101, acc-0.3213\n",
      "Iter-6380, train loss-2.1888, acc-0.4000, valid loss-2.2078, acc-0.3234, test loss-2.2100, acc-0.3209\n",
      "Iter-6390, train loss-2.1905, acc-0.3600, valid loss-2.2077, acc-0.3232, test loss-2.2099, acc-0.3208\n",
      "Iter-6400, train loss-2.1914, acc-0.3000, valid loss-2.2076, acc-0.3232, test loss-2.2098, acc-0.3210\n",
      "Iter-6410, train loss-2.2150, acc-0.2800, valid loss-2.2075, acc-0.3232, test loss-2.2097, acc-0.3212\n",
      "Iter-6420, train loss-2.2225, acc-0.3400, valid loss-2.2074, acc-0.3232, test loss-2.2095, acc-0.3212\n",
      "Iter-6430, train loss-2.2208, acc-0.3400, valid loss-2.2073, acc-0.3232, test loss-2.2094, acc-0.3214\n",
      "Iter-6440, train loss-2.2129, acc-0.2600, valid loss-2.2072, acc-0.3232, test loss-2.2093, acc-0.3212\n",
      "Iter-6450, train loss-2.1879, acc-0.3800, valid loss-2.2071, acc-0.3232, test loss-2.2092, acc-0.3214\n",
      "Iter-6460, train loss-2.2216, acc-0.2400, valid loss-2.2070, acc-0.3228, test loss-2.2091, acc-0.3213\n",
      "Iter-6470, train loss-2.1865, acc-0.3400, valid loss-2.2069, acc-0.3236, test loss-2.2090, acc-0.3213\n",
      "Iter-6480, train loss-2.2334, acc-0.2800, valid loss-2.2067, acc-0.3232, test loss-2.2089, acc-0.3214\n",
      "Iter-6490, train loss-2.1981, acc-0.3800, valid loss-2.2066, acc-0.3240, test loss-2.2088, acc-0.3217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-2.2188, acc-0.3200, valid loss-2.2065, acc-0.3240, test loss-2.2087, acc-0.3217\n",
      "Iter-6510, train loss-2.2369, acc-0.2200, valid loss-2.2064, acc-0.3242, test loss-2.2086, acc-0.3223\n",
      "Iter-6520, train loss-2.1823, acc-0.4200, valid loss-2.2063, acc-0.3244, test loss-2.2085, acc-0.3222\n",
      "Iter-6530, train loss-2.2181, acc-0.2200, valid loss-2.2062, acc-0.3240, test loss-2.2084, acc-0.3226\n",
      "Iter-6540, train loss-2.2174, acc-0.2600, valid loss-2.2061, acc-0.3246, test loss-2.2083, acc-0.3227\n",
      "Iter-6550, train loss-2.2005, acc-0.2600, valid loss-2.2060, acc-0.3250, test loss-2.2082, acc-0.3227\n",
      "Iter-6560, train loss-2.1873, acc-0.4200, valid loss-2.2059, acc-0.3254, test loss-2.2081, acc-0.3231\n",
      "Iter-6570, train loss-2.2042, acc-0.4000, valid loss-2.2058, acc-0.3258, test loss-2.2079, acc-0.3231\n",
      "Iter-6580, train loss-2.2031, acc-0.3600, valid loss-2.2057, acc-0.3260, test loss-2.2078, acc-0.3232\n",
      "Iter-6590, train loss-2.1919, acc-0.3200, valid loss-2.2055, acc-0.3264, test loss-2.2077, acc-0.3233\n",
      "Iter-6600, train loss-2.2187, acc-0.3800, valid loss-2.2054, acc-0.3262, test loss-2.2076, acc-0.3237\n",
      "Iter-6610, train loss-2.2213, acc-0.3000, valid loss-2.2053, acc-0.3264, test loss-2.2075, acc-0.3238\n",
      "Iter-6620, train loss-2.2083, acc-0.3400, valid loss-2.2052, acc-0.3262, test loss-2.2074, acc-0.3240\n",
      "Iter-6630, train loss-2.2161, acc-0.3000, valid loss-2.2051, acc-0.3268, test loss-2.2073, acc-0.3241\n",
      "Iter-6640, train loss-2.1922, acc-0.3600, valid loss-2.2050, acc-0.3266, test loss-2.2072, acc-0.3242\n",
      "Iter-6650, train loss-2.2296, acc-0.3000, valid loss-2.2049, acc-0.3270, test loss-2.2071, acc-0.3244\n",
      "Iter-6660, train loss-2.2219, acc-0.3600, valid loss-2.2048, acc-0.3266, test loss-2.2070, acc-0.3247\n",
      "Iter-6670, train loss-2.2151, acc-0.3200, valid loss-2.2047, acc-0.3278, test loss-2.2069, acc-0.3248\n",
      "Iter-6680, train loss-2.2313, acc-0.2000, valid loss-2.2046, acc-0.3276, test loss-2.2068, acc-0.3248\n",
      "Iter-6690, train loss-2.2091, acc-0.3200, valid loss-2.2045, acc-0.3272, test loss-2.2067, acc-0.3247\n",
      "Iter-6700, train loss-2.1998, acc-0.3600, valid loss-2.2044, acc-0.3278, test loss-2.2066, acc-0.3251\n",
      "Iter-6710, train loss-2.1971, acc-0.4000, valid loss-2.2043, acc-0.3276, test loss-2.2065, acc-0.3252\n",
      "Iter-6720, train loss-2.2171, acc-0.2400, valid loss-2.2042, acc-0.3282, test loss-2.2064, acc-0.3252\n",
      "Iter-6730, train loss-2.2072, acc-0.3200, valid loss-2.2041, acc-0.3284, test loss-2.2063, acc-0.3253\n",
      "Iter-6740, train loss-2.1971, acc-0.3400, valid loss-2.2040, acc-0.3286, test loss-2.2062, acc-0.3254\n",
      "Iter-6750, train loss-2.2149, acc-0.3400, valid loss-2.2039, acc-0.3286, test loss-2.2061, acc-0.3254\n",
      "Iter-6760, train loss-2.1918, acc-0.4200, valid loss-2.2038, acc-0.3290, test loss-2.2060, acc-0.3257\n",
      "Iter-6770, train loss-2.2007, acc-0.3600, valid loss-2.2037, acc-0.3298, test loss-2.2058, acc-0.3257\n",
      "Iter-6780, train loss-2.1819, acc-0.4600, valid loss-2.2035, acc-0.3298, test loss-2.2057, acc-0.3256\n",
      "Iter-6790, train loss-2.1820, acc-0.4400, valid loss-2.2034, acc-0.3302, test loss-2.2056, acc-0.3258\n",
      "Iter-6800, train loss-2.1919, acc-0.3000, valid loss-2.2033, acc-0.3304, test loss-2.2055, acc-0.3258\n",
      "Iter-6810, train loss-2.2477, acc-0.2000, valid loss-2.2032, acc-0.3306, test loss-2.2054, acc-0.3255\n",
      "Iter-6820, train loss-2.2164, acc-0.3800, valid loss-2.2031, acc-0.3306, test loss-2.2053, acc-0.3257\n",
      "Iter-6830, train loss-2.2196, acc-0.2800, valid loss-2.2030, acc-0.3314, test loss-2.2052, acc-0.3262\n",
      "Iter-6840, train loss-2.2167, acc-0.3600, valid loss-2.2029, acc-0.3310, test loss-2.2051, acc-0.3257\n",
      "Iter-6850, train loss-2.1972, acc-0.3000, valid loss-2.2028, acc-0.3308, test loss-2.2050, acc-0.3260\n",
      "Iter-6860, train loss-2.2186, acc-0.3600, valid loss-2.2027, acc-0.3314, test loss-2.2049, acc-0.3266\n",
      "Iter-6870, train loss-2.1842, acc-0.4000, valid loss-2.2026, acc-0.3312, test loss-2.2048, acc-0.3265\n",
      "Iter-6880, train loss-2.1900, acc-0.3800, valid loss-2.2025, acc-0.3316, test loss-2.2047, acc-0.3266\n",
      "Iter-6890, train loss-2.1993, acc-0.3400, valid loss-2.2023, acc-0.3316, test loss-2.2045, acc-0.3269\n",
      "Iter-6900, train loss-2.1688, acc-0.4600, valid loss-2.2022, acc-0.3316, test loss-2.2044, acc-0.3272\n",
      "Iter-6910, train loss-2.1851, acc-0.3800, valid loss-2.2021, acc-0.3316, test loss-2.2043, acc-0.3274\n",
      "Iter-6920, train loss-2.2133, acc-0.3400, valid loss-2.2020, acc-0.3320, test loss-2.2042, acc-0.3272\n",
      "Iter-6930, train loss-2.2138, acc-0.3000, valid loss-2.2019, acc-0.3334, test loss-2.2041, acc-0.3272\n",
      "Iter-6940, train loss-2.1846, acc-0.4000, valid loss-2.2018, acc-0.3334, test loss-2.2040, acc-0.3273\n",
      "Iter-6950, train loss-2.1846, acc-0.3600, valid loss-2.2017, acc-0.3338, test loss-2.2039, acc-0.3275\n",
      "Iter-6960, train loss-2.1894, acc-0.3400, valid loss-2.2016, acc-0.3340, test loss-2.2038, acc-0.3278\n",
      "Iter-6970, train loss-2.2189, acc-0.1800, valid loss-2.2015, acc-0.3338, test loss-2.2037, acc-0.3282\n",
      "Iter-6980, train loss-2.2206, acc-0.3000, valid loss-2.2014, acc-0.3342, test loss-2.2036, acc-0.3286\n",
      "Iter-6990, train loss-2.1948, acc-0.3200, valid loss-2.2013, acc-0.3340, test loss-2.2035, acc-0.3285\n",
      "Iter-7000, train loss-2.1982, acc-0.3400, valid loss-2.2012, acc-0.3340, test loss-2.2034, acc-0.3288\n",
      "Iter-7010, train loss-2.2303, acc-0.2600, valid loss-2.2011, acc-0.3342, test loss-2.2033, acc-0.3289\n",
      "Iter-7020, train loss-2.1779, acc-0.3800, valid loss-2.2010, acc-0.3346, test loss-2.2032, acc-0.3289\n",
      "Iter-7030, train loss-2.2018, acc-0.2800, valid loss-2.2009, acc-0.3346, test loss-2.2031, acc-0.3292\n",
      "Iter-7040, train loss-2.1599, acc-0.5400, valid loss-2.2008, acc-0.3342, test loss-2.2030, acc-0.3292\n",
      "Iter-7050, train loss-2.2082, acc-0.3400, valid loss-2.2006, acc-0.3342, test loss-2.2029, acc-0.3292\n",
      "Iter-7060, train loss-2.2045, acc-0.3000, valid loss-2.2005, acc-0.3344, test loss-2.2027, acc-0.3290\n",
      "Iter-7070, train loss-2.2012, acc-0.3600, valid loss-2.2004, acc-0.3342, test loss-2.2026, acc-0.3292\n",
      "Iter-7080, train loss-2.1902, acc-0.3400, valid loss-2.2003, acc-0.3348, test loss-2.2025, acc-0.3293\n",
      "Iter-7090, train loss-2.2042, acc-0.2600, valid loss-2.2002, acc-0.3352, test loss-2.2024, acc-0.3294\n",
      "Iter-7100, train loss-2.1765, acc-0.4000, valid loss-2.2001, acc-0.3358, test loss-2.2023, acc-0.3297\n",
      "Iter-7110, train loss-2.2118, acc-0.3400, valid loss-2.2000, acc-0.3358, test loss-2.2022, acc-0.3295\n",
      "Iter-7120, train loss-2.2213, acc-0.2400, valid loss-2.1999, acc-0.3368, test loss-2.2021, acc-0.3300\n",
      "Iter-7130, train loss-2.2212, acc-0.3000, valid loss-2.1998, acc-0.3368, test loss-2.2020, acc-0.3299\n",
      "Iter-7140, train loss-2.2173, acc-0.1600, valid loss-2.1997, acc-0.3372, test loss-2.2019, acc-0.3302\n",
      "Iter-7150, train loss-2.1660, acc-0.3600, valid loss-2.1996, acc-0.3370, test loss-2.2018, acc-0.3302\n",
      "Iter-7160, train loss-2.1717, acc-0.3600, valid loss-2.1995, acc-0.3372, test loss-2.2017, acc-0.3302\n",
      "Iter-7170, train loss-2.1761, acc-0.4000, valid loss-2.1994, acc-0.3372, test loss-2.2016, acc-0.3306\n",
      "Iter-7180, train loss-2.1983, acc-0.3200, valid loss-2.1993, acc-0.3372, test loss-2.2015, acc-0.3307\n",
      "Iter-7190, train loss-2.1929, acc-0.3200, valid loss-2.1992, acc-0.3374, test loss-2.2014, acc-0.3307\n",
      "Iter-7200, train loss-2.2011, acc-0.2600, valid loss-2.1991, acc-0.3376, test loss-2.2013, acc-0.3307\n",
      "Iter-7210, train loss-2.1776, acc-0.4000, valid loss-2.1989, acc-0.3378, test loss-2.2012, acc-0.3306\n",
      "Iter-7220, train loss-2.1948, acc-0.3000, valid loss-2.1988, acc-0.3374, test loss-2.2011, acc-0.3313\n",
      "Iter-7230, train loss-2.2042, acc-0.3800, valid loss-2.1987, acc-0.3380, test loss-2.2010, acc-0.3312\n",
      "Iter-7240, train loss-2.2123, acc-0.2800, valid loss-2.1986, acc-0.3382, test loss-2.2009, acc-0.3313\n",
      "Iter-7250, train loss-2.1851, acc-0.4200, valid loss-2.1985, acc-0.3380, test loss-2.2007, acc-0.3315\n",
      "Iter-7260, train loss-2.1752, acc-0.3600, valid loss-2.1984, acc-0.3382, test loss-2.2006, acc-0.3318\n",
      "Iter-7270, train loss-2.1756, acc-0.3600, valid loss-2.1983, acc-0.3382, test loss-2.2005, acc-0.3315\n",
      "Iter-7280, train loss-2.2075, acc-0.2800, valid loss-2.1982, acc-0.3382, test loss-2.2004, acc-0.3316\n",
      "Iter-7290, train loss-2.2148, acc-0.2600, valid loss-2.1981, acc-0.3384, test loss-2.2003, acc-0.3321\n",
      "Iter-7300, train loss-2.1886, acc-0.4400, valid loss-2.1980, acc-0.3384, test loss-2.2002, acc-0.3320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-2.2086, acc-0.3400, valid loss-2.1979, acc-0.3382, test loss-2.2001, acc-0.3321\n",
      "Iter-7320, train loss-2.2084, acc-0.3200, valid loss-2.1978, acc-0.3384, test loss-2.2000, acc-0.3324\n",
      "Iter-7330, train loss-2.2118, acc-0.3000, valid loss-2.1977, acc-0.3384, test loss-2.1999, acc-0.3325\n",
      "Iter-7340, train loss-2.2099, acc-0.3000, valid loss-2.1976, acc-0.3388, test loss-2.1998, acc-0.3327\n",
      "Iter-7350, train loss-2.1727, acc-0.3200, valid loss-2.1975, acc-0.3394, test loss-2.1997, acc-0.3329\n",
      "Iter-7360, train loss-2.1839, acc-0.3400, valid loss-2.1974, acc-0.3398, test loss-2.1996, acc-0.3329\n",
      "Iter-7370, train loss-2.1738, acc-0.4000, valid loss-2.1973, acc-0.3398, test loss-2.1995, acc-0.3330\n",
      "Iter-7380, train loss-2.2189, acc-0.3200, valid loss-2.1972, acc-0.3394, test loss-2.1994, acc-0.3328\n",
      "Iter-7390, train loss-2.1967, acc-0.4000, valid loss-2.1971, acc-0.3392, test loss-2.1993, acc-0.3325\n",
      "Iter-7400, train loss-2.2161, acc-0.2800, valid loss-2.1970, acc-0.3392, test loss-2.1992, acc-0.3330\n",
      "Iter-7410, train loss-2.1622, acc-0.5000, valid loss-2.1969, acc-0.3400, test loss-2.1991, acc-0.3330\n",
      "Iter-7420, train loss-2.1786, acc-0.4400, valid loss-2.1968, acc-0.3410, test loss-2.1990, acc-0.3331\n",
      "Iter-7430, train loss-2.2151, acc-0.3600, valid loss-2.1966, acc-0.3410, test loss-2.1989, acc-0.3331\n",
      "Iter-7440, train loss-2.1499, acc-0.4000, valid loss-2.1965, acc-0.3412, test loss-2.1988, acc-0.3334\n",
      "Iter-7450, train loss-2.1714, acc-0.4600, valid loss-2.1964, acc-0.3414, test loss-2.1987, acc-0.3334\n",
      "Iter-7460, train loss-2.1947, acc-0.3200, valid loss-2.1963, acc-0.3412, test loss-2.1986, acc-0.3336\n",
      "Iter-7470, train loss-2.1801, acc-0.3400, valid loss-2.1962, acc-0.3414, test loss-2.1985, acc-0.3343\n",
      "Iter-7480, train loss-2.1869, acc-0.4400, valid loss-2.1961, acc-0.3414, test loss-2.1984, acc-0.3342\n",
      "Iter-7490, train loss-2.2057, acc-0.3600, valid loss-2.1960, acc-0.3414, test loss-2.1983, acc-0.3344\n",
      "Iter-7500, train loss-2.2047, acc-0.3400, valid loss-2.1959, acc-0.3408, test loss-2.1982, acc-0.3346\n",
      "Iter-7510, train loss-2.1921, acc-0.3800, valid loss-2.1958, acc-0.3412, test loss-2.1981, acc-0.3343\n",
      "Iter-7520, train loss-2.2038, acc-0.2800, valid loss-2.1957, acc-0.3414, test loss-2.1980, acc-0.3344\n",
      "Iter-7530, train loss-2.2116, acc-0.2800, valid loss-2.1956, acc-0.3414, test loss-2.1978, acc-0.3347\n",
      "Iter-7540, train loss-2.1687, acc-0.4000, valid loss-2.1955, acc-0.3416, test loss-2.1977, acc-0.3347\n",
      "Iter-7550, train loss-2.1990, acc-0.3200, valid loss-2.1954, acc-0.3412, test loss-2.1976, acc-0.3348\n",
      "Iter-7560, train loss-2.1785, acc-0.4600, valid loss-2.1953, acc-0.3416, test loss-2.1975, acc-0.3351\n",
      "Iter-7570, train loss-2.1912, acc-0.4200, valid loss-2.1952, acc-0.3416, test loss-2.1974, acc-0.3351\n",
      "Iter-7580, train loss-2.2353, acc-0.2400, valid loss-2.1951, acc-0.3414, test loss-2.1973, acc-0.3354\n",
      "Iter-7590, train loss-2.1924, acc-0.3400, valid loss-2.1950, acc-0.3416, test loss-2.1972, acc-0.3354\n",
      "Iter-7600, train loss-2.2097, acc-0.2800, valid loss-2.1948, acc-0.3416, test loss-2.1971, acc-0.3359\n",
      "Iter-7610, train loss-2.2269, acc-0.3000, valid loss-2.1947, acc-0.3416, test loss-2.1970, acc-0.3362\n",
      "Iter-7620, train loss-2.2192, acc-0.2400, valid loss-2.1946, acc-0.3418, test loss-2.1969, acc-0.3363\n",
      "Iter-7630, train loss-2.1951, acc-0.3000, valid loss-2.1945, acc-0.3422, test loss-2.1968, acc-0.3365\n",
      "Iter-7640, train loss-2.2112, acc-0.3400, valid loss-2.1944, acc-0.3424, test loss-2.1967, acc-0.3365\n",
      "Iter-7650, train loss-2.2123, acc-0.3000, valid loss-2.1943, acc-0.3422, test loss-2.1966, acc-0.3367\n",
      "Iter-7660, train loss-2.1948, acc-0.3800, valid loss-2.1942, acc-0.3426, test loss-2.1965, acc-0.3370\n",
      "Iter-7670, train loss-2.1927, acc-0.3800, valid loss-2.1941, acc-0.3426, test loss-2.1964, acc-0.3370\n",
      "Iter-7680, train loss-2.2142, acc-0.2800, valid loss-2.1940, acc-0.3424, test loss-2.1963, acc-0.3370\n",
      "Iter-7690, train loss-2.1930, acc-0.3000, valid loss-2.1939, acc-0.3432, test loss-2.1962, acc-0.3372\n",
      "Iter-7700, train loss-2.1855, acc-0.3000, valid loss-2.1938, acc-0.3428, test loss-2.1961, acc-0.3370\n",
      "Iter-7710, train loss-2.1626, acc-0.4400, valid loss-2.1937, acc-0.3432, test loss-2.1960, acc-0.3377\n",
      "Iter-7720, train loss-2.1913, acc-0.3600, valid loss-2.1936, acc-0.3432, test loss-2.1959, acc-0.3378\n",
      "Iter-7730, train loss-2.2043, acc-0.3400, valid loss-2.1935, acc-0.3432, test loss-2.1958, acc-0.3376\n",
      "Iter-7740, train loss-2.2057, acc-0.2400, valid loss-2.1934, acc-0.3436, test loss-2.1957, acc-0.3376\n",
      "Iter-7750, train loss-2.2194, acc-0.2400, valid loss-2.1933, acc-0.3440, test loss-2.1956, acc-0.3378\n",
      "Iter-7760, train loss-2.1865, acc-0.4000, valid loss-2.1932, acc-0.3444, test loss-2.1954, acc-0.3381\n",
      "Iter-7770, train loss-2.2153, acc-0.2600, valid loss-2.1930, acc-0.3442, test loss-2.1953, acc-0.3382\n",
      "Iter-7780, train loss-2.2072, acc-0.3000, valid loss-2.1929, acc-0.3442, test loss-2.1952, acc-0.3383\n",
      "Iter-7790, train loss-2.1789, acc-0.3600, valid loss-2.1928, acc-0.3442, test loss-2.1951, acc-0.3383\n",
      "Iter-7800, train loss-2.1803, acc-0.4400, valid loss-2.1927, acc-0.3444, test loss-2.1950, acc-0.3382\n",
      "Iter-7810, train loss-2.2128, acc-0.3000, valid loss-2.1926, acc-0.3444, test loss-2.1949, acc-0.3386\n",
      "Iter-7820, train loss-2.1727, acc-0.4200, valid loss-2.1925, acc-0.3444, test loss-2.1948, acc-0.3389\n",
      "Iter-7830, train loss-2.1765, acc-0.3800, valid loss-2.1924, acc-0.3448, test loss-2.1947, acc-0.3390\n",
      "Iter-7840, train loss-2.2089, acc-0.2800, valid loss-2.1923, acc-0.3452, test loss-2.1946, acc-0.3394\n",
      "Iter-7850, train loss-2.2155, acc-0.2200, valid loss-2.1922, acc-0.3450, test loss-2.1945, acc-0.3394\n",
      "Iter-7860, train loss-2.1672, acc-0.4600, valid loss-2.1921, acc-0.3450, test loss-2.1944, acc-0.3396\n",
      "Iter-7870, train loss-2.2234, acc-0.2800, valid loss-2.1920, acc-0.3452, test loss-2.1943, acc-0.3397\n",
      "Iter-7880, train loss-2.1906, acc-0.4000, valid loss-2.1919, acc-0.3456, test loss-2.1942, acc-0.3397\n",
      "Iter-7890, train loss-2.1860, acc-0.3800, valid loss-2.1918, acc-0.3456, test loss-2.1941, acc-0.3397\n",
      "Iter-7900, train loss-2.1938, acc-0.3200, valid loss-2.1917, acc-0.3460, test loss-2.1940, acc-0.3400\n",
      "Iter-7910, train loss-2.1934, acc-0.2600, valid loss-2.1916, acc-0.3460, test loss-2.1939, acc-0.3403\n",
      "Iter-7920, train loss-2.2266, acc-0.2200, valid loss-2.1915, acc-0.3460, test loss-2.1938, acc-0.3410\n",
      "Iter-7930, train loss-2.2022, acc-0.3600, valid loss-2.1914, acc-0.3460, test loss-2.1937, acc-0.3409\n",
      "Iter-7940, train loss-2.2123, acc-0.3000, valid loss-2.1913, acc-0.3460, test loss-2.1936, acc-0.3407\n",
      "Iter-7950, train loss-2.1996, acc-0.3400, valid loss-2.1912, acc-0.3462, test loss-2.1935, acc-0.3405\n",
      "Iter-7960, train loss-2.2086, acc-0.3800, valid loss-2.1911, acc-0.3464, test loss-2.1934, acc-0.3408\n",
      "Iter-7970, train loss-2.1891, acc-0.3600, valid loss-2.1910, acc-0.3462, test loss-2.1933, acc-0.3407\n",
      "Iter-7980, train loss-2.2171, acc-0.2200, valid loss-2.1908, acc-0.3464, test loss-2.1932, acc-0.3408\n",
      "Iter-7990, train loss-2.1761, acc-0.4000, valid loss-2.1908, acc-0.3464, test loss-2.1931, acc-0.3410\n",
      "Iter-8000, train loss-2.1623, acc-0.4600, valid loss-2.1906, acc-0.3458, test loss-2.1930, acc-0.3407\n",
      "Iter-8010, train loss-2.1726, acc-0.3400, valid loss-2.1905, acc-0.3456, test loss-2.1929, acc-0.3410\n",
      "Iter-8020, train loss-2.1801, acc-0.3400, valid loss-2.1904, acc-0.3456, test loss-2.1928, acc-0.3409\n",
      "Iter-8030, train loss-2.1854, acc-0.3200, valid loss-2.1903, acc-0.3456, test loss-2.1927, acc-0.3410\n",
      "Iter-8040, train loss-2.2154, acc-0.1800, valid loss-2.1902, acc-0.3460, test loss-2.1926, acc-0.3413\n",
      "Iter-8050, train loss-2.2174, acc-0.2800, valid loss-2.1901, acc-0.3460, test loss-2.1924, acc-0.3409\n",
      "Iter-8060, train loss-2.1839, acc-0.3400, valid loss-2.1900, acc-0.3458, test loss-2.1923, acc-0.3412\n",
      "Iter-8070, train loss-2.1896, acc-0.3000, valid loss-2.1899, acc-0.3458, test loss-2.1922, acc-0.3413\n",
      "Iter-8080, train loss-2.1774, acc-0.3200, valid loss-2.1898, acc-0.3458, test loss-2.1921, acc-0.3415\n",
      "Iter-8090, train loss-2.1765, acc-0.4200, valid loss-2.1897, acc-0.3458, test loss-2.1920, acc-0.3418\n",
      "Iter-8100, train loss-2.2053, acc-0.2800, valid loss-2.1896, acc-0.3468, test loss-2.1919, acc-0.3423\n",
      "Iter-8110, train loss-2.2088, acc-0.3200, valid loss-2.1895, acc-0.3466, test loss-2.1918, acc-0.3425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-2.1805, acc-0.3600, valid loss-2.1894, acc-0.3470, test loss-2.1917, acc-0.3424\n",
      "Iter-8130, train loss-2.2248, acc-0.2000, valid loss-2.1893, acc-0.3472, test loss-2.1916, acc-0.3427\n",
      "Iter-8140, train loss-2.2098, acc-0.3000, valid loss-2.1892, acc-0.3476, test loss-2.1915, acc-0.3428\n",
      "Iter-8150, train loss-2.1964, acc-0.3400, valid loss-2.1891, acc-0.3474, test loss-2.1914, acc-0.3428\n",
      "Iter-8160, train loss-2.1760, acc-0.4400, valid loss-2.1890, acc-0.3474, test loss-2.1913, acc-0.3429\n",
      "Iter-8170, train loss-2.1871, acc-0.2800, valid loss-2.1889, acc-0.3472, test loss-2.1912, acc-0.3436\n",
      "Iter-8180, train loss-2.2285, acc-0.2400, valid loss-2.1888, acc-0.3472, test loss-2.1911, acc-0.3441\n",
      "Iter-8190, train loss-2.1884, acc-0.3600, valid loss-2.1887, acc-0.3476, test loss-2.1910, acc-0.3442\n",
      "Iter-8200, train loss-2.1739, acc-0.4800, valid loss-2.1886, acc-0.3480, test loss-2.1909, acc-0.3440\n",
      "Iter-8210, train loss-2.2012, acc-0.2800, valid loss-2.1885, acc-0.3480, test loss-2.1908, acc-0.3443\n",
      "Iter-8220, train loss-2.1835, acc-0.3400, valid loss-2.1883, acc-0.3478, test loss-2.1907, acc-0.3443\n",
      "Iter-8230, train loss-2.2091, acc-0.2600, valid loss-2.1882, acc-0.3484, test loss-2.1906, acc-0.3443\n",
      "Iter-8240, train loss-2.1843, acc-0.3600, valid loss-2.1881, acc-0.3486, test loss-2.1905, acc-0.3446\n",
      "Iter-8250, train loss-2.2139, acc-0.3000, valid loss-2.1880, acc-0.3486, test loss-2.1904, acc-0.3444\n",
      "Iter-8260, train loss-2.1878, acc-0.3800, valid loss-2.1879, acc-0.3482, test loss-2.1903, acc-0.3448\n",
      "Iter-8270, train loss-2.2080, acc-0.2800, valid loss-2.1878, acc-0.3486, test loss-2.1902, acc-0.3449\n",
      "Iter-8280, train loss-2.2032, acc-0.3200, valid loss-2.1877, acc-0.3486, test loss-2.1901, acc-0.3447\n",
      "Iter-8290, train loss-2.1688, acc-0.3600, valid loss-2.1876, acc-0.3486, test loss-2.1900, acc-0.3448\n",
      "Iter-8300, train loss-2.2026, acc-0.2800, valid loss-2.1875, acc-0.3482, test loss-2.1899, acc-0.3449\n",
      "Iter-8310, train loss-2.1994, acc-0.3800, valid loss-2.1874, acc-0.3482, test loss-2.1898, acc-0.3450\n",
      "Iter-8320, train loss-2.1757, acc-0.3800, valid loss-2.1873, acc-0.3490, test loss-2.1897, acc-0.3451\n",
      "Iter-8330, train loss-2.2041, acc-0.3600, valid loss-2.1872, acc-0.3484, test loss-2.1896, acc-0.3452\n",
      "Iter-8340, train loss-2.1916, acc-0.3600, valid loss-2.1871, acc-0.3482, test loss-2.1895, acc-0.3453\n",
      "Iter-8350, train loss-2.1988, acc-0.2600, valid loss-2.1870, acc-0.3480, test loss-2.1894, acc-0.3454\n",
      "Iter-8360, train loss-2.2009, acc-0.3600, valid loss-2.1869, acc-0.3488, test loss-2.1893, acc-0.3455\n",
      "Iter-8370, train loss-2.1711, acc-0.4200, valid loss-2.1868, acc-0.3494, test loss-2.1892, acc-0.3458\n",
      "Iter-8380, train loss-2.1746, acc-0.3400, valid loss-2.1867, acc-0.3492, test loss-2.1891, acc-0.3459\n",
      "Iter-8390, train loss-2.1978, acc-0.3600, valid loss-2.1866, acc-0.3490, test loss-2.1890, acc-0.3458\n",
      "Iter-8400, train loss-2.1804, acc-0.3400, valid loss-2.1865, acc-0.3490, test loss-2.1889, acc-0.3458\n",
      "Iter-8410, train loss-2.1831, acc-0.4000, valid loss-2.1864, acc-0.3494, test loss-2.1888, acc-0.3458\n",
      "Iter-8420, train loss-2.1992, acc-0.3000, valid loss-2.1863, acc-0.3498, test loss-2.1887, acc-0.3461\n",
      "Iter-8430, train loss-2.1966, acc-0.2600, valid loss-2.1862, acc-0.3496, test loss-2.1886, acc-0.3462\n",
      "Iter-8440, train loss-2.1716, acc-0.3800, valid loss-2.1861, acc-0.3494, test loss-2.1885, acc-0.3460\n",
      "Iter-8450, train loss-2.1754, acc-0.4000, valid loss-2.1860, acc-0.3494, test loss-2.1884, acc-0.3459\n",
      "Iter-8460, train loss-2.1936, acc-0.3800, valid loss-2.1859, acc-0.3496, test loss-2.1883, acc-0.3460\n",
      "Iter-8470, train loss-2.1978, acc-0.3800, valid loss-2.1858, acc-0.3500, test loss-2.1882, acc-0.3463\n",
      "Iter-8480, train loss-2.1953, acc-0.3800, valid loss-2.1857, acc-0.3500, test loss-2.1881, acc-0.3465\n",
      "Iter-8490, train loss-2.1738, acc-0.3200, valid loss-2.1856, acc-0.3504, test loss-2.1880, acc-0.3464\n",
      "Iter-8500, train loss-2.2018, acc-0.3800, valid loss-2.1855, acc-0.3504, test loss-2.1879, acc-0.3465\n",
      "Iter-8510, train loss-2.1743, acc-0.4200, valid loss-2.1854, acc-0.3510, test loss-2.1878, acc-0.3465\n",
      "Iter-8520, train loss-2.1542, acc-0.4600, valid loss-2.1853, acc-0.3518, test loss-2.1877, acc-0.3467\n",
      "Iter-8530, train loss-2.1714, acc-0.4800, valid loss-2.1852, acc-0.3516, test loss-2.1876, acc-0.3469\n",
      "Iter-8540, train loss-2.1611, acc-0.4200, valid loss-2.1850, acc-0.3520, test loss-2.1875, acc-0.3474\n",
      "Iter-8550, train loss-2.2294, acc-0.2000, valid loss-2.1849, acc-0.3522, test loss-2.1874, acc-0.3475\n",
      "Iter-8560, train loss-2.1783, acc-0.4000, valid loss-2.1848, acc-0.3524, test loss-2.1873, acc-0.3480\n",
      "Iter-8570, train loss-2.1866, acc-0.3800, valid loss-2.1847, acc-0.3526, test loss-2.1872, acc-0.3484\n",
      "Iter-8580, train loss-2.1983, acc-0.3000, valid loss-2.1846, acc-0.3522, test loss-2.1871, acc-0.3489\n",
      "Iter-8590, train loss-2.2036, acc-0.2600, valid loss-2.1845, acc-0.3522, test loss-2.1870, acc-0.3488\n",
      "Iter-8600, train loss-2.1884, acc-0.3400, valid loss-2.1844, acc-0.3524, test loss-2.1869, acc-0.3488\n",
      "Iter-8610, train loss-2.2043, acc-0.3200, valid loss-2.1843, acc-0.3522, test loss-2.1867, acc-0.3491\n",
      "Iter-8620, train loss-2.1827, acc-0.4000, valid loss-2.1842, acc-0.3530, test loss-2.1866, acc-0.3489\n",
      "Iter-8630, train loss-2.1714, acc-0.3600, valid loss-2.1841, acc-0.3534, test loss-2.1865, acc-0.3492\n",
      "Iter-8640, train loss-2.1936, acc-0.3600, valid loss-2.1840, acc-0.3542, test loss-2.1864, acc-0.3495\n",
      "Iter-8650, train loss-2.1689, acc-0.4600, valid loss-2.1839, acc-0.3540, test loss-2.1863, acc-0.3496\n",
      "Iter-8660, train loss-2.1671, acc-0.4600, valid loss-2.1838, acc-0.3542, test loss-2.1862, acc-0.3498\n",
      "Iter-8670, train loss-2.2162, acc-0.2800, valid loss-2.1837, acc-0.3542, test loss-2.1861, acc-0.3501\n",
      "Iter-8680, train loss-2.1938, acc-0.3000, valid loss-2.1836, acc-0.3542, test loss-2.1860, acc-0.3501\n",
      "Iter-8690, train loss-2.1605, acc-0.4200, valid loss-2.1835, acc-0.3544, test loss-2.1859, acc-0.3505\n",
      "Iter-8700, train loss-2.1605, acc-0.4400, valid loss-2.1834, acc-0.3542, test loss-2.1858, acc-0.3500\n",
      "Iter-8710, train loss-2.1887, acc-0.3200, valid loss-2.1833, acc-0.3540, test loss-2.1857, acc-0.3504\n",
      "Iter-8720, train loss-2.1844, acc-0.3200, valid loss-2.1832, acc-0.3544, test loss-2.1856, acc-0.3503\n",
      "Iter-8730, train loss-2.1988, acc-0.3200, valid loss-2.1831, acc-0.3544, test loss-2.1855, acc-0.3505\n",
      "Iter-8740, train loss-2.1712, acc-0.3800, valid loss-2.1830, acc-0.3540, test loss-2.1854, acc-0.3507\n",
      "Iter-8750, train loss-2.1926, acc-0.3400, valid loss-2.1829, acc-0.3542, test loss-2.1853, acc-0.3509\n",
      "Iter-8760, train loss-2.1607, acc-0.4800, valid loss-2.1828, acc-0.3542, test loss-2.1852, acc-0.3506\n",
      "Iter-8770, train loss-2.1718, acc-0.4000, valid loss-2.1827, acc-0.3542, test loss-2.1851, acc-0.3504\n",
      "Iter-8780, train loss-2.2000, acc-0.3400, valid loss-2.1826, acc-0.3542, test loss-2.1850, acc-0.3507\n",
      "Iter-8790, train loss-2.1897, acc-0.4000, valid loss-2.1825, acc-0.3544, test loss-2.1849, acc-0.3503\n",
      "Iter-8800, train loss-2.1702, acc-0.3200, valid loss-2.1824, acc-0.3542, test loss-2.1848, acc-0.3504\n",
      "Iter-8810, train loss-2.1939, acc-0.3000, valid loss-2.1823, acc-0.3544, test loss-2.1847, acc-0.3502\n",
      "Iter-8820, train loss-2.1741, acc-0.3400, valid loss-2.1822, acc-0.3548, test loss-2.1846, acc-0.3504\n",
      "Iter-8830, train loss-2.2104, acc-0.2800, valid loss-2.1821, acc-0.3548, test loss-2.1845, acc-0.3507\n",
      "Iter-8840, train loss-2.1707, acc-0.4400, valid loss-2.1820, acc-0.3550, test loss-2.1844, acc-0.3507\n",
      "Iter-8850, train loss-2.1820, acc-0.3200, valid loss-2.1819, acc-0.3554, test loss-2.1843, acc-0.3505\n",
      "Iter-8860, train loss-2.1616, acc-0.3000, valid loss-2.1818, acc-0.3558, test loss-2.1842, acc-0.3508\n",
      "Iter-8870, train loss-2.2214, acc-0.3000, valid loss-2.1817, acc-0.3556, test loss-2.1841, acc-0.3507\n",
      "Iter-8880, train loss-2.1941, acc-0.2600, valid loss-2.1815, acc-0.3560, test loss-2.1840, acc-0.3509\n",
      "Iter-8890, train loss-2.2013, acc-0.3000, valid loss-2.1814, acc-0.3560, test loss-2.1839, acc-0.3511\n",
      "Iter-8900, train loss-2.1610, acc-0.3200, valid loss-2.1813, acc-0.3560, test loss-2.1838, acc-0.3510\n",
      "Iter-8910, train loss-2.1733, acc-0.4000, valid loss-2.1812, acc-0.3562, test loss-2.1837, acc-0.3508\n",
      "Iter-8920, train loss-2.1696, acc-0.3400, valid loss-2.1811, acc-0.3562, test loss-2.1836, acc-0.3508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-2.1660, acc-0.3800, valid loss-2.1810, acc-0.3562, test loss-2.1835, acc-0.3508\n",
      "Iter-8940, train loss-2.1949, acc-0.3200, valid loss-2.1809, acc-0.3562, test loss-2.1834, acc-0.3514\n",
      "Iter-8950, train loss-2.1592, acc-0.4000, valid loss-2.1808, acc-0.3560, test loss-2.1833, acc-0.3516\n",
      "Iter-8960, train loss-2.1832, acc-0.3600, valid loss-2.1807, acc-0.3558, test loss-2.1832, acc-0.3515\n",
      "Iter-8970, train loss-2.2024, acc-0.1800, valid loss-2.1806, acc-0.3560, test loss-2.1831, acc-0.3515\n",
      "Iter-8980, train loss-2.1863, acc-0.3400, valid loss-2.1805, acc-0.3560, test loss-2.1830, acc-0.3515\n",
      "Iter-8990, train loss-2.1905, acc-0.3400, valid loss-2.1804, acc-0.3564, test loss-2.1829, acc-0.3515\n",
      "Iter-9000, train loss-2.1668, acc-0.4800, valid loss-2.1803, acc-0.3560, test loss-2.1828, acc-0.3515\n",
      "Iter-9010, train loss-2.1393, acc-0.4600, valid loss-2.1802, acc-0.3566, test loss-2.1827, acc-0.3516\n",
      "Iter-9020, train loss-2.1938, acc-0.2600, valid loss-2.1801, acc-0.3564, test loss-2.1826, acc-0.3513\n",
      "Iter-9030, train loss-2.1853, acc-0.3800, valid loss-2.1800, acc-0.3562, test loss-2.1825, acc-0.3512\n",
      "Iter-9040, train loss-2.2102, acc-0.2400, valid loss-2.1799, acc-0.3566, test loss-2.1824, acc-0.3516\n",
      "Iter-9050, train loss-2.1692, acc-0.4000, valid loss-2.1798, acc-0.3560, test loss-2.1823, acc-0.3514\n",
      "Iter-9060, train loss-2.1518, acc-0.4400, valid loss-2.1797, acc-0.3560, test loss-2.1822, acc-0.3521\n",
      "Iter-9070, train loss-2.1716, acc-0.4800, valid loss-2.1796, acc-0.3562, test loss-2.1821, acc-0.3523\n",
      "Iter-9080, train loss-2.1809, acc-0.3200, valid loss-2.1795, acc-0.3566, test loss-2.1820, acc-0.3520\n",
      "Iter-9090, train loss-2.2007, acc-0.3600, valid loss-2.1794, acc-0.3572, test loss-2.1819, acc-0.3526\n",
      "Iter-9100, train loss-2.1643, acc-0.3400, valid loss-2.1793, acc-0.3572, test loss-2.1818, acc-0.3530\n",
      "Iter-9110, train loss-2.1729, acc-0.5000, valid loss-2.1792, acc-0.3574, test loss-2.1817, acc-0.3533\n",
      "Iter-9120, train loss-2.1916, acc-0.2800, valid loss-2.1791, acc-0.3572, test loss-2.1816, acc-0.3534\n",
      "Iter-9130, train loss-2.1880, acc-0.3800, valid loss-2.1790, acc-0.3574, test loss-2.1815, acc-0.3535\n",
      "Iter-9140, train loss-2.1697, acc-0.3200, valid loss-2.1789, acc-0.3572, test loss-2.1814, acc-0.3538\n",
      "Iter-9150, train loss-2.1701, acc-0.4200, valid loss-2.1788, acc-0.3574, test loss-2.1813, acc-0.3536\n",
      "Iter-9160, train loss-2.1835, acc-0.3600, valid loss-2.1787, acc-0.3576, test loss-2.1812, acc-0.3537\n",
      "Iter-9170, train loss-2.1740, acc-0.4200, valid loss-2.1786, acc-0.3572, test loss-2.1811, acc-0.3539\n",
      "Iter-9180, train loss-2.2019, acc-0.3600, valid loss-2.1785, acc-0.3572, test loss-2.1810, acc-0.3541\n",
      "Iter-9190, train loss-2.1882, acc-0.4000, valid loss-2.1784, acc-0.3574, test loss-2.1809, acc-0.3541\n",
      "Iter-9200, train loss-2.1702, acc-0.4000, valid loss-2.1782, acc-0.3576, test loss-2.1808, acc-0.3542\n",
      "Iter-9210, train loss-2.1698, acc-0.4400, valid loss-2.1781, acc-0.3574, test loss-2.1807, acc-0.3545\n",
      "Iter-9220, train loss-2.1894, acc-0.3400, valid loss-2.1780, acc-0.3576, test loss-2.1806, acc-0.3545\n",
      "Iter-9230, train loss-2.1850, acc-0.3400, valid loss-2.1779, acc-0.3576, test loss-2.1805, acc-0.3545\n",
      "Iter-9240, train loss-2.1696, acc-0.3800, valid loss-2.1778, acc-0.3580, test loss-2.1804, acc-0.3551\n",
      "Iter-9250, train loss-2.1795, acc-0.3000, valid loss-2.1777, acc-0.3584, test loss-2.1803, acc-0.3552\n",
      "Iter-9260, train loss-2.1589, acc-0.4200, valid loss-2.1776, acc-0.3590, test loss-2.1802, acc-0.3550\n",
      "Iter-9270, train loss-2.1938, acc-0.3200, valid loss-2.1775, acc-0.3592, test loss-2.1801, acc-0.3553\n",
      "Iter-9280, train loss-2.1725, acc-0.3800, valid loss-2.1774, acc-0.3590, test loss-2.1800, acc-0.3551\n",
      "Iter-9290, train loss-2.1825, acc-0.4400, valid loss-2.1773, acc-0.3590, test loss-2.1799, acc-0.3552\n",
      "Iter-9300, train loss-2.1667, acc-0.4200, valid loss-2.1772, acc-0.3590, test loss-2.1798, acc-0.3552\n",
      "Iter-9310, train loss-2.1993, acc-0.3200, valid loss-2.1771, acc-0.3592, test loss-2.1797, acc-0.3554\n",
      "Iter-9320, train loss-2.1910, acc-0.3600, valid loss-2.1770, acc-0.3588, test loss-2.1796, acc-0.3556\n",
      "Iter-9330, train loss-2.2073, acc-0.3000, valid loss-2.1769, acc-0.3588, test loss-2.1795, acc-0.3560\n",
      "Iter-9340, train loss-2.1481, acc-0.4800, valid loss-2.1768, acc-0.3590, test loss-2.1794, acc-0.3558\n",
      "Iter-9350, train loss-2.1840, acc-0.3400, valid loss-2.1767, acc-0.3584, test loss-2.1793, acc-0.3556\n",
      "Iter-9360, train loss-2.1858, acc-0.3400, valid loss-2.1766, acc-0.3592, test loss-2.1792, acc-0.3557\n",
      "Iter-9370, train loss-2.1791, acc-0.4400, valid loss-2.1765, acc-0.3590, test loss-2.1791, acc-0.3559\n",
      "Iter-9380, train loss-2.1685, acc-0.3600, valid loss-2.1764, acc-0.3592, test loss-2.1790, acc-0.3559\n",
      "Iter-9390, train loss-2.1497, acc-0.4200, valid loss-2.1763, acc-0.3588, test loss-2.1789, acc-0.3557\n",
      "Iter-9400, train loss-2.1619, acc-0.4200, valid loss-2.1762, acc-0.3590, test loss-2.1788, acc-0.3557\n",
      "Iter-9410, train loss-2.1908, acc-0.3200, valid loss-2.1761, acc-0.3590, test loss-2.1787, acc-0.3556\n",
      "Iter-9420, train loss-2.1950, acc-0.3000, valid loss-2.1760, acc-0.3590, test loss-2.1786, acc-0.3556\n",
      "Iter-9430, train loss-2.1611, acc-0.4200, valid loss-2.1759, acc-0.3598, test loss-2.1785, acc-0.3555\n",
      "Iter-9440, train loss-2.1948, acc-0.4000, valid loss-2.1758, acc-0.3596, test loss-2.1784, acc-0.3554\n",
      "Iter-9450, train loss-2.2207, acc-0.2200, valid loss-2.1757, acc-0.3596, test loss-2.1783, acc-0.3556\n",
      "Iter-9460, train loss-2.1546, acc-0.4200, valid loss-2.1756, acc-0.3594, test loss-2.1782, acc-0.3560\n",
      "Iter-9470, train loss-2.2063, acc-0.2000, valid loss-2.1755, acc-0.3598, test loss-2.1781, acc-0.3561\n",
      "Iter-9480, train loss-2.1879, acc-0.3200, valid loss-2.1754, acc-0.3592, test loss-2.1780, acc-0.3561\n",
      "Iter-9490, train loss-2.1828, acc-0.2800, valid loss-2.1753, acc-0.3596, test loss-2.1779, acc-0.3560\n",
      "Iter-9500, train loss-2.1650, acc-0.3400, valid loss-2.1752, acc-0.3600, test loss-2.1778, acc-0.3567\n",
      "Iter-9510, train loss-2.1813, acc-0.4000, valid loss-2.1751, acc-0.3600, test loss-2.1777, acc-0.3567\n",
      "Iter-9520, train loss-2.1759, acc-0.3600, valid loss-2.1750, acc-0.3602, test loss-2.1776, acc-0.3565\n",
      "Iter-9530, train loss-2.1667, acc-0.4000, valid loss-2.1749, acc-0.3604, test loss-2.1775, acc-0.3571\n",
      "Iter-9540, train loss-2.2064, acc-0.2600, valid loss-2.1748, acc-0.3612, test loss-2.1774, acc-0.3570\n",
      "Iter-9550, train loss-2.1725, acc-0.3400, valid loss-2.1747, acc-0.3614, test loss-2.1773, acc-0.3571\n",
      "Iter-9560, train loss-2.1541, acc-0.4200, valid loss-2.1746, acc-0.3618, test loss-2.1772, acc-0.3570\n",
      "Iter-9570, train loss-2.1737, acc-0.3000, valid loss-2.1745, acc-0.3614, test loss-2.1771, acc-0.3571\n",
      "Iter-9580, train loss-2.1571, acc-0.4200, valid loss-2.1744, acc-0.3614, test loss-2.1769, acc-0.3569\n",
      "Iter-9590, train loss-2.1867, acc-0.3000, valid loss-2.1743, acc-0.3616, test loss-2.1768, acc-0.3570\n",
      "Iter-9600, train loss-2.1888, acc-0.2800, valid loss-2.1742, acc-0.3616, test loss-2.1767, acc-0.3572\n",
      "Iter-9610, train loss-2.1764, acc-0.3600, valid loss-2.1741, acc-0.3624, test loss-2.1766, acc-0.3571\n",
      "Iter-9620, train loss-2.1402, acc-0.4400, valid loss-2.1740, acc-0.3624, test loss-2.1765, acc-0.3569\n",
      "Iter-9630, train loss-2.1950, acc-0.3600, valid loss-2.1739, acc-0.3622, test loss-2.1764, acc-0.3573\n",
      "Iter-9640, train loss-2.1809, acc-0.3800, valid loss-2.1738, acc-0.3624, test loss-2.1763, acc-0.3573\n",
      "Iter-9650, train loss-2.2041, acc-0.3200, valid loss-2.1737, acc-0.3624, test loss-2.1762, acc-0.3573\n",
      "Iter-9660, train loss-2.1706, acc-0.3600, valid loss-2.1736, acc-0.3626, test loss-2.1761, acc-0.3577\n",
      "Iter-9670, train loss-2.1650, acc-0.3200, valid loss-2.1735, acc-0.3626, test loss-2.1760, acc-0.3577\n",
      "Iter-9680, train loss-2.1750, acc-0.3200, valid loss-2.1733, acc-0.3624, test loss-2.1759, acc-0.3582\n",
      "Iter-9690, train loss-2.1741, acc-0.3800, valid loss-2.1732, acc-0.3626, test loss-2.1758, acc-0.3577\n",
      "Iter-9700, train loss-2.1980, acc-0.3600, valid loss-2.1731, acc-0.3620, test loss-2.1757, acc-0.3576\n",
      "Iter-9710, train loss-2.1766, acc-0.3600, valid loss-2.1730, acc-0.3622, test loss-2.1756, acc-0.3577\n",
      "Iter-9720, train loss-2.1761, acc-0.3200, valid loss-2.1729, acc-0.3620, test loss-2.1755, acc-0.3581\n",
      "Iter-9730, train loss-2.1659, acc-0.3600, valid loss-2.1728, acc-0.3622, test loss-2.1754, acc-0.3582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-2.1523, acc-0.4200, valid loss-2.1727, acc-0.3622, test loss-2.1753, acc-0.3583\n",
      "Iter-9750, train loss-2.1506, acc-0.3200, valid loss-2.1726, acc-0.3622, test loss-2.1752, acc-0.3585\n",
      "Iter-9760, train loss-2.1959, acc-0.3600, valid loss-2.1725, acc-0.3622, test loss-2.1751, acc-0.3586\n",
      "Iter-9770, train loss-2.1906, acc-0.1800, valid loss-2.1724, acc-0.3626, test loss-2.1750, acc-0.3587\n",
      "Iter-9780, train loss-2.1577, acc-0.3600, valid loss-2.1723, acc-0.3628, test loss-2.1749, acc-0.3590\n",
      "Iter-9790, train loss-2.1710, acc-0.3000, valid loss-2.1722, acc-0.3630, test loss-2.1748, acc-0.3593\n",
      "Iter-9800, train loss-2.2061, acc-0.2600, valid loss-2.1721, acc-0.3624, test loss-2.1747, acc-0.3592\n",
      "Iter-9810, train loss-2.1674, acc-0.4200, valid loss-2.1720, acc-0.3624, test loss-2.1746, acc-0.3593\n",
      "Iter-9820, train loss-2.2036, acc-0.2400, valid loss-2.1719, acc-0.3622, test loss-2.1745, acc-0.3592\n",
      "Iter-9830, train loss-2.1818, acc-0.3200, valid loss-2.1718, acc-0.3626, test loss-2.1744, acc-0.3591\n",
      "Iter-9840, train loss-2.1921, acc-0.3800, valid loss-2.1717, acc-0.3628, test loss-2.1743, acc-0.3592\n",
      "Iter-9850, train loss-2.1907, acc-0.3000, valid loss-2.1716, acc-0.3626, test loss-2.1742, acc-0.3594\n",
      "Iter-9860, train loss-2.1802, acc-0.3000, valid loss-2.1715, acc-0.3628, test loss-2.1741, acc-0.3594\n",
      "Iter-9870, train loss-2.1877, acc-0.3200, valid loss-2.1714, acc-0.3632, test loss-2.1740, acc-0.3596\n",
      "Iter-9880, train loss-2.1906, acc-0.2800, valid loss-2.1713, acc-0.3632, test loss-2.1739, acc-0.3595\n",
      "Iter-9890, train loss-2.1516, acc-0.4800, valid loss-2.1712, acc-0.3634, test loss-2.1738, acc-0.3599\n",
      "Iter-9900, train loss-2.1788, acc-0.3400, valid loss-2.1711, acc-0.3636, test loss-2.1737, acc-0.3600\n",
      "Iter-9910, train loss-2.1397, acc-0.5200, valid loss-2.1710, acc-0.3638, test loss-2.1736, acc-0.3602\n",
      "Iter-9920, train loss-2.1850, acc-0.3600, valid loss-2.1709, acc-0.3640, test loss-2.1735, acc-0.3605\n",
      "Iter-9930, train loss-2.1812, acc-0.3600, valid loss-2.1708, acc-0.3638, test loss-2.1734, acc-0.3609\n",
      "Iter-9940, train loss-2.1859, acc-0.2400, valid loss-2.1707, acc-0.3642, test loss-2.1733, acc-0.3610\n",
      "Iter-9950, train loss-2.2099, acc-0.2400, valid loss-2.1706, acc-0.3642, test loss-2.1732, acc-0.3611\n",
      "Iter-9960, train loss-2.1858, acc-0.3400, valid loss-2.1705, acc-0.3638, test loss-2.1731, acc-0.3615\n",
      "Iter-9970, train loss-2.1839, acc-0.3200, valid loss-2.1704, acc-0.3640, test loss-2.1730, acc-0.3614\n",
      "Iter-9980, train loss-2.1472, acc-0.4000, valid loss-2.1703, acc-0.3640, test loss-2.1729, acc-0.3615\n",
      "Iter-9990, train loss-2.1521, acc-0.4400, valid loss-2.1702, acc-0.3636, test loss-2.1728, acc-0.3614\n",
      "Iter-10000, train loss-2.1452, acc-0.3800, valid loss-2.1701, acc-0.3634, test loss-2.1727, acc-0.3614\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzApcHMzMrMDFwczMClwczMys4D8M867XvuASUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11780d6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1138b9470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
