{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y = np.tanh(y)\n",
    "#         y, _ = l.relu_forward(X=y)\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#         y = l.sigmoid(X=y) # non-linearity\n",
    "        X = y.copy() # pass to the next layer\n",
    "        caches.append(fc_cache) # caches[0]\n",
    "        ys.append(y) # ys[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y = np.tanh(y)\n",
    "#             y, _ = l.relu_forward(X=y)\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = np.exp(y) #/ np.exp(y).sum(axis=1).reshape(-1, 1) # txn\n",
    "#             y = l.sigmoid(X=y) # non-linearity\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "        caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]\n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= self.ys[1][layer] - self.ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= self.ys[0] - self.ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.2995 valid loss: 2.3021, valid accuracy: 0.0694\n",
      "Iter-20 train loss: 2.3032 valid loss: 2.3021, valid accuracy: 0.0696\n",
      "Iter-30 train loss: 2.3012 valid loss: 2.3021, valid accuracy: 0.0698\n",
      "Iter-40 train loss: 2.3016 valid loss: 2.3021, valid accuracy: 0.0698\n",
      "Iter-50 train loss: 2.2994 valid loss: 2.3021, valid accuracy: 0.0698\n",
      "Iter-60 train loss: 2.3011 valid loss: 2.3021, valid accuracy: 0.0712\n",
      "Iter-70 train loss: 2.3035 valid loss: 2.3021, valid accuracy: 0.0710\n",
      "Iter-80 train loss: 2.3076 valid loss: 2.3020, valid accuracy: 0.0710\n",
      "Iter-90 train loss: 2.3041 valid loss: 2.3020, valid accuracy: 0.0716\n",
      "Iter-100 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.0724\n",
      "Iter-110 train loss: 2.3036 valid loss: 2.3020, valid accuracy: 0.0726\n",
      "Iter-120 train loss: 2.2996 valid loss: 2.3020, valid accuracy: 0.0734\n",
      "Iter-130 train loss: 2.2990 valid loss: 2.3020, valid accuracy: 0.0732\n",
      "Iter-140 train loss: 2.3019 valid loss: 2.3020, valid accuracy: 0.0732\n",
      "Iter-150 train loss: 2.3007 valid loss: 2.3020, valid accuracy: 0.0742\n",
      "Iter-160 train loss: 2.3021 valid loss: 2.3020, valid accuracy: 0.0742\n",
      "Iter-170 train loss: 2.3050 valid loss: 2.3020, valid accuracy: 0.0736\n",
      "Iter-180 train loss: 2.3044 valid loss: 2.3020, valid accuracy: 0.0740\n",
      "Iter-190 train loss: 2.3041 valid loss: 2.3020, valid accuracy: 0.0748\n",
      "Iter-200 train loss: 2.3020 valid loss: 2.3020, valid accuracy: 0.0746\n",
      "Iter-210 train loss: 2.3016 valid loss: 2.3020, valid accuracy: 0.0754\n",
      "Iter-220 train loss: 2.3009 valid loss: 2.3020, valid accuracy: 0.0748\n",
      "Iter-230 train loss: 2.3017 valid loss: 2.3020, valid accuracy: 0.0758\n",
      "Iter-240 train loss: 2.2982 valid loss: 2.3020, valid accuracy: 0.0764\n",
      "Iter-250 train loss: 2.2977 valid loss: 2.3019, valid accuracy: 0.0762\n",
      "Iter-260 train loss: 2.3000 valid loss: 2.3019, valid accuracy: 0.0778\n",
      "Iter-270 train loss: 2.3019 valid loss: 2.3019, valid accuracy: 0.0788\n",
      "Iter-280 train loss: 2.3017 valid loss: 2.3019, valid accuracy: 0.0790\n",
      "Iter-290 train loss: 2.3044 valid loss: 2.3019, valid accuracy: 0.0798\n",
      "Iter-300 train loss: 2.2998 valid loss: 2.3019, valid accuracy: 0.0810\n",
      "Iter-310 train loss: 2.3028 valid loss: 2.3019, valid accuracy: 0.0800\n",
      "Iter-320 train loss: 2.3036 valid loss: 2.3019, valid accuracy: 0.0812\n",
      "Iter-330 train loss: 2.2980 valid loss: 2.3019, valid accuracy: 0.0826\n",
      "Iter-340 train loss: 2.3002 valid loss: 2.3019, valid accuracy: 0.0832\n",
      "Iter-350 train loss: 2.3038 valid loss: 2.3019, valid accuracy: 0.0832\n",
      "Iter-360 train loss: 2.3050 valid loss: 2.3019, valid accuracy: 0.0848\n",
      "Iter-370 train loss: 2.3051 valid loss: 2.3019, valid accuracy: 0.0842\n",
      "Iter-380 train loss: 2.2999 valid loss: 2.3019, valid accuracy: 0.0856\n",
      "Iter-390 train loss: 2.3026 valid loss: 2.3019, valid accuracy: 0.0852\n",
      "Iter-400 train loss: 2.3062 valid loss: 2.3019, valid accuracy: 0.0854\n",
      "Iter-410 train loss: 2.2994 valid loss: 2.3018, valid accuracy: 0.0860\n",
      "Iter-420 train loss: 2.3012 valid loss: 2.3018, valid accuracy: 0.0862\n",
      "Iter-430 train loss: 2.3025 valid loss: 2.3018, valid accuracy: 0.0880\n",
      "Iter-440 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.0890\n",
      "Iter-450 train loss: 2.3040 valid loss: 2.3018, valid accuracy: 0.0884\n",
      "Iter-460 train loss: 2.3017 valid loss: 2.3018, valid accuracy: 0.0908\n",
      "Iter-470 train loss: 2.3035 valid loss: 2.3018, valid accuracy: 0.0910\n",
      "Iter-480 train loss: 2.3045 valid loss: 2.3018, valid accuracy: 0.0916\n",
      "Iter-490 train loss: 2.3000 valid loss: 2.3018, valid accuracy: 0.0926\n",
      "Iter-500 train loss: 2.2987 valid loss: 2.3018, valid accuracy: 0.0926\n",
      "Iter-510 train loss: 2.3014 valid loss: 2.3018, valid accuracy: 0.0928\n",
      "Iter-520 train loss: 2.3040 valid loss: 2.3018, valid accuracy: 0.0916\n",
      "Iter-530 train loss: 2.2997 valid loss: 2.3018, valid accuracy: 0.0934\n",
      "Iter-540 train loss: 2.3047 valid loss: 2.3018, valid accuracy: 0.0938\n",
      "Iter-550 train loss: 2.3048 valid loss: 2.3018, valid accuracy: 0.0954\n",
      "Iter-560 train loss: 2.2997 valid loss: 2.3017, valid accuracy: 0.0960\n",
      "Iter-570 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.0966\n",
      "Iter-580 train loss: 2.3038 valid loss: 2.3017, valid accuracy: 0.0978\n",
      "Iter-590 train loss: 2.3038 valid loss: 2.3017, valid accuracy: 0.0976\n",
      "Iter-600 train loss: 2.3054 valid loss: 2.3017, valid accuracy: 0.0978\n",
      "Iter-610 train loss: 2.2978 valid loss: 2.3017, valid accuracy: 0.0986\n",
      "Iter-620 train loss: 2.3016 valid loss: 2.3017, valid accuracy: 0.0988\n",
      "Iter-630 train loss: 2.3006 valid loss: 2.3017, valid accuracy: 0.0986\n",
      "Iter-640 train loss: 2.3017 valid loss: 2.3017, valid accuracy: 0.0988\n",
      "Iter-650 train loss: 2.2981 valid loss: 2.3017, valid accuracy: 0.0986\n",
      "Iter-660 train loss: 2.2984 valid loss: 2.3017, valid accuracy: 0.0994\n",
      "Iter-670 train loss: 2.2975 valid loss: 2.3017, valid accuracy: 0.0996\n",
      "Iter-680 train loss: 2.3012 valid loss: 2.3017, valid accuracy: 0.1020\n",
      "Iter-690 train loss: 2.2988 valid loss: 2.3017, valid accuracy: 0.1022\n",
      "Iter-700 train loss: 2.3030 valid loss: 2.3017, valid accuracy: 0.1026\n",
      "Iter-710 train loss: 2.3026 valid loss: 2.3017, valid accuracy: 0.1026\n",
      "Iter-720 train loss: 2.3047 valid loss: 2.3017, valid accuracy: 0.1046\n",
      "Iter-730 train loss: 2.3024 valid loss: 2.3016, valid accuracy: 0.1054\n",
      "Iter-740 train loss: 2.3030 valid loss: 2.3016, valid accuracy: 0.1056\n",
      "Iter-750 train loss: 2.2987 valid loss: 2.3016, valid accuracy: 0.1060\n",
      "Iter-760 train loss: 2.3012 valid loss: 2.3016, valid accuracy: 0.1056\n",
      "Iter-770 train loss: 2.2950 valid loss: 2.3016, valid accuracy: 0.1044\n",
      "Iter-780 train loss: 2.2959 valid loss: 2.3016, valid accuracy: 0.1060\n",
      "Iter-790 train loss: 2.3008 valid loss: 2.3016, valid accuracy: 0.1072\n",
      "Iter-800 train loss: 2.3005 valid loss: 2.3016, valid accuracy: 0.1076\n",
      "Iter-810 train loss: 2.2987 valid loss: 2.3016, valid accuracy: 0.1076\n",
      "Iter-820 train loss: 2.3004 valid loss: 2.3016, valid accuracy: 0.1082\n",
      "Iter-830 train loss: 2.3028 valid loss: 2.3016, valid accuracy: 0.1092\n",
      "Iter-840 train loss: 2.3014 valid loss: 2.3016, valid accuracy: 0.1102\n",
      "Iter-850 train loss: 2.3046 valid loss: 2.3016, valid accuracy: 0.1104\n",
      "Iter-860 train loss: 2.3021 valid loss: 2.3016, valid accuracy: 0.1102\n",
      "Iter-870 train loss: 2.3046 valid loss: 2.3016, valid accuracy: 0.1100\n",
      "Iter-880 train loss: 2.2976 valid loss: 2.3016, valid accuracy: 0.1112\n",
      "Iter-890 train loss: 2.2997 valid loss: 2.3016, valid accuracy: 0.1130\n",
      "Iter-900 train loss: 2.2953 valid loss: 2.3016, valid accuracy: 0.1132\n",
      "Iter-910 train loss: 2.3040 valid loss: 2.3016, valid accuracy: 0.1136\n",
      "Iter-920 train loss: 2.3002 valid loss: 2.3016, valid accuracy: 0.1132\n",
      "Iter-930 train loss: 2.2986 valid loss: 2.3016, valid accuracy: 0.1156\n",
      "Iter-940 train loss: 2.3016 valid loss: 2.3015, valid accuracy: 0.1156\n",
      "Iter-950 train loss: 2.2998 valid loss: 2.3015, valid accuracy: 0.1168\n",
      "Iter-960 train loss: 2.2978 valid loss: 2.3015, valid accuracy: 0.1162\n",
      "Iter-970 train loss: 2.3016 valid loss: 2.3015, valid accuracy: 0.1168\n",
      "Iter-980 train loss: 2.3033 valid loss: 2.3015, valid accuracy: 0.1186\n",
      "Iter-990 train loss: 2.3008 valid loss: 2.3015, valid accuracy: 0.1188\n",
      "Iter-1000 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1196\n",
      "Iter-1010 train loss: 2.2997 valid loss: 2.3015, valid accuracy: 0.1190\n",
      "Iter-1020 train loss: 2.3055 valid loss: 2.3015, valid accuracy: 0.1200\n",
      "Iter-1030 train loss: 2.3060 valid loss: 2.3015, valid accuracy: 0.1198\n",
      "Iter-1040 train loss: 2.3015 valid loss: 2.3015, valid accuracy: 0.1204\n",
      "Iter-1050 train loss: 2.3028 valid loss: 2.3015, valid accuracy: 0.1208\n",
      "Iter-1060 train loss: 2.3011 valid loss: 2.3015, valid accuracy: 0.1228\n",
      "Iter-1070 train loss: 2.3081 valid loss: 2.3015, valid accuracy: 0.1226\n",
      "Iter-1080 train loss: 2.2966 valid loss: 2.3015, valid accuracy: 0.1226\n",
      "Iter-1090 train loss: 2.2998 valid loss: 2.3015, valid accuracy: 0.1206\n",
      "Iter-1100 train loss: 2.3045 valid loss: 2.3015, valid accuracy: 0.1218\n",
      "Iter-1110 train loss: 2.3066 valid loss: 2.3015, valid accuracy: 0.1210\n",
      "Iter-1120 train loss: 2.3014 valid loss: 2.3014, valid accuracy: 0.1212\n",
      "Iter-1130 train loss: 2.3037 valid loss: 2.3014, valid accuracy: 0.1222\n",
      "Iter-1140 train loss: 2.2986 valid loss: 2.3014, valid accuracy: 0.1228\n",
      "Iter-1150 train loss: 2.3033 valid loss: 2.3014, valid accuracy: 0.1234\n",
      "Iter-1160 train loss: 2.3011 valid loss: 2.3014, valid accuracy: 0.1232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.2976 valid loss: 2.3014, valid accuracy: 0.1240\n",
      "Iter-1180 train loss: 2.3043 valid loss: 2.3014, valid accuracy: 0.1246\n",
      "Iter-1190 train loss: 2.3026 valid loss: 2.3014, valid accuracy: 0.1262\n",
      "Iter-1200 train loss: 2.3012 valid loss: 2.3014, valid accuracy: 0.1284\n",
      "Iter-1210 train loss: 2.3027 valid loss: 2.3014, valid accuracy: 0.1288\n",
      "Iter-1220 train loss: 2.2991 valid loss: 2.3014, valid accuracy: 0.1290\n",
      "Iter-1230 train loss: 2.2982 valid loss: 2.3014, valid accuracy: 0.1288\n",
      "Iter-1240 train loss: 2.3027 valid loss: 2.3014, valid accuracy: 0.1280\n",
      "Iter-1250 train loss: 2.2999 valid loss: 2.3014, valid accuracy: 0.1284\n",
      "Iter-1260 train loss: 2.2976 valid loss: 2.3014, valid accuracy: 0.1290\n",
      "Iter-1270 train loss: 2.2994 valid loss: 2.3014, valid accuracy: 0.1286\n",
      "Iter-1280 train loss: 2.3071 valid loss: 2.3014, valid accuracy: 0.1290\n",
      "Iter-1290 train loss: 2.3049 valid loss: 2.3014, valid accuracy: 0.1294\n",
      "Iter-1300 train loss: 2.3009 valid loss: 2.3014, valid accuracy: 0.1298\n",
      "Iter-1310 train loss: 2.3023 valid loss: 2.3014, valid accuracy: 0.1308\n",
      "Iter-1320 train loss: 2.3004 valid loss: 2.3013, valid accuracy: 0.1316\n",
      "Iter-1330 train loss: 2.3089 valid loss: 2.3013, valid accuracy: 0.1312\n",
      "Iter-1340 train loss: 2.3011 valid loss: 2.3013, valid accuracy: 0.1320\n",
      "Iter-1350 train loss: 2.2973 valid loss: 2.3013, valid accuracy: 0.1322\n",
      "Iter-1360 train loss: 2.3027 valid loss: 2.3013, valid accuracy: 0.1338\n",
      "Iter-1370 train loss: 2.3016 valid loss: 2.3013, valid accuracy: 0.1332\n",
      "Iter-1380 train loss: 2.3027 valid loss: 2.3013, valid accuracy: 0.1336\n",
      "Iter-1390 train loss: 2.3019 valid loss: 2.3013, valid accuracy: 0.1344\n",
      "Iter-1400 train loss: 2.3011 valid loss: 2.3013, valid accuracy: 0.1330\n",
      "Iter-1410 train loss: 2.3066 valid loss: 2.3013, valid accuracy: 0.1334\n",
      "Iter-1420 train loss: 2.3074 valid loss: 2.3013, valid accuracy: 0.1332\n",
      "Iter-1430 train loss: 2.3025 valid loss: 2.3013, valid accuracy: 0.1326\n",
      "Iter-1440 train loss: 2.2989 valid loss: 2.3013, valid accuracy: 0.1332\n",
      "Iter-1450 train loss: 2.3040 valid loss: 2.3013, valid accuracy: 0.1330\n",
      "Iter-1460 train loss: 2.2975 valid loss: 2.3013, valid accuracy: 0.1326\n",
      "Iter-1470 train loss: 2.3044 valid loss: 2.3012, valid accuracy: 0.1332\n",
      "Iter-1480 train loss: 2.3069 valid loss: 2.3012, valid accuracy: 0.1328\n",
      "Iter-1490 train loss: 2.3064 valid loss: 2.3012, valid accuracy: 0.1336\n",
      "Iter-1500 train loss: 2.3009 valid loss: 2.3012, valid accuracy: 0.1338\n",
      "Iter-1510 train loss: 2.2979 valid loss: 2.3012, valid accuracy: 0.1338\n",
      "Iter-1520 train loss: 2.3079 valid loss: 2.3012, valid accuracy: 0.1342\n",
      "Iter-1530 train loss: 2.3006 valid loss: 2.3012, valid accuracy: 0.1340\n",
      "Iter-1540 train loss: 2.2987 valid loss: 2.3012, valid accuracy: 0.1342\n",
      "Iter-1550 train loss: 2.3082 valid loss: 2.3012, valid accuracy: 0.1328\n",
      "Iter-1560 train loss: 2.2985 valid loss: 2.3012, valid accuracy: 0.1330\n",
      "Iter-1570 train loss: 2.2993 valid loss: 2.3012, valid accuracy: 0.1334\n",
      "Iter-1580 train loss: 2.3033 valid loss: 2.3012, valid accuracy: 0.1342\n",
      "Iter-1590 train loss: 2.3064 valid loss: 2.3012, valid accuracy: 0.1348\n",
      "Iter-1600 train loss: 2.3061 valid loss: 2.3012, valid accuracy: 0.1346\n",
      "Iter-1610 train loss: 2.3038 valid loss: 2.3012, valid accuracy: 0.1346\n",
      "Iter-1620 train loss: 2.3003 valid loss: 2.3012, valid accuracy: 0.1348\n",
      "Iter-1630 train loss: 2.3039 valid loss: 2.3012, valid accuracy: 0.1354\n",
      "Iter-1640 train loss: 2.3010 valid loss: 2.3012, valid accuracy: 0.1356\n",
      "Iter-1650 train loss: 2.2983 valid loss: 2.3012, valid accuracy: 0.1370\n",
      "Iter-1660 train loss: 2.3014 valid loss: 2.3011, valid accuracy: 0.1376\n",
      "Iter-1670 train loss: 2.3068 valid loss: 2.3011, valid accuracy: 0.1376\n",
      "Iter-1680 train loss: 2.3023 valid loss: 2.3011, valid accuracy: 0.1376\n",
      "Iter-1690 train loss: 2.3010 valid loss: 2.3011, valid accuracy: 0.1386\n",
      "Iter-1700 train loss: 2.3055 valid loss: 2.3011, valid accuracy: 0.1382\n",
      "Iter-1710 train loss: 2.3079 valid loss: 2.3011, valid accuracy: 0.1380\n",
      "Iter-1720 train loss: 2.3018 valid loss: 2.3011, valid accuracy: 0.1384\n",
      "Iter-1730 train loss: 2.2974 valid loss: 2.3011, valid accuracy: 0.1384\n",
      "Iter-1740 train loss: 2.3045 valid loss: 2.3011, valid accuracy: 0.1382\n",
      "Iter-1750 train loss: 2.3020 valid loss: 2.3011, valid accuracy: 0.1382\n",
      "Iter-1760 train loss: 2.3013 valid loss: 2.3011, valid accuracy: 0.1386\n",
      "Iter-1770 train loss: 2.3055 valid loss: 2.3011, valid accuracy: 0.1382\n",
      "Iter-1780 train loss: 2.3031 valid loss: 2.3011, valid accuracy: 0.1378\n",
      "Iter-1790 train loss: 2.3044 valid loss: 2.3011, valid accuracy: 0.1382\n",
      "Iter-1800 train loss: 2.3029 valid loss: 2.3011, valid accuracy: 0.1378\n",
      "Iter-1810 train loss: 2.2998 valid loss: 2.3011, valid accuracy: 0.1378\n",
      "Iter-1820 train loss: 2.3036 valid loss: 2.3011, valid accuracy: 0.1380\n",
      "Iter-1830 train loss: 2.3007 valid loss: 2.3011, valid accuracy: 0.1374\n",
      "Iter-1840 train loss: 2.3003 valid loss: 2.3011, valid accuracy: 0.1372\n",
      "Iter-1850 train loss: 2.3017 valid loss: 2.3011, valid accuracy: 0.1374\n",
      "Iter-1860 train loss: 2.3012 valid loss: 2.3011, valid accuracy: 0.1376\n",
      "Iter-1870 train loss: 2.3036 valid loss: 2.3011, valid accuracy: 0.1386\n",
      "Iter-1880 train loss: 2.2983 valid loss: 2.3011, valid accuracy: 0.1388\n",
      "Iter-1890 train loss: 2.2986 valid loss: 2.3011, valid accuracy: 0.1388\n",
      "Iter-1900 train loss: 2.2979 valid loss: 2.3010, valid accuracy: 0.1384\n",
      "Iter-1910 train loss: 2.3022 valid loss: 2.3011, valid accuracy: 0.1384\n",
      "Iter-1920 train loss: 2.3010 valid loss: 2.3010, valid accuracy: 0.1382\n",
      "Iter-1930 train loss: 2.2942 valid loss: 2.3010, valid accuracy: 0.1388\n",
      "Iter-1940 train loss: 2.2997 valid loss: 2.3010, valid accuracy: 0.1388\n",
      "Iter-1950 train loss: 2.2963 valid loss: 2.3010, valid accuracy: 0.1386\n",
      "Iter-1960 train loss: 2.3016 valid loss: 2.3010, valid accuracy: 0.1388\n",
      "Iter-1970 train loss: 2.2992 valid loss: 2.3010, valid accuracy: 0.1394\n",
      "Iter-1980 train loss: 2.2951 valid loss: 2.3010, valid accuracy: 0.1386\n",
      "Iter-1990 train loss: 2.3008 valid loss: 2.3010, valid accuracy: 0.1394\n",
      "Iter-2000 train loss: 2.3019 valid loss: 2.3010, valid accuracy: 0.1394\n",
      "Iter-2010 train loss: 2.3003 valid loss: 2.3010, valid accuracy: 0.1394\n",
      "Iter-2020 train loss: 2.3016 valid loss: 2.3010, valid accuracy: 0.1398\n",
      "Iter-2030 train loss: 2.2998 valid loss: 2.3010, valid accuracy: 0.1396\n",
      "Iter-2040 train loss: 2.3004 valid loss: 2.3010, valid accuracy: 0.1408\n",
      "Iter-2050 train loss: 2.3046 valid loss: 2.3010, valid accuracy: 0.1412\n",
      "Iter-2060 train loss: 2.3035 valid loss: 2.3010, valid accuracy: 0.1412\n",
      "Iter-2070 train loss: 2.3000 valid loss: 2.3010, valid accuracy: 0.1404\n",
      "Iter-2080 train loss: 2.3021 valid loss: 2.3010, valid accuracy: 0.1394\n",
      "Iter-2090 train loss: 2.3012 valid loss: 2.3010, valid accuracy: 0.1410\n",
      "Iter-2100 train loss: 2.3039 valid loss: 2.3009, valid accuracy: 0.1420\n",
      "Iter-2110 train loss: 2.3009 valid loss: 2.3009, valid accuracy: 0.1406\n",
      "Iter-2120 train loss: 2.2988 valid loss: 2.3010, valid accuracy: 0.1418\n",
      "Iter-2130 train loss: 2.3010 valid loss: 2.3009, valid accuracy: 0.1406\n",
      "Iter-2140 train loss: 2.3023 valid loss: 2.3009, valid accuracy: 0.1412\n",
      "Iter-2150 train loss: 2.3032 valid loss: 2.3009, valid accuracy: 0.1410\n",
      "Iter-2160 train loss: 2.3011 valid loss: 2.3009, valid accuracy: 0.1414\n",
      "Iter-2170 train loss: 2.3033 valid loss: 2.3009, valid accuracy: 0.1422\n",
      "Iter-2180 train loss: 2.3008 valid loss: 2.3009, valid accuracy: 0.1416\n",
      "Iter-2190 train loss: 2.3003 valid loss: 2.3009, valid accuracy: 0.1424\n",
      "Iter-2200 train loss: 2.3032 valid loss: 2.3009, valid accuracy: 0.1428\n",
      "Iter-2210 train loss: 2.3091 valid loss: 2.3009, valid accuracy: 0.1424\n",
      "Iter-2220 train loss: 2.2982 valid loss: 2.3009, valid accuracy: 0.1424\n",
      "Iter-2230 train loss: 2.2967 valid loss: 2.3009, valid accuracy: 0.1428\n",
      "Iter-2240 train loss: 2.3047 valid loss: 2.3009, valid accuracy: 0.1424\n",
      "Iter-2250 train loss: 2.3029 valid loss: 2.3009, valid accuracy: 0.1424\n",
      "Iter-2260 train loss: 2.2990 valid loss: 2.3009, valid accuracy: 0.1430\n",
      "Iter-2270 train loss: 2.2962 valid loss: 2.3009, valid accuracy: 0.1428\n",
      "Iter-2280 train loss: 2.3049 valid loss: 2.3009, valid accuracy: 0.1434\n",
      "Iter-2290 train loss: 2.3029 valid loss: 2.3009, valid accuracy: 0.1438\n",
      "Iter-2300 train loss: 2.3004 valid loss: 2.3008, valid accuracy: 0.1430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.3084 valid loss: 2.3008, valid accuracy: 0.1418\n",
      "Iter-2320 train loss: 2.2924 valid loss: 2.3008, valid accuracy: 0.1416\n",
      "Iter-2330 train loss: 2.3058 valid loss: 2.3008, valid accuracy: 0.1426\n",
      "Iter-2340 train loss: 2.3007 valid loss: 2.3008, valid accuracy: 0.1422\n",
      "Iter-2350 train loss: 2.3024 valid loss: 2.3008, valid accuracy: 0.1430\n",
      "Iter-2360 train loss: 2.2966 valid loss: 2.3008, valid accuracy: 0.1426\n",
      "Iter-2370 train loss: 2.3000 valid loss: 2.3008, valid accuracy: 0.1426\n",
      "Iter-2380 train loss: 2.3014 valid loss: 2.3008, valid accuracy: 0.1436\n",
      "Iter-2390 train loss: 2.3028 valid loss: 2.3008, valid accuracy: 0.1444\n",
      "Iter-2400 train loss: 2.2979 valid loss: 2.3008, valid accuracy: 0.1450\n",
      "Iter-2410 train loss: 2.3004 valid loss: 2.3008, valid accuracy: 0.1450\n",
      "Iter-2420 train loss: 2.2999 valid loss: 2.3008, valid accuracy: 0.1444\n",
      "Iter-2430 train loss: 2.3016 valid loss: 2.3008, valid accuracy: 0.1442\n",
      "Iter-2440 train loss: 2.3013 valid loss: 2.3008, valid accuracy: 0.1444\n",
      "Iter-2450 train loss: 2.3073 valid loss: 2.3008, valid accuracy: 0.1450\n",
      "Iter-2460 train loss: 2.2948 valid loss: 2.3008, valid accuracy: 0.1448\n",
      "Iter-2470 train loss: 2.3018 valid loss: 2.3008, valid accuracy: 0.1456\n",
      "Iter-2480 train loss: 2.3080 valid loss: 2.3008, valid accuracy: 0.1454\n",
      "Iter-2490 train loss: 2.3036 valid loss: 2.3008, valid accuracy: 0.1454\n",
      "Iter-2500 train loss: 2.3035 valid loss: 2.3008, valid accuracy: 0.1450\n",
      "Iter-2510 train loss: 2.2970 valid loss: 2.3008, valid accuracy: 0.1448\n",
      "Iter-2520 train loss: 2.3060 valid loss: 2.3007, valid accuracy: 0.1458\n",
      "Iter-2530 train loss: 2.3048 valid loss: 2.3007, valid accuracy: 0.1458\n",
      "Iter-2540 train loss: 2.3010 valid loss: 2.3007, valid accuracy: 0.1448\n",
      "Iter-2550 train loss: 2.2954 valid loss: 2.3007, valid accuracy: 0.1444\n",
      "Iter-2560 train loss: 2.2971 valid loss: 2.3007, valid accuracy: 0.1442\n",
      "Iter-2570 train loss: 2.2969 valid loss: 2.3007, valid accuracy: 0.1444\n",
      "Iter-2580 train loss: 2.2970 valid loss: 2.3007, valid accuracy: 0.1448\n",
      "Iter-2590 train loss: 2.3022 valid loss: 2.3007, valid accuracy: 0.1448\n",
      "Iter-2600 train loss: 2.3033 valid loss: 2.3007, valid accuracy: 0.1448\n",
      "Iter-2610 train loss: 2.3050 valid loss: 2.3007, valid accuracy: 0.1452\n",
      "Iter-2620 train loss: 2.2994 valid loss: 2.3007, valid accuracy: 0.1440\n",
      "Iter-2630 train loss: 2.2990 valid loss: 2.3007, valid accuracy: 0.1440\n",
      "Iter-2640 train loss: 2.3057 valid loss: 2.3007, valid accuracy: 0.1442\n",
      "Iter-2650 train loss: 2.3059 valid loss: 2.3007, valid accuracy: 0.1444\n",
      "Iter-2660 train loss: 2.3048 valid loss: 2.3007, valid accuracy: 0.1444\n",
      "Iter-2670 train loss: 2.2988 valid loss: 2.3007, valid accuracy: 0.1442\n",
      "Iter-2680 train loss: 2.3005 valid loss: 2.3007, valid accuracy: 0.1446\n",
      "Iter-2690 train loss: 2.3000 valid loss: 2.3007, valid accuracy: 0.1446\n",
      "Iter-2700 train loss: 2.3048 valid loss: 2.3007, valid accuracy: 0.1452\n",
      "Iter-2710 train loss: 2.3007 valid loss: 2.3006, valid accuracy: 0.1446\n",
      "Iter-2720 train loss: 2.2999 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2730 train loss: 2.3010 valid loss: 2.3006, valid accuracy: 0.1444\n",
      "Iter-2740 train loss: 2.2975 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2750 train loss: 2.3000 valid loss: 2.3006, valid accuracy: 0.1446\n",
      "Iter-2760 train loss: 2.2998 valid loss: 2.3006, valid accuracy: 0.1446\n",
      "Iter-2770 train loss: 2.3037 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2780 train loss: 2.3000 valid loss: 2.3006, valid accuracy: 0.1448\n",
      "Iter-2790 train loss: 2.3039 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2800 train loss: 2.3013 valid loss: 2.3006, valid accuracy: 0.1454\n",
      "Iter-2810 train loss: 2.3003 valid loss: 2.3006, valid accuracy: 0.1448\n",
      "Iter-2820 train loss: 2.3012 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2830 train loss: 2.2950 valid loss: 2.3006, valid accuracy: 0.1456\n",
      "Iter-2840 train loss: 2.3024 valid loss: 2.3006, valid accuracy: 0.1452\n",
      "Iter-2850 train loss: 2.2903 valid loss: 2.3006, valid accuracy: 0.1448\n",
      "Iter-2860 train loss: 2.3030 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2870 train loss: 2.3012 valid loss: 2.3006, valid accuracy: 0.1444\n",
      "Iter-2880 train loss: 2.2991 valid loss: 2.3006, valid accuracy: 0.1452\n",
      "Iter-2890 train loss: 2.3075 valid loss: 2.3006, valid accuracy: 0.1446\n",
      "Iter-2900 train loss: 2.3026 valid loss: 2.3006, valid accuracy: 0.1450\n",
      "Iter-2910 train loss: 2.3001 valid loss: 2.3005, valid accuracy: 0.1450\n",
      "Iter-2920 train loss: 2.2952 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-2930 train loss: 2.3031 valid loss: 2.3005, valid accuracy: 0.1450\n",
      "Iter-2940 train loss: 2.2957 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-2950 train loss: 2.3016 valid loss: 2.3005, valid accuracy: 0.1450\n",
      "Iter-2960 train loss: 2.3000 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-2970 train loss: 2.2998 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-2980 train loss: 2.3037 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-2990 train loss: 2.3025 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3000 train loss: 2.2962 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3010 train loss: 2.2979 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-3020 train loss: 2.3035 valid loss: 2.3005, valid accuracy: 0.1444\n",
      "Iter-3030 train loss: 2.3040 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-3040 train loss: 2.2991 valid loss: 2.3005, valid accuracy: 0.1446\n",
      "Iter-3050 train loss: 2.3047 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3060 train loss: 2.2989 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3070 train loss: 2.3064 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-3080 train loss: 2.2998 valid loss: 2.3005, valid accuracy: 0.1446\n",
      "Iter-3090 train loss: 2.3003 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3100 train loss: 2.3002 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3110 train loss: 2.2964 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-3120 train loss: 2.3012 valid loss: 2.3005, valid accuracy: 0.1446\n",
      "Iter-3130 train loss: 2.2983 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-3140 train loss: 2.3047 valid loss: 2.3005, valid accuracy: 0.1444\n",
      "Iter-3150 train loss: 2.3016 valid loss: 2.3005, valid accuracy: 0.1450\n",
      "Iter-3160 train loss: 2.3024 valid loss: 2.3005, valid accuracy: 0.1452\n",
      "Iter-3170 train loss: 2.3090 valid loss: 2.3005, valid accuracy: 0.1448\n",
      "Iter-3180 train loss: 2.3022 valid loss: 2.3004, valid accuracy: 0.1448\n",
      "Iter-3190 train loss: 2.2992 valid loss: 2.3004, valid accuracy: 0.1446\n",
      "Iter-3200 train loss: 2.3039 valid loss: 2.3004, valid accuracy: 0.1446\n",
      "Iter-3210 train loss: 2.2934 valid loss: 2.3004, valid accuracy: 0.1442\n",
      "Iter-3220 train loss: 2.3000 valid loss: 2.3004, valid accuracy: 0.1448\n",
      "Iter-3230 train loss: 2.3035 valid loss: 2.3004, valid accuracy: 0.1442\n",
      "Iter-3240 train loss: 2.2998 valid loss: 2.3004, valid accuracy: 0.1446\n",
      "Iter-3250 train loss: 2.2973 valid loss: 2.3004, valid accuracy: 0.1446\n",
      "Iter-3260 train loss: 2.3034 valid loss: 2.3004, valid accuracy: 0.1454\n",
      "Iter-3270 train loss: 2.3019 valid loss: 2.3004, valid accuracy: 0.1450\n",
      "Iter-3280 train loss: 2.2944 valid loss: 2.3004, valid accuracy: 0.1452\n",
      "Iter-3290 train loss: 2.2986 valid loss: 2.3004, valid accuracy: 0.1446\n",
      "Iter-3300 train loss: 2.2965 valid loss: 2.3004, valid accuracy: 0.1446\n",
      "Iter-3310 train loss: 2.2978 valid loss: 2.3004, valid accuracy: 0.1450\n",
      "Iter-3320 train loss: 2.3032 valid loss: 2.3004, valid accuracy: 0.1450\n",
      "Iter-3330 train loss: 2.2928 valid loss: 2.3004, valid accuracy: 0.1448\n",
      "Iter-3340 train loss: 2.3018 valid loss: 2.3004, valid accuracy: 0.1444\n",
      "Iter-3350 train loss: 2.3003 valid loss: 2.3004, valid accuracy: 0.1444\n",
      "Iter-3360 train loss: 2.3008 valid loss: 2.3004, valid accuracy: 0.1442\n",
      "Iter-3370 train loss: 2.2967 valid loss: 2.3004, valid accuracy: 0.1440\n",
      "Iter-3380 train loss: 2.3050 valid loss: 2.3004, valid accuracy: 0.1434\n",
      "Iter-3390 train loss: 2.2979 valid loss: 2.3004, valid accuracy: 0.1444\n",
      "Iter-3400 train loss: 2.2993 valid loss: 2.3004, valid accuracy: 0.1444\n",
      "Iter-3410 train loss: 2.2998 valid loss: 2.3003, valid accuracy: 0.1438\n",
      "Iter-3420 train loss: 2.2999 valid loss: 2.3003, valid accuracy: 0.1438\n",
      "Iter-3430 train loss: 2.3069 valid loss: 2.3003, valid accuracy: 0.1436\n",
      "Iter-3440 train loss: 2.2983 valid loss: 2.3003, valid accuracy: 0.1438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.3011 valid loss: 2.3003, valid accuracy: 0.1434\n",
      "Iter-3460 train loss: 2.2969 valid loss: 2.3003, valid accuracy: 0.1438\n",
      "Iter-3470 train loss: 2.3038 valid loss: 2.3003, valid accuracy: 0.1440\n",
      "Iter-3480 train loss: 2.3033 valid loss: 2.3003, valid accuracy: 0.1440\n",
      "Iter-3490 train loss: 2.3006 valid loss: 2.3003, valid accuracy: 0.1442\n",
      "Iter-3500 train loss: 2.2974 valid loss: 2.3003, valid accuracy: 0.1440\n",
      "Iter-3510 train loss: 2.3006 valid loss: 2.3003, valid accuracy: 0.1438\n",
      "Iter-3520 train loss: 2.3007 valid loss: 2.3003, valid accuracy: 0.1444\n",
      "Iter-3530 train loss: 2.3061 valid loss: 2.3003, valid accuracy: 0.1440\n",
      "Iter-3540 train loss: 2.3048 valid loss: 2.3003, valid accuracy: 0.1444\n",
      "Iter-3550 train loss: 2.3027 valid loss: 2.3003, valid accuracy: 0.1442\n",
      "Iter-3560 train loss: 2.3059 valid loss: 2.3003, valid accuracy: 0.1442\n",
      "Iter-3570 train loss: 2.3067 valid loss: 2.3003, valid accuracy: 0.1444\n",
      "Iter-3580 train loss: 2.2995 valid loss: 2.3003, valid accuracy: 0.1446\n",
      "Iter-3590 train loss: 2.2905 valid loss: 2.3003, valid accuracy: 0.1444\n",
      "Iter-3600 train loss: 2.3003 valid loss: 2.3003, valid accuracy: 0.1440\n",
      "Iter-3610 train loss: 2.3044 valid loss: 2.3003, valid accuracy: 0.1442\n",
      "Iter-3620 train loss: 2.3007 valid loss: 2.3003, valid accuracy: 0.1438\n",
      "Iter-3630 train loss: 2.3008 valid loss: 2.3003, valid accuracy: 0.1438\n",
      "Iter-3640 train loss: 2.3009 valid loss: 2.3003, valid accuracy: 0.1434\n",
      "Iter-3650 train loss: 2.3049 valid loss: 2.3002, valid accuracy: 0.1440\n",
      "Iter-3660 train loss: 2.3039 valid loss: 2.3002, valid accuracy: 0.1434\n",
      "Iter-3670 train loss: 2.2990 valid loss: 2.3002, valid accuracy: 0.1436\n",
      "Iter-3680 train loss: 2.2967 valid loss: 2.3002, valid accuracy: 0.1434\n",
      "Iter-3690 train loss: 2.3039 valid loss: 2.3002, valid accuracy: 0.1438\n",
      "Iter-3700 train loss: 2.2999 valid loss: 2.3002, valid accuracy: 0.1438\n",
      "Iter-3710 train loss: 2.2925 valid loss: 2.3002, valid accuracy: 0.1434\n",
      "Iter-3720 train loss: 2.2918 valid loss: 2.3002, valid accuracy: 0.1430\n",
      "Iter-3730 train loss: 2.2965 valid loss: 2.3002, valid accuracy: 0.1424\n",
      "Iter-3740 train loss: 2.2950 valid loss: 2.3002, valid accuracy: 0.1420\n",
      "Iter-3750 train loss: 2.2999 valid loss: 2.3002, valid accuracy: 0.1420\n",
      "Iter-3760 train loss: 2.2982 valid loss: 2.3002, valid accuracy: 0.1422\n",
      "Iter-3770 train loss: 2.2991 valid loss: 2.3002, valid accuracy: 0.1422\n",
      "Iter-3780 train loss: 2.2917 valid loss: 2.3002, valid accuracy: 0.1424\n",
      "Iter-3790 train loss: 2.2900 valid loss: 2.3002, valid accuracy: 0.1424\n",
      "Iter-3800 train loss: 2.3019 valid loss: 2.3002, valid accuracy: 0.1422\n",
      "Iter-3810 train loss: 2.3028 valid loss: 2.3002, valid accuracy: 0.1422\n",
      "Iter-3820 train loss: 2.2950 valid loss: 2.3002, valid accuracy: 0.1422\n",
      "Iter-3830 train loss: 2.3026 valid loss: 2.3002, valid accuracy: 0.1422\n",
      "Iter-3840 train loss: 2.2995 valid loss: 2.3001, valid accuracy: 0.1422\n",
      "Iter-3850 train loss: 2.3016 valid loss: 2.3001, valid accuracy: 0.1422\n",
      "Iter-3860 train loss: 2.2988 valid loss: 2.3001, valid accuracy: 0.1422\n",
      "Iter-3870 train loss: 2.3083 valid loss: 2.3001, valid accuracy: 0.1418\n",
      "Iter-3880 train loss: 2.3034 valid loss: 2.3001, valid accuracy: 0.1420\n",
      "Iter-3890 train loss: 2.2980 valid loss: 2.3001, valid accuracy: 0.1424\n",
      "Iter-3900 train loss: 2.3022 valid loss: 2.3001, valid accuracy: 0.1418\n",
      "Iter-3910 train loss: 2.2989 valid loss: 2.3001, valid accuracy: 0.1416\n",
      "Iter-3920 train loss: 2.2994 valid loss: 2.3001, valid accuracy: 0.1422\n",
      "Iter-3930 train loss: 2.3022 valid loss: 2.3001, valid accuracy: 0.1424\n",
      "Iter-3940 train loss: 2.2998 valid loss: 2.3001, valid accuracy: 0.1420\n",
      "Iter-3950 train loss: 2.2960 valid loss: 2.3001, valid accuracy: 0.1420\n",
      "Iter-3960 train loss: 2.2921 valid loss: 2.3001, valid accuracy: 0.1420\n",
      "Iter-3970 train loss: 2.3037 valid loss: 2.3001, valid accuracy: 0.1420\n",
      "Iter-3980 train loss: 2.3027 valid loss: 2.3001, valid accuracy: 0.1418\n",
      "Iter-3990 train loss: 2.2987 valid loss: 2.3001, valid accuracy: 0.1418\n",
      "Iter-4000 train loss: 2.3094 valid loss: 2.3001, valid accuracy: 0.1418\n",
      "Iter-4010 train loss: 2.2969 valid loss: 2.3001, valid accuracy: 0.1422\n",
      "Iter-4020 train loss: 2.3044 valid loss: 2.3000, valid accuracy: 0.1420\n",
      "Iter-4030 train loss: 2.3006 valid loss: 2.3000, valid accuracy: 0.1418\n",
      "Iter-4040 train loss: 2.2977 valid loss: 2.3000, valid accuracy: 0.1418\n",
      "Iter-4050 train loss: 2.2952 valid loss: 2.3000, valid accuracy: 0.1422\n",
      "Iter-4060 train loss: 2.3047 valid loss: 2.3000, valid accuracy: 0.1422\n",
      "Iter-4070 train loss: 2.3029 valid loss: 2.3000, valid accuracy: 0.1420\n",
      "Iter-4080 train loss: 2.2964 valid loss: 2.3000, valid accuracy: 0.1420\n",
      "Iter-4090 train loss: 2.3013 valid loss: 2.3000, valid accuracy: 0.1422\n",
      "Iter-4100 train loss: 2.2930 valid loss: 2.3000, valid accuracy: 0.1426\n",
      "Iter-4110 train loss: 2.2978 valid loss: 2.3000, valid accuracy: 0.1418\n",
      "Iter-4120 train loss: 2.3019 valid loss: 2.3000, valid accuracy: 0.1412\n",
      "Iter-4130 train loss: 2.3019 valid loss: 2.3000, valid accuracy: 0.1406\n",
      "Iter-4140 train loss: 2.3013 valid loss: 2.3000, valid accuracy: 0.1412\n",
      "Iter-4150 train loss: 2.3027 valid loss: 2.3000, valid accuracy: 0.1402\n",
      "Iter-4160 train loss: 2.2963 valid loss: 2.3000, valid accuracy: 0.1394\n",
      "Iter-4170 train loss: 2.2971 valid loss: 2.3000, valid accuracy: 0.1388\n",
      "Iter-4180 train loss: 2.2985 valid loss: 2.3000, valid accuracy: 0.1388\n",
      "Iter-4190 train loss: 2.3053 valid loss: 2.3000, valid accuracy: 0.1386\n",
      "Iter-4200 train loss: 2.2947 valid loss: 2.3000, valid accuracy: 0.1382\n",
      "Iter-4210 train loss: 2.3002 valid loss: 2.3000, valid accuracy: 0.1388\n",
      "Iter-4220 train loss: 2.3033 valid loss: 2.3000, valid accuracy: 0.1390\n",
      "Iter-4230 train loss: 2.3057 valid loss: 2.3000, valid accuracy: 0.1388\n",
      "Iter-4240 train loss: 2.3015 valid loss: 2.2999, valid accuracy: 0.1396\n",
      "Iter-4250 train loss: 2.3036 valid loss: 2.2999, valid accuracy: 0.1402\n",
      "Iter-4260 train loss: 2.2918 valid loss: 2.2999, valid accuracy: 0.1398\n",
      "Iter-4270 train loss: 2.3028 valid loss: 2.2999, valid accuracy: 0.1402\n",
      "Iter-4280 train loss: 2.3095 valid loss: 2.2999, valid accuracy: 0.1394\n",
      "Iter-4290 train loss: 2.3020 valid loss: 2.2999, valid accuracy: 0.1400\n",
      "Iter-4300 train loss: 2.2982 valid loss: 2.2999, valid accuracy: 0.1388\n",
      "Iter-4310 train loss: 2.3040 valid loss: 2.2999, valid accuracy: 0.1392\n",
      "Iter-4320 train loss: 2.3017 valid loss: 2.2999, valid accuracy: 0.1392\n",
      "Iter-4330 train loss: 2.3061 valid loss: 2.2999, valid accuracy: 0.1392\n",
      "Iter-4340 train loss: 2.3004 valid loss: 2.2999, valid accuracy: 0.1390\n",
      "Iter-4350 train loss: 2.2984 valid loss: 2.2999, valid accuracy: 0.1392\n",
      "Iter-4360 train loss: 2.3037 valid loss: 2.2999, valid accuracy: 0.1390\n",
      "Iter-4370 train loss: 2.2961 valid loss: 2.2999, valid accuracy: 0.1390\n",
      "Iter-4380 train loss: 2.2961 valid loss: 2.2999, valid accuracy: 0.1394\n",
      "Iter-4390 train loss: 2.2962 valid loss: 2.2999, valid accuracy: 0.1394\n",
      "Iter-4400 train loss: 2.3080 valid loss: 2.2999, valid accuracy: 0.1388\n",
      "Iter-4410 train loss: 2.3101 valid loss: 2.2999, valid accuracy: 0.1390\n",
      "Iter-4420 train loss: 2.3071 valid loss: 2.2999, valid accuracy: 0.1390\n",
      "Iter-4430 train loss: 2.2936 valid loss: 2.2999, valid accuracy: 0.1386\n",
      "Iter-4440 train loss: 2.3018 valid loss: 2.2999, valid accuracy: 0.1384\n",
      "Iter-4450 train loss: 2.3012 valid loss: 2.2999, valid accuracy: 0.1384\n",
      "Iter-4460 train loss: 2.2992 valid loss: 2.2998, valid accuracy: 0.1384\n",
      "Iter-4470 train loss: 2.3077 valid loss: 2.2998, valid accuracy: 0.1388\n",
      "Iter-4480 train loss: 2.2962 valid loss: 2.2998, valid accuracy: 0.1390\n",
      "Iter-4490 train loss: 2.3003 valid loss: 2.2998, valid accuracy: 0.1390\n",
      "Iter-4500 train loss: 2.2960 valid loss: 2.2998, valid accuracy: 0.1388\n",
      "Iter-4510 train loss: 2.2961 valid loss: 2.2998, valid accuracy: 0.1386\n",
      "Iter-4520 train loss: 2.2949 valid loss: 2.2998, valid accuracy: 0.1392\n",
      "Iter-4530 train loss: 2.2982 valid loss: 2.2998, valid accuracy: 0.1390\n",
      "Iter-4540 train loss: 2.2965 valid loss: 2.2998, valid accuracy: 0.1388\n",
      "Iter-4550 train loss: 2.3011 valid loss: 2.2998, valid accuracy: 0.1390\n",
      "Iter-4560 train loss: 2.2998 valid loss: 2.2998, valid accuracy: 0.1392\n",
      "Iter-4570 train loss: 2.3032 valid loss: 2.2998, valid accuracy: 0.1386\n",
      "Iter-4580 train loss: 2.3008 valid loss: 2.2998, valid accuracy: 0.1388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.2949 valid loss: 2.2998, valid accuracy: 0.1390\n",
      "Iter-4600 train loss: 2.2971 valid loss: 2.2998, valid accuracy: 0.1388\n",
      "Iter-4610 train loss: 2.3060 valid loss: 2.2998, valid accuracy: 0.1390\n",
      "Iter-4620 train loss: 2.2982 valid loss: 2.2998, valid accuracy: 0.1392\n",
      "Iter-4630 train loss: 2.3072 valid loss: 2.2998, valid accuracy: 0.1386\n",
      "Iter-4640 train loss: 2.3017 valid loss: 2.2998, valid accuracy: 0.1388\n",
      "Iter-4650 train loss: 2.2997 valid loss: 2.2998, valid accuracy: 0.1386\n",
      "Iter-4660 train loss: 2.3023 valid loss: 2.2998, valid accuracy: 0.1386\n",
      "Iter-4670 train loss: 2.2972 valid loss: 2.2998, valid accuracy: 0.1380\n",
      "Iter-4680 train loss: 2.3034 valid loss: 2.2998, valid accuracy: 0.1372\n",
      "Iter-4690 train loss: 2.2980 valid loss: 2.2997, valid accuracy: 0.1374\n",
      "Iter-4700 train loss: 2.3007 valid loss: 2.2997, valid accuracy: 0.1376\n",
      "Iter-4710 train loss: 2.3033 valid loss: 2.2997, valid accuracy: 0.1378\n",
      "Iter-4720 train loss: 2.2979 valid loss: 2.2997, valid accuracy: 0.1374\n",
      "Iter-4730 train loss: 2.3026 valid loss: 2.2997, valid accuracy: 0.1366\n",
      "Iter-4740 train loss: 2.2970 valid loss: 2.2997, valid accuracy: 0.1364\n",
      "Iter-4750 train loss: 2.2950 valid loss: 2.2997, valid accuracy: 0.1358\n",
      "Iter-4760 train loss: 2.2984 valid loss: 2.2997, valid accuracy: 0.1358\n",
      "Iter-4770 train loss: 2.3025 valid loss: 2.2997, valid accuracy: 0.1360\n",
      "Iter-4780 train loss: 2.3035 valid loss: 2.2997, valid accuracy: 0.1356\n",
      "Iter-4790 train loss: 2.2972 valid loss: 2.2997, valid accuracy: 0.1356\n",
      "Iter-4800 train loss: 2.2943 valid loss: 2.2997, valid accuracy: 0.1354\n",
      "Iter-4810 train loss: 2.2983 valid loss: 2.2997, valid accuracy: 0.1356\n",
      "Iter-4820 train loss: 2.2972 valid loss: 2.2997, valid accuracy: 0.1356\n",
      "Iter-4830 train loss: 2.3046 valid loss: 2.2997, valid accuracy: 0.1356\n",
      "Iter-4840 train loss: 2.3051 valid loss: 2.2997, valid accuracy: 0.1360\n",
      "Iter-4850 train loss: 2.2991 valid loss: 2.2997, valid accuracy: 0.1354\n",
      "Iter-4860 train loss: 2.2991 valid loss: 2.2997, valid accuracy: 0.1354\n",
      "Iter-4870 train loss: 2.3050 valid loss: 2.2997, valid accuracy: 0.1354\n",
      "Iter-4880 train loss: 2.3015 valid loss: 2.2997, valid accuracy: 0.1350\n",
      "Iter-4890 train loss: 2.3040 valid loss: 2.2997, valid accuracy: 0.1350\n",
      "Iter-4900 train loss: 2.2931 valid loss: 2.2997, valid accuracy: 0.1350\n",
      "Iter-4910 train loss: 2.2945 valid loss: 2.2997, valid accuracy: 0.1348\n",
      "Iter-4920 train loss: 2.2949 valid loss: 2.2997, valid accuracy: 0.1344\n",
      "Iter-4930 train loss: 2.2956 valid loss: 2.2997, valid accuracy: 0.1344\n",
      "Iter-4940 train loss: 2.3075 valid loss: 2.2996, valid accuracy: 0.1348\n",
      "Iter-4950 train loss: 2.3008 valid loss: 2.2996, valid accuracy: 0.1342\n",
      "Iter-4960 train loss: 2.2961 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-4970 train loss: 2.2977 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-4980 train loss: 2.2910 valid loss: 2.2996, valid accuracy: 0.1342\n",
      "Iter-4990 train loss: 2.3016 valid loss: 2.2996, valid accuracy: 0.1342\n",
      "Iter-5000 train loss: 2.2973 valid loss: 2.2996, valid accuracy: 0.1344\n",
      "Iter-5010 train loss: 2.2980 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-5020 train loss: 2.3036 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-5030 train loss: 2.2952 valid loss: 2.2996, valid accuracy: 0.1338\n",
      "Iter-5040 train loss: 2.3027 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-5050 train loss: 2.2994 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-5060 train loss: 2.3000 valid loss: 2.2996, valid accuracy: 0.1342\n",
      "Iter-5070 train loss: 2.2974 valid loss: 2.2996, valid accuracy: 0.1340\n",
      "Iter-5080 train loss: 2.3018 valid loss: 2.2996, valid accuracy: 0.1338\n",
      "Iter-5090 train loss: 2.2981 valid loss: 2.2996, valid accuracy: 0.1326\n",
      "Iter-5100 train loss: 2.3010 valid loss: 2.2996, valid accuracy: 0.1326\n",
      "Iter-5110 train loss: 2.2945 valid loss: 2.2996, valid accuracy: 0.1326\n",
      "Iter-5120 train loss: 2.3014 valid loss: 2.2996, valid accuracy: 0.1328\n",
      "Iter-5130 train loss: 2.3008 valid loss: 2.2996, valid accuracy: 0.1326\n",
      "Iter-5140 train loss: 2.2980 valid loss: 2.2996, valid accuracy: 0.1324\n",
      "Iter-5150 train loss: 2.3038 valid loss: 2.2996, valid accuracy: 0.1324\n",
      "Iter-5160 train loss: 2.3007 valid loss: 2.2996, valid accuracy: 0.1322\n",
      "Iter-5170 train loss: 2.3038 valid loss: 2.2996, valid accuracy: 0.1328\n",
      "Iter-5180 train loss: 2.3020 valid loss: 2.2996, valid accuracy: 0.1326\n",
      "Iter-5190 train loss: 2.3062 valid loss: 2.2996, valid accuracy: 0.1332\n",
      "Iter-5200 train loss: 2.2977 valid loss: 2.2996, valid accuracy: 0.1330\n",
      "Iter-5210 train loss: 2.3027 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5220 train loss: 2.2983 valid loss: 2.2995, valid accuracy: 0.1332\n",
      "Iter-5230 train loss: 2.2953 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5240 train loss: 2.2967 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5250 train loss: 2.2988 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5260 train loss: 2.3003 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5270 train loss: 2.2956 valid loss: 2.2995, valid accuracy: 0.1322\n",
      "Iter-5280 train loss: 2.3065 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5290 train loss: 2.2956 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5300 train loss: 2.3022 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5310 train loss: 2.3050 valid loss: 2.2995, valid accuracy: 0.1328\n",
      "Iter-5320 train loss: 2.2995 valid loss: 2.2995, valid accuracy: 0.1324\n",
      "Iter-5330 train loss: 2.3060 valid loss: 2.2995, valid accuracy: 0.1324\n",
      "Iter-5340 train loss: 2.3045 valid loss: 2.2995, valid accuracy: 0.1324\n",
      "Iter-5350 train loss: 2.3048 valid loss: 2.2995, valid accuracy: 0.1326\n",
      "Iter-5360 train loss: 2.2999 valid loss: 2.2995, valid accuracy: 0.1324\n",
      "Iter-5370 train loss: 2.2995 valid loss: 2.2995, valid accuracy: 0.1322\n",
      "Iter-5380 train loss: 2.2958 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5390 train loss: 2.3039 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5400 train loss: 2.3018 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5410 train loss: 2.3107 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5420 train loss: 2.3043 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5430 train loss: 2.2953 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5440 train loss: 2.2947 valid loss: 2.2995, valid accuracy: 0.1330\n",
      "Iter-5450 train loss: 2.2939 valid loss: 2.2995, valid accuracy: 0.1332\n",
      "Iter-5460 train loss: 2.2968 valid loss: 2.2995, valid accuracy: 0.1334\n",
      "Iter-5470 train loss: 2.3042 valid loss: 2.2995, valid accuracy: 0.1334\n",
      "Iter-5480 train loss: 2.3013 valid loss: 2.2995, valid accuracy: 0.1332\n",
      "Iter-5490 train loss: 2.3077 valid loss: 2.2995, valid accuracy: 0.1336\n",
      "Iter-5500 train loss: 2.2991 valid loss: 2.2995, valid accuracy: 0.1338\n",
      "Iter-5510 train loss: 2.2997 valid loss: 2.2995, valid accuracy: 0.1342\n",
      "Iter-5520 train loss: 2.3002 valid loss: 2.2995, valid accuracy: 0.1338\n",
      "Iter-5530 train loss: 2.2958 valid loss: 2.2994, valid accuracy: 0.1334\n",
      "Iter-5540 train loss: 2.2992 valid loss: 2.2994, valid accuracy: 0.1340\n",
      "Iter-5550 train loss: 2.2977 valid loss: 2.2994, valid accuracy: 0.1334\n",
      "Iter-5560 train loss: 2.2994 valid loss: 2.2994, valid accuracy: 0.1328\n",
      "Iter-5570 train loss: 2.3075 valid loss: 2.2994, valid accuracy: 0.1324\n",
      "Iter-5580 train loss: 2.3059 valid loss: 2.2994, valid accuracy: 0.1330\n",
      "Iter-5590 train loss: 2.3120 valid loss: 2.2994, valid accuracy: 0.1326\n",
      "Iter-5600 train loss: 2.3053 valid loss: 2.2994, valid accuracy: 0.1324\n",
      "Iter-5610 train loss: 2.3035 valid loss: 2.2994, valid accuracy: 0.1322\n",
      "Iter-5620 train loss: 2.3040 valid loss: 2.2994, valid accuracy: 0.1326\n",
      "Iter-5630 train loss: 2.2993 valid loss: 2.2994, valid accuracy: 0.1320\n",
      "Iter-5640 train loss: 2.2959 valid loss: 2.2994, valid accuracy: 0.1316\n",
      "Iter-5650 train loss: 2.2985 valid loss: 2.2994, valid accuracy: 0.1318\n",
      "Iter-5660 train loss: 2.3016 valid loss: 2.2994, valid accuracy: 0.1318\n",
      "Iter-5670 train loss: 2.2963 valid loss: 2.2994, valid accuracy: 0.1318\n",
      "Iter-5680 train loss: 2.3056 valid loss: 2.2994, valid accuracy: 0.1318\n",
      "Iter-5690 train loss: 2.3068 valid loss: 2.2994, valid accuracy: 0.1312\n",
      "Iter-5700 train loss: 2.2987 valid loss: 2.2994, valid accuracy: 0.1306\n",
      "Iter-5710 train loss: 2.3050 valid loss: 2.2994, valid accuracy: 0.1310\n",
      "Iter-5720 train loss: 2.2941 valid loss: 2.2994, valid accuracy: 0.1306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.3050 valid loss: 2.2994, valid accuracy: 0.1308\n",
      "Iter-5740 train loss: 2.3055 valid loss: 2.2994, valid accuracy: 0.1310\n",
      "Iter-5750 train loss: 2.3003 valid loss: 2.2994, valid accuracy: 0.1310\n",
      "Iter-5760 train loss: 2.2980 valid loss: 2.2994, valid accuracy: 0.1312\n",
      "Iter-5770 train loss: 2.2951 valid loss: 2.2994, valid accuracy: 0.1314\n",
      "Iter-5780 train loss: 2.2967 valid loss: 2.2994, valid accuracy: 0.1312\n",
      "Iter-5790 train loss: 2.3092 valid loss: 2.2994, valid accuracy: 0.1314\n",
      "Iter-5800 train loss: 2.3100 valid loss: 2.2993, valid accuracy: 0.1314\n",
      "Iter-5810 train loss: 2.2990 valid loss: 2.2994, valid accuracy: 0.1312\n",
      "Iter-5820 train loss: 2.3098 valid loss: 2.2993, valid accuracy: 0.1314\n",
      "Iter-5830 train loss: 2.2983 valid loss: 2.2993, valid accuracy: 0.1312\n",
      "Iter-5840 train loss: 2.3068 valid loss: 2.2993, valid accuracy: 0.1308\n",
      "Iter-5850 train loss: 2.3011 valid loss: 2.2993, valid accuracy: 0.1306\n",
      "Iter-5860 train loss: 2.2990 valid loss: 2.2993, valid accuracy: 0.1306\n",
      "Iter-5870 train loss: 2.2978 valid loss: 2.2993, valid accuracy: 0.1308\n",
      "Iter-5880 train loss: 2.3024 valid loss: 2.2993, valid accuracy: 0.1312\n",
      "Iter-5890 train loss: 2.3105 valid loss: 2.2993, valid accuracy: 0.1312\n",
      "Iter-5900 train loss: 2.3048 valid loss: 2.2993, valid accuracy: 0.1308\n",
      "Iter-5910 train loss: 2.3028 valid loss: 2.2993, valid accuracy: 0.1308\n",
      "Iter-5920 train loss: 2.2968 valid loss: 2.2993, valid accuracy: 0.1306\n",
      "Iter-5930 train loss: 2.3000 valid loss: 2.2993, valid accuracy: 0.1302\n",
      "Iter-5940 train loss: 2.3027 valid loss: 2.2993, valid accuracy: 0.1306\n",
      "Iter-5950 train loss: 2.2996 valid loss: 2.2993, valid accuracy: 0.1304\n",
      "Iter-5960 train loss: 2.2995 valid loss: 2.2993, valid accuracy: 0.1302\n",
      "Iter-5970 train loss: 2.2887 valid loss: 2.2993, valid accuracy: 0.1302\n",
      "Iter-5980 train loss: 2.3097 valid loss: 2.2993, valid accuracy: 0.1300\n",
      "Iter-5990 train loss: 2.2983 valid loss: 2.2993, valid accuracy: 0.1298\n",
      "Iter-6000 train loss: 2.2944 valid loss: 2.2993, valid accuracy: 0.1288\n",
      "Iter-6010 train loss: 2.2949 valid loss: 2.2993, valid accuracy: 0.1288\n",
      "Iter-6020 train loss: 2.2969 valid loss: 2.2993, valid accuracy: 0.1288\n",
      "Iter-6030 train loss: 2.2997 valid loss: 2.2993, valid accuracy: 0.1286\n",
      "Iter-6040 train loss: 2.3000 valid loss: 2.2993, valid accuracy: 0.1280\n",
      "Iter-6050 train loss: 2.2986 valid loss: 2.2993, valid accuracy: 0.1280\n",
      "Iter-6060 train loss: 2.2956 valid loss: 2.2993, valid accuracy: 0.1280\n",
      "Iter-6070 train loss: 2.2946 valid loss: 2.2992, valid accuracy: 0.1278\n",
      "Iter-6080 train loss: 2.2996 valid loss: 2.2992, valid accuracy: 0.1282\n",
      "Iter-6090 train loss: 2.2943 valid loss: 2.2992, valid accuracy: 0.1278\n",
      "Iter-6100 train loss: 2.2981 valid loss: 2.2992, valid accuracy: 0.1270\n",
      "Iter-6110 train loss: 2.2990 valid loss: 2.2992, valid accuracy: 0.1278\n",
      "Iter-6120 train loss: 2.2933 valid loss: 2.2992, valid accuracy: 0.1272\n",
      "Iter-6130 train loss: 2.3102 valid loss: 2.2992, valid accuracy: 0.1272\n",
      "Iter-6140 train loss: 2.3018 valid loss: 2.2992, valid accuracy: 0.1270\n",
      "Iter-6150 train loss: 2.2932 valid loss: 2.2992, valid accuracy: 0.1266\n",
      "Iter-6160 train loss: 2.3023 valid loss: 2.2992, valid accuracy: 0.1264\n",
      "Iter-6170 train loss: 2.3030 valid loss: 2.2992, valid accuracy: 0.1266\n",
      "Iter-6180 train loss: 2.3137 valid loss: 2.2992, valid accuracy: 0.1264\n",
      "Iter-6190 train loss: 2.2888 valid loss: 2.2992, valid accuracy: 0.1268\n",
      "Iter-6200 train loss: 2.3106 valid loss: 2.2992, valid accuracy: 0.1268\n",
      "Iter-6210 train loss: 2.3052 valid loss: 2.2992, valid accuracy: 0.1268\n",
      "Iter-6220 train loss: 2.2954 valid loss: 2.2992, valid accuracy: 0.1258\n",
      "Iter-6230 train loss: 2.3028 valid loss: 2.2992, valid accuracy: 0.1260\n",
      "Iter-6240 train loss: 2.3023 valid loss: 2.2992, valid accuracy: 0.1260\n",
      "Iter-6250 train loss: 2.3001 valid loss: 2.2992, valid accuracy: 0.1258\n",
      "Iter-6260 train loss: 2.2942 valid loss: 2.2992, valid accuracy: 0.1254\n",
      "Iter-6270 train loss: 2.2996 valid loss: 2.2992, valid accuracy: 0.1256\n",
      "Iter-6280 train loss: 2.2982 valid loss: 2.2992, valid accuracy: 0.1254\n",
      "Iter-6290 train loss: 2.2967 valid loss: 2.2991, valid accuracy: 0.1254\n",
      "Iter-6300 train loss: 2.3025 valid loss: 2.2991, valid accuracy: 0.1256\n",
      "Iter-6310 train loss: 2.2999 valid loss: 2.2991, valid accuracy: 0.1252\n",
      "Iter-6320 train loss: 2.2940 valid loss: 2.2991, valid accuracy: 0.1254\n",
      "Iter-6330 train loss: 2.3066 valid loss: 2.2991, valid accuracy: 0.1252\n",
      "Iter-6340 train loss: 2.3026 valid loss: 2.2991, valid accuracy: 0.1254\n",
      "Iter-6350 train loss: 2.2890 valid loss: 2.2991, valid accuracy: 0.1248\n",
      "Iter-6360 train loss: 2.3088 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6370 train loss: 2.2960 valid loss: 2.2991, valid accuracy: 0.1252\n",
      "Iter-6380 train loss: 2.2988 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6390 train loss: 2.3061 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6400 train loss: 2.3038 valid loss: 2.2991, valid accuracy: 0.1246\n",
      "Iter-6410 train loss: 2.2949 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6420 train loss: 2.3017 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6430 train loss: 2.3118 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6440 train loss: 2.3022 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6450 train loss: 2.2997 valid loss: 2.2991, valid accuracy: 0.1248\n",
      "Iter-6460 train loss: 2.2979 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6470 train loss: 2.2981 valid loss: 2.2991, valid accuracy: 0.1252\n",
      "Iter-6480 train loss: 2.3066 valid loss: 2.2991, valid accuracy: 0.1250\n",
      "Iter-6490 train loss: 2.3048 valid loss: 2.2991, valid accuracy: 0.1246\n",
      "Iter-6500 train loss: 2.3010 valid loss: 2.2991, valid accuracy: 0.1248\n",
      "Iter-6510 train loss: 2.2966 valid loss: 2.2991, valid accuracy: 0.1248\n",
      "Iter-6520 train loss: 2.2947 valid loss: 2.2991, valid accuracy: 0.1246\n",
      "Iter-6530 train loss: 2.3030 valid loss: 2.2991, valid accuracy: 0.1252\n",
      "Iter-6540 train loss: 2.3001 valid loss: 2.2991, valid accuracy: 0.1252\n",
      "Iter-6550 train loss: 2.3099 valid loss: 2.2991, valid accuracy: 0.1246\n",
      "Iter-6560 train loss: 2.2882 valid loss: 2.2991, valid accuracy: 0.1246\n",
      "Iter-6570 train loss: 2.2984 valid loss: 2.2991, valid accuracy: 0.1246\n",
      "Iter-6580 train loss: 2.2986 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6590 train loss: 2.2966 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6600 train loss: 2.2940 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6610 train loss: 2.3085 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6620 train loss: 2.2983 valid loss: 2.2990, valid accuracy: 0.1250\n",
      "Iter-6630 train loss: 2.3105 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6640 train loss: 2.3003 valid loss: 2.2990, valid accuracy: 0.1244\n",
      "Iter-6650 train loss: 2.2978 valid loss: 2.2990, valid accuracy: 0.1242\n",
      "Iter-6660 train loss: 2.2942 valid loss: 2.2990, valid accuracy: 0.1244\n",
      "Iter-6670 train loss: 2.3017 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6680 train loss: 2.3022 valid loss: 2.2990, valid accuracy: 0.1252\n",
      "Iter-6690 train loss: 2.2973 valid loss: 2.2990, valid accuracy: 0.1244\n",
      "Iter-6700 train loss: 2.2991 valid loss: 2.2990, valid accuracy: 0.1244\n",
      "Iter-6710 train loss: 2.3007 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6720 train loss: 2.2930 valid loss: 2.2990, valid accuracy: 0.1244\n",
      "Iter-6730 train loss: 2.3026 valid loss: 2.2990, valid accuracy: 0.1244\n",
      "Iter-6740 train loss: 2.3106 valid loss: 2.2990, valid accuracy: 0.1246\n",
      "Iter-6750 train loss: 2.3060 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6760 train loss: 2.2982 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6770 train loss: 2.2979 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6780 train loss: 2.3019 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6790 train loss: 2.2978 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6800 train loss: 2.3058 valid loss: 2.2990, valid accuracy: 0.1248\n",
      "Iter-6810 train loss: 2.3041 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-6820 train loss: 2.3040 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6830 train loss: 2.2996 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-6840 train loss: 2.2985 valid loss: 2.2989, valid accuracy: 0.1244\n",
      "Iter-6850 train loss: 2.2936 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6860 train loss: 2.2985 valid loss: 2.2989, valid accuracy: 0.1246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.3043 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6880 train loss: 2.2964 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6890 train loss: 2.2964 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-6900 train loss: 2.2968 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-6910 train loss: 2.3041 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-6920 train loss: 2.3049 valid loss: 2.2989, valid accuracy: 0.1242\n",
      "Iter-6930 train loss: 2.3057 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6940 train loss: 2.2944 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6950 train loss: 2.3187 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-6960 train loss: 2.3065 valid loss: 2.2989, valid accuracy: 0.1244\n",
      "Iter-6970 train loss: 2.2975 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-6980 train loss: 2.3047 valid loss: 2.2989, valid accuracy: 0.1250\n",
      "Iter-6990 train loss: 2.3060 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-7000 train loss: 2.3001 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-7010 train loss: 2.3002 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-7020 train loss: 2.3001 valid loss: 2.2989, valid accuracy: 0.1246\n",
      "Iter-7030 train loss: 2.3062 valid loss: 2.2989, valid accuracy: 0.1250\n",
      "Iter-7040 train loss: 2.3100 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-7050 train loss: 2.2967 valid loss: 2.2989, valid accuracy: 0.1250\n",
      "Iter-7060 train loss: 2.2956 valid loss: 2.2989, valid accuracy: 0.1248\n",
      "Iter-7070 train loss: 2.3052 valid loss: 2.2989, valid accuracy: 0.1244\n",
      "Iter-7080 train loss: 2.2940 valid loss: 2.2989, valid accuracy: 0.1242\n",
      "Iter-7090 train loss: 2.2956 valid loss: 2.2989, valid accuracy: 0.1240\n",
      "Iter-7100 train loss: 2.3000 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7110 train loss: 2.2988 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7120 train loss: 2.2939 valid loss: 2.2988, valid accuracy: 0.1238\n",
      "Iter-7130 train loss: 2.2937 valid loss: 2.2988, valid accuracy: 0.1240\n",
      "Iter-7140 train loss: 2.2982 valid loss: 2.2988, valid accuracy: 0.1240\n",
      "Iter-7150 train loss: 2.3054 valid loss: 2.2988, valid accuracy: 0.1238\n",
      "Iter-7160 train loss: 2.2953 valid loss: 2.2988, valid accuracy: 0.1240\n",
      "Iter-7170 train loss: 2.2933 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7180 train loss: 2.2951 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7190 train loss: 2.3008 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7200 train loss: 2.2983 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7210 train loss: 2.3022 valid loss: 2.2988, valid accuracy: 0.1246\n",
      "Iter-7220 train loss: 2.3027 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7230 train loss: 2.3023 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7240 train loss: 2.3072 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7250 train loss: 2.3006 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7260 train loss: 2.2885 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7270 train loss: 2.3097 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7280 train loss: 2.3043 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7290 train loss: 2.3080 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7300 train loss: 2.2949 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7310 train loss: 2.2991 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7320 train loss: 2.2989 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7330 train loss: 2.3051 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7340 train loss: 2.3068 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7350 train loss: 2.2972 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7360 train loss: 2.2988 valid loss: 2.2988, valid accuracy: 0.1244\n",
      "Iter-7370 train loss: 2.2959 valid loss: 2.2988, valid accuracy: 0.1248\n",
      "Iter-7380 train loss: 2.3094 valid loss: 2.2988, valid accuracy: 0.1246\n",
      "Iter-7390 train loss: 2.3005 valid loss: 2.2988, valid accuracy: 0.1242\n",
      "Iter-7400 train loss: 2.3031 valid loss: 2.2987, valid accuracy: 0.1244\n",
      "Iter-7410 train loss: 2.2944 valid loss: 2.2987, valid accuracy: 0.1244\n",
      "Iter-7420 train loss: 2.2999 valid loss: 2.2987, valid accuracy: 0.1246\n",
      "Iter-7430 train loss: 2.3046 valid loss: 2.2987, valid accuracy: 0.1244\n",
      "Iter-7440 train loss: 2.2969 valid loss: 2.2987, valid accuracy: 0.1242\n",
      "Iter-7450 train loss: 2.2987 valid loss: 2.2987, valid accuracy: 0.1242\n",
      "Iter-7460 train loss: 2.3052 valid loss: 2.2987, valid accuracy: 0.1242\n",
      "Iter-7470 train loss: 2.2996 valid loss: 2.2987, valid accuracy: 0.1240\n",
      "Iter-7480 train loss: 2.3026 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7490 train loss: 2.3003 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7500 train loss: 2.3063 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7510 train loss: 2.3053 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7520 train loss: 2.2922 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7530 train loss: 2.2978 valid loss: 2.2987, valid accuracy: 0.1236\n",
      "Iter-7540 train loss: 2.3064 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7550 train loss: 2.2977 valid loss: 2.2987, valid accuracy: 0.1240\n",
      "Iter-7560 train loss: 2.3003 valid loss: 2.2987, valid accuracy: 0.1244\n",
      "Iter-7570 train loss: 2.3024 valid loss: 2.2987, valid accuracy: 0.1244\n",
      "Iter-7580 train loss: 2.2994 valid loss: 2.2987, valid accuracy: 0.1244\n",
      "Iter-7590 train loss: 2.3037 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7600 train loss: 2.2916 valid loss: 2.2987, valid accuracy: 0.1236\n",
      "Iter-7610 train loss: 2.2931 valid loss: 2.2987, valid accuracy: 0.1238\n",
      "Iter-7620 train loss: 2.2979 valid loss: 2.2987, valid accuracy: 0.1232\n",
      "Iter-7630 train loss: 2.2943 valid loss: 2.2987, valid accuracy: 0.1234\n",
      "Iter-7640 train loss: 2.2923 valid loss: 2.2987, valid accuracy: 0.1236\n",
      "Iter-7650 train loss: 2.2985 valid loss: 2.2987, valid accuracy: 0.1236\n",
      "Iter-7660 train loss: 2.2968 valid loss: 2.2987, valid accuracy: 0.1234\n",
      "Iter-7670 train loss: 2.3018 valid loss: 2.2987, valid accuracy: 0.1236\n",
      "Iter-7680 train loss: 2.2927 valid loss: 2.2986, valid accuracy: 0.1232\n",
      "Iter-7690 train loss: 2.2992 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7700 train loss: 2.2935 valid loss: 2.2986, valid accuracy: 0.1232\n",
      "Iter-7710 train loss: 2.3002 valid loss: 2.2986, valid accuracy: 0.1232\n",
      "Iter-7720 train loss: 2.2913 valid loss: 2.2986, valid accuracy: 0.1232\n",
      "Iter-7730 train loss: 2.3027 valid loss: 2.2986, valid accuracy: 0.1238\n",
      "Iter-7740 train loss: 2.3019 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7750 train loss: 2.2984 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7760 train loss: 2.2965 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7770 train loss: 2.2962 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7780 train loss: 2.2961 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7790 train loss: 2.3028 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7800 train loss: 2.2899 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7810 train loss: 2.3013 valid loss: 2.2986, valid accuracy: 0.1230\n",
      "Iter-7820 train loss: 2.2932 valid loss: 2.2986, valid accuracy: 0.1230\n",
      "Iter-7830 train loss: 2.2952 valid loss: 2.2986, valid accuracy: 0.1232\n",
      "Iter-7840 train loss: 2.2987 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7850 train loss: 2.3042 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7860 train loss: 2.2992 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7870 train loss: 2.2892 valid loss: 2.2986, valid accuracy: 0.1238\n",
      "Iter-7880 train loss: 2.2940 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7890 train loss: 2.2921 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7900 train loss: 2.2935 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7910 train loss: 2.2971 valid loss: 2.2986, valid accuracy: 0.1236\n",
      "Iter-7920 train loss: 2.2966 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7930 train loss: 2.2932 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7940 train loss: 2.3089 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7950 train loss: 2.3060 valid loss: 2.2986, valid accuracy: 0.1234\n",
      "Iter-7960 train loss: 2.2936 valid loss: 2.2985, valid accuracy: 0.1232\n",
      "Iter-7970 train loss: 2.2971 valid loss: 2.2985, valid accuracy: 0.1234\n",
      "Iter-7980 train loss: 2.2957 valid loss: 2.2985, valid accuracy: 0.1234\n",
      "Iter-7990 train loss: 2.3078 valid loss: 2.2985, valid accuracy: 0.1234\n",
      "Iter-8000 train loss: 2.2971 valid loss: 2.2985, valid accuracy: 0.1234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 2.2996 valid loss: 2.2985, valid accuracy: 0.1236\n",
      "Iter-8020 train loss: 2.3049 valid loss: 2.2985, valid accuracy: 0.1236\n",
      "Iter-8030 train loss: 2.2962 valid loss: 2.2985, valid accuracy: 0.1236\n",
      "Iter-8040 train loss: 2.2951 valid loss: 2.2985, valid accuracy: 0.1234\n",
      "Iter-8050 train loss: 2.2986 valid loss: 2.2985, valid accuracy: 0.1230\n",
      "Iter-8060 train loss: 2.3147 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8070 train loss: 2.2984 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8080 train loss: 2.3011 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8090 train loss: 2.3014 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8100 train loss: 2.2966 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8110 train loss: 2.2994 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8120 train loss: 2.2870 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8130 train loss: 2.2990 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8140 train loss: 2.2938 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8150 train loss: 2.3027 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8160 train loss: 2.2945 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8170 train loss: 2.3034 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8180 train loss: 2.2964 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8190 train loss: 2.3008 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8200 train loss: 2.2996 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8210 train loss: 2.2954 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8220 train loss: 2.2895 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8230 train loss: 2.3073 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8240 train loss: 2.3059 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8250 train loss: 2.3009 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8260 train loss: 2.2965 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8270 train loss: 2.3011 valid loss: 2.2985, valid accuracy: 0.1230\n",
      "Iter-8280 train loss: 2.3056 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8290 train loss: 2.3017 valid loss: 2.2985, valid accuracy: 0.1228\n",
      "Iter-8300 train loss: 2.3062 valid loss: 2.2985, valid accuracy: 0.1226\n",
      "Iter-8310 train loss: 2.2975 valid loss: 2.2985, valid accuracy: 0.1222\n",
      "Iter-8320 train loss: 2.2862 valid loss: 2.2984, valid accuracy: 0.1224\n",
      "Iter-8330 train loss: 2.2947 valid loss: 2.2984, valid accuracy: 0.1222\n",
      "Iter-8340 train loss: 2.2970 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8350 train loss: 2.2987 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8360 train loss: 2.3012 valid loss: 2.2984, valid accuracy: 0.1222\n",
      "Iter-8370 train loss: 2.2992 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8380 train loss: 2.3037 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8390 train loss: 2.3047 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8400 train loss: 2.2957 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8410 train loss: 2.3082 valid loss: 2.2984, valid accuracy: 0.1218\n",
      "Iter-8420 train loss: 2.3045 valid loss: 2.2984, valid accuracy: 0.1220\n",
      "Iter-8430 train loss: 2.2970 valid loss: 2.2984, valid accuracy: 0.1218\n",
      "Iter-8440 train loss: 2.3025 valid loss: 2.2984, valid accuracy: 0.1214\n",
      "Iter-8450 train loss: 2.2951 valid loss: 2.2984, valid accuracy: 0.1214\n",
      "Iter-8460 train loss: 2.2942 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8470 train loss: 2.2961 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8480 train loss: 2.3081 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8490 train loss: 2.3045 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8500 train loss: 2.2986 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8510 train loss: 2.2960 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8520 train loss: 2.2986 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8530 train loss: 2.2989 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8540 train loss: 2.2865 valid loss: 2.2984, valid accuracy: 0.1214\n",
      "Iter-8550 train loss: 2.2958 valid loss: 2.2984, valid accuracy: 0.1214\n",
      "Iter-8560 train loss: 2.2987 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8570 train loss: 2.3089 valid loss: 2.2984, valid accuracy: 0.1212\n",
      "Iter-8580 train loss: 2.3036 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8590 train loss: 2.3038 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8600 train loss: 2.2916 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8610 train loss: 2.2989 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8620 train loss: 2.2993 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8630 train loss: 2.3000 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8640 train loss: 2.3007 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8650 train loss: 2.2996 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8660 train loss: 2.2936 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8670 train loss: 2.2965 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8680 train loss: 2.2890 valid loss: 2.2984, valid accuracy: 0.1208\n",
      "Iter-8690 train loss: 2.2881 valid loss: 2.2983, valid accuracy: 0.1206\n",
      "Iter-8700 train loss: 2.3037 valid loss: 2.2983, valid accuracy: 0.1202\n",
      "Iter-8710 train loss: 2.2969 valid loss: 2.2983, valid accuracy: 0.1202\n",
      "Iter-8720 train loss: 2.2850 valid loss: 2.2983, valid accuracy: 0.1202\n",
      "Iter-8730 train loss: 2.2974 valid loss: 2.2983, valid accuracy: 0.1198\n",
      "Iter-8740 train loss: 2.2946 valid loss: 2.2983, valid accuracy: 0.1198\n",
      "Iter-8750 train loss: 2.3042 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8760 train loss: 2.2917 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8770 train loss: 2.2971 valid loss: 2.2983, valid accuracy: 0.1194\n",
      "Iter-8780 train loss: 2.2975 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8790 train loss: 2.3125 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8800 train loss: 2.2994 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8810 train loss: 2.2971 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8820 train loss: 2.3031 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8830 train loss: 2.2891 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8840 train loss: 2.3006 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8850 train loss: 2.3036 valid loss: 2.2983, valid accuracy: 0.1196\n",
      "Iter-8860 train loss: 2.3057 valid loss: 2.2983, valid accuracy: 0.1194\n",
      "Iter-8870 train loss: 2.3028 valid loss: 2.2983, valid accuracy: 0.1194\n",
      "Iter-8880 train loss: 2.2933 valid loss: 2.2983, valid accuracy: 0.1194\n",
      "Iter-8890 train loss: 2.2901 valid loss: 2.2983, valid accuracy: 0.1194\n",
      "Iter-8900 train loss: 2.3020 valid loss: 2.2983, valid accuracy: 0.1190\n",
      "Iter-8910 train loss: 2.2909 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-8920 train loss: 2.3048 valid loss: 2.2982, valid accuracy: 0.1196\n",
      "Iter-8930 train loss: 2.3027 valid loss: 2.2982, valid accuracy: 0.1196\n",
      "Iter-8940 train loss: 2.2949 valid loss: 2.2982, valid accuracy: 0.1194\n",
      "Iter-8950 train loss: 2.2871 valid loss: 2.2982, valid accuracy: 0.1188\n",
      "Iter-8960 train loss: 2.2993 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-8970 train loss: 2.3030 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-8980 train loss: 2.3036 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-8990 train loss: 2.3029 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-9000 train loss: 2.3004 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-9010 train loss: 2.2950 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-9020 train loss: 2.3037 valid loss: 2.2982, valid accuracy: 0.1190\n",
      "Iter-9030 train loss: 2.3040 valid loss: 2.2982, valid accuracy: 0.1192\n",
      "Iter-9040 train loss: 2.3130 valid loss: 2.2982, valid accuracy: 0.1192\n",
      "Iter-9050 train loss: 2.2978 valid loss: 2.2982, valid accuracy: 0.1192\n",
      "Iter-9060 train loss: 2.2955 valid loss: 2.2982, valid accuracy: 0.1192\n",
      "Iter-9070 train loss: 2.2944 valid loss: 2.2982, valid accuracy: 0.1188\n",
      "Iter-9080 train loss: 2.2923 valid loss: 2.2982, valid accuracy: 0.1188\n",
      "Iter-9090 train loss: 2.2967 valid loss: 2.2982, valid accuracy: 0.1186\n",
      "Iter-9100 train loss: 2.3021 valid loss: 2.2982, valid accuracy: 0.1184\n",
      "Iter-9110 train loss: 2.2901 valid loss: 2.2982, valid accuracy: 0.1184\n",
      "Iter-9120 train loss: 2.3011 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9130 train loss: 2.2898 valid loss: 2.2982, valid accuracy: 0.1180\n",
      "Iter-9140 train loss: 2.3066 valid loss: 2.2982, valid accuracy: 0.1180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.2947 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9160 train loss: 2.2964 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9170 train loss: 2.3030 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9180 train loss: 2.2990 valid loss: 2.2982, valid accuracy: 0.1180\n",
      "Iter-9190 train loss: 2.3059 valid loss: 2.2982, valid accuracy: 0.1180\n",
      "Iter-9200 train loss: 2.3010 valid loss: 2.2982, valid accuracy: 0.1180\n",
      "Iter-9210 train loss: 2.2963 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9220 train loss: 2.2943 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9230 train loss: 2.2916 valid loss: 2.2982, valid accuracy: 0.1182\n",
      "Iter-9240 train loss: 2.2901 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9250 train loss: 2.2964 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9260 train loss: 2.3053 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9270 train loss: 2.3043 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9280 train loss: 2.2970 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9290 train loss: 2.2968 valid loss: 2.2981, valid accuracy: 0.1180\n",
      "Iter-9300 train loss: 2.2947 valid loss: 2.2981, valid accuracy: 0.1180\n",
      "Iter-9310 train loss: 2.3067 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9320 train loss: 2.2991 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9330 train loss: 2.3024 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9340 train loss: 2.3075 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9350 train loss: 2.3018 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9360 train loss: 2.2998 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9370 train loss: 2.2988 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9380 train loss: 2.2977 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9390 train loss: 2.2917 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9400 train loss: 2.3024 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9410 train loss: 2.3046 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9420 train loss: 2.2973 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9430 train loss: 2.2956 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9440 train loss: 2.2961 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9450 train loss: 2.3006 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9460 train loss: 2.3044 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9470 train loss: 2.2991 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9480 train loss: 2.2961 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9490 train loss: 2.2953 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9500 train loss: 2.3036 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9510 train loss: 2.2975 valid loss: 2.2981, valid accuracy: 0.1182\n",
      "Iter-9520 train loss: 2.2925 valid loss: 2.2981, valid accuracy: 0.1180\n",
      "Iter-9530 train loss: 2.2957 valid loss: 2.2981, valid accuracy: 0.1180\n",
      "Iter-9540 train loss: 2.2975 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9550 train loss: 2.2972 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9560 train loss: 2.2931 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9570 train loss: 2.3003 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9580 train loss: 2.3085 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9590 train loss: 2.2961 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9600 train loss: 2.3014 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9610 train loss: 2.3010 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9620 train loss: 2.3015 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9630 train loss: 2.2871 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9640 train loss: 2.3061 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9650 train loss: 2.2960 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9660 train loss: 2.2991 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9670 train loss: 2.2964 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9680 train loss: 2.3027 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9690 train loss: 2.3043 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9700 train loss: 2.3000 valid loss: 2.2980, valid accuracy: 0.1182\n",
      "Iter-9710 train loss: 2.3067 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9720 train loss: 2.3049 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9730 train loss: 2.2946 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9740 train loss: 2.3055 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9750 train loss: 2.2934 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9760 train loss: 2.2980 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9770 train loss: 2.3040 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9780 train loss: 2.2954 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9790 train loss: 2.2965 valid loss: 2.2980, valid accuracy: 0.1180\n",
      "Iter-9800 train loss: 2.2972 valid loss: 2.2980, valid accuracy: 0.1178\n",
      "Iter-9810 train loss: 2.3055 valid loss: 2.2980, valid accuracy: 0.1178\n",
      "Iter-9820 train loss: 2.3045 valid loss: 2.2980, valid accuracy: 0.1178\n",
      "Iter-9830 train loss: 2.3048 valid loss: 2.2980, valid accuracy: 0.1174\n",
      "Iter-9840 train loss: 2.2993 valid loss: 2.2980, valid accuracy: 0.1174\n",
      "Iter-9850 train loss: 2.2865 valid loss: 2.2980, valid accuracy: 0.1172\n",
      "Iter-9860 train loss: 2.3070 valid loss: 2.2979, valid accuracy: 0.1172\n",
      "Iter-9870 train loss: 2.2946 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9880 train loss: 2.2984 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9890 train loss: 2.2983 valid loss: 2.2979, valid accuracy: 0.1176\n",
      "Iter-9900 train loss: 2.2948 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9910 train loss: 2.2947 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9920 train loss: 2.2902 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9930 train loss: 2.2960 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9940 train loss: 2.3095 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9950 train loss: 2.3034 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9960 train loss: 2.2957 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9970 train loss: 2.2892 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9980 train loss: 2.2910 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-9990 train loss: 2.2872 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Iter-10000 train loss: 2.3052 valid loss: 2.2979, valid accuracy: 0.1174\n",
      "Last iteration - Test accuracy mean: 0.1170, std: 0.0000, loss: 2.2985\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 10 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1220fe588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD7CAYAAACBiVhwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122106eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
