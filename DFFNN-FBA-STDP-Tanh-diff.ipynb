{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y = np.tanh(y)\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y = np.tanh(y)\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2992 valid loss: 2.2959, valid accuracy: 0.1118\n",
      "Iter-200 train loss: 2.3126 valid loss: 2.2933, valid accuracy: 0.1122\n",
      "Iter-300 train loss: 2.2856 valid loss: 2.2906, valid accuracy: 0.1172\n",
      "Iter-400 train loss: 2.2994 valid loss: 2.2879, valid accuracy: 0.1258\n",
      "Iter-500 train loss: 2.3074 valid loss: 2.2852, valid accuracy: 0.1426\n",
      "Iter-600 train loss: 2.2802 valid loss: 2.2824, valid accuracy: 0.1598\n",
      "Iter-700 train loss: 2.2843 valid loss: 2.2793, valid accuracy: 0.1778\n",
      "Iter-800 train loss: 2.2810 valid loss: 2.2759, valid accuracy: 0.1904\n",
      "Iter-900 train loss: 2.2998 valid loss: 2.2722, valid accuracy: 0.1976\n",
      "Iter-1000 train loss: 2.2593 valid loss: 2.2683, valid accuracy: 0.2006\n",
      "Iter-1100 train loss: 2.2853 valid loss: 2.2642, valid accuracy: 0.2050\n",
      "Iter-1200 train loss: 2.2480 valid loss: 2.2598, valid accuracy: 0.2100\n",
      "Iter-1300 train loss: 2.2499 valid loss: 2.2553, valid accuracy: 0.2146\n",
      "Iter-1400 train loss: 2.2975 valid loss: 2.2506, valid accuracy: 0.2162\n",
      "Iter-1500 train loss: 2.2721 valid loss: 2.2454, valid accuracy: 0.2192\n",
      "Iter-1600 train loss: 2.2371 valid loss: 2.2401, valid accuracy: 0.2248\n",
      "Iter-1700 train loss: 2.2007 valid loss: 2.2345, valid accuracy: 0.2292\n",
      "Iter-1800 train loss: 2.2426 valid loss: 2.2288, valid accuracy: 0.2350\n",
      "Iter-1900 train loss: 2.2133 valid loss: 2.2228, valid accuracy: 0.2382\n",
      "Iter-2000 train loss: 2.1834 valid loss: 2.2166, valid accuracy: 0.2440\n",
      "Iter-2100 train loss: 2.2373 valid loss: 2.2099, valid accuracy: 0.2530\n",
      "Iter-2200 train loss: 2.1838 valid loss: 2.2034, valid accuracy: 0.2586\n",
      "Iter-2300 train loss: 2.1379 valid loss: 2.1964, valid accuracy: 0.2652\n",
      "Iter-2400 train loss: 2.1753 valid loss: 2.1892, valid accuracy: 0.2722\n",
      "Iter-2500 train loss: 2.1599 valid loss: 2.1816, valid accuracy: 0.2826\n",
      "Iter-2600 train loss: 2.1306 valid loss: 2.1739, valid accuracy: 0.2922\n",
      "Iter-2700 train loss: 2.1621 valid loss: 2.1662, valid accuracy: 0.2998\n",
      "Iter-2800 train loss: 2.1785 valid loss: 2.1582, valid accuracy: 0.3098\n",
      "Iter-2900 train loss: 2.1641 valid loss: 2.1500, valid accuracy: 0.3196\n",
      "Iter-3000 train loss: 2.1495 valid loss: 2.1414, valid accuracy: 0.3362\n",
      "Iter-3100 train loss: 2.1307 valid loss: 2.1329, valid accuracy: 0.3514\n",
      "Iter-3200 train loss: 2.1446 valid loss: 2.1241, valid accuracy: 0.3658\n",
      "Iter-3300 train loss: 2.1315 valid loss: 2.1149, valid accuracy: 0.3830\n",
      "Iter-3400 train loss: 2.1672 valid loss: 2.1056, valid accuracy: 0.3976\n",
      "Iter-3500 train loss: 2.0885 valid loss: 2.0961, valid accuracy: 0.4156\n",
      "Iter-3600 train loss: 2.1481 valid loss: 2.0864, valid accuracy: 0.4328\n",
      "Iter-3700 train loss: 2.0159 valid loss: 2.0764, valid accuracy: 0.4474\n",
      "Iter-3800 train loss: 2.1130 valid loss: 2.0663, valid accuracy: 0.4590\n",
      "Iter-3900 train loss: 2.0072 valid loss: 2.0562, valid accuracy: 0.4668\n",
      "Iter-4000 train loss: 2.0581 valid loss: 2.0459, valid accuracy: 0.4770\n",
      "Iter-4100 train loss: 1.9711 valid loss: 2.0354, valid accuracy: 0.4848\n",
      "Iter-4200 train loss: 2.0695 valid loss: 2.0244, valid accuracy: 0.4902\n",
      "Iter-4300 train loss: 2.1124 valid loss: 2.0135, valid accuracy: 0.4974\n",
      "Iter-4400 train loss: 2.0145 valid loss: 2.0024, valid accuracy: 0.5036\n",
      "Iter-4500 train loss: 1.9755 valid loss: 1.9912, valid accuracy: 0.5078\n",
      "Iter-4600 train loss: 1.9632 valid loss: 1.9797, valid accuracy: 0.5124\n",
      "Iter-4700 train loss: 1.9876 valid loss: 1.9683, valid accuracy: 0.5164\n",
      "Iter-4800 train loss: 1.9827 valid loss: 1.9567, valid accuracy: 0.5208\n",
      "Iter-4900 train loss: 1.9485 valid loss: 1.9449, valid accuracy: 0.5236\n",
      "Iter-5000 train loss: 1.8909 valid loss: 1.9331, valid accuracy: 0.5258\n",
      "Iter-5100 train loss: 1.9212 valid loss: 1.9211, valid accuracy: 0.5302\n",
      "Iter-5200 train loss: 1.9596 valid loss: 1.9091, valid accuracy: 0.5322\n",
      "Iter-5300 train loss: 1.8620 valid loss: 1.8969, valid accuracy: 0.5324\n",
      "Iter-5400 train loss: 1.9333 valid loss: 1.8846, valid accuracy: 0.5364\n",
      "Iter-5500 train loss: 1.7503 valid loss: 1.8724, valid accuracy: 0.5392\n",
      "Iter-5600 train loss: 1.8715 valid loss: 1.8599, valid accuracy: 0.5416\n",
      "Iter-5700 train loss: 1.9615 valid loss: 1.8475, valid accuracy: 0.5442\n",
      "Iter-5800 train loss: 1.8922 valid loss: 1.8350, valid accuracy: 0.5456\n",
      "Iter-5900 train loss: 1.8114 valid loss: 1.8224, valid accuracy: 0.5464\n",
      "Iter-6000 train loss: 1.8234 valid loss: 1.8098, valid accuracy: 0.5478\n",
      "Iter-6100 train loss: 1.7804 valid loss: 1.7972, valid accuracy: 0.5494\n",
      "Iter-6200 train loss: 1.7340 valid loss: 1.7844, valid accuracy: 0.5520\n",
      "Iter-6300 train loss: 1.7023 valid loss: 1.7715, valid accuracy: 0.5536\n",
      "Iter-6400 train loss: 1.7600 valid loss: 1.7587, valid accuracy: 0.5556\n",
      "Iter-6500 train loss: 1.6167 valid loss: 1.7459, valid accuracy: 0.5570\n",
      "Iter-6600 train loss: 1.8422 valid loss: 1.7330, valid accuracy: 0.5594\n",
      "Iter-6700 train loss: 1.7276 valid loss: 1.7203, valid accuracy: 0.5612\n",
      "Iter-6800 train loss: 1.6636 valid loss: 1.7074, valid accuracy: 0.5626\n",
      "Iter-6900 train loss: 1.8093 valid loss: 1.6949, valid accuracy: 0.5650\n",
      "Iter-7000 train loss: 1.7265 valid loss: 1.6824, valid accuracy: 0.5660\n",
      "Iter-7100 train loss: 1.6932 valid loss: 1.6697, valid accuracy: 0.5672\n",
      "Iter-7200 train loss: 1.7417 valid loss: 1.6572, valid accuracy: 0.5686\n",
      "Iter-7300 train loss: 1.6975 valid loss: 1.6448, valid accuracy: 0.5702\n",
      "Iter-7400 train loss: 1.7205 valid loss: 1.6325, valid accuracy: 0.5730\n",
      "Iter-7500 train loss: 1.5893 valid loss: 1.6202, valid accuracy: 0.5744\n",
      "Iter-7600 train loss: 1.5977 valid loss: 1.6081, valid accuracy: 0.5768\n",
      "Iter-7700 train loss: 1.5250 valid loss: 1.5959, valid accuracy: 0.5790\n",
      "Iter-7800 train loss: 1.7436 valid loss: 1.5840, valid accuracy: 0.5810\n",
      "Iter-7900 train loss: 1.5307 valid loss: 1.5718, valid accuracy: 0.5828\n",
      "Iter-8000 train loss: 1.5268 valid loss: 1.5601, valid accuracy: 0.5852\n",
      "Iter-8100 train loss: 1.5363 valid loss: 1.5482, valid accuracy: 0.5888\n",
      "Iter-8200 train loss: 1.5785 valid loss: 1.5365, valid accuracy: 0.5920\n",
      "Iter-8300 train loss: 1.4814 valid loss: 1.5251, valid accuracy: 0.5950\n",
      "Iter-8400 train loss: 1.4239 valid loss: 1.5137, valid accuracy: 0.5968\n",
      "Iter-8500 train loss: 1.5891 valid loss: 1.5024, valid accuracy: 0.6008\n",
      "Iter-8600 train loss: 1.5517 valid loss: 1.4913, valid accuracy: 0.6042\n",
      "Iter-8700 train loss: 1.5458 valid loss: 1.4804, valid accuracy: 0.6080\n",
      "Iter-8800 train loss: 1.5138 valid loss: 1.4693, valid accuracy: 0.6136\n",
      "Iter-8900 train loss: 1.5546 valid loss: 1.4585, valid accuracy: 0.6174\n",
      "Iter-9000 train loss: 1.4246 valid loss: 1.4478, valid accuracy: 0.6192\n",
      "Iter-9100 train loss: 1.3852 valid loss: 1.4373, valid accuracy: 0.6232\n",
      "Iter-9200 train loss: 1.4931 valid loss: 1.4269, valid accuracy: 0.6270\n",
      "Iter-9300 train loss: 1.4021 valid loss: 1.4166, valid accuracy: 0.6292\n",
      "Iter-9400 train loss: 1.4557 valid loss: 1.4064, valid accuracy: 0.6322\n",
      "Iter-9500 train loss: 1.3656 valid loss: 1.3964, valid accuracy: 0.6370\n",
      "Iter-9600 train loss: 1.4175 valid loss: 1.3864, valid accuracy: 0.6416\n",
      "Iter-9700 train loss: 1.3821 valid loss: 1.3766, valid accuracy: 0.6478\n",
      "Iter-9800 train loss: 1.3980 valid loss: 1.3670, valid accuracy: 0.6524\n",
      "Iter-9900 train loss: 1.3111 valid loss: 1.3573, valid accuracy: 0.6562\n",
      "Iter-10000 train loss: 1.3989 valid loss: 1.3478, valid accuracy: 0.6596\n",
      "Iter-10100 train loss: 1.3602 valid loss: 1.3382, valid accuracy: 0.6632\n",
      "Iter-10200 train loss: 1.3404 valid loss: 1.3290, valid accuracy: 0.6656\n",
      "Iter-10300 train loss: 1.1445 valid loss: 1.3197, valid accuracy: 0.6678\n",
      "Iter-10400 train loss: 1.3624 valid loss: 1.3105, valid accuracy: 0.6718\n",
      "Iter-10500 train loss: 1.3642 valid loss: 1.3016, valid accuracy: 0.6750\n",
      "Iter-10600 train loss: 1.4143 valid loss: 1.2928, valid accuracy: 0.6792\n",
      "Iter-10700 train loss: 1.4372 valid loss: 1.2842, valid accuracy: 0.6818\n",
      "Iter-10800 train loss: 1.2288 valid loss: 1.2756, valid accuracy: 0.6862\n",
      "Iter-10900 train loss: 1.3518 valid loss: 1.2671, valid accuracy: 0.6898\n",
      "Iter-11000 train loss: 1.3650 valid loss: 1.2588, valid accuracy: 0.6950\n",
      "Iter-11100 train loss: 1.3201 valid loss: 1.2505, valid accuracy: 0.6970\n",
      "Iter-11200 train loss: 1.1350 valid loss: 1.2424, valid accuracy: 0.7020\n",
      "Iter-11300 train loss: 1.3727 valid loss: 1.2344, valid accuracy: 0.7060\n",
      "Iter-11400 train loss: 1.1957 valid loss: 1.2263, valid accuracy: 0.7102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 1.2241 valid loss: 1.2184, valid accuracy: 0.7122\n",
      "Iter-11600 train loss: 1.1549 valid loss: 1.2107, valid accuracy: 0.7146\n",
      "Iter-11700 train loss: 1.1156 valid loss: 1.2029, valid accuracy: 0.7192\n",
      "Iter-11800 train loss: 1.2298 valid loss: 1.1952, valid accuracy: 0.7222\n",
      "Iter-11900 train loss: 1.2009 valid loss: 1.1877, valid accuracy: 0.7242\n",
      "Iter-12000 train loss: 1.2186 valid loss: 1.1802, valid accuracy: 0.7266\n",
      "Iter-12100 train loss: 1.1160 valid loss: 1.1729, valid accuracy: 0.7286\n",
      "Iter-12200 train loss: 1.2257 valid loss: 1.1656, valid accuracy: 0.7300\n",
      "Iter-12300 train loss: 1.1560 valid loss: 1.1584, valid accuracy: 0.7330\n",
      "Iter-12400 train loss: 1.1761 valid loss: 1.1513, valid accuracy: 0.7358\n",
      "Iter-12500 train loss: 1.1202 valid loss: 1.1443, valid accuracy: 0.7388\n",
      "Iter-12600 train loss: 1.0546 valid loss: 1.1374, valid accuracy: 0.7390\n",
      "Iter-12700 train loss: 1.1793 valid loss: 1.1307, valid accuracy: 0.7410\n",
      "Iter-12800 train loss: 1.2785 valid loss: 1.1240, valid accuracy: 0.7428\n",
      "Iter-12900 train loss: 1.0828 valid loss: 1.1173, valid accuracy: 0.7436\n",
      "Iter-13000 train loss: 0.9723 valid loss: 1.1107, valid accuracy: 0.7448\n",
      "Iter-13100 train loss: 1.0381 valid loss: 1.1042, valid accuracy: 0.7458\n",
      "Iter-13200 train loss: 1.1061 valid loss: 1.0979, valid accuracy: 0.7488\n",
      "Iter-13300 train loss: 1.1659 valid loss: 1.0916, valid accuracy: 0.7510\n",
      "Iter-13400 train loss: 1.0853 valid loss: 1.0855, valid accuracy: 0.7524\n",
      "Iter-13500 train loss: 1.0689 valid loss: 1.0794, valid accuracy: 0.7542\n",
      "Iter-13600 train loss: 1.0674 valid loss: 1.0733, valid accuracy: 0.7554\n",
      "Iter-13700 train loss: 1.0801 valid loss: 1.0673, valid accuracy: 0.7566\n",
      "Iter-13800 train loss: 1.0666 valid loss: 1.0614, valid accuracy: 0.7584\n",
      "Iter-13900 train loss: 1.0057 valid loss: 1.0555, valid accuracy: 0.7608\n",
      "Iter-14000 train loss: 1.0811 valid loss: 1.0497, valid accuracy: 0.7622\n",
      "Iter-14100 train loss: 0.9744 valid loss: 1.0439, valid accuracy: 0.7642\n",
      "Iter-14200 train loss: 1.0922 valid loss: 1.0383, valid accuracy: 0.7658\n",
      "Iter-14300 train loss: 1.1384 valid loss: 1.0327, valid accuracy: 0.7668\n",
      "Iter-14400 train loss: 1.0893 valid loss: 1.0272, valid accuracy: 0.7678\n",
      "Iter-14500 train loss: 1.1392 valid loss: 1.0217, valid accuracy: 0.7702\n",
      "Iter-14600 train loss: 0.9926 valid loss: 1.0165, valid accuracy: 0.7706\n",
      "Iter-14700 train loss: 1.0875 valid loss: 1.0112, valid accuracy: 0.7720\n",
      "Iter-14800 train loss: 0.8950 valid loss: 1.0060, valid accuracy: 0.7726\n",
      "Iter-14900 train loss: 1.0244 valid loss: 1.0008, valid accuracy: 0.7732\n",
      "Iter-15000 train loss: 1.0818 valid loss: 0.9957, valid accuracy: 0.7752\n",
      "Iter-15100 train loss: 1.0769 valid loss: 0.9906, valid accuracy: 0.7764\n",
      "Iter-15200 train loss: 0.7794 valid loss: 0.9858, valid accuracy: 0.7774\n",
      "Iter-15300 train loss: 1.0391 valid loss: 0.9808, valid accuracy: 0.7780\n",
      "Iter-15400 train loss: 1.0313 valid loss: 0.9759, valid accuracy: 0.7782\n",
      "Iter-15500 train loss: 1.1783 valid loss: 0.9711, valid accuracy: 0.7794\n",
      "Iter-15600 train loss: 1.0293 valid loss: 0.9665, valid accuracy: 0.7810\n",
      "Iter-15700 train loss: 0.9251 valid loss: 0.9618, valid accuracy: 0.7814\n",
      "Iter-15800 train loss: 0.8439 valid loss: 0.9571, valid accuracy: 0.7820\n",
      "Iter-15900 train loss: 1.0315 valid loss: 0.9526, valid accuracy: 0.7826\n",
      "Iter-16000 train loss: 0.9177 valid loss: 0.9481, valid accuracy: 0.7842\n",
      "Iter-16100 train loss: 1.0006 valid loss: 0.9436, valid accuracy: 0.7862\n",
      "Iter-16200 train loss: 1.0347 valid loss: 0.9392, valid accuracy: 0.7866\n",
      "Iter-16300 train loss: 0.9329 valid loss: 0.9348, valid accuracy: 0.7868\n",
      "Iter-16400 train loss: 0.9843 valid loss: 0.9305, valid accuracy: 0.7872\n",
      "Iter-16500 train loss: 0.8233 valid loss: 0.9263, valid accuracy: 0.7880\n",
      "Iter-16600 train loss: 0.8604 valid loss: 0.9221, valid accuracy: 0.7880\n",
      "Iter-16700 train loss: 0.8933 valid loss: 0.9181, valid accuracy: 0.7886\n",
      "Iter-16800 train loss: 0.8783 valid loss: 0.9139, valid accuracy: 0.7886\n",
      "Iter-16900 train loss: 0.9512 valid loss: 0.9098, valid accuracy: 0.7890\n",
      "Iter-17000 train loss: 0.9510 valid loss: 0.9058, valid accuracy: 0.7892\n",
      "Iter-17100 train loss: 1.0354 valid loss: 0.9019, valid accuracy: 0.7900\n",
      "Iter-17200 train loss: 0.9537 valid loss: 0.8981, valid accuracy: 0.7910\n",
      "Iter-17300 train loss: 0.8600 valid loss: 0.8943, valid accuracy: 0.7922\n",
      "Iter-17400 train loss: 0.7787 valid loss: 0.8906, valid accuracy: 0.7930\n",
      "Iter-17500 train loss: 0.9229 valid loss: 0.8869, valid accuracy: 0.7938\n",
      "Iter-17600 train loss: 0.9147 valid loss: 0.8830, valid accuracy: 0.7942\n",
      "Iter-17700 train loss: 0.9078 valid loss: 0.8794, valid accuracy: 0.7956\n",
      "Iter-17800 train loss: 0.9327 valid loss: 0.8758, valid accuracy: 0.7966\n",
      "Iter-17900 train loss: 0.8811 valid loss: 0.8723, valid accuracy: 0.7980\n",
      "Iter-18000 train loss: 0.8119 valid loss: 0.8687, valid accuracy: 0.7988\n",
      "Iter-18100 train loss: 0.7961 valid loss: 0.8652, valid accuracy: 0.7992\n",
      "Iter-18200 train loss: 1.0835 valid loss: 0.8618, valid accuracy: 0.7996\n",
      "Iter-18300 train loss: 0.6392 valid loss: 0.8583, valid accuracy: 0.8008\n",
      "Iter-18400 train loss: 0.8495 valid loss: 0.8549, valid accuracy: 0.8012\n",
      "Iter-18500 train loss: 0.7904 valid loss: 0.8515, valid accuracy: 0.8026\n",
      "Iter-18600 train loss: 0.8409 valid loss: 0.8482, valid accuracy: 0.8026\n",
      "Iter-18700 train loss: 0.6957 valid loss: 0.8450, valid accuracy: 0.8024\n",
      "Iter-18800 train loss: 0.9442 valid loss: 0.8417, valid accuracy: 0.8040\n",
      "Iter-18900 train loss: 0.8867 valid loss: 0.8386, valid accuracy: 0.8038\n",
      "Iter-19000 train loss: 0.8489 valid loss: 0.8353, valid accuracy: 0.8046\n",
      "Iter-19100 train loss: 0.7823 valid loss: 0.8322, valid accuracy: 0.8052\n",
      "Iter-19200 train loss: 0.6755 valid loss: 0.8292, valid accuracy: 0.8056\n",
      "Iter-19300 train loss: 0.7859 valid loss: 0.8260, valid accuracy: 0.8060\n",
      "Iter-19400 train loss: 0.9304 valid loss: 0.8229, valid accuracy: 0.8066\n",
      "Iter-19500 train loss: 0.8569 valid loss: 0.8199, valid accuracy: 0.8066\n",
      "Iter-19600 train loss: 0.8594 valid loss: 0.8169, valid accuracy: 0.8062\n",
      "Iter-19700 train loss: 0.9368 valid loss: 0.8140, valid accuracy: 0.8064\n",
      "Iter-19800 train loss: 0.8071 valid loss: 0.8111, valid accuracy: 0.8066\n",
      "Iter-19900 train loss: 1.1290 valid loss: 0.8082, valid accuracy: 0.8074\n",
      "Iter-20000 train loss: 1.0105 valid loss: 0.8055, valid accuracy: 0.8084\n",
      "Iter-20100 train loss: 0.7392 valid loss: 0.8027, valid accuracy: 0.8080\n",
      "Iter-20200 train loss: 0.8043 valid loss: 0.7999, valid accuracy: 0.8082\n",
      "Iter-20300 train loss: 0.8886 valid loss: 0.7971, valid accuracy: 0.8094\n",
      "Iter-20400 train loss: 0.8633 valid loss: 0.7944, valid accuracy: 0.8098\n",
      "Iter-20500 train loss: 0.8950 valid loss: 0.7917, valid accuracy: 0.8106\n",
      "Iter-20600 train loss: 0.8312 valid loss: 0.7890, valid accuracy: 0.8102\n",
      "Iter-20700 train loss: 1.0398 valid loss: 0.7865, valid accuracy: 0.8108\n",
      "Iter-20800 train loss: 0.7622 valid loss: 0.7838, valid accuracy: 0.8122\n",
      "Iter-20900 train loss: 0.8702 valid loss: 0.7814, valid accuracy: 0.8128\n",
      "Iter-21000 train loss: 0.9338 valid loss: 0.7789, valid accuracy: 0.8132\n",
      "Iter-21100 train loss: 0.7683 valid loss: 0.7763, valid accuracy: 0.8136\n",
      "Iter-21200 train loss: 0.7514 valid loss: 0.7739, valid accuracy: 0.8136\n",
      "Iter-21300 train loss: 0.8923 valid loss: 0.7715, valid accuracy: 0.8142\n",
      "Iter-21400 train loss: 0.8226 valid loss: 0.7691, valid accuracy: 0.8142\n",
      "Iter-21500 train loss: 0.8371 valid loss: 0.7667, valid accuracy: 0.8144\n",
      "Iter-21600 train loss: 0.8534 valid loss: 0.7643, valid accuracy: 0.8146\n",
      "Iter-21700 train loss: 0.7641 valid loss: 0.7620, valid accuracy: 0.8146\n",
      "Iter-21800 train loss: 0.7880 valid loss: 0.7597, valid accuracy: 0.8144\n",
      "Iter-21900 train loss: 0.7720 valid loss: 0.7574, valid accuracy: 0.8144\n",
      "Iter-22000 train loss: 0.7523 valid loss: 0.7550, valid accuracy: 0.8152\n",
      "Iter-22100 train loss: 0.7740 valid loss: 0.7528, valid accuracy: 0.8156\n",
      "Iter-22200 train loss: 0.6720 valid loss: 0.7505, valid accuracy: 0.8160\n",
      "Iter-22300 train loss: 0.9151 valid loss: 0.7483, valid accuracy: 0.8164\n",
      "Iter-22400 train loss: 0.6370 valid loss: 0.7462, valid accuracy: 0.8170\n",
      "Iter-22500 train loss: 0.6733 valid loss: 0.7440, valid accuracy: 0.8180\n",
      "Iter-22600 train loss: 0.6071 valid loss: 0.7419, valid accuracy: 0.8180\n",
      "Iter-22700 train loss: 0.6647 valid loss: 0.7397, valid accuracy: 0.8190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 0.6988 valid loss: 0.7377, valid accuracy: 0.8196\n",
      "Iter-22900 train loss: 0.6617 valid loss: 0.7356, valid accuracy: 0.8198\n",
      "Iter-23000 train loss: 0.8358 valid loss: 0.7335, valid accuracy: 0.8206\n",
      "Iter-23100 train loss: 0.7381 valid loss: 0.7314, valid accuracy: 0.8210\n",
      "Iter-23200 train loss: 0.5704 valid loss: 0.7293, valid accuracy: 0.8210\n",
      "Iter-23300 train loss: 0.9364 valid loss: 0.7273, valid accuracy: 0.8212\n",
      "Iter-23400 train loss: 0.5737 valid loss: 0.7253, valid accuracy: 0.8212\n",
      "Iter-23500 train loss: 0.6807 valid loss: 0.7233, valid accuracy: 0.8216\n",
      "Iter-23600 train loss: 0.7454 valid loss: 0.7213, valid accuracy: 0.8216\n",
      "Iter-23700 train loss: 0.7527 valid loss: 0.7194, valid accuracy: 0.8224\n",
      "Iter-23800 train loss: 0.7224 valid loss: 0.7175, valid accuracy: 0.8226\n",
      "Iter-23900 train loss: 0.8262 valid loss: 0.7156, valid accuracy: 0.8234\n",
      "Iter-24000 train loss: 0.7419 valid loss: 0.7138, valid accuracy: 0.8236\n",
      "Iter-24100 train loss: 0.5689 valid loss: 0.7120, valid accuracy: 0.8240\n",
      "Iter-24200 train loss: 0.6102 valid loss: 0.7101, valid accuracy: 0.8250\n",
      "Iter-24300 train loss: 0.7077 valid loss: 0.7083, valid accuracy: 0.8248\n",
      "Iter-24400 train loss: 0.7663 valid loss: 0.7064, valid accuracy: 0.8250\n",
      "Iter-24500 train loss: 0.6776 valid loss: 0.7046, valid accuracy: 0.8258\n",
      "Iter-24600 train loss: 0.7012 valid loss: 0.7029, valid accuracy: 0.8264\n",
      "Iter-24700 train loss: 0.8783 valid loss: 0.7011, valid accuracy: 0.8266\n",
      "Iter-24800 train loss: 0.8220 valid loss: 0.6994, valid accuracy: 0.8272\n",
      "Iter-24900 train loss: 0.6125 valid loss: 0.6977, valid accuracy: 0.8272\n",
      "Iter-25000 train loss: 0.5363 valid loss: 0.6960, valid accuracy: 0.8268\n",
      "Iter-25100 train loss: 0.6661 valid loss: 0.6943, valid accuracy: 0.8272\n",
      "Iter-25200 train loss: 0.6041 valid loss: 0.6926, valid accuracy: 0.8272\n",
      "Iter-25300 train loss: 0.4496 valid loss: 0.6910, valid accuracy: 0.8272\n",
      "Iter-25400 train loss: 0.7362 valid loss: 0.6893, valid accuracy: 0.8278\n",
      "Iter-25500 train loss: 0.6628 valid loss: 0.6877, valid accuracy: 0.8284\n",
      "Iter-25600 train loss: 0.8428 valid loss: 0.6860, valid accuracy: 0.8288\n",
      "Iter-25700 train loss: 0.7591 valid loss: 0.6844, valid accuracy: 0.8292\n",
      "Iter-25800 train loss: 0.5841 valid loss: 0.6828, valid accuracy: 0.8286\n",
      "Iter-25900 train loss: 0.4930 valid loss: 0.6812, valid accuracy: 0.8286\n",
      "Iter-26000 train loss: 0.7849 valid loss: 0.6796, valid accuracy: 0.8292\n",
      "Iter-26100 train loss: 0.5550 valid loss: 0.6780, valid accuracy: 0.8294\n",
      "Iter-26200 train loss: 0.5941 valid loss: 0.6765, valid accuracy: 0.8294\n",
      "Iter-26300 train loss: 0.6364 valid loss: 0.6749, valid accuracy: 0.8302\n",
      "Iter-26400 train loss: 0.7530 valid loss: 0.6734, valid accuracy: 0.8302\n",
      "Iter-26500 train loss: 0.5848 valid loss: 0.6720, valid accuracy: 0.8302\n",
      "Iter-26600 train loss: 0.8301 valid loss: 0.6705, valid accuracy: 0.8302\n",
      "Iter-26700 train loss: 0.6910 valid loss: 0.6690, valid accuracy: 0.8304\n",
      "Iter-26800 train loss: 0.6440 valid loss: 0.6675, valid accuracy: 0.8306\n",
      "Iter-26900 train loss: 0.5560 valid loss: 0.6660, valid accuracy: 0.8312\n",
      "Iter-27000 train loss: 0.7332 valid loss: 0.6645, valid accuracy: 0.8322\n",
      "Iter-27100 train loss: 0.7056 valid loss: 0.6631, valid accuracy: 0.8324\n",
      "Iter-27200 train loss: 0.5830 valid loss: 0.6617, valid accuracy: 0.8324\n",
      "Iter-27300 train loss: 0.6803 valid loss: 0.6604, valid accuracy: 0.8320\n",
      "Iter-27400 train loss: 0.6930 valid loss: 0.6590, valid accuracy: 0.8322\n",
      "Iter-27500 train loss: 0.5787 valid loss: 0.6576, valid accuracy: 0.8326\n",
      "Iter-27600 train loss: 0.7115 valid loss: 0.6563, valid accuracy: 0.8322\n",
      "Iter-27700 train loss: 0.5821 valid loss: 0.6549, valid accuracy: 0.8320\n",
      "Iter-27800 train loss: 0.7173 valid loss: 0.6536, valid accuracy: 0.8326\n",
      "Iter-27900 train loss: 0.7359 valid loss: 0.6522, valid accuracy: 0.8324\n",
      "Iter-28000 train loss: 0.7195 valid loss: 0.6509, valid accuracy: 0.8324\n",
      "Iter-28100 train loss: 0.6471 valid loss: 0.6495, valid accuracy: 0.8332\n",
      "Iter-28200 train loss: 0.7910 valid loss: 0.6482, valid accuracy: 0.8332\n",
      "Iter-28300 train loss: 0.5690 valid loss: 0.6469, valid accuracy: 0.8336\n",
      "Iter-28400 train loss: 0.5821 valid loss: 0.6456, valid accuracy: 0.8336\n",
      "Iter-28500 train loss: 0.6969 valid loss: 0.6444, valid accuracy: 0.8342\n",
      "Iter-28600 train loss: 0.8143 valid loss: 0.6431, valid accuracy: 0.8342\n",
      "Iter-28700 train loss: 0.8789 valid loss: 0.6419, valid accuracy: 0.8340\n",
      "Iter-28800 train loss: 0.6635 valid loss: 0.6406, valid accuracy: 0.8338\n",
      "Iter-28900 train loss: 0.4989 valid loss: 0.6394, valid accuracy: 0.8344\n",
      "Iter-29000 train loss: 0.6378 valid loss: 0.6382, valid accuracy: 0.8346\n",
      "Iter-29100 train loss: 0.5699 valid loss: 0.6370, valid accuracy: 0.8350\n",
      "Iter-29200 train loss: 0.7681 valid loss: 0.6357, valid accuracy: 0.8352\n",
      "Iter-29300 train loss: 0.6117 valid loss: 0.6345, valid accuracy: 0.8360\n",
      "Iter-29400 train loss: 0.7120 valid loss: 0.6333, valid accuracy: 0.8364\n",
      "Iter-29500 train loss: 0.8112 valid loss: 0.6321, valid accuracy: 0.8366\n",
      "Iter-29600 train loss: 0.5370 valid loss: 0.6310, valid accuracy: 0.8366\n",
      "Iter-29700 train loss: 0.6863 valid loss: 0.6298, valid accuracy: 0.8364\n",
      "Iter-29800 train loss: 0.6566 valid loss: 0.6286, valid accuracy: 0.8368\n",
      "Iter-29900 train loss: 0.5893 valid loss: 0.6274, valid accuracy: 0.8374\n",
      "Iter-30000 train loss: 0.5351 valid loss: 0.6263, valid accuracy: 0.8376\n",
      "Iter-30100 train loss: 0.7336 valid loss: 0.6251, valid accuracy: 0.8378\n",
      "Iter-30200 train loss: 0.8737 valid loss: 0.6240, valid accuracy: 0.8380\n",
      "Iter-30300 train loss: 0.6000 valid loss: 0.6229, valid accuracy: 0.8380\n",
      "Iter-30400 train loss: 0.5618 valid loss: 0.6218, valid accuracy: 0.8382\n",
      "Iter-30500 train loss: 0.7952 valid loss: 0.6207, valid accuracy: 0.8386\n",
      "Iter-30600 train loss: 0.7177 valid loss: 0.6196, valid accuracy: 0.8390\n",
      "Iter-30700 train loss: 0.4482 valid loss: 0.6185, valid accuracy: 0.8390\n",
      "Iter-30800 train loss: 0.6463 valid loss: 0.6175, valid accuracy: 0.8390\n",
      "Iter-30900 train loss: 0.5427 valid loss: 0.6164, valid accuracy: 0.8392\n",
      "Iter-31000 train loss: 0.4517 valid loss: 0.6154, valid accuracy: 0.8396\n",
      "Iter-31100 train loss: 0.6165 valid loss: 0.6143, valid accuracy: 0.8396\n",
      "Iter-31200 train loss: 0.6028 valid loss: 0.6133, valid accuracy: 0.8398\n",
      "Iter-31300 train loss: 0.5838 valid loss: 0.6123, valid accuracy: 0.8400\n",
      "Iter-31400 train loss: 0.5685 valid loss: 0.6112, valid accuracy: 0.8404\n",
      "Iter-31500 train loss: 0.7870 valid loss: 0.6103, valid accuracy: 0.8406\n",
      "Iter-31600 train loss: 0.5877 valid loss: 0.6092, valid accuracy: 0.8406\n",
      "Iter-31700 train loss: 0.7081 valid loss: 0.6082, valid accuracy: 0.8408\n",
      "Iter-31800 train loss: 0.7049 valid loss: 0.6072, valid accuracy: 0.8412\n",
      "Iter-31900 train loss: 0.5546 valid loss: 0.6062, valid accuracy: 0.8412\n",
      "Iter-32000 train loss: 0.6181 valid loss: 0.6052, valid accuracy: 0.8414\n",
      "Iter-32100 train loss: 0.6999 valid loss: 0.6043, valid accuracy: 0.8416\n",
      "Iter-32200 train loss: 0.7121 valid loss: 0.6032, valid accuracy: 0.8418\n",
      "Iter-32300 train loss: 0.7900 valid loss: 0.6023, valid accuracy: 0.8420\n",
      "Iter-32400 train loss: 0.6169 valid loss: 0.6013, valid accuracy: 0.8418\n",
      "Iter-32500 train loss: 0.5865 valid loss: 0.6003, valid accuracy: 0.8420\n",
      "Iter-32600 train loss: 0.6510 valid loss: 0.5994, valid accuracy: 0.8424\n",
      "Iter-32700 train loss: 0.5398 valid loss: 0.5984, valid accuracy: 0.8420\n",
      "Iter-32800 train loss: 0.6252 valid loss: 0.5975, valid accuracy: 0.8424\n",
      "Iter-32900 train loss: 0.6261 valid loss: 0.5965, valid accuracy: 0.8428\n",
      "Iter-33000 train loss: 0.6222 valid loss: 0.5956, valid accuracy: 0.8436\n",
      "Iter-33100 train loss: 0.5948 valid loss: 0.5947, valid accuracy: 0.8440\n",
      "Iter-33200 train loss: 0.4928 valid loss: 0.5938, valid accuracy: 0.8440\n",
      "Iter-33300 train loss: 0.6100 valid loss: 0.5929, valid accuracy: 0.8444\n",
      "Iter-33400 train loss: 0.5836 valid loss: 0.5920, valid accuracy: 0.8446\n",
      "Iter-33500 train loss: 0.6098 valid loss: 0.5911, valid accuracy: 0.8450\n",
      "Iter-33600 train loss: 0.4541 valid loss: 0.5902, valid accuracy: 0.8448\n",
      "Iter-33700 train loss: 0.7701 valid loss: 0.5893, valid accuracy: 0.8452\n",
      "Iter-33800 train loss: 0.5571 valid loss: 0.5884, valid accuracy: 0.8452\n",
      "Iter-33900 train loss: 0.4794 valid loss: 0.5876, valid accuracy: 0.8454\n",
      "Iter-34000 train loss: 0.4880 valid loss: 0.5867, valid accuracy: 0.8452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 0.4658 valid loss: 0.5859, valid accuracy: 0.8456\n",
      "Iter-34200 train loss: 0.7345 valid loss: 0.5850, valid accuracy: 0.8458\n",
      "Iter-34300 train loss: 0.4584 valid loss: 0.5841, valid accuracy: 0.8464\n",
      "Iter-34400 train loss: 0.5435 valid loss: 0.5833, valid accuracy: 0.8468\n",
      "Iter-34500 train loss: 0.5279 valid loss: 0.5824, valid accuracy: 0.8468\n",
      "Iter-34600 train loss: 0.6659 valid loss: 0.5815, valid accuracy: 0.8470\n",
      "Iter-34700 train loss: 0.6664 valid loss: 0.5807, valid accuracy: 0.8468\n",
      "Iter-34800 train loss: 0.9102 valid loss: 0.5799, valid accuracy: 0.8472\n",
      "Iter-34900 train loss: 0.7287 valid loss: 0.5791, valid accuracy: 0.8472\n",
      "Iter-35000 train loss: 0.7391 valid loss: 0.5783, valid accuracy: 0.8470\n",
      "Iter-35100 train loss: 0.7342 valid loss: 0.5774, valid accuracy: 0.8472\n",
      "Iter-35200 train loss: 0.4896 valid loss: 0.5767, valid accuracy: 0.8474\n",
      "Iter-35300 train loss: 0.5491 valid loss: 0.5759, valid accuracy: 0.8474\n",
      "Iter-35400 train loss: 0.6847 valid loss: 0.5751, valid accuracy: 0.8476\n",
      "Iter-35500 train loss: 0.6059 valid loss: 0.5743, valid accuracy: 0.8478\n",
      "Iter-35600 train loss: 0.6517 valid loss: 0.5734, valid accuracy: 0.8478\n",
      "Iter-35700 train loss: 0.6535 valid loss: 0.5726, valid accuracy: 0.8478\n",
      "Iter-35800 train loss: 0.5308 valid loss: 0.5719, valid accuracy: 0.8474\n",
      "Iter-35900 train loss: 0.6488 valid loss: 0.5711, valid accuracy: 0.8478\n",
      "Iter-36000 train loss: 0.5951 valid loss: 0.5703, valid accuracy: 0.8484\n",
      "Iter-36100 train loss: 0.4583 valid loss: 0.5696, valid accuracy: 0.8486\n",
      "Iter-36200 train loss: 0.6948 valid loss: 0.5689, valid accuracy: 0.8490\n",
      "Iter-36300 train loss: 0.6967 valid loss: 0.5681, valid accuracy: 0.8492\n",
      "Iter-36400 train loss: 0.4362 valid loss: 0.5674, valid accuracy: 0.8492\n",
      "Iter-36500 train loss: 0.4924 valid loss: 0.5667, valid accuracy: 0.8494\n",
      "Iter-36600 train loss: 0.5824 valid loss: 0.5659, valid accuracy: 0.8496\n",
      "Iter-36700 train loss: 0.5361 valid loss: 0.5652, valid accuracy: 0.8496\n",
      "Iter-36800 train loss: 0.4996 valid loss: 0.5645, valid accuracy: 0.8498\n",
      "Iter-36900 train loss: 0.6245 valid loss: 0.5637, valid accuracy: 0.8496\n",
      "Iter-37000 train loss: 1.0476 valid loss: 0.5630, valid accuracy: 0.8500\n",
      "Iter-37100 train loss: 0.5349 valid loss: 0.5622, valid accuracy: 0.8502\n",
      "Iter-37200 train loss: 0.7343 valid loss: 0.5615, valid accuracy: 0.8502\n",
      "Iter-37300 train loss: 0.6502 valid loss: 0.5608, valid accuracy: 0.8504\n",
      "Iter-37400 train loss: 0.6149 valid loss: 0.5600, valid accuracy: 0.8506\n",
      "Iter-37500 train loss: 0.4977 valid loss: 0.5594, valid accuracy: 0.8508\n",
      "Iter-37600 train loss: 0.5768 valid loss: 0.5587, valid accuracy: 0.8510\n",
      "Iter-37700 train loss: 0.5650 valid loss: 0.5580, valid accuracy: 0.8508\n",
      "Iter-37800 train loss: 0.7105 valid loss: 0.5573, valid accuracy: 0.8508\n",
      "Iter-37900 train loss: 0.6249 valid loss: 0.5566, valid accuracy: 0.8510\n",
      "Iter-38000 train loss: 0.7155 valid loss: 0.5559, valid accuracy: 0.8512\n",
      "Iter-38100 train loss: 0.7066 valid loss: 0.5553, valid accuracy: 0.8510\n",
      "Iter-38200 train loss: 0.4397 valid loss: 0.5546, valid accuracy: 0.8508\n",
      "Iter-38300 train loss: 0.5971 valid loss: 0.5539, valid accuracy: 0.8510\n",
      "Iter-38400 train loss: 0.5197 valid loss: 0.5533, valid accuracy: 0.8510\n",
      "Iter-38500 train loss: 0.4530 valid loss: 0.5525, valid accuracy: 0.8516\n",
      "Iter-38600 train loss: 0.6488 valid loss: 0.5519, valid accuracy: 0.8516\n",
      "Iter-38700 train loss: 0.6504 valid loss: 0.5511, valid accuracy: 0.8518\n",
      "Iter-38800 train loss: 0.6077 valid loss: 0.5504, valid accuracy: 0.8512\n",
      "Iter-38900 train loss: 0.5468 valid loss: 0.5498, valid accuracy: 0.8510\n",
      "Iter-39000 train loss: 0.5097 valid loss: 0.5492, valid accuracy: 0.8514\n",
      "Iter-39100 train loss: 0.5630 valid loss: 0.5486, valid accuracy: 0.8514\n",
      "Iter-39200 train loss: 0.5332 valid loss: 0.5479, valid accuracy: 0.8520\n",
      "Iter-39300 train loss: 0.8158 valid loss: 0.5473, valid accuracy: 0.8520\n",
      "Iter-39400 train loss: 0.5696 valid loss: 0.5466, valid accuracy: 0.8522\n",
      "Iter-39500 train loss: 0.4349 valid loss: 0.5460, valid accuracy: 0.8524\n",
      "Iter-39600 train loss: 0.5762 valid loss: 0.5454, valid accuracy: 0.8526\n",
      "Iter-39700 train loss: 0.6437 valid loss: 0.5447, valid accuracy: 0.8530\n",
      "Iter-39800 train loss: 0.7577 valid loss: 0.5441, valid accuracy: 0.8530\n",
      "Iter-39900 train loss: 0.6059 valid loss: 0.5434, valid accuracy: 0.8528\n",
      "Iter-40000 train loss: 0.5144 valid loss: 0.5428, valid accuracy: 0.8528\n",
      "Iter-40100 train loss: 0.6874 valid loss: 0.5422, valid accuracy: 0.8532\n",
      "Iter-40200 train loss: 0.4302 valid loss: 0.5416, valid accuracy: 0.8532\n",
      "Iter-40300 train loss: 0.3741 valid loss: 0.5410, valid accuracy: 0.8532\n",
      "Iter-40400 train loss: 0.8032 valid loss: 0.5405, valid accuracy: 0.8534\n",
      "Iter-40500 train loss: 0.3866 valid loss: 0.5399, valid accuracy: 0.8538\n",
      "Iter-40600 train loss: 0.3732 valid loss: 0.5393, valid accuracy: 0.8536\n",
      "Iter-40700 train loss: 0.7838 valid loss: 0.5387, valid accuracy: 0.8536\n",
      "Iter-40800 train loss: 0.4517 valid loss: 0.5380, valid accuracy: 0.8536\n",
      "Iter-40900 train loss: 0.6059 valid loss: 0.5375, valid accuracy: 0.8538\n",
      "Iter-41000 train loss: 0.6021 valid loss: 0.5368, valid accuracy: 0.8538\n",
      "Iter-41100 train loss: 0.5689 valid loss: 0.5363, valid accuracy: 0.8538\n",
      "Iter-41200 train loss: 0.4227 valid loss: 0.5357, valid accuracy: 0.8540\n",
      "Iter-41300 train loss: 0.5815 valid loss: 0.5351, valid accuracy: 0.8540\n",
      "Iter-41400 train loss: 0.6212 valid loss: 0.5345, valid accuracy: 0.8540\n",
      "Iter-41500 train loss: 0.8435 valid loss: 0.5339, valid accuracy: 0.8542\n",
      "Iter-41600 train loss: 0.5807 valid loss: 0.5334, valid accuracy: 0.8542\n",
      "Iter-41700 train loss: 0.6683 valid loss: 0.5329, valid accuracy: 0.8542\n",
      "Iter-41800 train loss: 0.4783 valid loss: 0.5323, valid accuracy: 0.8544\n",
      "Iter-41900 train loss: 0.3940 valid loss: 0.5317, valid accuracy: 0.8546\n",
      "Iter-42000 train loss: 0.5911 valid loss: 0.5312, valid accuracy: 0.8546\n",
      "Iter-42100 train loss: 0.6942 valid loss: 0.5306, valid accuracy: 0.8548\n",
      "Iter-42200 train loss: 0.6685 valid loss: 0.5301, valid accuracy: 0.8550\n",
      "Iter-42300 train loss: 0.7370 valid loss: 0.5295, valid accuracy: 0.8552\n",
      "Iter-42400 train loss: 0.3478 valid loss: 0.5290, valid accuracy: 0.8552\n",
      "Iter-42500 train loss: 0.5984 valid loss: 0.5284, valid accuracy: 0.8556\n",
      "Iter-42600 train loss: 0.4312 valid loss: 0.5279, valid accuracy: 0.8554\n",
      "Iter-42700 train loss: 0.4487 valid loss: 0.5274, valid accuracy: 0.8560\n",
      "Iter-42800 train loss: 0.4696 valid loss: 0.5269, valid accuracy: 0.8560\n",
      "Iter-42900 train loss: 0.5802 valid loss: 0.5264, valid accuracy: 0.8562\n",
      "Iter-43000 train loss: 0.7161 valid loss: 0.5259, valid accuracy: 0.8562\n",
      "Iter-43100 train loss: 0.4623 valid loss: 0.5253, valid accuracy: 0.8564\n",
      "Iter-43200 train loss: 0.6029 valid loss: 0.5248, valid accuracy: 0.8564\n",
      "Iter-43300 train loss: 0.4400 valid loss: 0.5242, valid accuracy: 0.8566\n",
      "Iter-43400 train loss: 0.5191 valid loss: 0.5237, valid accuracy: 0.8568\n",
      "Iter-43500 train loss: 0.5853 valid loss: 0.5231, valid accuracy: 0.8568\n",
      "Iter-43600 train loss: 0.3487 valid loss: 0.5226, valid accuracy: 0.8566\n",
      "Iter-43700 train loss: 0.4633 valid loss: 0.5221, valid accuracy: 0.8570\n",
      "Iter-43800 train loss: 0.4436 valid loss: 0.5216, valid accuracy: 0.8570\n",
      "Iter-43900 train loss: 0.3652 valid loss: 0.5211, valid accuracy: 0.8572\n",
      "Iter-44000 train loss: 0.4551 valid loss: 0.5205, valid accuracy: 0.8570\n",
      "Iter-44100 train loss: 0.5268 valid loss: 0.5200, valid accuracy: 0.8570\n",
      "Iter-44200 train loss: 0.4861 valid loss: 0.5195, valid accuracy: 0.8576\n",
      "Iter-44300 train loss: 0.3674 valid loss: 0.5190, valid accuracy: 0.8578\n",
      "Iter-44400 train loss: 0.4359 valid loss: 0.5185, valid accuracy: 0.8578\n",
      "Iter-44500 train loss: 0.6195 valid loss: 0.5180, valid accuracy: 0.8580\n",
      "Iter-44600 train loss: 0.6850 valid loss: 0.5175, valid accuracy: 0.8580\n",
      "Iter-44700 train loss: 0.5374 valid loss: 0.5171, valid accuracy: 0.8580\n",
      "Iter-44800 train loss: 0.6182 valid loss: 0.5166, valid accuracy: 0.8582\n",
      "Iter-44900 train loss: 0.6437 valid loss: 0.5161, valid accuracy: 0.8584\n",
      "Iter-45000 train loss: 0.3454 valid loss: 0.5156, valid accuracy: 0.8582\n",
      "Iter-45100 train loss: 0.4385 valid loss: 0.5151, valid accuracy: 0.8582\n",
      "Iter-45200 train loss: 0.6088 valid loss: 0.5146, valid accuracy: 0.8584\n",
      "Iter-45300 train loss: 0.3652 valid loss: 0.5142, valid accuracy: 0.8584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 0.4006 valid loss: 0.5137, valid accuracy: 0.8584\n",
      "Iter-45500 train loss: 0.4703 valid loss: 0.5132, valid accuracy: 0.8584\n",
      "Iter-45600 train loss: 0.5235 valid loss: 0.5127, valid accuracy: 0.8582\n",
      "Iter-45700 train loss: 0.6124 valid loss: 0.5123, valid accuracy: 0.8584\n",
      "Iter-45800 train loss: 0.8073 valid loss: 0.5119, valid accuracy: 0.8584\n",
      "Iter-45900 train loss: 0.5353 valid loss: 0.5114, valid accuracy: 0.8584\n",
      "Iter-46000 train loss: 0.8074 valid loss: 0.5109, valid accuracy: 0.8584\n",
      "Iter-46100 train loss: 0.4710 valid loss: 0.5105, valid accuracy: 0.8584\n",
      "Iter-46200 train loss: 0.5152 valid loss: 0.5101, valid accuracy: 0.8586\n",
      "Iter-46300 train loss: 0.5205 valid loss: 0.5096, valid accuracy: 0.8590\n",
      "Iter-46400 train loss: 0.4404 valid loss: 0.5091, valid accuracy: 0.8590\n",
      "Iter-46500 train loss: 0.6596 valid loss: 0.5087, valid accuracy: 0.8590\n",
      "Iter-46600 train loss: 0.4304 valid loss: 0.5083, valid accuracy: 0.8592\n",
      "Iter-46700 train loss: 0.4493 valid loss: 0.5078, valid accuracy: 0.8592\n",
      "Iter-46800 train loss: 0.5429 valid loss: 0.5074, valid accuracy: 0.8594\n",
      "Iter-46900 train loss: 0.4988 valid loss: 0.5069, valid accuracy: 0.8594\n",
      "Iter-47000 train loss: 0.6341 valid loss: 0.5064, valid accuracy: 0.8594\n",
      "Iter-47100 train loss: 0.7820 valid loss: 0.5060, valid accuracy: 0.8592\n",
      "Iter-47200 train loss: 0.5698 valid loss: 0.5055, valid accuracy: 0.8594\n",
      "Iter-47300 train loss: 0.3955 valid loss: 0.5051, valid accuracy: 0.8594\n",
      "Iter-47400 train loss: 0.4554 valid loss: 0.5047, valid accuracy: 0.8598\n",
      "Iter-47500 train loss: 0.7338 valid loss: 0.5042, valid accuracy: 0.8600\n",
      "Iter-47600 train loss: 0.7120 valid loss: 0.5038, valid accuracy: 0.8600\n",
      "Iter-47700 train loss: 0.2906 valid loss: 0.5034, valid accuracy: 0.8606\n",
      "Iter-47800 train loss: 0.4734 valid loss: 0.5029, valid accuracy: 0.8604\n",
      "Iter-47900 train loss: 0.4664 valid loss: 0.5025, valid accuracy: 0.8604\n",
      "Iter-48000 train loss: 0.6286 valid loss: 0.5020, valid accuracy: 0.8606\n",
      "Iter-48100 train loss: 0.7188 valid loss: 0.5016, valid accuracy: 0.8608\n",
      "Iter-48200 train loss: 0.5637 valid loss: 0.5012, valid accuracy: 0.8608\n",
      "Iter-48300 train loss: 0.4564 valid loss: 0.5007, valid accuracy: 0.8608\n",
      "Iter-48400 train loss: 0.3867 valid loss: 0.5003, valid accuracy: 0.8608\n",
      "Iter-48500 train loss: 0.5719 valid loss: 0.4999, valid accuracy: 0.8612\n",
      "Iter-48600 train loss: 0.5305 valid loss: 0.4995, valid accuracy: 0.8612\n",
      "Iter-48700 train loss: 0.5699 valid loss: 0.4991, valid accuracy: 0.8612\n",
      "Iter-48800 train loss: 0.4584 valid loss: 0.4987, valid accuracy: 0.8614\n",
      "Iter-48900 train loss: 0.6354 valid loss: 0.4982, valid accuracy: 0.8614\n",
      "Iter-49000 train loss: 0.3443 valid loss: 0.4978, valid accuracy: 0.8618\n",
      "Iter-49100 train loss: 0.6299 valid loss: 0.4974, valid accuracy: 0.8616\n",
      "Iter-49200 train loss: 0.5048 valid loss: 0.4971, valid accuracy: 0.8620\n",
      "Iter-49300 train loss: 0.7469 valid loss: 0.4966, valid accuracy: 0.8622\n",
      "Iter-49400 train loss: 0.4446 valid loss: 0.4961, valid accuracy: 0.8622\n",
      "Iter-49500 train loss: 0.5458 valid loss: 0.4958, valid accuracy: 0.8624\n",
      "Iter-49600 train loss: 0.7302 valid loss: 0.4954, valid accuracy: 0.8626\n",
      "Iter-49700 train loss: 0.2801 valid loss: 0.4950, valid accuracy: 0.8626\n",
      "Iter-49800 train loss: 0.7620 valid loss: 0.4946, valid accuracy: 0.8628\n",
      "Iter-49900 train loss: 0.5075 valid loss: 0.4942, valid accuracy: 0.8628\n",
      "Iter-50000 train loss: 0.4859 valid loss: 0.4938, valid accuracy: 0.8632\n",
      "Iter-50100 train loss: 0.5481 valid loss: 0.4934, valid accuracy: 0.8634\n",
      "Iter-50200 train loss: 0.5524 valid loss: 0.4930, valid accuracy: 0.8638\n",
      "Iter-50300 train loss: 0.5632 valid loss: 0.4926, valid accuracy: 0.8642\n",
      "Iter-50400 train loss: 0.6568 valid loss: 0.4923, valid accuracy: 0.8644\n",
      "Iter-50500 train loss: 0.5558 valid loss: 0.4919, valid accuracy: 0.8648\n",
      "Iter-50600 train loss: 0.5250 valid loss: 0.4915, valid accuracy: 0.8652\n",
      "Iter-50700 train loss: 0.4674 valid loss: 0.4911, valid accuracy: 0.8650\n",
      "Iter-50800 train loss: 0.6441 valid loss: 0.4907, valid accuracy: 0.8656\n",
      "Iter-50900 train loss: 0.6082 valid loss: 0.4903, valid accuracy: 0.8658\n",
      "Iter-51000 train loss: 0.2683 valid loss: 0.4899, valid accuracy: 0.8660\n",
      "Iter-51100 train loss: 0.5139 valid loss: 0.4895, valid accuracy: 0.8660\n",
      "Iter-51200 train loss: 0.4112 valid loss: 0.4891, valid accuracy: 0.8660\n",
      "Iter-51300 train loss: 0.4326 valid loss: 0.4887, valid accuracy: 0.8662\n",
      "Iter-51400 train loss: 0.2771 valid loss: 0.4883, valid accuracy: 0.8664\n",
      "Iter-51500 train loss: 0.5313 valid loss: 0.4879, valid accuracy: 0.8668\n",
      "Iter-51600 train loss: 0.3825 valid loss: 0.4876, valid accuracy: 0.8668\n",
      "Iter-51700 train loss: 0.3991 valid loss: 0.4872, valid accuracy: 0.8668\n",
      "Iter-51800 train loss: 0.5602 valid loss: 0.4869, valid accuracy: 0.8668\n",
      "Iter-51900 train loss: 0.5869 valid loss: 0.4865, valid accuracy: 0.8668\n",
      "Iter-52000 train loss: 0.3490 valid loss: 0.4861, valid accuracy: 0.8668\n",
      "Iter-52100 train loss: 0.5328 valid loss: 0.4857, valid accuracy: 0.8668\n",
      "Iter-52200 train loss: 0.4823 valid loss: 0.4853, valid accuracy: 0.8668\n",
      "Iter-52300 train loss: 0.5226 valid loss: 0.4849, valid accuracy: 0.8668\n",
      "Iter-52400 train loss: 0.5266 valid loss: 0.4846, valid accuracy: 0.8670\n",
      "Iter-52500 train loss: 0.3033 valid loss: 0.4842, valid accuracy: 0.8672\n",
      "Iter-52600 train loss: 0.4316 valid loss: 0.4838, valid accuracy: 0.8674\n",
      "Iter-52700 train loss: 0.5324 valid loss: 0.4835, valid accuracy: 0.8676\n",
      "Iter-52800 train loss: 0.3944 valid loss: 0.4831, valid accuracy: 0.8676\n",
      "Iter-52900 train loss: 0.5273 valid loss: 0.4827, valid accuracy: 0.8674\n",
      "Iter-53000 train loss: 0.5774 valid loss: 0.4824, valid accuracy: 0.8674\n",
      "Iter-53100 train loss: 0.3348 valid loss: 0.4820, valid accuracy: 0.8680\n",
      "Iter-53200 train loss: 0.5502 valid loss: 0.4816, valid accuracy: 0.8680\n",
      "Iter-53300 train loss: 0.5532 valid loss: 0.4813, valid accuracy: 0.8680\n",
      "Iter-53400 train loss: 0.5413 valid loss: 0.4809, valid accuracy: 0.8680\n",
      "Iter-53500 train loss: 0.4183 valid loss: 0.4806, valid accuracy: 0.8678\n",
      "Iter-53600 train loss: 0.5795 valid loss: 0.4803, valid accuracy: 0.8680\n",
      "Iter-53700 train loss: 0.4589 valid loss: 0.4799, valid accuracy: 0.8682\n",
      "Iter-53800 train loss: 0.5896 valid loss: 0.4796, valid accuracy: 0.8680\n",
      "Iter-53900 train loss: 0.3015 valid loss: 0.4792, valid accuracy: 0.8682\n",
      "Iter-54000 train loss: 0.6090 valid loss: 0.4789, valid accuracy: 0.8684\n",
      "Iter-54100 train loss: 0.5101 valid loss: 0.4786, valid accuracy: 0.8684\n",
      "Iter-54200 train loss: 0.3133 valid loss: 0.4782, valid accuracy: 0.8686\n",
      "Iter-54300 train loss: 0.4536 valid loss: 0.4779, valid accuracy: 0.8686\n",
      "Iter-54400 train loss: 0.4128 valid loss: 0.4776, valid accuracy: 0.8686\n",
      "Iter-54500 train loss: 0.8428 valid loss: 0.4772, valid accuracy: 0.8684\n",
      "Iter-54600 train loss: 0.5038 valid loss: 0.4769, valid accuracy: 0.8684\n",
      "Iter-54700 train loss: 0.4658 valid loss: 0.4765, valid accuracy: 0.8684\n",
      "Iter-54800 train loss: 0.5847 valid loss: 0.4763, valid accuracy: 0.8688\n",
      "Iter-54900 train loss: 0.5731 valid loss: 0.4759, valid accuracy: 0.8688\n",
      "Iter-55000 train loss: 0.4615 valid loss: 0.4756, valid accuracy: 0.8686\n",
      "Iter-55100 train loss: 0.3883 valid loss: 0.4753, valid accuracy: 0.8688\n",
      "Iter-55200 train loss: 0.5562 valid loss: 0.4750, valid accuracy: 0.8688\n",
      "Iter-55300 train loss: 0.5488 valid loss: 0.4747, valid accuracy: 0.8688\n",
      "Iter-55400 train loss: 0.4515 valid loss: 0.4743, valid accuracy: 0.8688\n",
      "Iter-55500 train loss: 0.4497 valid loss: 0.4740, valid accuracy: 0.8690\n",
      "Iter-55600 train loss: 0.5279 valid loss: 0.4736, valid accuracy: 0.8690\n",
      "Iter-55700 train loss: 0.4030 valid loss: 0.4733, valid accuracy: 0.8690\n",
      "Iter-55800 train loss: 0.4726 valid loss: 0.4730, valid accuracy: 0.8694\n",
      "Iter-55900 train loss: 0.6900 valid loss: 0.4727, valid accuracy: 0.8694\n",
      "Iter-56000 train loss: 0.6641 valid loss: 0.4723, valid accuracy: 0.8696\n",
      "Iter-56100 train loss: 0.6257 valid loss: 0.4720, valid accuracy: 0.8696\n",
      "Iter-56200 train loss: 0.3704 valid loss: 0.4718, valid accuracy: 0.8694\n",
      "Iter-56300 train loss: 0.5166 valid loss: 0.4715, valid accuracy: 0.8696\n",
      "Iter-56400 train loss: 0.4458 valid loss: 0.4712, valid accuracy: 0.8698\n",
      "Iter-56500 train loss: 0.4757 valid loss: 0.4708, valid accuracy: 0.8700\n",
      "Iter-56600 train loss: 0.4452 valid loss: 0.4706, valid accuracy: 0.8702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 0.5324 valid loss: 0.4702, valid accuracy: 0.8704\n",
      "Iter-56800 train loss: 0.4215 valid loss: 0.4699, valid accuracy: 0.8704\n",
      "Iter-56900 train loss: 0.6112 valid loss: 0.4696, valid accuracy: 0.8704\n",
      "Iter-57000 train loss: 0.3070 valid loss: 0.4693, valid accuracy: 0.8704\n",
      "Iter-57100 train loss: 0.7164 valid loss: 0.4690, valid accuracy: 0.8704\n",
      "Iter-57200 train loss: 0.4822 valid loss: 0.4687, valid accuracy: 0.8706\n",
      "Iter-57300 train loss: 0.8565 valid loss: 0.4684, valid accuracy: 0.8704\n",
      "Iter-57400 train loss: 0.3759 valid loss: 0.4681, valid accuracy: 0.8710\n",
      "Iter-57500 train loss: 0.2882 valid loss: 0.4679, valid accuracy: 0.8710\n",
      "Iter-57600 train loss: 0.5062 valid loss: 0.4676, valid accuracy: 0.8710\n",
      "Iter-57700 train loss: 0.7726 valid loss: 0.4672, valid accuracy: 0.8712\n",
      "Iter-57800 train loss: 0.7252 valid loss: 0.4669, valid accuracy: 0.8710\n",
      "Iter-57900 train loss: 0.6628 valid loss: 0.4666, valid accuracy: 0.8712\n",
      "Iter-58000 train loss: 0.5335 valid loss: 0.4663, valid accuracy: 0.8714\n",
      "Iter-58100 train loss: 0.3113 valid loss: 0.4659, valid accuracy: 0.8716\n",
      "Iter-58200 train loss: 0.2689 valid loss: 0.4657, valid accuracy: 0.8716\n",
      "Iter-58300 train loss: 0.4901 valid loss: 0.4654, valid accuracy: 0.8714\n",
      "Iter-58400 train loss: 0.7375 valid loss: 0.4651, valid accuracy: 0.8714\n",
      "Iter-58500 train loss: 0.2688 valid loss: 0.4648, valid accuracy: 0.8712\n",
      "Iter-58600 train loss: 0.5739 valid loss: 0.4645, valid accuracy: 0.8712\n",
      "Iter-58700 train loss: 0.7529 valid loss: 0.4643, valid accuracy: 0.8712\n",
      "Iter-58800 train loss: 0.2311 valid loss: 0.4640, valid accuracy: 0.8716\n",
      "Iter-58900 train loss: 0.3982 valid loss: 0.4637, valid accuracy: 0.8716\n",
      "Iter-59000 train loss: 0.4262 valid loss: 0.4634, valid accuracy: 0.8718\n",
      "Iter-59100 train loss: 0.4664 valid loss: 0.4631, valid accuracy: 0.8718\n",
      "Iter-59200 train loss: 0.4190 valid loss: 0.4629, valid accuracy: 0.8724\n",
      "Iter-59300 train loss: 0.4711 valid loss: 0.4625, valid accuracy: 0.8724\n",
      "Iter-59400 train loss: 0.3622 valid loss: 0.4622, valid accuracy: 0.8718\n",
      "Iter-59500 train loss: 0.3412 valid loss: 0.4620, valid accuracy: 0.8720\n",
      "Iter-59600 train loss: 0.7328 valid loss: 0.4617, valid accuracy: 0.8720\n",
      "Iter-59700 train loss: 0.4394 valid loss: 0.4615, valid accuracy: 0.8720\n",
      "Iter-59800 train loss: 0.2817 valid loss: 0.4612, valid accuracy: 0.8720\n",
      "Iter-59900 train loss: 0.2982 valid loss: 0.4609, valid accuracy: 0.8720\n",
      "Iter-60000 train loss: 0.5910 valid loss: 0.4606, valid accuracy: 0.8720\n",
      "Iter-60100 train loss: 0.3868 valid loss: 0.4603, valid accuracy: 0.8724\n",
      "Iter-60200 train loss: 0.5049 valid loss: 0.4601, valid accuracy: 0.8728\n",
      "Iter-60300 train loss: 0.4347 valid loss: 0.4598, valid accuracy: 0.8732\n",
      "Iter-60400 train loss: 0.5738 valid loss: 0.4596, valid accuracy: 0.8736\n",
      "Iter-60500 train loss: 0.3759 valid loss: 0.4593, valid accuracy: 0.8736\n",
      "Iter-60600 train loss: 0.7121 valid loss: 0.4591, valid accuracy: 0.8736\n",
      "Iter-60700 train loss: 0.3693 valid loss: 0.4589, valid accuracy: 0.8736\n",
      "Iter-60800 train loss: 0.4779 valid loss: 0.4586, valid accuracy: 0.8738\n",
      "Iter-60900 train loss: 0.4195 valid loss: 0.4583, valid accuracy: 0.8740\n",
      "Iter-61000 train loss: 0.3780 valid loss: 0.4580, valid accuracy: 0.8740\n",
      "Iter-61100 train loss: 0.4384 valid loss: 0.4578, valid accuracy: 0.8740\n",
      "Iter-61200 train loss: 0.3183 valid loss: 0.4575, valid accuracy: 0.8740\n",
      "Iter-61300 train loss: 0.6311 valid loss: 0.4572, valid accuracy: 0.8740\n",
      "Iter-61400 train loss: 0.7334 valid loss: 0.4570, valid accuracy: 0.8742\n",
      "Iter-61500 train loss: 0.5510 valid loss: 0.4567, valid accuracy: 0.8742\n",
      "Iter-61600 train loss: 0.4985 valid loss: 0.4564, valid accuracy: 0.8742\n",
      "Iter-61700 train loss: 0.3686 valid loss: 0.4562, valid accuracy: 0.8744\n",
      "Iter-61800 train loss: 0.5507 valid loss: 0.4560, valid accuracy: 0.8744\n",
      "Iter-61900 train loss: 0.4945 valid loss: 0.4557, valid accuracy: 0.8746\n",
      "Iter-62000 train loss: 0.6434 valid loss: 0.4554, valid accuracy: 0.8748\n",
      "Iter-62100 train loss: 0.2766 valid loss: 0.4552, valid accuracy: 0.8746\n",
      "Iter-62200 train loss: 0.5480 valid loss: 0.4549, valid accuracy: 0.8746\n",
      "Iter-62300 train loss: 0.4856 valid loss: 0.4547, valid accuracy: 0.8744\n",
      "Iter-62400 train loss: 0.4618 valid loss: 0.4544, valid accuracy: 0.8746\n",
      "Iter-62500 train loss: 0.3821 valid loss: 0.4541, valid accuracy: 0.8746\n",
      "Iter-62600 train loss: 0.3476 valid loss: 0.4539, valid accuracy: 0.8744\n",
      "Iter-62700 train loss: 0.3470 valid loss: 0.4537, valid accuracy: 0.8742\n",
      "Iter-62800 train loss: 0.7700 valid loss: 0.4534, valid accuracy: 0.8744\n",
      "Iter-62900 train loss: 0.5188 valid loss: 0.4531, valid accuracy: 0.8746\n",
      "Iter-63000 train loss: 0.3935 valid loss: 0.4528, valid accuracy: 0.8746\n",
      "Iter-63100 train loss: 0.3651 valid loss: 0.4525, valid accuracy: 0.8746\n",
      "Iter-63200 train loss: 0.4351 valid loss: 0.4523, valid accuracy: 0.8748\n",
      "Iter-63300 train loss: 0.4800 valid loss: 0.4521, valid accuracy: 0.8746\n",
      "Iter-63400 train loss: 0.5658 valid loss: 0.4518, valid accuracy: 0.8746\n",
      "Iter-63500 train loss: 0.7153 valid loss: 0.4515, valid accuracy: 0.8748\n",
      "Iter-63600 train loss: 0.3109 valid loss: 0.4512, valid accuracy: 0.8750\n",
      "Iter-63700 train loss: 0.4100 valid loss: 0.4510, valid accuracy: 0.8750\n",
      "Iter-63800 train loss: 0.7147 valid loss: 0.4508, valid accuracy: 0.8750\n",
      "Iter-63900 train loss: 0.4886 valid loss: 0.4505, valid accuracy: 0.8748\n",
      "Iter-64000 train loss: 0.3700 valid loss: 0.4502, valid accuracy: 0.8750\n",
      "Iter-64100 train loss: 0.3245 valid loss: 0.4500, valid accuracy: 0.8752\n",
      "Iter-64200 train loss: 0.4572 valid loss: 0.4497, valid accuracy: 0.8752\n",
      "Iter-64300 train loss: 0.5973 valid loss: 0.4495, valid accuracy: 0.8752\n",
      "Iter-64400 train loss: 0.3394 valid loss: 0.4492, valid accuracy: 0.8754\n",
      "Iter-64500 train loss: 0.6054 valid loss: 0.4490, valid accuracy: 0.8752\n",
      "Iter-64600 train loss: 0.3603 valid loss: 0.4487, valid accuracy: 0.8754\n",
      "Iter-64700 train loss: 0.5597 valid loss: 0.4485, valid accuracy: 0.8752\n",
      "Iter-64800 train loss: 0.5079 valid loss: 0.4483, valid accuracy: 0.8752\n",
      "Iter-64900 train loss: 0.5422 valid loss: 0.4480, valid accuracy: 0.8752\n",
      "Iter-65000 train loss: 0.4241 valid loss: 0.4477, valid accuracy: 0.8750\n",
      "Iter-65100 train loss: 0.5570 valid loss: 0.4475, valid accuracy: 0.8754\n",
      "Iter-65200 train loss: 0.4078 valid loss: 0.4472, valid accuracy: 0.8756\n",
      "Iter-65300 train loss: 0.5446 valid loss: 0.4470, valid accuracy: 0.8756\n",
      "Iter-65400 train loss: 0.4459 valid loss: 0.4467, valid accuracy: 0.8756\n",
      "Iter-65500 train loss: 0.5881 valid loss: 0.4465, valid accuracy: 0.8758\n",
      "Iter-65600 train loss: 0.4518 valid loss: 0.4463, valid accuracy: 0.8758\n",
      "Iter-65700 train loss: 0.3522 valid loss: 0.4460, valid accuracy: 0.8758\n",
      "Iter-65800 train loss: 0.6786 valid loss: 0.4458, valid accuracy: 0.8758\n",
      "Iter-65900 train loss: 0.6014 valid loss: 0.4456, valid accuracy: 0.8758\n",
      "Iter-66000 train loss: 0.6273 valid loss: 0.4453, valid accuracy: 0.8760\n",
      "Iter-66100 train loss: 0.4263 valid loss: 0.4451, valid accuracy: 0.8760\n",
      "Iter-66200 train loss: 0.6102 valid loss: 0.4448, valid accuracy: 0.8762\n",
      "Iter-66300 train loss: 0.3915 valid loss: 0.4446, valid accuracy: 0.8760\n",
      "Iter-66400 train loss: 0.4336 valid loss: 0.4444, valid accuracy: 0.8762\n",
      "Iter-66500 train loss: 0.5648 valid loss: 0.4442, valid accuracy: 0.8762\n",
      "Iter-66600 train loss: 0.3901 valid loss: 0.4440, valid accuracy: 0.8762\n",
      "Iter-66700 train loss: 0.3120 valid loss: 0.4437, valid accuracy: 0.8762\n",
      "Iter-66800 train loss: 0.3678 valid loss: 0.4435, valid accuracy: 0.8762\n",
      "Iter-66900 train loss: 0.4347 valid loss: 0.4433, valid accuracy: 0.8762\n",
      "Iter-67000 train loss: 0.6791 valid loss: 0.4431, valid accuracy: 0.8762\n",
      "Iter-67100 train loss: 0.4856 valid loss: 0.4429, valid accuracy: 0.8760\n",
      "Iter-67200 train loss: 0.4697 valid loss: 0.4426, valid accuracy: 0.8760\n",
      "Iter-67300 train loss: 0.4078 valid loss: 0.4424, valid accuracy: 0.8760\n",
      "Iter-67400 train loss: 0.3335 valid loss: 0.4422, valid accuracy: 0.8760\n",
      "Iter-67500 train loss: 0.5129 valid loss: 0.4420, valid accuracy: 0.8760\n",
      "Iter-67600 train loss: 0.3563 valid loss: 0.4418, valid accuracy: 0.8760\n",
      "Iter-67700 train loss: 0.4157 valid loss: 0.4416, valid accuracy: 0.8760\n",
      "Iter-67800 train loss: 0.4431 valid loss: 0.4413, valid accuracy: 0.8760\n",
      "Iter-67900 train loss: 0.5303 valid loss: 0.4411, valid accuracy: 0.8762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 0.3338 valid loss: 0.4409, valid accuracy: 0.8760\n",
      "Iter-68100 train loss: 0.5761 valid loss: 0.4407, valid accuracy: 0.8758\n",
      "Iter-68200 train loss: 0.7876 valid loss: 0.4404, valid accuracy: 0.8758\n",
      "Iter-68300 train loss: 0.6711 valid loss: 0.4402, valid accuracy: 0.8758\n",
      "Iter-68400 train loss: 0.5980 valid loss: 0.4400, valid accuracy: 0.8762\n",
      "Iter-68500 train loss: 0.5833 valid loss: 0.4398, valid accuracy: 0.8764\n",
      "Iter-68600 train loss: 0.3276 valid loss: 0.4396, valid accuracy: 0.8760\n",
      "Iter-68700 train loss: 0.4409 valid loss: 0.4394, valid accuracy: 0.8762\n",
      "Iter-68800 train loss: 0.4189 valid loss: 0.4392, valid accuracy: 0.8762\n",
      "Iter-68900 train loss: 0.3966 valid loss: 0.4390, valid accuracy: 0.8760\n",
      "Iter-69000 train loss: 0.2476 valid loss: 0.4388, valid accuracy: 0.8760\n",
      "Iter-69100 train loss: 0.3947 valid loss: 0.4386, valid accuracy: 0.8760\n",
      "Iter-69200 train loss: 0.5146 valid loss: 0.4383, valid accuracy: 0.8764\n",
      "Iter-69300 train loss: 0.2703 valid loss: 0.4381, valid accuracy: 0.8762\n",
      "Iter-69400 train loss: 0.4611 valid loss: 0.4379, valid accuracy: 0.8764\n",
      "Iter-69500 train loss: 0.7459 valid loss: 0.4377, valid accuracy: 0.8764\n",
      "Iter-69600 train loss: 0.8429 valid loss: 0.4375, valid accuracy: 0.8762\n",
      "Iter-69700 train loss: 0.4576 valid loss: 0.4373, valid accuracy: 0.8764\n",
      "Iter-69800 train loss: 0.7193 valid loss: 0.4371, valid accuracy: 0.8764\n",
      "Iter-69900 train loss: 0.6035 valid loss: 0.4369, valid accuracy: 0.8762\n",
      "Iter-70000 train loss: 0.4556 valid loss: 0.4367, valid accuracy: 0.8760\n",
      "Iter-70100 train loss: 0.4738 valid loss: 0.4365, valid accuracy: 0.8760\n",
      "Iter-70200 train loss: 0.4411 valid loss: 0.4364, valid accuracy: 0.8760\n",
      "Iter-70300 train loss: 0.4273 valid loss: 0.4362, valid accuracy: 0.8764\n",
      "Iter-70400 train loss: 0.4000 valid loss: 0.4360, valid accuracy: 0.8760\n",
      "Iter-70500 train loss: 0.5368 valid loss: 0.4358, valid accuracy: 0.8756\n",
      "Iter-70600 train loss: 0.3207 valid loss: 0.4356, valid accuracy: 0.8758\n",
      "Iter-70700 train loss: 0.4514 valid loss: 0.4354, valid accuracy: 0.8754\n",
      "Iter-70800 train loss: 0.4685 valid loss: 0.4352, valid accuracy: 0.8756\n",
      "Iter-70900 train loss: 0.5732 valid loss: 0.4350, valid accuracy: 0.8756\n",
      "Iter-71000 train loss: 0.2358 valid loss: 0.4348, valid accuracy: 0.8758\n",
      "Iter-71100 train loss: 0.4407 valid loss: 0.4345, valid accuracy: 0.8762\n",
      "Iter-71200 train loss: 0.5507 valid loss: 0.4343, valid accuracy: 0.8760\n",
      "Iter-71300 train loss: 0.2182 valid loss: 0.4341, valid accuracy: 0.8764\n",
      "Iter-71400 train loss: 0.4189 valid loss: 0.4339, valid accuracy: 0.8762\n",
      "Iter-71500 train loss: 0.3312 valid loss: 0.4337, valid accuracy: 0.8766\n",
      "Iter-71600 train loss: 0.5022 valid loss: 0.4335, valid accuracy: 0.8768\n",
      "Iter-71700 train loss: 0.2225 valid loss: 0.4333, valid accuracy: 0.8768\n",
      "Iter-71800 train loss: 0.2807 valid loss: 0.4331, valid accuracy: 0.8768\n",
      "Iter-71900 train loss: 0.3433 valid loss: 0.4329, valid accuracy: 0.8770\n",
      "Iter-72000 train loss: 0.3921 valid loss: 0.4327, valid accuracy: 0.8770\n",
      "Iter-72100 train loss: 0.4341 valid loss: 0.4325, valid accuracy: 0.8768\n",
      "Iter-72200 train loss: 0.3890 valid loss: 0.4323, valid accuracy: 0.8770\n",
      "Iter-72300 train loss: 0.4256 valid loss: 0.4321, valid accuracy: 0.8770\n",
      "Iter-72400 train loss: 0.3547 valid loss: 0.4319, valid accuracy: 0.8772\n",
      "Iter-72500 train loss: 0.2461 valid loss: 0.4317, valid accuracy: 0.8772\n",
      "Iter-72600 train loss: 0.4759 valid loss: 0.4315, valid accuracy: 0.8766\n",
      "Iter-72700 train loss: 0.3801 valid loss: 0.4313, valid accuracy: 0.8770\n",
      "Iter-72800 train loss: 0.4830 valid loss: 0.4311, valid accuracy: 0.8770\n",
      "Iter-72900 train loss: 0.4396 valid loss: 0.4309, valid accuracy: 0.8774\n",
      "Iter-73000 train loss: 0.5988 valid loss: 0.4308, valid accuracy: 0.8774\n",
      "Iter-73100 train loss: 0.3638 valid loss: 0.4306, valid accuracy: 0.8774\n",
      "Iter-73200 train loss: 0.7467 valid loss: 0.4304, valid accuracy: 0.8772\n",
      "Iter-73300 train loss: 0.3666 valid loss: 0.4303, valid accuracy: 0.8772\n",
      "Iter-73400 train loss: 0.4373 valid loss: 0.4301, valid accuracy: 0.8772\n",
      "Iter-73500 train loss: 0.4217 valid loss: 0.4298, valid accuracy: 0.8768\n",
      "Iter-73600 train loss: 0.4621 valid loss: 0.4296, valid accuracy: 0.8768\n",
      "Iter-73700 train loss: 0.7637 valid loss: 0.4295, valid accuracy: 0.8768\n",
      "Iter-73800 train loss: 0.4940 valid loss: 0.4292, valid accuracy: 0.8772\n",
      "Iter-73900 train loss: 0.2574 valid loss: 0.4291, valid accuracy: 0.8774\n",
      "Iter-74000 train loss: 0.4392 valid loss: 0.4289, valid accuracy: 0.8772\n",
      "Iter-74100 train loss: 0.1958 valid loss: 0.4287, valid accuracy: 0.8774\n",
      "Iter-74200 train loss: 0.4770 valid loss: 0.4285, valid accuracy: 0.8776\n",
      "Iter-74300 train loss: 0.3643 valid loss: 0.4283, valid accuracy: 0.8774\n",
      "Iter-74400 train loss: 0.4112 valid loss: 0.4281, valid accuracy: 0.8772\n",
      "Iter-74500 train loss: 0.3042 valid loss: 0.4279, valid accuracy: 0.8778\n",
      "Iter-74600 train loss: 0.5756 valid loss: 0.4277, valid accuracy: 0.8776\n",
      "Iter-74700 train loss: 0.8575 valid loss: 0.4275, valid accuracy: 0.8776\n",
      "Iter-74800 train loss: 0.4469 valid loss: 0.4274, valid accuracy: 0.8778\n",
      "Iter-74900 train loss: 0.5177 valid loss: 0.4272, valid accuracy: 0.8778\n",
      "Iter-75000 train loss: 0.7457 valid loss: 0.4270, valid accuracy: 0.8784\n",
      "Iter-75100 train loss: 0.3504 valid loss: 0.4268, valid accuracy: 0.8784\n",
      "Iter-75200 train loss: 0.4159 valid loss: 0.4266, valid accuracy: 0.8784\n",
      "Iter-75300 train loss: 0.4268 valid loss: 0.4263, valid accuracy: 0.8786\n",
      "Iter-75400 train loss: 0.4728 valid loss: 0.4262, valid accuracy: 0.8788\n",
      "Iter-75500 train loss: 0.3531 valid loss: 0.4260, valid accuracy: 0.8788\n",
      "Iter-75600 train loss: 0.5012 valid loss: 0.4258, valid accuracy: 0.8792\n",
      "Iter-75700 train loss: 0.5198 valid loss: 0.4257, valid accuracy: 0.8792\n",
      "Iter-75800 train loss: 0.4587 valid loss: 0.4255, valid accuracy: 0.8796\n",
      "Iter-75900 train loss: 0.7052 valid loss: 0.4253, valid accuracy: 0.8796\n",
      "Iter-76000 train loss: 0.5965 valid loss: 0.4252, valid accuracy: 0.8798\n",
      "Iter-76100 train loss: 0.3154 valid loss: 0.4250, valid accuracy: 0.8796\n",
      "Iter-76200 train loss: 0.3174 valid loss: 0.4248, valid accuracy: 0.8796\n",
      "Iter-76300 train loss: 0.4647 valid loss: 0.4246, valid accuracy: 0.8796\n",
      "Iter-76400 train loss: 0.4110 valid loss: 0.4244, valid accuracy: 0.8796\n",
      "Iter-76500 train loss: 0.7218 valid loss: 0.4243, valid accuracy: 0.8794\n",
      "Iter-76600 train loss: 0.3754 valid loss: 0.4241, valid accuracy: 0.8792\n",
      "Iter-76700 train loss: 0.5433 valid loss: 0.4239, valid accuracy: 0.8792\n",
      "Iter-76800 train loss: 0.3691 valid loss: 0.4237, valid accuracy: 0.8792\n",
      "Iter-76900 train loss: 0.2957 valid loss: 0.4235, valid accuracy: 0.8792\n",
      "Iter-77000 train loss: 0.3283 valid loss: 0.4234, valid accuracy: 0.8794\n",
      "Iter-77100 train loss: 0.5734 valid loss: 0.4231, valid accuracy: 0.8794\n",
      "Iter-77200 train loss: 0.3669 valid loss: 0.4230, valid accuracy: 0.8792\n",
      "Iter-77300 train loss: 0.4471 valid loss: 0.4228, valid accuracy: 0.8794\n",
      "Iter-77400 train loss: 0.6001 valid loss: 0.4227, valid accuracy: 0.8796\n",
      "Iter-77500 train loss: 0.4879 valid loss: 0.4225, valid accuracy: 0.8794\n",
      "Iter-77600 train loss: 0.3837 valid loss: 0.4223, valid accuracy: 0.8794\n",
      "Iter-77700 train loss: 0.5288 valid loss: 0.4222, valid accuracy: 0.8802\n",
      "Iter-77800 train loss: 0.3879 valid loss: 0.4220, valid accuracy: 0.8800\n",
      "Iter-77900 train loss: 0.2731 valid loss: 0.4218, valid accuracy: 0.8800\n",
      "Iter-78000 train loss: 0.5920 valid loss: 0.4216, valid accuracy: 0.8804\n",
      "Iter-78100 train loss: 0.3019 valid loss: 0.4215, valid accuracy: 0.8802\n",
      "Iter-78200 train loss: 0.4466 valid loss: 0.4213, valid accuracy: 0.8802\n",
      "Iter-78300 train loss: 0.6056 valid loss: 0.4211, valid accuracy: 0.8802\n",
      "Iter-78400 train loss: 0.5007 valid loss: 0.4209, valid accuracy: 0.8802\n",
      "Iter-78500 train loss: 0.5618 valid loss: 0.4208, valid accuracy: 0.8806\n",
      "Iter-78600 train loss: 0.1924 valid loss: 0.4206, valid accuracy: 0.8806\n",
      "Iter-78700 train loss: 0.6066 valid loss: 0.4204, valid accuracy: 0.8808\n",
      "Iter-78800 train loss: 0.3944 valid loss: 0.4203, valid accuracy: 0.8810\n",
      "Iter-78900 train loss: 0.1362 valid loss: 0.4201, valid accuracy: 0.8808\n",
      "Iter-79000 train loss: 0.4749 valid loss: 0.4199, valid accuracy: 0.8806\n",
      "Iter-79100 train loss: 0.4688 valid loss: 0.4197, valid accuracy: 0.8806\n",
      "Iter-79200 train loss: 0.5917 valid loss: 0.4195, valid accuracy: 0.8806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 0.2624 valid loss: 0.4193, valid accuracy: 0.8806\n",
      "Iter-79400 train loss: 0.3777 valid loss: 0.4192, valid accuracy: 0.8806\n",
      "Iter-79500 train loss: 0.3616 valid loss: 0.4190, valid accuracy: 0.8806\n",
      "Iter-79600 train loss: 0.4966 valid loss: 0.4188, valid accuracy: 0.8804\n",
      "Iter-79700 train loss: 0.2781 valid loss: 0.4186, valid accuracy: 0.8802\n",
      "Iter-79800 train loss: 0.2898 valid loss: 0.4185, valid accuracy: 0.8802\n",
      "Iter-79900 train loss: 0.1964 valid loss: 0.4183, valid accuracy: 0.8802\n",
      "Iter-80000 train loss: 0.6655 valid loss: 0.4182, valid accuracy: 0.8804\n",
      "Iter-80100 train loss: 0.4300 valid loss: 0.4180, valid accuracy: 0.8806\n",
      "Iter-80200 train loss: 0.3300 valid loss: 0.4178, valid accuracy: 0.8804\n",
      "Iter-80300 train loss: 0.5082 valid loss: 0.4176, valid accuracy: 0.8806\n",
      "Iter-80400 train loss: 0.6881 valid loss: 0.4175, valid accuracy: 0.8808\n",
      "Iter-80500 train loss: 0.6413 valid loss: 0.4173, valid accuracy: 0.8812\n",
      "Iter-80600 train loss: 0.5705 valid loss: 0.4171, valid accuracy: 0.8810\n",
      "Iter-80700 train loss: 0.5463 valid loss: 0.4170, valid accuracy: 0.8814\n",
      "Iter-80800 train loss: 0.5349 valid loss: 0.4168, valid accuracy: 0.8816\n",
      "Iter-80900 train loss: 0.3953 valid loss: 0.4166, valid accuracy: 0.8818\n",
      "Iter-81000 train loss: 0.3607 valid loss: 0.4165, valid accuracy: 0.8818\n",
      "Iter-81100 train loss: 0.3742 valid loss: 0.4163, valid accuracy: 0.8820\n",
      "Iter-81200 train loss: 0.8297 valid loss: 0.4161, valid accuracy: 0.8820\n",
      "Iter-81300 train loss: 0.4694 valid loss: 0.4160, valid accuracy: 0.8822\n",
      "Iter-81400 train loss: 0.3661 valid loss: 0.4158, valid accuracy: 0.8820\n",
      "Iter-81500 train loss: 0.3974 valid loss: 0.4156, valid accuracy: 0.8818\n",
      "Iter-81600 train loss: 0.3381 valid loss: 0.4155, valid accuracy: 0.8818\n",
      "Iter-81700 train loss: 0.2623 valid loss: 0.4154, valid accuracy: 0.8820\n",
      "Iter-81800 train loss: 0.3470 valid loss: 0.4152, valid accuracy: 0.8818\n",
      "Iter-81900 train loss: 0.3726 valid loss: 0.4151, valid accuracy: 0.8820\n",
      "Iter-82000 train loss: 0.3509 valid loss: 0.4149, valid accuracy: 0.8822\n",
      "Iter-82100 train loss: 0.2339 valid loss: 0.4147, valid accuracy: 0.8822\n",
      "Iter-82200 train loss: 0.3235 valid loss: 0.4146, valid accuracy: 0.8820\n",
      "Iter-82300 train loss: 0.3557 valid loss: 0.4144, valid accuracy: 0.8818\n",
      "Iter-82400 train loss: 0.5505 valid loss: 0.4143, valid accuracy: 0.8818\n",
      "Iter-82500 train loss: 0.3522 valid loss: 0.4141, valid accuracy: 0.8818\n",
      "Iter-82600 train loss: 0.3374 valid loss: 0.4140, valid accuracy: 0.8816\n",
      "Iter-82700 train loss: 0.5891 valid loss: 0.4139, valid accuracy: 0.8822\n",
      "Iter-82800 train loss: 0.5082 valid loss: 0.4138, valid accuracy: 0.8822\n",
      "Iter-82900 train loss: 0.2104 valid loss: 0.4136, valid accuracy: 0.8822\n",
      "Iter-83000 train loss: 0.5041 valid loss: 0.4135, valid accuracy: 0.8822\n",
      "Iter-83100 train loss: 0.3747 valid loss: 0.4133, valid accuracy: 0.8826\n",
      "Iter-83200 train loss: 0.3156 valid loss: 0.4132, valid accuracy: 0.8826\n",
      "Iter-83300 train loss: 0.3433 valid loss: 0.4130, valid accuracy: 0.8824\n",
      "Iter-83400 train loss: 0.2838 valid loss: 0.4128, valid accuracy: 0.8826\n",
      "Iter-83500 train loss: 0.2859 valid loss: 0.4127, valid accuracy: 0.8826\n",
      "Iter-83600 train loss: 0.3556 valid loss: 0.4125, valid accuracy: 0.8828\n",
      "Iter-83700 train loss: 0.3832 valid loss: 0.4124, valid accuracy: 0.8828\n",
      "Iter-83800 train loss: 0.4834 valid loss: 0.4123, valid accuracy: 0.8826\n",
      "Iter-83900 train loss: 0.4888 valid loss: 0.4122, valid accuracy: 0.8826\n",
      "Iter-84000 train loss: 0.3424 valid loss: 0.4120, valid accuracy: 0.8828\n",
      "Iter-84100 train loss: 0.5971 valid loss: 0.4118, valid accuracy: 0.8826\n",
      "Iter-84200 train loss: 0.5727 valid loss: 0.4117, valid accuracy: 0.8826\n",
      "Iter-84300 train loss: 0.4515 valid loss: 0.4116, valid accuracy: 0.8830\n",
      "Iter-84400 train loss: 0.3811 valid loss: 0.4114, valid accuracy: 0.8830\n",
      "Iter-84500 train loss: 0.4134 valid loss: 0.4112, valid accuracy: 0.8826\n",
      "Iter-84600 train loss: 0.4101 valid loss: 0.4110, valid accuracy: 0.8824\n",
      "Iter-84700 train loss: 0.4363 valid loss: 0.4109, valid accuracy: 0.8826\n",
      "Iter-84800 train loss: 0.2766 valid loss: 0.4108, valid accuracy: 0.8828\n",
      "Iter-84900 train loss: 0.4126 valid loss: 0.4106, valid accuracy: 0.8830\n",
      "Iter-85000 train loss: 0.4102 valid loss: 0.4105, valid accuracy: 0.8828\n",
      "Iter-85100 train loss: 0.4554 valid loss: 0.4103, valid accuracy: 0.8828\n",
      "Iter-85200 train loss: 0.3359 valid loss: 0.4102, valid accuracy: 0.8830\n",
      "Iter-85300 train loss: 0.5692 valid loss: 0.4100, valid accuracy: 0.8830\n",
      "Iter-85400 train loss: 0.2801 valid loss: 0.4099, valid accuracy: 0.8828\n",
      "Iter-85500 train loss: 0.1847 valid loss: 0.4097, valid accuracy: 0.8828\n",
      "Iter-85600 train loss: 0.3057 valid loss: 0.4096, valid accuracy: 0.8826\n",
      "Iter-85700 train loss: 0.2153 valid loss: 0.4094, valid accuracy: 0.8828\n",
      "Iter-85800 train loss: 0.6563 valid loss: 0.4093, valid accuracy: 0.8834\n",
      "Iter-85900 train loss: 0.4551 valid loss: 0.4092, valid accuracy: 0.8834\n",
      "Iter-86000 train loss: 0.2996 valid loss: 0.4090, valid accuracy: 0.8834\n",
      "Iter-86100 train loss: 0.4440 valid loss: 0.4089, valid accuracy: 0.8832\n",
      "Iter-86200 train loss: 0.6801 valid loss: 0.4087, valid accuracy: 0.8832\n",
      "Iter-86300 train loss: 0.6143 valid loss: 0.4086, valid accuracy: 0.8834\n",
      "Iter-86400 train loss: 0.5714 valid loss: 0.4084, valid accuracy: 0.8836\n",
      "Iter-86500 train loss: 0.4664 valid loss: 0.4083, valid accuracy: 0.8834\n",
      "Iter-86600 train loss: 0.4318 valid loss: 0.4082, valid accuracy: 0.8836\n",
      "Iter-86700 train loss: 0.3467 valid loss: 0.4080, valid accuracy: 0.8836\n",
      "Iter-86800 train loss: 0.3240 valid loss: 0.4079, valid accuracy: 0.8838\n",
      "Iter-86900 train loss: 0.2448 valid loss: 0.4078, valid accuracy: 0.8838\n",
      "Iter-87000 train loss: 0.2001 valid loss: 0.4076, valid accuracy: 0.8836\n",
      "Iter-87100 train loss: 0.4580 valid loss: 0.4075, valid accuracy: 0.8836\n",
      "Iter-87200 train loss: 0.5427 valid loss: 0.4073, valid accuracy: 0.8838\n",
      "Iter-87300 train loss: 0.3035 valid loss: 0.4072, valid accuracy: 0.8838\n",
      "Iter-87400 train loss: 0.4672 valid loss: 0.4071, valid accuracy: 0.8838\n",
      "Iter-87500 train loss: 0.5663 valid loss: 0.4069, valid accuracy: 0.8842\n",
      "Iter-87600 train loss: 0.6028 valid loss: 0.4068, valid accuracy: 0.8840\n",
      "Iter-87700 train loss: 0.4239 valid loss: 0.4067, valid accuracy: 0.8842\n",
      "Iter-87800 train loss: 0.6402 valid loss: 0.4065, valid accuracy: 0.8844\n",
      "Iter-87900 train loss: 0.5488 valid loss: 0.4063, valid accuracy: 0.8844\n",
      "Iter-88000 train loss: 0.4714 valid loss: 0.4062, valid accuracy: 0.8844\n",
      "Iter-88100 train loss: 0.6289 valid loss: 0.4060, valid accuracy: 0.8848\n",
      "Iter-88200 train loss: 0.3455 valid loss: 0.4059, valid accuracy: 0.8848\n",
      "Iter-88300 train loss: 0.5210 valid loss: 0.4058, valid accuracy: 0.8848\n",
      "Iter-88400 train loss: 0.4982 valid loss: 0.4056, valid accuracy: 0.8848\n",
      "Iter-88500 train loss: 0.3232 valid loss: 0.4055, valid accuracy: 0.8850\n",
      "Iter-88600 train loss: 0.4098 valid loss: 0.4054, valid accuracy: 0.8846\n",
      "Iter-88700 train loss: 0.5132 valid loss: 0.4052, valid accuracy: 0.8846\n",
      "Iter-88800 train loss: 0.4347 valid loss: 0.4051, valid accuracy: 0.8846\n",
      "Iter-88900 train loss: 0.2465 valid loss: 0.4049, valid accuracy: 0.8850\n",
      "Iter-89000 train loss: 0.3105 valid loss: 0.4048, valid accuracy: 0.8848\n",
      "Iter-89100 train loss: 0.7008 valid loss: 0.4047, valid accuracy: 0.8854\n",
      "Iter-89200 train loss: 0.4743 valid loss: 0.4046, valid accuracy: 0.8850\n",
      "Iter-89300 train loss: 0.3899 valid loss: 0.4044, valid accuracy: 0.8854\n",
      "Iter-89400 train loss: 0.2853 valid loss: 0.4043, valid accuracy: 0.8852\n",
      "Iter-89500 train loss: 0.3270 valid loss: 0.4042, valid accuracy: 0.8852\n",
      "Iter-89600 train loss: 0.4040 valid loss: 0.4040, valid accuracy: 0.8854\n",
      "Iter-89700 train loss: 0.3464 valid loss: 0.4039, valid accuracy: 0.8854\n",
      "Iter-89800 train loss: 0.3214 valid loss: 0.4037, valid accuracy: 0.8852\n",
      "Iter-89900 train loss: 0.3688 valid loss: 0.4036, valid accuracy: 0.8852\n",
      "Iter-90000 train loss: 0.4022 valid loss: 0.4035, valid accuracy: 0.8852\n",
      "Iter-90100 train loss: 0.4644 valid loss: 0.4033, valid accuracy: 0.8852\n",
      "Iter-90200 train loss: 0.3366 valid loss: 0.4032, valid accuracy: 0.8852\n",
      "Iter-90300 train loss: 0.4180 valid loss: 0.4031, valid accuracy: 0.8852\n",
      "Iter-90400 train loss: 0.3005 valid loss: 0.4030, valid accuracy: 0.8852\n",
      "Iter-90500 train loss: 0.6202 valid loss: 0.4028, valid accuracy: 0.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 0.3013 valid loss: 0.4027, valid accuracy: 0.8856\n",
      "Iter-90700 train loss: 0.3740 valid loss: 0.4026, valid accuracy: 0.8852\n",
      "Iter-90800 train loss: 0.2395 valid loss: 0.4024, valid accuracy: 0.8854\n",
      "Iter-90900 train loss: 0.3951 valid loss: 0.4023, valid accuracy: 0.8854\n",
      "Iter-91000 train loss: 0.4658 valid loss: 0.4022, valid accuracy: 0.8854\n",
      "Iter-91100 train loss: 0.4007 valid loss: 0.4020, valid accuracy: 0.8854\n",
      "Iter-91200 train loss: 0.4086 valid loss: 0.4019, valid accuracy: 0.8854\n",
      "Iter-91300 train loss: 0.4149 valid loss: 0.4018, valid accuracy: 0.8854\n",
      "Iter-91400 train loss: 0.3748 valid loss: 0.4016, valid accuracy: 0.8854\n",
      "Iter-91500 train loss: 0.3002 valid loss: 0.4015, valid accuracy: 0.8856\n",
      "Iter-91600 train loss: 0.5214 valid loss: 0.4014, valid accuracy: 0.8854\n",
      "Iter-91700 train loss: 0.4870 valid loss: 0.4013, valid accuracy: 0.8854\n",
      "Iter-91800 train loss: 0.3177 valid loss: 0.4011, valid accuracy: 0.8854\n",
      "Iter-91900 train loss: 0.3189 valid loss: 0.4010, valid accuracy: 0.8854\n",
      "Iter-92000 train loss: 0.5470 valid loss: 0.4009, valid accuracy: 0.8854\n",
      "Iter-92100 train loss: 0.3522 valid loss: 0.4007, valid accuracy: 0.8856\n",
      "Iter-92200 train loss: 0.5580 valid loss: 0.4006, valid accuracy: 0.8856\n",
      "Iter-92300 train loss: 0.3615 valid loss: 0.4005, valid accuracy: 0.8856\n",
      "Iter-92400 train loss: 0.4086 valid loss: 0.4003, valid accuracy: 0.8856\n",
      "Iter-92500 train loss: 0.4740 valid loss: 0.4002, valid accuracy: 0.8856\n",
      "Iter-92600 train loss: 0.4321 valid loss: 0.4001, valid accuracy: 0.8856\n",
      "Iter-92700 train loss: 0.4816 valid loss: 0.4000, valid accuracy: 0.8858\n",
      "Iter-92800 train loss: 0.4653 valid loss: 0.3999, valid accuracy: 0.8860\n",
      "Iter-92900 train loss: 0.3738 valid loss: 0.3998, valid accuracy: 0.8862\n",
      "Iter-93000 train loss: 0.6992 valid loss: 0.3997, valid accuracy: 0.8860\n",
      "Iter-93100 train loss: 0.6263 valid loss: 0.3996, valid accuracy: 0.8864\n",
      "Iter-93200 train loss: 0.3789 valid loss: 0.3995, valid accuracy: 0.8864\n",
      "Iter-93300 train loss: 0.4715 valid loss: 0.3994, valid accuracy: 0.8862\n",
      "Iter-93400 train loss: 0.4532 valid loss: 0.3992, valid accuracy: 0.8860\n",
      "Iter-93500 train loss: 0.4871 valid loss: 0.3991, valid accuracy: 0.8866\n",
      "Iter-93600 train loss: 0.4049 valid loss: 0.3989, valid accuracy: 0.8862\n",
      "Iter-93700 train loss: 0.3630 valid loss: 0.3988, valid accuracy: 0.8866\n",
      "Iter-93800 train loss: 0.6983 valid loss: 0.3987, valid accuracy: 0.8866\n",
      "Iter-93900 train loss: 0.5060 valid loss: 0.3986, valid accuracy: 0.8866\n",
      "Iter-94000 train loss: 0.7357 valid loss: 0.3984, valid accuracy: 0.8862\n",
      "Iter-94100 train loss: 0.6756 valid loss: 0.3982, valid accuracy: 0.8864\n",
      "Iter-94200 train loss: 0.6455 valid loss: 0.3981, valid accuracy: 0.8866\n",
      "Iter-94300 train loss: 0.9442 valid loss: 0.3980, valid accuracy: 0.8868\n",
      "Iter-94400 train loss: 0.4385 valid loss: 0.3979, valid accuracy: 0.8866\n",
      "Iter-94500 train loss: 0.1812 valid loss: 0.3977, valid accuracy: 0.8866\n",
      "Iter-94600 train loss: 0.3828 valid loss: 0.3976, valid accuracy: 0.8864\n",
      "Iter-94700 train loss: 0.3907 valid loss: 0.3975, valid accuracy: 0.8866\n",
      "Iter-94800 train loss: 0.5189 valid loss: 0.3974, valid accuracy: 0.8868\n",
      "Iter-94900 train loss: 0.5427 valid loss: 0.3972, valid accuracy: 0.8868\n",
      "Iter-95000 train loss: 0.3687 valid loss: 0.3971, valid accuracy: 0.8868\n",
      "Iter-95100 train loss: 0.6426 valid loss: 0.3970, valid accuracy: 0.8872\n",
      "Iter-95200 train loss: 0.5140 valid loss: 0.3969, valid accuracy: 0.8872\n",
      "Iter-95300 train loss: 0.4494 valid loss: 0.3967, valid accuracy: 0.8870\n",
      "Iter-95400 train loss: 0.2855 valid loss: 0.3965, valid accuracy: 0.8872\n",
      "Iter-95500 train loss: 0.3093 valid loss: 0.3964, valid accuracy: 0.8868\n",
      "Iter-95600 train loss: 0.2403 valid loss: 0.3963, valid accuracy: 0.8870\n",
      "Iter-95700 train loss: 0.6537 valid loss: 0.3962, valid accuracy: 0.8868\n",
      "Iter-95800 train loss: 0.5148 valid loss: 0.3961, valid accuracy: 0.8870\n",
      "Iter-95900 train loss: 0.5817 valid loss: 0.3960, valid accuracy: 0.8870\n",
      "Iter-96000 train loss: 0.4537 valid loss: 0.3958, valid accuracy: 0.8870\n",
      "Iter-96100 train loss: 0.4170 valid loss: 0.3957, valid accuracy: 0.8874\n",
      "Iter-96200 train loss: 0.3770 valid loss: 0.3956, valid accuracy: 0.8872\n",
      "Iter-96300 train loss: 0.4450 valid loss: 0.3954, valid accuracy: 0.8872\n",
      "Iter-96400 train loss: 0.3665 valid loss: 0.3953, valid accuracy: 0.8870\n",
      "Iter-96500 train loss: 0.4420 valid loss: 0.3952, valid accuracy: 0.8870\n",
      "Iter-96600 train loss: 0.3202 valid loss: 0.3950, valid accuracy: 0.8872\n",
      "Iter-96700 train loss: 0.5187 valid loss: 0.3949, valid accuracy: 0.8874\n",
      "Iter-96800 train loss: 0.4101 valid loss: 0.3947, valid accuracy: 0.8874\n",
      "Iter-96900 train loss: 0.3494 valid loss: 0.3946, valid accuracy: 0.8874\n",
      "Iter-97000 train loss: 0.4318 valid loss: 0.3945, valid accuracy: 0.8878\n",
      "Iter-97100 train loss: 0.2255 valid loss: 0.3944, valid accuracy: 0.8874\n",
      "Iter-97200 train loss: 0.6264 valid loss: 0.3943, valid accuracy: 0.8876\n",
      "Iter-97300 train loss: 0.1604 valid loss: 0.3942, valid accuracy: 0.8876\n",
      "Iter-97400 train loss: 0.3651 valid loss: 0.3940, valid accuracy: 0.8880\n",
      "Iter-97500 train loss: 0.4007 valid loss: 0.3939, valid accuracy: 0.8876\n",
      "Iter-97600 train loss: 0.4373 valid loss: 0.3938, valid accuracy: 0.8878\n",
      "Iter-97700 train loss: 0.2257 valid loss: 0.3937, valid accuracy: 0.8876\n",
      "Iter-97800 train loss: 0.5976 valid loss: 0.3935, valid accuracy: 0.8876\n",
      "Iter-97900 train loss: 0.5360 valid loss: 0.3934, valid accuracy: 0.8878\n",
      "Iter-98000 train loss: 0.4802 valid loss: 0.3933, valid accuracy: 0.8878\n",
      "Iter-98100 train loss: 0.7026 valid loss: 0.3932, valid accuracy: 0.8878\n",
      "Iter-98200 train loss: 0.6612 valid loss: 0.3931, valid accuracy: 0.8880\n",
      "Iter-98300 train loss: 0.3358 valid loss: 0.3929, valid accuracy: 0.8880\n",
      "Iter-98400 train loss: 0.5899 valid loss: 0.3928, valid accuracy: 0.8880\n",
      "Iter-98500 train loss: 0.6375 valid loss: 0.3927, valid accuracy: 0.8878\n",
      "Iter-98600 train loss: 0.2105 valid loss: 0.3926, valid accuracy: 0.8876\n",
      "Iter-98700 train loss: 0.3283 valid loss: 0.3924, valid accuracy: 0.8880\n",
      "Iter-98800 train loss: 0.5704 valid loss: 0.3923, valid accuracy: 0.8876\n",
      "Iter-98900 train loss: 0.4051 valid loss: 0.3922, valid accuracy: 0.8878\n",
      "Iter-99000 train loss: 0.3454 valid loss: 0.3921, valid accuracy: 0.8878\n",
      "Iter-99100 train loss: 0.4173 valid loss: 0.3919, valid accuracy: 0.8876\n",
      "Iter-99200 train loss: 0.3331 valid loss: 0.3918, valid accuracy: 0.8876\n",
      "Iter-99300 train loss: 0.6058 valid loss: 0.3917, valid accuracy: 0.8876\n",
      "Iter-99400 train loss: 0.4915 valid loss: 0.3916, valid accuracy: 0.8876\n",
      "Iter-99500 train loss: 0.9131 valid loss: 0.3915, valid accuracy: 0.8878\n",
      "Iter-99600 train loss: 0.4507 valid loss: 0.3914, valid accuracy: 0.8878\n",
      "Iter-99700 train loss: 0.3273 valid loss: 0.3912, valid accuracy: 0.8880\n",
      "Iter-99800 train loss: 0.5313 valid loss: 0.3911, valid accuracy: 0.8878\n",
      "Iter-99900 train loss: 0.2930 valid loss: 0.3910, valid accuracy: 0.8882\n",
      "Iter-100000 train loss: 0.4543 valid loss: 0.3909, valid accuracy: 0.8884\n",
      "Last iteration - Test accuracy mean: 0.8843, std: 0.0000, loss: 0.4034\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XeYFEX6wPFvLeySd8kZdiWDItkjeWAWVIxEFcRT0eMU\nTz1EDODpGX6mE4FTFDBwIOipIEEUBRWUqOScc4YlrYTd+v1RMzupZ6Znd+Lu+3meeaa7urq7Zhb6\nna7qqlJaa4QQQoikWBdACCFEfJCAIIQQApCAIIQQwkECghBCCEACghBCCAcJCEIIIQAbAUEpVVMp\n9YNSao1SapVS6hGLPJ2UUseVUr85Xs9EprhCCCEipaiNPBeAx7TWy5VSpYFlSqlvtdbrvfL9pLXu\nFv4iCiGEiIagdwha6/1a6+WO5VPAOqCGRVYV5rIJIYSIopDaEJRSGUBzYJHF5nZKqeVKqRlKqSZh\nKJsQQogoslNlBICjuuhzYJDjTsHdMqC21vqMUqoL8BXQIHzFFEIIEWnKzlhGSqmiwHRgltb6bRv5\ntwGttNZHvdJl4CQhhMgDrXXEq+XtVhmNA9b6CwZKqSpuy5dhAs1Rq7xaa3lpzbBhw2Jehnh5yXch\n34V8F4Ff0RK0ykgp1QG4E1illPod0MBQIB3QWusxwB1KqYeA80AW0DNyRRZCCBEJQQOC1noBUCRI\nnlHAqHAVSgghRPRJT+UY6dy5c6yLEDfku3CR78JFvovos9WoHLaTKaWjeT4hhCgIlFLoKDQq237s\nVAhRsGRkZLBjx45YF0O4SU9PZ/v27TE7v9whCFFIOX51xroYwo2/v0m07hCkDUEIIQQgAUEIIYSD\nBAQhhBCABAQhRAGXk5NDmTJl2L17d8j7btmyhaSkwnOZLDyfVAiREMqUKUNqaiqpqakUKVKEkiVL\n5qZNmjQp5OMlJSVx8uRJatasmafyKFV4RvaP+mOnWVlQokS0zyqESBQnT57MXa5Tpw5jx47liiuu\n8Js/OzubIkUCDqYgbIr6HULJktCzJ3TqFO0zCyESjdXgbs8++yy9evWiT58+pKWl8d///peFCxfS\nrl07ypUrR40aNRg0aBDZ2dmACRhJSUns3LkTgLvvvptBgwbRtWtXUlNT6dChg+3+GHv27OGmm26i\nQoUKNGzYkPHjx+duW7RoEa1atSItLY1q1arx5JNPApCVlcWdd95JxYoVKVeuHG3btuXoUcuxP2Mu\nJlVGU6bATz/Beu9JOIUQwoavvvqKu+66i8zMTHr27ElycjIjRozg6NGjLFiwgNmzZ/Pee+/l5veu\n9pk0aRL/+te/OHbsGLVq1eLZZ5+1dd6ePXtSt25d9u/fz6effsrgwYP5+eefAXj44YcZPHgwmZmZ\nbN68mTvuuAOA8ePHk5WVxd69ezl69CijR4+mePHiYfomwiumbQiNG8OIEbEsgRDCH6XC84qEjh07\n0rVrVwCKFStGq1ataNOmDUopMjIyuP/++/nxxx9z83vfZdxxxx20aNGCIkWKcOedd7J8+fKg59y2\nbRtLlizhlVdeITk5mRYtWtC/f38++eQTAFJSUti0aRNHjx6lVKlStGnTBoDk5GQOHz7Mxo0bUUrR\nsmVLSpYsGa6vIqyiHxDK7PVYHTQo6iUQQtigdXhekVCrVi2P9Q0bNnDjjTdSrVo10tLSGDZsGIcP\nH/a7f9WqVXOXS5YsyalT3pNA+tq3bx8VK1b0+HWfnp7Onj17AHMnsGbNGho2bEjbtm2ZNWsWAPfc\ncw9XX301PXr0oFatWgwdOpScnJyQPm+0RD8g3N8Gas/3SCpEjfhCiDDwrgIaMGAATZs2ZevWrWRm\nZvL888+HfViO6tWrc/jwYbKysnLTdu7cSY0aNQCoX78+kyZN4tChQzz22GPcfvvtnDt3juTkZJ57\n7jnWrl3L/Pnz+eKLL/jvf/8b1rKFS/QDwrQPoMft0Po/HslKSZuCECJvTp48SVpaGiVKlGDdunUe\n7Qf55QwsGRkZtG7dmqFDh3Lu3DmWL1/O+PHjufvuuwGYMGECR44cASA1NZWkpCSSkpKYO3cua9as\nQWtN6dKlSU5Ojtu+DdEv1eYuMPYXaP8GtHzfY1PjxlEvjRAijtntA/DGG2/w4YcfkpqaykMPPUSv\nXr38HifUfgXu+SdPnszGjRupWrUqPXr04JVXXuHyyy8HYObMmTRu3Ji0tDQGDx7MlClTKFq0KHv3\n7uW2224jLS2Npk2bcu2119KnT5+QyhAtUR/t1MzACZTfDPd2gCmfw87Lc/McOwZly0atSEIUWjLa\nafwpdKOd5nY0PFoPpo6H2+6G5NO524cPl4ZmIYSIhZjMh+Bxx3bbXXCyOnz3fx55X3sN7r8f0tKi\nVjwhChW5Q4g/sb5DiH1AKHUA/toUPp4DBy71yN+6NSxZErXiCVGoSECIP4UyIJhltw0t34eWY01j\ns/asxZJ/r0JEhgSE+BPrgBCzZ59eeMFt5fe/gMqBJp/FqjhCCFHoxXRO5exsKOocb7Xut9DlERi9\nGnJcg7DKDxghIkPuEOJPob1DAChSxG0o7C3XwJkK0Ph/HnkOHYp+uYQQojCKeXe5q65yLimYPwQ6\nvkpuXwXAMWKtEEKICIt5QPj4Y7eVTTdAkbNQZ05uUuvWcOBA9MslhEhMO3bsICkpKXcAua5du+aO\nSBosr7eLLrqIH374IWJljTcxDwjlykG1ao4VnQQLnoQOnn0S3AYmFEIUcF26dGH48OE+6VOnTqVa\ntWq2Rgp1H25i5syZueMNBctb2MU8IAA89pjbyupeUGUllN8Us/IIIWKnX79+TJgwwSd9woQJ3H33\n3XE7MFxBEBff7BNPQMOGjpXsFFjRD1qN8cgzalT0yyWEiL5bbrmFI0eOMH++a5j848ePM336dPr2\n7QuYX/0tW7YkLS2N9PR0nn/+eb/Hu+KKKxg3bhwAOTk5PPHEE1SqVIl69eoxY8YM2+U6d+4cjz76\nKDVq1KBmzZr8/e9/5/z58wAcOXKEm266iXLlylGhQgU6uc0R/Oqrr1KzZk1SU1Np3Lgxc+fODen7\niKa4CAgA69a5rSx7AJp/ZNoTHP72t+iXSQgRfcWLF6d79+587NbAOHnyZBo3bswll1wCQOnSpfnk\nk0/IzMxkxowZvPvuu0ybNi3osceMGcPMmTNZsWIFS5cu5fPPP7ddrhdffJHFixezcuVKVqxYweLF\ni3nxxRcBM9pqrVq1OHLkCAcPHuSll14CYOPGjYwaNYply5Zx4sQJZs+eTUZGRgjfRnQVDZ4lOjyq\n8Y7WM8NYNP4CVvfOTX7+eRg2LPplE6IwUs+Hp25dDwu9r0O/fv248cYbGTlyJCkpKXzyySf069cv\nd/uf//zn3OVLLrmEXr168eOPP9KtW7eAx/3ss8949NFHqV69OgBPPfWUx1SbgUycOJFRo0ZRoUIF\nAIYNG8aDDz7I888/T3JyMvv27WPbtm3UrVuXDh06AFCkSBHOnTvH6tWrqVChArVr1w7pe4g6rXXU\nXuZ0/j31lNvEe02maO7p5DMhnxAiPIL9f4y1+vXr68mTJ+stW7bolJQUffDgwdxtixYt0ldccYWu\nVKmSTktL0yVKlNB9+/bVWmu9fft2nZSUpLOzs7XWWnfu3FmPHTtWa611o0aN9MyZM3OPs2HDBo+8\n3jIyMvT333+vtda6RIkSeu3atbnb1q9fr4sVK6a11vrkyZP68ccf13Xq1NF169bVr7zySm6+SZMm\n6Y4dO+ry5cvr3r1767179/r9zP7+Jo70iF+j46bKCOBf/3Jb2XAzVFwPFTZ45MnIgDNnolosIUQM\n3H333Xz00UdMmDCB6667jkqVKuVu69OnD7fccgt79uzh+PHjDBgwwFav62rVqrFr167c9R07dtgu\nT/Xq1T3y79ixI/dOo3Tp0rz++uts2bKFadOm8eabb+a2FfTq1Yuff/45d98hQ4bYPme0xVVA8Kg2\nyk6BVX2gmefzwzt2SLWREIVB3759mTNnDh988IFHdRHAqVOnKFeuHMnJySxevJiJEyd6bPcXHHr0\n6MGIESPYs2cPx44d49VXX7Vdnt69e/Piiy9y+PBhDh8+zAsvvJD7OOuMGTPYsmULAGXKlKFo0aIk\nJSWxceNG5s6dy7lz50hJSaFEiRJx/ZRU3JVsjPvDRSvuhksnmIHv3CxcGN0yCSGiLz09nfbt23Pm\nzBmftoHRo0fz7LPPkpaWxosvvkjPnj09tvubMvP+++/nuuuuo1mzZrRu3Zrbb789YBnc933mmWdo\n3bo1l156ae7+Tz/9NACbNm3i6quvpkyZMnTo0IGBAwfSqVMnzp49y5AhQ6hUqRLVq1fn0KFDvPzy\ny3n+TiIt6OB2SqmawMdAFSAHeF9rPcIi3wigC3AauEdrvdwij7ZzW+f6G2h46FKYORJ2dPLIs2ED\nNGgQ9FBCCD9kcLv4kwiD210AHtNaXwy0AwYqpRq5Z1BKdQHqaq3rAwOAd/NTKEe1HKBg5V3QdKJP\nHsfjv0IIIcIkaEDQWu93/trXWp8C1gE1vLLdjLmLQGu9CEhTSlXJa6H693dbWdMTmvwPki7k9XBC\nCCFsCKkNQSmVATQHFnltqgHsclvfg2/QsM3R18M4ngHHLoL0n/J6OCGEEDbY7pimlCoNfA4Mctwp\n5In7oFWdO3emc+fOwXdad5vppLbtSo/kBQvA0f9DCCEKjHnz5jFv3ryon9fWjGlKqaLAdGCW1vpt\ni+3vAnO11pMd6+uBTlrrA175bDUqm7xuKxU2QL8r4a1dMueyEGEijcrxJxEalQHGAWutgoHDNKAv\ngFKqLXDcOxiEat8+t5UjDeFsGlRfkp9DCiGECCBolZFSqgNwJ7BKKfU7ZjqzoUA6pjv1GK31TKVU\nV6XUZsxjp/39H9GeqlWhcWO3Qe+c1UZ7/pTfQwshMM/5y1wA8SU9PT2m57dVZRS2k4VQZQRw8CBU\ncT6rVO03uKMnvLMRcP0jljteIURBF29VRjFRubLbyr4WUOQ8VF7tkUd+4AghRHjEdUDwpGDdrdD4\nS58tMueyEELkX9wHhJtvdltxtiN4kV7LQgiRf3EfEN58021lV3sovQ/KbfXI8/jjcOhQdMslhBAF\nTdwHhDp13IKCLmLmSWjkWW00ZYpXe4MQQoiQxX1AAGjf3m3FT7URQAjzZQshhPCSEAHBYz6JbVdC\npbWm6sjLjTdGr0xCCFHQJERAaNXKbSU7BbZcBw3kdkAIIcIpIQKCz4xzG7pBw2kxKYsQQhRUCREQ\nfGy+HjLmQdEsn03Hj0e/OEIIURAkZkDIKm96Ll8012dTuXIxKI8QQhQACRMQfC70G2+EBtNjUhYh\nhCiIEiYgHD0KS9xHv84NCDK6nRBChEPCBASA1q3dVg43Mk8cVVnlk++dd8z7N9/A8uXRKZsQQiS6\nuB7+2voYbivXD4LTVeDnoT75tDZ5mzSBNWvydUohhIgpGf7ajk03QH3r/ggTJ0a5LEIIkeASLiBU\nq+a2sr2TmR+h5GGffIMHm3eZL0EIIexJuIBwySVuK9nFYPsVUO8bn3wyk5oQQoQm4QLC1KleCRut\nq42cAUHuEIQQwp6ECwglSnglbOoK9WZD0gWP5H2Ose9OnYpOuYQQItElXEDwcbIGHM+Amr9abt6+\nPaqlEUKIhJX4AQFMtZGMfiqEEPlSMAJCgMdPhRBC2JOQAeGxx7wS9rSBUgeh7PaQjzVqlEysI4QQ\nkKABoXt388qli5jG5QZfh3ysiRNl6k0hhIAEDQht28KUKV6JG2+ChqEHBCGEEEZCBgRLW66Fmguh\n2AmfTevXw5EjMSiTEEIkkIITEM6Vhl3toe5sn02NG8M990S/SEIIkUgSOiBcc41Xwgb/1UbTp0Nm\nZuTLJIQQiSqhA8K333olbOgG9Wf69Fp2+sZ3yCMhhBAOCR0QfJyoBcfTodaCWJdECCESTsIHhNWr\nvRI23AyNvrLM26tX5MsjhBCJKuEDwsUXeyWsvQMu/gxUjmX+2bNh1izXugyTLYQQRtFYFyDsDjWB\nrHJmsLtdHXw2d+0KOTlw4QIUKRKD8gkhRJxK+DsEgHvv9UpY0wMu9u65ZuQ4bhwOH5a7AyGEcBc0\nICilxiqlDiilVvrZ3kkpdVwp9Zvj9Uz4ixnYAw94JazpEbDaCKBqVdc0m0IIIezdIYwHrguS5yet\ndUvH68UwlCt/jjSE05WDPm20dGmUyiOEEAkgaEDQWs8HjgXJFtOJKlu0sEgMUG3klJUVmfIIIUQi\nClcbQjul1HKl1AylVJMwHdO2lBSLxDXdocnnoLL97rdoEfxqPdGaEEIUOuEICMuA2lrr5sBIwLoT\nQITleDcXHK0Pp6pB+s+xKI4QQiScfD92qrU+5bY8Syk1WilVXmt91Cr/8OHDc5c7d+5M586d81sE\nAJRVpZWz2mi7vXOcOwdvvQVPPhmWIgkhRJ7MmzePefPmRf28Stt49lIplQF8rbVuarGtitb6gGP5\nMmCK1jrDz3G0nfPllU9QKLcF/tIe3twDOYFjn9awahVceqk8jiqEiC9KKbTWEW+rtfPY6UTgF6CB\nUmqnUqq/UmqAUsr5sOcdSqnVSqnfgX8DPSNY3oAGDvRKOFbXjG+U/lPQfceMkUAghCjcglYZaa37\nBNk+ChgVthLlQ7lyFonOaqNtVwbc9+mnTWc1MIHBsgpKCCEKsALRUzmgNd2h8Rd+h8R2cgYDgDlz\nIlwmIYSIQwUqIHTvbpF4/CI4dhFkzLV9nGuvDV+ZhBAiURSogHDppX4u5jY6qVn57TfYuzf/5RJC\niERQoAICQJ06Folr74DGX0LS+ZCO1aoV9IxZE7kQQkRXgQsI7dpZJGamw+GGUH+WxcbAcnJg505Y\nty7/ZRNCiHhmqx9C2E4W4X4IrvNYJLYYCw2mw+QvbR1j0yaoX98s16kDW7fKY6lCiNiIm34IBcaa\nHnDRXCh1wFZ2ZzAA04NZCCEKugIbEMqU8Uo4VwbW3QrNPgn5WLt3+6YtCDyythBCJJwCGRB+/BF+\nsuqc/PtfoMU4IG91Py+/DNu3w4ED0LFjfkoohBDxp0AGhD//GZo3t9iws4MZDrtW3sa8HjrU9GiW\ntgQhREFUIAOCfwp+uw9ajcnzESZODGNxhBAijhTogOAzRwLA8nug4VQoYTk6d1j89JOMhSSESDwF\nOiBYXpTPVIKNN0LzDyN23lWrInZoIYSImAIdEPxa8ldo/R9QVrcQ9vXrF6byCCFEHCicAWF3W/MY\nar3Qey67+/hj63Tnnck//pGvwwshRFQVzoCAgl+egA6vhe2IU6bALK/48vrr1nnXrbPu2yCEELFU\nIIeu8Dynnw1J5+GRejDlc9jbJqRjPv88DBtmlrU2jddFikD58qafQmqqK6/Vx1UKmjSBNWtCOq0Q\nopCSoSvCZP16uP9+iw05ybDwUWjv52d8AM5gYLW+bZu9Y8hwGEKIeFPgA0LDhvDcc342/nYf1Pke\nytq8ivuxdm2+ds+VlibzLwghYqfAB4SAzpUxQaHdW3k+RJLbN3j0aP7GODpxArZsyfv+QgiRH4U7\nIAAsegQunZDnjmpae7YT/PWv1vmOHPFc37zZOt+2bdKpTQgRGxIQTlaH9TdDm1ERPU3FinDyZPB8\n0rdBCBErhSIgVKgANWsGyLDgSfjTO5Bi44ptwfvXvz8XLvimffMNDByYp9MKIURYFYqAUKIE7NgR\nIMPhRrD1amj7dp6ObznUtsOJE65lqzL85z8werRvur9qoxEjzIirQggRbgW+H4KT1p4NwD7Kb4b7\n2sLI9XCmYljPvXQptG7tKofzYq813HwzTJvmme5eZm+VKsHhwzIEtxCFifRDiLaj9WB1T+j4ctgP\n7QwGQggRzwpVQJg0KUiGn541o6Cm7YxYGdyrh2bPNncHefX9936G+BZhl5UFy5bFuhRCRFahCQhK\nQa9eQTKdqgpLH4LOwyNWDvcG5OuvD5z33Xf9b5s+Ha6+GhYuDE+5RGBvvil3eqLgKzQBwbYF/4AG\n06FSmLof58NDD8H//Z/1NuevVWdbgtbwxRfRKVc8OHYMNm6M3vnOno3euYSIFQkI3s6mwYLBcOUz\nUT3t3LnW6XZ7Pm/ZArffDj/84EpbvhyOHw+9LB9/DI88Evp+0dS/vxmWJBzOn5eqNyGgkAaE++4L\nkmHxQKi+FGrPj0p5AK68MvD2nBwzUJ8355NJn39u3t2HvmjRAv7+99DL8tZb8M47oe8XTXY6+dlV\nqhQMGhTdOw4h4lGhCwinTgWumwfgQgmY/QbcOACKxMewpP/6FzRubB45dResUVpGVQ3u/HkYOTLw\nHYcMJyKsjBgBr4VvWpWYK3QBoVQpM3dBUGvvgGN1oIOfSvwocbYR+Bux9dVXw39OufgJYc8//gGD\nB8e6FOFT6AKC08iRwXIomDkK2v4bKmyIRpHC4oEHfGdus0spz57Vwli6FP75z1iXIjEdOBDrEohQ\nFNqA0KCBjUyZteGnZ+DGB4HYdA1esAB++803/bPP/O/TtatrecUKV/uCHVlZ9vPa9ccfiX3X8b//\nuZZPnYrMOZTyrQ6MpaFD4eDB/B+nalX4+ef8H0dER6ENCMEacXMtehhSTpkOazFw9Ci0auWbHmz6\nTed8zmvWQPfuoZ3z99+D59Ha84kmd/ffb+5UnE84OYPMqVOweHFoZQnF77+Ht7HZyiuvRO7Y+/ZF\n7tihevllmDEjPMfKy5NuBVW1aqbNKl4FDQhKqbFKqQNKqZUB8oxQSm1SSi1XSjUPbxEjw1Y7AoAu\nAl+PgauHQKkw/GSKAK3hyy890154IbRjLF8eev6rrrLe9sEH8P77UKUKPPwwnDlj0l9+Gf70J7Nc\nvz6sWxf8PKdP2yuPUtCyJTwT4aeFs7NDy3/unP1pVUXBt3+/6/9DPLJzhzAeuM7fRqVUF6Cu1ro+\nMAAI9gxPXOnRw0am/S1gRV+4Lg/PcEbB66/n/5f3G2+Y9+Zu4fzsWVi92ix71wXbeW7/3DnTVrPB\n0QTj/sto8+bgZX75ZShdOvh5vM8ZLl984furPdRBBV96CerUCV+ZImHlSutAJwMoBpfIVaFWggYE\nrfV84FiALDcDHzvyLgLSlFJVwlO8yFq2DCZMsJl53nCo9Ss0zMfgQxFi9ZSDv8ZhrU0/DH8ztu3f\n71r+97+haVPTBlC1qmcQyMswDrt2hZZ/6FDXfnZ7YYfzInb77fmvIjoW6H9OnGjWDKZMidzxY3nR\nPHQIVq2K3fkTTTjaEGoA7v/V9zjS4l7LlpCcbDPz+VLw5cdw0wNQOo4qe0O0YQOMHQuPP24arM+d\ng4wM67zOW1vnL/y8/sd2XqQ//dQ6PZhhw8zFGUybyoUL1pMNAbz3nmv57Fn48cfQyhoNr71mfZGK\n5C/y/fvh2mv9b//jj+iWJ1r69YNLL411Kew7eza2AbTQNip7a9rURqadHWHpg3Db3aBCrEyOA19+\naTq3genQ1rGjqbbZsSPvjweGc8gH9/mpDx2yzlOhggnigSYlcmraFDp3zn+ZvNlt1/Bn8GBz92VH\n796udpfBg+GxxwLnz8mxrjZbsgS++y60chYEiTYGVazbF4qG4Rh7gFpu6zUdaZaGDx+eu9y5c2c6\n5/d/bBgkJZm7BVu3lj89A32vhk4vmGqkBPHMM9ZDXzh/Gdq9WIwY4TnukvOJpPLlTV359u2++9j9\npXnvveauZcoUM+xGfm3alP9jeMvKMu0ae/eaJ0ZCUaZM6FVIc+a4Hkd94w1zwX/zTf/5X3oJnn02\nPL/urY5x6JAJyGXL2j9OOH7xnjgBM2faGLHYzcCB/p+EC5dw/5p3Hm/evHnMmzcvvAe3Q2sd9AVk\nAKv8bOsKzHAstwUWBjiOjlfTpjl/n9p4ld6neayGpv4M+/vE+JWUZJ3+7bf+93nuOc/17GytW7f2\nTBsyxLx/9pl5b9HC9zjffee5rrV5Hz/e829Qr55JnzfPM3///p77eR/LO907bft2rceM0bpBA+u/\nvdW+zvSHH3Z9RvfX5s32/l0NGuRZ9jNnXJ/J+1wrVpjvyl3Fiq79nX/Dq6826xs3av3VV575e/Xy\n/Rxaa/311yb95Enrzz9unG/a++/75i1VSutmzTzThg/Xul0737zO40yfbr0tFCNHWn+uQPz9XUOV\nkqL16NG+6Tk5oR8ftD5+3P/248etj+e4dhLpl53HTicCvwANlFI7lVL9lVIDlFIPOK7wM4FtSqnN\nwHvAX8MdtKLBdr8EMPMmfDYZbu4P5bZGrEzhlJeqnZkzPddnzPDtJOdsdHUOqmf1i8m7s5v7FKJW\nvEd+zcuvMPdb74wM+PZb38HrHnrIdNyzEqyM3unHjsGNN4ZeTu9jXnONdX2++znnzDHvjzwCt9wS\n2jnKlLH+TNnZZn7vYE6fht27PdOmTYNffw283/vvJ0YDu5Vz56yfiPvqK//77N5tPVd6MLF+asnO\nU0Z9tNbVtdbFtNa1tdbjtdbvaa3HuOX5m9a6nta6mdbaol9t/CtVKsQddnUwM6z17gbFMiNSpmiw\nqkZyWrrUc71bN/+BZcgQ827V2Btqj9fnn/dcd3ZsCmVYDTsXt3ffhYkTfdOdTzcF4n1RXb06tI5c\ngf7jR+KiEOyY27fDXyP4U+6BB3wfKoi2AwegffvQ9glUjbzHq2L8wgVXx8j33vOcDMuuuA8IIoDF\nA2HHn6F7T0jy89hLnAv3vAf+nv6xcu+95j0rK3BvVucjp6H08MxrY/fDDwceFiSvnP/RA10UrS4G\n7oHH391KIKtXm7vfnUFmhd0a4xvdw4cj34N35UpzJ2O3R/i+fZ6PO2dn+7+jBHjqKUhNNcsvvpi3\nMkpASGgKZo0AraDLIxCj8Y7iXbBb5+bNoXLlvB37/Hn4739D2yfQrz7vQQ/9XYRHjDBzdIf6VMjj\nj7uO+/TT5iLi1KxZaMeyY84cUwXn/uvf6jNZzTeelwAUiPN4hw75/nCoVMn3ztDfMYI9/HHqVOAx\np5o08U07ccK3Abp6ddM50mnyZM+Om9789e3xprWZ/jYeSUBwM3OmqWsOSU5R+Hwy1FoAf/5XRMqV\nSKyGaVinZRczAAAbg0lEQVTpd9ATc2u9caO5sOdlcLcFC+Cuu3zTvX9pOde3bw/+XLq/Onx3I0dC\nnz7QpYtZ93fxDPSL76WXTBvMiBHBz+dt50745huz/Msvoe8fquxs16PAge6+du/2nefbu2qlcmXr\nqWG92ybcOb/H++4L/vdr3960lfhjdTf6+uvWQ7G4P7bqry0sL77/3jrd/Zham2qu++/P+3lCJQHB\nTZcueezEcjYVJnwDzT6Cdm+EvVyJxGq01EC/NB980LWcl0HQQh0Qzk61hPeFKdCYS86GaufF2S73\n//iDBnluy8kxDbCBLjjp6a7lDh1cx9yxI7RyBCq3+99t9GjXXVygxuE774R27TzTBgzwPZ7V383O\nOFHjxgXP4+/vFej7zO/dUF6CQ8mSvmnuI8N+840ZIeCDD/JerlBJQPBSpUoeO/CcqgYfzYU2o+FP\nb4e9XIksXFUPdo+jVPAxjZwXn2AB4sMPYepU/9udQ304G7F37bJ3cQj0WUqWdLWvHDkS/Fju1q51\nvb//vvUUqjfcAMWLm+VAwe74cVOltXWr55Amdvib0CkQ92FkDh0KfMcQjwI9deTOeX3JyvKtpnLe\ncSoVvN0nEiQgWKhaNY87nqgJH/1gJtVpMyqsZUpk4QoILVvaz+t9MXE2FN92m3l3Dg/+1luBj2On\nV3KrVq67m2AXMbuPXtq9uHjLdDzwdvHF5skeK99846oKCdQG8s47pkqrbl3r7Ur5n+vghRfy10u4\nUyeoVQtGjQpflUkov+ID/VBwPqlk53jOR4TduU9gNX68/30jPZS7FQkIFipUyMfOmekmKHT4P2j7\nFtLQHL6AYDU4nr9qJqvHScE1eqvzMdlwsJrA6J//NA2Q/i4aoQ5/YaddI1Tnz3s2mjrTnOzU6bs/\ntuz9Wb2ro+w8dnrmjBma3nlnNHKkqTKxezG/9lr/T7qF8u/wnXf8bwvW58Kds7osUUhAsFCtmvnH\n415PG5LjF8H4n6DFOLjhr5AUxzNiJDh/z85nxqhryIcfmvdhwwI/ohjq6KL9+gXevmxZaMcDSEnx\nDUz+5ge2W2XkPsaUd6e5+fNh9myzvHUr9O/vu39mpmlDCeXX/ObNrqeK7Fb3njpl/aPBeTcSrl/n\nW7fC11/73+4epEIdDTgSJCAEsHCh+Ue8eXPgemRLmekwbgGU3QF33gDFZdqowmDMGM/1UCfU8SfY\n3BFWVRN54a9R2k5jrh19+5r3mTNN8HzuOXud9LzHpbrtNle1Wv368Oijgc/7yy+ef4syZUwDuLP/\nhfNi7GzAdRtyLWjZgunWzd5+8+d7rsdiPmoJCAFUrWqe4Khb1/ePasvZVJg0DQ43gr+0T5hhLkT4\nhKu6zGrQwEj48kvXUON2KJW/QQT9zeznvGg6G72923q+/BJuvRXuuMOsB+vF3qGDdZWVc3j0jz4K\nvH+kOs3t2uW/n46znSuaJCBEWk5R03lt8UC4t4PpryAKjUB10eG0ZEn4jmV3MiKnSDR+2m0z+d//\n7B8zlF703kLt/GjFeXfk7qefzBAX33xj+rXEmgSEaFkyEKaOh163QtMw/OsSwk0oF8Z4Vr26ec9L\nn5RgbX5WsyOGcgfn/ojur7+Gfvf3ySf+t4U6p3mkhGM+BGHX5uvNE0i9b4JK68x8CjnyJxCJK9Zj\n7zjldfypv/zF1ecjGPd+AaEOkpco5A4h2g5eAh8sghqLTLtCxQA9g4SIc1aP3AYS6si3seDduBsO\n7kNVWAVR9zGtYkkCQiycrgyffAu/3wv3Xg7tX0vIKTmFyMuY//Fm+nTP9csvt86Xn1Fw333XtRzP\n80JIQIgZZeZnHrME6s80gaHCxuC7CSHCyl/fi0hx9sWIRxIQYu34RfDx97Cqj6lCavuW3C0IUYCF\n2ks9miQg5NH06a4ZkQKNkW6LToLFf4MPFkKjr+D+P0GtKIxpLIQIe7uGczpZd/F8V+BOAkIInOPB\njxplRowM+yQXR+vBh/Ng4aNwR0/zks5sQkRUqCPKBlOvnm9aLAaqywsJCCGoWNG8ly9v3m+5BXr2\nhIsuCudZFKy8C0auh0MXw/2XwS39oMKGcJ5ECCF8SEDIA/fHxj79FEqUiMBJzpeCH5+DEZvNncO9\nHeH23lB5dQROJoQQEhBCdvvt0LFjFE/4R1n46Vl4eyvsbwF9r4aet0G1EB8AF0KIIJQO90zagU6m\nlI7m+aLlzjv9j78fdslnoOX70OE12N8cfnoGdreN0smFELGh0FpHvF+43CEkmvMlYdEgU5W08Ua4\noxf0uwKafCbzLggh8kXuEMLgo49g6FDYuzcGJ086D42/gDb/MR3bfrvPvDJrx6AwQojIiM4dggSE\nMDlxAtLSXOs1a8ZgkvBKa6D1u9B0kqlOWt4P1t8K50pHuSBCiPCSgJBQMjOhbFnX+hVXQKlSvuOk\nREXRP6DB19D8I6j9M2y/AtbfAhtugqz8TBgthIgNCQgJxRkQTp2COnXgwQdN+j//GdtyUfwYNJgB\njb6EOnNgXysTHNbfItVKQiQMCQgJ5eRJSE31nDTjwgVITo5dmXwUzYK635rhMRp+DcczYN2tJjgc\nagLEyeD2QggvEhASzsKF0NbrCdA5c+Caa8xEHGPHxqZclpIumOqkxl+aAHGhuAkOm7vArnaQXSzW\nJRRC5JKAUGDUrw9vv23GP4pP2nR0a/QV1JttJu3Z3Q62XgXbroJ9LUAXiXUhhSjEJCAUOPEy3WBQ\nxY9Dxjy46Huo8z2U3g87O8KOy837vpZyByFEVElAKHDcA0K3bjBtWuzKEpLS+yD9Z6i1AGrPh4ob\nzF3D3tYmOOxrBYcbyl2EEBEjAaHAcQaEnBwzUmrCBARvKSehxmKovsxUNVVbBmX2wf5mjgDhCBKH\nGkNOPLWqC5GoJCAUODt2mJFRK1eG8ePh3ntN+g8/wJVXxrZs+VYsE6r97goQ1X6DtJ1w8BJXgNjX\n0qxnp8S6tEIkGAkIBV63bvD11+ZR1c8+gx49Yl2iMEs5CVVXuAJE9WVmwp9DjeHApeZRV+crM93M\nHCeEsBBHAUEpdT3wb8xgeGO11q96be8ETAWc03t9obV+0eI4EhDcbN5snkByfiWPPQZvvRXbMkVc\n8hmossLM61BpHVRaa14ljsDhRiY4HG5s2iSO1oejdc3cEEIUatEJCEWDFkOpJGAkcBWwF1iilJqq\ntV7vlfUnrXW3CJSxwPKOjVZT7xU450uaR1p3t/NML3YCKq434zFVWgfNPoHym8wdRVZ5M0nQ0fqO\nd+erLpwrE5vPIUQBFDQgAJcBm7TWOwCUUp8CNwPeASFRHqqMG86pOJ369zdBoU+f8M/zGvfOpsKe\ny8zLncqB1N0mOJTfbF5NFzmWt5iB+45dZHpdZ6ab9+PpjuV0CRhChMBOQKgB7HJb340JEt7aKaWW\nA3uAf2it14ahfAVahQqedwklSsC110LLlvDdd7ErV1zRSWbMpczappOcO5UDpQ5AuW1Qdjuk7YAq\nK83AfmW3Q9kdpgf28XTXMU7UcrzXhMxacKqaNHIL4WAnINixDKittT6jlOoCfAU0sMo4fPjw3OXO\nnTvTuXPnMBWh4LBqZrnnHvjww2iXJM7pJHNBP1UNdrW3ygAlD5vAkLrLPPWUtguqLzV3Ham7Tae7\nrPJwsjqcqGHeT9ZwLDvXq5lRYqXRW0TNPMcruoI2Kiul2gLDtdbXO9aHANq7Ydlrn21AK631Ua90\naVS24ZprzBhI7iZONFVJIsxUNpQ6CKl7oMwex/te13LpfWY95RScrmKCw6mqjkBUxaSdruxYrmxe\nf5RFalBFeMVJozKwBKinlEoH9gG9gN7uGZRSVbTWBxzLl2ECzVGfIwlb3n4b1q6F7t1jXZJCQBdx\n3WXQ2n++on+Yu4nS+0wnvFIHzHrlVVD6gFkvdci8J2fBmQomOJypBKcrmfczFc3y6cpmOau8ufPI\nKm+qtoSIsaABQWudrZT6G/AtrsdO1ymlBpjNegxwh1LqIeA8kAX0jGShC7omTcxr7FgzSuoll0Cb\nNrEuVSF3obijwTojeN4iZ01VValDUPKQuQMpdcikVV1u1ksegRJHzeO2JY9AdrIrOJyp4BkszlSA\nP8o50spDlmP5j3JwvgRyNyLCRTqmxbFx40xAcH5lSUmu5RtugBkzYlc2EU4aUk67goN7oCh+zPVe\n4qjjdcxsL3HM7O4MFH+kwdk0U2WVu2zx/kdZ1/K5MtI2khDip8pIxIh37OzbFz76yCxPnAiff24C\nhkh0yjw+e660eVw2FMlnXMGieKYZQqR4phmx1rmcttNt23HPfMmnzXk9gkZZ8xiw5auMo6xlXGV2\npl0ojtytJDYJCAmkXDnzvm2bmZ3tnntMQHjrLfj736FSJTh0KKZFFNF2vqR5nayRt/1VtukU6B4k\nimWaNPdX6f1mKJKUU+ZVzLl80rWedME3SFgFjlDScuQSFU3ybcex5s2hiNuI0tddB3PnQkaGWXev\nQurUCRo2hA0bTD8GMHcRd91lRlcVwpIuYtoi/iiX/2MlnTdVX+5Bwj1ouKeV3hc8T8pJM1ruudJw\nrpQZwuR8ScdySdfrQonA6QFfJWREXjfShlAAFSsG587BrFnQtat1vwYh4p82T3elnDJVYymnzXvy\nadd60SxHmsW6x8s7/bRrWSe5BZAS5v1CcVfAcG67UNxru+M9dz+LZct9SuThzkfaEEQebd0KNWua\n+RckGIjEpRwX0hIRPIeGIuddwaHoH46gkuV6Tz5jlov+4Uh3y1M802ubn3zOZec2sBdcLhQ3r6kR\n/ArcSEAogGq4VScPGgTbt8O770K1avb279vXPMFU6MZTEoWQMkOXZKc4OhRGSdJ538BhGVQcy4yP\nSrGkyqiA+vJLuPFGSHarHm3XDhYuDLzf0qXQrBkULeo55ecTT8Drr0emrEKIYKJTZSQPIBdQt97q\nGQwAfv3VN99//mPeUxzju5UsaYIBmMbpChXM8l13RaacQoj4IXcIhYxS8MwzkJUFb7xh2hiUMgGh\nWDHYuRPKet05r1kDF19s8hUpAtnZrm3t28Mvv0T3MwhR+MgdgoiA9evh6ad905WCEyd8gwGYYAAw\ndSr07u25zflI6+LF1ucrVizvZRVCRJcEhEKmYUMoXtyzfcCubt0gLc0s33033HyzeQf/Yy3t2ZO3\ncgohok8CggDsB4hXX4VNm+Djj+Grr0xQ8Ka1adAWQiQWCQiC//0PPvvMXt5SpezN/fzSSzDe60m5\n2bM91509ru3485/t5xVC5I30Qyik+vVzNQ7fdlv4jlvC0YeoaVPzOnHCta1KFdfyd9/BVVeZ4Tec\n0tNhxw7r4/74Y96quYQQ9skdQiF1ySXw5pvhP26/fp7rqanmvWZNz4v/1VebC/wTT7jSvvkGFiww\ny0OGwO7dnse65hp7ZXCeUwgRGgkIIl+qVjUzvAGMHAmPPeY/b6VK5t39sdXeveHKK027Q6NG5jFW\nMH0hangN4PnKK9bHfeEF8962rXm3erL5iy8Cfw4hhAQEkU9FisAjj5jlgQOhfn3rfBUrmuChteed\nQsuW8P33gc9x1VWuvM6L/Z13wvvvw/TpMHSoSevc2bxfcYVr36pV4cAB01HPyu23+z9vp06ByyVE\nQSMBQUTc7t0wZ479/OPGwV//6lpv1co3j1Jw331m5jj3AHP6tHn66Y8/XGmVK5t3rX3ni3jpJc9z\nPPiga1uJIGOqPf645/rcuYHzCxHvJCCIiKtRwzUEhh39+wcfiM+7gfnRR83wGiVLmm3+OsRVrOg5\n9WiDBua9fHnz7nyCyioIeXO2lzz/vCvN2Wv70UeD72+HVSdCISJFAoKIa3feCd27+6Z7B4S33nL1\nqA6ma1fP9R07YMoUs+y8m1AK3nsPfvjB3FmMHOm/DM8950pr1868p6aaOSmcgbBuXfM+daorCPnj\nbBOpWBEeesgsd+kS/HPl11NPRf4cIr5JQBBxbcIEaN3aN93uI6hJfv6Fz54Nv/1mlmvXdg3Z4X5n\nUbu2qz1i4EDTB8NdkyaedxvekpOhY0ez/Pnn5r1bN9983u0uTZrAqVOwZYu5c8nIgNGj/Z/Hm52B\nCJ2DGrorU8b+OUTBJAFBJKRGjYLnWbUKfv7Zetu110KLFp5py5a5GpmddwruVqwwY0E5JSX53m04\nOaugPvzQdNBznwrVaetWc/excaNnenq6CT6pqaYdY9s2z0A1bZqrId/btGmup75+/906D8CAAf63\nvfqqa3nQIP/58sJZNhGntNZRe5nTCZE/f/yhdXZ25I6/Z4/WmZn+t5vLuG/a3LlmeedOrc+f99y+\ncqVrnwYNrPe3Oq5TVpbZNnGi1jk55vNfuODaRynffdevN2nlynke35nPO23jRvO+bZvWN99sli9c\nMN+Hd17vY82c6Zn26qvWeXNyPNdr1gx8bPdX+/b28xa8F1rn8bobykvuEETCKVbMf1VQOFSvHrhz\n2/z5ZgY6f2rVcs0pYcWq7IcO+fa7cFe8uLks9O5tqsuSksxdx+nT/vdxOngQevUy40tZNXa3amW2\n168Pa9eaKqp//tNsK1LEfB+TJlkf29mRsEsXWLnSLGsNgwebzo/evKv6mjULXn4nq+OFwjnnRziN\nGxf+Y8ZUNKKO84XcIYgCyv0OwUpWltaDBpnlbdu0/v133zyZmVovWpS3c3/1ldbTpnmm79+vfe4a\nzp7V+sgRs9ymje92dwcPupazsrR+7TWTf9w4rd991ywvW+bKs2qV5/EuucT3l66zvM7XgQNa//ab\nvV/JAwa4lh94QOtbb9V6yRL7v7K9zw1ap6T4z//ww/aOef31wfO98UZe7grcX2gdjWt0NE6SezIJ\nCKKAGjXKXDRjAbQ+eTL0/bKyzMU41HPNm+da3rXLtS0nR+tZs1zrL7zgeVGbPNm1X8uWrguq1lrX\nqmXWz583788849qvVCmtn3pK60cecaWdOOHat3NnV/pll3lehJct07pSJde5xo1zbS9eXOt69TzL\n2KKFeb9wwRVQQevHH3ctd+/uGRCys7VesSLwBf2jjyQgSEAQIgpmzIjeudwDgt385ctr3bChK+3Z\nZ7X+/nvPfNOnuy6woPX8+VovX26Wz50z6ZmZWi9e7BsQvv7aBIs1a8z6DTdonZHh2t6jh2fwue8+\n1/ptt5nle+4xQeDCBa0/+MCVt04dra+5RuuhQ1377NzpGRCcSpWyvphnZJjPAFp36KD1jh1aV6um\n9b//rfWTT7raRl5+WevU1NgGBJlCUwhhm1Jm5Fm7w5EvXQrNmwduUwFYsgQuu8xc+urUgYULrZ/0\ncpbhxAn/j8meP29m8nM+mTVxIowa5Wrv+PlnGDvWPAF2/jxcuBC8V/rTT5te7c7LV7Fipp+J9+Vs\n3jw4e9Y87TV6tOd257J3O8qxY6YtqGZN0yemUSPT095z9N/oTKEpw18LIWybPRs6dLCf36oPiZU2\nbUzfCzCP4wYyd27gPhPJyZ7rffqYl9Pll5uXM693fjvee89c+L05x9O66CLzcIE7f31nypUzLzBB\nICvLBISsLNMxc9as0MuXV3KHIIQQQezZYzohPvBAdM/70kvO4Uuic4cgAUEIIeKU1qZKKyUlOgFB\n+iEIIUScUipvVVp5JQFBCCEEIAFBCCGEgwQEIYQQgM2AoJS6Xim1Xim1USn1pJ88I5RSm5RSy5VS\nzcNbTCGEEJEWNCAopZKAkcB1wMVAb6VUI688XYC6Wuv6wAAgwNBfAmDevHmxLkLckO/CRb4LF/ku\nos/OHcJlwCat9Q6t9XngU+Bmrzw3Ax8DaK0XAWlKqSphLWkBI//YXeS7cJHvwkW+i+izExBqALvc\n1nc70gLl2WORRwghRByTRmUhhBCAjZ7KSqm2wHCt9fWO9SGYkfdedcvzLjBXaz3Zsb4e6KS1PuB1\nLOmmLIQQeRAvg9stAeoppdKBfUAvoLdXnmnAQGCyI4Ac9w4GEJ0PJIQQIm+CBgStdbZS6m/At5gq\nprFa63VKqQFmsx6jtZ6plOqqlNoMnAb6R7bYQgghwi2qg9sJIYSIX1FrVLbTuS3RKKVqKqV+UEqt\nUUqtUko94kgvp5T6Vim1QSk1WymV5rbPU44OfOuUUte6pbdUSq10fD//dktPUUp96tjnV6VU7eh+\nytAopZKUUr8ppaY51gvld6GUSlNKfeb4bGuUUn8qxN/F35VSqx2f47+OsheK70IpNVYpdUAptdIt\nLSqfXSnVz5F/g1Kqr60CR2NaNkzg2QykA8nAcqBRNM4d4c9VFWjuWC4NbAAaAa8Cgx3pTwKvOJab\nAL9jquoyHN+J8y5tEdDGsTwTuM6x/BAw2rHcE/g01p87yHfyd2ACMM2xXii/C+BDoL9juSiQVhi/\nC6A6sBVIcaxPBvoVlu8C6Ag0B1a6pUX8swPlgC2Of3dlnctByxulL6UtMMttfQjwZKz/WBH4nF8B\nVwPrgSqOtKrAeqvPDcwC/uTIs9YtvRfwH8fyN8CfHMtFgEOx/pwBPn9N4DugM66AUOi+CyAV2GKR\nXhi/i+rADscFqijmAZRC9X8E80PYPSBE8rMf9M7jWP8P0DNYWaNVZWSnc1tCU0plYH4JLMT8sQ8A\naK33A87ZYf114KuB+U6c3L+f3H201tnAcaVU+Yh8iPx7C/gH4N4wVRi/i4uAw0qp8Y7qszFKqZIU\nwu9Ca70XeAPYiflcmVrrORTC78JN5Qh+9kzHZ89TZ2HpmBYGSqnSwOfAIK31KTwviFis5+t0YTxW\n2CilbgAOaK2XE7iMBf67wPwSbgmM0lq3xDx5N4TC+e+iLGZom3TM3UIppdSdFMLvIoC4+ezRCgh7\nAPeGnpqOtISnlCqKCQafaK2nOpIPKMdYTkqpqsBBR/oewH3qbef34C/dYx+lVBEgVWt9NAIfJb86\nAN2UUluBScCVSqlPgP2F8LvYDezSWi91rP8PEyAK47+Lq4GtWuujjl+wXwLtKZzfhVM0PnuerrnR\nCgi5nduUUimY+q1pUTp3pI3D1O+97ZY2DbjHsdwPmOqW3svxZMBFQD1gseO2MVMpdZlSSgF9vfbp\n51juDvwQsU+SD1rroVrr2lrrOpi/7w9a67uBryl838UBYJdSqoEj6SpgDYXw3wWmqqitUqq44zNc\nBaylcH0XCs9f7tH47LOBa5R52q0ccI0jLbAoNqxcj3kKZxMwJNYNPWH6TB2AbMxTU78Dvzk+Z3lg\njuPzfguUddvnKczTA+uAa93SWwGrHN/P227pxYApjvSFQEasP7eN76UTrkblQvldAM0wP4SWA19g\nnvYorN/FMMfnWgl8hHnSsFB8F8BEYC9wFhMc+2Ma2CP+2TFBZxOwEehrp7zSMU0IIQQgjcpCCCEc\nJCAIIYQAJCAIIYRwkIAghBACkIAghBDCQQKCEEIIQAKCEEIIBwkIQgghAPh/3iAYm+4m7MwAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f97eb4d60b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w+wc1+eM9uICgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f97eb4d6be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
