{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "\n",
    "class GRU2:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth_train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "    \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        # concat: [h, x]: D+H=Z\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        # gate: h_prob\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        # signal: h_pred\n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h_next and y_pred\n",
    "        h = h_old + hz * (hh - h_old)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (h_old, X_one_hot, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache)\n",
    "        else:\n",
    "            cache = h_old, X_one_hot, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            h_old, X_one_hot, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            h_old, X_one_hot, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "\n",
    "        # h_next and y_pred\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "\n",
    "        # signal: h_pred\n",
    "        dhh = hz * dh\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "        dh_old2 = dX[:, :self.H]\n",
    "        dX1 = dX[:, self.H:]\n",
    "\n",
    "        # gate: h_prob\n",
    "        dhz = (hh - h_old) * dh\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dX, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "        dX2 = dX[:, self.H:]\n",
    "\n",
    "        # concat: [h, x]\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX1 + dX2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for layer in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.0\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth_train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU2(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRv4/1DUZ0QlNMPcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f5ac201d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['smooth_train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
