{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    # for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=200) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 331.1303\n",
      " sshaisynth  unm  etktorkd  enrntlor0shriina rgpey meyy seineasndacrro  n uivi aen y  ietaf cahunyans dnnkG  uyrhc her ninxetlr atant ashsehfaIaNe ixb e ni8t arra es ,aein wh incfetIiinnexa s so5hgi At\n",
      "Iter-2 loss: 319.6571\n",
      " dat ses, WnhIaasld 2iRsa tf nst hcskn fnbiiet te ta h tdifhibeddh a k i5 irne xexruteulto iwvirod an xiipys l crhi ti ny net Dxplt xbart t ileenpIet e om s sh hht ahoaoet, ss tih IhfoC, s bd1ctttelonj\n",
      "Iter-3 loss: 302.9954\n",
      " ieshmrxluit -nees 5if xnrlh ts nyeJuliys jde yoabrh k iiv ti. ilesr iwy nEtned fs rntt urcngiy tis p-txutiy in Inouvoky tte rri. seyh -ptents ianeh eesedd peaimi' th sNctyy y sehyt -tte br Gtoie ns at\n",
      "Iter-4 loss: 304.6708\n",
      " , letlef G an ath 日nndh if inge2–sh f  ab poHan Jitn W  f R1nn xin  1aGnhe Ht;3e s riblyd f  iIvif    alh-o JfAs mpc7naf Df ip Wab, e–af f  aIe  paiAt v3  arrs In i7c 0-mjf owbb 9pnvy 1Gpnf 本aWWl- sor\n",
      "Iter-5 loss: 242.7744\n",
      " pextat Cothe lhensi, und lanhy Re bad tate Cinns tey leth uns terxrng retutiFt in vovis lhi6 wocoSy tes wigh xunin ty Cin y Fot eeke meas ort n ang rumil5es one Dot toId mit In tos 0IWon cindes lonlen\n",
      "Iter-6 loss: 224.7093\n",
      " Co4n wod ad Fin the wit enk e, an bard is mistuty by Ixpin lope nard ward woth ib 1n5ido te boror. coged west id Nopogeke Palasd-mho mrarcin, irn rDof 0entunis to ry we isg Atang in Japared GD8e  9x7 \n",
      "Iter-7 loss: 216.3444\n",
      " Wlitry Runtret Japan the Gben sourbepifit un, in, moglecs moliltidesa bitex, ipdethe oof palliof Caty ins t ovecevesog dy Otion the win, folant ans baunn the sauntuleatiok the counme host bountry ebca\n",
      "Iter-8 loss: 207.9683\n",
      " 2f ainky unshy moulth ealt anst rapered borton the Uf th lapemartank wisilecise the worl Hyart erin fhat the houty rheseosl the EI09was andemer Dicored I the lingan, is Wingst is esinkry Japan etitirt\n",
      "Iter-9 loss: 205.2886\n",
      " daunlest int fokt roted in a parat an toud-ian ecopineey rent esiche purthe Gugatiny, sios the wort Japan tosed in urand sele licomatory unsiss Naklary Jakan ar Ekan and is lexletsy in the wolimest is\n",
      "Iter-10 loss: 205.6136\n",
      " fobsture. Jada 3oumityes ind the worlgert int is the Japan and unfatitet in an lovegenmy the noun. Ston wrid in cartins ounsting robgunt Is expentituren-rate e3. Pive exptome fokekery Norbesest in. ri\n",
      "Iter-11 loss: 206.2288\n",
      " woll 35 mhimed inten es of Wily baly Wargestritithito barg in crankesbenourecectotys enkat and onutry in courry e. Japan, the UN Wind lountry world of the Hilhton. 4s a eseate andary of Japamed resing\n",
      "Iter-12 loss: 198.2470\n",
      " merinkry Arolawe unficexkas ry In is \"onsudares I this in. I sexskand rompest fokrd and rhind lorang inmlectresisitites the parlity urtry fountry woh wis the Eagaly-Senan esupbat if the nofhrn-ualeced\n",
      "Iter-13 loss: 193.6550\n",
      " Japan Tho ghond indary in the Eastry fegrus I-4ish romeeltity rrof the nounkry Nlare eanclacar in eext courtry westion is fookctyisory by raunidernl orest Japan is floulttelich woll lighithe muecturex\n",
      "Iter-14 loss: 196.5591\n",
      " Hres offlcst Japan ent mitared rins-urin\"dess its robesinat In entye melexuture the Empofios Chinan of In of DDilhure an abeat of the colelticy of inoung mokaturby unsel the himacter Index 3s lirsed 1\n",
      "Iter-15 loss: 190.4876\n",
      " Japanoly Asitith in the C Jepan id viting rombecy from Hinily-war of rucetire of Japan expantion expintion porowed its wit for whis Ching in the Glountry nhin Sinto-CE6pan, Japan in thics expest opet \n",
      "Iter-16 loss: 188.9611\n",
      " Japan is eath Index-siule heln5 the country The high the chomen, Chiiny poulitetes rexa, Nippeet in tha hosichitat unokad randated in forst ficelitrety chort pis in the Septurt-rina is of the Comkec p\n",
      "Iter-17 loss: 194.1901\n",
      " and Afoung is is en-selithe and Naratexte poburekinan as in enfecty corsiage, of Japan his wher rence honate, ransiderin, popefsc and tation of of of the Wargescy Werd s Japaress third the world, rank\n",
      "Iter-18 loss: 191.8097\n",
      " Chintes nountry wint the Global e96 unfreslometatitater of larl the Globerl epor cevection of unne, in on the the world. buded popeletimale Wargicoslure to eser. Japan lenseringerns the Glopan, is ef \n",
      "Iter-19 loss: 182.3061\n",
      " Turropink pering the Sepourted i s usserint country in th lian, the choveclecide, in esiningry Innty in the countution perowes, the wimelexle miltaly bunted ry sourth is a linituys boun mextert pren i\n",
      "Iter-20 loss: 181.4131\n",
      " undecs and oppome, rate high and uthilal canbed ounte, rins the firsoloung peres eats Irleclecterch by giclatimy. Thicallead is is perised in the with ropelided porlo2is war upborias the filllole wivi\n",
      "Iter-21 loss: 183.5998\n",
      " Weraldhst rislinat in the Diurtrent porlates revine, ith the . the dirn Cated of popef of operto pere 3s ats ritat of inhn into a inting of of ucl in islmnr a by powred. Wis marg in eclicolexped ranke\n",
      "Iter-22 loss: 184.5445\n",
      " Nagpitar and Hiverest country in the world. The with is th consing mintlopred in th the he gokeked romepperof en enfelecced by pith and its ceftt regins and 116 of bcforld 186pperrh nlith oun ith nouo\n",
      "Iter-23 loss: 183.2558\n",
      " the Global fie, durre CompelecouDles of Eanting lountry with the world, the econand southe malart inf of the thich with canted a meginn on on Dex. Thh liging Runsing initatith obokadery first in the R\n",
      "Iter-24 loss: 183.6658\n",
      " and fortulad, stary andatex a defing the stuna. The ftrnof Japan, whinaled sh esitar in the Countby the SD61lfolst th orlthed umbin has fourlhlced biny rnx undtby inst. Japan was runsian lelpente of W\n",
      "Iter-25 loss: 180.5694\n",
      " in, molenture of 165s tankry hive thin-rean e. Japan to expercinf, In and itich lopphe to mevin. inis-uden nounged Woklowed Chinas al parth lounthe nentern nolomertotery rin the world is the world is \n",
      "Iter-26 loss: 170.2550\n",
      " Hegeal of higt fourth country ind, feuganting pourthe thiching lokekit in the fourth ligitanke nounthe nountry a Upolikan's ugeted thichard molerchina esian islatat or and peupmemed in th lecsicsrand \n",
      "Iter-27 loss: 173.4472\n",
      " Wixth counth wil the nuintry ourst milo an percons ecrop oprst ecesicand country in the Eaxpelane a the dicturpercest biny rounge lixf eos the in 12Crsok murn. Japan the Global. 4s inith consturing en\n",
      "Iter-28 loss: 170.2231\n",
      " in the deu7sest conked loun. in the 193 andang earnhc mortary on the tho sirst orectarchidecislloraat peror of and stal opthest the hind, thed is fho thest cenopeter eft is the first boulhliced bunted\n",
      "Iter-29 loss: 170.4351\n",
      " Himsary nortar ectures. Japan was ranked Gin the Upno country in 196 mond standal fortar and lobanked ligion ecterced in 194 sl. Japan endernoun his, efiucex, reopecorc military in the naldgercearly s\n",
      "Iter-30 loss: 164.5279\n",
      " Wilidgexted, felboths ecrisory rigtith hig thirsololowe, hainaledes larked of the Wekhokhrd in 1947, which counthy-war, ther efionter in the margevelomectary in the Global Peace Aepored of anje eith l\n",
      "Iter-31 loss: 164.3140\n",
      " the Gstmang ronstivitary intot a Nesichatest constare militar in the wolltress andt cive thicallicl Empiropse, in the horlcthicedercint revurcater rinkich laeghertilicheced of Japanes total eften is r\n",
      "Iter-32 loss: 153.8242\n",
      " Nearound Index. Japan in the country ining lowet cuoppen ressith rand Repan of inst in the furre thich lisa, it ectentry rentiving frcof lithour esthe highest reatersl nioltet and forthlleopenese, the\n",
      "Iter-33 loss: 148.4873\n",
      " Empero0. m of an is ronery, the Ocisichee, infros eiwind, sexth leviod millomutity of in the urlaral of Neopentersi line Eurest large counkyh in the GlobhllouDend the highest reopex. wh expini th lofi\n",
      "Iter-34 loss: 141.3520\n",
      " Wixth list-rest ored thing country in the world, and poI he in the Eastary the four expercunsetith rempite \"preepetcent powle is lardhouniys, mith largest oplioc rantary the furre Asian is a dithantry\n",
      "Iter-35 loss: 139.2931\n",
      " in the numericolsed of Nobeilition the nortar of islrtaislresed the world's latry mirn andary the firtats largest military telturcegsud, worlkate hintorol the Russed in the Rfkilarcyd rislions, the Mi\n",
      "Iter-36 loss: 150.9028\n",
      " the Gsoffecountry in is ic thil and, is is a houl caliwal of as parthe highest ceared't pre isroserse forrowes eate and Wilitalitacary titunt mird counthy in the country in isolt, is enforsountry revi\n",
      "Iter-37 loss: 139.3718\n",
      " the Emperol Nayed in 197 milodelest in ihiche thir of istundsryy hinst-in the furre is country revirolsial the and ragins ofitald Honped first wirs to the is ejstmart larg Indete conmed bonstiring por\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-38 loss: 148.7369\n",
      " Wigieng fy moun to hond first lisicarch runseladest in the north lowesked, largest ereollowes that largest Ireopectire in istin-revighe for. ERfuthh milltn of Cctority holankhhily Wourd Hinar power la\n",
      "Iter-39 loss: 140.1436\n",
      " in the Gloub Peades, in the Grofset in ragedst unolivand a Deuthe is rofelect op is thming lobsed liwal the War aed cone the sixth largest mill mation is in the thirst country in East ghontrkited in t\n",
      "Iter-40 loss: 141.9873\n",
      " Ni9prng, is the highest-ranked bony forr w rof-ipenest-reften a whor, whith esed-ralgentured unti. Lowache, byk wor world. Japan has offecirilegeside thindst efterce bilth largest militar and conse in\n",
      "Iter-41 loss: 132.8979\n",
      " in the north liet exping powet-laveternd in the segtest reates covelation the nourthe bon the third lirterth country wirth lichatime a pertokea, stwoch rinit poweturceg, of icronfologlitar of iseledes\n",
      "Iter-42 loss: 132.2083\n",
      " the world, and fegtentic-tentitutrend country in the Paman is amperorcens Eo, the 20the Okhh country is worr worlates, whurchichorias offond sef larity. ApEesen sule of Nivr of, Cang and I sivividanid\n",
      "Iter-43 loss: 134.1605\n",
      " the G8, expor. 6 withs expics ecerlagerturby in the Rysial Ching.. ealrst hin the Rysion ofllsc anf Hfdios forst th country in 196 highiss fbeftturcegiderthe fortar hovaled nat. Japan to ect militar a\n",
      "Iter-44 loss: 123.8265\n",
      " Eufsed foutr live the fore thed eskeat 2lhy a con t enfirst re injt the and tanked first country in Eand vine and Worty the fursslate Euban ealed of fi-wirysss ofuted Report with larperi lirm Indthe i\n",
      "Iter-45 loss: 141.7184\n",
      " the UN. the Global Coubintalalal Inan 1947, Japan was the world, is laritexlivined fuontry in East Ingedithat hokohtry undetent. Abeakurroliopese in the duce larry lexritys rountry in Eas, whurarturby\n",
      "Iter-46 loss: 124.7754\n",
      " 12, whar thins. Japan is off nortate in 199x. 6 Japan was in the Emperorterse Wea megbeats lifl cesth largect Infivex Sainal world firth lourthe eatses. Japan is randeceimed in the Eust. Nearnd Eapal \n",
      "Iter-47 loss: 132.5454\n",
      " the 1pist in the Global Cokopited is the first country in 1868 lobe ticolidecenting of aceli ditat is in the higtresion ths ifurcevith liest und. Sina, whe urthe mirthe Warse. Japan was rofitacr and W\n",
      "Iter-48 loss: 116.6710\n",
      " the world, highenst ricorlardest is is a levisith lard tovenated in thing from the nat of tl fortuntry the Emperor. Japan of inx. in the world icountry regising nountrils and the highest-ranntry in fo\n",
      "Iter-49 loss: 128.8800\n",
      " the Gpopurlolidas ron the Oexped pre himeticrregicats rearlh Index. Japan has a 9e 3ipen-tain. The Coleth legopmesidedecsear e. Ind largest militar and parity in Worltlliced Ari. In the numreates, Jap\n",
      "Iter-50 loss: 123.5176\n",
      " Ni9percessive fe 4ELs. Japan's largest in eokitedernal the gobllliondecs. bmalled to texte thin lountry in the Global Countuled in Htana Ifdte world fiurth lies romes. on the Curbougtreviage Weurcoums\n",
      "Iter-51 loss: 119.1270\n",
      " pearchosing Reprtatern bourld margenty counthy expex. Indeced onf thing power ramsian begtoed expical raged roneran the ncigoritern ty purchar I al the Global dixpecstand war, the G3I a Gseptures al t\n",
      "Iter-52 loss: 110.2139\n",
      " Wourtect cefest milife boky. fceliveat of isolf Japacearderes rolseatal 12016 20 and is sifl the Gpobertex, stare Tokgysith frowometed Werteltincend of ravictarion, In is court lity in Tant unnt molon\n",
      "Iter-53 loss: 107.7494\n",
      " and Huturry country in the Rusdeatity an the leanfecd os, the world, in the Global Couptune the numris and warth lopest wirt-largest is is oflrota ceates romang malle riinternndnternor Hloporictary te\n",
      "Iter-54 loss: 105.6869\n",
      " Emperor wart of Wovthe 189x arded's raed make urolth country in EmpE9s as of the narte in the G8kast const in the Global Peace binyane powpertitune bog thaed Hunrry boubty. Altrecosed 9s aighot. resid\n",
      "Iter-55 loss: 101.7298\n",
      " in Hlopan'n' sefomee, inf in the Sineo mored lonthy make the nopund the fourtha, Alina population enfore siltho an t lime tos War of 1945 . Japan endeed in E98,ted ranked first in the Global Coupen to\n",
      "Iter-56 loss: 98.0141\n",
      " in ETroked of the Rising State of Japan's syate of Nuce warth latrysteth esth largest militar any nourty-ran tallaclareoptresourted in tha Emperor andese inf. As whe thidercang expexy the four wan, Ru\n",
      "Iter-57 loss: 120.1798\n",
      " in Easf Culest renedec first urperic bokuhry by Emperor. Japan endern burtth centocigicalide expictarin fouritatily in Wokh. 9t leorn the Gloralang in the bon the is a Seoutly of 9f1Japerowetron in in\n",
      "Iter-58 loss: 102.2094\n",
      " Weloduledes Paneent militar of Hloper and first in the Global Coupeng early bounked in Aepiced 9s firowestrenoclalas Infin divicant as Deviche a ked Emperor andetivesest military Injoffn thing ranked \n",
      "Iter-59 loss: 99.9720\n",
      " the Grealore Sisurreas the Greatel siun, piwetard numrliled untureat maintainas fourth leipelleoltry in the of Japarterd wartantin rectirefed-uso and Human the nopured Arcanked sipantard militarmi ent\n",
      "Iter-60 loss: 95.2021\n",
      " Asial 127dant mortality in the Country Brand Indeland-reation 17to aleitancalian to the West. Necicliig of 196 Nare Wobefticalict and in the hight lorec. Japan is ranked of decris raltred in the Sume \n",
      "Iter-61 loss: 96.9217\n",
      " the Greater and in the deopured in the lompeby wirth leorperend in thing from the 20, Ittomates of isl. counted in the Global Competititese a peese of 6,858 the thirdant mortality in the from a dedein\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 300 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
