{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 36.1206\n",
      "can and i4ay as renrluthoed eoxdornd 193n inesn oprmsd aniesnl CChn efpr-wpepmp py ipitotan panse 99B\n",
      "Iter-20 loss: 35.9957\n",
      "c ane'igur, 1Nikhe xphorpmpa'Cheerd the Hhereor Soryer' orlaJaosteyyicenat ol thesoatecienc-oNparaoar\n",
      "Iter-30 loss: 35.7361\n",
      "coupantappCbes eikalyrphe WE3aordera,o, ofokuprri, obr is iin mphe AamalrPheorceapapsad opalofolbbis \n",
      "Iter-40 loss: 35.5218\n",
      "caipe Joomr ehm,rGhasoraukhdeurmermero Japaponatestiriigereuwermpdasre bompeapaxopupehhieuulamPalaalh\n",
      "Iter-50 loss: 35.2915\n",
      "c and any m0R. R9L8al mitheoHare'edheheerapaaoapireo'uushe.hr3f, abhe Taporlherrighiuthhe.,ri8j'erthe\n",
      "Iter-60 loss: 35.1323\n",
      "c9Hal Krmho biplesuuKheicheanaapherouShhijhoubhthhiophh8hsu1,h8NiG414xlphhesomeamho日4\n",
      "Chieapari. oHof\n",
      "Iter-70 loss: 35.0055\n",
      "cion The AaparoiLh,chghkeuWubamfeapmaomphlhiphemhhoreuchi4.h iunmh1647\"ithithichs8ghsishmiauslhighlhi\n",
      "Iter-80 loss: 34.7831\n",
      "ch omuthe phheutAaJaapaacacafathrisiu,ydespapeapuphehaeepphosmShshueh7m4\"u8FhhhhoImemareh0gheomaaania\n",
      "Iter-90 loss: 34.4999\n",
      "cest akige limhiiupirouE4H9.1C4WiphiihlhighicsiomouDbhhishourhi80W-O14S44437SarlehmroI4K6arechuhhh 96\n",
      "Iter-100 loss: 34.1740\n",
      "corandes thiuthe wof,r, aodgicapopouh\"SfJJaphhhhhem94747rchhhhis,soxhhhhhasho,o4HWDHaraabheum84本Jamap\n",
      "Iter-110 loss: 33.7667\n",
      "corte toref, oxlelds. angivg ntg patrivieichesghussuargh,, apapiehio4\"K7.ermherobhRho49z74ghhipopphh8\n",
      "Iter-120 loss: 33.3217\n",
      "cinalamed Sruu8 Thorghrpo1B4MonheabhmichhosixhjRho40447emophhhhuhhhNi4SnxohHiJapauhhihseom44Jupphhhho\n",
      "Iter-130 loss: 32.8703\n",
      "ch vecnurimas bhe Jaaamooh34I4–7%lepmearhhheoS474JSmaouhhhh8Haxlhh04%7bhuch,hhsshchsuu44UH45hohhishch\n",
      "Iter-140 loss: 32.3843\n",
      "caml ji8s, chechiu8\"4shho,ithhhhi,hco,u.83(p.erhe rofhNhhhovhrbh8,hh,ixpou14.3G67y ihhihou894j%.aryov\n",
      "Iter-150 loss: 31.8362\n",
      "c Solaro masNhmi4,h8h.hathhi80x-4Aphhh,h,oo;7m9mNhhhiommmemihihhhiicou047whhhohh80bhRisovhhhhiSsarges\n",
      "Iter-160 loss: 31.2093\n",
      "col Smta oithhhhihobhojmhisrofixJarhhhho0uWoJJaeaiihhiathissiSouIh9%44445 ofihithio4.imofhihsimhthhh,\n",
      "Iter-170 loss: 30.4941\n",
      "ca anduenterard, 9PDiaafomSSh44zoduhihS–4.74\"0therio-biNhob47, Japi9494Nho,hmobaphrhhna4khchhichbvhow\n",
      "Iter-180 loss: 29.7286\n",
      "cerl Natats cgintnoled wofimlly silhasajg lousgyent riishisiisishsosis Si089H7hhi,sthh,isoafhes,shisc\n",
      "Iter-190 loss: 28.8561\n",
      "cftien emearadd-Apamumhi34944liphhhhicobhishivhusihosou,ihchomhifh8shurhhissthighssuofubhiarhhoissorh\n",
      "Iter-200 loss: 27.8840\n",
      "cfite poutery hohh RhhEEFEpEp ofeh––45horhh0issimiirhinssouuthiasomhhhReshshigs,urosufiothhohthmothht\n",
      "Iter-210 loss: 26.8173\n",
      "ch sinokeds, aryy cu,,anhesg ghval,esd. mearibyacantt. agaseoml my imtontesdevasguagesiglalgesed tere\n",
      "Iter-220 loss: 25.6382\n",
      "cy buntal csyeylgen simatallity. 0) this lydgesligllasekhgigouEt umpeory ihaiho-844z7bhosuhohh44Lhomh\n",
      "Iter-230 loss: 24.4169\n",
      "cof al thichthissmamesophhoh8,ho,h8husuchiEisslmhiihissscarasisgurheachoushohhhssisthisosishathhiEssa\n",
      "Iter-240 loss: 23.3492\n",
      "caltu antital xtamareohhhhoihoghsssicadras hiiiosmourhhi1RhcoxbhissvhosodsThhio\"hssiusisithicsodexhea\n",
      "Iter-250 loss: 22.4586\n",
      "ce bficid a neas anthary ma porapothhhobamibo\".xaosthhh,h,thistuiSapeooeorba09,, ntouut miearoDoeuoth\n",
      "Iter-260 loss: 21.7084\n",
      "ch thithitasg ChhiimoumeiSio134%hihis,oshisssiof oPhhiaohh,ihogoEu7hoodhhihosou,hhhh,vuom\"s meiihiiis\n",
      "Iter-270 loss: 20.8752\n",
      "ceonomt,ote rhhhh,icast of witortercheus RuN33847 parhhiiEsgo Ss isevhouo46oJSiiiishhsschissi, salodi\n",
      "Iter-280 loss: 20.0268\n",
      "cint ith reaeron ioiofthe NibRNE4–L \"stsy ivitheisssu-,hssad as test nbi5M3%g8o u, ihohhhhiosou,s-ixt\n",
      "Iter-290 loss: 19.1624\n",
      "citelion Wes analenfe ig, mop prand an..iPrhiiofhiusavorge. orolyyy on xnmoncdgsesgivkltal sty inthin\n",
      "Iter-300 loss: 18.3321\n",
      "c of Asiomivintaigost-saateoeiiginirgu gla. wid cefgeccntlll s. 22kBD. Ngolnpat atasisss-dydya aslivi\n",
      "Iter-310 loss: 17.6251\n",
      "che wowsdsndy'seusi. –A77porbe toutory ioshhhis-styesovich-testest re 1937 inchee ou, horshosoogogosg\n",
      "Iter-320 loss: 17.3584\n",
      "cetarestarl ivigind Euowd's any tejM in2). Ty perrgisdaved preececinntdo Jakpp,iEEEl-budy. The to 87T\n",
      "Iter-330 loss: 17.2489\n",
      "canase wond lticlollll'.i, ghsina,-gthunamaerheastery..hn hinossicityony ititorisssisssssumad inta ro\n",
      "Iter-340 loss: 16.2361\n",
      "cy pintat the srtrathhis-,goss outhy retha hesg wa, bape iOz. Japan thas molumumithhivessssc jilo ull\n",
      "Iter-350 loss: 15.4917\n",
      "cintaros a oniintte th in8 GHmalioan bintlorg rodissai9hh,ichinas, rheaneeceane) gou mPalamapparuiouu\n",
      "Iter-360 loss: 14.8396\n",
      "cat is and inteed Jagesta, hautheaeeemetianingasis conkedgasoreiOthty os thhiss-ouu-suob-Iko o matatu\n",
      "Iter-370 loss: 14.2628\n",
      "cy Cith resiaivecokury iscrangaving arseatafeinndan iginllak),,, a anke 16n, nnoutand svennd Sad, oe,\n",
      "Iter-380 loss: 13.6591\n",
      "chest Sinto Jatihatetokeeuteolethint in foreocothoihitiassissand'dsaxlslynunrfmby indvacej,edgad apte\n",
      "Iter-390 loss: 13.0345\n",
      "cy mlemte rosh %oros-y Shiathomeitetatatesturthihanosmatithiiathissanang ghe ilicitirechasgil auariaa\n",
      "Iter-400 loss: 12.4433\n",
      "c. Japane ipiotechissmhiissuruczio Japanter Jaterirs, oushso-uuuh;iss-uuhhh-usuhu. Jaaoby igontututhi\n",
      "Iter-410 loss: 11.9751\n",
      "c pecemen camae,ssiunaraaianeei6ithi cormiuhstothoushios-tosy oflalicenfM\n",
      "2,, –E0EE117pirc, in ininid\n",
      "Iter-420 loss: 11.4417\n",
      "cobapllged. iriahiathosihshasios gestureasandan nfCP本d d I7,rr,udd-daddgansg wunapareaneinnkme 2 0p p\n",
      "Iter-430 loss: 10.9009\n",
      "capagest laboreehHhssurhhhhhhr hi,iicaadureaamadanennd 1本本0m Pmpto tathistommasiaraaeatory in niy nin\n",
      "Iter-440 loss: 10.3166\n",
      "c Werpacleacefiisirhi8, nalaaraiestotinndebi) 9EEE;r, 1kokop of i. Japapeetest prn man 2piom-e ouhias\n",
      "Iter-450 loss: 9.7587\n",
      "copoltititoeohrhhhhssssoma papartemithirhussigto sod de dofh35日日–577 the couthmeihinsasmme S.stunsed \n",
      "Iter-460 loss: 9.2273\n",
      "c and teistand. rheciauchi. Japante pinlaro hhiaasouthitmathiamathisstartas a.elare ainafho, n papast\n",
      "Iter-470 loss: 8.7013\n",
      "ced sevor. O78. -5C3L Enu Japant iinsara aduoyeiiziand Preoucs, s, ;nndbesiou-iiiaiioiiihinSiS arupap\n",
      "Iter-480 loss: 8.1874\n",
      "consthe lage-aaaeaaanandre::rp日–日–B\n",
      "7lld fe is sounbino bou-iiotueaaatesennmea in fihihi-iicieatuthe \n",
      "Iter-490 loss: 7.7471\n",
      "c anstiond Nhimbbobu hs bapgdodrd' aininde Toffpess, stalally' disnnm6,, anllsciarieihississs a andds\n",
      "Iter-500 loss: 7.6951\n",
      "c. Japan wag man.erammpitismaniandd aan i5idind in Pimand I UrH754Ls ceviasioiuihichii,ssime Sappreap\n",
      "Iter-510 loss: 7.3075\n",
      "consuchan olemariisitianisEgsoreadglskginmeas annnl,epyyesusuunntio. Jaaayy iSF7ttople miorhhictaatg \n",
      "Iter-520 loss: 7.0497\n",
      "calese fhe, c, to in the nith riagiga,tykiit rhiciathi.aand cane in initird ing malsouter Niamn thoro\n",
      "Iter-530 loss: 6.7857\n",
      "cainedexndllnd inrtouttimi oo,s mayy hin indigoss it iin iintieinaathitind itouned itoniigstising mup\n",
      "Iter-540 loss: 6.5440\n",
      "cal of innllbRsssussmonu a hhh rhotiitain aeaaaiesinary i'7n teinlistellbvllsllgomutboto ohhuuhhh,tho\n",
      "Iter-550 loss: 6.2475\n",
      "cted llaks my idofooo thhiuhhhirotutsy michinanad'saapalktiss fled leszy a'j llat sas thllegangen ceg\n",
      "Iter-560 loss: 5.9030\n",
      "chy with an iheithimsiotargodes.lanetinfr mmky rconltarogouturaiiag ld hhecaionemmommmathissma Risgh \n",
      "Iter-570 loss: 5.7762\n",
      "cy, the thiciass. Thopipilatiincan\" ayes i Han eigallanesuinnl Jamapaisinnget napan thistonime-bouuhh\n",
      "Iter-580 loss: 5.6342\n",
      "capangest bareainod oubapadsis,,htod, and riu7nton infosso iioto evinto fouurristgosnd ayy 21z–ouftho\n",
      "Iter-590 loss: 5.4810\n",
      "c. Japanes and rnreacong owhheasthosn riog. Thitaiictariond. paryaytitingod o ady-nilten Jipport. e m\n",
      "Iter-600 loss: 5.3280\n",
      "caasived en taccelalamm2P00B arafiss-innd coun lestanensiaunnnoboladgasgees un 1N;7tlomecionome.amapa\n",
      "Iter-610 loss: 4.9734\n",
      "ch icslyoued th thiithss ioghint mpaeatititakt mmm my mp 1250\n",
      "\n",
      ". erhiasirus-od wedyy infiand wg ard3s\n",
      "Iter-620 loss: 4.8047\n",
      "cameiind ferted lekle wadltoryrradidssssde','','ug aldd-pandan inn Japapckam 1)t. dhes a-lhactallict \n",
      "Iter-630 loss: 4.7525\n",
      "corm. Japan hisutry in theciciaaio memectaiitiigiJobannt. Taiteian conde adean injiapass. vaueapaly t\n",
      "Iter-640 loss: 4.6013\n",
      "c the sixte Anase imiincbabgssma wadd micear-ae Fri, t as hh iaaiieeitaras etureestintiof riciamucmmm\n",
      "Iter-650 loss: 4.4996\n",
      "ch inche Sipaneeso-mayareant marhtemeitianmt aroleryiorriin perpowitiiichissa edopop eiitiomiatouoml \n",
      "Iter-660 loss: 4.4347\n",
      "ch largest meriona-naabole iiciatuthat-, eaeation eepantenet rn AEtpmmbpboJoupiJaoooariama, hitiitiit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 4.4080\n",
      "cat coniten inddunwang N2Jppirueeiiiusic toryldyg tataiisdddsag it in firnn dg sggasld rheee anito in\n",
      "Iter-680 loss: 4.0219\n",
      "cs. Japan is geccerinimSiJaasunee i2llllle on ofroD ousokupopfofoni1mBt is tobonllly 9hhhhss Sobu ss \n",
      "Iter-690 loss: 3.9748\n",
      "ced coutitaid ga tal was m. thhh Jauiasiion,opmparoshuntnocdepan iisisusuvokky-,iiomat. Sulolky.ihiia\n",
      "Iter-700 loss: 4.0466\n",
      "ch cingects atose ipiriniinintr. bhiciicmiite mpipaeith heriathiciaciat lapad. Saaaseeipare annde ann\n",
      "Iter-710 loss: 3.9211\n",
      "capital wi \"eeeciciachhimr l.erane indeinto rinnJaurwaaeis-Ri,, th chimiathitiini a-olepaissauntel cg\n",
      "Iter-720 loss: 4.0567\n",
      "ctetily wes 8ereh.utere-torly afe laleviaes,sgang son aeeoay tPmi2本 Jpand inssssEEpnnd mey in yteppas\n",
      "Iter-730 loss: 3.6302\n",
      "con. kho ole'tity momeitiarge searare easaeddllinn;n thistesl. 1s8iuChiat hes heithiGt. Japan i nd an\n",
      "Iter-740 loss: 3.6355\n",
      "c Games.\n",
      ". lalaly's, nr,, tarnlsskht in orhiss-uo-outi-tihiiiicmp mouru. n lopwreiiasmhisuragosk. ira\n",
      "Iter-750 loss: 3.7591\n",
      "che 1st ecllskeealeanoneaaaoolomiabs io sourimat Shinaroaope, isiiathicturic\" bapad'y 1:n日BL 1pon al \n",
      "Iter-760 loss: 3.6437\n",
      "came of ntrapafioh80 6 ,he in Jaaly fn C%%2本 th orhio humhhoauo--o-sd,y. th vh.iath isiiathimiim acaa\n",
      "Iter-770 loss: 3.4798\n",
      "cating tastethe andlannan lllkpapomu n tadecdinf 5GP%B本 NSiiion Em. rhethitiic Ea,ushalarls.eteassoo,\n",
      "Iter-780 loss: 3.4316\n",
      "conomy. Apooomiiioturib\". red seiea afibg \"tnd the flojhttokokhe ohhosssissiicog uthiichiathicirissss\n",
      "Iter-790 loss: 3.4332\n",
      "came to derdpo ::\n",
      "%\n",
      "2Lhorouthothourhhios.\n",
      ". rhhimimat ber a oftea llo1wpickumltatoke, ofouihhhohhions\n",
      "Iter-800 loss: 3.4077\n",
      "cy, the thicioa. Eamlatiisssiinad aaalatiisss racing nd ane wh st rrihh hoso-oureinderff untertosss n\n",
      "Iter-810 loss: 3.3111\n",
      "ced colkki,ssis-gg prepeperissirainetirong into popolacterbaioog xp ipiro Japama aicataiceatataro, ca\n",
      "Iter-820 loss: 3.3144\n",
      "chy xutrlaniaimsiismat pheseaceninEkbu ufsed..shhithe laeeallarios cammeresmt mouleeao iiiihiiduchhil\n",
      "Iter-830 loss: 3.3438\n",
      "ccessive fes lyvmes as aunla tis lamt buml poro9H0220oy rouninnng cicilet ailalieiGsiigan, aantan l J\n",
      "Iter-840 loss: 3.2492\n",
      "came to exnteda I 50D7 npandeaced ste-thiasigg andd. a ace rre frond en renseineggg ondet ie nan inti\n",
      "Iter-850 loss: 3.2305\n",
      "ch isavethekth aturhicaiossusurhnt aueiaaconfnntoumbaiox,,Rjma ar of oreaahi i,s, d remennpe nnnary r\n",
      "Iter-860 loss: 3.2244\n",
      "citye y d tac. shs: nL-jjotaland at ecist fory Nssdng eniog ul lon ii..,nIrlessasten thst cealt enges\n",
      "Iter-870 loss: 3.2019\n",
      "clkou l hissivisisd hego, eufaapandgosedu peer ofllh-nSefposu usoiiiuss i,\n",
      "h.t. chKaaihhsssssiase ag \n",
      "Iter-880 loss: 3.1598\n",
      "capital cityr inosoubfo-o-oiog mbo-n fessst Sfyo condlowend lanlalgk%mp roruihs totaty a rs an thes h\n",
      "Iter-890 loss: 2.9623\n",
      "chy with an who'odeiondibof CphCm n os of withiisiitio. Oumaporouhmhiuis-i,.\n",
      "y meoty ssnunm, maelal,,\n",
      "Iter-900 loss: 3.1807\n",
      "c Games.\n",
      ".. aresy aj4ton ternlleplou-F0Hpphio'4447tnmayacacelios DA1本 Itr inHsssus meeya-acnHtonokJ g\n",
      "Iter-910 loss: 3.1285\n",
      "cy, the thicttopiit maisuna whooroutthicoureasiit. Jaaiooinss an eegomlub-Jalardssoga\"\"P,eaiodor-N37n\n",
      "Iter-920 loss: 3.0987\n",
      "capity owhs in hhorisss ihithitiiithiothis hos gurhsisimm porerrp3n:MW–0N%n toly. Wotratat is euresg \n",
      "Iter-930 loss: 3.1065\n",
      "city of To Pn–eDHodusu j.17Jpperceated endatisigggon- audonty innneeaaapa liFHNNWh -47n Japaly infnnf\n",
      "Iter-940 loss: 2.9426\n",
      "catit vis lan m.iramrombugh na,, necelanle tacen cbcccomgopu, Eoft tihiiiiciomtiticalile agokg, sind \n",
      "Iter-950 loss: 3.0636\n",
      "ctanac sgemat Jaatirouuntt byor. theciigisgg stured\"aap ouu.75trothy in nale iimzyo orohhoP-u.;ndedgs\n",
      "Iter-960 loss: 3.0656\n",
      "ch includesuluoihhiicataacas. Japane irlilideacer iochiimmmboti. lg disske ghgt he hiiihiiuditiatiiti\n",
      "Iter-970 loss: 3.0161\n",
      "chy wurthe Gusur520 , nofy th sesndosso- sosoogiobon--touniitid s a ofexpplr'Hi5CcSte, selaree ac-cal\n",
      "Iter-980 loss: 3.0853\n",
      "chy with andss%i iE5n dive in n leoleafnllcapap of CH,hhoobouthiro a-uomiieisssasm oullld R00her, thi\n",
      "Iter-990 loss: 3.0954\n",
      "came Toirg l I Iyoy h55%-%本 7nnt merteomus orR-isoogigo mosshmionitihost atst re chin Uubpppeng taran\n",
      "Iter-1000 loss: 2.9881\n",
      "cd sevevel clalisihm pop pi  hhhcitaoauta uJpok. hhhiamalidisk riissiam aladded a  N :7n pounal oy' i\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJQERkST2/wF49Uz2RPmEPQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bde7278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
