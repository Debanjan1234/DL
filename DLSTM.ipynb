{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 92.5354\n",
      "e phinawken 'ountn, ,he faait ind iandted iirith feevv8nt wnurdy, 8her cpelreyh pecioagl thiro. Tem a\n",
      "Iter-20 loss: 84.1521\n",
      "es Japitimicerc nrind  anait. 18 ae poro8. cmean int the stodd the 1an the kith  and in thed Aobede i\n",
      "Iter-30 loss: 79.9337\n",
      "e turte the rmied lloin thet Japate mith\" th ist ofd ppee ind Jafakoury x fomita the the ast reaed Ja\n",
      "Iter-40 loss: 76.8690\n",
      "ed epobs thina ig ta L9on antEconces it 1j, 1is mpean ke the thit miant wurt 1e txemy. Japenard velin\n",
      "Iter-50 loss: 75.0561\n",
      "eklolysis alkorter aso os Japaor esthe boes talldinad arthes :llex. cheanmealed panr. Japorele Sin Re\n",
      "Iter-60 loss: 70.7603\n",
      "es Oserres Gngafth. Japane and of mald laomle in Sinbe the iscea asclalldeuverse Oithi Kourius in whh\n",
      "Iter-70 loss: 71.6356\n",
      "es Cankorsut of hest Soutso Japan thir casgmesten eril Ris ompome In Gfo heond of, expur1 ther with .\n",
      "Iter-80 loss: 78.9748\n",
      "e, comthe, porigoht besestcigesrelded aroke ofrl-aathy a asan hatasiming, Eareeod lmiom of corish Wi-\n",
      "Iter-90 loss: 75.5540\n",
      "e ro pne bourted Strno, wand the Wats Janat Sigedt. wthis sarle Sesurcedrodea arest oulatg pat of Rut\n",
      "Iter-100 loss: 74.0782\n",
      "ean's fonu and In aigint ease Indicita ekate Kounithol TEen The Ufith the Statarsdy-Japared's atuplou\n",
      "Iter-110 loss: 73.1064\n",
      "eniny this. sutist the and asticalpepimycy mithor wo9d,, Japtond toInjy Oswivemio5 war miin ry ises s\n",
      "Iter-120 loss: 72.2780\n",
      "e; lwituve 2cenry Cuxtha and ma and bokpard sisched. 18re Ea and. uhithe Wat Asealita arme \"akings Ne\n",
      "Iter-130 loss: 71.5049\n",
      "ewo bith in Muse the Japa, 2945 1208om 1non Pa, Iiche Tha and Sing ended Nish longen Pan men. Wgethe \n",
      "Iter-140 loss: 70.7228\n",
      "ewicules latory byr eand iP thig sian'sestry Japan \"s. woxnte Warce wop th G85 Kor. thar an. and  Jmp\n",
      "Iter-150 loss: 69.8906\n",
      "ed kistr ul'se Uminilal wekoko upor lobag lacas Phita wy it inltury Pan of mpald, nea anced mictectrr\n",
      "Iter-160 loss: 68.9928\n",
      "eweciced-raa th medeke in the eg chen's apes is anato forlara lad it io ond bot outes miont righ ha T\n",
      "Iter-170 loss: 68.0303\n",
      "encexkac ond Japand copntery Sin Wakin tuwis Da thewices of Asward Japnne pere-lan eaom ef ind foulll\n",
      "Iter-180 loss: 67.0102\n",
      "edes forof Japar Ches. whiss the the Norlligest Asiso tan the te kino fyitho domdered thet and counal\n",
      "Iter-190 loss: 65.9399\n",
      "eCthea in; ary shes wored Asis the friostun Area est wakak print restala Upobe checty in inke cith in\n",
      "Iter-200 loss: 64.8234\n",
      "e hof the dhepesid waoly, egunthiny tivee Epand 29anchus sigithe bomite anji Copan, Japan on sith n p\n",
      "Iter-210 loss: 63.6639\n",
      "ex, Japand of Tha Gntomeit of orpare toss apand in (Japaneses Ramamalithe a2gesh ceased arctaear pary\n",
      "Iter-220 loss: 62.4827\n",
      "e pirving Japan lapia 's nasts waris ctobunN, ih panth litho . Chiand tivel Osior amced ald sedad of \n",
      "Iter-230 loss: 61.3202\n",
      "e tionty we lilacesr fren in tarlo of 185tho as and Larlated il ca, the woriod scofrled iverte. Asic \n",
      "Iter-240 loss: 60.2252\n",
      "es, Wa an ewond womith entile ballse ASiof Rest isith ind Wa an te tobyes hive, prectoricower aned se\n",
      "Iter-250 loss: 59.3498\n",
      "es is upio per, the Tre the iostien Crlargest To ande forl tomines angs Loust. Chonowase Emperiod bas\n",
      "Iter-260 loss: 61.4997\n",
      "eg irlaalesiy in 37hh rlowfre Mer. . Japan, the Upion 194, wo in the wrurtity' the Epperinji the Nirl\n",
      "Iter-270 loss: 73.9472\n",
      "e cot pren se soce-ldore ghta an t e te sof in cala ir hinghsudeeprreld Jaswe im. th  frin am the ico\n",
      "Iter-280 loss: 65.6658\n",
      "e Japan the fmestr en6tury EIr inter leceirnde T7 efourt a 1he rina fonm iometir \"47 iterigingst wura\n",
      "Iter-290 loss: 64.4293\n",
      "e pawhe wopertaly War Incitory Tolata un1 pepetin par Tounto es ortery sio Nan In 47 Astha conse ist \n",
      "Iter-300 loss: 63.9832\n",
      "en ir an and pfolurecs refie parl on is a Nilingin-denome Oisho isslargusigound Japion to Nito wonty \n",
      "Iter-310 loss: 63.0452\n",
      "ed is the cardet un and Japan, whilladed Japath is tho agal The surind baigest right mgesteme allicgl\n",
      "Iter-320 loss: 62.2947\n",
      "ery Ind risty of 1D 19the bol and leand, whe uriagt fo anges masias and's bing Japan Japan Was lapese\n",
      "Iter-330 loss: 76.4302\n",
      "eded Erecy sist 67th lors lalrovenindos.d %al reatestiry ont tirey. Armean in86, rsthees 1D  wotldice\n",
      "Iter-340 loss: 72.5732\n",
      "es enopurtg pertamy roveac matias ig the Wat ned purlan Huntacat ano sexpall the floi  Emkor larimey\n",
      "\n",
      "Iter-350 loss: 67.3017\n",
      "edd an the fomx. Jatht ined whic inrtopbin ulithe Oconeturliyc myetits of reantictec Cowh coreas the \n",
      "Iter-360 loss: 65.4131\n",
      "eutoun Asmiand infang. hanose fepron liongexthe tabto the the Kedpeat whe the Ayhisthel xand cepre in\n",
      "Iter-370 loss: 64.7255\n",
      "ered OghDloras ahd thincena Ewer sin Conthin. Chander fomyl war Japond's OEmNparom The JapautPely T8k\n",
      "Iter-380 loss: 63.8286\n",
      "emyd it oreas of Emptow limasts Sema comlopeary, of of Japan's inghicse livided Japan, by expinde sir\n",
      "Iter-390 loss: 62.4571\n",
      "e filithe worre fofr and cotn it ta it lith loes, chinht Coufrmy bast Oisthe:, pand Japan an ayos as \n",
      "Iter-400 loss: 60.7354\n",
      "ed cemeator thiny Wcedly we turt Sea. lyoulld Empar arkese lion i1 15 icotan fila fith com 18, instom\n",
      "Iter-410 loss: 58.8839\n",
      "ed Tuse wisen tirag chanjo-Japanes liond's d ar ald linaccceary porld inalaisuns purines anat red, at\n",
      "Iter-420 loss: 57.0503\n",
      "e iterono of Japanes cithe countury. Thoe werly Japand Coulitorid 17laigest Nanas a lirord and exthe \n",
      "Iter-430 loss: 66.6457\n",
      "erry Nohe omea Wopn with led arviisa, forerl hasols 日antt mefte sentuth stivint ofilad penf. Iton. pe\n",
      "Iter-440 loss: 64.6147\n",
      "ed un lDefky, eheresed warl regithes. chos inthe kok oro lichatrol orithesithe Peesur anded Empixthic\n",
      "Iter-450 loss: 69.4689\n",
      "erkuPPedar Arof ikht eset porlamoN45 ucloprenbes wirlde caigast. Japen of the wiostocedalde Asivetrer\n",
      "Iter-460 loss: 86.2798\n",
      "e Sichen nun ture hired hiceren mmelons e stariton\" 62in pat mo 9intoh thecthin thr imis c anges ir l\n",
      "Iter-470 loss: 80.8231\n",
      "e Gr 947 if iselachong thend's tersuvert. the sonde por copens f an shiodedes ean iPolowi ton 1ivi.oc\n",
      "Iter-480 loss: 77.9600\n",
      "e toflod is hon theaGth Ga latolluxpson the 2Cicesed )sin Thest pand nal hini G85opand makaor of 1oni\n",
      "Iter-490 loss: 76.0632\n",
      "e WiriChe tbophal ic cingre ithe susturuItsean. Japauns the cof ostiion wokaic Dos Se to. theth of 本a\n",
      "Iter-500 loss: 74.3431\n",
      "e wwon. Th rege, itorgof miroxpenn Jandlurton, thu, chemG. Nusung thed parlyol y8e ahichite patexwast\n",
      "Iter-510 loss: 72.6507\n",
      "eS a in\"8. The atatlen Chind Jaseasin thic cocl a okiopor, tasclased Tar, S 7in; purun larlrolcof bur\n",
      "Iter-520 loss: 71.0542\n",
      "e na asil )his-Sulallated The mpiiseac urnges Eanghisistr arghs N-korel Stowh. 12, Japens kol .868, E\n",
      "Iter-530 loss: 69.6529\n",
      "eind-Japatanl of ron tin;-Japer aedir mes ch-red bun te mors-Japalas omsth-of GSe pan, S allearhic wh\n",
      "Iter-540 loss: 68.5888\n",
      "e Japertcease Kyarn, Wast ana, e d co lase e ond aired thict en, Th of copuporon Nas hur arigod iaigo\n",
      "Iter-550 loss: 67.9810\n",
      "ed Ialathe Der-lamerlyP fitrer is mindeavered 20iof whorind Japenas obolcexpeand Ssa eand's thokal 18\n",
      "Iter-560 loss: 67.7699\n",
      "e Su ban eilare the inte Ward Stesics icrto n tere Japratt recoku 201 the and's an, of hin se Nese iP\n",
      "Iter-570 loss: 67.7532\n",
      "e Japasin urte wond in wh ilaciaedenu, a pokarcer ictreld Husunit, ecrte thes in icedec. Iopete Wa Ch\n",
      "Iter-580 loss: 67.7796\n",
      "elymint olaty miviole irstn din pume hoon the thith . ciesoc ofom taons furre: Rupenh Koren man the i\n",
      "Iter-590 loss: 67.7895\n",
      "enty firg the antilardes a Upan bur is in men the Amomer. an 18539. Swerla. area mpeno ulllyzed ryd p\n",
      "Iter-600 loss: 67.7550\n",
      "e nithex. Seang Hokapino, thicherofladeante Serseedeand an in ibar ap8thind 日Aitaoky Jarse the fourd'\n",
      "Iter-610 loss: 67.6440\n",
      "er wird urest Gist apatsthu foustr. Torer Ocokarte Wi a menom merin 685本 Japan tion estore Ceaponl om\n",
      "Iter-620 loss: 67.4640\n",
      "e the of is and of a ma alerte tount dente of 9oloky 201, and Tomkyo thest ectorialcof isto ane linas\n",
      "Iter-630 loss: 67.1793\n",
      "e Asirlevic thit che fiitily. Tokmencountry voultrtr hardempealy in thon, an erventy Counto the fin\")\n",
      "Iter-640 loss: 66.7154\n",
      "ered ishur, frata con maled cegha seasiangs seveson 191l of the Och lond's meveng-rap.kueed morto isc\n",
      "Iter-650 loss: 66.0567\n",
      "ellilevencecta-das of 18685h wDithinu, the the memplior bac \"Land, frect end resed hecanal indid cura\n",
      "Iter-660 loss: 65.1506\n",
      "ef Abatha Hof in latan the n penuced in 19th reate in OEapanen Tath hagughise Japetery Wond tet Chinv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 64.0845\n",
      "et mithit mome Py ef is a Chest of tho of of of Japanes Glea Contry the Sof and peurire sounst of Jap\n",
      "Iter-680 loss: 62.9968\n",
      "ebind a Nhivinth-latalod rinomed mometicardes Brangeburgesf burth in and restuof inst hin, Kortert in\n",
      "Iter-690 loss: 61.9383\n",
      "enthe UNmo meO15) BresicaCam explarges of of on cicat, mabe ames eate n ith upent-lotase the is the f\n",
      "Iter-700 loss: 60.7549\n",
      "en Itobal Kyurgest alde panden. The bulgocs an winh withing and higas of pornarla ardand countrlino t\n",
      "Iter-710 loss: 59.3851\n",
      "ectorthe anded The ef ia, mino lins a it Huland mean's th renco ferurth tex, lacinest of of khor and \n",
      "Iter-720 loss: 57.9277\n",
      "ed asical insorly in 1941. mally ist of in pyolith counory Torye moper. The ty sturud pared rovino in\n",
      "Iter-730 loss: 60.1827\n",
      "e terono mal it in ang s and chenceded, worthing-latusekory in pares a to ante ta sival of th ome the\n",
      "Iter-740 loss: 72.0052\n",
      "ercarles of pero5 werllditall latasttasiro-reates ictangitho 47tingem Th copd secry tho d the litar i\n",
      "Iter-750 loss: 68.6766\n",
      "erto ingerse an an lokaweve try frent ef bed Deen 日l thesing in cithe poging pal atary Lo ana, ind Ch\n",
      "Iter-760 loss: 67.0215\n",
      "eina, Japas ta sevode frin thep istted iolliouthede um pored inge Sty Nouthichecurestef in 1. S arla,\n",
      "Iter-770 loss: 66.5269\n",
      "et onorin ly copinowatathing poped ban ly Japan Pane Glamet milasidme latis She Japanghinco the thina\n",
      "Iter-780 loss: 66.3215\n",
      "eatevestrestimeo the ta S 17, Ch ian in thet ex, loperlioure Japan's lar Chat the 19loz Japan in stro\n",
      "Iter-790 loss: 66.1209\n",
      "erchesed Sio f andoun Star ex, Ky om ihet, lanor 日本 Noprlopre ec ar esnurgergertry Cu berbe the Me th\n",
      "Iter-800 loss: 65.8920\n",
      "erianthil to is Cilly 47stry D mecth lith inmery ind gh th tory the f urdiow or Kye te corline Glisen\n",
      "Iter-810 loss: 65.6303\n",
      "emonmility prerni. Asy posted-lortor is tn ur. Theitseurgunt Olea a 1 shishir Waitand Japaneanted and\n",
      "Iter-820 loss: 65.3760\n",
      "ercionged ican the fo, tho Eag t of Nea, wita h d ss ghunorid whope Aremy . Japapare Co the tas it it\n",
      "Iter-830 loss: 65.1971\n",
      "emeaterte dectins as 12 Niensired th deth ty th om Locer, stud ru. Ind byllorefte ciollis lesekun\") B\n",
      "Iter-840 loss: 65.0943\n",
      "estioured wotod 17the perigithe andectar. Asowiotond pomber, momaid Aseatitan thecit lat ren an estur\n",
      "Iter-850 loss: 65.0003\n",
      "erfiofrllrgicf, Eased Sta in chit pesokokuan es inct the ange Warler. Frraa itht mectenowa, Worite ri\n",
      "Iter-860 loss: 64.8561\n",
      "ed ofolit of 1937d Apepon, Ch wastmali's the co As and pan I Na1dest man ballast of th rae surensing \n",
      "Iter-870 loss: 64.6241\n",
      "eloban in lan insthonas lioar ingy ha aincowaty. Laycer, whintt in th intramutieasu \". by Gld's sun r\n",
      "Iter-880 loss: 64.2839\n",
      "erpefecttre co th d whergesiol anghican Wer wallitithy 35本 ofofrilld Wion hoke eatersichmitolliollary\n",
      "Iter-890 loss: 63.8340\n",
      "eivekeored Asian ol archory. a par, in 1947, Areanese Japenoly. Theas marte fotl inse Fillle nina, wh\n",
      "Iter-900 loss: 63.2838\n",
      "ex Niasto n.. The ca. fista \"Land cikedevont and mat rog sura. loliou. mentou, the an alcthe counr wh\n",
      "Iter-910 loss: 62.6349\n",
      "erna, ran Warld's fithm as fral the tom Abas th th-lecith cal and the to stun; Japan Nhice perd, the \n",
      "Iter-920 loss: 61.8669\n",
      "ed forthe wheg the Gike Chokured sian iilto war med wr thi es int em ir E1seth. The was mist riorse d\n",
      "Iter-930 loss: 60.9950\n",
      "ellored Tokyby the red la, preatur sion in the poped me thopeveld's foun, iml loflorlonf. As the ceko\n",
      "Iter-940 loss: 60.0866\n",
      "ed aig th unory whith paod in AsAlith ce an en, the Upprory en of 9853 r pppon eper. Arcancidomed of \n",
      "Iter-950 loss: 59.1570\n",
      "ercecinenes of Japan wiryd preanestintiom oflam Defty rengishas fry wal sencerod world's the Emper..\n",
      "\n",
      "Iter-960 loss: 58.1448\n",
      "ed Niporlal whin8. uD malar war in lan Warten. colasular in the malint Brenges romed rity imetheutern\n",
      "Iter-970 loss: 56.8952\n",
      "e Eupo end is thtced Wer, Homlacetigated higstaingsy suled burt Curod The feges ald Ste probed Arease\n",
      "Iter-980 loss: 57.7578\n",
      "elliced ofr melerobat Sia dined W945 EmpanEtowt lar II, titho, Ky uxpandts rencolutind im tiontiin ra\n",
      "Iter-990 loss: 72.4618\n",
      "ec tion\")EChina om ofrensted acEapen, tha, 685–201853  fofrerias lese reg forod ih Arpand of Aremea. \n",
      "Iter-1000 loss: 64.3332\n",
      "embage lin renstreurcal ortlys it Areag lag f tiof Okyop lalat high as ind in poptored efpercon perio\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7n1Qcf0MAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1105e13c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
