{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        # h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        # h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    # for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=200) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 330.9961\n",
      " snnhi ulstnnn,eoe eoaioteppaosahrasapNks hs fvoaei mf omkisAr oHa edfvist5shivkasncrA,rciro hnitgrJiaooaneisnrievnrai tx h not7yaednthhptekt ysesstta  eIoraccea ae e aidei ieiaar pHhitt iessnan ne tia\n",
      "Iter-2 loss: 331.6618\n",
      "  nnegfn r2dnhian suooi oxsueo 1abtees rra 2g ed-uhnei   a sdathhrhlaal en e,mte6ryereagtydde xtartdaaidaveiiua-  snx nty leDylnsum rylgre-yeya 3 aatItnvghtr2vet tsaokNsy nsxpfnaluby1nntospnpyont hyius\n",
      "Iter-3 loss: 316.8404\n",
      " gys n 5J1n  trd lna; aet pan es ,eiorinutIreahxd ylxre maitaubn's 9oc 'bt ibn1ts'd iye ahttIyusihoproht onaaD3t hbnst ea aWsl hiisrs y eop9rht0rkd aai t2ie iius  hdoxtrpnis iyue9wrarlaohmftrm rs Joi o\n",
      "Iter-4 loss: 261.1497\n",
      " natd , uisy fawan torinaa Jte lloJ ,tit  wphiey is ng erger. yd Uir Was henst sans oan rbte Giry Wth nhe an 8ind 9be eea hd Jthe a nreiy kof emiiteemet bond Ateeate pepnret irt Ieare G\"he coin in brce\n",
      "Iter-5 loss: 227.8087\n",
      " on Costed Jirhy Jang, Injs uganer.0 Jaf chuonas can bipag ankest in tin, Wod 1Mag Wores) AodWit cjufed Wa etaresl Jiban eimely 8an Heenint ond cand oe tiled rarl. Areany in in the Nhedal 1y61 w20it sa\n",
      "Iter-6 loss: 210.7523\n",
      " Ugea ered baof JapaD ac ganldy Nalrthig Galexa Glauntatoncestuntan, Ii. Ith in Inder6 ontan. Janrth an the in in in 5. Javthe rund Intond If tored of Japan and the cortreaind pantarld vand Apaveceawes\n",
      "Iter-7 loss: 199.2606\n",
      " in moming bowet exuntry mared Jrpapih-cithe sa ming the 94honls Afien, proirtikh holapal-laun, is soucing. AftalexN0x06 Nopanuand pomrih-tan, the Irlarest pirorgint Sevined elajirty Afipas choma naled\n",
      "Iter-8 loss: 202.1044\n",
      " Ixaty ths bertes Japame minst piwat ry an the wopis and the litas hist wipates sin-laliviin-mtopined. Asion tDo pomofotites norriwan moren the hopunaty bople sentoind che intty bost portising suagliwe\n",
      "Iter-9 loss: 196.8339\n",
      " Hrans. Thily, lar. Japan ent Nipybern and in the anmy igs G. Japan he joriwesiturgarld's lovoml's fimt Noviunaturh verte Cokmed foust in the  sotacesas the GNefese rand whist hiliand efekext In in the\n",
      "Iter-10 loss: 189.5995\n",
      " covilitanty bung forst hing in the atel-ilolex, is and and it th and corat Cimectary plrectu and incta anting langest inlissorin the woben, Afea an of thilaty 5sofet the Gcelitith soturdodetoss inty-r\n",
      "Iter-11 loss: 194.2872\n",
      " unded frentse corst in the 日as the comnt rountry the if living rondt laise.It in the and Hrekimansicaled of Jaian host in the an the corst in the Gfowst in the the RCperititally cofl om of peace paigt\n",
      "Iter-12 loss: 190.8770\n",
      " xostith lorget in the and ild rant by in. Nocarly and is the higheskerad Suntith its revecuuht In, it the it le Nilowat bou, \"sst in Japan is rarted and in the Gn dilatityh ta dokultion Japan rose bur\n",
      "Iter-13 loss: 196.0412\n",
      " the in the Glorgefendexnr's the Gf lorst in holo-lakive lower in the Gbepingom-gertarcis-largest Develpanc, is the SiDed the core, Cho mpest prectar, fat pead of the and tlies the centi migitsivt ent \n",
      "Iter-14 loss: 192.5283\n",
      " Hecthe country is ectaines fowrt bejesond-led intos on ped-xrurhost xopen econsidas. Thexwand the the Arles's largest roihe merbed \"storsitard Index. Japan ts tain. Thesse nrowesututiled in Japan in t\n",
      "Iter-15 loss: 182.3165\n",
      " and ins. Japan is romst recase has, vired kirt Reomined in and bukun-kaky taitith-lerd. the G8 in the world and corerals. Japan in the largest om the was po peasic mice and it levpe Arest lapect ihe t\n",
      "Iter-16 loss: 176.1985\n",
      " and in country in Asia.u the in fourth-largest in the Nobapinal Gmpounked in the highest-ranc, East and the Gfosi,. The world Indexan Norea. In in the the Glowestututitig pesc. Int isese oprol in 194t\n",
      "Iter-17 loss: 175.8922\n",
      " fourth fol upur. Japs6 an ent on the the and fe per. Tositat ent Jalan romente the Gand-in peridest. Afiest in the countrios ans inicy Demenf eated belolalitation cant om. Apupthas and ronce and felic\n",
      "Iter-18 loss: 172.5727\n",
      " parinonaly as ent ury is the world, gpertest in the Re7orld ty bber and himse aimancay Deseateked expand in barga. Lored Japan, 5se froplfe Wirted the highest rencyar. JapaneU 28, the countbin laress \n",
      "Iter-19 loss: 170.8931\n",
      " lare country in countr-leand Ise farst pourtarines . thirg lidest ectarc tt isent ofeest Taing Asiath largest reand pecof Jaaan, the world's foudt fourt In in open piporinky country i . milloriled fre\n",
      "Iter-20 loss: 160.8993\n",
      " pelouth no the 126 and fourth-largest pour of thira and Inskinget. Japan is ranked in country in a ente pe 1yeth fiu. 日eloped foplef and In ised Nerea econational Noant De Nercean, the world's fourth-\n",
      "Iter-21 loss: 160.9426\n",
      " Deared incted ceand the wored in the Glovic Caunt migest ecenn, ry, crent r.1 and Index, ranked te 2ent arg perigesture econg besean Index, the was fllower. Jarea has f llemgersufares oftoper in count\n",
      "Iter-22 loss: 159.8403\n",
      " War. millorg restures a leaed folet an in the Gupunces Sino-tung lerpered Astory the the East China divinily Ixlle pupuwitiited penrest-ranked fir the Empirerd an hyargest expared in the emuped and it\n",
      "Iter-23 loss: 149.8672\n",
      " pefiinclr--Collarese pied first in the lospte parc. The precoxas in the world Inded Asiancy int it ringsilarese pelre parcly the of thic wans efount pounjinimy barld, Wan cyred Japanese the hich laind\n",
      "Iter-24 loss: 146.6952\n",
      " Indes in hoef and and a gilion pergippend Kyrante dexrysutares, in the Uled the the G8, ant ring its rmake uper inf 186as land and raske oar, it mict-uresenndort hine douches flef Jaras githe in revec\n",
      "Iter-25 loss: 143.1325\n",
      " Toibat paresasu untal pymapth the world, its revento my. The Ixred u en. The country in the Global Peace lymbe peror as a menetory urital De the called Pared's ear, it sivin-lurch. The and In oplfed t\n",
      "Iter-26 loss: 138.6355\n",
      " rrest expowh corerasulan--meered Index, heage luented earls in efern Nearg bomer Ninalita as the Fuunty Impertor-leamed into Deand Index, ranked sixsh of pired in expertirg frele of Wat in the Global \n",
      "Iter-27 loss: 132.8872\n",
      " Pore camilic Cinsm Emprorlaghest urinal Empirte tontit th c lige R03% the Gge tocted fourth copoubal it ored Japan the Gelice cope mentiured in the Gm0t mivikil argest Emperiinar-ale surcaly archil Co\n",
      "Iter-28 loss: 142.0336\n",
      " Howtauna ered and its revest ern lecised cou thitul God ectoon hokuly Hislaress mict rese kime of nomivitains lode peloice belomed country by 1c. In efenfed eft titivin an Japan has offarteruding its \n",
      "Iter-29 loss: 128.9525\n",
      " fourth-largest rige mogings of anter cenered and in 1945 lupinse thich lacest life ect ar, which in earl worese Index, randesarn Index, ranked first in the Ulled the world's lidercectry cestiy renerca\n",
      "Iter-30 loss: 120.4752\n",
      " countrly in the of Japan\") inde 1sent of Japanese War of fegionait Ge7 expan Asiranked first in the lake peoven fourth fit luentiund fedomer calded the world's elentbllliliruese fourth-leare it araly \n",
      "Iter-31 loss: 119.5957\n",
      " siupiod in 1947, bolese Inded intiined fourtirdive comijhilide War, and early an optpopintarest ecore and peacekeepon the world, and the soli pe beerest-leaed fourthit on entopulano-ulinal Dikese from\n",
      "Iter-32 loss: 120.3337\n",
      " elected elenoro Ust early aintion enjoye pirsitation of Japan Eas the East China Sead of shu, the world's fourth-ledared, the UN ferer, the capital me. Japan entered into pyrenjined in the hoeking Chi\n",
      "Iter-33 loss: 111.4264\n",
      " the hy cintaimo 1937 Brorg milutiined a unitury CE1 77 perclar.. 07 proped in 1945 the Country it the East China Sea and ficllpied infant rughices in the Gxpecern Itetiind in the selored and fourth-la\n",
      "Iter-34 loss: 109.2111\n",
      " the Hrecome early 2015t wy th th- feudttes enji y'. Arpyeed countric dissive necou the sorc he lorgenth largest mring and Human Development Index siwihos lureand is elea eatered in the Gpert ig lomind\n",
      "Iter-35 loss: 107.9159\n",
      " an the Gfof the OmP and earl In and emge namitinat of peviod Japan is ranked surtalicy in a seadingirly its ares, nexpurgercete pegeimed parted of Hichaurea. The world's pombentporce 1ubest of 1th cat\n",
      "Iter-36 loss: 111.7100\n",
      " period. Jiprochos decoded is the world's largestitutise foullpanlese hine country hit Jan n a developed country winth sgperioxpon-kupmed Jargit mithi e prenter a gulitarig pupule menjong pefountiined \n",
      "Iter-37 loss: 104.7843\n",
      " fourth-largiits frof thin-lererivoth to the East Asia. ci sthictures tenturies, lifentruntory cututaiss fpontrmy burt this lanse War of Sicy with third lolesital 1868, ch apfo-kighes efitalityy prope \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-38 loss: 108.1997\n",
      " resiatiof nerudered a period of state in the Japan has ended forlclarce frorcle Grentil an it emento 1senked first hinet 17esirates of vivinly, poxpllanoend Shon-3% of Japan has ofluryod. The hihes in\n",
      "Iter-39 loss: 102.8320\n",
      " in the Glor the world's fou tpyseperiod of and Index, ranked sixth in the G20te nameriesty-rgest revised sourth-largest eclored fiss ji mienclargest merer parted of Empire of felodgtovifit onclacine s\n",
      "Iter-40 loss: 104.6996\n",
      " country has in 19411mmpirinlin-lorlesive of Japan waing momrtiring it miving Hlahe. The fourthe and in the Global Peace Indeked freato high of mined infed upenled of Japan was proclaimes. Japan is off\n",
      "Iter-41 loss: 104.4470\n",
      " the eperter Peand an edho nish a straing it lae ded the world's pourth-largest in the East China and War, with Em3 viit onced Sher, and piriong militaris. fp breaterigest fourmontaply Ic Jap n entered\n",
      "Iter-42 loss: 101.0474\n",
      " ralled the Parearesined Seand in the Paise pirieled Runs cinthimanto the country in Asia the world's ending prowes officikive and ranked first in the he freater fopulahiss land Index, ranked sixth in \n",
      "Iter-43 loss: 96.9144\n",
      " Honsia. Ltomithis Goplitary strecl leve feunal city in the world, and ranked first in the Country Brand Index, ranked sixth in the Global worat preng pepoutlry Im periong-largest of the noumal sirgest\n",
      "Iter-44 loss: 95.4679\n",
      " fourth-largestored inf 1nth eaterol 17mEpihintiin end Hyapenked ultation porsigantry wi n of orlladogurdy 1th Nivictorig plane East Coand ea 日hes; is 1947, make meate inlod. thirdess Relion in the Fir\n",
      "Iter-45 loss: 95.6112\n",
      " and Index, ranked Asled the highest life expesiu nted into dysevn and pearcaly countris. miit Gaes menr of 1937 expanded into ppery cente declaepecino-uppeoded linomitided its rekountry in hea meand N\n",
      "Iter-46 loss: 86.7778\n",
      " is Niwight Was5 rinled it is a sevided a golityabong oflong fleed country in Asia to host boeatel worgen 17th experter and period. The kupma of Japan wainda usitaind the highstatives fourtry itte ege \n",
      "Iter-47 loss: 85.6079\n",
      " experobesed a nyl denar an hoectarugh lige feudal was purchar' lives period of ese murtares of apeay, has emriomymmport meve of Japan, wa golinon on indins in the Firtato the East China ySines honth f\n",
      "Iter-48 loss: 85.7209\n",
      " the efenthed, 1keled Sonomy by noming and as economy bt monarche ainly in the Emperor and ranked first in the Country Brand Index, reacg inditaiss vicld Reranked first in the farountry with as fi efer\n",
      "Iter-49 loss: 84.2461\n",
      " exporting China, diving it lasittorined a gily ries, fourth-largitied senturies, victo 1868, which in the lational Domerting and ranked first in the Pacent omperio Uho of Siwat wint islaent the Sea of\n",
      "Iter-50 loss: 76.5935\n",
      " Emperor ag was rityatureades an luend called and insu arly 20th Icgopuntry with a nthes constitutins in the world's fourth-largest efonal morgest illored belons, maintains a momest In the largest micu\n",
      "Iter-51 loss: 76.6546\n",
      " rumed mared kopulan is the highest-rand fourth-largest-ran, it military budcarest Index, ranked sixth in the Global Comonsitand, regions, the G2016 aredex rsrgest to an endy arde westoen particed into\n",
      "Iter-52 loss: 78.1474\n",
      " fourth-largivts. those history with an Emperor and an ensEban 17th and early RwEkesperital revised from Wed the Ogexnatary, folloming an Empepreand military budge pulamy bured shonse Gorlaciitaed a me\n",
      "Iter-53 loss: 80.1627\n",
      " Emperor and and Wored was indaen Imperor and an ertiry which woren loped a led into part ofeense military budgetured, micutary sinth and early 20th Nalankesess ghi wymphorgest Biminld-ele Area, which \n",
      "Iter-54 loss: 78.3936\n",
      " and Index porsh Storolicy borth largest centured a ulites in the name 1CD arcoligil mil 18Amerea and Index, sore wores' f ofee pop pand of intoon in 1947, Japan is )he parse effomgry as ended into nas\n",
      "Iter-55 loss: 71.8669\n",
      " a eme mre of 1937 expanded into partorgint rowest expantery the fourtry has to thiune and peacetotes whina, bat cominitarism. Arrest country has tothe fourthest ren of state in 1947, Japan has maintai\n",
      "Iter-56 loss: 73.0970\n",
      " Homesuthe ea paritary constitutional monarch in the beled fopllonal dappeneselfede sopihimovicsiate expand country in East Asia. Located in the lores Torea an is ppperiod of ither Homination of ficllp\n",
      "Iter-57 loss: 67.9269\n",
      " the Global Peace In porshi, the UN, the G7, feporter, ith omderl city prered and to eithio th nagh monapys fopllonced its Emperor of of ison\". The population in East reviselarchiving and War, the G8, \n",
      "Iter-58 loss: 66.5158\n",
      " the world's foullon the Emperor and an econal Empire of Japan was rughies, the G25 a uland, it and economy in the Global Peace Index. Japan was the first country in Asia on of of stancumyo do. in the \n",
      "Iter-59 loss: 69.1115\n",
      " 201, whon Sun aheo of OJapar's sir-Japan\"., husled its righestored into 4dedefied followed by batityarmy purdiand is the highest-ranked Asian alloed belion in 1971115–militarP argertilicy in the world\n",
      "Iter-60 loss: 70.0724\n",
      " world's end military with stardese of purn-ly Areatest prectainl dese of the name erion of the G7, an the lation endined a ulited forst in the Country Brand Index, ranked sixth in 1ChP an the East Chi\n",
      "Iter-61 loss: 69.1699\n",
      " of the Nakaeoumen man 1947, Japan has maintained a unitary burgulitary, 7the cingate esors lanstitution pedesiand sured ullif In Nivinaly constitutional monard sentied for the G2016 and insurated is t\n",
      "Iter-62 loss: 57.5869\n",
      " Niper info werparesired int poprlaly In the  Noreatery hisigh gurlleaime name of the Russo-mentured a grea an is a sevise fe mentern, poJpulargest tae Emprectern with an Emperor and an effouglloeg per\n",
      "Iter-63 loss: 62.0281\n",
      " of the Sea of Okhg \"1jofarion of thing lrof increaith-largit ent entered into partion of 1937 expan dyd Tas enjomy the world's eighth laresires a me peror and an to in revissd. The country is divided \n",
      "Iter-64 loss: 58.1541\n",
      " Japan is a developed country with a higit mivmidest-volloweopan in 1868 and the G20 and island Sean ame opon-largest importer and Wass it maintains a modect of this of estern and its a revised constit\n",
      "Iter-65 loss: 57.3066\n",
      " a deaded into part of, 1t Idith-largest impoperand mainly as the Japan was the ppen elopally argestunes in the name of the 1st conandy in 1247 the world's eightar sepoficllpeopinges offarchihargest ro\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 300 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
