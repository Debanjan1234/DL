{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 106.7223\n",
      "ey)vhona h e   tleh Arm gtoteais ot  tnli dpt  f te sido, idnnfala;u Wtre s loutATc Ir nE h\n",
      "gwhwP aie\n",
      "Iter-20 loss: 106.3389\n",
      "e-ezisfe-e rttt dr R ao,O5Un viulado otl tlasn  h4itfipe iea ioasd uap nl1pngdphtn5h W3pt itdiecorheh\n",
      "Iter-30 loss: 105.6320\n",
      "eJCUAIdon ne\"tgs  Weop sa el suaoo nl s lhtyd um aialdpyhhm em  e Jn7d hpat\"h yd e gs ccAthr.srlt,lai\n",
      "Iter-40 loss: 104.8859\n",
      "esN;Nesle  . n  ta eapoow ;wa anaruhwtmtooGis nSodRea loioaxl -ttdar ilarnrmpamta iidncmnytln si3snhy\n",
      "Iter-50 loss: 104.0985\n",
      "eUhNd1 nzwt li rx elaici manwwTn Ekaapa parle rr ahks1ls a alphhincrdalintat e d As2hian oohud wase日e\n",
      "Iter-60 loss: 103.3099\n",
      "eJn uwmeialantael  tmynvt t oonan6a.aren aate  mh,8tenne, laoraipo  , esoecn iokto olsw das   ecrhehn\n",
      "Iter-70 loss: 102.4708\n",
      "es7\n",
      "kcit(x tur rJs Jaythn  7AorgtolprpEsnih rtaryoff dy arhwa thkap nspe ,ro tha htt soarri ed oioiaa\n",
      "Iter-80 loss: 101.6033\n",
      "ec9mKln etvi cspan  anrlartsrtrd Rn fa ndtapoonr wnhaddopicfiSai ns  e iwettet oiet  1t1to tnoriTeaI \n",
      "Iter-90 loss: 100.7300\n",
      "eJFeno hc ihyn9 mTe evau mrcndes ioEJriuontwyn1. siaone mrcgpeiloapuauheh ndAaSr,cuDuyctJu'1dthenrmde\n",
      "Iter-100 loss: 99.8719\n",
      "eon1hJytc8n  arrth  ohisg ges fnre6u eotn wanc  ds nn f .ais  y isulplale elreduf ltpod,t1 pshe7itc .\n",
      "Iter-110 loss: 99.0434\n",
      "eWpsob tas  fintogkgf orinhes pt in halde drsaou lreetao mheke pdas rhe mnontee n bagpirathe日rsA i,nG\n",
      "Iter-120 loss: 98.2507\n",
      "evrelit ig Tulsos JaAdrrimege ifnge,-a iJnanpocnoutrichale pea eonNson rcanecest aDa. akis r2wtep ,ma\n",
      "Iter-130 loss: 97.4930\n",
      "etc(enomrt ed gshe i eerle yesbik ci dcaes 2hiill cJTDinu t Atheoktes. Cdaw, eoth\" tkoibgHlnfus8 r so\n",
      "Iter-140 loss: 96.7662\n",
      "e Nyl i.tman w\" flpJa,acn9 aopest d n wyf anue t theiso nanecran drbt lAwdaA0nr liitechv  W,a it homw\n",
      "Iter-150 loss: 96.0674\n",
      "et2,d rnRciel yutrlf lemupNaok lhefidu an taf  maetreea tonhehacn lt7io 8 ta5 ri nin cd aplcwoo 本umes\n",
      "Iter-160 loss: 95.3962\n",
      "et e ronrean  utinldy ihe, c93s risoCtars cirt m wet i pee pedt cyn med,r:masep Jnlapot csan whe sari\n",
      "Iter-170 loss: 94.7550\n",
      "eWdn oTjein oTotoprONdar. anyooolethal w8oti l.ind dala1 laraee a. waWy dcexh rata txeNaEoruwleoerSin\n",
      "Iter-180 loss: 94.1475\n",
      "ekuchdwd, Cieae ci otiSiaTn'oapcayd ns,ndeoo.nttrW Weto 0ra dititt'mirecsherasois iNtherNwa,id oyy me\n",
      "Iter-190 loss: 93.5769\n",
      "egicat is fodni7s ,hisand fpou6p gns ho oouleuuJan sh thed og doinmuaIfn gstyr rinre wonrii' rhd ils \n",
      "Iter-200 loss: 93.0432\n",
      "e日OroureoGe is 'an thomno oris Wfhe tay -and rosdr soso fulonota ios bGde ilxle.1k nluin me'cJ can.ai\n",
      "Iter-210 loss: 92.5428\n",
      "eJbther andofhe, t piin lotih  p lean oipatd in an laoleIna ghs  Gnl .fanaree ghkr fiIdw liinmy se 3r\n",
      "Iter-220 loss: 92.0721\n",
      "ephljlam0i,  .ods, aoaten ninol co2t atdet tn ralic inild oingi0n Nheg He spnmthth tn iipe ge fontes \n",
      "Iter-230 loss: 91.6270\n",
      "eEoSol'ore%9 pTsaweJeit bhe  mretbnw wir rbe yhreteves lifosrhan o. ns rnkhit ilgek shivNte ,fmald ha\n",
      "Iter-240 loss: 91.1877\n",
      "ef8;ofsthi phae Fame thae the who-it . ooptn 1m rhe un keInyp,  in fe asd ih ,6n wnnorllc,ias s, inhe\n",
      "Iter-250 loss: 90.8017\n",
      "etsbp ind Rpe maeide the t4 an 8tre  .rhhe fil fann inso ucrnrtut.  an thelat tu th canrikd  teatecEg\n",
      "Iter-260 loss: 90.3890\n",
      "eCN. lttrlen i ifo wct npgyfStah beke.an farbyy in entcloI'a t in  yar, in 9Cc waous wainiked dg dnss\n",
      "Iter-270 loss: 90.1009\n",
      "eRgysd e 1,eskate kos n viea1ee in sozt 2ns yetx miatii  SanesI.eJcasptI nd9 af 8h  selpird es thh Gc\n",
      "Iter-280 loss: 89.7683\n",
      "ea9 op Kand wh ceetlaJaakaHBs thzorihy fderentut cutod lathe EinaggvSnnsx uarto. foawe pannan ei as o\n",
      "Iter-290 loss: 89.5722\n",
      "er xmes mory EanrhhcaF muredaldd inodltinif mtalesk su''ait oul de and osf,ogerye .kon chuenx she in \n",
      "Iter-300 loss: 90.0997\n",
      "en4d uhrnled Jadncatigotge  ao ilerleam geanke paft marevan ih in the Gsg 1heanJ4,sthe wspgt\"eng e re\n",
      "Iter-310 loss: 88.8712\n",
      "e5nervy Gs lcalecty lnre bon isthet-2Hlsmshe Ihan bieuislWraplanevte forn as ss  ouandea ou te Aay an\n",
      "Iter-320 loss: 88.7784\n",
      "e\n",
      " Tm7't thetaJa2in  Naf One rop,oah. apek'h in thebhe w.peJon pel Rh Bsmmeedfd ca nsgoth lcaklt  wce\n",
      "Iter-330 loss: 88.3177\n",
      "eT Aprandte1oof th rolakGe. iof io iE th uapeland rgol ofy aner mtecta topa. pe oritae vhe inigee ome\n",
      "Iter-340 loss: 88.3710\n",
      "e  ocatte. laroth 4ts5thekeftWof. znd ar by Jfoss xepdreteaccom t iogty  icuondstorine ss t, vatel in\n",
      "Iter-350 loss: 88.1708\n",
      "ebofr whes the an0g-liml1olitie cond ,ole Jalwin th id cond Jeraral in  ekeptorin chy womeed Jannd ut\n",
      "Iter-360 loss: 88.0182\n",
      "eithe or 8-bo,s anethe meth,  iar naldiiom an'esotheAty iile the fithxddwinl iat lfelise cd nlpzvitn \n",
      "Iter-370 loss: 87.2813\n",
      "eU.d JDpdl7 lanh and isxty 1p mote anse 2n ad Jarl'eed 0itat chiznd Tnurisa y in st and imian ava he \n",
      "Iter-380 loss: 87.3088\n",
      "es paansie bs GPaneWe meoro.s poten rare fbarcouies ouenlie theJkeipin Raare ff oreapercopins Baolt 日\n",
      "Iter-390 loss: 87.1907\n",
      "e. wanhort aiotn jbgr5icay sh meto rad myse thib本 rontoac fis as'awho本ukpopant lIiill 1is te wilagi t\n",
      "Iter-400 loss: 88.0713\n",
      "e's Weroe the umet ranli deo f rukcofiT fSf dannaran gou\"Wan ind wiron dy 2inare iag cintan il  ond a\n",
      "Iter-410 loss: 87.3344\n",
      "e8Bld u9danhd yh un sa ann noeurar ihelafrt chrmals oPans sanos rnthe fhes sof  6ian ws Atihed cur5i \n",
      "Iter-420 loss: 86.6119\n",
      "eth ih titreathl isosmoisothgrxpanges ranej, owg of la \"daeaclons \"onowd dofrsi本es rhe Jartoikere int\n",
      "Iter-430 loss: 86.3789\n",
      "eh1 ingat tioan ufr insache ourest nd angen neger my tuors te r7doc plInpas Jirge e poront more1rM ge\n",
      "Iter-440 loss: 86.6239\n",
      "e\n",
      " pealen r1u. ra thei ans Htoly ed toer tnt er.. the axurixore rmed ,rore anore mpepar  nerint ,, on\n",
      "Iter-450 loss: 85.8961\n",
      "eNLury' teicot Jnpa olla uapy Cith itheitn mere mad  aandTd of thle Eurrt eureant ona0cof  mertareaAo\n",
      "Iter-460 loss: 84.8784\n",
      "e2T iolalic Peiconespr estirh, treat, red'– comgy caonsir vhy furte chetabe mintse oponElgefat iht 0a\n",
      "Iter-470 loss: 85.0283\n",
      "eu–laJyss  issth wy as tsir Topuratis. winto co watha Elet thesi. d Ph winicgegscSin E4or 3h woARoure\n",
      "Iter-480 loss: 84.9580\n",
      "e\"wunumllaned whe slaEcop9lore isrith. Wmr.i omt r, bof ware Jaminu  fhte Gollgileatn piy e omed fare\n",
      "Iter-490 loss: 84.7718\n",
      "exond Jautep 1urig ltorixn, jiof furi,trogNesth inlc anae khenth fn rum ve as oshiif Kyon t–mwi ulaWe\n",
      "Iter-500 loss: 84.8134\n",
      "eBapes JaUnaar. uPoloc copanine  ioate ehpekan weokGeu ane larebpendn 6es Rhed theusbi1st figwhD Lans\n",
      "Iter-510 loss: 84.9582\n",
      "eCf\"6tndiin lesgetin leesont an te\"tumlape waind  rury.yvCn otenfrud s inoe findons sofsodnuron llae \n",
      "Iter-520 loss: 83.8388\n",
      "eNShade Sicurif. the in bed. Chte o本the an sisabnuld cy dedopy, on isikoerde. in cyy flusy E7rd . Jap\n",
      "Iter-530 loss: 83.3043\n",
      "ewInd poryat in Wwpares ifa wy8imuve\" pret i alaeCe ASte ipi日es rarilu0the ta cin. bot eawGof tikimig\n",
      "Iter-540 loss: 83.4394\n",
      "etit esax onttryed anoinutirirde9, 6khaa Df rtturyds ofme festbIom5  CWinwote ecoan de  ovt 11blonsan\n",
      "Iter-550 loss: 83.3852\n",
      "ebH5urgcasgy Niitsc lhit mhe  flNtirin sigig urte Gy ovored wutisf oIs thliore napance4 Itokosg cgre'\n",
      "Iter-560 loss: 83.2524\n",
      "eSweson tomei irutas whes Japaly  Tscisiclanco2y gh lakar :se iv mapende ilg whosekLanxeggGgicorthen \n",
      "Iter-570 loss: 83.1228\n",
      "eF6 the laisggomair ihug Rs curansity en iglerte En,iphl omar Japy critr ctol inta milapeomooa rgary \n",
      "Iter-580 loss: 83.0153\n",
      "euRndI n fiins  tasid tad -taeit iid so ctorili1 mtopt rtcoparge axcurand Jamped on sh ti amand o88-3\n",
      "Iter-590 loss: 82.8625\n",
      "e)uin Copaparleohcars te irluucilas  for the the. w8- Iopar Jas inse Isperand the Tan ie Janau aic ls\n",
      "Iter-600 loss: 82.6313\n",
      "euly risn The uicheballiy ilhiss railutet se pareigte of theu:lhagtasasselofy randnd Lsdecast nopange\n",
      "Iter-610 loss: 93.4220\n",
      "ednd. Japeao cp ippratt napan iosericreopetherpt8 8iwopran pppthhipun paran rtindrTorha5 pare5  holha\n",
      "Iter-620 loss: 87.8486\n",
      "e, fordatles Aleeiti Nioance r cotl mif lanas onf mpovi sthe UW pangn wos whkpe\",.lssoJ, ihal aand. p\n",
      "Iter-630 loss: 85.8655\n",
      "e日-alaaDo and Nrun pes h.\"wis hy Js inatore parip mop micy cf dofy th.emkeitothe EPu1cy mo0ocon nepro\n",
      "Iter-640 loss: 84.7032\n",
      "efs 8n Kcote ted Jpe curinelwd fi 9mrlon i% of Jautl tg hh GEmelelkporsuran onds Forh' sorcy his9 kon\n",
      "Iter-650 loss: 83.7763\n",
      "eNeit Jopband ganesrous vo:lurin 'Hord wanc0ocUd e balary concorsge theiTe Gic linvert the Uon om'Us \n",
      "Iter-660 loss: 83.4321\n",
      "eUs lan'd minhins of the wasu imo of withe laf un ea 5ed at. 2ik, ,e whind foWastona Dio nkeapethe nr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 83.3437\n",
      "ete ind rhete oios wy f amonipt imed of untin aunongaitithegeo marnit e oppuold CarhTho fin legotnth \n",
      "Iter-680 loss: 83.1588\n",
      "endjotic.pangd ipaphe Kurnd cor,the t fallmethin tata iosa , ipenodu ind of ipAtopistalith- voMpurerr\n",
      "Iter-690 loss: 83.0148\n",
      "eed Han aes inc tely fithan ion toto co,lgtpan lced6f Hilatee1''l thegds ingte oChes thgievte th tohi\n",
      "Iter-700 loss: 83.0903\n",
      "eOry ses ongurelou tor i1tr of rhetreona hs inNxpped rhr gosigil hed. J0an C de,ouhe nhe Hld otoveUp \n",
      "Iter-710 loss: 83.7433\n",
      "ex Sesangipese talresteing, anerf wopethutar Nisgis. wh thes ihe th Jlad in aus pias ahat oc9, Euron'\n",
      "Iter-720 loss: 80.9235\n",
      "esbith 5hd ohuth nore fwm y tnrysvy J8bye 47op'a, ofiorl-arltalvesd leure yipouhe Esth is lolitd onot\n",
      "Iter-730 loss: 87.2750\n",
      "e an capve n.efhr ts  Se arito  am Nerourore ridt GkoPas laahegathee Dhilh bhototied Shea4, e4E, onir\n",
      "Iter-740 loss: 88.0750\n",
      "e7(frtloac frrtv  Toric yth u2ed eoofwoflpem G;ond esogal mtehe Shis d d Wilareoexufmryed .sa iIOanba\n",
      "Iter-750 loss: 89.2738\n",
      "e Gan iJan tit. us ifxGarge hhe ev-aSegegwolAcilesllasintheinhun th 1of kyorin SeoWaG is W and arlare\n",
      "Iter-760 loss: 92.2212\n",
      "emGedtouTerr cor t aciAptpinan.chivs con, hh sipon pa antep nHe th  s avx,rroran theJapwr. 9ttatnsrin\n",
      "Iter-770 loss: 90.5557\n",
      "ed Hinsasnn ort ick segioceampu pht iun ausende P ieonJ lanoeato-atteeJi4toel1e5R tiand Apcripaompedo\n",
      "Iter-780 loss: 89.4798\n",
      "ekApadst ioniar Worouint n A me9uhimaualecasn taronn sls kk lhsGreredlud cfl a f Jant  hlewrn canr fe\n",
      "Iter-790 loss: 88.7780\n",
      "ev. dfon, ton jitta dJaa1al8 hmth   nooprl ale paticiosted, theiond Hs lanth Citor tl m reeciananl nd\n",
      "Iter-800 loss: 88.3680\n",
      "ewos tegit s ayhs tofat-diseop of win  ort Si 0hesti orl ciu, as, an tll h maf 1dr ditg gomeoitinapur\n",
      "Iter-810 loss: 88.0551\n",
      "e:;本Uany ttthe ao un ararnedid inces er tnat s p9-tepatalgs8 thinarill holh Eraulitheinuun San an rhi\n",
      "Iter-820 loss: 87.7944\n",
      "eath onsti otdas es tl rhund  wfate inohasli Jongn2af mir rilas 9tuyior oNon oir fre iretepaper ofles\n",
      "Iter-830 loss: 87.5734\n",
      "eand ind niceUed achy Cleor pa.ar 'agrr Wames  sw8tw tirf woarlletro8loli fiaopyy. mmeu brue\"teand r.\n",
      "Iter-840 loss: 87.3821\n",
      "e Hlalarge on e1lanldte tfeGg ano yesFprewtmc sirancer,d p 6an epbhif ytl Gan E 3x inodeceitexd sif v\n",
      "Iter-850 loss: 87.2123\n",
      "ep.1and sesbT, Te a pecDos weatge cohNacy oinomd his iyand gna losse eotpare Hoa  in ah an p1k icmg i\n",
      "Iter-860 loss: 87.0579\n",
      "erlegsg tnt nirly msint ihu' wps D. is dwiirate 4oppuret,typed.tIp  bhec-way N9 is an oICionld v1  an\n",
      "Iter-870 loss: 86.9146\n",
      "eke khervrvold Ctul. an  Iary locinhe7ors cocrtn 9 f Piceniole r8isn coNs par,gap onhn a. flesiond xh\n",
      "Iter-880 loss: 86.7794\n",
      "ef1', Fit,eoAv'a Soeorh Wranthys Kyg fth n miukico. ihu'otal fas  ofiith Im f aopatocilan sopa ed f7r\n",
      "Iter-890 loss: 86.6502\n",
      "eMHe atie lottap g tioing Si,arongantiecr vi1eapan 8re,erwr we l Ecan hanthw aarhm she thyeeotha d pe\n",
      "Iter-900 loss: 86.5256\n",
      "eWred Is'to atiddh  nas tapei tenmpven mtentsobeap Ktterxary fundleap ciotileEd'nf oporeskho rp cist \n",
      "Iter-910 loss: 86.4045\n",
      "e'd insto tal 9,a,nl6tsiIiren2 Eoro O mgoahe iltphoturta. rT apy desued .U fi ipiropintit asanSteesao\n",
      "Iter-920 loss: 86.2859\n",
      "eds Ild;aatte temarlap is siseamehxdhid ofi1s ra an d Jdend txun ins an oman. paas  aps i58riludacula\n",
      "Iter-930 loss: 86.1694\n",
      "e Banlini o sswt cl hited tyen tho 4retite he\n",
      " ale thuryy fats Tchesheo  een yedete ae an inut i w0o \n",
      "Iter-940 loss: 86.0542\n",
      "e xed  pa,aor th ban tat h G1saresriCh in 2s,orhinthw Fonsr opeed nfun iarra ces mpoark8 c tbor.win l\n",
      "Iter-950 loss: 85.9399\n",
      "e1d't I d rKaxlottrecthend pipon onmaw dihi f Uyap t wednet he\"duthim, saro9 ipes dom wpas tarheh, a-\n",
      "Iter-960 loss: 85.8260\n",
      "e,k antt insaesed ange are,oowind iugd Khes. th  atitle ave iDutlefr ouch ast haladircy c, snegucioca\n",
      "Iter-970 loss: 85.7121\n",
      "eted sac: th0 womaesulir condew , motnn onre3e rrirkrdincilmpanpmlis. ror g aratir irt e JaHytinihara\n",
      "Iter-980 loss: 85.5978\n",
      "econlarloc8on thariin fhed Oevion waentig piraeand mflc-alof lasromceelithatleoebleranecoreine fowhef\n",
      "Iter-990 loss: 85.4826\n",
      "eKy warss inI Esandf. ihet.sop7-if s iNJ8apetu ahesureithitincilidtWare wtu th tig u oNopy ofctoNi5ee\n",
      "Iter-1000 loss: 85.3661\n",
      "ed lapenedlCa por y v on h, wekes  sgs robed pihsvaseho turintehekhure 3uthe ismas,iasio bd 2poft ip \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e951550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
