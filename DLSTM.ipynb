{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    # P_dropout == keep_prob in this case!\n",
    "    # q = keep_prob and p_dropout = p ???\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache)\n",
    "        else: # train=False\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000 loss: 7.9514\n",
      "ced of siaillisitogoecgtest. ghitl ap as laides\"-Japan stountiogttoteficgly. Furom6ithe lionses. fosu\n",
      "Iter-2000 loss: 7.1681\n",
      "cc peoprefoposioccanioenging of aroctatoeligiggtto ontttetotengescc eseen,u,s,uea, H o,s, nulicollele\n",
      "Iter-3000 loss: 2.8108\n",
      "city propere oa soarl con lstar th hineeuparyo the nallomby mn n pugtugieglatner 日本 rerea,  Japan whs\n",
      "Iter-4000 loss: 4.4521\n",
      "city propere of Japan catind frrTccconent t oivitogingitituginging enteotarynto t anfefsts esa as) it\n",
      "Iter-5000 loss: 3.2757\n",
      "chy with an oh andointentex5ntante W pint IIne insttred regsexra Sinese ases Huma, suleHousourlucimii\n",
      "Iter-6000 loss: 4.3151\n",
      "city proper porerex. Jalan con dse Himeofoococoloeeonottrarevesedser ar n pangcoutanese f ar a a wt p\n",
      "Iter-7000 loss: 2.4252\n",
      "came to an the coun rinestnopon-tountetationean cn fed lroue hend fimsireururofeufJopurer Impino ount\n",
      "Iter-8000 loss: 3.9700\n",
      "c Games.\n",
      ". an Ocenea ar whe  und Wtrrporette in in terpaopoowttetitioionticgity ctuthtttttttttttttiti\n",
      "Iter-9000 loss: 2.4374\n",
      "capital cititoionttituiigititu intro, Sino Od. o n, nucouutedolccs ind ffou9ces, as wictaendewse pJap\n",
      "Iter-10000 loss: 3.1979\n",
      "city propere eas of a wind as wind Huln,, wisiUUppec of nuitutitstitut thin eaed ch the Eos rom n wad\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112969358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
