{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    # P_dropout == keep_prob in this case!\n",
    "    # q = keep_prob and p_dropout = p ???\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache)\n",
    "        else: # train=False\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 110.5852\n",
      "el0btc\n",
      "uesds7 wv hHWhu  aa日preheaatt te.raareu edi dc o\"; i 本poii (hnot rp cfca mtd  aslrceCdhaeeuo p\n",
      "Iter-20 loss: 107.4541\n",
      "elg日 SnWeskfjips ly odOThotdnlte:h nutTnc woe tst  aenmeod,ietshrl ysa tsa ahu F a aPasoi p sa,caor p\n",
      "Iter-30 loss: 105.4295\n",
      "en –cte6eaaJ dtiex  a aIt ep1zfla.rlhisutniheshepnr. o aeixpopaee s  aoa ddIeacuiuotuenemnedslotne,' \n",
      "Iter-40 loss: 107.0768\n",
      "ey0-gdE Joeoy  apids rtlf e idmeaim eersac  eysno  hoeamc roemi a.o  y9rirnes eh uitrgfu rea.dsecrt日N\n",
      "Iter-50 loss: 102.1308\n",
      "ekJf e ralHhogho4bea srctae.nebou  ayya1niy sdorrJmmhuorcctttwCta yrea8 bre 0pregie siea sc theleteik\n",
      "Iter-60 loss: 107.4662\n",
      "e:MWPbxipn 7rrlrheicRtospektaar etgitteoo ih iai mFWeo本ska  yi,tbdpaf ehcrJxinepahteecfRitnrro sstua \n",
      "Iter-70 loss: 103.9386\n",
      "e;Ctmii b ei \"fuo ya0x' looiertiOttehihbo,ti tit das'he ubniliNtn pah.d tngd wigUcaasaEHhanmied ng, a\n",
      "Iter-80 loss: 104.0695\n",
      "epharoephei crtinbrvs  pet'fmtd i ohfrttk eh J1crhb mTl nitE Jl8iuhr r vEaEe a'py riwo tt y eohioeii \n",
      "Iter-90 loss: 101.0184\n",
      "edcect tce k.ta7  pstiba pa ahe cinfe li8to ghwa7 sCniman tr cdrd-bil  cu.paiaited  eonol en.- aendaf\n",
      "Iter-100 loss: 101.1034\n",
      "eta.ofvrn n Et aopmiagkgd pL r'crhoe e(xkh. ittr eche U vhigfnol  caimeds tpe lai 2e e 5sh fcarn  nek\n",
      "Iter-110 loss: 102.6865\n",
      "eTcy1rcn e be ert%nh pi ugry strsw derc8aleef a oclectse ndhureor jhil ab eWpeesc ot3hio,sb fhsoe  ua\n",
      "Iter-120 loss: 98.3916\n",
      "emoykineCi.ox ,a tiw, srpihaie e.tc–dliha to sckuJteaty rT l.,a fon\"u'e  ulaefi thigeinsdt, nil pavDe\n",
      "Iter-130 loss: 95.4733\n",
      "etcapaTtisesis aoteand ,reitrleii aryu Irnrr minet ona ntaoa s, wiu tiatesotaiel atipers poJacatn ccf\n",
      "Iter-140 loss: 99.1585\n",
      "etFe  l thueehK Renceaotl, -1lpeivsrheasne  w o2n c ahelfDo ukpeenfitWrCdi\"e fr rWpdit aaist 8lponue \n",
      "Iter-150 loss: 96.0649\n",
      "e :liuanuki at he ope omtopntd app1mins t0is1d 1 oGry lStore dd ujuatN8 ,-cer,e mu0ifi tfeW.diJ7owudi\n",
      "Iter-160 loss: 100.2656\n",
      "eT\" f7n p ibes theNontpCl areyw prNter int -)xa tne w in1s ttapetri.cgaCaritg ail mat re, sr iy aea \"\n",
      "Iter-170 loss: 98.6283\n",
      "er4indrfg mwy ateaatsl yamintpane gSno kuwnWi arsur bfda.eomupheleand coag toJ i0ySnol icei wsdthe ro\n",
      "Iter-180 loss: 93.8730\n",
      "enL prdtIi unsldsllapaptiliy\"o–s erntan in ef wWt iomes he pnsruhi amnoSpyhy atJagNt pmpet.adnig s Sa\n",
      "Iter-190 loss: 93.3085\n",
      "eyn x. at- 日ttisd  ionns siohlw, anr ket zkiin--n lrotuey  frdt hpin N,er '\" r1edto pblvpsolyaG iands\n",
      "Iter-200 loss: 92.9280\n",
      "einOodf we tuog nO f onhe 0lof anni cilanke n.o ethehe eesdkborfcoarluyi ini fDtareSh t he curope agr\n",
      "Iter-210 loss: 93.6193\n",
      "ehtN Sl,te ,ppatdeetuan i'd sr, TfdoruxT8Grdnpcoeit taed ,oh. iwsr hinocwh le mtad axreco-j thiryre 1\n",
      "Iter-220 loss: 89.0781\n",
      "eD92gsed puohn ton lis sitrcr ychith an tiiitsgbo pbthelryetd AorJlas 1odp natedie fe fo hoen lIn tht\n",
      "Iter-230 loss: 89.2798\n",
      "e6IkRinn 44. Jbuticut auhIl ulApin, Jtes.d gaafL oilmeoannsews ahathetri yd pa-yms mo8uarg aS mhem ir\n",
      "Iter-240 loss: 93.9014\n",
      "ed5IJSesolas 0KhisenEl Oanlsmos  theennuOd  fi6tit  ryurism JeGtothr uO4d an mgesr  p,pae mHrgter.9 c\n",
      "Iter-250 loss: 91.6729\n",
      "eGJWtitam,. orcaUan spon owin S2umarrs esNanr, Jrasy 5on Thohring tho Uaat in i2c\"josd AS inthen ufr \n",
      "Iter-260 loss: 92.4071\n",
      "eea wanes sAhohaky oy hod tpe fho Jan'faly,.predonand aanghein. untenSdrtol7s heat heoO pap thix\"wiso\n",
      "Iter-270 loss: 88.5605\n",
      "eosanus bnricel rhonl oiyntt mxwaetrse an isomnrpeSnanest\"k: eoas riny itk xr toles Leato a日y o8utkrj\n",
      "Iter-280 loss: 84.3361\n",
      "esiA ,- iwhat whyims 1h ,ob Gish ghh7le kdhe mafuictlinhhs hinh a, oa twers iKn SmgFl gsut piof –nan \n",
      "Iter-290 loss: 89.1251\n",
      "e'xP oubecirluithoy r,C catx.lceuN5'of i7t3 JhenungIstein attnse Ciyhe Ea, pn hef t1wt s in iotuydes \n",
      "Iter-300 loss: 87.8915\n",
      "e4. and th iuo tRzbtimr'roor' Euad pin atxwitt shmeaN if, NWtnts orn toIIs oan ian tarslocn aroros Nd\n",
      "Iter-310 loss: 88.5477\n",
      "e9E;hey uuanacdirittighhoaCswe Maf n, Tlr5s o fisponydeno tindTth c1rsPy,7on2 lluolauocihedD Isuis f7\n",
      "Iter-320 loss: 93.6618\n",
      "eP2 aandoankintirlkincdeSacsrinan  ias 1pparendr uhoef.dicar it man-hkoas dtliale  ureef\n",
      "i iv ann fmc\n",
      "Iter-330 loss: 89.4028\n",
      "e53jeilonitJadr sanc wirwis.rye' apinNs iures thJeadggd n oj9mef SeaJangerke diveroxis thi errant lNe\n",
      "Iter-340 loss: 88.1098\n",
      "eHWe-iAklptuote mnUt i\"ocam m6 2nrlooterleif Obe ipand ilh eghhtd iweuo \"iOD ioh. ta andPald amrg. lh\n",
      "Iter-350 loss: 89.3752\n",
      "e an lciotted We-aSduk aib Ite coed inth paapes h CuoadseluTcond zith 3 4ryp aies in. cin hen mofelod\n",
      "Iter-360 loss: 93.5166\n",
      "enrg,iannd h oni d iany' 11. Fomgeln Wost  日ond eroJgman6etsen. omlstvianwr. s ard the coinedjkf\n",
      "k RI\n",
      "Iter-370 loss: 86.8146\n",
      "e3b thonomata. bsxlroja ma6l Sf 2luyent8c )oand ulfs a trelithedeho ou df anu ain amondd ion, in we l\n",
      "Iter-380 loss: 84.9789\n",
      "ente aly  as ig ihd.  ppalanse Cu tai. inhvcthe Junac fJarpG erurf  iah  clresed onsxrim. Gotth Eoofr\n",
      "Iter-390 loss: 89.2022\n",
      "exo th D, isredurlathlee .s weo okbory The in folmcith ernl. amsrh tCe Simle on  rinrd oan .etae tyrt\n",
      "Iter-400 loss: 89.9753\n",
      "ellyy an miludaice. corSeit alrhet likulirho l omigond ar; Tare fiI iosOaagtery .kinston od ro:, iDgd\n",
      "Iter-410 loss: 85.7219\n",
      "ecugtuaNoellest in Jdec Areas Ewh pupeon mprrirerit asi 2vlemca 1ta.a. Hmin oandport. lugl ois korith\n",
      "Iter-420 loss: 85.3745\n",
      "eKUy aacorunpontdb cmof urerd wb tinc9k- Jumr,on ra ae iine hfe,hv Ch arps the aiil romd-owd. otir  l\n",
      "Iter-430 loss: 86.5356\n",
      "e%y oh wIlrame t% tetlmle, 8N nhtpI,eciO, anirettun\"ued.iNcinginbbond tmdicss of anwf eheuntual kit c\n",
      "Iter-440 loss: 88.8007\n",
      "ePof5vonc hyhe topcinc ireg,toeuniud 本yh erhos Gopant heiadon lanoed manmiessd irith agd ioh eat annt\n",
      "Iter-450 loss: 85.0272\n",
      "ePR nate madantogre.tTijt oWrays if baereasgs frDorgajas gB4N iponD,  earoetdaS Kefton tser ithinn cf\n",
      "Iter-460 loss: 90.4179\n",
      "esoWnanand sfryacr vhaojFese alyeny - 7pef ure aan the he oh rcdt rad idh an t aho,oopan Npuanhin we \n",
      "Iter-470 loss: 89.6009\n",
      "e\"Hce farke coretagiis nom bhitlondstt8t1–Ilo8 aEg ponon vutpanes tarl1gh cssenis h to fd ant Sity Wf\n",
      "Iter-480 loss: 91.5803\n",
      "en Owetden (e .heeonf anth ouitlkrtere neaky sattuCm. ae tax areestany fanli telocch ,id tufanrUi-Gat\n",
      "Iter-490 loss: 94.5617\n",
      "ehi of-einu6a n the uaad la, iTt ant eameturititih thisremo, Th-nle Do sf tan outhetohe indreretol Nh\n",
      "Iter-500 loss: 89.0272\n",
      "e7oinl. Iup apans ufr tames cotNo, Janprti wuebeupanlbw  leexith. mardd mrey th iNurtaatathorldi roar\n",
      "Iter-510 loss: 93.5977\n",
      "eySnB urcerictceemhoerim arer. na T)de hc toes ergte Jansef JanpUpeeoru ft gedha Ce, uHiSanfe uhciche\n",
      "Iter-520 loss: 85.4950\n",
      "ezse bolr uhaid. freucc2uy pth seUyycr0 inse192tThwr aransg thi t o iny ajdedsdhrct urr%See aGkh opir\n",
      "Iter-530 loss: 84.8914\n",
      "eGrl, (fs futee\"9 Eanoxdidit Janall wart'e nstin JhCaust Siturorasan. is' heann. Um,desJiaoph' e ndin\n",
      "Iter-540 loss: 84.4740\n",
      "eDM ofmein 7aJalansan' pif tulaoin .o 1p ayd tar liko.t iry t anathelii,ci7n6 slrthe JpCostIakn Japan\n",
      "Iter-550 loss: 86.1733\n",
      "e hxacxulasirledioc.s p5uy ivoNasly ed eae of fThead anr obst. hen as lipare cidss aak itoro to-onti \n",
      "Iter-560 loss: 88.0881\n",
      "e-nas b8ot ih th,isin contnd wode iton 9iSEho U0h'dat in tinEd n sipy A,ins oTeh8 Nf6rd vlhinoo7jd cf\n",
      "Iter-570 loss: 91.3134\n",
      "exdy the d uhes  muiaretftT,4en Jactapan digl shethimako-9mid tlith Sfpwisg iku 8nlura ey 8g loluea a\n",
      "Iter-580 loss: 82.9725\n",
      "erge thee Ig ,iod Cmpctures. ri-Eant erett oots ipourllig heujorld irivhh m-ipbe ur5'ecde hitm dsteon\n",
      "Iter-590 loss: 85.0092\n",
      "e. pheeoanoms Hia, y poh rne tn al iust Te ,ofgta.  he elauion cuinbot wmWiveest raas1 ,y of anan g a\n",
      "Iter-600 loss: 85.5689\n",
      "eT. one anIin ind thet uhoSs1 Sw7akldaith cesr feofcgvensiih sas ng nitierhieSin fsCotara, sod mfo1ut\n",
      "Iter-610 loss: 84.9172\n",
      "ep,rtepan omic oo tarey rha6s fboutosG the auntyisctor fmreis R,ico,d mt eiteluorere t Sf'r't to5orvo\n",
      "Iter-620 loss: 83.8837\n",
      "eSInI ao sonvis ist eot serue ba.l tet wopses iosd Hrmase Nobfn tate piurei mS f90toar9d ca ved Jinta\n",
      "Iter-630 loss: 83.6856\n",
      "erd Th. ony-wan trinacpon anfit \"to6 T Eeato einapl fwk9OmRoAe iola unesvisc pamtef 6arex2let pa, urg\n",
      "Iter-640 loss: 86.1483\n",
      "eritvcnNtym pangJarolde6e ,es,is-an lurknt cesan mfeTau db tannd si iAaulinatists aeadgeIm,son if. fi\n",
      "Iter-650 loss: 82.2839\n",
      "e\"kon onleltoW sianyth se JaAmes the,leiao rtarasee h a inaunxatrer ph lurpearir 日Tath puzias of wwia\n",
      "Iter-660 loss: 83.3446\n",
      "eotdas wherwuned eeanoane \"wr erdc-Japesot lertoun lecere oGedr 日nd an an Japto Wan tohe linltr. ghe \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 85.8901\n",
      "edpedrior Wos ra t, he wmis it hie in. th a6luE23 womond w7andcortInSnpenlisifg rakumuSias RAippta ot\n",
      "Iter-680 loss: 83.3091\n",
      "eWacand  onAaxuxind tn the wur\n",
      "ley Ehesord kHakusd bplamce rerrotl. ed Fnhee noJnote pathoranty cs d \n",
      "Iter-690 loss: 84.6414\n",
      "ejzha laol fomeurlithe iJapas tise fe firerTfalo5 Tltu O1nr Aeisnsetilath wheu as1ancaes(n.Unt wen ao\n",
      "Iter-700 loss: 82.2220\n",
      "e wu's threopnof Jbon anuc. lalindes t, TTor'sy  it Nn enscey he inasod cloins Litond irwacos t.r spE\n",
      "Iter-710 loss: 86.6183\n",
      "etd Nlarrgt5 an .6y Henana dh elketoeins sin ssioh5 wanky was. un omirrdd ogungsushe 2ered iIdpirorr.\n",
      "Iter-720 loss: 90.3069\n",
      "exFvm5erian se Th aur tute ron r. mit8lod ocitor. h ccunslo uhirisur anintan, Rirg; find h - eprurlan\n",
      "Iter-730 loss: 82.7304\n",
      "er Sescuere dad oty fC Coont it mhecunifarmeanIetp c-arrgeshd  whaNriil t xbAian rn aeitei7 nhy Rwvar\n",
      "Iter-740 loss: 85.0371\n",
      "eeS1tunry 14n1cChand sred ind AIl latJNcunust  29t5y.y letary yannin u89ht 0ert zoxto.4SvJynkco;nkm c\n",
      "Iter-750 loss: 88.4251\n",
      "e0iuLd an, The dshe (fis 1daof Japange che oyan NCheea a6kuLlintiit durlyyof vhe raistecokutais ain ,\n",
      "Iter-760 loss: 86.0368\n",
      "e:rf man onuya ya c,ho pnseaisil aan ionm wl. an. Jthicuprheve li1fkorhPde the Ihs 8t aEmsi ticthe uo\n",
      "Iter-770 loss: 82.6281\n",
      "es ppiurrest o8 thpreutand enleare tRer tare. n DootnkaDi,gh gdNef ciptar INA.thertyo Japared siNcGrp\n",
      "Iter-780 loss: 88.9537\n",
      "etatsbt inrtmaIr'sty cion oukeEC ntoth iNxaner Gger moary ton man,.o hirt whis. J6tinra ted nacp poro\n",
      "Iter-790 loss: 86.5948\n",
      "es anry Noin Ssitcliinagg thh oporur the nompanllyof Rf tor clcian 0  SCuxak lerlucealeaolr a liaderl\n",
      "Iter-800 loss: 96.2932\n",
      "e- ehe Aoct. ifemPxis JareGlaos GNthe anhe thaa, ah iag tem -pintd tO munaot the JantepoitroWs ondes \n",
      "Iter-810 loss: 81.7939\n",
      "eisWar, Rfsutsrcb(mard w0\n",
      "6 the caunink lilpminndBy Eaftsmuponsthe8topexrte as aso uppeanrto anteecht\n",
      "Iter-820 loss: 87.7708\n",
      "eIl tue insd nLi, Jaasas rtod ry 1ard h and i l8mlte ar arlius,e ceris isnmGt otunC logiocnkt7 nostom\n",
      "Iter-830 loss: 85.5786\n",
      "e2ed epamaedtedsTiDofid nho cs ith ape (io0galf wo lardl apa thirhdtipitnto ande tin hit Thogconll wa\n",
      "Iter-840 loss: 86.8596\n",
      "eSe\"sti. Rix wape2meotrers The'r Can. muh Jarenae if turd Fgje up clelep1escofurdyikr the thh Tfhemhe\n",
      "Iter-850 loss: 86.0872\n",
      "edhe Nirrearog cblroy nod svdare fhed 2sinlossi thosicand tho tndigadorot ho;d f\", fh9 Juppaner crp9u\n",
      "Iter-860 loss: 89.2374\n",
      "eanl asisst rivis te fikoed ud we rligh lfilrgcan Eneu\n",
      "be Sse  aptoo le gosme aroardlssi fir1 a. k.t \n",
      "Iter-870 loss: 87.5114\n",
      "eJaran lolrtati. Ehuuenand wsa JtapG upd ond pioge tanc R-ans1ndione aantred woheirs an in wke C che \n",
      "Iter-880 loss: 87.8545\n",
      "ent anr nk wisotle .i lotif orNhes nad kne(nkghe alelh an Jan aa,es antasor Gisa d. wTule pennusRpxJa\n",
      "Iter-890 loss: 86.2771\n",
      "ed onanSenti himanscadin Eoy cary tke r7 fksa oake pane won ilouthe Tipashef lmir otit i,totoomg as i\n",
      "Iter-900 loss: 89.1183\n",
      "evC. Aetroncdea inl tethath tlhagalathaose ulalad.. oas cis wigms cehare yKeoth. Janpane ohes ArOthet\n",
      "Iter-910 loss: 90.8213\n",
      "e-e ano sse Nimn coafly an sar aaolist urldvue catre Eman ts Sotf oipt oand esrin urlapf nirclin tha \n",
      "Iter-920 loss: 88.2502\n",
      "ean bhitath onsh wletei'edkredk ctelean Sf ns kertiis at goused te e8Lpssd f cand f Iarris thd th mrr\n",
      "Iter-930 loss: 89.0216\n",
      "ePr. 1fhunctre Janan tis theerin sean Smres osy Ty wourrdssso1 le istyrl'ceaunolderyadmr ini, pss nSe\n",
      "Iter-940 loss: 88.8744\n",
      "e apmity tonte Ced feun is -apasre cripsn –a in IA's an, .he wu tig we G2SdeAo Ga2-ch noir esvwr cwin\n",
      "Iter-950 loss: 90.0840\n",
      "eDof Thhein wo, Tyornleleerd of 9hJuanint fean1o w ad twetEire Jar Sminat moas omkisof baery Aest ihe\n",
      "Iter-960 loss: 90.7857\n",
      "edu theeane wu.ilo. ronn cfho7lpt cepstrhon. no nlf ohisa or matgymi sher 0o Jappane6ste an 1ct bin R\n",
      "Iter-970 loss: 86.5410\n",
      "e\n",
      "o0g, th Far4 epwmn ppextupdsere. unt thi aaldin oinpen on. in emi he the cU id'e Sn xive npelouspal\n",
      "Iter-980 loss: 91.9758\n",
      "e1mgitirgeind td Apxpiloserloru. orst in ed anf Bhc 4swo% whearale taines amgle\" Jpos atomodung cith \n",
      "Iter-990 loss: 86.6444\n",
      "e日nh Japefut ly En C reexes leompreif tpecertdtirnt evaparrimaracond irgit iCaon ss omprreldi any the\n",
      "Iter-1000 loss: 85.3892\n",
      "ecSen aate aedst ap ryred thi Gled pan ee medTan iwo . h hhes-lEed ouncil wast 18uzth mefor1manddin t\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWEIwxhgDwP8HYTiQceZ7vwAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1067e1320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
