{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    # P_dropout == keep_prob in this case!\n",
    "    # q = keep_prob and p_dropout = p ???\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache)\n",
    "        else: # train=False\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "#     for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 106.3546\n",
      "e;WD – e.oe adhsT f biemleenunse nannrntyisl ic. 'urb nhoo o7ao\"Cd s dpildehaao c pi hl anhliea o dia\n",
      "Iter-20 loss: 109.6803\n",
      "epEProhS nd iyg nii hoc -of  3yoecoe hhe ziiExnrn8bc Lo rhonb5eolan bhome idaah.Stnta noSoantverr,t w\n",
      "Iter-30 loss: 106.2750\n",
      "e;j1f6tWeyaC taado,i ieapsfonrhdrend   reo uih el tt2ittrr2o f hcopEhD teeryhcorgahastaenpiandarnid  \n",
      "Iter-40 loss: 104.5462\n",
      "eW.JaLtm9etirtrm idwaahehre r)iuhrldle\"nwfrirasc on htead nahhd ietphiaa ntrei C h-eatseplrhm,6n erCe\n",
      "Iter-50 loss: 108.9700\n",
      "e6W' l SiSs lt aodsa.oAn8ay eida.efdhuaaer risrurwhSs tfcatmlrckilrh   aoaorNi hior aoe arDh e,wSe  d\n",
      "Iter-60 loss: 105.6447\n",
      "eehh:Seg,i J, ulfgnmeapensIt uJ,iuy ittulrno.aGio ui  Goednsoai  cvog  wo rasAn'ceftrEah esluh hp本g i\n",
      "Iter-70 loss: 102.7725\n",
      "eU faoce  ehei e.yturtthhadensta0Wacttbo Girkar tnKalnrSteihnJ aaeelEeconisltdad iotn5newhru e foop n\n",
      "Iter-80 loss: 100.7437\n",
      "eucasernidnB  Ileitd cok coobteeo fh anateot endhs5istwauanar atnyss.untIren iglaea m m npstfAe(Wn re\n",
      "Iter-90 loss: 100.5084\n",
      "eGkfnxepd iisteetdhp2 %.eiangCestp y cfio  ahi S ogrpfn ryga3opnuli oa9eDTd aa nx hiv ies -ettlsAt Ko\n",
      "Iter-100 loss: 100.7453\n",
      "eerDwa, tmer deccsanH hicesut nr94nwj teitf7nn mbnah JStndesSa ir un ord  htaeinattthio wgeeebahgm.  \n",
      "Iter-110 loss: 101.1468\n",
      "eKd iw. an ptRi dcreyrTpOox owhr ferdttkcboe oy i ntsest1 av in tsk nd hsy amrediolana uctiepn fou at\n",
      "Iter-120 loss: 99.4045\n",
      "ehparreRowaheni el-naraat'tcarte Noyorus the i ohieecns owstuau sh'b,r pohreB itney1t ntu n iopheiwan\n",
      "Iter-130 loss: 99.5841\n",
      "ex,d usoxI haoi. oraKlsout  icatopesti5cpgmrleft nun crciow is rSna jmlcoere ist aini ncntpaimd strdJ\n",
      "Iter-140 loss: 97.6923\n",
      "eevttletuleinp.thgora eao th ffis inhr2keee J4raiivn'n sinys i, dhapas mi' penthk ihe  rkuiot alian E\n",
      "Iter-150 loss: 100.3994\n",
      "e,-jxu ekod uran ocetaed owWn mernLpean uadat oanod, E, okrrmg8ryto pr ,.'T9or2tt n efeuttl pisyEh Ah\n",
      "Iter-160 loss: 98.8749\n",
      "ekrindf ar Psiobaa ,dg otfan5ctc eire.P ui日he s intg  bontI bp9Ge eEre vepnon trnrardn STpJtoFoats td\n",
      "Iter-170 loss: 94.5213\n",
      "eCCht ainn aul anragm nanrcanMn igori\"h DCi,e wed ihdohth addS,lnomaifar ecrosol htssfomlntery lhnsee\n",
      "Iter-180 loss: 98.9573\n",
      "ejs't8eEla1 t Ncetade ad ersnip nis ceseuord fa. hestod.tt lx, porauds RRlhhey w orty  nrAmase htakrs\n",
      "Iter-190 loss: 98.7467\n",
      "eeUEPledctlard,stan hirtfse oonet ano mf 1inenrteoe  tithcissoy  iory neo vlleiokletdlsmt ckepins oiy\n",
      "Iter-200 loss: 106.3957\n",
      "ewbudnaty ein ed  anlae tseievmeredgn asc torr bgars.at 8rportucy3vis.ibiwn.r9nthua fi itdIciHor dits\n",
      "Iter-210 loss: 101.6730\n",
      "eTe agecanllahkJaiK  rurg srfd h-hesso dhi adAhcnon nxcetel ti hecoipe sotdsi essgiiCmunrf eorhry irf\n",
      "Iter-220 loss: 98.8591\n",
      "evkcoCn1l klon mle9onr an 1mhlloul xCtetaiottoag nw r ssthowo e,a9 rireiee Jhu ylwaneftlUoaheharSslia\n",
      "Iter-230 loss: 98.1877\n",
      "ehb reHsa tan eflc ace landuilg dn  stthiPE r latedt rcireypArtuian e ieip8tceih,durvereohanrsikk-yhn\n",
      "Iter-240 loss: 101.9113\n",
      "e Apasn wepeth cntd wopegeschlh nJthel ssarpcat-le tcyus oeewiwcdeb2rtpes uvr thioridsnaoo taahh tPRg\n",
      "Iter-250 loss: 95.4596\n",
      "eAsSr ase tot. frlGoenAeT noue wtis ghdhe uhouarohes8  rgatdisshirt pavore \" cvtah7woeTslro lN本,1euoh\n",
      "Iter-260 loss: 97.2005\n",
      "eTorboai an op din efUreSTol. Epaseiy h8m0ta boO f1Dd aoh rcnsI cerCstetrihed agofn re(i1ethtah ardN7\n",
      "Iter-270 loss: 96.8446\n",
      "eoleno. peS e einm8iLt. c8ronupd wjptd  ahuaOp srar tth florepty oce n tx plit Htipaal erlatveye4 est\n",
      "Iter-280 loss: 99.2254\n",
      "epcorntwb. hestodot diyg lSurio cecrh'tbrisa re cli9ahe iolptiwad ems, 6toy e –gke anis0 afd ih' paan\n",
      "Iter-290 loss: 96.8343\n",
      "e4bytpn nD ertitee sparlo i whta d Us cha su, ao.d5h. gssp clana das irmvwoditstnhd Jaha, lhs NSa aii\n",
      "Iter-300 loss: 95.8089\n",
      "ea8Wg Tios errsi, jwmad,uod 1eg9e fxaGh, n oxtdnd cl ey  8nmnda Gthn;'raan gi'n 'n-oL.Icgtp. Wh6\"tnde\n",
      "Iter-310 loss: 95.3912\n",
      "ee. nnn 5heinz2 krin monj9 lxe'ctnflta ih eroi9) hs he wPard wan sS(i ,ghyd ,bc wwr eevnol tpe  fJtWt\n",
      "Iter-320 loss: 95.2567\n",
      "eIGrk ioiteoe ofit. otter 4at–levd w9at ared e DLh inye atyt.a tipg's hkawrieulsiahv wft-( ode'4tunI \n",
      "Iter-330 loss: 95.2400\n",
      "eglaroo ith.ut S9tpetilh arnt,n.t wdT harey af an mhf5 anJaa s' enJyea peyt si wvthalylthoE9yi2Adl as\n",
      "Iter-340 loss: 90.6365\n",
      "eH–e thine smhtt, xipn2.xm.  perWFig thecmcldhiar -anal blnths eon s8\"tak R Wtndtm  wJSanydrlomhn in \n",
      "Iter-350 loss: 99.0892\n",
      "ey tome ofohdauauntdazRytoepla noge nopahomwlglaat nean ow eetiopanar, rh wodoJnotaa  an el tl  manl \n",
      "Iter-360 loss: 103.9822\n",
      "ed Gmhastle ettr.h  tan 1phecetiteato ic Lh 'se w2uhaindr wppanste rode utNervitl ,sh yeNreanid ikami\n",
      "Iter-370 loss: 101.5093\n",
      "eEEt ao oorco ur nhy9ygGirumtsd fpotngausnrAa  ftpu am co 1f inadtipodpiyrltyis 日c1b mvgrrpetmd,s cpp\n",
      "Iter-380 loss: 96.4819\n",
      "erd oirtmrhirft,\n",
      "7sfla fJpaey onoherseauiint ioiye Irttd punase Tcovmtndaosetgerise wrrli imso. ngsmt\n",
      "Iter-390 loss: 96.6403\n",
      "etFreGdee dtooaay Frst x r aIL papea inna eoldee.ft fstao.d se salamnsi i  aoh f7cfe.mkw hpanlwak twe\n",
      "Iter-400 loss: 96.5080\n",
      "e.itn dioratrlthe suhkelwrlad an tesr enm1huceaomilanbmgihae apeoueo, eanaee,I lxnd huto nsed bcaorto\n",
      "Iter-410 loss: 97.6310\n",
      "ePf reheer2Setklhpoorcy hoU) \"nol iek otg tnonlih hpaob  eise uth4 elan aoes thasaca earsist imt pauW\n",
      "Iter-420 loss: 96.1638\n",
      "eMi) an0omrnst rekd-rtsaaead  orkne nc'sran nr ,r in  Aindlciegicarim ioto drsy ieuriss a ,cr ynd iao\n",
      "Iter-430 loss: 98.7565\n",
      "e arx invndti so av aras ec nm aomC eoWJant本sy t ihal topn ii ut,a ocar dar  han fnoae tFyrch PrlmiHN\n",
      "Iter-440 loss: 93.6632\n",
      "e;cFy  eh 1nhiIgae faa9l6cootea asga, ur\"nmttelakpe  i4wufranIt an aan8 sinicirnooork9yo  ih Gois ac\"\n",
      "Iter-450 loss: 92.8754\n",
      "ecw tnytk uno ot,hraitg itei  thiiss reto  oheo .sFsTyf th, t, ictnnlmppe ami-iin e ithuotat haielder\n",
      "Iter-460 loss: 93.9391\n",
      "e) ClTwicnfmHouof Dl ol rtsedfmnoe 2sm e.4aan, Wlopittha thn kg D0iuIb'nil dan gn ean e'os wippSndgar\n",
      "Iter-470 loss: 94.7259\n",
      "evEin-angiT po1wds anpoutDhd ghsegts itoBGerlama lherom glol ugegsn mfithie p\"intn. 9l sgee e t artio\n",
      "Iter-480 loss: 96.5623\n",
      "ed1vfOcasinawd fotdaiek orditine iltnasthk Jifoooet Es ofede c.t e rot otny h sa ,dpesto ioi ob onel \n",
      "Iter-490 loss: 91.4843\n",
      "eu)iN wnlovubeey idm b\"trde Aendatlrte oranc upingTdrlaioct res cneG Jlirols1lstd  hpirriohdic ss ato\n",
      "Iter-500 loss: 95.7322\n",
      "e78m,  pplet pthanardope mtheitar irt5f , xrdaf nth ian helsis vnm panand aii5y r.e Jauan Janan las t\n",
      "Iter-510 loss: 97.1715\n",
      "ey本n m5ame's 6oleroeye  Ao,d lcsecor itahe yy thew ote.opcfegl aplmoe f renfitbgit4e i1tteaf  o.utlan\n",
      "Iter-520 loss: 94.4014\n",
      "e 2Wrl8f t aatd. apeered 5ra7dlggh ,nsi.nthJial t3diJtos ggr9e mapg,o eimil auetodeee uasg of2fsb tal\n",
      "Iter-530 loss: 92.8553\n",
      "eI Jand1an.  he,d blasd fe etoh fel herpt9ed bhte,,reonl aa inlooJxan ah ywactlhenostm.ed aNdah tolre\n",
      "Iter-540 loss: 93.2060\n",
      "eh本inldg up Pod irawt ede wn anah il.itty  ornto irpinetsewporpariG JelNiea;te pron Ty i fe oNkPy ldd\n",
      "Iter-550 loss: 93.5264\n",
      "eK, oForpeoIan ed ih6se, nsedice afin Is oledmg soiroharcvotfexos guwe it cinatd  e eh C.hoenimnlG.as\n",
      "Iter-560 loss: 92.2658\n",
      "e.dinj n rfheawrlinIeded ivir1e mt ateyd I seo cih aClaecfale,nareyav micvntorsatWn rT, an il wotetCt\n",
      "Iter-570 loss: 99.2171\n",
      "eIsj tthe dnsituy  f he6orno -ln .in axlc nsou bokts. mfe the, da,tgyd pry mno, to, itaes EalarasooeS\n",
      "Iter-580 loss: 91.5699\n",
      "edJalln%ooapeThe tunae  ndg9en ouyiod opea. c1n tur rAeand id .alane l grotinthy  marrtih che.rasaiut\n",
      "Iter-590 loss: 94.9258\n",
      "e:C cin stefU vha, Jepilaucutctmethsrgenae'ritey.t mpatf fo ohee fipim lsCRathpass tmt pnrway ohertng\n",
      "Iter-600 loss: 92.3300\n",
      "esya l hDhJangs th2 ipon thk-efih 1n they cpa.rit theif Jthe,d as bncesWtu ctelnrcern f fea 3h irJans\n",
      "Iter-610 loss: 94.6236\n",
      "eEnanan Itcsg peao5 akeee Jthcy lpitlekmn  aronegmtehe ianeth Wapendutsean.mfrn isae nf. as alet18i w\n",
      "Iter-620 loss: 92.9632\n",
      "eMorse ori ssx1hAtsey cf eni1 oGth ctpeO thiolithf  asheon 9usd yh tome , y7y c e- an. lod 1WhondWd h\n",
      "Iter-630 loss: 96.4422\n",
      "e: Amnanan,le.ietsnds LhWshan h. ain oefkes cmthieta l f  fine ep,of or 7onn Wswaxata'nLith' ve pfpto\n",
      "Iter-640 loss: 97.1305\n",
      "eg6gtand iah Emf harii7its., ooars is,istaw e eno'c isCwsn ilsmee anl 'asld ked lhar wfra mnal Nkan 7\n",
      "Iter-650 loss: 96.6779\n",
      "ejTxer cRiapan sszinoth v,ord Rituirelmrlatle rl5.lJaal okTe lrlihl thoa, 1nl n5rlunr ay J,htan an me\n",
      "Iter-660 loss: 96.3692\n",
      "eO., ,ian the fisen4E tpn o. acssaitbte oed f its\n",
      "e5nwded unetjtr dcor2otraplloerayasa opaseod nt-l e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 97.0444\n",
      "eRlE8tpoo korapan utumgten onr rnilein tarditsh cit. Em, fesneastmmrinit.iton. ppheaod,azi,ast ttos t\n",
      "Iter-680 loss: 98.7486\n",
      "es-Pkuaureliaoadsaan itazie ilmohaniluinarm rhesrgtndU eurt ,s yfs axp goh theoa andikc etl le4ulier \n",
      "Iter-690 loss: 94.7891\n",
      "eyxwov liod tpun fegyeai tothise  aI 1a9,rs tnthi.'as ,wuegtotthe.ilaedpmioiree topangtes neitsotddda\n",
      "Iter-700 loss: 93.9695\n",
      "ebets mearohn h5.esurana rmhie stN , phhe Aneh mog 8ts rrotsftor xtei ah apetNmd ldaTiys fgir, iae cn\n",
      "Iter-710 loss: 92.3880\n",
      "e pa日ied hiaoun .whGpresd ffrud laridthe miiSthin uthy epocGcyflbcteg itiap eondJoiss d oarrlourdd gn\n",
      "Iter-720 loss: 98.5979\n",
      "emJ%d Byil. ar le  to-and ow r.n tethetiif fitnd it reas Itre otC renvb'trcaro aNun hgh man ee Dg tan\n",
      "Iter-730 loss: 99.8371\n",
      "e25uln opon 96thxehe te ieo cty  ko9 gsd ated anlnd Fip 5 fAIy thl l.gskoh oedd ftlan torotun pafss c\n",
      "Iter-740 loss: 97.6345\n",
      "eOith4fxndgAbitl6e tngJgurf5nogthainc Ltopnrosh iarl odard fc sh euntioc tppilitijfan2e bioig othoo t\n",
      "Iter-750 loss: 94.4145\n",
      "e;o ApatmeriG. a-uoigcoten pr aen kmift wwy abi F2n theaphre-hog whetthesed paha1jo Is tmf oan Nuchid\n",
      "Iter-760 loss: 95.4219\n",
      "ea1ei -o at7th ro eyster tnke atplet l-iieae obet neici teromnodi hCnuar rNope nHCJic1e etun lJoaslha\n",
      "Iter-770 loss: 96.8627\n",
      "euy  indoaw irreha2m5dsn 2tfarl-ice'kat its acnthetllapextJmI2,  aibs Ooorcs tos hhecintruad in NeaF \n",
      "Iter-780 loss: 94.2136\n",
      "eNan I3lce hie Gle saa asa Juis  mhe E. whe6aos reT p JatP isehpoianomepkek, iolomerad itoItseiss lco\n",
      "Iter-790 loss: 91.3879\n",
      "e本e8noiooiltsat thoekeead yony g moonitoi. iherd ithelr8taso ved me i'hib–ghd aror th inge 1 mhde  le\n",
      "Iter-800 loss: 94.6594\n",
      "eadyC.toa\" s itg asWes ietecite ar,yoe gtpGl o\"ndine ciin apshssasgs rlh he tsettW af iael ornhlanthe\n",
      "Iter-810 loss: 100.2021\n",
      "e; tioue iJ Lnhur7 bleJfhlan inaiol ooicol an aatc liothalsJapxat uuatth  eiasitpred 6ym snest h ar,i\n",
      "Iter-820 loss: 93.5365\n",
      "ectttApeineOmxUoy i nanxhat jfotsh.gJanettetJyad giritount2 t thhe tati,y tf ph iIeostorcad orangu  E\n",
      "Iter-830 loss: 94.6129\n",
      "ey  birdesloacGingce urey itunlthiss al hEapsshetc TI aT therd winadGnrd Kyopastaro mand monst Gmeeua\n",
      "Iter-840 loss: 93.8623\n",
      "elctere eanelanath\"hk far ofaised wthisirewin ecey toodtorooh aned re phJoes and peeod al cusvhec 'on\n",
      "Iter-850 loss: 94.3505\n",
      "erherlotiwtn wh nohoanItE dpopsas\"s 5f h twoogyartwe  ege r aon i lsik  ene s rgirr. c-xelemed arey t\n",
      "Iter-860 loss: 94.2297\n",
      "eioid il oninFd  swaf g:\" aned lan pem o-aoT uocplel iuerralclalae lote s t Sirn tre oo, .hym npplan \n",
      "Iter-870 loss: 95.0797\n",
      "eOkyistw e\"–'i  fe ohay o piee1 seras nthe 1gin,tsinl stctiss eg,JaatsaalcaD Gr日9d mas rgstl chorocrs\n",
      "Iter-880 loss: 98.7225\n",
      "e2)etA7inand ha, toonfs iipenn su atd iihe日olicaNsthi iisd itha an1chei tNoloamd t1aas toy es Cpaftoe\n",
      "Iter-890 loss: 96.4192\n",
      "emAtomiug ran iheuEnJan Erf EuNvree d ahe Sw, thLnd aran. ane titotheNe Chy thec iis eor se  Wil asle\n",
      "Iter-900 loss: 96.4479\n",
      "evk\"e a nonotoar i pfg,ponntitmllottrwthery,lth al hen 1o-rmaa eis anatty e fNeo inkmotet andilem soh\n",
      "Iter-910 loss: 100.2813\n",
      "e.hJanTas n usnothesir1-xoorled Jarc hangctntrid wotaretred heh and Gure TTek ed iftys the awarltSela\n",
      "Iter-920 loss: 95.4818\n",
      "e本zCi9oKvnt1osridcylatf r amean fctoe itrtle at ine. s  f am g ami rCwpfid kane ngse rlgkeome bdar'kS\n",
      "Iter-930 loss: 96.9215\n",
      "eoR\"tgonet8ooubShevgorrojd toan fdahe e ord'slcpius ekyisTo Jhah he andas thisin Emaz  f11t thekrdeSc\n",
      "Iter-940 loss: 100.4048\n",
      "epominet xToh c ahorporsCurontioy iosthearl wiwcre rapintpe heant8ug opoeod n Ge lamoswohiGy boee g a\n",
      "Iter-950 loss: 95.2418\n",
      "eh- AfIpof fn than in oaut ; Ei shk bh oreatrec3ae mOy arinineog ih ipar cis agy e, haland, glateo Ji\n",
      "Iter-960 loss: 98.6845\n",
      "emcye tSouttnli7tnextd J0n mapn ,antak ohe urud irec citois g ce ystmleocase cdaih erey cai2ts cu,ir \n",
      "Iter-970 loss: 95.8275\n",
      "es3 Eiorg 8en 5me hr lalecreng ns anos theo un iIl uhertonte WirdeJid e0hley sins xeuuhc itoSialtheit\n",
      "Iter-980 loss: 100.4531\n",
      "e.an tm herius 6ctostupe,  mit u arthid man an wth arioes eapol ametego8l aro Th iane th, ial ty the \n",
      "Iter-990 loss: 95.8231\n",
      "elae( ranIn ufJ uh mial rob1ihs mhgk anits tumsird nDssitoheth eapih aun' d adctand degvecc–letasg sm\n",
      "Iter-1000 loss: 98.3780\n",
      "e,y in tih: ttho80st tsreun i, icl0telen \"noly bh fae, I edfctplclan miltsoms bigarity zedot, Eimeolu\n",
      "Iter-1010 loss: 95.7958\n",
      "edsrd ipaod ththss eois lee lre8ystWmkelgink 0am wE pKain5ng-5 elm arolin y5 msn E(tcehiti melorditol\n",
      "Iter-1020 loss: 93.5716\n",
      "eguny y5 Th Sinr bsotperve Gekeaicctsd Tune To\"si5hele l Nn kcasevgrae isil upmobne Jlan\"s yhletth e \n",
      "Iter-1030 loss: 94.7995\n",
      "ea kbobe cu日 aTn o, Ch ageg Iil,e 1red 8sh;this mwylgDvntsl,ae ithe tin ihedrd uosiE khcaprmtnd wyuru\n",
      "Iter-1040 loss: 92.1752\n",
      "eOcn'n cutts ar c.Jalilipge jr, cWon maind eCpn r hoh iheon WAecomute Lenaty tool ipakter ted ow,dos \n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
