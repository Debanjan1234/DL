{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 30.0761\n",
      "civ ens whe lauryeded Nepanco, derccrupordhirgod of on a's Haf ikidiligaHa sioan CormI bin igorlapas \n",
      "Iter-20 loss: 15.7951\n",
      "ci isilaighises 8se of of tax)opech is The 1826inrto en whas. 1968duntutive Gmiiss roukhas ope larlal\n",
      "Iter-30 loss: 11.9748\n",
      "cates largest of isouring and Ocolis ineu, illor-Mestonl. ar is to ilomperorrt 20D(ing en, es, pagead\n",
      "Iter-40 loss: 8.6708\n",
      "conolcis a partirl, Nopel Srecigunttyry whyord berld World Wart ecover fimliot. Japan of Ikhidencith \n",
      "Iter-50 loss: 7.8114\n",
      "c ales the country Wirt of in the reseagisys rofese with couanmy bbapinused a percatical fouwe: 2mati\n",
      "Iter-60 loss: 5.4132\n",
      "constinsingiticin's 2Nastery xworlate Olappore Country of Japakiitolyliciteas of isolast mity of Japa\n",
      "Iter-70 loss: 4.5778\n",
      "cato olincean in 1894ate conution to ef Japanese nal 1939, Wan e inolics rras, palital War Suni, ist \n",
      "Iter-80 loss: 3.6724\n",
      "chi period. The Grof cicstuver tha tha-laterion of Nobecexpanded is sixis in the the coune, Kedeoran \n",
      "Iter-90 loss: 3.3677\n",
      "chutatury . Nits ecdegeapeconoGmmen marean untied the pegidasin pe of Japan and Warl. If isoluthe Eam\n",
      "Iter-100 loss: 3.2593\n",
      "conomy by pent a andich is das rugh prrpagasaxth. Abuman lergeltiverlad dandes in the sounthe countul\n",
      "Iter-110 loss: 3.1420\n",
      "cencingigurand lory tertan in 1947 whuren in the souleated is of Jppeoge the thiidlacicas citefilithi\n",
      "Iter-120 loss: 2.8932\n",
      "cdesirest. I ppore worl U94. tho antid cith in the poppelon the Ea. legidass oa colelici decopuna, ge\n",
      "Iter-130 loss: 2.9273\n",
      "c Gamilit 1khi lorest 8201 oimexo4n. Iopber and Worlg in the sourth-conudeky maf the istruian, is in \n",
      "Iter-140 loss: 2.8362\n",
      "ccefoud milimailetion early ast letion ha, si's nololmor Wint ankes, bpalurs an an pegilater of Nobec\n",
      "Iter-150 loss: 3.0935\n",
      "chy with and Nakoition, py hesies ofith. Warter was res, in and Warl civintiaf cith the world, the G,\n",
      "Iter-160 loss: 2.8350\n",
      "capital citury Comunican, whes on o, Japan G7 ing sigshitar obeis ared intortes of 1934eas constrrcen\n",
      "Iter-170 loss: 2.6805\n",
      "crai romere oa period of isomagtth ed ss is Raan cam bory y moke Seapered cofelopme ties tressury  ba\n",
      "Iter-180 loss: 2.9264\n",
      "conomy by part morle Counter the Sory bortassicu Ind is the hight . Asiate f,untry an reis icoliis pe\n",
      "Iter-190 loss: 2.6408\n",
      "city of Japunation of 193x35 has earlitint en wisuth and fibit iana. Ia popintolilatiosich Japunesing\n",
      "Iter-200 loss: 2.7211\n",
      "ch includes larided intored hureat Wanes. Frof dofeng perinotiopenthed Sstan of Nomeo laticated in ol\n",
      "Iter-210 loss: 2.5251\n",
      "cted legidlacaso. Wr the sort rlegglad corees. Nearst piefopud the 1868, in the nmy in tountry in the\n",
      "Iter-220 loss: 2.7865\n",
      "constitution in 1947 twhe worlarly an the No-Nop, the \"s, malliog a ginciosyas mive with and Wodterte\n",
      "Iter-230 loss: 3.7762\n",
      "cyku ocke worth ben firsd the oainls of 1932t hich usincpa cin. The country in Asiake US,, cictory th\n",
      "Iter-240 loss: 2.5277\n",
      "chy with and rountry and Htsea Shiseas ond city of Japan'ke 201 Japan cakess endery cagn and 1Nommu l\n",
      "Iter-250 loss: 4.3446\n",
      "constitution into an'kicy Japan is of Japan's lonlty nterized Japan en, which siCsasing in the sounth\n",
      "Iter-260 loss: 2.6233\n",
      "constitution into 47, thicea, aicitamen in iiet lity is Epeprre makeal moborna. lo. I, Earlly Slait C\n",
      "Iter-270 loss: 2.8513\n",
      "ch inced aglatilit is the UN,bary lugintulion cith of the UN,binoaggt Sunal morer cacountI ping the w\n",
      "Iter-280 loss: 2.6782\n",
      "conomy by periot. Japan is in the oar iis Olopre const care lart olcan largel the sortes lailiginolar\n",
      "Iter-290 loss: 2.4859\n",
      "ch includes conste ent mie confliopyoG, Japanation is in Asia sas hosiratiid marine ukaand cas turice\n",
      "Iter-300 loss: 2.5937\n",
      "chy with and rentionoded country in Asia and In ras end is ecention, inkeeke mod mitith and contacicu\n",
      "Iter-310 loss: 2.5676\n",
      "chy with and rukokkeid itir insas malliopored nall-1S1kear pawepn Tevel. ILinle 19dsth in The sokhu, \n",
      "Iter-320 loss: 2.5719\n",
      "ccessive fembiis ad the Ubpered Japan ead manacend hic nore re earcis seaparnag the conto and Worly a\n",
      "Iter-330 loss: 2.5273\n",
      "caller Noftole Japan ente Japan is in the solthe G9.bTats and Jass, ranker country in the solthe sin\"\n",
      "Iter-340 loss: 3.2694\n",
      "ch inced Japan to-osiun. A the the the Sumperor an in the thims rakeith ryotiing and to theres the hi\n",
      "Iter-350 loss: 2.6291\n",
      "curacinss of into f. Japan is in the Pa, the fourth in 1947mpelenty War Diifoty intorted wrofs eran w\n",
      "Iter-360 loss: 2.8206\n",
      "constite of Noku ho, condesrrrecevinad mal mieitelinlory coupartary colillory bolics Nipreigest of in\n",
      "Iter-370 loss: 2.6635\n",
      "ccessive feglatid siki. ond in the nels o5 the aly Sicy Ni.ke siku iii\", in the OEmprer as rest-rant \n",
      "Iter-380 loss: 2.9729\n",
      "cy, the this Singt en of Japan\") is intortese Inoukh-mat mion Eaporter ry ,igacieseat loly Sea of Jap\n",
      "Iter-390 loss: 2.6336\n",
      "che 1st rexgmatich in 193Kmeaping Rusalorn War. Abha an conshooct xppercekeestore aad mree Inded int \n",
      "Iter-400 loss: 2.7649\n",
      "cted legislaisy Ashooogest --meragest courth-in lorer reas of isounh intras ran decures, is the world\n",
      "Iter-410 loss: 3.0487\n",
      "chy with and Nofres characisiscon-19.ths.eal --fose an islanded in outh cones resturctefory texpanile\n",
      "Iter-420 loss: 2.7232\n",
      "cen, fourth-se Japan e pore. Shas Wcale, it 195y mally Sinfsexmaer is Worly Nearly as coverad os powe\n",
      "Iter-430 loss: 3.3871\n",
      "city a porly Japan is in the number wust oncer wreipanaan Japan i ated s, the 2Japan has of Japan\") i\n",
      "Iter-440 loss: 2.8671\n",
      "cykes in 1947, Japan is in the Global Paan somhitar camention rexpand in the nu5 in the nty the War a\n",
      "Iter-450 loss: 2.9222\n",
      "conomy by periods Ris.to lefed the winst ountry in the 12. and Nas the \"Lan sies, is eigh tore Olobom\n",
      "Iter-460 loss: 2.8093\n",
      "capital cite of the Nichamacigintaineke Gbelre. and Wintugs. peror paper rubatass ryclally ASEaary co\n",
      "Iter-470 loss: 2.9241\n",
      "constitution Pacith congle the Asearl Soa en of Japan\") is the world's to and insury fims the Rshount\n",
      "Iter-480 loss: 2.7918\n",
      "cted legislore tored fes re Nirstaty an a ungaby periods Riatontary as Japan en, whe pontam in in the\n",
      "Iter-490 loss: 3.1771\n",
      "city proper was er and Wint larry pgefecisil miond irs th sed constraies, is the world, the conturics\n",
      "Iter-500 loss: 2.7229\n",
      "csiis is recistory pagose Chinularl--mencicularly aftion. an e of the UN,b power anded Japan to-Jssic\n",
      "Iter-510 loss: 2.7578\n",
      "city proper Japan onts rracing Saa an the siunes in eighits right in. Japanese War ari ate South of J\n",
      "Iter-520 loss: 2.7064\n",
      "c Games.\n",
      ". bomin country in the OEmrol lar iol are Emperor. -lorg perced Epper constrafs Japat i sed \n",
      "Iter-530 loss: 2.6549\n",
      "ccessive fevisa, matirlan wruge Epper and it narly Ily.\n",
      "2 that Japantan perceke wiried in\"an a and th\n",
      "Iter-540 loss: 2.4911\n",
      "ch includes 9 A2Cictare lake fountrome of Japan\") is, maldush,edean 2081 Pkee the athe East China Sea\n",
      "Iter-550 loss: 2.5049\n",
      "constitution enji i, 1uth of in 1937 en the Glended intowudst fery the carended intorld Warn peno-in \n",
      "Iter-560 loss: 2.5019\n",
      "chy with and lory als res, is the core to an tiese first in the gory as the first expart a mower as p\n",
      "Iter-570 loss: 2.5858\n",
      "chy with and bontan tfeks which rallowed the \"Lan is Inre ca leres a Ubbnd an es the G7, to en Japane\n",
      "Iter-580 loss: 2.6084\n",
      "capital city in 194785polaras hon Hu0ok enjy of the Upper of Japan ented isto and World War thies it \n",
      "Iter-590 loss: 2.5141\n",
      "constitution in Asia tion is wistretive fobpan the Global Paan sountoasing Russo-Japan axpope, the is\n",
      "Iter-600 loss: 2.6619\n",
      "centuric's a meled-Ifli. 1Jpab lar onee toneas Sea of the Gdearl 9.5., whisl rea, the Japanated caldr\n",
      "Iter-610 loss: 2.8333\n",
      "capital city in 19th allower and lartitint of CDch ompental os in the no8g bun\"th in 1jI I Japanffis \n",
      "Iter-620 loss: 2.4997\n",
      "cted legislored the Reafo5, iosolat mi sountre Japan is ranked which is and Huma. meaf meansr8s panai\n",
      "Iter-630 loss: 3.0745\n",
      "cungth in 1937 whh  cofen. by popent are 1868 am lart I folsion in to an larl mikheseapen the Globeke\n",
      "Iter-640 loss: 2.5282\n",
      "ccessive febpe band is conste in 1854mas end in the soure the G20 as the Oapod period. The Greesear c\n",
      "Iter-650 loss: 4.2930\n",
      "city proper. lorg grcha,igisatomexs Japan es in the the hist me of Japan't leyo, the Menjy Ilicy Japa\n",
      "Iter-660 loss: 2.4723\n",
      "ch includes bel randed in the nort v Japan expander bexglaticun flire contrecentury cafitsjs areare c\n",
      "Iter-670 loss: 1.8427\n",
      "came to an tory a aconal Corusmacis's min to deropor aat Chkair Tiiturigh ins of Warl and Shina, was \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-680 loss: 2.4283\n",
      "constitution in the sourth-largest economy. A, the world's towed an ean counconoth to and a portin th\n",
      "Iter-690 loss: 2.4059\n",
      "city propering in the nolorlit fllirges in the nti es  arti thims whiin in the the incisome of Tovent\n",
      "Iter-700 loss: 2.3835\n",
      "chy with and country in the Paly Swiobird confea, is 1858ipan country in isolatits ulartincing renouw\n",
      "Iter-710 loss: 2.5074\n",
      "c Game precas of isolatity. Japan toiin tras Japan hasl-922al monarced In the 15 int mit ric pegishes\n",
      "Iter-720 loss: 2.5832\n",
      "capital city in 1868 lake uatchin  int economy. Aname yonlllo-Nnad Resicoustes in 1947, Japan ealite \n",
      "Iter-730 loss: 2.3778\n",
      "came to an Japan ented in 1947, Japan ea origisicdaation eariest bery Osyorly Nobyll civin conf-large\n",
      "Iter-740 loss: 2.4636\n",
      "ccessive eally...1 ikoar of Japan't The opighest ren, which in the OE8porte n mit the sich make up Ja\n",
      "Iter-750 loss: 2.4573\n",
      "capital city in 1947, Japan ented colelove bopulation. ffIlter Insincind rontha ser mn the nti's in E\n",
      "Iter-760 loss: 2.3933\n",
      "city propere fenligest acoune, partitin Sun the countsy omme lercannsterllliIo35%ejy n Sinate of Japa\n",
      "Iter-770 loss: 6.2281\n",
      "capital cits and Sru the sou the world's eighthileetion enjoke Sinas wucily lirt mif Japansabe confli\n",
      "Iter-780 loss: 2.2553\n",
      "capital citsiistrearligstensen of the 2Landexth ing rencighe eat China, fourth-largest ecountry in th\n",
      "Iter-790 loss: 2.8003\n",
      "cted legislod.-Japanese halol Emperocas make up Jappe fomurth-lare the Index, hast olesterld, wh in, \n",
      "Iter-800 loss: 2.2345\n",
      "c Games.\n",
      ". A Tiiudo mbecitals in the soulation enj-lore of Nobllon resean deut ared Japan ented of No\n",
      "Iter-810 loss: 2.1626\n",
      "cove fourth lerized of Japan's lovere fenpande siacingxthe Empore centusure lariole mart merease kiis\n",
      "Iter-820 loss: 2.2232\n",
      "ccessive febpy haa dite mandlint amemlelinsaan called Ww to decitembyos, is the contrethe Gath infenk\n",
      "Iter-830 loss: 2.4354\n",
      "ccessive fevyln a pore conilery. We, als the world, atced earcurar and. the Cencing est bughth of Jap\n",
      "Iter-840 loss: 2.2585\n",
      "ccessive fest Japan is in the souutints. The kare sorlly 2A8192,flentes cantyd and Nairake mareate of\n",
      "Iter-850 loss: 2.3236\n",
      "const anre conthe Emperor. gudyon hankel menithe pariga. If in 1947. In an lake wihane deopultioss. T\n",
      "Iter-860 loss: 2.3119\n",
      "conomy by populy Smif Japan ente. Ind is in the sounugthiry in the soun 1868, ht ilidich of the Pa, t\n",
      "Iter-870 loss: 2.3464\n",
      "cdativen in the sountreapan isis'x tour thy thimit matintaan Sinon of Nibent okuit my trest oftien th\n",
      "Iter-880 loss: 2.2717\n",
      "cy, is the world's folefoppere fourth leoges city proper. Wed Japin\"5g the first in the sountreane ce\n",
      "Iter-890 loss: 2.1718\n",
      "conceapanese War and mity th sanovy constris the hight ily military bumaneshoankeised Rushumargestitu\n",
      "Iter-900 loss: 2.1456\n",
      "ccessive felpentit mallir. Nobal city in earle deprchanontury. ban in 1947th eight e, the the Melopme\n",
      "Iter-910 loss: 2.2745\n",
      "capital city a porecthi Omperor. the G20 and gulpeon of the UNglban an es in 1868 cts, the Japan est \n",
      "Iter-920 loss: 2.2869\n",
      "came to an ens,ina  oper of Japan's lofes, pary sekececiulatided consturing a pored a dover is the wo\n",
      "Iter-930 loss: 2.1581\n",
      "ch includes in 1947, Japan\") is an the Warse Ryigut ceuntry in ited Iftion Hummkoi. 8th and Japal Chi\n",
      "Iter-940 loss: 2.1490\n",
      "ccessive fenlity. The Greante. I the G7, is the world, twhe worl makeae and Tairs of the Warterlan ma\n",
      "Iter-950 loss: 2.2500\n",
      "capital citalicatere of the UN,lsiles the world's fourth-lan country in the Japan, and Japa\" ad Japan\n",
      "Iter-960 loss: 2.2667\n",
      "capital citalation fest Japan is in the sush:acdabe fergesixrrtanstered Japan is in the sountry in As\n",
      "Iter-970 loss: 2.2152\n",
      "constitution is ruropain the number calleolich Warte in 1853 whe Emugolasecled If the re, the country\n",
      "Iter-980 loss: 4.1186\n",
      "constitution of 16 of the wortaritation and World, whics of the sorld-92thiuatome taring torer as the\n",
      "Iter-990 loss: 2.3643\n",
      "conomy by periodsima my the G7, the sight in the sountrean callee Warte an foupit th to lar\"cils, and\n",
      "Iter-1000 loss: 7.9396\n",
      "chy with and Wintia and Japafa est cen est cuty mitar and the higed bcuntras hitalexcanted rand mir c\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8/9Kyt3gn+n3RQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bfe2e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
