{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, ho, ho_cache, ho_sigm_cache, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 105.7600\n",
      "eisa ass a sa Jta e,t  ledloswrHypi ecenian sre  t aOspisdtGrlhnpn h ai  otois,phitsrf\"r tJrTiiaya ei\n",
      "Iter-20 loss: 104.4576\n",
      "erb tfmns s9iuskeflu lorfmsag fn 1trTlpmcC tnr GnarofrorcktpoR r ttedo eiit\"oteC t idao  c xtaSr ,erl\n",
      "Iter-30 loss: 104.1630\n",
      "eIflnn1tafarotAsruri % .ne aedBnednirhne tnnuraawem  outncd n.eottotepho  u  p  adoka  1l 9lg,t8 s, r\n",
      "Iter-40 loss: 104.0916\n",
      "eju  nh1i ise me itptpLshfdioh-a s  e oi\"urlaumsecnaa s \n",
      "9einaat ctkiJ wus.i  onriohdo,a s  tuD nna6l\n",
      "Iter-50 loss: 104.0622\n",
      "eagve o, l   lxmntmesyegtsitEnniSa  tontoe8 u dt,oJ1eulsariue  ne glemuotysohesieud ft ir ,sugea ea l\n",
      "Iter-60 loss: 103.8792\n",
      "eatb ripithshfeyeoroin .mt. od rvt5Cbe1biehituh m i 3no na3lopae, lhnyobasffkteig1shsfa JiE ks lcy ai\n",
      "Iter-70 loss: 103.7218\n",
      "eldenld-t htyinycsee-as,l d  aghuna2ot2iksi5trraassteacetdogsyoh dhishmj on  slr   itl lpdrS evxral  \n",
      "Iter-80 loss: 103.4990\n",
      "e  a aielonsr  pvatbmu ndihniet9tna d.rnfsnniunt  no .r loSwrl cujJd9lrisect i wil 4r Jttmhadrl  JTln\n",
      "Iter-90 loss: 102.9740\n",
      "e ulcc.d oadh apnAlaowr  tCrJhtutn, i,seatlpndeneegye9Wathady  satoiersnaausrnia . s ieiiu t aisiseei\n",
      "Iter-100 loss: 102.1873\n",
      "eoreoitrrneRgprSiud  o o. irlcapa6rtost or hrtiarpnnnlyreepohfe nntd nSrsae dndmai  nrapEgitentlu7s  \n",
      "Iter-110 loss: 100.7716\n",
      "eouarrayoediet one tls lmeacnbeh aaeeo Heyf ak., de o ae tts orlGaeotnes enror ycnpkhie 2nlriat fis r\n",
      "Iter-120 loss: 98.8490\n",
      "e. rhpoiadns e s a ti3geen i'ruhslhlI2\"yo akchmaefp2tnoo alae ess mee ttnte apaNwwafun tie– paed fror\n",
      "Iter-130 loss: 107.7567\n",
      "eoodd tilehs l ec bieneetoednnupudnst1hle leWhgne ehnny  ati ad aubetea . irndJSpe tpAwwacidea oraoo \n",
      "Iter-140 loss: 103.4970\n",
      "e1 innGwf inas n eysoo ont4oi nn1LI shea g ax, d     pro aiphdnys d ifr, lfmnv 4po aos. drnolieje ysf\n",
      "Iter-150 loss: 101.3509\n",
      "e tii  EalnrGrcafpetsd aroBn6ota ariasouee iiwroe,i achr otn hl to. eohd oi osnvohtntealed ipxa hss t\n",
      "Iter-160 loss: 100.8133\n",
      "eke phcovwlIogucre reansunnhnngsr mpcaad Snr hnnd ahehj5de my ethmoarAey oklloron,e elihetge ehrxttsn\n",
      "Iter-170 loss: 100.2613\n",
      "e 1he t,eeg sa 90i ed uiehtgohe h. y aopNoty sn Ehoy hnliNR2n1 ,n ciiph1ot aa bJJjmimRdt ddr ctema gl\n",
      "Iter-180 loss: 98.9237\n",
      "e errrelpatlmee airouemiotsie in pRuucaue, s yr te iyr dy ptr uf wnyd lsta,te Jhh plo4rn anusettisg t\n",
      "Iter-190 loss: 98.8988\n",
      "e n cr shtiJht . ra7i nsy ftriiu sie inngisednns pa ee t s kWiGeplAhlast  et emtt6mEonsitht lmttc9-e \n",
      "Iter-200 loss: 99.1067\n",
      "er oei yetioh6sle8ne, r'ad yuHeuehny ae lhhd hm-ietan e6r ene,  NtU icttss7a hnre ouIoae anntihed ot1\n",
      "Iter-210 loss: 100.0821\n",
      "er d se ance iAty lnli eecaeela-allbadd pend Jihgtntulch Jhp\"Nlhren-y ce . tte aeh ipanlegg, 9am sglC\n",
      "Iter-220 loss: 98.4397\n",
      "ec iinahttanotafhtd,wter fn ktar,lcnypciiPgiu'ecgmpannoilie . 9OEgtrGrnntnebmogsmTa.uy bakn wHpaw7 ti\n",
      "Iter-230 loss: 97.3640\n",
      "elowttio pNa hi orarrpEioretc9whchon4d yh rsha aatnio fscMhen Niiswn pae Jote Rnro aoe liif oer oy py\n",
      "Iter-240 loss: 96.7406\n",
      "ehe so Ayn g aahoenkvAJawee aees iirhslalpinliscr mitcsetdlJgloruaa his oyds ut penf a hk) Tet bo, ap\n",
      "Iter-250 loss: 96.2323\n",
      "enuo frsgs stvSIn'd etJhfrfrnphod omofna' tmaeippht roten it knma1 isfpmcesd ioe mimhtttasde y  eeoy \n",
      "Iter-260 loss: 95.9110\n",
      "edd ions ngse dn Sloce rhctse rnlb日 ahtiucre ys kteeh ehta Fhuat ah Scaeertunteddedt ae ae e,bgncseji\n",
      "Iter-270 loss: 95.7377\n",
      "een arhtankci1 hrce 1tdlDuegssd aun 1t eu  etie pfstti siorTin ee iontmy  o hrag rirtTidn, tsotrmTans\n",
      "Iter-280 loss: 95.0191\n",
      "ee ai eediSo. ilfn oors Eie, srpphctaplio duit ce CfiMnnheds lrfaos fnliktd is Nan papord itar teuaec\n",
      "Iter-290 loss: 95.1055\n",
      "ed sdsoe ah 1, aurian aoejg In inhse rne afrnhrmBahaaes: tJea lo aete% e onicae ets scRrgl innteldty \n",
      "Iter-300 loss: 94.4810\n",
      "e, thr Cnfinonsn 1reuhh owf fotctn. xxertr cwon Japetpoe ehermpii rD y osita tJ\"as ettdllIvle  boo, e\n",
      "Iter-310 loss: 92.3864\n",
      "er sctes thnn otlse y Johu. iwto5 an, 1f spaitnlieed l3 enciitn7 eh as ees and ran he d rc ar lencirp\n",
      "Iter-320 loss: 91.5370\n",
      "emfe a-vareongrPthn ttSiwtfdssJisrd ekn afehh7ciatlliejd eo en of Tanaliiliancrs id ian lWmlafelsn No\n",
      "Iter-330 loss: 91.6630\n",
      "emps ccf mag hine len, Jakih ouiy ddd ihpinro, tt, ie SWendglifas. lpietsEthtliarsen ted Jaan Jimany \n",
      "Iter-340 loss: 91.6927\n",
      "euonrl am beste Nsen ihtttna Jiho pnre l.,laa agued. feWat pesgkarsd )malarnrtettig 1iaen  indcor rhc\n",
      "Iter-350 loss: 92.2066\n",
      "ereolnd and toec th te8 asscrtnnuee,ose Hoaofcs5Eay.. oalmoe'lerhod oc Epe7 1'. dn.. 2xfftIlwrtttncal\n",
      "Iter-360 loss: 91.3746\n",
      "emans ce wtnw ae lhe per antiaion, c  ar fsrEpoe anl pirrsnd winm9erNwahoc is pp\"teua ard iectiiol. J\n",
      "Iter-370 loss: 92.3964\n",
      "eohsece anne Setpeiohe J-tpel hao, Gnr iPhtlfansnd Spa, ts tai aestn5mEpta, Jaha hn wmnone on ponsenp\n",
      "Iter-380 loss: 94.9673\n",
      "ewD cne onh eIctonsavan. minr eron 4ci hs wwenmins attsuantia o'a. akonttcrfvlhaf fnsn. waf'hn\n",
      " ald o\n",
      "Iter-390 loss: 94.3385\n",
      "eiiaa tontoseah il p\"ah , iponsttoiocgeslleod Tate ttf lolriponly. -hr, Esnaid lf whndure g 1se ltarl\n",
      "Iter-400 loss: 94.5930\n",
      "ePhniPau t6u intie he lh ana cand Thh mane iatserte paesercertfee tmy anr inherlaie0 padgd wores en o\n",
      "Iter-410 loss: 95.2412\n",
      "eSaa td 8donuty oear ioe.stlie ttoa lno3eos orattmi, msse are s, Tsotenoid arrppara iwinselscnu. iae \n",
      "Iter-420 loss: 102.4897\n",
      "euienirs od fuper ay Feriorerhudnsea \"hi fpnmstcrlatmares Nared tiss cpohe mTan torlun ttn Wmpe retsd\n",
      "Iter-430 loss: 96.9008\n",
      "e ated sa cnI an (tho pieol ae ah tau yotWatdglarim uau dnd Ghge usstbo 1fiunie  end oiebandnnir oe 9\n",
      "Iter-440 loss: 97.0621\n",
      "erh8tgsre. i, 2aecutht wla3ttEmaccukeidxirdgayasiiahr9rbannres ces ofonses ItThevlsr hgllhe tmoa ontl\n",
      "Iter-450 loss: 96.5368\n",
      "elaal urD \"r poatunpepaur's's entsnde8d ued.s Dona'st ioven3nitun art rn entinicace it thed ss and pn\n",
      "Iter-460 loss: 104.2695\n",
      "eoe rd5ns'rgae tle iorolend O Nbanif Jaion'f tJah, an oy ae catse'(Aflcigoui sh tie vinfoaev on hntdk\n",
      "Iter-470 loss: 93.2711\n",
      "epa3 ece wty an–本 ai the Won ftthesin cm hng ind colat aol 8fh inothsi'w aoiltanrtls thie mpoenu iand\n",
      "Iter-480 loss: 90.5456\n",
      "eaaa. Gth-laiwhundidtty, thh Iedo3ox len insittaed pnkh ese ietNforumpha, lcentewmrhe y Jarompgnke Sa\n",
      "Iter-490 loss: 89.2855\n",
      "elaiaionitcenf cowond oborigid-rsgrnam fel Sefothe cape Gk aed lhon frar irpnnso tied ml aan, pofy ta\n",
      "Iter-500 loss: 88.7121\n",
      "euaereseltandimpenhe'laus  anttitsclnaon fe, O9Koalofanf Jhantd. Eastthe; .ee, Cupanamsaded aotiiry a\n",
      "Iter-510 loss: 88.5812\n",
      "e apareopat thenrpan the fed Gwinas Stind bplansChes tnsi fkanase iI varru-Srphesos mmud Ttopaiss NIe\n",
      "Iter-520 loss: 88.0938\n",
      "eoPse omal Jaelmorlyytar Nompare ied e thl an 4f if tol tam thor chcy ore\", Iushtsn eh npondee of Tin\n",
      "Iter-530 loss: 86.8697\n",
      "ey topon's  rnct Wpges, Ap4m I d', ih indg tower npecerintioun ef an werse th Socalotes angorais nNia\n",
      "Iter-540 loss: 87.9023\n",
      "eiiharhelof lota pos ullhtocetor sh ohartte ebon-Sparsonceenkheeuc pos tonay Jararta  Emfnbiin mvenrn\n",
      "Iter-550 loss: 84.0062\n",
      "e ay Ropcporega, Duld'nrthoran omettoldeltn mocund o ese orowind iptan thigdt che in topen9ase redlc,\n",
      "Iter-560 loss: 84.6759\n",
      "e iiheg chrtenge iololateroled the rery. Sed ririnrtir hhe alowact, Snstdncy \"mre, thid mandorand Cao\n",
      "Iter-570 loss: 82.8504\n",
      "eBah a cina,ao palareped irsor iraed lie cepeos. mlose cos o upenir Thh stes s ar WrlacioselAn0ting p\n",
      "Iter-580 loss: 81.8755\n",
      "elon toreois ised Iegest cece ma tuloki fesond cest wos mi uilicilofe aullallden Eansrof rhl se 1ofev\n",
      "Iter-590 loss: 81.9621\n",
      "ey haf y pe outouse ki,ang ch uand angerth the Ntvond orung Oaled bo enliced or, Cherhe Aapires en an\n",
      "Iter-600 loss: 82.4509\n",
      "eJaa'd wea a gebasaly, whithe wonsa-lppan Sest. ensy th onrtho che fauke altoreva an cocil woseciges \n",
      "Iter-610 loss: 80.9869\n",
      "eateeulotatorrurgibet Phe Iro, h Umpan worm nrtte'p anh che potitake o ches otivenciacithe iap Jaige,\n",
      "Iter-620 loss: 87.2989\n",
      "eAgaea7garlcaliky laos sand Jpiane, Fnusirit SiretaI dli, oude th ike aar. mof Jppcotif Taon9fofiimif\n",
      "Iter-630 loss: 86.0459\n",
      "eola oy, wege fincertcarld'kren28sos, i ot ArEeound mos Wanthirt inacocess win ci5 at-ce thesn thetoo\n",
      "Iter-640 loss: 85.9783\n",
      "erhiri, 2826 fu pat saturnoos S10. 17\"% tat merus  ig Wpn oan on ceetd Pan mon thif cupan ror Ubontd \n",
      "Iter-650 loss: 85.5826\n",
      "eiotat-Jghaintho und . Japa Iry Japat ipente teatle 1red ule atteobete lankese Jaabla5 pary. , Jaan C\n",
      "Iter-660 loss: 84.2117\n",
      "eoo mf alerld, murocos. Sityd Wys sumtaougela, imle d Teopingirl0 iniritol Cthegtore os Wind Seam Rop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 82.7196\n",
      "eoy ttl unteeled ban firgicn worgnf. lo orof. rn Roso unfd ba, Jbooetto Japanar parothe1%e thi, gente\n",
      "Iter-680 loss: 80.8377\n",
      "ery  East ottto the iald ware erppen, wes, Wir cinjrund aorty en me ony Simon Ird mpinke, igurtar c-p\n",
      "Iter-690 loss: 79.9184\n",
      "em e a. Wowis rtd Cy. The th an mertos wartor. finndt und the chr sm Gwake f OChicsy so fh surededse \n",
      "Iter-700 loss: 78.6107\n",
      "eme aeopiniles an sude the Wan6 G3 chiartd Gsucthe (s7mesind and to ang are d M's at Irive paritaroki\n",
      "Iter-710 loss: 76.7443\n",
      "ena.steirerla Ugolaed coon mrirtde of Gary expae onte AnCabit ingextfeot ton the wobexsict he hhaweov\n",
      "Iter-720 loss: 76.6329\n",
      "e wiol and ind stilatssh ars gepeh fed Ind Jaby ance tatd d lan ton inceithi ialathe, me Eserty Caran\n",
      "Iter-730 loss: 76.7479\n",
      "e nf. the Cholcf-derhe in kof-9simeilaly tis chereodargesy wiaes fisa th is bece ins th chorcy moth. \n",
      "Iter-740 loss: 77.3363\n",
      "eunfeunf ing is ind inas mebein thilt itarl Iopiry luceime the wy the wirlt o pore tiingevelasillina \n",
      "Iter-750 loss: 97.9139\n",
      "epr d Thtc Jacse 1d ln d slihd Ca5d statothietr pedes lte. Japoln sy dgr botol ciok War ep Wir tar th\n",
      "Iter-760 loss: 82.6786\n",
      "e nasa5 , Ke'stsedsy\n",
      ".s in fed pol chegha, 20rlan, Jo1sk d th isindeciy. ,sgontd onJapan efece es an \n",
      "Iter-770 loss: 81.7779\n",
      "eTad wtsed d'835 rs on Esotltse slalal  eupeh irl hhes imear atdrr intsy Nhand an derf o, Thar and th\n",
      "Iter-780 loss: 83.1275\n",
      "etio twoptrinir ry pertber pthiteror, Aspant ar mis, shit Res. kop ofeuxcith ls bertheereasg 1本 Jaiof\n",
      "Iter-790 loss: 82.0644\n",
      "ed \"96 toovekl the oplota. Frudtcelin oslaliovoired pstaronie an thetuocteofocetry the CEwpl tan thy \n",
      "Iter-800 loss: 82.2052\n",
      "e a tapanderare as be , \"Asini as in he fof.rcesu-Japao,8ffmoror \" tar tanind Jaary a1d ul nof.rIn th\n",
      "Iter-810 loss: 82.9439\n",
      "erufes, sul Jamnirlg axmorgesed inje Ga an the of ifidiwe ia, Ia u5 29und indhird uieh, Nosatearg ghy\n",
      "Iter-820 loss: 82.4782\n",
      "e.-ph 10unwor slia8 anicy fhes to cofle carled in Ualdan andesr an NoJausiun t1khile28 s-rns-fad dn 1\n",
      "Iter-830 loss: 82.9243\n",
      "er eiecu-uturgins an ar Naupas tourbetxt miinece coumirderider omith largestce Eber o alhithti. Sirfi\n",
      "Iter-840 loss: 82.0128\n",
      "efeaJaE17is of., ry Japan torte the miantle pponcecrpent ains sen, an in'ld's-mang bor ghiwsdinn mill\n",
      "Iter-850 loss: 85.1689\n",
      "erh itored'sd cse is ad inan Humcakle Gtionl R. 6osctd sed indipa beuning firurmureve timledinjounth \n",
      "Iter-860 loss: 81.8511\n",
      "elar158rpang hinar as oby Tihi iand JiAwhake the bmot an an8sea, Korufunse G3rld'n. Kyidg pparon'2017\n",
      "Iter-870 loss: 81.7076\n",
      "e ntI phl Miveotebion orcmonS spata boun urtho, sty Iath. Japave olis ls chargh the the tearp a ponas\n",
      "Iter-880 loss: 74.6644\n",
      "e nnwns isr ODot Cerleg est upy atleg worese foli margeitad ae an\"d Palis-onang nnfe argino anflule n\n",
      "Iter-890 loss: 73.4434\n",
      "e nr lwrt CGLhite in virly was riin's of , wor wha tary lhol Suttoreg Japing worgist intiin he Rud884\n",
      "Iter-900 loss: 74.9583\n",
      "e pe aed tho. whested ethe 1s, the Wares , ton fein monst una, the wirted on aruheshlear fise W cte t\n",
      "Iter-910 loss: 73.9362\n",
      "e np pordiirt unhilaat manJepan eatind Iry thilasly sl coulal ans Is Sea. The iand ise wor the wekte \n",
      "Iter-920 loss: 74.6424\n",
      "erislvery sf wondes in ricty tre pe. the in ip pens ind the chovy 9dilalote and sned Oriceir und eopl\n",
      "Iter-930 loss: 73.7725\n",
      "eJunoid Japanitec Soun inity. thld's the the thed pe picnr-dsre un the whe thorgid. porldste the thit\n",
      "Iter-940 loss: 74.5359\n",
      "eJacae\"a Kurery 926 frmerinth anteir lhar Eo Jainaf Honios pina8. Japolare mauned ors Japition in pin\n",
      "Iter-950 loss: 72.4160\n",
      "e. Tasoinhe in expeile'p teestry Japalaran exorgest the by sele heas venory thesio thiInd the Warund \n",
      "Iter-960 loss: 72.4576\n",
      "e r in. Lanhted ig ran weg chtin tea, Japer arlat antand whaltot Alted sibornsexf pertyrbone, redeng \n",
      "Iter-970 loss: 73.0541\n",
      "eof l日rSifery I8 imago alo, ins on en\"I ban roudaf the Wan mu ron pend is os son iom of dathiarst teo\n",
      "Iter-980 loss: 90.3438\n",
      "ed  Asoes deslabines an es, pollioliys th isgeuwyd th-dalt an pexndeben medry Wer tocectgecp so besi \n",
      "Iter-990 loss: 88.7558\n",
      "e rfamlinitomesro an the sa iumand the anct bu Hoc\"ousofentheretoy Tiopan .\n",
      " therin ou9) Jhiantironan\n",
      "Iter-1000 loss: 84.0567\n",
      "e nswis dld 1 In the \" y5 D8, sfofith and of th ity ite, migicg civarilrireer in-popo fe ie rrn thy 2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYAnBGGOMxxKCMcYYAP4fDstSAyv77nwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1100c5518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
