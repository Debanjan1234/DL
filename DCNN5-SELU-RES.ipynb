{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import impl.utils as utils\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# import numpy as np\n",
    "import impl.loss as loss_fun\n",
    "import impl.layer as l\n",
    "import impl.NN as nn\n",
    "\n",
    "class DCNN(nn.NN):\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout, lam=1e-3, loss='cross_ent', nonlin='relu'):\n",
    "        self.p_dropout = p_dropout\n",
    "        self.loss = loss\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        super().__init__(D, C, H, lam, p_dropout, loss, nonlin)\n",
    "        \n",
    "    def _init_model(self, D, C, H):\n",
    "        self.model = []\n",
    "        \n",
    "        # Input layer of Conv\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "            W1_res=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1_res=np.zeros((H, 1))\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        self.bn_caches = []\n",
    "        for layer in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "                    W2_res=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2_res=np.zeros((H, 1))\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    #     # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"Dropout to a value with rescaling.\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    #     # NumPy implementation\n",
    "    #     def dropout_selu_forward(self, X, p_dropout):\n",
    "    #         #         alpha = 1.6732632423543772848170429916717\n",
    "    #         #         scale = 1.0507009873554804934193349852946\n",
    "    #         #         alpha * scale = 1.7580993408473766\n",
    "    #         alpha= -1.7580993408473766\n",
    "    #         fixedPointMean=0.0 \n",
    "    #         fixedPointVar=1.0\n",
    "    #         keep_prob = 1.0 - p_dropout\n",
    "    #         #     noise_shape = noise_shape.reshape(X)\n",
    "    #         random_tensor = keep_prob\n",
    "    #         #         random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "    #         random_tensor += np.random.uniform(size=X.shape) # low=0, high=1\n",
    "    #         #         binary_tensor = math_ops.floor(random_tensor)\n",
    "    #         binary_tensor = np.floor(random_tensor)\n",
    "    #         #         ret = X * binary_tensor + alpha * (1-binary_tensor)\n",
    "    #         X = X * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "    #         #         a = tf.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * tf.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "    #         a = np.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * ((alpha-fixedPointMean)**2) + fixedPointVar)))\n",
    "\n",
    "    #         b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "    #         #         ret = a * ret + b\n",
    "    #         out = a * X + b\n",
    "    #         #         ret.set_shape(x.get_shape())\n",
    "    #         #         ret = ret.reshape(X.shape)\n",
    "    #         cache = a, binary_tensor\n",
    "    #         return out, cache\n",
    "\n",
    "    #     def dropout_selu_backward(self, dout, cache):\n",
    "    #         a, binary_tensor = cache\n",
    "    #         dX = dout * a\n",
    "    #         dX = dX * binary_tensor\n",
    "    #         return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train=False):\n",
    "\n",
    "        # 1st layer: Input to Conv\n",
    "        h1_res, conv1_res_cache = l.conv_forward(X=X, W=self.model[0]['W1_res'], b=self.model[0]['b1_res']) \n",
    "        h1, conv1_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1']) \n",
    "        #         h2, nl1_cache = l.relu_forward(h1)\n",
    "        h2, nl1_cache = self.selu_forward(h1)\n",
    "        do1_cache = None\n",
    "        #         if train: h2, do1_cache = self.dropout_selu_forward(X=h2, p_dropout=self.p_dropout)\n",
    "        if train: h2, do1_cache = self.alpha_dropout_fwd(h=h2, q=1.0-self.p_dropout)\n",
    "        # x = x + f(x)\n",
    "        h1_res += h2\n",
    "        h1_cache = (conv1_res_cache, conv1_cache, nl1_cache, do1_cache)\n",
    "\n",
    "        ###########################################################################################\n",
    "        h2_cache = []\n",
    "        for layer in range(self.L):\n",
    "            # midst layer: Convnet 1\n",
    "            if not layer == 0: h2 = h2.reshape(nl1_cache.shape)\n",
    "\n",
    "            h2_res, conv2_res_cache = l.conv_forward(X=h2, W=self.model[1][layer]['W2_res'], \n",
    "                                                     b=self.model[1][layer]['b2_res'])\n",
    "            h2_res = h2_res.reshape([nl1_cache.shape[0], -1])\n",
    "\n",
    "            h2, conv2_cache = l.conv_forward(X=h2, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "            \n",
    "            #             h2, nl2_cache = l.relu_forward(h2)\n",
    "            h2, nl2_cache = self.selu_forward(X=h2)\n",
    "            do2_cache = None # ERROR: referenced before assigned!\n",
    "            #             if train: h2, do2_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "            #             if train: h2, do2_cache = self.dropout_selu_forward(X=h2, p_dropout=self.p_dropout)\n",
    "            if train: h2, do2_cache = self.alpha_dropout_fwd(h=h2, q=1.0-self.p_dropout)\n",
    "            h2_res += h2    \n",
    "\n",
    "            cache = (conv2_res_cache, conv2_cache, nl2_cache, do2_cache)\n",
    "            h2_cache.append(cache)\n",
    "        ############################################################################################\n",
    "            \n",
    "        # last layer : FC to Output\n",
    "        h3, h3_cache = l.fc_forward(X=h2, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "\n",
    "        cache = (h1_cache, h2_cache, h3_cache)\n",
    "        return h3, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "        h1_cache, h2_cache, h3_cache = cache\n",
    "        conv1_res_cache, conv1_cache, nl1_cache, do1_cache = h1_cache\n",
    "\n",
    "        # last layer\n",
    "        dh2, dw3, db3 = l.fc_backward(dout=dy, cache=h3_cache)\n",
    "        \n",
    "        # midst layer 2\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            conv2_res_cache, conv2_cache, nl2_cache, do2_cache = h2_cache[layer]\n",
    "            dh2_res = dh2.reshape(nl1_cache.shape)\n",
    "            dh2_res, dw2_res, db2_res = l.conv_backward(dout=dh2_res, cache=conv2_res_cache)\n",
    "            #             dh2 = l.dropout_backward(dout=dh2, cache=do2_cache)\n",
    "            #             dh2 = self.dropout_selu_backward(dout=dh2, cache=do2_cache)\n",
    "            dh2 = self.alpha_dropout_bwd(dout=dh2, cache=do2_cache)\n",
    "            #             dh2 = l.relu_backward(dout=dh2, cache=nl2_cache)\n",
    "            dh2 = self.selu_backward(dout=dh2, cache=nl2_cache)\n",
    "            dh2 = dh2.reshape(nl1_cache.shape)\n",
    "            dh2, dw2, db2 = l.conv_backward(dout=dh2, cache=conv2_cache)\n",
    "            dh2 += dh2_res\n",
    "            if not layer==0: dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2,\n",
    "                    W2_res=dw2_res,\n",
    "                    b2_res=db2_res\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer\n",
    "        #         conv1_res_cache, conv1_cache, nl1_cache, do1_cache = h1_cache\n",
    "        dX_res, dw1_res, db1_res = l.conv_backward(dout=dh2, cache=conv1_res_cache)\n",
    "        #         dh2 = self.dropout_selu_backward(dout=dh2, cache=do1_cache)\n",
    "        dh2 = self.alpha_dropout_bwd(dout=dh2, cache=do1_cache)\n",
    "        #         dh1 = l.relu_backward(dout=dh2, cache=nl1_cache)\n",
    "        dh1 = self.selu_backward(dout=dh2, cache=nl1_cache)\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh1, cache=conv1_cache)\n",
    "        dX += dX_res\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1,\n",
    "            W1_res=dw1_res, \n",
    "            b1_res=db1_res\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "# import numpy as np\n",
    "# import impl.utils as util\n",
    "# import impl.constant as c\n",
    "# import copy\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam(nn, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size)\n",
    "\n",
    "    if val_set:\n",
    "        X_val, y_val = val_set\n",
    "        \n",
    "    M, R = [], []\n",
    "    M.append({k: np.zeros_like(v) for k, v in nn.model[0].items()})\n",
    "    R.append({k: np.zeros_like(v) for k, v in nn.model[0].items()})\n",
    "    \n",
    "    M_, R_ = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M_.append({k: np.zeros_like(v) for k, v in nn.model[1][layer].items()})\n",
    "        R_.append({k: np.zeros_like(v) for k, v in nn.model[1][layer].items()})\n",
    "    M.append(M_)\n",
    "    R.append(R_)\n",
    "\n",
    "    M.append({k: np.zeros_like(v) for k, v in nn.model[2].items()})\n",
    "    R.append({k: np.zeros_like(v) for k, v in nn.model[2].items()})\n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        #         grad, loss = nn.train_step(X_mini, y_mini)\n",
    "        #         def train_step(self, X_train, y_train):\n",
    "        #         \"\"\"\n",
    "        #         Single training step over minibatch: forward, loss, backprop\n",
    "        #         \"\"\"\n",
    "        y, cache = nn.forward(X_mini, train=True)\n",
    "        loss, dy = nn.loss_function(y, y_mini)\n",
    "        dX, grad = nn.backward(dy, cache)\n",
    "        #         return grad, loss\n",
    "\n",
    "        if iter % print_after == 0:\n",
    "            if val_set:\n",
    "                val_acc = l.accuracy(y_val, nn.test(X_val))\n",
    "                print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))\n",
    "\n",
    "        for k in grad[0]:\n",
    "            M[0][k] = l.exp_running_avg(M[0][k], grad[0][k], beta1)\n",
    "            R[0][k] = l.exp_running_avg(R[0][k], grad[0][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[0][k] / (1. - (beta1**(iter)))\n",
    "            r_k_hat = R[0][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "            nn.model[0][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "        \n",
    "        for layer in range(nn.L):\n",
    "            for k in grad[1][0]:\n",
    "                M[1][layer][k] = l.exp_running_avg(M[1][layer][k], grad[1][layer][k], beta1)\n",
    "                R[1][layer][k] = l.exp_running_avg(R[1][layer][k], grad[1][layer][k]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[1][layer][k] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[1][layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                nn.model[1][layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                        \n",
    "        for k in grad[2]:\n",
    "            M[2][k] = l.exp_running_avg(M[2][k], grad[2][k], beta1)\n",
    "            R[2][k] = l.exp_running_avg(R[2][k], grad[2][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[2][k] / (1. - (beta1**(iter)))\n",
    "            r_k_hat = R[2][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "            nn.model[2][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 40 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 2 # depth \n",
    "print_after = 1 # print loss for train, valid, and test\n",
    "num_hidden_units = 8\n",
    "p_dropout = 0.05 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 26.9109 validation accuracy: 0.080400\n",
      "Iter-2 training loss: 19.0822 validation accuracy: 0.120200\n",
      "Iter-3 training loss: 15.1873 validation accuracy: 0.109400\n",
      "Iter-4 training loss: 16.0085 validation accuracy: 0.077800\n",
      "Iter-5 training loss: 18.9570 validation accuracy: 0.086000\n",
      "Iter-6 training loss: 15.2785 validation accuracy: 0.108600\n",
      "Iter-7 training loss: 11.2255 validation accuracy: 0.145800\n",
      "Iter-8 training loss: 11.9566 validation accuracy: 0.194000\n",
      "Iter-9 training loss: 9.9362 validation accuracy: 0.226000\n",
      "Iter-10 training loss: 11.6925 validation accuracy: 0.264200\n",
      "Iter-11 training loss: 10.5592 validation accuracy: 0.307800\n",
      "Iter-12 training loss: 10.7979 validation accuracy: 0.332600\n",
      "Iter-13 training loss: 8.3487 validation accuracy: 0.362200\n",
      "Iter-14 training loss: 9.8731 validation accuracy: 0.401400\n",
      "Iter-15 training loss: 9.4016 validation accuracy: 0.466800\n",
      "Iter-16 training loss: 8.6683 validation accuracy: 0.514400\n",
      "Iter-17 training loss: 10.0367 validation accuracy: 0.544600\n",
      "Iter-18 training loss: 7.0920 validation accuracy: 0.530200\n",
      "Iter-19 training loss: 6.0064 validation accuracy: 0.517000\n",
      "Iter-20 training loss: 5.8154 validation accuracy: 0.523800\n",
      "Iter-21 training loss: 5.4049 validation accuracy: 0.542600\n",
      "Iter-22 training loss: 5.0532 validation accuracy: 0.567400\n",
      "Iter-23 training loss: 6.2889 validation accuracy: 0.592800\n",
      "Iter-24 training loss: 5.9221 validation accuracy: 0.621600\n",
      "Iter-25 training loss: 8.5977 validation accuracy: 0.636600\n",
      "Iter-26 training loss: 3.6363 validation accuracy: 0.636600\n",
      "Iter-27 training loss: 4.2736 validation accuracy: 0.611800\n",
      "Iter-28 training loss: 7.3849 validation accuracy: 0.588000\n",
      "Iter-29 training loss: 5.8810 validation accuracy: 0.572200\n",
      "Iter-30 training loss: 6.1375 validation accuracy: 0.568000\n",
      "Iter-31 training loss: 5.8567 validation accuracy: 0.577800\n",
      "Iter-32 training loss: 6.5926 validation accuracy: 0.592600\n",
      "Iter-33 training loss: 3.9782 validation accuracy: 0.615800\n",
      "Iter-34 training loss: 4.6335 validation accuracy: 0.637600\n",
      "Iter-35 training loss: 5.1854 validation accuracy: 0.657400\n",
      "Iter-36 training loss: 3.0993 validation accuracy: 0.686000\n",
      "Iter-37 training loss: 4.4996 validation accuracy: 0.709200\n",
      "Iter-38 training loss: 4.5611 validation accuracy: 0.729400\n",
      "Iter-39 training loss: 3.7004 validation accuracy: 0.744400\n",
      "Iter-40 training loss: 4.1888 validation accuracy: 0.749000\n",
      "\n",
      "Test Mean accuracy: 0.7628, std: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# build the model/NN and learn it: running session.\n",
    "net = DCNN(C=C, D=D, H=num_layers, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "net = adam(nn=net, X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = net.predict(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "\n",
    "print()\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
