{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.utils as utils\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "X_train, X_val, X_test = utils.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "# import numpy as np\n",
    "import impl.loss as loss_fun\n",
    "import impl.layer as l\n",
    "import impl.NN as nn\n",
    "\n",
    "class DCNN(nn.NN):\n",
    "\n",
    "    def __init__(self, D, C, H, p_dropout, lam=1e-3, loss='cross_ent', nonlin='relu'):\n",
    "        self.p_dropout = p_dropout # prob, keep_prob, or prob_dropout\n",
    "        self.loss = loss # minimum cross_entropy\n",
    "        self.mode = 'classification'\n",
    "        super().__init__(D, C, H, lam, p_dropout, loss, nonlin)\n",
    "        \n",
    "    def _init_model(self, D, C, H):\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1))\n",
    "        ))\n",
    "        self.model.append(dict(\n",
    "            W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "            b2=np.zeros((H, 1)),\n",
    "            gamma2=np.ones((1, H*D)),\n",
    "            beta2=np.zeros((1, H*D)),\n",
    "\n",
    "            W22=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "            b22=np.zeros((H, 1)),\n",
    "            gamma22=np.ones((1, H*D)),\n",
    "            beta22=np.zeros((1, H*D)),\n",
    "\n",
    "            W23=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "            b23=np.zeros((H, 1)),\n",
    "            gamma23=np.ones((1, H*D)),\n",
    "            beta23=np.zeros((1, H*D)),\n",
    "\n",
    "            W24=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "            b24=np.zeros((H, 1)),\n",
    "            gamma24=np.ones((1, H*D)),\n",
    "            beta24=np.zeros((1, H*D)),\n",
    "\n",
    "            W25=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "            b25=np.zeros((H, 1)),\n",
    "            gamma25=np.ones((1, H*D)),\n",
    "            beta25=np.zeros((1, H*D)),\n",
    "        ))\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "        self.bn_caches = dict(            \n",
    "            bn2_mean=np.zeros((1, H*D)),\n",
    "            bn2_var=np.zeros((1, H*D)),\n",
    "\n",
    "            bn22_mean=np.zeros((1, H*D)),\n",
    "            bn22_var=np.zeros((1, H*D)),\n",
    "\n",
    "            bn23_mean=np.zeros((1, H*D)),\n",
    "            bn23_var=np.zeros((1, H*D)),\n",
    "\n",
    "            bn24_mean=np.zeros((1, H*D)),\n",
    "            bn24_var=np.zeros((1, H*D)),\n",
    "\n",
    "            bn25_mean=np.zeros((1, H*D)),\n",
    "            bn25_var=np.zeros((1, H*D))\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, train=False):\n",
    "\n",
    "        # 1st layer: Input to Conv\n",
    "        h1, h1_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1']) \n",
    "        h1, nl1_cache = l.relu_forward(h1)\n",
    "\n",
    "        ############################################################################################\n",
    "        # midst layer: Convnet 1\n",
    "        h2, h2_cache = l.conv_forward(X=h1, W=self.model[1]['W2'], b=self.model[1]['b2'])\n",
    "        h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "        h2, bn2_cache, self.bn_caches['bn2_mean'], self.bn_caches['bn2_var'] = l.bn_forward(h2, \n",
    "                                                self.model[1]['gamma2'], self.model[1]['beta2'], \n",
    "                                                (self.bn_caches['bn2_mean'], self.bn_caches['bn2_var']), \n",
    "                                                train=train)\n",
    "        h2, nl2_cache = l.relu_forward(h2)\n",
    "        do2_cache = None # ERROR: referenced before assigned!\n",
    "        if train: h2, do2_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "        \n",
    "        # midst layer: Convnet 2\n",
    "        h2 = h2.reshape(nl1_cache.shape)\n",
    "        h2, h22_cache = l.conv_forward(X=h2, W=self.model[1]['W22'], b=self.model[1]['b22'])\n",
    "        h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "        h2, bn22_cache, self.bn_caches['bn22_mean'], self.bn_caches['bn22_var'] = l.bn_forward(h2, \n",
    "                                                self.model[1]['gamma22'], self.model[1]['beta22'], \n",
    "                                                (self.bn_caches['bn22_mean'], self.bn_caches['bn22_var']), \n",
    "                                                train=train)\n",
    "        h2, nl22_cache = l.relu_forward(h2)\n",
    "        do22_cache = None # ERROR: referenced before assigned!\n",
    "        if train: h2, do22_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "\n",
    "        # midst layer: Convnet 3\n",
    "        h2 = h2.reshape(nl1_cache.shape)\n",
    "        h2, h23_cache = l.conv_forward(X=h2, W=self.model[1]['W23'], b=self.model[1]['b23'])\n",
    "        h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "        h2, bn23_cache, self.bn_caches['bn23_mean'], self.bn_caches['bn23_var'] = l.bn_forward(h2, \n",
    "                                                self.model[1]['gamma23'], self.model[1]['beta23'], \n",
    "                                                (self.bn_caches['bn23_mean'], self.bn_caches['bn23_var']), \n",
    "                                                train=train)\n",
    "        h2, nl23_cache = l.relu_forward(h2)\n",
    "        do23_cache = None # ERROR: referenced before assigned!\n",
    "        if train: h2, do23_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "            \n",
    "        # midst layer: Convnet 4\n",
    "        h2 = h2.reshape(nl1_cache.shape)\n",
    "        h2, h24_cache = l.conv_forward(X=h2, W=self.model[1]['W24'], b=self.model[1]['b24'])\n",
    "        h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "        h2, bn24_cache, self.bn_caches['bn24_mean'], self.bn_caches['bn24_var'] = l.bn_forward(h2, \n",
    "                                                self.model[1]['gamma24'], self.model[1]['beta24'], \n",
    "                                                (self.bn_caches['bn24_mean'], self.bn_caches['bn24_var']), \n",
    "                                                train=train)\n",
    "        h2, nl24_cache = l.relu_forward(h2)\n",
    "        do24_cache = None # ERROR: referenced before assigned!\n",
    "        if train: h2, do24_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "            \n",
    "        # midst layer: Convnet 5\n",
    "        h2 = h2.reshape(nl1_cache.shape)\n",
    "        h2, h25_cache = l.conv_forward(X=h2, W=self.model[1]['W25'], b=self.model[1]['b25'])\n",
    "        h2 = h2.reshape([nl1_cache.shape[0], -1])\n",
    "        h2, bn25_cache, self.bn_caches['bn25_mean'], self.bn_caches['bn25_var'] = l.bn_forward(h2, \n",
    "                                                self.model[1]['gamma25'], self.model[1]['beta25'], \n",
    "                                                (self.bn_caches['bn25_mean'], self.bn_caches['bn25_var']), \n",
    "                                                train=train)\n",
    "        h2, nl25_cache = l.relu_forward(h2)\n",
    "        do25_cache = None # ERROR: referenced before assigned!\n",
    "        if train: h2, do25_cache = l.dropout_forward(X=h2, p_dropout=self.p_dropout)\n",
    "        ############################################################################################\n",
    "            \n",
    "        # last layer : FC to Output\n",
    "        h3, h3_cache = l.fc_forward(X=h2, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "\n",
    "        cache = (h1_cache, nl1_cache, h2_cache, bn2_cache, nl2_cache, do2_cache, h22_cache, bn22_cache, nl22_cache, do22_cache, h23_cache, bn23_cache, nl23_cache, do23_cache, h24_cache, bn24_cache, nl24_cache, do24_cache, h25_cache, bn25_cache, nl25_cache, do25_cache, h3_cache)\n",
    "        return h3, cache\n",
    "    \n",
    "    def loss_function(self, y, y_train):\n",
    "#         loss = self.loss_funs[self.loss](self.model[3], y, y_train, self.lam)\n",
    "        loss = self.loss_funs[self.loss](y, y_train)\n",
    "        dy = self.dloss_funs[self.loss](y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache):\n",
    "\n",
    "        h1_cache, nl1_cache, h2_cache, bn2_cache, nl2_cache, do2_cache, h22_cache, bn22_cache, nl22_cache, do22_cache, h23_cache, bn23_cache, nl23_cache, do23_cache, h24_cache, bn24_cache, nl24_cache, do24_cache, h25_cache, bn25_cache, nl25_cache, do25_cache, h3_cache = cache\n",
    "\n",
    "        # last layer\n",
    "        dh2, dw3, db3 = l.fc_backward(dout=dy, cache=h3_cache)\n",
    "        \n",
    "        # midst layer 5\n",
    "        dh2 = l.dropout_backward(dout=dh2, cache=do25_cache)\n",
    "        dh2 = l.relu_backward(dout=dh2, cache=nl25_cache)\n",
    "        dh2, dgamma25, dbeta25 = l.bn_backward(dout=dh2, cache=bn25_cache)\n",
    "        dh2 = dh2.reshape(nl1_cache.shape)\n",
    "        dh2, dw25, db25 = l.conv_backward(dout=dh2, cache=h25_cache)\n",
    "        dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "\n",
    "        # midst layer 4\n",
    "        dh2 = l.dropout_backward(dout=dh2, cache=do24_cache)\n",
    "        dh2 = l.relu_backward(dout=dh2, cache=nl24_cache)\n",
    "        dh2, dgamma24, dbeta24 = l.bn_backward(dout=dh2, cache=bn24_cache)\n",
    "        dh2 = dh2.reshape(nl1_cache.shape)\n",
    "        dh2, dw24, db24 = l.conv_backward(dout=dh2, cache=h24_cache)\n",
    "        dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "        \n",
    "        # midst layer 3\n",
    "        dh2 = l.dropout_backward(dout=dh2, cache=do23_cache)\n",
    "        dh2 = l.relu_backward(dout=dh2, cache=nl23_cache)\n",
    "        dh2, dgamma23, dbeta23 = l.bn_backward(dout=dh2, cache=bn23_cache)\n",
    "        dh2 = dh2.reshape(nl1_cache.shape)\n",
    "        dh2, dw23, db23 = l.conv_backward(dout=dh2, cache=h23_cache)\n",
    "        dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "        \n",
    "        # midst layer 2\n",
    "        dh2 = l.dropout_backward(dout=dh2, cache=do22_cache)\n",
    "        dh2 = l.relu_backward(dout=dh2, cache=nl22_cache)\n",
    "        dh2, dgamma22, dbeta22 = l.bn_backward(dout=dh2, cache=bn22_cache)\n",
    "        dh2 = dh2.reshape(nl1_cache.shape)\n",
    "        dh2, dw22, db22 = l.conv_backward(dout=dh2, cache=h22_cache)\n",
    "        dh2 = dh2.reshape([nl1_cache.shape[0], -1])\n",
    "\n",
    "        # midst layer 1\n",
    "        dh2 = l.dropout_backward(dout=dh2, cache=do2_cache)\n",
    "        dh2 = l.relu_backward(dout=dh2, cache=nl2_cache)\n",
    "        dh2, dgamma2, dbeta2 = l.bn_backward(dout=dh2, cache=bn2_cache)\n",
    "        dh2 = dh2.reshape(nl1_cache.shape)\n",
    "        dh1, dw2, db2 = l.conv_backward(dout=dh2, cache=h2_cache)\n",
    "\n",
    "        # 1st layer\n",
    "        dh1 = l.relu_backward(dout=dh1, cache=nl1_cache)\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh1, cache=h1_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        grad.append(dict(\n",
    "            W2=dw2, \n",
    "            b2=db2,\n",
    "            gamma2=dgamma2,\n",
    "            beta2=dbeta2,\n",
    "            \n",
    "            W22=dw22, \n",
    "            b22=db22,\n",
    "            gamma22=dgamma22,\n",
    "            beta22=dbeta22,\n",
    "\n",
    "            W23=dw23, \n",
    "            b23=db23,\n",
    "            gamma23=dgamma23,\n",
    "            beta23=dbeta23,\n",
    "            \n",
    "            W24=dw24, \n",
    "            b24=db24,\n",
    "            gamma24=dgamma24,\n",
    "            beta24=dbeta24,\n",
    "\n",
    "            W25=dw25, \n",
    "            b25=db25,\n",
    "            gamma25=dgamma25,\n",
    "            beta25=dbeta25\n",
    "        ))\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = util.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "# import numpy as np\n",
    "import impl.utils as util\n",
    "import impl.constant as c\n",
    "import copy\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam(nn, X_train, y_train, val_set=None, alpha=0.001, mb_size=256, n_iter=2000, print_after=100):\n",
    "    M, R = [], []\n",
    "    M.append({k: np.zeros_like(v) for k, v in nn.model[0].items()})\n",
    "    R.append({k: np.zeros_like(v) for k, v in nn.model[0].items()})\n",
    "    M.append({k: np.zeros_like(v) for k, v in nn.model[1].items()})\n",
    "    R.append({k: np.zeros_like(v) for k, v in nn.model[1].items()})\n",
    "    M.append({k: np.zeros_like(v) for k, v in nn.model[2].items()})\n",
    "    R.append({k: np.zeros_like(v) for k, v in nn.model[2].items()})\n",
    "    beta1 = .9\n",
    "    beta2 = .999\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size)\n",
    "\n",
    "    if val_set:\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        t = iter\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        #         grad, loss = nn.train_step(X_mini, y_mini)\n",
    "        #         def train_step(self, X_train, y_train):\n",
    "        #         \"\"\"\n",
    "        #         Single training step over minibatch: forward, loss, backprop\n",
    "        #         \"\"\"\n",
    "        y, cache = nn.forward(X_mini, train=True)\n",
    "        loss, dy = nn.loss_function(y, y_mini)\n",
    "        dX, grad = nn.backward(dy, cache)\n",
    "        #         return grad, loss\n",
    "\n",
    "        if iter % print_after == 0:\n",
    "            if val_set:\n",
    "                val_acc = util.accuracy(y_val, nn.test(X_val))\n",
    "                print('Iter-{} loss: {:.4f} validation: {:4f}'.format(iter, loss, val_acc))\n",
    "            else:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "\n",
    "        for k in grad[0]:\n",
    "            M[0][k] = util.exp_running_avg(M[0][k], grad[0][k], beta1)\n",
    "            R[0][k] = util.exp_running_avg(R[0][k], grad[0][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[0][k] / (1. - beta1**(t))\n",
    "            r_k_hat = R[0][k] / (1. - beta2**(t))\n",
    "\n",
    "            nn.model[0][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + c.eps)\n",
    "            \n",
    "        for k in grad[1]:\n",
    "            M[1][k] = util.exp_running_avg(M[1][k], grad[1][k], beta1)\n",
    "            R[1][k] = util.exp_running_avg(R[1][k], grad[1][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[1][k] / (1. - beta1**(t))\n",
    "            r_k_hat = R[1][k] / (1. - beta2**(t))\n",
    "\n",
    "            nn.model[1][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + c.eps)\n",
    "            \n",
    "        for k in grad[2]:\n",
    "            M[2][k] = util.exp_running_avg(M[2][k], grad[2][k], beta1)\n",
    "            R[2][k] = util.exp_running_avg(R[2][k], grad[2][k]**2, beta2)\n",
    "\n",
    "            m_k_hat = M[2][k] / (1. - beta1**(t))\n",
    "            r_k_hat = R[2][k] / (1. - beta2**(t))\n",
    "\n",
    "            nn.model[2][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + c.eps)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 64 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # print loss for train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 3.1479 validation: 0.106600\n",
      "Iter-2 loss: 3.0901 validation: 0.095800\n",
      "Iter-3 loss: 3.0528 validation: 0.096000\n",
      "Iter-4 loss: 3.1227 validation: 0.096600\n",
      "Iter-5 loss: 2.6991 validation: 0.096800\n",
      "Iter-6 loss: 2.7405 validation: 0.095800\n",
      "Iter-7 loss: 2.8990 validation: 0.095800\n",
      "Iter-8 loss: 2.5033 validation: 0.098600\n",
      "Iter-9 loss: 2.4098 validation: 0.098600\n",
      "Iter-10 loss: 2.2245 validation: 0.098600\n",
      "Iter-11 loss: 2.6369 validation: 0.098600\n",
      "Iter-12 loss: 2.1226 validation: 0.098600\n",
      "Iter-13 loss: 1.7940 validation: 0.098600\n",
      "Iter-14 loss: 1.9219 validation: 0.098600\n",
      "Iter-15 loss: 2.3919 validation: 0.098600\n",
      "Iter-16 loss: 2.0263 validation: 0.098600\n",
      "Iter-17 loss: 1.9438 validation: 0.098600\n",
      "Iter-18 loss: 2.1798 validation: 0.110000\n",
      "Iter-19 loss: 2.3725 validation: 0.110000\n",
      "Iter-20 loss: 1.7419 validation: 0.109800\n",
      "Iter-21 loss: 1.7917 validation: 0.109800\n",
      "Iter-22 loss: 1.8384 validation: 0.111200\n",
      "Iter-23 loss: 2.0731 validation: 0.111600\n",
      "Iter-24 loss: 1.9383 validation: 0.110200\n",
      "Iter-25 loss: 1.6997 validation: 0.110400\n",
      "Iter-26 loss: 1.9782 validation: 0.110400\n",
      "Iter-27 loss: 1.7428 validation: 0.110400\n",
      "Iter-28 loss: 2.1119 validation: 0.110600\n",
      "Iter-29 loss: 1.6497 validation: 0.110600\n",
      "Iter-30 loss: 1.8736 validation: 0.110800\n",
      "Iter-31 loss: 2.0769 validation: 0.111000\n",
      "Iter-32 loss: 1.8518 validation: 0.111000\n",
      "Iter-33 loss: 1.2619 validation: 0.111600\n",
      "Iter-34 loss: 1.6051 validation: 0.111400\n",
      "Iter-35 loss: 1.3674 validation: 0.111000\n",
      "Iter-36 loss: 1.9738 validation: 0.111400\n",
      "Iter-37 loss: 1.6159 validation: 0.111800\n",
      "Iter-38 loss: 1.6609 validation: 0.112400\n",
      "Iter-39 loss: 1.7173 validation: 0.098400\n",
      "Iter-40 loss: 1.7747 validation: 0.092800\n",
      "Iter-41 loss: 1.7164 validation: 0.092600\n",
      "Iter-42 loss: 1.3241 validation: 0.110200\n",
      "Iter-43 loss: 1.6513 validation: 0.110200\n",
      "Iter-44 loss: 1.5634 validation: 0.110200\n",
      "Iter-45 loss: 1.5118 validation: 0.110200\n",
      "Iter-46 loss: 1.6676 validation: 0.110200\n",
      "Iter-47 loss: 1.5761 validation: 0.110200\n",
      "Iter-48 loss: 1.5511 validation: 0.110200\n",
      "Iter-49 loss: 1.0442 validation: 0.110200\n",
      "Iter-50 loss: 1.3497 validation: 0.110200\n",
      "Iter-51 loss: 1.5476 validation: 0.110200\n",
      "Iter-52 loss: 2.0461 validation: 0.094800\n",
      "Iter-53 loss: 1.2901 validation: 0.093200\n",
      "Iter-54 loss: 1.6227 validation: 0.099800\n",
      "Iter-55 loss: 1.3168 validation: 0.110600\n",
      "Iter-56 loss: 1.3178 validation: 0.110600\n",
      "Iter-57 loss: 1.0368 validation: 0.110400\n",
      "Iter-58 loss: 1.3580 validation: 0.110400\n",
      "Iter-59 loss: 1.1825 validation: 0.110600\n",
      "Iter-60 loss: 1.4317 validation: 0.110400\n",
      "Iter-61 loss: 1.4775 validation: 0.110400\n",
      "Iter-62 loss: 1.0097 validation: 0.110800\n",
      "Iter-63 loss: 1.5594 validation: 0.110600\n",
      "Iter-64 loss: 1.2064 validation: 0.092400\n",
      "Iter-65 loss: 1.0852 validation: 0.092400\n",
      "Iter-66 loss: 1.0974 validation: 0.092400\n",
      "Iter-67 loss: 1.3713 validation: 0.092400\n",
      "Iter-68 loss: 1.1431 validation: 0.092400\n",
      "Iter-69 loss: 1.3796 validation: 0.092400\n",
      "Iter-70 loss: 1.4144 validation: 0.092400\n",
      "Iter-71 loss: 0.9111 validation: 0.092400\n",
      "Iter-72 loss: 1.2509 validation: 0.092400\n",
      "Iter-73 loss: 1.4748 validation: 0.096600\n",
      "Iter-74 loss: 1.0189 validation: 0.096400\n",
      "Iter-75 loss: 0.9664 validation: 0.096200\n",
      "Iter-76 loss: 1.1535 validation: 0.096000\n",
      "Iter-77 loss: 1.0562 validation: 0.096000\n",
      "Iter-78 loss: 1.2403 validation: 0.096000\n",
      "Iter-79 loss: 0.8512 validation: 0.096000\n",
      "Iter-80 loss: 1.3011 validation: 0.096200\n",
      "Iter-81 loss: 1.2908 validation: 0.095800\n",
      "Iter-82 loss: 1.3506 validation: 0.095800\n",
      "Iter-83 loss: 1.2116 validation: 0.095800\n",
      "Iter-84 loss: 1.5072 validation: 0.095800\n"
     ]
    }
   ],
   "source": [
    "net = DCNN(C=C, D=D, H=8, p_dropout=0.5)\n",
    "\n",
    "net = adam(nn=net, X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = net.predict(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "\n",
    "print()\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
