{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.3097 valid loss: 2.3107, valid accuracy: 0.0802\n",
      "Iter-200 train loss: 2.3105 valid loss: 2.3123, valid accuracy: 0.0812\n",
      "Iter-300 train loss: 2.3117 valid loss: 2.3137, valid accuracy: 0.0794\n",
      "Iter-400 train loss: 2.3183 valid loss: 2.3149, valid accuracy: 0.0798\n",
      "Iter-500 train loss: 2.3100 valid loss: 2.3159, valid accuracy: 0.0792\n",
      "Iter-600 train loss: 2.3147 valid loss: 2.3165, valid accuracy: 0.0796\n",
      "Iter-700 train loss: 2.3244 valid loss: 2.3167, valid accuracy: 0.0794\n",
      "Iter-800 train loss: 2.3077 valid loss: 2.3166, valid accuracy: 0.0806\n",
      "Iter-900 train loss: 2.3267 valid loss: 2.3158, valid accuracy: 0.0836\n",
      "Iter-1000 train loss: 2.3127 valid loss: 2.3146, valid accuracy: 0.0854\n",
      "Iter-1100 train loss: 2.3310 valid loss: 2.3131, valid accuracy: 0.0862\n",
      "Iter-1200 train loss: 2.3438 valid loss: 2.3109, valid accuracy: 0.0872\n",
      "Iter-1300 train loss: 2.2932 valid loss: 2.3082, valid accuracy: 0.0890\n",
      "Iter-1400 train loss: 2.2944 valid loss: 2.3050, valid accuracy: 0.0906\n",
      "Iter-1500 train loss: 2.2670 valid loss: 2.3014, valid accuracy: 0.0924\n",
      "Iter-1600 train loss: 2.3037 valid loss: 2.2972, valid accuracy: 0.0962\n",
      "Iter-1700 train loss: 2.2897 valid loss: 2.2924, valid accuracy: 0.0976\n",
      "Iter-1800 train loss: 2.2960 valid loss: 2.2870, valid accuracy: 0.1014\n",
      "Iter-1900 train loss: 2.2653 valid loss: 2.2811, valid accuracy: 0.1056\n",
      "Iter-2000 train loss: 2.2671 valid loss: 2.2750, valid accuracy: 0.1112\n",
      "Iter-2100 train loss: 2.2786 valid loss: 2.2681, valid accuracy: 0.1178\n",
      "Iter-2200 train loss: 2.2592 valid loss: 2.2603, valid accuracy: 0.1234\n",
      "Iter-2300 train loss: 2.2517 valid loss: 2.2523, valid accuracy: 0.1280\n",
      "Iter-2400 train loss: 2.2222 valid loss: 2.2439, valid accuracy: 0.1346\n",
      "Iter-2500 train loss: 2.2316 valid loss: 2.2351, valid accuracy: 0.1432\n",
      "Iter-2600 train loss: 2.2018 valid loss: 2.2261, valid accuracy: 0.1528\n",
      "Iter-2700 train loss: 2.1820 valid loss: 2.2163, valid accuracy: 0.1602\n",
      "Iter-2800 train loss: 2.2000 valid loss: 2.2061, valid accuracy: 0.1720\n",
      "Iter-2900 train loss: 2.1871 valid loss: 2.1952, valid accuracy: 0.1882\n",
      "Iter-3000 train loss: 2.1748 valid loss: 2.1841, valid accuracy: 0.2124\n",
      "Iter-3100 train loss: 2.1804 valid loss: 2.1724, valid accuracy: 0.2412\n",
      "Iter-3200 train loss: 2.1514 valid loss: 2.1606, valid accuracy: 0.2692\n",
      "Iter-3300 train loss: 2.1582 valid loss: 2.1484, valid accuracy: 0.2938\n",
      "Iter-3400 train loss: 2.1029 valid loss: 2.1358, valid accuracy: 0.3214\n",
      "Iter-3500 train loss: 2.1521 valid loss: 2.1233, valid accuracy: 0.3378\n",
      "Iter-3600 train loss: 2.0946 valid loss: 2.1107, valid accuracy: 0.3520\n",
      "Iter-3700 train loss: 2.0893 valid loss: 2.0977, valid accuracy: 0.3620\n",
      "Iter-3800 train loss: 2.0730 valid loss: 2.0845, valid accuracy: 0.3714\n",
      "Iter-3900 train loss: 2.0285 valid loss: 2.0715, valid accuracy: 0.3788\n",
      "Iter-4000 train loss: 2.0595 valid loss: 2.0585, valid accuracy: 0.3828\n",
      "Iter-4100 train loss: 2.0227 valid loss: 2.0454, valid accuracy: 0.3886\n",
      "Iter-4200 train loss: 2.0317 valid loss: 2.0319, valid accuracy: 0.3924\n",
      "Iter-4300 train loss: 1.9673 valid loss: 2.0181, valid accuracy: 0.3960\n",
      "Iter-4400 train loss: 2.0075 valid loss: 2.0046, valid accuracy: 0.4014\n",
      "Iter-4500 train loss: 1.9956 valid loss: 1.9911, valid accuracy: 0.4048\n",
      "Iter-4600 train loss: 2.0091 valid loss: 1.9777, valid accuracy: 0.4064\n",
      "Iter-4700 train loss: 1.9814 valid loss: 1.9646, valid accuracy: 0.4108\n",
      "Iter-4800 train loss: 1.9197 valid loss: 1.9511, valid accuracy: 0.4120\n",
      "Iter-4900 train loss: 1.8398 valid loss: 1.9379, valid accuracy: 0.4138\n",
      "Iter-5000 train loss: 1.9093 valid loss: 1.9242, valid accuracy: 0.4142\n",
      "Iter-5100 train loss: 2.0108 valid loss: 1.9113, valid accuracy: 0.4166\n",
      "Iter-5200 train loss: 1.8948 valid loss: 1.8981, valid accuracy: 0.4176\n",
      "Iter-5300 train loss: 1.9250 valid loss: 1.8853, valid accuracy: 0.4190\n",
      "Iter-5400 train loss: 1.8144 valid loss: 1.8726, valid accuracy: 0.4206\n",
      "Iter-5500 train loss: 1.7918 valid loss: 1.8597, valid accuracy: 0.4222\n",
      "Iter-5600 train loss: 1.8312 valid loss: 1.8470, valid accuracy: 0.4246\n",
      "Iter-5700 train loss: 1.8291 valid loss: 1.8342, valid accuracy: 0.4266\n",
      "Iter-5800 train loss: 1.8638 valid loss: 1.8219, valid accuracy: 0.4288\n",
      "Iter-5900 train loss: 1.9513 valid loss: 1.8097, valid accuracy: 0.4318\n",
      "Iter-6000 train loss: 1.7591 valid loss: 1.7973, valid accuracy: 0.4328\n",
      "Iter-6100 train loss: 1.8785 valid loss: 1.7853, valid accuracy: 0.4354\n",
      "Iter-6200 train loss: 1.7463 valid loss: 1.7732, valid accuracy: 0.4386\n",
      "Iter-6300 train loss: 1.8459 valid loss: 1.7610, valid accuracy: 0.4406\n",
      "Iter-6400 train loss: 1.7533 valid loss: 1.7493, valid accuracy: 0.4410\n",
      "Iter-6500 train loss: 1.6743 valid loss: 1.7373, valid accuracy: 0.4434\n",
      "Iter-6600 train loss: 1.8082 valid loss: 1.7255, valid accuracy: 0.4470\n",
      "Iter-6700 train loss: 1.8566 valid loss: 1.7139, valid accuracy: 0.4504\n",
      "Iter-6800 train loss: 1.6952 valid loss: 1.7020, valid accuracy: 0.4522\n",
      "Iter-6900 train loss: 1.5730 valid loss: 1.6901, valid accuracy: 0.4562\n",
      "Iter-7000 train loss: 1.5815 valid loss: 1.6787, valid accuracy: 0.4594\n",
      "Iter-7100 train loss: 1.6256 valid loss: 1.6671, valid accuracy: 0.4652\n",
      "Iter-7200 train loss: 1.6828 valid loss: 1.6555, valid accuracy: 0.4690\n",
      "Iter-7300 train loss: 1.7353 valid loss: 1.6439, valid accuracy: 0.4760\n",
      "Iter-7400 train loss: 1.6914 valid loss: 1.6323, valid accuracy: 0.4830\n",
      "Iter-7500 train loss: 1.6017 valid loss: 1.6208, valid accuracy: 0.4902\n",
      "Iter-7600 train loss: 1.5711 valid loss: 1.6092, valid accuracy: 0.4988\n",
      "Iter-7700 train loss: 1.5490 valid loss: 1.5979, valid accuracy: 0.5046\n",
      "Iter-7800 train loss: 1.5908 valid loss: 1.5867, valid accuracy: 0.5108\n",
      "Iter-7900 train loss: 1.5337 valid loss: 1.5755, valid accuracy: 0.5200\n",
      "Iter-8000 train loss: 1.6031 valid loss: 1.5643, valid accuracy: 0.5266\n",
      "Iter-8100 train loss: 1.5795 valid loss: 1.5529, valid accuracy: 0.5342\n",
      "Iter-8200 train loss: 1.4528 valid loss: 1.5418, valid accuracy: 0.5414\n",
      "Iter-8300 train loss: 1.5391 valid loss: 1.5307, valid accuracy: 0.5446\n",
      "Iter-8400 train loss: 1.4945 valid loss: 1.5195, valid accuracy: 0.5514\n",
      "Iter-8500 train loss: 1.4755 valid loss: 1.5087, valid accuracy: 0.5556\n",
      "Iter-8600 train loss: 1.4302 valid loss: 1.4976, valid accuracy: 0.5620\n",
      "Iter-8700 train loss: 1.4477 valid loss: 1.4868, valid accuracy: 0.5662\n",
      "Iter-8800 train loss: 1.3663 valid loss: 1.4757, valid accuracy: 0.5720\n",
      "Iter-8900 train loss: 1.4395 valid loss: 1.4649, valid accuracy: 0.5774\n",
      "Iter-9000 train loss: 1.4917 valid loss: 1.4539, valid accuracy: 0.5868\n",
      "Iter-9100 train loss: 1.4188 valid loss: 1.4432, valid accuracy: 0.5906\n",
      "Iter-9200 train loss: 1.4599 valid loss: 1.4323, valid accuracy: 0.5952\n",
      "Iter-9300 train loss: 1.4838 valid loss: 1.4215, valid accuracy: 0.6022\n",
      "Iter-9400 train loss: 1.4007 valid loss: 1.4111, valid accuracy: 0.6066\n",
      "Iter-9500 train loss: 1.4138 valid loss: 1.4003, valid accuracy: 0.6108\n",
      "Iter-9600 train loss: 1.3820 valid loss: 1.3898, valid accuracy: 0.6168\n",
      "Iter-9700 train loss: 1.3263 valid loss: 1.3795, valid accuracy: 0.6224\n",
      "Iter-9800 train loss: 1.3407 valid loss: 1.3688, valid accuracy: 0.6282\n",
      "Iter-9900 train loss: 1.2770 valid loss: 1.3584, valid accuracy: 0.6332\n",
      "Iter-10000 train loss: 1.2348 valid loss: 1.3481, valid accuracy: 0.6368\n",
      "Iter-10100 train loss: 1.2634 valid loss: 1.3378, valid accuracy: 0.6414\n",
      "Iter-10200 train loss: 1.4021 valid loss: 1.3278, valid accuracy: 0.6454\n",
      "Iter-10300 train loss: 1.2434 valid loss: 1.3178, valid accuracy: 0.6496\n",
      "Iter-10400 train loss: 1.4045 valid loss: 1.3079, valid accuracy: 0.6550\n",
      "Iter-10500 train loss: 1.2653 valid loss: 1.2982, valid accuracy: 0.6586\n",
      "Iter-10600 train loss: 1.3156 valid loss: 1.2885, valid accuracy: 0.6616\n",
      "Iter-10700 train loss: 1.3402 valid loss: 1.2787, valid accuracy: 0.6638\n",
      "Iter-10800 train loss: 1.2905 valid loss: 1.2690, valid accuracy: 0.6668\n",
      "Iter-10900 train loss: 1.2255 valid loss: 1.2593, valid accuracy: 0.6702\n",
      "Iter-11000 train loss: 1.2598 valid loss: 1.2500, valid accuracy: 0.6738\n",
      "Iter-11100 train loss: 1.2803 valid loss: 1.2407, valid accuracy: 0.6788\n",
      "Iter-11200 train loss: 1.2697 valid loss: 1.2315, valid accuracy: 0.6830\n",
      "Iter-11300 train loss: 1.2996 valid loss: 1.2223, valid accuracy: 0.6852\n",
      "Iter-11400 train loss: 1.1637 valid loss: 1.2132, valid accuracy: 0.6896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 1.3247 valid loss: 1.2044, valid accuracy: 0.6922\n",
      "Iter-11600 train loss: 1.3651 valid loss: 1.1956, valid accuracy: 0.6960\n",
      "Iter-11700 train loss: 1.1599 valid loss: 1.1867, valid accuracy: 0.6964\n",
      "Iter-11800 train loss: 1.0998 valid loss: 1.1777, valid accuracy: 0.6988\n",
      "Iter-11900 train loss: 1.0984 valid loss: 1.1693, valid accuracy: 0.7012\n",
      "Iter-12000 train loss: 1.1258 valid loss: 1.1610, valid accuracy: 0.7036\n",
      "Iter-12100 train loss: 1.1328 valid loss: 1.1527, valid accuracy: 0.7074\n",
      "Iter-12200 train loss: 1.0046 valid loss: 1.1446, valid accuracy: 0.7100\n",
      "Iter-12300 train loss: 1.2347 valid loss: 1.1365, valid accuracy: 0.7122\n",
      "Iter-12400 train loss: 1.2578 valid loss: 1.1288, valid accuracy: 0.7130\n",
      "Iter-12500 train loss: 1.0239 valid loss: 1.1210, valid accuracy: 0.7132\n",
      "Iter-12600 train loss: 1.1510 valid loss: 1.1133, valid accuracy: 0.7160\n",
      "Iter-12700 train loss: 1.0684 valid loss: 1.1057, valid accuracy: 0.7178\n",
      "Iter-12800 train loss: 1.0345 valid loss: 1.0983, valid accuracy: 0.7206\n",
      "Iter-12900 train loss: 1.1497 valid loss: 1.0909, valid accuracy: 0.7216\n",
      "Iter-13000 train loss: 1.1127 valid loss: 1.0838, valid accuracy: 0.7238\n",
      "Iter-13100 train loss: 0.9447 valid loss: 1.0767, valid accuracy: 0.7240\n",
      "Iter-13200 train loss: 1.1720 valid loss: 1.0697, valid accuracy: 0.7268\n",
      "Iter-13300 train loss: 0.9939 valid loss: 1.0626, valid accuracy: 0.7284\n",
      "Iter-13400 train loss: 1.0542 valid loss: 1.0557, valid accuracy: 0.7304\n",
      "Iter-13500 train loss: 1.1257 valid loss: 1.0491, valid accuracy: 0.7344\n",
      "Iter-13600 train loss: 1.1563 valid loss: 1.0426, valid accuracy: 0.7362\n",
      "Iter-13700 train loss: 1.0780 valid loss: 1.0360, valid accuracy: 0.7368\n",
      "Iter-13800 train loss: 1.1025 valid loss: 1.0296, valid accuracy: 0.7380\n",
      "Iter-13900 train loss: 1.0661 valid loss: 1.0233, valid accuracy: 0.7402\n",
      "Iter-14000 train loss: 1.1417 valid loss: 1.0171, valid accuracy: 0.7432\n",
      "Iter-14100 train loss: 1.0087 valid loss: 1.0109, valid accuracy: 0.7446\n",
      "Iter-14200 train loss: 1.0925 valid loss: 1.0049, valid accuracy: 0.7470\n",
      "Iter-14300 train loss: 1.0055 valid loss: 0.9989, valid accuracy: 0.7482\n",
      "Iter-14400 train loss: 0.8622 valid loss: 0.9931, valid accuracy: 0.7496\n",
      "Iter-14500 train loss: 0.8888 valid loss: 0.9872, valid accuracy: 0.7512\n",
      "Iter-14600 train loss: 1.1485 valid loss: 0.9815, valid accuracy: 0.7522\n",
      "Iter-14700 train loss: 1.0513 valid loss: 0.9758, valid accuracy: 0.7542\n",
      "Iter-14800 train loss: 1.0310 valid loss: 0.9703, valid accuracy: 0.7548\n",
      "Iter-14900 train loss: 1.0107 valid loss: 0.9648, valid accuracy: 0.7562\n",
      "Iter-15000 train loss: 0.9272 valid loss: 0.9597, valid accuracy: 0.7570\n",
      "Iter-15100 train loss: 0.9205 valid loss: 0.9545, valid accuracy: 0.7580\n",
      "Iter-15200 train loss: 1.1019 valid loss: 0.9494, valid accuracy: 0.7590\n",
      "Iter-15300 train loss: 1.1085 valid loss: 0.9443, valid accuracy: 0.7596\n",
      "Iter-15400 train loss: 0.9096 valid loss: 0.9391, valid accuracy: 0.7614\n",
      "Iter-15500 train loss: 0.9705 valid loss: 0.9342, valid accuracy: 0.7634\n",
      "Iter-15600 train loss: 0.7586 valid loss: 0.9293, valid accuracy: 0.7646\n",
      "Iter-15700 train loss: 0.8976 valid loss: 0.9245, valid accuracy: 0.7648\n",
      "Iter-15800 train loss: 0.8447 valid loss: 0.9199, valid accuracy: 0.7660\n",
      "Iter-15900 train loss: 0.8903 valid loss: 0.9152, valid accuracy: 0.7660\n",
      "Iter-16000 train loss: 0.8916 valid loss: 0.9106, valid accuracy: 0.7674\n",
      "Iter-16100 train loss: 0.8689 valid loss: 0.9060, valid accuracy: 0.7680\n",
      "Iter-16200 train loss: 0.8552 valid loss: 0.9015, valid accuracy: 0.7686\n",
      "Iter-16300 train loss: 0.8907 valid loss: 0.8971, valid accuracy: 0.7694\n",
      "Iter-16400 train loss: 0.9138 valid loss: 0.8927, valid accuracy: 0.7704\n",
      "Iter-16500 train loss: 0.8733 valid loss: 0.8884, valid accuracy: 0.7710\n",
      "Iter-16600 train loss: 0.8679 valid loss: 0.8842, valid accuracy: 0.7712\n",
      "Iter-16700 train loss: 0.7854 valid loss: 0.8802, valid accuracy: 0.7712\n",
      "Iter-16800 train loss: 0.8316 valid loss: 0.8761, valid accuracy: 0.7724\n",
      "Iter-16900 train loss: 0.8451 valid loss: 0.8721, valid accuracy: 0.7742\n",
      "Iter-17000 train loss: 0.8023 valid loss: 0.8681, valid accuracy: 0.7746\n",
      "Iter-17100 train loss: 0.6738 valid loss: 0.8642, valid accuracy: 0.7758\n",
      "Iter-17200 train loss: 0.7969 valid loss: 0.8603, valid accuracy: 0.7770\n",
      "Iter-17300 train loss: 1.1135 valid loss: 0.8564, valid accuracy: 0.7786\n",
      "Iter-17400 train loss: 0.8201 valid loss: 0.8526, valid accuracy: 0.7806\n",
      "Iter-17500 train loss: 0.9833 valid loss: 0.8489, valid accuracy: 0.7818\n",
      "Iter-17600 train loss: 0.8463 valid loss: 0.8453, valid accuracy: 0.7820\n",
      "Iter-17700 train loss: 0.9797 valid loss: 0.8417, valid accuracy: 0.7844\n",
      "Iter-17800 train loss: 0.8824 valid loss: 0.8382, valid accuracy: 0.7848\n",
      "Iter-17900 train loss: 0.7923 valid loss: 0.8346, valid accuracy: 0.7848\n",
      "Iter-18000 train loss: 1.0053 valid loss: 0.8312, valid accuracy: 0.7860\n",
      "Iter-18100 train loss: 1.0035 valid loss: 0.8278, valid accuracy: 0.7878\n",
      "Iter-18200 train loss: 0.6334 valid loss: 0.8245, valid accuracy: 0.7884\n",
      "Iter-18300 train loss: 0.8932 valid loss: 0.8211, valid accuracy: 0.7890\n",
      "Iter-18400 train loss: 0.7695 valid loss: 0.8178, valid accuracy: 0.7900\n",
      "Iter-18500 train loss: 0.7962 valid loss: 0.8146, valid accuracy: 0.7902\n",
      "Iter-18600 train loss: 0.8968 valid loss: 0.8114, valid accuracy: 0.7904\n",
      "Iter-18700 train loss: 0.7308 valid loss: 0.8081, valid accuracy: 0.7910\n",
      "Iter-18800 train loss: 0.6362 valid loss: 0.8050, valid accuracy: 0.7914\n",
      "Iter-18900 train loss: 0.8571 valid loss: 0.8020, valid accuracy: 0.7916\n",
      "Iter-19000 train loss: 0.7229 valid loss: 0.7989, valid accuracy: 0.7926\n",
      "Iter-19100 train loss: 0.8822 valid loss: 0.7958, valid accuracy: 0.7934\n",
      "Iter-19200 train loss: 0.9198 valid loss: 0.7929, valid accuracy: 0.7936\n",
      "Iter-19300 train loss: 0.8436 valid loss: 0.7899, valid accuracy: 0.7932\n",
      "Iter-19400 train loss: 0.8420 valid loss: 0.7870, valid accuracy: 0.7946\n",
      "Iter-19500 train loss: 0.8201 valid loss: 0.7841, valid accuracy: 0.7952\n",
      "Iter-19600 train loss: 0.7592 valid loss: 0.7812, valid accuracy: 0.7962\n",
      "Iter-19700 train loss: 0.8259 valid loss: 0.7785, valid accuracy: 0.7976\n",
      "Iter-19800 train loss: 0.8897 valid loss: 0.7757, valid accuracy: 0.7986\n",
      "Iter-19900 train loss: 0.7434 valid loss: 0.7729, valid accuracy: 0.7984\n",
      "Iter-20000 train loss: 0.8009 valid loss: 0.7701, valid accuracy: 0.7998\n",
      "Iter-20100 train loss: 0.8371 valid loss: 0.7673, valid accuracy: 0.8004\n",
      "Iter-20200 train loss: 0.8411 valid loss: 0.7645, valid accuracy: 0.8020\n",
      "Iter-20300 train loss: 0.7506 valid loss: 0.7619, valid accuracy: 0.8024\n",
      "Iter-20400 train loss: 0.6712 valid loss: 0.7592, valid accuracy: 0.8036\n",
      "Iter-20500 train loss: 0.7671 valid loss: 0.7567, valid accuracy: 0.8048\n",
      "Iter-20600 train loss: 0.8699 valid loss: 0.7541, valid accuracy: 0.8050\n",
      "Iter-20700 train loss: 0.7853 valid loss: 0.7515, valid accuracy: 0.8052\n",
      "Iter-20800 train loss: 0.5399 valid loss: 0.7490, valid accuracy: 0.8062\n",
      "Iter-20900 train loss: 0.6977 valid loss: 0.7464, valid accuracy: 0.8072\n",
      "Iter-21000 train loss: 0.7875 valid loss: 0.7440, valid accuracy: 0.8076\n",
      "Iter-21100 train loss: 0.5655 valid loss: 0.7414, valid accuracy: 0.8072\n",
      "Iter-21200 train loss: 0.8086 valid loss: 0.7390, valid accuracy: 0.8080\n",
      "Iter-21300 train loss: 0.8416 valid loss: 0.7366, valid accuracy: 0.8094\n",
      "Iter-21400 train loss: 0.7042 valid loss: 0.7342, valid accuracy: 0.8108\n",
      "Iter-21500 train loss: 0.7174 valid loss: 0.7319, valid accuracy: 0.8106\n",
      "Iter-21600 train loss: 0.7685 valid loss: 0.7295, valid accuracy: 0.8114\n",
      "Iter-21700 train loss: 0.8724 valid loss: 0.7272, valid accuracy: 0.8118\n",
      "Iter-21800 train loss: 0.7164 valid loss: 0.7249, valid accuracy: 0.8122\n",
      "Iter-21900 train loss: 0.7651 valid loss: 0.7225, valid accuracy: 0.8126\n",
      "Iter-22000 train loss: 0.7328 valid loss: 0.7203, valid accuracy: 0.8134\n",
      "Iter-22100 train loss: 0.6603 valid loss: 0.7180, valid accuracy: 0.8144\n",
      "Iter-22200 train loss: 0.6947 valid loss: 0.7158, valid accuracy: 0.8148\n",
      "Iter-22300 train loss: 0.6516 valid loss: 0.7135, valid accuracy: 0.8146\n",
      "Iter-22400 train loss: 0.6976 valid loss: 0.7114, valid accuracy: 0.8152\n",
      "Iter-22500 train loss: 0.6674 valid loss: 0.7093, valid accuracy: 0.8160\n",
      "Iter-22600 train loss: 0.5155 valid loss: 0.7072, valid accuracy: 0.8168\n",
      "Iter-22700 train loss: 0.7733 valid loss: 0.7051, valid accuracy: 0.8172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 0.7429 valid loss: 0.7031, valid accuracy: 0.8172\n",
      "Iter-22900 train loss: 0.8107 valid loss: 0.7010, valid accuracy: 0.8182\n",
      "Iter-23000 train loss: 0.6681 valid loss: 0.6989, valid accuracy: 0.8196\n",
      "Iter-23100 train loss: 0.6926 valid loss: 0.6969, valid accuracy: 0.8206\n",
      "Iter-23200 train loss: 0.7128 valid loss: 0.6950, valid accuracy: 0.8212\n",
      "Iter-23300 train loss: 0.6668 valid loss: 0.6930, valid accuracy: 0.8222\n",
      "Iter-23400 train loss: 0.8144 valid loss: 0.6910, valid accuracy: 0.8224\n",
      "Iter-23500 train loss: 0.6615 valid loss: 0.6891, valid accuracy: 0.8232\n",
      "Iter-23600 train loss: 0.7539 valid loss: 0.6871, valid accuracy: 0.8230\n",
      "Iter-23700 train loss: 0.6680 valid loss: 0.6851, valid accuracy: 0.8238\n",
      "Iter-23800 train loss: 0.7885 valid loss: 0.6833, valid accuracy: 0.8244\n",
      "Iter-23900 train loss: 0.7384 valid loss: 0.6814, valid accuracy: 0.8254\n",
      "Iter-24000 train loss: 0.7731 valid loss: 0.6795, valid accuracy: 0.8258\n",
      "Iter-24100 train loss: 0.4166 valid loss: 0.6776, valid accuracy: 0.8258\n",
      "Iter-24200 train loss: 0.7403 valid loss: 0.6758, valid accuracy: 0.8260\n",
      "Iter-24300 train loss: 0.5897 valid loss: 0.6740, valid accuracy: 0.8276\n",
      "Iter-24400 train loss: 0.5655 valid loss: 0.6722, valid accuracy: 0.8278\n",
      "Iter-24500 train loss: 0.6170 valid loss: 0.6705, valid accuracy: 0.8288\n",
      "Iter-24600 train loss: 0.7837 valid loss: 0.6687, valid accuracy: 0.8286\n",
      "Iter-24700 train loss: 0.7103 valid loss: 0.6669, valid accuracy: 0.8290\n",
      "Iter-24800 train loss: 0.6219 valid loss: 0.6651, valid accuracy: 0.8296\n",
      "Iter-24900 train loss: 0.6339 valid loss: 0.6634, valid accuracy: 0.8302\n",
      "Iter-25000 train loss: 0.8353 valid loss: 0.6617, valid accuracy: 0.8302\n",
      "Iter-25100 train loss: 0.6651 valid loss: 0.6600, valid accuracy: 0.8306\n",
      "Iter-25200 train loss: 0.6867 valid loss: 0.6583, valid accuracy: 0.8308\n",
      "Iter-25300 train loss: 0.6641 valid loss: 0.6565, valid accuracy: 0.8318\n",
      "Iter-25400 train loss: 0.6686 valid loss: 0.6548, valid accuracy: 0.8316\n",
      "Iter-25500 train loss: 0.6228 valid loss: 0.6532, valid accuracy: 0.8324\n",
      "Iter-25600 train loss: 0.5331 valid loss: 0.6515, valid accuracy: 0.8332\n",
      "Iter-25700 train loss: 0.5591 valid loss: 0.6498, valid accuracy: 0.8340\n",
      "Iter-25800 train loss: 0.5554 valid loss: 0.6481, valid accuracy: 0.8344\n",
      "Iter-25900 train loss: 0.6154 valid loss: 0.6465, valid accuracy: 0.8350\n",
      "Iter-26000 train loss: 0.4937 valid loss: 0.6449, valid accuracy: 0.8350\n",
      "Iter-26100 train loss: 0.7978 valid loss: 0.6433, valid accuracy: 0.8356\n",
      "Iter-26200 train loss: 0.6317 valid loss: 0.6417, valid accuracy: 0.8362\n",
      "Iter-26300 train loss: 0.5954 valid loss: 0.6401, valid accuracy: 0.8364\n",
      "Iter-26400 train loss: 0.6692 valid loss: 0.6385, valid accuracy: 0.8366\n",
      "Iter-26500 train loss: 0.8456 valid loss: 0.6370, valid accuracy: 0.8374\n",
      "Iter-26600 train loss: 0.8590 valid loss: 0.6354, valid accuracy: 0.8372\n",
      "Iter-26700 train loss: 0.5709 valid loss: 0.6339, valid accuracy: 0.8378\n",
      "Iter-26800 train loss: 0.5941 valid loss: 0.6324, valid accuracy: 0.8378\n",
      "Iter-26900 train loss: 0.6663 valid loss: 0.6309, valid accuracy: 0.8384\n",
      "Iter-27000 train loss: 0.5729 valid loss: 0.6294, valid accuracy: 0.8388\n",
      "Iter-27100 train loss: 0.5045 valid loss: 0.6280, valid accuracy: 0.8394\n",
      "Iter-27200 train loss: 1.0270 valid loss: 0.6265, valid accuracy: 0.8398\n",
      "Iter-27300 train loss: 0.5345 valid loss: 0.6250, valid accuracy: 0.8408\n",
      "Iter-27400 train loss: 0.8732 valid loss: 0.6236, valid accuracy: 0.8410\n",
      "Iter-27500 train loss: 0.9685 valid loss: 0.6223, valid accuracy: 0.8416\n",
      "Iter-27600 train loss: 0.5864 valid loss: 0.6208, valid accuracy: 0.8416\n",
      "Iter-27700 train loss: 0.5612 valid loss: 0.6194, valid accuracy: 0.8414\n",
      "Iter-27800 train loss: 0.7883 valid loss: 0.6179, valid accuracy: 0.8416\n",
      "Iter-27900 train loss: 0.4598 valid loss: 0.6165, valid accuracy: 0.8424\n",
      "Iter-28000 train loss: 0.5919 valid loss: 0.6151, valid accuracy: 0.8428\n",
      "Iter-28100 train loss: 0.9456 valid loss: 0.6137, valid accuracy: 0.8428\n",
      "Iter-28200 train loss: 0.5861 valid loss: 0.6124, valid accuracy: 0.8430\n",
      "Iter-28300 train loss: 0.5863 valid loss: 0.6110, valid accuracy: 0.8428\n",
      "Iter-28400 train loss: 0.7104 valid loss: 0.6097, valid accuracy: 0.8430\n",
      "Iter-28500 train loss: 0.6031 valid loss: 0.6083, valid accuracy: 0.8434\n",
      "Iter-28600 train loss: 0.6916 valid loss: 0.6070, valid accuracy: 0.8438\n",
      "Iter-28700 train loss: 0.7830 valid loss: 0.6057, valid accuracy: 0.8446\n",
      "Iter-28800 train loss: 0.5272 valid loss: 0.6044, valid accuracy: 0.8446\n",
      "Iter-28900 train loss: 0.5398 valid loss: 0.6030, valid accuracy: 0.8446\n",
      "Iter-29000 train loss: 0.6402 valid loss: 0.6017, valid accuracy: 0.8450\n",
      "Iter-29100 train loss: 0.5040 valid loss: 0.6005, valid accuracy: 0.8452\n",
      "Iter-29200 train loss: 0.4748 valid loss: 0.5992, valid accuracy: 0.8456\n",
      "Iter-29300 train loss: 0.6951 valid loss: 0.5979, valid accuracy: 0.8460\n",
      "Iter-29400 train loss: 0.5518 valid loss: 0.5965, valid accuracy: 0.8466\n",
      "Iter-29500 train loss: 0.5149 valid loss: 0.5952, valid accuracy: 0.8472\n",
      "Iter-29600 train loss: 0.5948 valid loss: 0.5940, valid accuracy: 0.8470\n",
      "Iter-29700 train loss: 0.6896 valid loss: 0.5928, valid accuracy: 0.8474\n",
      "Iter-29800 train loss: 0.4873 valid loss: 0.5915, valid accuracy: 0.8484\n",
      "Iter-29900 train loss: 0.6384 valid loss: 0.5903, valid accuracy: 0.8490\n",
      "Iter-30000 train loss: 0.6562 valid loss: 0.5891, valid accuracy: 0.8492\n",
      "Iter-30100 train loss: 0.4305 valid loss: 0.5879, valid accuracy: 0.8494\n",
      "Iter-30200 train loss: 0.6689 valid loss: 0.5867, valid accuracy: 0.8494\n",
      "Iter-30300 train loss: 0.6104 valid loss: 0.5855, valid accuracy: 0.8498\n",
      "Iter-30400 train loss: 0.5816 valid loss: 0.5842, valid accuracy: 0.8504\n",
      "Iter-30500 train loss: 0.7623 valid loss: 0.5831, valid accuracy: 0.8504\n",
      "Iter-30600 train loss: 0.6209 valid loss: 0.5820, valid accuracy: 0.8502\n",
      "Iter-30700 train loss: 0.5971 valid loss: 0.5808, valid accuracy: 0.8504\n",
      "Iter-30800 train loss: 0.6838 valid loss: 0.5797, valid accuracy: 0.8502\n",
      "Iter-30900 train loss: 0.6649 valid loss: 0.5785, valid accuracy: 0.8506\n",
      "Iter-31000 train loss: 0.6738 valid loss: 0.5774, valid accuracy: 0.8514\n",
      "Iter-31100 train loss: 0.8174 valid loss: 0.5762, valid accuracy: 0.8514\n",
      "Iter-31200 train loss: 0.6053 valid loss: 0.5751, valid accuracy: 0.8520\n",
      "Iter-31300 train loss: 0.5126 valid loss: 0.5739, valid accuracy: 0.8524\n",
      "Iter-31400 train loss: 0.4958 valid loss: 0.5728, valid accuracy: 0.8530\n",
      "Iter-31500 train loss: 0.6337 valid loss: 0.5718, valid accuracy: 0.8528\n",
      "Iter-31600 train loss: 0.5153 valid loss: 0.5707, valid accuracy: 0.8530\n",
      "Iter-31700 train loss: 0.5625 valid loss: 0.5696, valid accuracy: 0.8534\n",
      "Iter-31800 train loss: 0.5577 valid loss: 0.5685, valid accuracy: 0.8534\n",
      "Iter-31900 train loss: 0.7792 valid loss: 0.5675, valid accuracy: 0.8536\n",
      "Iter-32000 train loss: 0.5717 valid loss: 0.5664, valid accuracy: 0.8536\n",
      "Iter-32100 train loss: 0.4405 valid loss: 0.5654, valid accuracy: 0.8536\n",
      "Iter-32200 train loss: 0.5250 valid loss: 0.5643, valid accuracy: 0.8540\n",
      "Iter-32300 train loss: 0.3264 valid loss: 0.5634, valid accuracy: 0.8540\n",
      "Iter-32400 train loss: 0.7747 valid loss: 0.5623, valid accuracy: 0.8546\n",
      "Iter-32500 train loss: 0.6299 valid loss: 0.5612, valid accuracy: 0.8546\n",
      "Iter-32600 train loss: 0.7337 valid loss: 0.5601, valid accuracy: 0.8552\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
