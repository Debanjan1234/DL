{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3411, acc-0.1000, valid loss-2.3321, acc-0.0536, test loss-2.3345, acc-0.0576\n",
      "Iter-20, train loss-2.3232, acc-0.0400, valid loss-2.3320, acc-0.0540, test loss-2.3343, acc-0.0579\n",
      "Iter-30, train loss-2.3223, acc-0.0400, valid loss-2.3316, acc-0.0544, test loss-2.3339, acc-0.0577\n",
      "Iter-40, train loss-2.3454, acc-0.0200, valid loss-2.3313, acc-0.0554, test loss-2.3336, acc-0.0579\n",
      "Iter-50, train loss-2.3123, acc-0.0600, valid loss-2.3311, acc-0.0552, test loss-2.3333, acc-0.0580\n",
      "Iter-60, train loss-2.3304, acc-0.0000, valid loss-2.3308, acc-0.0552, test loss-2.3331, acc-0.0586\n",
      "Iter-70, train loss-2.3281, acc-0.0400, valid loss-2.3305, acc-0.0554, test loss-2.3328, acc-0.0590\n",
      "Iter-80, train loss-2.3356, acc-0.0600, valid loss-2.3303, acc-0.0558, test loss-2.3325, acc-0.0594\n",
      "Iter-90, train loss-2.3341, acc-0.0200, valid loss-2.3301, acc-0.0556, test loss-2.3323, acc-0.0596\n",
      "Iter-100, train loss-2.3617, acc-0.0800, valid loss-2.3299, acc-0.0550, test loss-2.3320, acc-0.0587\n",
      "Iter-110, train loss-2.3356, acc-0.0200, valid loss-2.3296, acc-0.0554, test loss-2.3317, acc-0.0589\n",
      "Iter-120, train loss-2.3266, acc-0.0800, valid loss-2.3293, acc-0.0552, test loss-2.3314, acc-0.0591\n",
      "Iter-130, train loss-2.3295, acc-0.0400, valid loss-2.3291, acc-0.0554, test loss-2.3312, acc-0.0592\n",
      "Iter-140, train loss-2.3304, acc-0.0600, valid loss-2.3288, acc-0.0554, test loss-2.3309, acc-0.0592\n",
      "Iter-150, train loss-2.3419, acc-0.0400, valid loss-2.3285, acc-0.0558, test loss-2.3306, acc-0.0594\n",
      "Iter-160, train loss-2.3533, acc-0.0200, valid loss-2.3282, acc-0.0566, test loss-2.3303, acc-0.0598\n",
      "Iter-170, train loss-2.3353, acc-0.0400, valid loss-2.3279, acc-0.0570, test loss-2.3300, acc-0.0596\n",
      "Iter-180, train loss-2.3113, acc-0.0600, valid loss-2.3277, acc-0.0572, test loss-2.3298, acc-0.0595\n",
      "Iter-190, train loss-2.3409, acc-0.0800, valid loss-2.3274, acc-0.0578, test loss-2.3294, acc-0.0602\n",
      "Iter-200, train loss-2.3144, acc-0.0600, valid loss-2.3271, acc-0.0578, test loss-2.3291, acc-0.0608\n",
      "Iter-210, train loss-2.3170, acc-0.0400, valid loss-2.3268, acc-0.0580, test loss-2.3289, acc-0.0616\n",
      "Iter-220, train loss-2.3485, acc-0.0600, valid loss-2.3266, acc-0.0586, test loss-2.3286, acc-0.0621\n",
      "Iter-230, train loss-2.2972, acc-0.0800, valid loss-2.3263, acc-0.0590, test loss-2.3283, acc-0.0624\n",
      "Iter-240, train loss-2.3410, acc-0.0400, valid loss-2.3260, acc-0.0596, test loss-2.3280, acc-0.0629\n",
      "Iter-250, train loss-2.3063, acc-0.0600, valid loss-2.3257, acc-0.0602, test loss-2.3277, acc-0.0631\n",
      "Iter-260, train loss-2.3474, acc-0.0600, valid loss-2.3255, acc-0.0604, test loss-2.3274, acc-0.0632\n",
      "Iter-270, train loss-2.3371, acc-0.0800, valid loss-2.3253, acc-0.0604, test loss-2.3272, acc-0.0636\n",
      "Iter-280, train loss-2.3211, acc-0.0400, valid loss-2.3249, acc-0.0610, test loss-2.3269, acc-0.0642\n",
      "Iter-290, train loss-2.3411, acc-0.0800, valid loss-2.3247, acc-0.0616, test loss-2.3266, acc-0.0640\n",
      "Iter-300, train loss-2.3231, acc-0.0800, valid loss-2.3244, acc-0.0616, test loss-2.3263, acc-0.0641\n",
      "Iter-310, train loss-2.3307, acc-0.0800, valid loss-2.3242, acc-0.0614, test loss-2.3260, acc-0.0644\n",
      "Iter-320, train loss-2.3193, acc-0.1400, valid loss-2.3238, acc-0.0616, test loss-2.3257, acc-0.0650\n",
      "Iter-330, train loss-2.3180, acc-0.0600, valid loss-2.3236, acc-0.0618, test loss-2.3254, acc-0.0648\n",
      "Iter-340, train loss-2.3520, acc-0.1400, valid loss-2.3233, acc-0.0624, test loss-2.3251, acc-0.0653\n",
      "Iter-350, train loss-2.3326, acc-0.0600, valid loss-2.3230, acc-0.0630, test loss-2.3248, acc-0.0657\n",
      "Iter-360, train loss-2.3150, acc-0.1000, valid loss-2.3227, acc-0.0636, test loss-2.3245, acc-0.0662\n",
      "Iter-370, train loss-2.3139, acc-0.1000, valid loss-2.3224, acc-0.0636, test loss-2.3242, acc-0.0668\n",
      "Iter-380, train loss-2.2982, acc-0.0400, valid loss-2.3221, acc-0.0646, test loss-2.3239, acc-0.0677\n",
      "Iter-390, train loss-2.2948, acc-0.1000, valid loss-2.3218, acc-0.0652, test loss-2.3235, acc-0.0684\n",
      "Iter-400, train loss-2.3232, acc-0.0200, valid loss-2.3215, acc-0.0658, test loss-2.3232, acc-0.0690\n",
      "Iter-410, train loss-2.3217, acc-0.0800, valid loss-2.3211, acc-0.0666, test loss-2.3229, acc-0.0696\n",
      "Iter-420, train loss-2.3073, acc-0.0200, valid loss-2.3209, acc-0.0666, test loss-2.3226, acc-0.0696\n",
      "Iter-430, train loss-2.3311, acc-0.0200, valid loss-2.3206, acc-0.0670, test loss-2.3223, acc-0.0698\n",
      "Iter-440, train loss-2.3035, acc-0.1400, valid loss-2.3204, acc-0.0670, test loss-2.3220, acc-0.0700\n",
      "Iter-450, train loss-2.3659, acc-0.0200, valid loss-2.3201, acc-0.0678, test loss-2.3217, acc-0.0696\n",
      "Iter-460, train loss-2.3003, acc-0.1200, valid loss-2.3198, acc-0.0674, test loss-2.3214, acc-0.0695\n",
      "Iter-470, train loss-2.3297, acc-0.0400, valid loss-2.3195, acc-0.0670, test loss-2.3211, acc-0.0698\n",
      "Iter-480, train loss-2.3006, acc-0.1400, valid loss-2.3192, acc-0.0684, test loss-2.3208, acc-0.0704\n",
      "Iter-490, train loss-2.2766, acc-0.0800, valid loss-2.3190, acc-0.0692, test loss-2.3205, acc-0.0708\n",
      "Iter-500, train loss-2.2959, acc-0.0800, valid loss-2.3187, acc-0.0694, test loss-2.3202, acc-0.0708\n",
      "Iter-510, train loss-2.2915, acc-0.0400, valid loss-2.3183, acc-0.0698, test loss-2.3199, acc-0.0721\n",
      "Iter-520, train loss-2.3259, acc-0.0400, valid loss-2.3181, acc-0.0698, test loss-2.3196, acc-0.0729\n",
      "Iter-530, train loss-2.3013, acc-0.1200, valid loss-2.3177, acc-0.0710, test loss-2.3193, acc-0.0729\n",
      "Iter-540, train loss-2.3006, acc-0.0600, valid loss-2.3175, acc-0.0712, test loss-2.3190, acc-0.0738\n",
      "Iter-550, train loss-2.3229, acc-0.0800, valid loss-2.3172, acc-0.0714, test loss-2.3187, acc-0.0743\n",
      "Iter-560, train loss-2.3217, acc-0.0800, valid loss-2.3169, acc-0.0720, test loss-2.3184, acc-0.0740\n",
      "Iter-570, train loss-2.3028, acc-0.1000, valid loss-2.3166, acc-0.0716, test loss-2.3181, acc-0.0742\n",
      "Iter-580, train loss-2.3216, acc-0.0800, valid loss-2.3163, acc-0.0714, test loss-2.3178, acc-0.0744\n",
      "Iter-590, train loss-2.3013, acc-0.0400, valid loss-2.3160, acc-0.0718, test loss-2.3174, acc-0.0744\n",
      "Iter-600, train loss-2.3332, acc-0.0400, valid loss-2.3157, acc-0.0724, test loss-2.3171, acc-0.0748\n",
      "Iter-610, train loss-2.3200, acc-0.0800, valid loss-2.3153, acc-0.0730, test loss-2.3167, acc-0.0752\n",
      "Iter-620, train loss-2.3001, acc-0.1000, valid loss-2.3150, acc-0.0740, test loss-2.3164, acc-0.0756\n",
      "Iter-630, train loss-2.3120, acc-0.1000, valid loss-2.3147, acc-0.0736, test loss-2.3161, acc-0.0756\n",
      "Iter-640, train loss-2.2928, acc-0.0600, valid loss-2.3144, acc-0.0736, test loss-2.3157, acc-0.0757\n",
      "Iter-650, train loss-2.3256, acc-0.1000, valid loss-2.3140, acc-0.0738, test loss-2.3154, acc-0.0756\n",
      "Iter-660, train loss-2.3094, acc-0.1000, valid loss-2.3137, acc-0.0740, test loss-2.3150, acc-0.0759\n",
      "Iter-670, train loss-2.3238, acc-0.0600, valid loss-2.3133, acc-0.0740, test loss-2.3146, acc-0.0760\n",
      "Iter-680, train loss-2.3099, acc-0.0800, valid loss-2.3129, acc-0.0746, test loss-2.3142, acc-0.0764\n",
      "Iter-690, train loss-2.3221, acc-0.1000, valid loss-2.3125, acc-0.0752, test loss-2.3138, acc-0.0760\n",
      "Iter-700, train loss-2.3148, acc-0.0800, valid loss-2.3122, acc-0.0760, test loss-2.3134, acc-0.0768\n",
      "Iter-710, train loss-2.2796, acc-0.1800, valid loss-2.3118, acc-0.0756, test loss-2.3131, acc-0.0768\n",
      "Iter-720, train loss-2.3061, acc-0.0600, valid loss-2.3114, acc-0.0770, test loss-2.3127, acc-0.0775\n",
      "Iter-730, train loss-2.3048, acc-0.0400, valid loss-2.3111, acc-0.0774, test loss-2.3124, acc-0.0782\n",
      "Iter-740, train loss-2.2971, acc-0.1600, valid loss-2.3107, acc-0.0780, test loss-2.3120, acc-0.0780\n",
      "Iter-750, train loss-2.3028, acc-0.0800, valid loss-2.3104, acc-0.0786, test loss-2.3117, acc-0.0785\n",
      "Iter-760, train loss-2.3118, acc-0.0600, valid loss-2.3100, acc-0.0788, test loss-2.3112, acc-0.0794\n",
      "Iter-770, train loss-2.3373, acc-0.0800, valid loss-2.3096, acc-0.0786, test loss-2.3108, acc-0.0794\n",
      "Iter-780, train loss-2.3213, acc-0.0600, valid loss-2.3092, acc-0.0792, test loss-2.3104, acc-0.0796\n",
      "Iter-790, train loss-2.2973, acc-0.0800, valid loss-2.3088, acc-0.0788, test loss-2.3100, acc-0.0800\n",
      "Iter-800, train loss-2.3201, acc-0.0800, valid loss-2.3084, acc-0.0792, test loss-2.3096, acc-0.0805\n",
      "Iter-810, train loss-2.2828, acc-0.1200, valid loss-2.3080, acc-0.0800, test loss-2.3092, acc-0.0807\n",
      "Iter-820, train loss-2.3379, acc-0.0400, valid loss-2.3077, acc-0.0800, test loss-2.3089, acc-0.0817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.3208, acc-0.0600, valid loss-2.3074, acc-0.0802, test loss-2.3085, acc-0.0816\n",
      "Iter-840, train loss-2.3014, acc-0.0600, valid loss-2.3069, acc-0.0808, test loss-2.3081, acc-0.0822\n",
      "Iter-850, train loss-2.3057, acc-0.0600, valid loss-2.3066, acc-0.0810, test loss-2.3077, acc-0.0822\n",
      "Iter-860, train loss-2.3159, acc-0.0600, valid loss-2.3061, acc-0.0810, test loss-2.3073, acc-0.0829\n",
      "Iter-870, train loss-2.3097, acc-0.0600, valid loss-2.3057, acc-0.0820, test loss-2.3068, acc-0.0833\n",
      "Iter-880, train loss-2.3103, acc-0.0800, valid loss-2.3052, acc-0.0826, test loss-2.3064, acc-0.0836\n",
      "Iter-890, train loss-2.3184, acc-0.0000, valid loss-2.3049, acc-0.0836, test loss-2.3060, acc-0.0838\n",
      "Iter-900, train loss-2.2948, acc-0.1200, valid loss-2.3045, acc-0.0836, test loss-2.3056, acc-0.0837\n",
      "Iter-910, train loss-2.2769, acc-0.1200, valid loss-2.3041, acc-0.0844, test loss-2.3052, acc-0.0845\n",
      "Iter-920, train loss-2.3160, acc-0.0200, valid loss-2.3037, acc-0.0846, test loss-2.3047, acc-0.0848\n",
      "Iter-930, train loss-2.3188, acc-0.1200, valid loss-2.3032, acc-0.0842, test loss-2.3043, acc-0.0853\n",
      "Iter-940, train loss-2.3239, acc-0.0200, valid loss-2.3028, acc-0.0846, test loss-2.3038, acc-0.0853\n",
      "Iter-950, train loss-2.3424, acc-0.0000, valid loss-2.3023, acc-0.0848, test loss-2.3034, acc-0.0861\n",
      "Iter-960, train loss-2.2788, acc-0.1600, valid loss-2.3018, acc-0.0854, test loss-2.3028, acc-0.0866\n",
      "Iter-970, train loss-2.3380, acc-0.0600, valid loss-2.3013, acc-0.0860, test loss-2.3024, acc-0.0873\n",
      "Iter-980, train loss-2.3067, acc-0.0800, valid loss-2.3009, acc-0.0872, test loss-2.3020, acc-0.0872\n",
      "Iter-990, train loss-2.3008, acc-0.1200, valid loss-2.3005, acc-0.0874, test loss-2.3015, acc-0.0874\n",
      "Iter-1000, train loss-2.2769, acc-0.1800, valid loss-2.3001, acc-0.0882, test loss-2.3011, acc-0.0879\n",
      "Iter-1010, train loss-2.3067, acc-0.1200, valid loss-2.2996, acc-0.0886, test loss-2.3006, acc-0.0880\n",
      "Iter-1020, train loss-2.3239, acc-0.0200, valid loss-2.2992, acc-0.0890, test loss-2.3001, acc-0.0885\n",
      "Iter-1030, train loss-2.3073, acc-0.1000, valid loss-2.2987, acc-0.0896, test loss-2.2997, acc-0.0893\n",
      "Iter-1040, train loss-2.3075, acc-0.1000, valid loss-2.2982, acc-0.0892, test loss-2.2992, acc-0.0898\n",
      "Iter-1050, train loss-2.2825, acc-0.0800, valid loss-2.2978, acc-0.0894, test loss-2.2987, acc-0.0901\n",
      "Iter-1060, train loss-2.2902, acc-0.1200, valid loss-2.2973, acc-0.0894, test loss-2.2982, acc-0.0903\n",
      "Iter-1070, train loss-2.2920, acc-0.0800, valid loss-2.2968, acc-0.0906, test loss-2.2977, acc-0.0908\n",
      "Iter-1080, train loss-2.2903, acc-0.1000, valid loss-2.2964, acc-0.0906, test loss-2.2973, acc-0.0912\n",
      "Iter-1090, train loss-2.3085, acc-0.0600, valid loss-2.2959, acc-0.0910, test loss-2.2968, acc-0.0915\n",
      "Iter-1100, train loss-2.3208, acc-0.0800, valid loss-2.2955, acc-0.0916, test loss-2.2964, acc-0.0918\n",
      "Iter-1110, train loss-2.2937, acc-0.1200, valid loss-2.2950, acc-0.0926, test loss-2.2959, acc-0.0920\n",
      "Iter-1120, train loss-2.2866, acc-0.2000, valid loss-2.2945, acc-0.0920, test loss-2.2954, acc-0.0920\n",
      "Iter-1130, train loss-2.2924, acc-0.1000, valid loss-2.2941, acc-0.0930, test loss-2.2950, acc-0.0921\n",
      "Iter-1140, train loss-2.3084, acc-0.0800, valid loss-2.2936, acc-0.0926, test loss-2.2945, acc-0.0925\n",
      "Iter-1150, train loss-2.3099, acc-0.1200, valid loss-2.2931, acc-0.0946, test loss-2.2940, acc-0.0941\n",
      "Iter-1160, train loss-2.3055, acc-0.0800, valid loss-2.2926, acc-0.0950, test loss-2.2934, acc-0.0954\n",
      "Iter-1170, train loss-2.2846, acc-0.0600, valid loss-2.2920, acc-0.0952, test loss-2.2929, acc-0.0961\n",
      "Iter-1180, train loss-2.2756, acc-0.1800, valid loss-2.2915, acc-0.0960, test loss-2.2924, acc-0.0969\n",
      "Iter-1190, train loss-2.2897, acc-0.1200, valid loss-2.2910, acc-0.0964, test loss-2.2918, acc-0.0971\n",
      "Iter-1200, train loss-2.3232, acc-0.0800, valid loss-2.2905, acc-0.0964, test loss-2.2913, acc-0.0974\n",
      "Iter-1210, train loss-2.3081, acc-0.0600, valid loss-2.2899, acc-0.0970, test loss-2.2908, acc-0.0976\n",
      "Iter-1220, train loss-2.2861, acc-0.1400, valid loss-2.2894, acc-0.0982, test loss-2.2902, acc-0.0982\n",
      "Iter-1230, train loss-2.2958, acc-0.0600, valid loss-2.2889, acc-0.0988, test loss-2.2897, acc-0.0986\n",
      "Iter-1240, train loss-2.2839, acc-0.1000, valid loss-2.2884, acc-0.0996, test loss-2.2892, acc-0.0993\n",
      "Iter-1250, train loss-2.3084, acc-0.0600, valid loss-2.2878, acc-0.0996, test loss-2.2886, acc-0.1001\n",
      "Iter-1260, train loss-2.2823, acc-0.1400, valid loss-2.2873, acc-0.0996, test loss-2.2880, acc-0.1009\n",
      "Iter-1270, train loss-2.2678, acc-0.1200, valid loss-2.2867, acc-0.1002, test loss-2.2875, acc-0.1014\n",
      "Iter-1280, train loss-2.2746, acc-0.1000, valid loss-2.2861, acc-0.1000, test loss-2.2869, acc-0.1019\n",
      "Iter-1290, train loss-2.3214, acc-0.0400, valid loss-2.2857, acc-0.0998, test loss-2.2864, acc-0.1023\n",
      "Iter-1300, train loss-2.2853, acc-0.0600, valid loss-2.2851, acc-0.1006, test loss-2.2858, acc-0.1026\n",
      "Iter-1310, train loss-2.2927, acc-0.1200, valid loss-2.2845, acc-0.1010, test loss-2.2853, acc-0.1031\n",
      "Iter-1320, train loss-2.2982, acc-0.1000, valid loss-2.2839, acc-0.1018, test loss-2.2847, acc-0.1039\n",
      "Iter-1330, train loss-2.2808, acc-0.0800, valid loss-2.2834, acc-0.1022, test loss-2.2841, acc-0.1041\n",
      "Iter-1340, train loss-2.2844, acc-0.1000, valid loss-2.2828, acc-0.1022, test loss-2.2835, acc-0.1047\n",
      "Iter-1350, train loss-2.2801, acc-0.0600, valid loss-2.2822, acc-0.1026, test loss-2.2829, acc-0.1052\n",
      "Iter-1360, train loss-2.2636, acc-0.1200, valid loss-2.2816, acc-0.1032, test loss-2.2824, acc-0.1056\n",
      "Iter-1370, train loss-2.2735, acc-0.1000, valid loss-2.2811, acc-0.1040, test loss-2.2818, acc-0.1058\n",
      "Iter-1380, train loss-2.2918, acc-0.0600, valid loss-2.2805, acc-0.1046, test loss-2.2812, acc-0.1061\n",
      "Iter-1390, train loss-2.2713, acc-0.1200, valid loss-2.2799, acc-0.1050, test loss-2.2806, acc-0.1060\n",
      "Iter-1400, train loss-2.2603, acc-0.1200, valid loss-2.2793, acc-0.1056, test loss-2.2800, acc-0.1065\n",
      "Iter-1410, train loss-2.2910, acc-0.1000, valid loss-2.2788, acc-0.1060, test loss-2.2795, acc-0.1065\n",
      "Iter-1420, train loss-2.2996, acc-0.1000, valid loss-2.2782, acc-0.1068, test loss-2.2789, acc-0.1072\n",
      "Iter-1430, train loss-2.2978, acc-0.1200, valid loss-2.2776, acc-0.1064, test loss-2.2783, acc-0.1074\n",
      "Iter-1440, train loss-2.2832, acc-0.0600, valid loss-2.2770, acc-0.1070, test loss-2.2777, acc-0.1084\n",
      "Iter-1450, train loss-2.2616, acc-0.1200, valid loss-2.2764, acc-0.1076, test loss-2.2771, acc-0.1093\n",
      "Iter-1460, train loss-2.3067, acc-0.1000, valid loss-2.2758, acc-0.1086, test loss-2.2765, acc-0.1098\n",
      "Iter-1470, train loss-2.2983, acc-0.1000, valid loss-2.2752, acc-0.1098, test loss-2.2758, acc-0.1111\n",
      "Iter-1480, train loss-2.2883, acc-0.0600, valid loss-2.2745, acc-0.1098, test loss-2.2752, acc-0.1110\n",
      "Iter-1490, train loss-2.2554, acc-0.1800, valid loss-2.2739, acc-0.1098, test loss-2.2746, acc-0.1112\n",
      "Iter-1500, train loss-2.2626, acc-0.1000, valid loss-2.2733, acc-0.1106, test loss-2.2739, acc-0.1116\n",
      "Iter-1510, train loss-2.2716, acc-0.1200, valid loss-2.2727, acc-0.1112, test loss-2.2733, acc-0.1118\n",
      "Iter-1520, train loss-2.2935, acc-0.1400, valid loss-2.2721, acc-0.1124, test loss-2.2727, acc-0.1126\n",
      "Iter-1530, train loss-2.2524, acc-0.1400, valid loss-2.2715, acc-0.1128, test loss-2.2722, acc-0.1131\n",
      "Iter-1540, train loss-2.2683, acc-0.1600, valid loss-2.2708, acc-0.1132, test loss-2.2715, acc-0.1135\n",
      "Iter-1550, train loss-2.2672, acc-0.1000, valid loss-2.2702, acc-0.1138, test loss-2.2708, acc-0.1138\n",
      "Iter-1560, train loss-2.2739, acc-0.0600, valid loss-2.2695, acc-0.1134, test loss-2.2702, acc-0.1143\n",
      "Iter-1570, train loss-2.2578, acc-0.1600, valid loss-2.2690, acc-0.1136, test loss-2.2696, acc-0.1152\n",
      "Iter-1580, train loss-2.3118, acc-0.0400, valid loss-2.2683, acc-0.1138, test loss-2.2689, acc-0.1155\n",
      "Iter-1590, train loss-2.2897, acc-0.1000, valid loss-2.2676, acc-0.1138, test loss-2.2683, acc-0.1159\n",
      "Iter-1600, train loss-2.2466, acc-0.1400, valid loss-2.2669, acc-0.1140, test loss-2.2676, acc-0.1160\n",
      "Iter-1610, train loss-2.2604, acc-0.1200, valid loss-2.2663, acc-0.1154, test loss-2.2669, acc-0.1164\n",
      "Iter-1620, train loss-2.2851, acc-0.1200, valid loss-2.2656, acc-0.1162, test loss-2.2663, acc-0.1166\n",
      "Iter-1630, train loss-2.2656, acc-0.1400, valid loss-2.2649, acc-0.1168, test loss-2.2656, acc-0.1174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.2782, acc-0.1200, valid loss-2.2643, acc-0.1178, test loss-2.2649, acc-0.1178\n",
      "Iter-1650, train loss-2.2414, acc-0.1800, valid loss-2.2636, acc-0.1188, test loss-2.2642, acc-0.1184\n",
      "Iter-1660, train loss-2.2713, acc-0.2000, valid loss-2.2628, acc-0.1192, test loss-2.2635, acc-0.1189\n",
      "Iter-1670, train loss-2.2541, acc-0.1600, valid loss-2.2621, acc-0.1198, test loss-2.2628, acc-0.1197\n",
      "Iter-1680, train loss-2.2561, acc-0.1200, valid loss-2.2615, acc-0.1204, test loss-2.2621, acc-0.1200\n",
      "Iter-1690, train loss-2.2828, acc-0.1600, valid loss-2.2608, acc-0.1210, test loss-2.2615, acc-0.1205\n",
      "Iter-1700, train loss-2.2742, acc-0.1200, valid loss-2.2602, acc-0.1210, test loss-2.2609, acc-0.1208\n",
      "Iter-1710, train loss-2.2678, acc-0.1400, valid loss-2.2595, acc-0.1216, test loss-2.2602, acc-0.1207\n",
      "Iter-1720, train loss-2.2846, acc-0.0800, valid loss-2.2588, acc-0.1226, test loss-2.2595, acc-0.1207\n",
      "Iter-1730, train loss-2.2894, acc-0.1200, valid loss-2.2581, acc-0.1226, test loss-2.2587, acc-0.1216\n",
      "Iter-1740, train loss-2.2463, acc-0.1000, valid loss-2.2574, acc-0.1234, test loss-2.2580, acc-0.1221\n",
      "Iter-1750, train loss-2.2840, acc-0.1400, valid loss-2.2567, acc-0.1244, test loss-2.2574, acc-0.1224\n",
      "Iter-1760, train loss-2.2737, acc-0.0800, valid loss-2.2560, acc-0.1244, test loss-2.2567, acc-0.1229\n",
      "Iter-1770, train loss-2.2172, acc-0.1800, valid loss-2.2553, acc-0.1252, test loss-2.2559, acc-0.1232\n",
      "Iter-1780, train loss-2.2617, acc-0.1000, valid loss-2.2546, acc-0.1252, test loss-2.2553, acc-0.1240\n",
      "Iter-1790, train loss-2.2781, acc-0.0800, valid loss-2.2539, acc-0.1260, test loss-2.2546, acc-0.1240\n",
      "Iter-1800, train loss-2.2231, acc-0.2200, valid loss-2.2532, acc-0.1262, test loss-2.2539, acc-0.1246\n",
      "Iter-1810, train loss-2.2450, acc-0.1200, valid loss-2.2525, acc-0.1266, test loss-2.2531, acc-0.1251\n",
      "Iter-1820, train loss-2.2672, acc-0.0600, valid loss-2.2518, acc-0.1274, test loss-2.2524, acc-0.1253\n",
      "Iter-1830, train loss-2.2424, acc-0.0600, valid loss-2.2511, acc-0.1278, test loss-2.2518, acc-0.1257\n",
      "Iter-1840, train loss-2.2510, acc-0.1000, valid loss-2.2504, acc-0.1296, test loss-2.2511, acc-0.1261\n",
      "Iter-1850, train loss-2.2152, acc-0.1000, valid loss-2.2497, acc-0.1306, test loss-2.2504, acc-0.1267\n",
      "Iter-1860, train loss-2.2709, acc-0.0800, valid loss-2.2491, acc-0.1306, test loss-2.2498, acc-0.1274\n",
      "Iter-1870, train loss-2.2597, acc-0.0600, valid loss-2.2483, acc-0.1302, test loss-2.2490, acc-0.1279\n",
      "Iter-1880, train loss-2.2702, acc-0.1400, valid loss-2.2477, acc-0.1306, test loss-2.2483, acc-0.1288\n",
      "Iter-1890, train loss-2.2462, acc-0.1000, valid loss-2.2469, acc-0.1312, test loss-2.2476, acc-0.1291\n",
      "Iter-1900, train loss-2.2424, acc-0.2000, valid loss-2.2464, acc-0.1314, test loss-2.2470, acc-0.1293\n",
      "Iter-1910, train loss-2.2409, acc-0.2200, valid loss-2.2457, acc-0.1320, test loss-2.2464, acc-0.1297\n",
      "Iter-1920, train loss-2.2603, acc-0.1000, valid loss-2.2450, acc-0.1340, test loss-2.2456, acc-0.1303\n",
      "Iter-1930, train loss-2.2586, acc-0.0600, valid loss-2.2442, acc-0.1342, test loss-2.2449, acc-0.1306\n",
      "Iter-1940, train loss-2.2449, acc-0.1600, valid loss-2.2434, acc-0.1348, test loss-2.2440, acc-0.1312\n",
      "Iter-1950, train loss-2.2443, acc-0.0800, valid loss-2.2427, acc-0.1360, test loss-2.2433, acc-0.1313\n",
      "Iter-1960, train loss-2.2508, acc-0.1000, valid loss-2.2419, acc-0.1378, test loss-2.2426, acc-0.1319\n",
      "Iter-1970, train loss-2.2441, acc-0.0400, valid loss-2.2411, acc-0.1382, test loss-2.2418, acc-0.1326\n",
      "Iter-1980, train loss-2.2830, acc-0.1400, valid loss-2.2403, acc-0.1388, test loss-2.2410, acc-0.1330\n",
      "Iter-1990, train loss-2.2778, acc-0.1000, valid loss-2.2396, acc-0.1386, test loss-2.2402, acc-0.1337\n",
      "Iter-2000, train loss-2.2569, acc-0.0800, valid loss-2.2387, acc-0.1392, test loss-2.2394, acc-0.1345\n",
      "Iter-2010, train loss-2.2544, acc-0.1000, valid loss-2.2379, acc-0.1398, test loss-2.2386, acc-0.1353\n",
      "Iter-2020, train loss-2.2502, acc-0.2200, valid loss-2.2372, acc-0.1400, test loss-2.2379, acc-0.1362\n",
      "Iter-2030, train loss-2.2628, acc-0.1000, valid loss-2.2364, acc-0.1408, test loss-2.2370, acc-0.1363\n",
      "Iter-2040, train loss-2.2566, acc-0.1200, valid loss-2.2356, acc-0.1414, test loss-2.2363, acc-0.1369\n",
      "Iter-2050, train loss-2.2519, acc-0.0800, valid loss-2.2348, acc-0.1432, test loss-2.2354, acc-0.1372\n",
      "Iter-2060, train loss-2.2553, acc-0.1200, valid loss-2.2340, acc-0.1452, test loss-2.2347, acc-0.1381\n",
      "Iter-2070, train loss-2.2710, acc-0.1200, valid loss-2.2332, acc-0.1468, test loss-2.2339, acc-0.1390\n",
      "Iter-2080, train loss-2.2353, acc-0.1800, valid loss-2.2324, acc-0.1476, test loss-2.2330, acc-0.1400\n",
      "Iter-2090, train loss-2.2418, acc-0.1200, valid loss-2.2315, acc-0.1486, test loss-2.2322, acc-0.1409\n",
      "Iter-2100, train loss-2.2240, acc-0.1200, valid loss-2.2308, acc-0.1510, test loss-2.2315, acc-0.1417\n",
      "Iter-2110, train loss-2.2377, acc-0.1200, valid loss-2.2300, acc-0.1518, test loss-2.2307, acc-0.1421\n",
      "Iter-2120, train loss-2.2299, acc-0.0800, valid loss-2.2293, acc-0.1526, test loss-2.2300, acc-0.1425\n",
      "Iter-2130, train loss-2.2032, acc-0.1800, valid loss-2.2285, acc-0.1534, test loss-2.2292, acc-0.1429\n",
      "Iter-2140, train loss-2.2392, acc-0.1000, valid loss-2.2276, acc-0.1554, test loss-2.2283, acc-0.1436\n",
      "Iter-2150, train loss-2.2361, acc-0.1400, valid loss-2.2270, acc-0.1558, test loss-2.2276, acc-0.1443\n",
      "Iter-2160, train loss-2.2509, acc-0.1400, valid loss-2.2261, acc-0.1564, test loss-2.2268, acc-0.1452\n",
      "Iter-2170, train loss-2.2744, acc-0.1200, valid loss-2.2254, acc-0.1570, test loss-2.2260, acc-0.1463\n",
      "Iter-2180, train loss-2.2426, acc-0.0600, valid loss-2.2246, acc-0.1584, test loss-2.2253, acc-0.1476\n",
      "Iter-2190, train loss-2.2021, acc-0.1400, valid loss-2.2238, acc-0.1586, test loss-2.2245, acc-0.1481\n",
      "Iter-2200, train loss-2.2124, acc-0.1600, valid loss-2.2230, acc-0.1596, test loss-2.2237, acc-0.1497\n",
      "Iter-2210, train loss-2.2299, acc-0.1800, valid loss-2.2221, acc-0.1610, test loss-2.2228, acc-0.1505\n",
      "Iter-2220, train loss-2.2253, acc-0.1800, valid loss-2.2213, acc-0.1624, test loss-2.2220, acc-0.1526\n",
      "Iter-2230, train loss-2.1835, acc-0.1800, valid loss-2.2205, acc-0.1640, test loss-2.2212, acc-0.1535\n",
      "Iter-2240, train loss-2.2137, acc-0.1200, valid loss-2.2197, acc-0.1642, test loss-2.2204, acc-0.1540\n",
      "Iter-2250, train loss-2.2421, acc-0.1000, valid loss-2.2188, acc-0.1662, test loss-2.2196, acc-0.1551\n",
      "Iter-2260, train loss-2.2478, acc-0.0600, valid loss-2.2180, acc-0.1672, test loss-2.2187, acc-0.1555\n",
      "Iter-2270, train loss-2.2290, acc-0.1000, valid loss-2.2172, acc-0.1668, test loss-2.2179, acc-0.1568\n",
      "Iter-2280, train loss-2.2257, acc-0.1000, valid loss-2.2164, acc-0.1674, test loss-2.2171, acc-0.1581\n",
      "Iter-2290, train loss-2.2395, acc-0.1200, valid loss-2.2155, acc-0.1694, test loss-2.2162, acc-0.1598\n",
      "Iter-2300, train loss-2.2228, acc-0.1200, valid loss-2.2146, acc-0.1696, test loss-2.2154, acc-0.1609\n",
      "Iter-2310, train loss-2.2114, acc-0.1400, valid loss-2.2138, acc-0.1726, test loss-2.2146, acc-0.1627\n",
      "Iter-2320, train loss-2.2118, acc-0.2200, valid loss-2.2130, acc-0.1728, test loss-2.2137, acc-0.1636\n",
      "Iter-2330, train loss-2.2433, acc-0.0800, valid loss-2.2121, acc-0.1750, test loss-2.2129, acc-0.1645\n",
      "Iter-2340, train loss-2.1895, acc-0.1400, valid loss-2.2112, acc-0.1758, test loss-2.2119, acc-0.1660\n",
      "Iter-2350, train loss-2.1944, acc-0.1800, valid loss-2.2103, acc-0.1768, test loss-2.2111, acc-0.1670\n",
      "Iter-2360, train loss-2.1866, acc-0.1400, valid loss-2.2094, acc-0.1774, test loss-2.2102, acc-0.1675\n",
      "Iter-2370, train loss-2.1712, acc-0.1000, valid loss-2.2086, acc-0.1792, test loss-2.2093, acc-0.1687\n",
      "Iter-2380, train loss-2.1907, acc-0.1000, valid loss-2.2078, acc-0.1792, test loss-2.2085, acc-0.1686\n",
      "Iter-2390, train loss-2.2054, acc-0.1600, valid loss-2.2069, acc-0.1796, test loss-2.2076, acc-0.1699\n",
      "Iter-2400, train loss-2.2733, acc-0.0600, valid loss-2.2060, acc-0.1808, test loss-2.2068, acc-0.1718\n",
      "Iter-2410, train loss-2.2203, acc-0.1000, valid loss-2.2052, acc-0.1812, test loss-2.2060, acc-0.1725\n",
      "Iter-2420, train loss-2.1877, acc-0.1800, valid loss-2.2042, acc-0.1830, test loss-2.2050, acc-0.1747\n",
      "Iter-2430, train loss-2.1857, acc-0.1200, valid loss-2.2034, acc-0.1832, test loss-2.2042, acc-0.1761\n",
      "Iter-2440, train loss-2.2148, acc-0.1800, valid loss-2.2026, acc-0.1844, test loss-2.2034, acc-0.1774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.2043, acc-0.1400, valid loss-2.2017, acc-0.1858, test loss-2.2025, acc-0.1780\n",
      "Iter-2460, train loss-2.1732, acc-0.1000, valid loss-2.2008, acc-0.1856, test loss-2.2016, acc-0.1796\n",
      "Iter-2470, train loss-2.1926, acc-0.1800, valid loss-2.1999, acc-0.1876, test loss-2.2007, acc-0.1811\n",
      "Iter-2480, train loss-2.2219, acc-0.2200, valid loss-2.1991, acc-0.1878, test loss-2.1999, acc-0.1829\n",
      "Iter-2490, train loss-2.2210, acc-0.0800, valid loss-2.1982, acc-0.1886, test loss-2.1990, acc-0.1839\n",
      "Iter-2500, train loss-2.1921, acc-0.1600, valid loss-2.1972, acc-0.1902, test loss-2.1981, acc-0.1844\n",
      "Iter-2510, train loss-2.2143, acc-0.1600, valid loss-2.1963, acc-0.1912, test loss-2.1972, acc-0.1854\n",
      "Iter-2520, train loss-2.2113, acc-0.1000, valid loss-2.1953, acc-0.1924, test loss-2.1962, acc-0.1865\n",
      "Iter-2530, train loss-2.2595, acc-0.1200, valid loss-2.1943, acc-0.1944, test loss-2.1952, acc-0.1888\n",
      "Iter-2540, train loss-2.2133, acc-0.1400, valid loss-2.1935, acc-0.1958, test loss-2.1944, acc-0.1906\n",
      "Iter-2550, train loss-2.2156, acc-0.1800, valid loss-2.1927, acc-0.1960, test loss-2.1936, acc-0.1918\n",
      "Iter-2560, train loss-2.1724, acc-0.2200, valid loss-2.1918, acc-0.1978, test loss-2.1927, acc-0.1927\n",
      "Iter-2570, train loss-2.2064, acc-0.2200, valid loss-2.1909, acc-0.1990, test loss-2.1918, acc-0.1940\n",
      "Iter-2580, train loss-2.1858, acc-0.2200, valid loss-2.1899, acc-0.2016, test loss-2.1908, acc-0.1946\n",
      "Iter-2590, train loss-2.2059, acc-0.2000, valid loss-2.1891, acc-0.2028, test loss-2.1900, acc-0.1959\n",
      "Iter-2600, train loss-2.2105, acc-0.2000, valid loss-2.1881, acc-0.2042, test loss-2.1890, acc-0.1969\n",
      "Iter-2610, train loss-2.2279, acc-0.0800, valid loss-2.1872, acc-0.2044, test loss-2.1881, acc-0.1989\n",
      "Iter-2620, train loss-2.1808, acc-0.2400, valid loss-2.1863, acc-0.2050, test loss-2.1872, acc-0.2007\n",
      "Iter-2630, train loss-2.1980, acc-0.1400, valid loss-2.1853, acc-0.2056, test loss-2.1862, acc-0.2018\n",
      "Iter-2640, train loss-2.1457, acc-0.2200, valid loss-2.1844, acc-0.2068, test loss-2.1854, acc-0.2026\n",
      "Iter-2650, train loss-2.1744, acc-0.2200, valid loss-2.1835, acc-0.2074, test loss-2.1845, acc-0.2041\n",
      "Iter-2660, train loss-2.2606, acc-0.1800, valid loss-2.1826, acc-0.2098, test loss-2.1836, acc-0.2054\n",
      "Iter-2670, train loss-2.2354, acc-0.0600, valid loss-2.1817, acc-0.2108, test loss-2.1827, acc-0.2066\n",
      "Iter-2680, train loss-2.1686, acc-0.1600, valid loss-2.1808, acc-0.2110, test loss-2.1818, acc-0.2080\n",
      "Iter-2690, train loss-2.1800, acc-0.2400, valid loss-2.1799, acc-0.2120, test loss-2.1809, acc-0.2089\n",
      "Iter-2700, train loss-2.1615, acc-0.2000, valid loss-2.1790, acc-0.2128, test loss-2.1799, acc-0.2104\n",
      "Iter-2710, train loss-2.1639, acc-0.1600, valid loss-2.1780, acc-0.2150, test loss-2.1790, acc-0.2126\n",
      "Iter-2720, train loss-2.1536, acc-0.1800, valid loss-2.1770, acc-0.2156, test loss-2.1781, acc-0.2135\n",
      "Iter-2730, train loss-2.2089, acc-0.1800, valid loss-2.1761, acc-0.2170, test loss-2.1771, acc-0.2145\n",
      "Iter-2740, train loss-2.2016, acc-0.1600, valid loss-2.1752, acc-0.2178, test loss-2.1762, acc-0.2157\n",
      "Iter-2750, train loss-2.1710, acc-0.1600, valid loss-2.1743, acc-0.2186, test loss-2.1753, acc-0.2171\n",
      "Iter-2760, train loss-2.2680, acc-0.1400, valid loss-2.1733, acc-0.2194, test loss-2.1743, acc-0.2181\n",
      "Iter-2770, train loss-2.1971, acc-0.2000, valid loss-2.1724, acc-0.2200, test loss-2.1734, acc-0.2193\n",
      "Iter-2780, train loss-2.1504, acc-0.3200, valid loss-2.1714, acc-0.2206, test loss-2.1724, acc-0.2196\n",
      "Iter-2790, train loss-2.2139, acc-0.2400, valid loss-2.1705, acc-0.2212, test loss-2.1715, acc-0.2202\n",
      "Iter-2800, train loss-2.2324, acc-0.1400, valid loss-2.1697, acc-0.2220, test loss-2.1707, acc-0.2214\n",
      "Iter-2810, train loss-2.1296, acc-0.2800, valid loss-2.1689, acc-0.2226, test loss-2.1699, acc-0.2225\n",
      "Iter-2820, train loss-2.2103, acc-0.1200, valid loss-2.1680, acc-0.2230, test loss-2.1690, acc-0.2231\n",
      "Iter-2830, train loss-2.2364, acc-0.1000, valid loss-2.1671, acc-0.2232, test loss-2.1681, acc-0.2233\n",
      "Iter-2840, train loss-2.1504, acc-0.2200, valid loss-2.1662, acc-0.2232, test loss-2.1672, acc-0.2240\n",
      "Iter-2850, train loss-2.1854, acc-0.2400, valid loss-2.1652, acc-0.2220, test loss-2.1663, acc-0.2234\n",
      "Iter-2860, train loss-2.2092, acc-0.1000, valid loss-2.1642, acc-0.2226, test loss-2.1652, acc-0.2233\n",
      "Iter-2870, train loss-2.1477, acc-0.2200, valid loss-2.1632, acc-0.2230, test loss-2.1642, acc-0.2230\n",
      "Iter-2880, train loss-2.2188, acc-0.1400, valid loss-2.1623, acc-0.2220, test loss-2.1634, acc-0.2228\n",
      "Iter-2890, train loss-2.1833, acc-0.1800, valid loss-2.1613, acc-0.2222, test loss-2.1624, acc-0.2226\n",
      "Iter-2900, train loss-2.1591, acc-0.2200, valid loss-2.1603, acc-0.2222, test loss-2.1614, acc-0.2227\n",
      "Iter-2910, train loss-2.1849, acc-0.2200, valid loss-2.1595, acc-0.2226, test loss-2.1605, acc-0.2229\n",
      "Iter-2920, train loss-2.0773, acc-0.3400, valid loss-2.1584, acc-0.2230, test loss-2.1595, acc-0.2232\n",
      "Iter-2930, train loss-2.1237, acc-0.3400, valid loss-2.1575, acc-0.2226, test loss-2.1585, acc-0.2231\n",
      "Iter-2940, train loss-2.1670, acc-0.2400, valid loss-2.1565, acc-0.2224, test loss-2.1576, acc-0.2230\n",
      "Iter-2950, train loss-2.1588, acc-0.2800, valid loss-2.1555, acc-0.2222, test loss-2.1566, acc-0.2226\n",
      "Iter-2960, train loss-2.1128, acc-0.2800, valid loss-2.1546, acc-0.2226, test loss-2.1557, acc-0.2228\n",
      "Iter-2970, train loss-2.1476, acc-0.2800, valid loss-2.1536, acc-0.2224, test loss-2.1547, acc-0.2226\n",
      "Iter-2980, train loss-2.1333, acc-0.2600, valid loss-2.1525, acc-0.2228, test loss-2.1536, acc-0.2229\n",
      "Iter-2990, train loss-2.1594, acc-0.2000, valid loss-2.1515, acc-0.2226, test loss-2.1526, acc-0.2229\n",
      "Iter-3000, train loss-2.1550, acc-0.1600, valid loss-2.1505, acc-0.2232, test loss-2.1517, acc-0.2230\n",
      "Iter-3010, train loss-2.1629, acc-0.1800, valid loss-2.1495, acc-0.2228, test loss-2.1506, acc-0.2232\n",
      "Iter-3020, train loss-2.1866, acc-0.2000, valid loss-2.1485, acc-0.2224, test loss-2.1497, acc-0.2232\n",
      "Iter-3030, train loss-2.2025, acc-0.1800, valid loss-2.1476, acc-0.2224, test loss-2.1487, acc-0.2233\n",
      "Iter-3040, train loss-2.1569, acc-0.1400, valid loss-2.1466, acc-0.2224, test loss-2.1478, acc-0.2233\n",
      "Iter-3050, train loss-2.1596, acc-0.1600, valid loss-2.1457, acc-0.2228, test loss-2.1469, acc-0.2237\n",
      "Iter-3060, train loss-2.0794, acc-0.3400, valid loss-2.1448, acc-0.2224, test loss-2.1459, acc-0.2235\n",
      "Iter-3070, train loss-2.1802, acc-0.2000, valid loss-2.1440, acc-0.2224, test loss-2.1451, acc-0.2236\n",
      "Iter-3080, train loss-2.1578, acc-0.2000, valid loss-2.1429, acc-0.2230, test loss-2.1440, acc-0.2237\n",
      "Iter-3090, train loss-2.1319, acc-0.1400, valid loss-2.1420, acc-0.2230, test loss-2.1431, acc-0.2237\n",
      "Iter-3100, train loss-2.1482, acc-0.2000, valid loss-2.1410, acc-0.2232, test loss-2.1421, acc-0.2239\n",
      "Iter-3110, train loss-2.1269, acc-0.1600, valid loss-2.1400, acc-0.2234, test loss-2.1412, acc-0.2243\n",
      "Iter-3120, train loss-2.1177, acc-0.2600, valid loss-2.1391, acc-0.2228, test loss-2.1402, acc-0.2241\n",
      "Iter-3130, train loss-2.1368, acc-0.2800, valid loss-2.1381, acc-0.2228, test loss-2.1393, acc-0.2240\n",
      "Iter-3140, train loss-2.1328, acc-0.2400, valid loss-2.1372, acc-0.2228, test loss-2.1384, acc-0.2242\n",
      "Iter-3150, train loss-2.1882, acc-0.2000, valid loss-2.1362, acc-0.2228, test loss-2.1374, acc-0.2244\n",
      "Iter-3160, train loss-2.1276, acc-0.1800, valid loss-2.1353, acc-0.2230, test loss-2.1365, acc-0.2249\n",
      "Iter-3170, train loss-2.1190, acc-0.2400, valid loss-2.1344, acc-0.2232, test loss-2.1355, acc-0.2251\n",
      "Iter-3180, train loss-2.1105, acc-0.2800, valid loss-2.1334, acc-0.2228, test loss-2.1346, acc-0.2254\n",
      "Iter-3190, train loss-2.1069, acc-0.2200, valid loss-2.1324, acc-0.2232, test loss-2.1336, acc-0.2256\n",
      "Iter-3200, train loss-2.1610, acc-0.2400, valid loss-2.1314, acc-0.2232, test loss-2.1326, acc-0.2257\n",
      "Iter-3210, train loss-2.1568, acc-0.2000, valid loss-2.1303, acc-0.2234, test loss-2.1315, acc-0.2258\n",
      "Iter-3220, train loss-2.1241, acc-0.2200, valid loss-2.1294, acc-0.2234, test loss-2.1306, acc-0.2261\n",
      "Iter-3230, train loss-2.1868, acc-0.1200, valid loss-2.1284, acc-0.2234, test loss-2.1296, acc-0.2263\n",
      "Iter-3240, train loss-2.1468, acc-0.1800, valid loss-2.1274, acc-0.2242, test loss-2.1287, acc-0.2268\n",
      "Iter-3250, train loss-2.1182, acc-0.2400, valid loss-2.1265, acc-0.2242, test loss-2.1277, acc-0.2269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.1796, acc-0.1600, valid loss-2.1255, acc-0.2250, test loss-2.1267, acc-0.2274\n",
      "Iter-3270, train loss-2.1306, acc-0.1400, valid loss-2.1245, acc-0.2250, test loss-2.1258, acc-0.2278\n",
      "Iter-3280, train loss-2.0998, acc-0.2600, valid loss-2.1236, acc-0.2244, test loss-2.1248, acc-0.2281\n",
      "Iter-3290, train loss-2.1232, acc-0.2200, valid loss-2.1226, acc-0.2246, test loss-2.1239, acc-0.2286\n",
      "Iter-3300, train loss-2.1201, acc-0.1800, valid loss-2.1216, acc-0.2246, test loss-2.1229, acc-0.2292\n",
      "Iter-3310, train loss-2.2139, acc-0.2000, valid loss-2.1206, acc-0.2258, test loss-2.1219, acc-0.2294\n",
      "Iter-3320, train loss-2.0755, acc-0.3400, valid loss-2.1197, acc-0.2260, test loss-2.1209, acc-0.2292\n",
      "Iter-3330, train loss-2.1639, acc-0.1600, valid loss-2.1187, acc-0.2258, test loss-2.1200, acc-0.2297\n",
      "Iter-3340, train loss-2.1461, acc-0.2200, valid loss-2.1177, acc-0.2268, test loss-2.1190, acc-0.2303\n",
      "Iter-3350, train loss-2.1085, acc-0.2000, valid loss-2.1168, acc-0.2270, test loss-2.1181, acc-0.2306\n",
      "Iter-3360, train loss-2.1394, acc-0.1800, valid loss-2.1158, acc-0.2274, test loss-2.1171, acc-0.2305\n",
      "Iter-3370, train loss-2.1352, acc-0.1800, valid loss-2.1148, acc-0.2276, test loss-2.1161, acc-0.2307\n",
      "Iter-3380, train loss-2.1851, acc-0.1200, valid loss-2.1139, acc-0.2278, test loss-2.1152, acc-0.2313\n",
      "Iter-3390, train loss-2.1393, acc-0.1600, valid loss-2.1128, acc-0.2276, test loss-2.1142, acc-0.2320\n",
      "Iter-3400, train loss-2.0810, acc-0.2800, valid loss-2.1118, acc-0.2282, test loss-2.1131, acc-0.2319\n",
      "Iter-3410, train loss-2.1138, acc-0.2400, valid loss-2.1110, acc-0.2292, test loss-2.1123, acc-0.2332\n",
      "Iter-3420, train loss-2.1106, acc-0.2600, valid loss-2.1099, acc-0.2292, test loss-2.1112, acc-0.2332\n",
      "Iter-3430, train loss-2.1930, acc-0.1800, valid loss-2.1089, acc-0.2300, test loss-2.1103, acc-0.2335\n",
      "Iter-3440, train loss-2.1551, acc-0.1400, valid loss-2.1079, acc-0.2298, test loss-2.1092, acc-0.2338\n",
      "Iter-3450, train loss-2.1237, acc-0.2000, valid loss-2.1070, acc-0.2302, test loss-2.1083, acc-0.2340\n",
      "Iter-3460, train loss-2.1053, acc-0.2000, valid loss-2.1060, acc-0.2310, test loss-2.1074, acc-0.2344\n",
      "Iter-3470, train loss-2.1219, acc-0.2600, valid loss-2.1050, acc-0.2310, test loss-2.1064, acc-0.2348\n",
      "Iter-3480, train loss-2.1596, acc-0.2400, valid loss-2.1041, acc-0.2316, test loss-2.1054, acc-0.2362\n",
      "Iter-3490, train loss-2.0999, acc-0.2200, valid loss-2.1031, acc-0.2322, test loss-2.1044, acc-0.2370\n",
      "Iter-3500, train loss-2.1013, acc-0.1800, valid loss-2.1022, acc-0.2324, test loss-2.1035, acc-0.2371\n",
      "Iter-3510, train loss-2.1070, acc-0.2600, valid loss-2.1013, acc-0.2336, test loss-2.1027, acc-0.2381\n",
      "Iter-3520, train loss-2.1064, acc-0.2200, valid loss-2.1002, acc-0.2342, test loss-2.1016, acc-0.2382\n",
      "Iter-3530, train loss-2.1108, acc-0.2600, valid loss-2.0991, acc-0.2346, test loss-2.1005, acc-0.2388\n",
      "Iter-3540, train loss-2.0501, acc-0.3600, valid loss-2.0982, acc-0.2350, test loss-2.0996, acc-0.2390\n",
      "Iter-3550, train loss-2.0816, acc-0.2000, valid loss-2.0972, acc-0.2358, test loss-2.0986, acc-0.2392\n",
      "Iter-3560, train loss-2.0840, acc-0.3200, valid loss-2.0962, acc-0.2364, test loss-2.0976, acc-0.2402\n",
      "Iter-3570, train loss-2.1027, acc-0.2400, valid loss-2.0952, acc-0.2368, test loss-2.0966, acc-0.2407\n",
      "Iter-3580, train loss-2.1546, acc-0.2600, valid loss-2.0942, acc-0.2372, test loss-2.0956, acc-0.2410\n",
      "Iter-3590, train loss-2.0805, acc-0.2400, valid loss-2.0932, acc-0.2374, test loss-2.0945, acc-0.2419\n",
      "Iter-3600, train loss-2.0958, acc-0.2800, valid loss-2.0921, acc-0.2378, test loss-2.0935, acc-0.2430\n",
      "Iter-3610, train loss-2.1212, acc-0.2200, valid loss-2.0911, acc-0.2378, test loss-2.0925, acc-0.2437\n",
      "Iter-3620, train loss-2.0112, acc-0.3800, valid loss-2.0901, acc-0.2386, test loss-2.0915, acc-0.2440\n",
      "Iter-3630, train loss-2.0821, acc-0.2200, valid loss-2.0891, acc-0.2392, test loss-2.0905, acc-0.2451\n",
      "Iter-3640, train loss-2.1176, acc-0.1800, valid loss-2.0880, acc-0.2404, test loss-2.0894, acc-0.2459\n",
      "Iter-3650, train loss-2.1150, acc-0.2800, valid loss-2.0871, acc-0.2408, test loss-2.0884, acc-0.2461\n",
      "Iter-3660, train loss-2.0565, acc-0.3000, valid loss-2.0860, acc-0.2412, test loss-2.0874, acc-0.2469\n",
      "Iter-3670, train loss-2.1049, acc-0.2200, valid loss-2.0849, acc-0.2420, test loss-2.0863, acc-0.2483\n",
      "Iter-3680, train loss-2.1260, acc-0.2200, valid loss-2.0840, acc-0.2420, test loss-2.0854, acc-0.2487\n",
      "Iter-3690, train loss-2.0423, acc-0.3200, valid loss-2.0831, acc-0.2426, test loss-2.0845, acc-0.2495\n",
      "Iter-3700, train loss-2.1111, acc-0.1800, valid loss-2.0821, acc-0.2430, test loss-2.0835, acc-0.2501\n",
      "Iter-3710, train loss-2.0910, acc-0.2400, valid loss-2.0811, acc-0.2444, test loss-2.0825, acc-0.2508\n",
      "Iter-3720, train loss-2.0603, acc-0.2800, valid loss-2.0802, acc-0.2460, test loss-2.0816, acc-0.2520\n",
      "Iter-3730, train loss-2.0689, acc-0.1600, valid loss-2.0791, acc-0.2466, test loss-2.0806, acc-0.2528\n",
      "Iter-3740, train loss-2.0595, acc-0.2800, valid loss-2.0782, acc-0.2478, test loss-2.0796, acc-0.2536\n",
      "Iter-3750, train loss-2.1519, acc-0.1200, valid loss-2.0773, acc-0.2482, test loss-2.0787, acc-0.2538\n",
      "Iter-3760, train loss-2.0823, acc-0.2400, valid loss-2.0762, acc-0.2490, test loss-2.0776, acc-0.2545\n",
      "Iter-3770, train loss-2.0701, acc-0.2200, valid loss-2.0752, acc-0.2494, test loss-2.0766, acc-0.2549\n",
      "Iter-3780, train loss-2.0277, acc-0.3000, valid loss-2.0741, acc-0.2504, test loss-2.0756, acc-0.2551\n",
      "Iter-3790, train loss-2.0152, acc-0.3400, valid loss-2.0732, acc-0.2514, test loss-2.0746, acc-0.2554\n",
      "Iter-3800, train loss-2.1032, acc-0.2400, valid loss-2.0721, acc-0.2516, test loss-2.0736, acc-0.2559\n",
      "Iter-3810, train loss-2.0270, acc-0.3000, valid loss-2.0711, acc-0.2522, test loss-2.0725, acc-0.2570\n",
      "Iter-3820, train loss-2.0734, acc-0.2800, valid loss-2.0700, acc-0.2532, test loss-2.0715, acc-0.2580\n",
      "Iter-3830, train loss-2.1057, acc-0.2000, valid loss-2.0689, acc-0.2532, test loss-2.0704, acc-0.2585\n",
      "Iter-3840, train loss-2.1136, acc-0.1600, valid loss-2.0679, acc-0.2542, test loss-2.0694, acc-0.2592\n",
      "Iter-3850, train loss-2.0781, acc-0.2200, valid loss-2.0669, acc-0.2552, test loss-2.0684, acc-0.2597\n",
      "Iter-3860, train loss-2.0609, acc-0.3600, valid loss-2.0660, acc-0.2574, test loss-2.0675, acc-0.2610\n",
      "Iter-3870, train loss-2.1164, acc-0.3400, valid loss-2.0650, acc-0.2582, test loss-2.0665, acc-0.2623\n",
      "Iter-3880, train loss-2.0586, acc-0.2800, valid loss-2.0641, acc-0.2582, test loss-2.0656, acc-0.2629\n",
      "Iter-3890, train loss-2.0118, acc-0.3400, valid loss-2.0631, acc-0.2586, test loss-2.0646, acc-0.2635\n",
      "Iter-3900, train loss-2.0625, acc-0.2600, valid loss-2.0621, acc-0.2596, test loss-2.0636, acc-0.2640\n",
      "Iter-3910, train loss-2.0969, acc-0.2400, valid loss-2.0611, acc-0.2612, test loss-2.0626, acc-0.2648\n",
      "Iter-3920, train loss-2.0638, acc-0.3400, valid loss-2.0602, acc-0.2622, test loss-2.0617, acc-0.2661\n",
      "Iter-3930, train loss-2.0539, acc-0.3000, valid loss-2.0593, acc-0.2642, test loss-2.0608, acc-0.2665\n",
      "Iter-3940, train loss-2.0605, acc-0.2200, valid loss-2.0583, acc-0.2654, test loss-2.0598, acc-0.2675\n",
      "Iter-3950, train loss-2.1002, acc-0.2200, valid loss-2.0573, acc-0.2658, test loss-2.0588, acc-0.2678\n",
      "Iter-3960, train loss-2.0174, acc-0.3600, valid loss-2.0563, acc-0.2660, test loss-2.0578, acc-0.2681\n",
      "Iter-3970, train loss-2.0583, acc-0.1800, valid loss-2.0552, acc-0.2656, test loss-2.0567, acc-0.2686\n",
      "Iter-3980, train loss-2.0234, acc-0.3200, valid loss-2.0542, acc-0.2672, test loss-2.0557, acc-0.2690\n",
      "Iter-3990, train loss-2.0563, acc-0.2400, valid loss-2.0533, acc-0.2678, test loss-2.0548, acc-0.2699\n",
      "Iter-4000, train loss-2.0805, acc-0.2200, valid loss-2.0523, acc-0.2690, test loss-2.0538, acc-0.2712\n",
      "Iter-4010, train loss-2.0599, acc-0.3000, valid loss-2.0512, acc-0.2696, test loss-2.0527, acc-0.2714\n",
      "Iter-4020, train loss-2.0746, acc-0.1800, valid loss-2.0501, acc-0.2694, test loss-2.0516, acc-0.2723\n",
      "Iter-4030, train loss-2.0645, acc-0.3000, valid loss-2.0490, acc-0.2704, test loss-2.0505, acc-0.2729\n",
      "Iter-4040, train loss-2.0932, acc-0.2400, valid loss-2.0480, acc-0.2720, test loss-2.0495, acc-0.2732\n",
      "Iter-4050, train loss-2.0352, acc-0.3200, valid loss-2.0470, acc-0.2726, test loss-2.0485, acc-0.2738\n",
      "Iter-4060, train loss-1.9566, acc-0.4000, valid loss-2.0458, acc-0.2734, test loss-2.0474, acc-0.2745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.0721, acc-0.2200, valid loss-2.0449, acc-0.2736, test loss-2.0465, acc-0.2751\n",
      "Iter-4080, train loss-2.1122, acc-0.2400, valid loss-2.0439, acc-0.2746, test loss-2.0454, acc-0.2757\n",
      "Iter-4090, train loss-1.9987, acc-0.2800, valid loss-2.0429, acc-0.2754, test loss-2.0445, acc-0.2766\n",
      "Iter-4100, train loss-1.9993, acc-0.3000, valid loss-2.0419, acc-0.2760, test loss-2.0434, acc-0.2779\n",
      "Iter-4110, train loss-2.0509, acc-0.3000, valid loss-2.0409, acc-0.2766, test loss-2.0424, acc-0.2787\n",
      "Iter-4120, train loss-2.1080, acc-0.2200, valid loss-2.0399, acc-0.2778, test loss-2.0415, acc-0.2792\n",
      "Iter-4130, train loss-2.0941, acc-0.2000, valid loss-2.0388, acc-0.2784, test loss-2.0404, acc-0.2798\n",
      "Iter-4140, train loss-2.0081, acc-0.2200, valid loss-2.0378, acc-0.2788, test loss-2.0394, acc-0.2806\n",
      "Iter-4150, train loss-2.1064, acc-0.2400, valid loss-2.0367, acc-0.2786, test loss-2.0383, acc-0.2807\n",
      "Iter-4160, train loss-2.0484, acc-0.2600, valid loss-2.0358, acc-0.2798, test loss-2.0374, acc-0.2817\n",
      "Iter-4170, train loss-2.1069, acc-0.1800, valid loss-2.0348, acc-0.2804, test loss-2.0364, acc-0.2825\n",
      "Iter-4180, train loss-2.0404, acc-0.3200, valid loss-2.0339, acc-0.2820, test loss-2.0355, acc-0.2831\n",
      "Iter-4190, train loss-2.0113, acc-0.3600, valid loss-2.0329, acc-0.2826, test loss-2.0345, acc-0.2837\n",
      "Iter-4200, train loss-2.0119, acc-0.3800, valid loss-2.0320, acc-0.2840, test loss-2.0336, acc-0.2848\n",
      "Iter-4210, train loss-2.0484, acc-0.3000, valid loss-2.0310, acc-0.2850, test loss-2.0326, acc-0.2855\n",
      "Iter-4220, train loss-2.0900, acc-0.2600, valid loss-2.0300, acc-0.2862, test loss-2.0316, acc-0.2867\n",
      "Iter-4230, train loss-1.9740, acc-0.3200, valid loss-2.0290, acc-0.2874, test loss-2.0306, acc-0.2875\n",
      "Iter-4240, train loss-1.9950, acc-0.3000, valid loss-2.0280, acc-0.2878, test loss-2.0296, acc-0.2883\n",
      "Iter-4250, train loss-2.0217, acc-0.3200, valid loss-2.0269, acc-0.2886, test loss-2.0286, acc-0.2895\n",
      "Iter-4260, train loss-2.0277, acc-0.3000, valid loss-2.0259, acc-0.2884, test loss-2.0275, acc-0.2904\n",
      "Iter-4270, train loss-2.0687, acc-0.2800, valid loss-2.0249, acc-0.2886, test loss-2.0266, acc-0.2909\n",
      "Iter-4280, train loss-2.0470, acc-0.3000, valid loss-2.0240, acc-0.2898, test loss-2.0256, acc-0.2919\n",
      "Iter-4290, train loss-2.0421, acc-0.3000, valid loss-2.0230, acc-0.2912, test loss-2.0247, acc-0.2930\n",
      "Iter-4300, train loss-2.0285, acc-0.2600, valid loss-2.0220, acc-0.2926, test loss-2.0237, acc-0.2935\n",
      "Iter-4310, train loss-2.0471, acc-0.2800, valid loss-2.0210, acc-0.2930, test loss-2.0226, acc-0.2936\n",
      "Iter-4320, train loss-1.9888, acc-0.3800, valid loss-2.0199, acc-0.2932, test loss-2.0215, acc-0.2942\n",
      "Iter-4330, train loss-2.0853, acc-0.3200, valid loss-2.0189, acc-0.2946, test loss-2.0206, acc-0.2953\n",
      "Iter-4340, train loss-1.9780, acc-0.4000, valid loss-2.0180, acc-0.2958, test loss-2.0196, acc-0.2966\n",
      "Iter-4350, train loss-1.9980, acc-0.3600, valid loss-2.0171, acc-0.2964, test loss-2.0187, acc-0.2972\n",
      "Iter-4360, train loss-1.9334, acc-0.5000, valid loss-2.0160, acc-0.2980, test loss-2.0177, acc-0.2991\n",
      "Iter-4370, train loss-2.0174, acc-0.3400, valid loss-2.0150, acc-0.2984, test loss-2.0167, acc-0.3000\n",
      "Iter-4380, train loss-2.0238, acc-0.2400, valid loss-2.0139, acc-0.2992, test loss-2.0156, acc-0.3005\n",
      "Iter-4390, train loss-2.0245, acc-0.2600, valid loss-2.0130, acc-0.3000, test loss-2.0146, acc-0.3018\n",
      "Iter-4400, train loss-2.0196, acc-0.3200, valid loss-2.0120, acc-0.3012, test loss-2.0137, acc-0.3025\n",
      "Iter-4410, train loss-2.0818, acc-0.2000, valid loss-2.0110, acc-0.3016, test loss-2.0126, acc-0.3028\n",
      "Iter-4420, train loss-1.9872, acc-0.3200, valid loss-2.0100, acc-0.3022, test loss-2.0117, acc-0.3036\n",
      "Iter-4430, train loss-2.0988, acc-0.1400, valid loss-2.0091, acc-0.3038, test loss-2.0107, acc-0.3043\n",
      "Iter-4440, train loss-2.0593, acc-0.3400, valid loss-2.0080, acc-0.3042, test loss-2.0097, acc-0.3046\n",
      "Iter-4450, train loss-2.0192, acc-0.2400, valid loss-2.0070, acc-0.3056, test loss-2.0087, acc-0.3053\n",
      "Iter-4460, train loss-2.0335, acc-0.1800, valid loss-2.0061, acc-0.3062, test loss-2.0078, acc-0.3054\n",
      "Iter-4470, train loss-1.9790, acc-0.3600, valid loss-2.0051, acc-0.3076, test loss-2.0068, acc-0.3059\n",
      "Iter-4480, train loss-2.0635, acc-0.2000, valid loss-2.0041, acc-0.3094, test loss-2.0058, acc-0.3075\n",
      "Iter-4490, train loss-2.0423, acc-0.2800, valid loss-2.0030, acc-0.3104, test loss-2.0048, acc-0.3080\n",
      "Iter-4500, train loss-1.9834, acc-0.2800, valid loss-2.0020, acc-0.3108, test loss-2.0037, acc-0.3083\n",
      "Iter-4510, train loss-2.0390, acc-0.2200, valid loss-2.0010, acc-0.3118, test loss-2.0027, acc-0.3085\n",
      "Iter-4520, train loss-1.9449, acc-0.3600, valid loss-2.0000, acc-0.3124, test loss-2.0017, acc-0.3092\n",
      "Iter-4530, train loss-1.9755, acc-0.2800, valid loss-1.9990, acc-0.3126, test loss-2.0007, acc-0.3096\n",
      "Iter-4540, train loss-2.0042, acc-0.2200, valid loss-1.9979, acc-0.3130, test loss-1.9996, acc-0.3109\n",
      "Iter-4550, train loss-1.9721, acc-0.2600, valid loss-1.9969, acc-0.3132, test loss-1.9986, acc-0.3121\n",
      "Iter-4560, train loss-2.0111, acc-0.2600, valid loss-1.9960, acc-0.3138, test loss-1.9977, acc-0.3123\n",
      "Iter-4570, train loss-1.9983, acc-0.3200, valid loss-1.9949, acc-0.3144, test loss-1.9967, acc-0.3135\n",
      "Iter-4580, train loss-2.0240, acc-0.3200, valid loss-1.9939, acc-0.3142, test loss-1.9956, acc-0.3142\n",
      "Iter-4590, train loss-2.0022, acc-0.2400, valid loss-1.9929, acc-0.3158, test loss-1.9946, acc-0.3147\n",
      "Iter-4600, train loss-2.0248, acc-0.3400, valid loss-1.9919, acc-0.3164, test loss-1.9937, acc-0.3162\n",
      "Iter-4610, train loss-2.0046, acc-0.3600, valid loss-1.9909, acc-0.3178, test loss-1.9927, acc-0.3170\n",
      "Iter-4620, train loss-2.0058, acc-0.2600, valid loss-1.9899, acc-0.3196, test loss-1.9917, acc-0.3181\n",
      "Iter-4630, train loss-1.9828, acc-0.4400, valid loss-1.9889, acc-0.3200, test loss-1.9907, acc-0.3184\n",
      "Iter-4640, train loss-2.0212, acc-0.2800, valid loss-1.9880, acc-0.3212, test loss-1.9898, acc-0.3193\n",
      "Iter-4650, train loss-1.9339, acc-0.2400, valid loss-1.9871, acc-0.3214, test loss-1.9889, acc-0.3198\n",
      "Iter-4660, train loss-1.9787, acc-0.3600, valid loss-1.9860, acc-0.3216, test loss-1.9878, acc-0.3206\n",
      "Iter-4670, train loss-1.9504, acc-0.3800, valid loss-1.9850, acc-0.3228, test loss-1.9868, acc-0.3207\n",
      "Iter-4680, train loss-2.0635, acc-0.2400, valid loss-1.9841, acc-0.3238, test loss-1.9859, acc-0.3213\n",
      "Iter-4690, train loss-2.0008, acc-0.2200, valid loss-1.9831, acc-0.3246, test loss-1.9849, acc-0.3215\n",
      "Iter-4700, train loss-1.9810, acc-0.2400, valid loss-1.9821, acc-0.3252, test loss-1.9839, acc-0.3221\n",
      "Iter-4710, train loss-1.9841, acc-0.3800, valid loss-1.9811, acc-0.3266, test loss-1.9829, acc-0.3230\n",
      "Iter-4720, train loss-2.0086, acc-0.3200, valid loss-1.9800, acc-0.3274, test loss-1.9818, acc-0.3244\n",
      "Iter-4730, train loss-1.9758, acc-0.3400, valid loss-1.9791, acc-0.3274, test loss-1.9809, acc-0.3243\n",
      "Iter-4740, train loss-1.9949, acc-0.3400, valid loss-1.9782, acc-0.3278, test loss-1.9800, acc-0.3246\n",
      "Iter-4750, train loss-1.9683, acc-0.3000, valid loss-1.9772, acc-0.3286, test loss-1.9790, acc-0.3257\n",
      "Iter-4760, train loss-1.9850, acc-0.3800, valid loss-1.9761, acc-0.3296, test loss-1.9780, acc-0.3263\n",
      "Iter-4770, train loss-1.9941, acc-0.3200, valid loss-1.9751, acc-0.3298, test loss-1.9770, acc-0.3272\n",
      "Iter-4780, train loss-1.9372, acc-0.3800, valid loss-1.9741, acc-0.3306, test loss-1.9760, acc-0.3280\n",
      "Iter-4790, train loss-1.9828, acc-0.2400, valid loss-1.9732, acc-0.3314, test loss-1.9750, acc-0.3286\n",
      "Iter-4800, train loss-1.9604, acc-0.3600, valid loss-1.9721, acc-0.3322, test loss-1.9740, acc-0.3301\n",
      "Iter-4810, train loss-1.9720, acc-0.2800, valid loss-1.9711, acc-0.3332, test loss-1.9730, acc-0.3304\n",
      "Iter-4820, train loss-1.9150, acc-0.3800, valid loss-1.9702, acc-0.3336, test loss-1.9720, acc-0.3308\n",
      "Iter-4830, train loss-1.9290, acc-0.4200, valid loss-1.9692, acc-0.3342, test loss-1.9711, acc-0.3314\n",
      "Iter-4840, train loss-1.9341, acc-0.4200, valid loss-1.9682, acc-0.3356, test loss-1.9701, acc-0.3326\n",
      "Iter-4850, train loss-1.9685, acc-0.3400, valid loss-1.9673, acc-0.3354, test loss-1.9692, acc-0.3326\n",
      "Iter-4860, train loss-1.9957, acc-0.3600, valid loss-1.9663, acc-0.3370, test loss-1.9682, acc-0.3339\n",
      "Iter-4870, train loss-2.0166, acc-0.2600, valid loss-1.9653, acc-0.3372, test loss-1.9672, acc-0.3349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-1.9987, acc-0.2800, valid loss-1.9643, acc-0.3382, test loss-1.9662, acc-0.3359\n",
      "Iter-4890, train loss-1.9740, acc-0.3200, valid loss-1.9634, acc-0.3390, test loss-1.9653, acc-0.3369\n",
      "Iter-4900, train loss-1.9737, acc-0.3800, valid loss-1.9624, acc-0.3400, test loss-1.9643, acc-0.3383\n",
      "Iter-4910, train loss-1.8979, acc-0.4000, valid loss-1.9614, acc-0.3406, test loss-1.9633, acc-0.3387\n",
      "Iter-4920, train loss-1.8962, acc-0.3600, valid loss-1.9604, acc-0.3416, test loss-1.9623, acc-0.3394\n",
      "Iter-4930, train loss-1.9848, acc-0.2800, valid loss-1.9595, acc-0.3422, test loss-1.9614, acc-0.3399\n",
      "Iter-4940, train loss-1.8875, acc-0.4400, valid loss-1.9585, acc-0.3426, test loss-1.9604, acc-0.3405\n",
      "Iter-4950, train loss-1.9478, acc-0.4400, valid loss-1.9575, acc-0.3432, test loss-1.9594, acc-0.3408\n",
      "Iter-4960, train loss-1.9724, acc-0.2600, valid loss-1.9565, acc-0.3438, test loss-1.9584, acc-0.3421\n",
      "Iter-4970, train loss-1.9182, acc-0.2800, valid loss-1.9556, acc-0.3446, test loss-1.9575, acc-0.3426\n",
      "Iter-4980, train loss-1.9302, acc-0.2600, valid loss-1.9546, acc-0.3458, test loss-1.9565, acc-0.3435\n",
      "Iter-4990, train loss-1.8939, acc-0.4000, valid loss-1.9536, acc-0.3462, test loss-1.9555, acc-0.3450\n",
      "Iter-5000, train loss-1.8919, acc-0.3800, valid loss-1.9526, acc-0.3476, test loss-1.9546, acc-0.3459\n",
      "Iter-5010, train loss-1.9185, acc-0.4200, valid loss-1.9516, acc-0.3484, test loss-1.9536, acc-0.3464\n",
      "Iter-5020, train loss-2.0324, acc-0.2000, valid loss-1.9507, acc-0.3504, test loss-1.9527, acc-0.3478\n",
      "Iter-5030, train loss-1.9651, acc-0.4200, valid loss-1.9499, acc-0.3516, test loss-1.9518, acc-0.3494\n",
      "Iter-5040, train loss-1.9196, acc-0.2800, valid loss-1.9490, acc-0.3516, test loss-1.9509, acc-0.3494\n",
      "Iter-5050, train loss-1.9229, acc-0.3200, valid loss-1.9480, acc-0.3528, test loss-1.9500, acc-0.3501\n",
      "Iter-5060, train loss-1.8833, acc-0.3600, valid loss-1.9470, acc-0.3534, test loss-1.9489, acc-0.3507\n",
      "Iter-5070, train loss-2.0133, acc-0.3000, valid loss-1.9459, acc-0.3540, test loss-1.9479, acc-0.3508\n",
      "Iter-5080, train loss-1.9591, acc-0.3000, valid loss-1.9449, acc-0.3544, test loss-1.9469, acc-0.3513\n",
      "Iter-5090, train loss-1.9133, acc-0.4400, valid loss-1.9439, acc-0.3546, test loss-1.9459, acc-0.3527\n",
      "Iter-5100, train loss-1.9284, acc-0.3200, valid loss-1.9429, acc-0.3548, test loss-1.9449, acc-0.3532\n",
      "Iter-5110, train loss-1.9746, acc-0.3200, valid loss-1.9419, acc-0.3562, test loss-1.9439, acc-0.3539\n",
      "Iter-5120, train loss-1.9480, acc-0.3000, valid loss-1.9409, acc-0.3582, test loss-1.9429, acc-0.3547\n",
      "Iter-5130, train loss-1.9982, acc-0.3400, valid loss-1.9400, acc-0.3580, test loss-1.9421, acc-0.3546\n",
      "Iter-5140, train loss-1.8603, acc-0.4400, valid loss-1.9390, acc-0.3590, test loss-1.9410, acc-0.3553\n",
      "Iter-5150, train loss-1.8618, acc-0.4200, valid loss-1.9379, acc-0.3590, test loss-1.9400, acc-0.3555\n",
      "Iter-5160, train loss-1.9596, acc-0.3000, valid loss-1.9369, acc-0.3600, test loss-1.9390, acc-0.3562\n",
      "Iter-5170, train loss-2.0075, acc-0.2400, valid loss-1.9360, acc-0.3614, test loss-1.9380, acc-0.3575\n",
      "Iter-5180, train loss-1.9316, acc-0.4800, valid loss-1.9350, acc-0.3624, test loss-1.9371, acc-0.3590\n",
      "Iter-5190, train loss-1.9180, acc-0.3400, valid loss-1.9340, acc-0.3636, test loss-1.9361, acc-0.3592\n",
      "Iter-5200, train loss-1.8821, acc-0.3800, valid loss-1.9331, acc-0.3636, test loss-1.9351, acc-0.3601\n",
      "Iter-5210, train loss-2.0127, acc-0.2800, valid loss-1.9321, acc-0.3642, test loss-1.9342, acc-0.3604\n",
      "Iter-5220, train loss-1.9266, acc-0.3000, valid loss-1.9311, acc-0.3662, test loss-1.9332, acc-0.3611\n",
      "Iter-5230, train loss-1.9325, acc-0.3800, valid loss-1.9301, acc-0.3674, test loss-1.9322, acc-0.3619\n",
      "Iter-5240, train loss-1.9527, acc-0.3200, valid loss-1.9291, acc-0.3686, test loss-1.9312, acc-0.3633\n",
      "Iter-5250, train loss-1.8789, acc-0.4000, valid loss-1.9280, acc-0.3688, test loss-1.9301, acc-0.3637\n",
      "Iter-5260, train loss-1.9863, acc-0.2400, valid loss-1.9271, acc-0.3698, test loss-1.9292, acc-0.3645\n",
      "Iter-5270, train loss-1.9957, acc-0.3400, valid loss-1.9262, acc-0.3702, test loss-1.9283, acc-0.3654\n",
      "Iter-5280, train loss-1.8702, acc-0.4600, valid loss-1.9253, acc-0.3704, test loss-1.9274, acc-0.3669\n",
      "Iter-5290, train loss-1.9701, acc-0.4400, valid loss-1.9244, acc-0.3706, test loss-1.9264, acc-0.3671\n",
      "Iter-5300, train loss-1.9467, acc-0.4200, valid loss-1.9234, acc-0.3714, test loss-1.9255, acc-0.3675\n",
      "Iter-5310, train loss-1.8789, acc-0.4400, valid loss-1.9225, acc-0.3730, test loss-1.9245, acc-0.3682\n",
      "Iter-5320, train loss-1.9146, acc-0.3800, valid loss-1.9215, acc-0.3742, test loss-1.9235, acc-0.3686\n",
      "Iter-5330, train loss-1.9565, acc-0.2800, valid loss-1.9206, acc-0.3746, test loss-1.9227, acc-0.3693\n",
      "Iter-5340, train loss-1.9924, acc-0.3600, valid loss-1.9197, acc-0.3746, test loss-1.9218, acc-0.3695\n",
      "Iter-5350, train loss-1.9195, acc-0.3800, valid loss-1.9187, acc-0.3746, test loss-1.9208, acc-0.3705\n",
      "Iter-5360, train loss-1.9692, acc-0.3600, valid loss-1.9178, acc-0.3752, test loss-1.9199, acc-0.3709\n",
      "Iter-5370, train loss-1.9121, acc-0.4000, valid loss-1.9168, acc-0.3762, test loss-1.9189, acc-0.3718\n",
      "Iter-5380, train loss-1.9630, acc-0.3200, valid loss-1.9159, acc-0.3766, test loss-1.9180, acc-0.3721\n",
      "Iter-5390, train loss-1.9649, acc-0.3200, valid loss-1.9150, acc-0.3780, test loss-1.9171, acc-0.3724\n",
      "Iter-5400, train loss-1.9471, acc-0.3400, valid loss-1.9140, acc-0.3792, test loss-1.9161, acc-0.3734\n",
      "Iter-5410, train loss-1.9109, acc-0.3200, valid loss-1.9130, acc-0.3796, test loss-1.9151, acc-0.3738\n",
      "Iter-5420, train loss-1.8538, acc-0.4400, valid loss-1.9121, acc-0.3798, test loss-1.9142, acc-0.3744\n",
      "Iter-5430, train loss-1.8775, acc-0.3800, valid loss-1.9112, acc-0.3802, test loss-1.9133, acc-0.3750\n",
      "Iter-5440, train loss-1.9454, acc-0.3200, valid loss-1.9103, acc-0.3810, test loss-1.9124, acc-0.3758\n",
      "Iter-5450, train loss-2.0145, acc-0.2800, valid loss-1.9093, acc-0.3818, test loss-1.9115, acc-0.3764\n",
      "Iter-5460, train loss-1.8368, acc-0.4800, valid loss-1.9083, acc-0.3818, test loss-1.9104, acc-0.3769\n",
      "Iter-5470, train loss-1.8268, acc-0.4800, valid loss-1.9074, acc-0.3818, test loss-1.9095, acc-0.3774\n",
      "Iter-5480, train loss-1.8843, acc-0.3200, valid loss-1.9064, acc-0.3826, test loss-1.9085, acc-0.3780\n",
      "Iter-5490, train loss-1.9510, acc-0.3800, valid loss-1.9055, acc-0.3834, test loss-1.9076, acc-0.3789\n",
      "Iter-5500, train loss-1.9463, acc-0.3000, valid loss-1.9046, acc-0.3834, test loss-1.9067, acc-0.3803\n",
      "Iter-5510, train loss-1.9301, acc-0.3600, valid loss-1.9036, acc-0.3850, test loss-1.9057, acc-0.3811\n",
      "Iter-5520, train loss-1.8528, acc-0.4200, valid loss-1.9026, acc-0.3856, test loss-1.9048, acc-0.3812\n",
      "Iter-5530, train loss-1.8885, acc-0.5000, valid loss-1.9016, acc-0.3870, test loss-1.9038, acc-0.3821\n",
      "Iter-5540, train loss-1.9396, acc-0.3600, valid loss-1.9007, acc-0.3872, test loss-1.9029, acc-0.3821\n",
      "Iter-5550, train loss-1.9003, acc-0.4200, valid loss-1.8997, acc-0.3878, test loss-1.9019, acc-0.3828\n",
      "Iter-5560, train loss-1.8519, acc-0.4200, valid loss-1.8988, acc-0.3884, test loss-1.9010, acc-0.3833\n",
      "Iter-5570, train loss-1.8767, acc-0.3600, valid loss-1.8978, acc-0.3900, test loss-1.9000, acc-0.3840\n",
      "Iter-5580, train loss-1.9082, acc-0.2800, valid loss-1.8969, acc-0.3910, test loss-1.8991, acc-0.3848\n",
      "Iter-5590, train loss-1.9025, acc-0.4200, valid loss-1.8959, acc-0.3914, test loss-1.8981, acc-0.3854\n",
      "Iter-5600, train loss-1.8252, acc-0.4000, valid loss-1.8950, acc-0.3918, test loss-1.8972, acc-0.3855\n",
      "Iter-5610, train loss-1.8643, acc-0.4400, valid loss-1.8941, acc-0.3924, test loss-1.8963, acc-0.3863\n",
      "Iter-5620, train loss-1.9770, acc-0.3600, valid loss-1.8931, acc-0.3924, test loss-1.8953, acc-0.3868\n",
      "Iter-5630, train loss-1.8429, acc-0.4600, valid loss-1.8922, acc-0.3926, test loss-1.8944, acc-0.3876\n",
      "Iter-5640, train loss-1.9601, acc-0.3600, valid loss-1.8912, acc-0.3938, test loss-1.8935, acc-0.3886\n",
      "Iter-5650, train loss-1.9711, acc-0.3200, valid loss-1.8903, acc-0.3944, test loss-1.8926, acc-0.3891\n",
      "Iter-5660, train loss-1.8389, acc-0.4000, valid loss-1.8894, acc-0.3956, test loss-1.8917, acc-0.3900\n",
      "Iter-5670, train loss-1.8985, acc-0.3200, valid loss-1.8885, acc-0.3962, test loss-1.8908, acc-0.3908\n",
      "Iter-5680, train loss-1.8883, acc-0.4200, valid loss-1.8876, acc-0.3964, test loss-1.8899, acc-0.3912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-1.9250, acc-0.3800, valid loss-1.8866, acc-0.3960, test loss-1.8889, acc-0.3919\n",
      "Iter-5700, train loss-1.7753, acc-0.5000, valid loss-1.8856, acc-0.3966, test loss-1.8880, acc-0.3926\n",
      "Iter-5710, train loss-1.8379, acc-0.5000, valid loss-1.8847, acc-0.3978, test loss-1.8870, acc-0.3933\n",
      "Iter-5720, train loss-1.9396, acc-0.3800, valid loss-1.8838, acc-0.3978, test loss-1.8861, acc-0.3937\n",
      "Iter-5730, train loss-1.7966, acc-0.4200, valid loss-1.8828, acc-0.3978, test loss-1.8852, acc-0.3938\n",
      "Iter-5740, train loss-1.8919, acc-0.3000, valid loss-1.8819, acc-0.3982, test loss-1.8842, acc-0.3944\n",
      "Iter-5750, train loss-1.9502, acc-0.2600, valid loss-1.8810, acc-0.3994, test loss-1.8833, acc-0.3954\n",
      "Iter-5760, train loss-1.8859, acc-0.3000, valid loss-1.8801, acc-0.3998, test loss-1.8824, acc-0.3965\n",
      "Iter-5770, train loss-1.8527, acc-0.4200, valid loss-1.8792, acc-0.4008, test loss-1.8815, acc-0.3972\n",
      "Iter-5780, train loss-2.0026, acc-0.2800, valid loss-1.8783, acc-0.4018, test loss-1.8807, acc-0.3983\n",
      "Iter-5790, train loss-1.7903, acc-0.4800, valid loss-1.8773, acc-0.4018, test loss-1.8797, acc-0.3986\n",
      "Iter-5800, train loss-1.8638, acc-0.4800, valid loss-1.8765, acc-0.4022, test loss-1.8788, acc-0.3987\n",
      "Iter-5810, train loss-1.8640, acc-0.3400, valid loss-1.8756, acc-0.4034, test loss-1.8779, acc-0.3995\n",
      "Iter-5820, train loss-1.8900, acc-0.3600, valid loss-1.8747, acc-0.4032, test loss-1.8770, acc-0.4001\n",
      "Iter-5830, train loss-1.8856, acc-0.4000, valid loss-1.8737, acc-0.4042, test loss-1.8761, acc-0.4006\n",
      "Iter-5840, train loss-1.8552, acc-0.4400, valid loss-1.8727, acc-0.4048, test loss-1.8751, acc-0.4015\n",
      "Iter-5850, train loss-1.7592, acc-0.4800, valid loss-1.8718, acc-0.4052, test loss-1.8741, acc-0.4022\n",
      "Iter-5860, train loss-1.8173, acc-0.5000, valid loss-1.8708, acc-0.4062, test loss-1.8731, acc-0.4024\n",
      "Iter-5870, train loss-1.9160, acc-0.3600, valid loss-1.8698, acc-0.4062, test loss-1.8722, acc-0.4030\n",
      "Iter-5880, train loss-1.8902, acc-0.3600, valid loss-1.8689, acc-0.4070, test loss-1.8713, acc-0.4039\n",
      "Iter-5890, train loss-1.9009, acc-0.3800, valid loss-1.8680, acc-0.4072, test loss-1.8703, acc-0.4043\n",
      "Iter-5900, train loss-1.8077, acc-0.4400, valid loss-1.8670, acc-0.4084, test loss-1.8693, acc-0.4054\n",
      "Iter-5910, train loss-1.7994, acc-0.4000, valid loss-1.8660, acc-0.4088, test loss-1.8684, acc-0.4061\n",
      "Iter-5920, train loss-1.8502, acc-0.4400, valid loss-1.8651, acc-0.4092, test loss-1.8675, acc-0.4073\n",
      "Iter-5930, train loss-1.8895, acc-0.2800, valid loss-1.8641, acc-0.4100, test loss-1.8665, acc-0.4073\n",
      "Iter-5940, train loss-1.9530, acc-0.3200, valid loss-1.8632, acc-0.4102, test loss-1.8656, acc-0.4076\n",
      "Iter-5950, train loss-1.9026, acc-0.3600, valid loss-1.8622, acc-0.4106, test loss-1.8646, acc-0.4081\n",
      "Iter-5960, train loss-1.9638, acc-0.3600, valid loss-1.8612, acc-0.4118, test loss-1.8636, acc-0.4090\n",
      "Iter-5970, train loss-1.9761, acc-0.3200, valid loss-1.8603, acc-0.4120, test loss-1.8627, acc-0.4096\n",
      "Iter-5980, train loss-1.8237, acc-0.4200, valid loss-1.8593, acc-0.4130, test loss-1.8618, acc-0.4102\n",
      "Iter-5990, train loss-1.7681, acc-0.4200, valid loss-1.8584, acc-0.4134, test loss-1.8609, acc-0.4108\n",
      "Iter-6000, train loss-1.9288, acc-0.3000, valid loss-1.8575, acc-0.4138, test loss-1.8600, acc-0.4115\n",
      "Iter-6010, train loss-1.8462, acc-0.4000, valid loss-1.8565, acc-0.4154, test loss-1.8590, acc-0.4123\n",
      "Iter-6020, train loss-1.8341, acc-0.4200, valid loss-1.8556, acc-0.4154, test loss-1.8581, acc-0.4123\n",
      "Iter-6030, train loss-1.9805, acc-0.3400, valid loss-1.8548, acc-0.4162, test loss-1.8573, acc-0.4132\n",
      "Iter-6040, train loss-1.9385, acc-0.3800, valid loss-1.8538, acc-0.4164, test loss-1.8563, acc-0.4138\n",
      "Iter-6050, train loss-1.8591, acc-0.3600, valid loss-1.8529, acc-0.4168, test loss-1.8554, acc-0.4142\n",
      "Iter-6060, train loss-1.8591, acc-0.4400, valid loss-1.8520, acc-0.4174, test loss-1.8545, acc-0.4153\n",
      "Iter-6070, train loss-1.8738, acc-0.4000, valid loss-1.8510, acc-0.4184, test loss-1.8536, acc-0.4157\n",
      "Iter-6080, train loss-1.8457, acc-0.3800, valid loss-1.8502, acc-0.4192, test loss-1.8527, acc-0.4164\n",
      "Iter-6090, train loss-1.8700, acc-0.5000, valid loss-1.8493, acc-0.4194, test loss-1.8518, acc-0.4166\n",
      "Iter-6100, train loss-1.8439, acc-0.4400, valid loss-1.8483, acc-0.4200, test loss-1.8508, acc-0.4172\n",
      "Iter-6110, train loss-1.9636, acc-0.2600, valid loss-1.8474, acc-0.4204, test loss-1.8499, acc-0.4181\n",
      "Iter-6120, train loss-1.9331, acc-0.3800, valid loss-1.8464, acc-0.4208, test loss-1.8490, acc-0.4183\n",
      "Iter-6130, train loss-1.9273, acc-0.3000, valid loss-1.8455, acc-0.4210, test loss-1.8481, acc-0.4187\n",
      "Iter-6140, train loss-1.8598, acc-0.3800, valid loss-1.8445, acc-0.4208, test loss-1.8470, acc-0.4196\n",
      "Iter-6150, train loss-1.8183, acc-0.5000, valid loss-1.8436, acc-0.4222, test loss-1.8461, acc-0.4200\n",
      "Iter-6160, train loss-1.8578, acc-0.4000, valid loss-1.8426, acc-0.4224, test loss-1.8451, acc-0.4205\n",
      "Iter-6170, train loss-1.8939, acc-0.4600, valid loss-1.8417, acc-0.4238, test loss-1.8442, acc-0.4206\n",
      "Iter-6180, train loss-1.7724, acc-0.4600, valid loss-1.8406, acc-0.4236, test loss-1.8432, acc-0.4210\n",
      "Iter-6190, train loss-1.7921, acc-0.5200, valid loss-1.8397, acc-0.4242, test loss-1.8423, acc-0.4218\n",
      "Iter-6200, train loss-2.0250, acc-0.2200, valid loss-1.8388, acc-0.4250, test loss-1.8414, acc-0.4220\n",
      "Iter-6210, train loss-1.9482, acc-0.3800, valid loss-1.8379, acc-0.4262, test loss-1.8405, acc-0.4225\n",
      "Iter-6220, train loss-1.9591, acc-0.3200, valid loss-1.8369, acc-0.4266, test loss-1.8395, acc-0.4231\n",
      "Iter-6230, train loss-1.8194, acc-0.4600, valid loss-1.8360, acc-0.4274, test loss-1.8386, acc-0.4232\n",
      "Iter-6240, train loss-1.8639, acc-0.4800, valid loss-1.8351, acc-0.4280, test loss-1.8376, acc-0.4236\n",
      "Iter-6250, train loss-1.8461, acc-0.3400, valid loss-1.8341, acc-0.4284, test loss-1.8367, acc-0.4241\n",
      "Iter-6260, train loss-1.9510, acc-0.2800, valid loss-1.8332, acc-0.4284, test loss-1.8357, acc-0.4247\n",
      "Iter-6270, train loss-1.7524, acc-0.5200, valid loss-1.8322, acc-0.4288, test loss-1.8348, acc-0.4258\n",
      "Iter-6280, train loss-1.8189, acc-0.3800, valid loss-1.8314, acc-0.4306, test loss-1.8339, acc-0.4268\n",
      "Iter-6290, train loss-1.7499, acc-0.6200, valid loss-1.8304, acc-0.4310, test loss-1.8329, acc-0.4276\n",
      "Iter-6300, train loss-1.9643, acc-0.3200, valid loss-1.8294, acc-0.4308, test loss-1.8320, acc-0.4284\n",
      "Iter-6310, train loss-1.7731, acc-0.5200, valid loss-1.8284, acc-0.4310, test loss-1.8311, acc-0.4283\n",
      "Iter-6320, train loss-1.6394, acc-0.5600, valid loss-1.8275, acc-0.4314, test loss-1.8301, acc-0.4288\n",
      "Iter-6330, train loss-1.8199, acc-0.4400, valid loss-1.8266, acc-0.4322, test loss-1.8292, acc-0.4291\n",
      "Iter-6340, train loss-1.8268, acc-0.4600, valid loss-1.8256, acc-0.4332, test loss-1.8283, acc-0.4292\n",
      "Iter-6350, train loss-1.8145, acc-0.3600, valid loss-1.8247, acc-0.4330, test loss-1.8274, acc-0.4298\n",
      "Iter-6360, train loss-1.8148, acc-0.3600, valid loss-1.8237, acc-0.4332, test loss-1.8264, acc-0.4299\n",
      "Iter-6370, train loss-1.7563, acc-0.4600, valid loss-1.8228, acc-0.4334, test loss-1.8254, acc-0.4301\n",
      "Iter-6380, train loss-1.7601, acc-0.4600, valid loss-1.8218, acc-0.4346, test loss-1.8245, acc-0.4303\n",
      "Iter-6390, train loss-1.8971, acc-0.3800, valid loss-1.8208, acc-0.4346, test loss-1.8235, acc-0.4306\n",
      "Iter-6400, train loss-1.6124, acc-0.4600, valid loss-1.8199, acc-0.4342, test loss-1.8226, acc-0.4307\n",
      "Iter-6410, train loss-1.9070, acc-0.4600, valid loss-1.8190, acc-0.4344, test loss-1.8217, acc-0.4311\n",
      "Iter-6420, train loss-1.8933, acc-0.3200, valid loss-1.8181, acc-0.4346, test loss-1.8208, acc-0.4312\n",
      "Iter-6430, train loss-1.8786, acc-0.4200, valid loss-1.8172, acc-0.4350, test loss-1.8199, acc-0.4323\n",
      "Iter-6440, train loss-1.9612, acc-0.3000, valid loss-1.8163, acc-0.4352, test loss-1.8190, acc-0.4328\n",
      "Iter-6450, train loss-1.8353, acc-0.4200, valid loss-1.8154, acc-0.4354, test loss-1.8181, acc-0.4329\n",
      "Iter-6460, train loss-1.7864, acc-0.4600, valid loss-1.8144, acc-0.4360, test loss-1.8171, acc-0.4333\n",
      "Iter-6470, train loss-1.7829, acc-0.4200, valid loss-1.8134, acc-0.4364, test loss-1.8162, acc-0.4334\n",
      "Iter-6480, train loss-1.9509, acc-0.3400, valid loss-1.8125, acc-0.4364, test loss-1.8153, acc-0.4341\n",
      "Iter-6490, train loss-1.7239, acc-0.5000, valid loss-1.8115, acc-0.4366, test loss-1.8143, acc-0.4343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-1.9421, acc-0.3200, valid loss-1.8106, acc-0.4368, test loss-1.8133, acc-0.4345\n",
      "Iter-6510, train loss-1.7503, acc-0.4600, valid loss-1.8097, acc-0.4372, test loss-1.8124, acc-0.4348\n",
      "Iter-6520, train loss-1.7317, acc-0.4000, valid loss-1.8088, acc-0.4382, test loss-1.8115, acc-0.4354\n",
      "Iter-6530, train loss-1.7116, acc-0.4200, valid loss-1.8078, acc-0.4380, test loss-1.8106, acc-0.4357\n",
      "Iter-6540, train loss-1.7965, acc-0.4600, valid loss-1.8069, acc-0.4392, test loss-1.8097, acc-0.4360\n",
      "Iter-6550, train loss-1.8256, acc-0.4200, valid loss-1.8059, acc-0.4398, test loss-1.8087, acc-0.4365\n",
      "Iter-6560, train loss-1.8169, acc-0.4400, valid loss-1.8050, acc-0.4402, test loss-1.8078, acc-0.4374\n",
      "Iter-6570, train loss-1.8609, acc-0.3800, valid loss-1.8041, acc-0.4404, test loss-1.8069, acc-0.4377\n",
      "Iter-6580, train loss-1.7917, acc-0.4600, valid loss-1.8032, acc-0.4410, test loss-1.8060, acc-0.4387\n",
      "Iter-6590, train loss-1.7705, acc-0.5200, valid loss-1.8023, acc-0.4404, test loss-1.8051, acc-0.4383\n",
      "Iter-6600, train loss-1.7356, acc-0.4800, valid loss-1.8013, acc-0.4414, test loss-1.8041, acc-0.4391\n",
      "Iter-6610, train loss-1.7495, acc-0.4800, valid loss-1.8003, acc-0.4416, test loss-1.8031, acc-0.4393\n",
      "Iter-6620, train loss-1.8159, acc-0.4200, valid loss-1.7995, acc-0.4426, test loss-1.8023, acc-0.4395\n",
      "Iter-6630, train loss-1.6874, acc-0.5600, valid loss-1.7984, acc-0.4422, test loss-1.8013, acc-0.4401\n",
      "Iter-6640, train loss-1.8207, acc-0.4800, valid loss-1.7975, acc-0.4426, test loss-1.8004, acc-0.4403\n",
      "Iter-6650, train loss-1.8533, acc-0.4000, valid loss-1.7966, acc-0.4430, test loss-1.7995, acc-0.4409\n",
      "Iter-6660, train loss-1.8382, acc-0.4400, valid loss-1.7957, acc-0.4432, test loss-1.7986, acc-0.4411\n",
      "Iter-6670, train loss-1.7826, acc-0.4600, valid loss-1.7948, acc-0.4442, test loss-1.7977, acc-0.4413\n",
      "Iter-6680, train loss-1.8062, acc-0.4400, valid loss-1.7938, acc-0.4452, test loss-1.7968, acc-0.4420\n",
      "Iter-6690, train loss-1.7151, acc-0.4800, valid loss-1.7929, acc-0.4454, test loss-1.7958, acc-0.4427\n",
      "Iter-6700, train loss-1.7336, acc-0.5000, valid loss-1.7920, acc-0.4470, test loss-1.7949, acc-0.4434\n",
      "Iter-6710, train loss-1.7033, acc-0.6200, valid loss-1.7910, acc-0.4470, test loss-1.7940, acc-0.4431\n",
      "Iter-6720, train loss-1.7498, acc-0.5000, valid loss-1.7900, acc-0.4472, test loss-1.7930, acc-0.4436\n",
      "Iter-6730, train loss-1.8025, acc-0.3400, valid loss-1.7891, acc-0.4480, test loss-1.7920, acc-0.4441\n",
      "Iter-6740, train loss-1.7863, acc-0.5000, valid loss-1.7882, acc-0.4480, test loss-1.7911, acc-0.4448\n",
      "Iter-6750, train loss-1.8572, acc-0.4000, valid loss-1.7872, acc-0.4482, test loss-1.7902, acc-0.4452\n",
      "Iter-6760, train loss-1.7378, acc-0.4600, valid loss-1.7862, acc-0.4488, test loss-1.7892, acc-0.4457\n",
      "Iter-6770, train loss-1.7826, acc-0.4600, valid loss-1.7853, acc-0.4486, test loss-1.7883, acc-0.4460\n",
      "Iter-6780, train loss-1.7972, acc-0.4600, valid loss-1.7844, acc-0.4492, test loss-1.7874, acc-0.4461\n",
      "Iter-6790, train loss-1.7974, acc-0.4400, valid loss-1.7835, acc-0.4500, test loss-1.7865, acc-0.4468\n",
      "Iter-6800, train loss-1.7865, acc-0.3800, valid loss-1.7825, acc-0.4504, test loss-1.7855, acc-0.4471\n",
      "Iter-6810, train loss-1.8237, acc-0.4200, valid loss-1.7816, acc-0.4508, test loss-1.7846, acc-0.4475\n",
      "Iter-6820, train loss-1.8023, acc-0.4200, valid loss-1.7806, acc-0.4510, test loss-1.7837, acc-0.4481\n",
      "Iter-6830, train loss-1.7575, acc-0.4600, valid loss-1.7796, acc-0.4512, test loss-1.7827, acc-0.4487\n",
      "Iter-6840, train loss-1.8149, acc-0.3600, valid loss-1.7787, acc-0.4518, test loss-1.7818, acc-0.4493\n",
      "Iter-6850, train loss-1.7538, acc-0.4400, valid loss-1.7777, acc-0.4526, test loss-1.7808, acc-0.4498\n",
      "Iter-6860, train loss-1.8095, acc-0.4600, valid loss-1.7768, acc-0.4528, test loss-1.7798, acc-0.4503\n",
      "Iter-6870, train loss-1.8591, acc-0.4200, valid loss-1.7758, acc-0.4532, test loss-1.7789, acc-0.4508\n",
      "Iter-6880, train loss-1.8356, acc-0.4400, valid loss-1.7748, acc-0.4532, test loss-1.7779, acc-0.4513\n",
      "Iter-6890, train loss-1.8773, acc-0.3800, valid loss-1.7739, acc-0.4534, test loss-1.7770, acc-0.4520\n",
      "Iter-6900, train loss-1.8010, acc-0.4200, valid loss-1.7730, acc-0.4538, test loss-1.7761, acc-0.4519\n",
      "Iter-6910, train loss-1.7028, acc-0.4800, valid loss-1.7721, acc-0.4540, test loss-1.7752, acc-0.4524\n",
      "Iter-6920, train loss-1.7945, acc-0.4000, valid loss-1.7712, acc-0.4544, test loss-1.7743, acc-0.4529\n",
      "Iter-6930, train loss-1.7708, acc-0.3800, valid loss-1.7703, acc-0.4536, test loss-1.7734, acc-0.4530\n",
      "Iter-6940, train loss-1.8215, acc-0.4000, valid loss-1.7694, acc-0.4540, test loss-1.7725, acc-0.4533\n",
      "Iter-6950, train loss-1.7628, acc-0.4400, valid loss-1.7685, acc-0.4548, test loss-1.7716, acc-0.4536\n",
      "Iter-6960, train loss-1.7541, acc-0.4800, valid loss-1.7675, acc-0.4544, test loss-1.7706, acc-0.4538\n",
      "Iter-6970, train loss-1.7523, acc-0.5200, valid loss-1.7665, acc-0.4534, test loss-1.7697, acc-0.4540\n",
      "Iter-6980, train loss-1.8701, acc-0.5000, valid loss-1.7656, acc-0.4536, test loss-1.7688, acc-0.4544\n",
      "Iter-6990, train loss-1.7581, acc-0.3800, valid loss-1.7647, acc-0.4536, test loss-1.7678, acc-0.4543\n",
      "Iter-7000, train loss-1.8194, acc-0.4600, valid loss-1.7638, acc-0.4540, test loss-1.7669, acc-0.4552\n",
      "Iter-7010, train loss-1.8973, acc-0.4200, valid loss-1.7627, acc-0.4540, test loss-1.7659, acc-0.4557\n",
      "Iter-7020, train loss-1.7930, acc-0.4400, valid loss-1.7619, acc-0.4542, test loss-1.7650, acc-0.4560\n",
      "Iter-7030, train loss-1.8354, acc-0.4400, valid loss-1.7609, acc-0.4546, test loss-1.7641, acc-0.4564\n",
      "Iter-7040, train loss-1.7384, acc-0.4600, valid loss-1.7599, acc-0.4548, test loss-1.7631, acc-0.4566\n",
      "Iter-7050, train loss-1.7439, acc-0.4600, valid loss-1.7589, acc-0.4542, test loss-1.7622, acc-0.4566\n",
      "Iter-7060, train loss-1.7617, acc-0.5600, valid loss-1.7581, acc-0.4546, test loss-1.7613, acc-0.4579\n",
      "Iter-7070, train loss-1.6737, acc-0.5800, valid loss-1.7571, acc-0.4548, test loss-1.7604, acc-0.4576\n",
      "Iter-7080, train loss-1.6698, acc-0.4800, valid loss-1.7562, acc-0.4550, test loss-1.7594, acc-0.4581\n",
      "Iter-7090, train loss-1.7418, acc-0.5200, valid loss-1.7553, acc-0.4560, test loss-1.7585, acc-0.4583\n",
      "Iter-7100, train loss-1.6947, acc-0.4800, valid loss-1.7544, acc-0.4560, test loss-1.7576, acc-0.4595\n",
      "Iter-7110, train loss-1.6573, acc-0.5000, valid loss-1.7534, acc-0.4564, test loss-1.7567, acc-0.4594\n",
      "Iter-7120, train loss-1.7370, acc-0.5000, valid loss-1.7525, acc-0.4564, test loss-1.7557, acc-0.4598\n",
      "Iter-7130, train loss-1.8152, acc-0.4200, valid loss-1.7516, acc-0.4580, test loss-1.7548, acc-0.4605\n",
      "Iter-7140, train loss-1.8274, acc-0.4400, valid loss-1.7507, acc-0.4578, test loss-1.7540, acc-0.4611\n",
      "Iter-7150, train loss-1.7784, acc-0.4600, valid loss-1.7497, acc-0.4576, test loss-1.7530, acc-0.4617\n",
      "Iter-7160, train loss-1.8255, acc-0.3800, valid loss-1.7487, acc-0.4580, test loss-1.7521, acc-0.4624\n",
      "Iter-7170, train loss-1.6534, acc-0.5600, valid loss-1.7478, acc-0.4584, test loss-1.7511, acc-0.4630\n",
      "Iter-7180, train loss-1.7660, acc-0.4000, valid loss-1.7470, acc-0.4586, test loss-1.7503, acc-0.4633\n",
      "Iter-7190, train loss-1.7838, acc-0.4800, valid loss-1.7460, acc-0.4588, test loss-1.7493, acc-0.4637\n",
      "Iter-7200, train loss-1.7428, acc-0.4400, valid loss-1.7451, acc-0.4590, test loss-1.7484, acc-0.4642\n",
      "Iter-7210, train loss-1.7109, acc-0.4400, valid loss-1.7441, acc-0.4592, test loss-1.7474, acc-0.4643\n",
      "Iter-7220, train loss-1.6590, acc-0.5000, valid loss-1.7432, acc-0.4592, test loss-1.7465, acc-0.4650\n",
      "Iter-7230, train loss-1.6227, acc-0.5200, valid loss-1.7423, acc-0.4598, test loss-1.7457, acc-0.4651\n",
      "Iter-7240, train loss-1.7610, acc-0.4200, valid loss-1.7414, acc-0.4600, test loss-1.7448, acc-0.4657\n",
      "Iter-7250, train loss-1.7470, acc-0.4800, valid loss-1.7405, acc-0.4600, test loss-1.7438, acc-0.4665\n",
      "Iter-7260, train loss-1.7609, acc-0.4600, valid loss-1.7395, acc-0.4610, test loss-1.7429, acc-0.4671\n",
      "Iter-7270, train loss-1.7265, acc-0.5000, valid loss-1.7386, acc-0.4622, test loss-1.7420, acc-0.4675\n",
      "Iter-7280, train loss-1.8174, acc-0.4600, valid loss-1.7376, acc-0.4626, test loss-1.7410, acc-0.4679\n",
      "Iter-7290, train loss-1.6662, acc-0.4200, valid loss-1.7366, acc-0.4630, test loss-1.7401, acc-0.4677\n",
      "Iter-7300, train loss-1.7852, acc-0.4200, valid loss-1.7357, acc-0.4626, test loss-1.7392, acc-0.4679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-1.7051, acc-0.4000, valid loss-1.7347, acc-0.4630, test loss-1.7382, acc-0.4681\n",
      "Iter-7320, train loss-1.7514, acc-0.4800, valid loss-1.7338, acc-0.4630, test loss-1.7373, acc-0.4689\n",
      "Iter-7330, train loss-1.8333, acc-0.4400, valid loss-1.7329, acc-0.4636, test loss-1.7363, acc-0.4695\n",
      "Iter-7340, train loss-1.8604, acc-0.2800, valid loss-1.7319, acc-0.4642, test loss-1.7354, acc-0.4697\n",
      "Iter-7350, train loss-1.6941, acc-0.4400, valid loss-1.7310, acc-0.4644, test loss-1.7345, acc-0.4699\n",
      "Iter-7360, train loss-1.6651, acc-0.5200, valid loss-1.7301, acc-0.4648, test loss-1.7336, acc-0.4703\n",
      "Iter-7370, train loss-1.6145, acc-0.5400, valid loss-1.7291, acc-0.4646, test loss-1.7327, acc-0.4706\n",
      "Iter-7380, train loss-1.7275, acc-0.4800, valid loss-1.7281, acc-0.4644, test loss-1.7317, acc-0.4709\n",
      "Iter-7390, train loss-1.7846, acc-0.5200, valid loss-1.7272, acc-0.4644, test loss-1.7308, acc-0.4714\n",
      "Iter-7400, train loss-1.6977, acc-0.3800, valid loss-1.7263, acc-0.4644, test loss-1.7299, acc-0.4720\n",
      "Iter-7410, train loss-1.7873, acc-0.4800, valid loss-1.7254, acc-0.4652, test loss-1.7289, acc-0.4725\n",
      "Iter-7420, train loss-1.7156, acc-0.6200, valid loss-1.7244, acc-0.4654, test loss-1.7280, acc-0.4727\n",
      "Iter-7430, train loss-1.8166, acc-0.3600, valid loss-1.7234, acc-0.4658, test loss-1.7270, acc-0.4732\n",
      "Iter-7440, train loss-1.6922, acc-0.5200, valid loss-1.7225, acc-0.4658, test loss-1.7261, acc-0.4737\n",
      "Iter-7450, train loss-1.7919, acc-0.4200, valid loss-1.7215, acc-0.4660, test loss-1.7252, acc-0.4740\n",
      "Iter-7460, train loss-1.6990, acc-0.4800, valid loss-1.7206, acc-0.4660, test loss-1.7242, acc-0.4745\n",
      "Iter-7470, train loss-1.6932, acc-0.4200, valid loss-1.7196, acc-0.4666, test loss-1.7232, acc-0.4749\n",
      "Iter-7480, train loss-1.7976, acc-0.3600, valid loss-1.7187, acc-0.4674, test loss-1.7224, acc-0.4750\n",
      "Iter-7490, train loss-1.7185, acc-0.4400, valid loss-1.7178, acc-0.4674, test loss-1.7214, acc-0.4752\n",
      "Iter-7500, train loss-1.7077, acc-0.4800, valid loss-1.7169, acc-0.4680, test loss-1.7205, acc-0.4750\n",
      "Iter-7510, train loss-1.7408, acc-0.4400, valid loss-1.7160, acc-0.4678, test loss-1.7196, acc-0.4751\n",
      "Iter-7520, train loss-1.8116, acc-0.3600, valid loss-1.7150, acc-0.4680, test loss-1.7186, acc-0.4753\n",
      "Iter-7530, train loss-1.7478, acc-0.4600, valid loss-1.7141, acc-0.4682, test loss-1.7178, acc-0.4759\n",
      "Iter-7540, train loss-1.6302, acc-0.5000, valid loss-1.7132, acc-0.4678, test loss-1.7168, acc-0.4763\n",
      "Iter-7550, train loss-1.6797, acc-0.5000, valid loss-1.7122, acc-0.4676, test loss-1.7159, acc-0.4767\n",
      "Iter-7560, train loss-1.8041, acc-0.3800, valid loss-1.7113, acc-0.4682, test loss-1.7149, acc-0.4772\n",
      "Iter-7570, train loss-1.7008, acc-0.4400, valid loss-1.7104, acc-0.4680, test loss-1.7140, acc-0.4776\n",
      "Iter-7580, train loss-1.6544, acc-0.6000, valid loss-1.7093, acc-0.4680, test loss-1.7131, acc-0.4778\n",
      "Iter-7590, train loss-1.6295, acc-0.5800, valid loss-1.7084, acc-0.4686, test loss-1.7121, acc-0.4785\n",
      "Iter-7600, train loss-1.6695, acc-0.5200, valid loss-1.7074, acc-0.4690, test loss-1.7111, acc-0.4791\n",
      "Iter-7610, train loss-1.6647, acc-0.5000, valid loss-1.7065, acc-0.4692, test loss-1.7102, acc-0.4798\n",
      "Iter-7620, train loss-1.7153, acc-0.5000, valid loss-1.7056, acc-0.4692, test loss-1.7093, acc-0.4805\n",
      "Iter-7630, train loss-1.5718, acc-0.5600, valid loss-1.7046, acc-0.4692, test loss-1.7084, acc-0.4810\n",
      "Iter-7640, train loss-1.6269, acc-0.5400, valid loss-1.7037, acc-0.4690, test loss-1.7075, acc-0.4811\n",
      "Iter-7650, train loss-1.7030, acc-0.4400, valid loss-1.7027, acc-0.4688, test loss-1.7065, acc-0.4817\n",
      "Iter-7660, train loss-1.7068, acc-0.5000, valid loss-1.7017, acc-0.4686, test loss-1.7056, acc-0.4817\n",
      "Iter-7670, train loss-1.6758, acc-0.5200, valid loss-1.7008, acc-0.4694, test loss-1.7046, acc-0.4824\n",
      "Iter-7680, train loss-1.6505, acc-0.6000, valid loss-1.6998, acc-0.4696, test loss-1.7037, acc-0.4827\n",
      "Iter-7690, train loss-1.8569, acc-0.3200, valid loss-1.6989, acc-0.4706, test loss-1.7028, acc-0.4835\n",
      "Iter-7700, train loss-1.7553, acc-0.4600, valid loss-1.6980, acc-0.4710, test loss-1.7019, acc-0.4839\n",
      "Iter-7710, train loss-1.7769, acc-0.4400, valid loss-1.6971, acc-0.4704, test loss-1.7010, acc-0.4836\n",
      "Iter-7720, train loss-1.7145, acc-0.4400, valid loss-1.6962, acc-0.4714, test loss-1.7002, acc-0.4844\n",
      "Iter-7730, train loss-1.7277, acc-0.4600, valid loss-1.6953, acc-0.4712, test loss-1.6993, acc-0.4848\n",
      "Iter-7740, train loss-1.7818, acc-0.4400, valid loss-1.6944, acc-0.4714, test loss-1.6984, acc-0.4852\n",
      "Iter-7750, train loss-1.6875, acc-0.4000, valid loss-1.6935, acc-0.4716, test loss-1.6975, acc-0.4853\n",
      "Iter-7760, train loss-1.7020, acc-0.5000, valid loss-1.6926, acc-0.4724, test loss-1.6966, acc-0.4857\n",
      "Iter-7770, train loss-1.5879, acc-0.5600, valid loss-1.6916, acc-0.4728, test loss-1.6956, acc-0.4861\n",
      "Iter-7780, train loss-1.7198, acc-0.5000, valid loss-1.6906, acc-0.4734, test loss-1.6946, acc-0.4866\n",
      "Iter-7790, train loss-1.8020, acc-0.4800, valid loss-1.6898, acc-0.4738, test loss-1.6938, acc-0.4868\n",
      "Iter-7800, train loss-1.7527, acc-0.4800, valid loss-1.6889, acc-0.4736, test loss-1.6929, acc-0.4868\n",
      "Iter-7810, train loss-1.6455, acc-0.4800, valid loss-1.6879, acc-0.4744, test loss-1.6920, acc-0.4869\n",
      "Iter-7820, train loss-1.8173, acc-0.4000, valid loss-1.6870, acc-0.4754, test loss-1.6910, acc-0.4880\n",
      "Iter-7830, train loss-1.6192, acc-0.5400, valid loss-1.6861, acc-0.4760, test loss-1.6901, acc-0.4885\n",
      "Iter-7840, train loss-1.6749, acc-0.5600, valid loss-1.6851, acc-0.4764, test loss-1.6892, acc-0.4887\n",
      "Iter-7850, train loss-1.8326, acc-0.4000, valid loss-1.6842, acc-0.4768, test loss-1.6883, acc-0.4891\n",
      "Iter-7860, train loss-1.7771, acc-0.3800, valid loss-1.6833, acc-0.4778, test loss-1.6874, acc-0.4898\n",
      "Iter-7870, train loss-1.7074, acc-0.4800, valid loss-1.6825, acc-0.4782, test loss-1.6865, acc-0.4902\n",
      "Iter-7880, train loss-1.6152, acc-0.5600, valid loss-1.6815, acc-0.4788, test loss-1.6856, acc-0.4897\n",
      "Iter-7890, train loss-1.7009, acc-0.4600, valid loss-1.6806, acc-0.4788, test loss-1.6847, acc-0.4900\n",
      "Iter-7900, train loss-1.7419, acc-0.5400, valid loss-1.6797, acc-0.4788, test loss-1.6838, acc-0.4900\n",
      "Iter-7910, train loss-1.6257, acc-0.5400, valid loss-1.6788, acc-0.4798, test loss-1.6829, acc-0.4904\n",
      "Iter-7920, train loss-1.7004, acc-0.5600, valid loss-1.6778, acc-0.4802, test loss-1.6820, acc-0.4905\n",
      "Iter-7930, train loss-1.6291, acc-0.5000, valid loss-1.6769, acc-0.4804, test loss-1.6811, acc-0.4907\n",
      "Iter-7940, train loss-1.8526, acc-0.3600, valid loss-1.6760, acc-0.4806, test loss-1.6801, acc-0.4909\n",
      "Iter-7950, train loss-1.6857, acc-0.4600, valid loss-1.6750, acc-0.4800, test loss-1.6792, acc-0.4913\n",
      "Iter-7960, train loss-1.7728, acc-0.3200, valid loss-1.6741, acc-0.4802, test loss-1.6783, acc-0.4916\n",
      "Iter-7970, train loss-1.6980, acc-0.4000, valid loss-1.6732, acc-0.4798, test loss-1.6774, acc-0.4922\n",
      "Iter-7980, train loss-1.5617, acc-0.5200, valid loss-1.6722, acc-0.4796, test loss-1.6764, acc-0.4922\n",
      "Iter-7990, train loss-1.7001, acc-0.5600, valid loss-1.6712, acc-0.4798, test loss-1.6754, acc-0.4925\n",
      "Iter-8000, train loss-1.6847, acc-0.5600, valid loss-1.6703, acc-0.4800, test loss-1.6745, acc-0.4927\n",
      "Iter-8010, train loss-1.7000, acc-0.5000, valid loss-1.6693, acc-0.4796, test loss-1.6736, acc-0.4930\n",
      "Iter-8020, train loss-1.5779, acc-0.6200, valid loss-1.6683, acc-0.4806, test loss-1.6727, acc-0.4934\n",
      "Iter-8030, train loss-1.6417, acc-0.5200, valid loss-1.6674, acc-0.4808, test loss-1.6718, acc-0.4939\n",
      "Iter-8040, train loss-1.6522, acc-0.4600, valid loss-1.6664, acc-0.4816, test loss-1.6708, acc-0.4945\n",
      "Iter-8050, train loss-1.5943, acc-0.6200, valid loss-1.6655, acc-0.4810, test loss-1.6698, acc-0.4946\n",
      "Iter-8060, train loss-1.5534, acc-0.5600, valid loss-1.6644, acc-0.4822, test loss-1.6688, acc-0.4949\n",
      "Iter-8070, train loss-1.7744, acc-0.4000, valid loss-1.6635, acc-0.4824, test loss-1.6680, acc-0.4953\n",
      "Iter-8080, train loss-1.7891, acc-0.4400, valid loss-1.6626, acc-0.4832, test loss-1.6671, acc-0.4958\n",
      "Iter-8090, train loss-1.5676, acc-0.5800, valid loss-1.6617, acc-0.4828, test loss-1.6662, acc-0.4963\n",
      "Iter-8100, train loss-1.7394, acc-0.4000, valid loss-1.6608, acc-0.4836, test loss-1.6652, acc-0.4965\n",
      "Iter-8110, train loss-1.6999, acc-0.3800, valid loss-1.6599, acc-0.4840, test loss-1.6643, acc-0.4969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-1.5241, acc-0.6000, valid loss-1.6589, acc-0.4836, test loss-1.6633, acc-0.4973\n",
      "Iter-8130, train loss-1.7382, acc-0.4600, valid loss-1.6581, acc-0.4842, test loss-1.6625, acc-0.4974\n",
      "Iter-8140, train loss-1.7716, acc-0.3000, valid loss-1.6572, acc-0.4842, test loss-1.6616, acc-0.4983\n",
      "Iter-8150, train loss-1.6846, acc-0.4200, valid loss-1.6563, acc-0.4846, test loss-1.6607, acc-0.4987\n",
      "Iter-8160, train loss-1.6201, acc-0.4200, valid loss-1.6554, acc-0.4848, test loss-1.6598, acc-0.4990\n",
      "Iter-8170, train loss-1.6687, acc-0.5200, valid loss-1.6545, acc-0.4852, test loss-1.6589, acc-0.4996\n",
      "Iter-8180, train loss-1.6269, acc-0.5400, valid loss-1.6536, acc-0.4856, test loss-1.6580, acc-0.4996\n",
      "Iter-8190, train loss-1.5595, acc-0.5200, valid loss-1.6527, acc-0.4856, test loss-1.6571, acc-0.4998\n",
      "Iter-8200, train loss-1.6377, acc-0.5000, valid loss-1.6518, acc-0.4858, test loss-1.6562, acc-0.4996\n",
      "Iter-8210, train loss-1.4573, acc-0.6000, valid loss-1.6508, acc-0.4858, test loss-1.6553, acc-0.5003\n",
      "Iter-8220, train loss-1.5745, acc-0.5200, valid loss-1.6499, acc-0.4868, test loss-1.6544, acc-0.5004\n",
      "Iter-8230, train loss-1.7832, acc-0.4400, valid loss-1.6490, acc-0.4868, test loss-1.6535, acc-0.5010\n",
      "Iter-8240, train loss-1.5488, acc-0.5400, valid loss-1.6481, acc-0.4872, test loss-1.6526, acc-0.5013\n",
      "Iter-8250, train loss-1.8064, acc-0.3600, valid loss-1.6472, acc-0.4870, test loss-1.6517, acc-0.5014\n",
      "Iter-8260, train loss-1.7169, acc-0.4000, valid loss-1.6463, acc-0.4876, test loss-1.6508, acc-0.5018\n",
      "Iter-8270, train loss-1.5925, acc-0.5800, valid loss-1.6454, acc-0.4882, test loss-1.6499, acc-0.5026\n",
      "Iter-8280, train loss-1.7501, acc-0.4000, valid loss-1.6445, acc-0.4892, test loss-1.6490, acc-0.5034\n",
      "Iter-8290, train loss-1.6827, acc-0.4200, valid loss-1.6435, acc-0.4902, test loss-1.6481, acc-0.5039\n",
      "Iter-8300, train loss-1.6796, acc-0.4400, valid loss-1.6426, acc-0.4902, test loss-1.6472, acc-0.5042\n",
      "Iter-8310, train loss-1.6170, acc-0.4400, valid loss-1.6417, acc-0.4902, test loss-1.6463, acc-0.5042\n",
      "Iter-8320, train loss-1.7425, acc-0.3600, valid loss-1.6409, acc-0.4908, test loss-1.6455, acc-0.5049\n",
      "Iter-8330, train loss-1.8352, acc-0.3800, valid loss-1.6399, acc-0.4906, test loss-1.6445, acc-0.5054\n",
      "Iter-8340, train loss-1.5670, acc-0.5000, valid loss-1.6391, acc-0.4912, test loss-1.6437, acc-0.5055\n",
      "Iter-8350, train loss-1.6500, acc-0.4400, valid loss-1.6382, acc-0.4912, test loss-1.6428, acc-0.5059\n",
      "Iter-8360, train loss-1.5689, acc-0.5400, valid loss-1.6373, acc-0.4924, test loss-1.6419, acc-0.5065\n",
      "Iter-8370, train loss-1.5746, acc-0.5000, valid loss-1.6363, acc-0.4918, test loss-1.6409, acc-0.5069\n",
      "Iter-8380, train loss-1.7878, acc-0.4600, valid loss-1.6353, acc-0.4924, test loss-1.6400, acc-0.5072\n",
      "Iter-8390, train loss-1.6308, acc-0.5200, valid loss-1.6345, acc-0.4934, test loss-1.6391, acc-0.5074\n",
      "Iter-8400, train loss-1.5636, acc-0.5800, valid loss-1.6336, acc-0.4936, test loss-1.6383, acc-0.5075\n",
      "Iter-8410, train loss-1.5850, acc-0.5400, valid loss-1.6327, acc-0.4940, test loss-1.6374, acc-0.5079\n",
      "Iter-8420, train loss-1.7093, acc-0.4000, valid loss-1.6318, acc-0.4938, test loss-1.6365, acc-0.5079\n",
      "Iter-8430, train loss-1.5448, acc-0.5600, valid loss-1.6308, acc-0.4944, test loss-1.6355, acc-0.5082\n",
      "Iter-8440, train loss-1.5807, acc-0.5800, valid loss-1.6299, acc-0.4952, test loss-1.6346, acc-0.5090\n",
      "Iter-8450, train loss-1.4824, acc-0.5800, valid loss-1.6290, acc-0.4954, test loss-1.6337, acc-0.5091\n",
      "Iter-8460, train loss-1.5993, acc-0.5600, valid loss-1.6281, acc-0.4956, test loss-1.6328, acc-0.5095\n",
      "Iter-8470, train loss-1.6131, acc-0.4800, valid loss-1.6272, acc-0.4962, test loss-1.6319, acc-0.5097\n",
      "Iter-8480, train loss-1.7722, acc-0.3400, valid loss-1.6263, acc-0.4962, test loss-1.6311, acc-0.5102\n",
      "Iter-8490, train loss-1.6403, acc-0.5200, valid loss-1.6255, acc-0.4964, test loss-1.6302, acc-0.5105\n",
      "Iter-8500, train loss-1.5711, acc-0.4600, valid loss-1.6245, acc-0.4972, test loss-1.6293, acc-0.5112\n",
      "Iter-8510, train loss-1.5690, acc-0.5200, valid loss-1.6237, acc-0.4976, test loss-1.6284, acc-0.5118\n",
      "Iter-8520, train loss-1.6178, acc-0.5600, valid loss-1.6227, acc-0.4982, test loss-1.6275, acc-0.5123\n",
      "Iter-8530, train loss-1.5827, acc-0.5600, valid loss-1.6219, acc-0.4984, test loss-1.6266, acc-0.5126\n",
      "Iter-8540, train loss-1.5839, acc-0.4800, valid loss-1.6209, acc-0.4990, test loss-1.6257, acc-0.5128\n",
      "Iter-8550, train loss-1.6484, acc-0.5400, valid loss-1.6200, acc-0.4990, test loss-1.6248, acc-0.5134\n",
      "Iter-8560, train loss-1.4682, acc-0.5400, valid loss-1.6192, acc-0.4992, test loss-1.6239, acc-0.5137\n",
      "Iter-8570, train loss-1.6576, acc-0.4000, valid loss-1.6182, acc-0.4994, test loss-1.6230, acc-0.5139\n",
      "Iter-8580, train loss-1.6730, acc-0.4200, valid loss-1.6173, acc-0.5002, test loss-1.6221, acc-0.5141\n",
      "Iter-8590, train loss-1.7870, acc-0.4200, valid loss-1.6164, acc-0.5004, test loss-1.6212, acc-0.5142\n",
      "Iter-8600, train loss-1.5579, acc-0.5400, valid loss-1.6155, acc-0.5006, test loss-1.6203, acc-0.5145\n",
      "Iter-8610, train loss-1.5453, acc-0.5400, valid loss-1.6147, acc-0.5008, test loss-1.6195, acc-0.5143\n",
      "Iter-8620, train loss-1.6221, acc-0.5200, valid loss-1.6137, acc-0.5002, test loss-1.6185, acc-0.5146\n",
      "Iter-8630, train loss-1.5813, acc-0.5000, valid loss-1.6130, acc-0.5014, test loss-1.6177, acc-0.5150\n",
      "Iter-8640, train loss-1.5904, acc-0.4800, valid loss-1.6121, acc-0.5016, test loss-1.6169, acc-0.5154\n",
      "Iter-8650, train loss-1.6160, acc-0.4800, valid loss-1.6112, acc-0.5014, test loss-1.6160, acc-0.5158\n",
      "Iter-8660, train loss-1.6872, acc-0.4000, valid loss-1.6103, acc-0.5016, test loss-1.6151, acc-0.5160\n",
      "Iter-8670, train loss-1.6597, acc-0.5600, valid loss-1.6095, acc-0.5026, test loss-1.6142, acc-0.5161\n",
      "Iter-8680, train loss-1.6798, acc-0.4400, valid loss-1.6086, acc-0.5026, test loss-1.6134, acc-0.5162\n",
      "Iter-8690, train loss-1.5749, acc-0.5200, valid loss-1.6077, acc-0.5030, test loss-1.6125, acc-0.5166\n",
      "Iter-8700, train loss-1.5511, acc-0.4600, valid loss-1.6068, acc-0.5036, test loss-1.6115, acc-0.5175\n",
      "Iter-8710, train loss-1.7299, acc-0.5000, valid loss-1.6059, acc-0.5054, test loss-1.6106, acc-0.5182\n",
      "Iter-8720, train loss-1.6751, acc-0.4400, valid loss-1.6050, acc-0.5060, test loss-1.6097, acc-0.5189\n",
      "Iter-8730, train loss-1.5720, acc-0.5000, valid loss-1.6041, acc-0.5060, test loss-1.6088, acc-0.5191\n",
      "Iter-8740, train loss-1.6496, acc-0.5000, valid loss-1.6032, acc-0.5066, test loss-1.6079, acc-0.5190\n",
      "Iter-8750, train loss-1.6988, acc-0.4600, valid loss-1.6024, acc-0.5066, test loss-1.6071, acc-0.5194\n",
      "Iter-8760, train loss-1.6705, acc-0.5000, valid loss-1.6015, acc-0.5066, test loss-1.6062, acc-0.5195\n",
      "Iter-8770, train loss-1.5744, acc-0.5400, valid loss-1.6006, acc-0.5072, test loss-1.6054, acc-0.5195\n",
      "Iter-8780, train loss-1.6713, acc-0.4000, valid loss-1.5997, acc-0.5076, test loss-1.6045, acc-0.5193\n",
      "Iter-8790, train loss-1.6532, acc-0.4400, valid loss-1.5988, acc-0.5078, test loss-1.6036, acc-0.5193\n",
      "Iter-8800, train loss-1.5219, acc-0.5400, valid loss-1.5980, acc-0.5082, test loss-1.6027, acc-0.5196\n",
      "Iter-8810, train loss-1.6083, acc-0.4400, valid loss-1.5971, acc-0.5088, test loss-1.6019, acc-0.5203\n",
      "Iter-8820, train loss-1.6330, acc-0.5200, valid loss-1.5962, acc-0.5094, test loss-1.6009, acc-0.5204\n",
      "Iter-8830, train loss-1.5363, acc-0.5200, valid loss-1.5953, acc-0.5106, test loss-1.6000, acc-0.5204\n",
      "Iter-8840, train loss-1.7796, acc-0.4600, valid loss-1.5944, acc-0.5098, test loss-1.5992, acc-0.5207\n",
      "Iter-8850, train loss-1.6105, acc-0.4400, valid loss-1.5935, acc-0.5098, test loss-1.5983, acc-0.5214\n",
      "Iter-8860, train loss-1.6160, acc-0.6000, valid loss-1.5926, acc-0.5102, test loss-1.5974, acc-0.5214\n",
      "Iter-8870, train loss-1.5958, acc-0.4200, valid loss-1.5917, acc-0.5112, test loss-1.5965, acc-0.5218\n",
      "Iter-8880, train loss-1.6009, acc-0.5200, valid loss-1.5907, acc-0.5122, test loss-1.5955, acc-0.5223\n",
      "Iter-8890, train loss-1.6835, acc-0.4800, valid loss-1.5897, acc-0.5124, test loss-1.5945, acc-0.5222\n",
      "Iter-8900, train loss-1.6149, acc-0.5000, valid loss-1.5888, acc-0.5130, test loss-1.5936, acc-0.5225\n",
      "Iter-8910, train loss-1.5731, acc-0.6000, valid loss-1.5879, acc-0.5140, test loss-1.5928, acc-0.5230\n",
      "Iter-8920, train loss-1.5719, acc-0.5400, valid loss-1.5871, acc-0.5144, test loss-1.5919, acc-0.5238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-1.6361, acc-0.5000, valid loss-1.5861, acc-0.5146, test loss-1.5910, acc-0.5237\n",
      "Iter-8940, train loss-1.5024, acc-0.4800, valid loss-1.5852, acc-0.5148, test loss-1.5901, acc-0.5245\n",
      "Iter-8950, train loss-1.4663, acc-0.6000, valid loss-1.5843, acc-0.5150, test loss-1.5892, acc-0.5247\n",
      "Iter-8960, train loss-1.4052, acc-0.6400, valid loss-1.5833, acc-0.5156, test loss-1.5882, acc-0.5249\n",
      "Iter-8970, train loss-1.6316, acc-0.4400, valid loss-1.5825, acc-0.5164, test loss-1.5874, acc-0.5255\n",
      "Iter-8980, train loss-1.5613, acc-0.6000, valid loss-1.5816, acc-0.5162, test loss-1.5865, acc-0.5254\n",
      "Iter-8990, train loss-1.5162, acc-0.5400, valid loss-1.5806, acc-0.5164, test loss-1.5856, acc-0.5258\n",
      "Iter-9000, train loss-1.7861, acc-0.4200, valid loss-1.5797, acc-0.5172, test loss-1.5847, acc-0.5261\n",
      "Iter-9010, train loss-1.5586, acc-0.5600, valid loss-1.5789, acc-0.5180, test loss-1.5838, acc-0.5263\n",
      "Iter-9020, train loss-1.5583, acc-0.5400, valid loss-1.5780, acc-0.5180, test loss-1.5830, acc-0.5264\n",
      "Iter-9030, train loss-1.5594, acc-0.5600, valid loss-1.5771, acc-0.5182, test loss-1.5821, acc-0.5268\n",
      "Iter-9040, train loss-1.5493, acc-0.5200, valid loss-1.5763, acc-0.5194, test loss-1.5813, acc-0.5269\n",
      "Iter-9050, train loss-1.5457, acc-0.6000, valid loss-1.5755, acc-0.5200, test loss-1.5804, acc-0.5277\n",
      "Iter-9060, train loss-1.5852, acc-0.5400, valid loss-1.5745, acc-0.5206, test loss-1.5796, acc-0.5278\n",
      "Iter-9070, train loss-1.6153, acc-0.4800, valid loss-1.5737, acc-0.5210, test loss-1.5787, acc-0.5280\n",
      "Iter-9080, train loss-1.6304, acc-0.5000, valid loss-1.5728, acc-0.5216, test loss-1.5778, acc-0.5288\n",
      "Iter-9090, train loss-1.5134, acc-0.6600, valid loss-1.5718, acc-0.5222, test loss-1.5769, acc-0.5294\n",
      "Iter-9100, train loss-1.6299, acc-0.4600, valid loss-1.5710, acc-0.5224, test loss-1.5760, acc-0.5297\n",
      "Iter-9110, train loss-1.5065, acc-0.4600, valid loss-1.5701, acc-0.5226, test loss-1.5751, acc-0.5295\n",
      "Iter-9120, train loss-1.5258, acc-0.6000, valid loss-1.5692, acc-0.5228, test loss-1.5742, acc-0.5299\n",
      "Iter-9130, train loss-1.5848, acc-0.5200, valid loss-1.5683, acc-0.5238, test loss-1.5734, acc-0.5303\n",
      "Iter-9140, train loss-1.5727, acc-0.5000, valid loss-1.5674, acc-0.5242, test loss-1.5725, acc-0.5311\n",
      "Iter-9150, train loss-1.6083, acc-0.4200, valid loss-1.5665, acc-0.5244, test loss-1.5716, acc-0.5311\n",
      "Iter-9160, train loss-1.6573, acc-0.4000, valid loss-1.5657, acc-0.5244, test loss-1.5707, acc-0.5316\n",
      "Iter-9170, train loss-1.5243, acc-0.5800, valid loss-1.5648, acc-0.5248, test loss-1.5699, acc-0.5320\n",
      "Iter-9180, train loss-1.8315, acc-0.4200, valid loss-1.5640, acc-0.5250, test loss-1.5690, acc-0.5324\n",
      "Iter-9190, train loss-1.6440, acc-0.3800, valid loss-1.5630, acc-0.5252, test loss-1.5681, acc-0.5330\n",
      "Iter-9200, train loss-1.6169, acc-0.4800, valid loss-1.5622, acc-0.5254, test loss-1.5672, acc-0.5334\n",
      "Iter-9210, train loss-1.7657, acc-0.4200, valid loss-1.5613, acc-0.5266, test loss-1.5664, acc-0.5337\n",
      "Iter-9220, train loss-1.6737, acc-0.4200, valid loss-1.5604, acc-0.5264, test loss-1.5655, acc-0.5344\n",
      "Iter-9230, train loss-1.6210, acc-0.5600, valid loss-1.5596, acc-0.5268, test loss-1.5647, acc-0.5345\n",
      "Iter-9240, train loss-1.5346, acc-0.6000, valid loss-1.5587, acc-0.5270, test loss-1.5638, acc-0.5344\n",
      "Iter-9250, train loss-1.6162, acc-0.5800, valid loss-1.5578, acc-0.5276, test loss-1.5629, acc-0.5347\n",
      "Iter-9260, train loss-1.6521, acc-0.5400, valid loss-1.5570, acc-0.5282, test loss-1.5621, acc-0.5351\n",
      "Iter-9270, train loss-1.6757, acc-0.5600, valid loss-1.5562, acc-0.5280, test loss-1.5613, acc-0.5350\n",
      "Iter-9280, train loss-1.5213, acc-0.6000, valid loss-1.5553, acc-0.5282, test loss-1.5604, acc-0.5356\n",
      "Iter-9290, train loss-1.5571, acc-0.5000, valid loss-1.5545, acc-0.5286, test loss-1.5596, acc-0.5360\n",
      "Iter-9300, train loss-1.6404, acc-0.5800, valid loss-1.5536, acc-0.5286, test loss-1.5587, acc-0.5362\n",
      "Iter-9310, train loss-1.5049, acc-0.5800, valid loss-1.5527, acc-0.5290, test loss-1.5578, acc-0.5373\n",
      "Iter-9320, train loss-1.5233, acc-0.6000, valid loss-1.5519, acc-0.5304, test loss-1.5570, acc-0.5372\n",
      "Iter-9330, train loss-1.6152, acc-0.5400, valid loss-1.5511, acc-0.5306, test loss-1.5562, acc-0.5378\n",
      "Iter-9340, train loss-1.5477, acc-0.5400, valid loss-1.5502, acc-0.5318, test loss-1.5553, acc-0.5383\n",
      "Iter-9350, train loss-1.5468, acc-0.5000, valid loss-1.5494, acc-0.5318, test loss-1.5544, acc-0.5389\n",
      "Iter-9360, train loss-1.5873, acc-0.5000, valid loss-1.5485, acc-0.5318, test loss-1.5536, acc-0.5389\n",
      "Iter-9370, train loss-1.5206, acc-0.7200, valid loss-1.5476, acc-0.5322, test loss-1.5527, acc-0.5392\n",
      "Iter-9380, train loss-1.6332, acc-0.5200, valid loss-1.5467, acc-0.5328, test loss-1.5518, acc-0.5401\n",
      "Iter-9390, train loss-1.5185, acc-0.6200, valid loss-1.5458, acc-0.5330, test loss-1.5509, acc-0.5405\n",
      "Iter-9400, train loss-1.5514, acc-0.5000, valid loss-1.5449, acc-0.5332, test loss-1.5500, acc-0.5406\n",
      "Iter-9410, train loss-1.5066, acc-0.6000, valid loss-1.5441, acc-0.5346, test loss-1.5492, acc-0.5413\n",
      "Iter-9420, train loss-1.5867, acc-0.5000, valid loss-1.5432, acc-0.5346, test loss-1.5484, acc-0.5415\n",
      "Iter-9430, train loss-1.6132, acc-0.5800, valid loss-1.5424, acc-0.5350, test loss-1.5475, acc-0.5416\n",
      "Iter-9440, train loss-1.5241, acc-0.5000, valid loss-1.5415, acc-0.5360, test loss-1.5467, acc-0.5427\n",
      "Iter-9450, train loss-1.4618, acc-0.6200, valid loss-1.5406, acc-0.5372, test loss-1.5458, acc-0.5435\n",
      "Iter-9460, train loss-1.5003, acc-0.6000, valid loss-1.5397, acc-0.5374, test loss-1.5449, acc-0.5439\n",
      "Iter-9470, train loss-1.7031, acc-0.5800, valid loss-1.5388, acc-0.5376, test loss-1.5440, acc-0.5443\n",
      "Iter-9480, train loss-1.6194, acc-0.5000, valid loss-1.5379, acc-0.5382, test loss-1.5431, acc-0.5451\n",
      "Iter-9490, train loss-1.5906, acc-0.6200, valid loss-1.5370, acc-0.5386, test loss-1.5423, acc-0.5455\n",
      "Iter-9500, train loss-1.5041, acc-0.6000, valid loss-1.5361, acc-0.5386, test loss-1.5414, acc-0.5457\n",
      "Iter-9510, train loss-1.4553, acc-0.5600, valid loss-1.5353, acc-0.5392, test loss-1.5406, acc-0.5459\n",
      "Iter-9520, train loss-1.4834, acc-0.5400, valid loss-1.5345, acc-0.5396, test loss-1.5397, acc-0.5466\n",
      "Iter-9530, train loss-1.5882, acc-0.6000, valid loss-1.5336, acc-0.5408, test loss-1.5389, acc-0.5467\n",
      "Iter-9540, train loss-1.5715, acc-0.5600, valid loss-1.5328, acc-0.5404, test loss-1.5381, acc-0.5470\n",
      "Iter-9550, train loss-1.5360, acc-0.5600, valid loss-1.5320, acc-0.5408, test loss-1.5373, acc-0.5469\n",
      "Iter-9560, train loss-1.6109, acc-0.6000, valid loss-1.5312, acc-0.5408, test loss-1.5364, acc-0.5470\n",
      "Iter-9570, train loss-1.4947, acc-0.6000, valid loss-1.5303, acc-0.5410, test loss-1.5355, acc-0.5477\n",
      "Iter-9580, train loss-1.4119, acc-0.6000, valid loss-1.5293, acc-0.5416, test loss-1.5346, acc-0.5479\n",
      "Iter-9590, train loss-1.5893, acc-0.4800, valid loss-1.5285, acc-0.5420, test loss-1.5338, acc-0.5483\n",
      "Iter-9600, train loss-1.4811, acc-0.6000, valid loss-1.5277, acc-0.5424, test loss-1.5329, acc-0.5489\n",
      "Iter-9610, train loss-1.3859, acc-0.5600, valid loss-1.5268, acc-0.5426, test loss-1.5321, acc-0.5493\n",
      "Iter-9620, train loss-1.4569, acc-0.6200, valid loss-1.5259, acc-0.5426, test loss-1.5312, acc-0.5491\n",
      "Iter-9630, train loss-1.5468, acc-0.6200, valid loss-1.5251, acc-0.5434, test loss-1.5304, acc-0.5492\n",
      "Iter-9640, train loss-1.5890, acc-0.5400, valid loss-1.5242, acc-0.5440, test loss-1.5295, acc-0.5504\n",
      "Iter-9650, train loss-1.4495, acc-0.6200, valid loss-1.5233, acc-0.5442, test loss-1.5287, acc-0.5505\n",
      "Iter-9660, train loss-1.5435, acc-0.5800, valid loss-1.5224, acc-0.5442, test loss-1.5277, acc-0.5511\n",
      "Iter-9670, train loss-1.5673, acc-0.4400, valid loss-1.5215, acc-0.5446, test loss-1.5268, acc-0.5517\n",
      "Iter-9680, train loss-1.5090, acc-0.5800, valid loss-1.5206, acc-0.5450, test loss-1.5259, acc-0.5515\n",
      "Iter-9690, train loss-1.6723, acc-0.4800, valid loss-1.5197, acc-0.5460, test loss-1.5251, acc-0.5523\n",
      "Iter-9700, train loss-1.5387, acc-0.5200, valid loss-1.5189, acc-0.5462, test loss-1.5242, acc-0.5525\n",
      "Iter-9710, train loss-1.4154, acc-0.6600, valid loss-1.5180, acc-0.5466, test loss-1.5234, acc-0.5528\n",
      "Iter-9720, train loss-1.4871, acc-0.5400, valid loss-1.5172, acc-0.5466, test loss-1.5225, acc-0.5531\n",
      "Iter-9730, train loss-1.4609, acc-0.6200, valid loss-1.5163, acc-0.5468, test loss-1.5217, acc-0.5536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-1.5017, acc-0.6200, valid loss-1.5155, acc-0.5470, test loss-1.5208, acc-0.5542\n",
      "Iter-9750, train loss-1.4119, acc-0.6000, valid loss-1.5147, acc-0.5476, test loss-1.5201, acc-0.5546\n",
      "Iter-9760, train loss-1.5882, acc-0.4200, valid loss-1.5139, acc-0.5480, test loss-1.5193, acc-0.5553\n",
      "Iter-9770, train loss-1.6032, acc-0.4800, valid loss-1.5130, acc-0.5484, test loss-1.5184, acc-0.5561\n",
      "Iter-9780, train loss-1.6584, acc-0.4400, valid loss-1.5122, acc-0.5486, test loss-1.5175, acc-0.5569\n",
      "Iter-9790, train loss-1.7140, acc-0.5200, valid loss-1.5113, acc-0.5488, test loss-1.5167, acc-0.5574\n",
      "Iter-9800, train loss-1.5066, acc-0.5600, valid loss-1.5105, acc-0.5492, test loss-1.5159, acc-0.5574\n",
      "Iter-9810, train loss-1.5919, acc-0.4800, valid loss-1.5096, acc-0.5496, test loss-1.5151, acc-0.5584\n",
      "Iter-9820, train loss-1.4241, acc-0.6600, valid loss-1.5088, acc-0.5498, test loss-1.5142, acc-0.5589\n",
      "Iter-9830, train loss-1.4917, acc-0.5000, valid loss-1.5080, acc-0.5508, test loss-1.5134, acc-0.5603\n",
      "Iter-9840, train loss-1.5763, acc-0.5400, valid loss-1.5071, acc-0.5514, test loss-1.5125, acc-0.5604\n",
      "Iter-9850, train loss-1.5212, acc-0.4800, valid loss-1.5063, acc-0.5516, test loss-1.5117, acc-0.5605\n",
      "Iter-9860, train loss-1.5223, acc-0.4600, valid loss-1.5054, acc-0.5518, test loss-1.5108, acc-0.5611\n",
      "Iter-9870, train loss-1.4570, acc-0.6600, valid loss-1.5046, acc-0.5518, test loss-1.5100, acc-0.5617\n",
      "Iter-9880, train loss-1.5224, acc-0.5800, valid loss-1.5037, acc-0.5520, test loss-1.5091, acc-0.5622\n",
      "Iter-9890, train loss-1.5728, acc-0.5000, valid loss-1.5028, acc-0.5526, test loss-1.5083, acc-0.5627\n",
      "Iter-9900, train loss-1.3741, acc-0.7200, valid loss-1.5020, acc-0.5530, test loss-1.5075, acc-0.5632\n",
      "Iter-9910, train loss-1.5983, acc-0.5800, valid loss-1.5012, acc-0.5538, test loss-1.5066, acc-0.5638\n",
      "Iter-9920, train loss-1.5553, acc-0.5600, valid loss-1.5004, acc-0.5534, test loss-1.5058, acc-0.5643\n",
      "Iter-9930, train loss-1.2989, acc-0.7200, valid loss-1.4995, acc-0.5534, test loss-1.5050, acc-0.5644\n",
      "Iter-9940, train loss-1.5029, acc-0.6000, valid loss-1.4987, acc-0.5542, test loss-1.5042, acc-0.5651\n",
      "Iter-9950, train loss-1.5116, acc-0.5200, valid loss-1.4979, acc-0.5546, test loss-1.5033, acc-0.5649\n",
      "Iter-9960, train loss-1.4861, acc-0.5600, valid loss-1.4971, acc-0.5554, test loss-1.5025, acc-0.5651\n",
      "Iter-9970, train loss-1.3847, acc-0.6200, valid loss-1.4962, acc-0.5554, test loss-1.5017, acc-0.5657\n",
      "Iter-9980, train loss-1.6226, acc-0.4800, valid loss-1.4954, acc-0.5558, test loss-1.5009, acc-0.5662\n",
      "Iter-9990, train loss-1.4364, acc-0.5400, valid loss-1.4946, acc-0.5558, test loss-1.5000, acc-0.5666\n",
      "Iter-10000, train loss-1.5572, acc-0.5000, valid loss-1.4938, acc-0.5564, test loss-1.4992, acc-0.5670\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11890b3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11890b518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
