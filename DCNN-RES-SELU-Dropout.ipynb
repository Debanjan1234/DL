{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = l.conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = self.selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = self.alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            if train:\n",
    "                h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "                dy = self.alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            else:\n",
    "                h_conv_cache, h_nl_cache = h_cache[layer]\n",
    "            dh = self.selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = l.conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8 # constant\n",
    "        smooth_train = 1.0\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            y, cache = self.forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grad = self.backward(dy, cache, train=True)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "            for key in grad[0]:\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for layer in range(self.L):\n",
    "                for key in grad[1][layer]:\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            for key in grad[2]:\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 256 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 50 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = nn.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 training loss: 2.8994 validation accuracy: 0.334800\n",
      "Test Mean accuracy: 0.3278, std: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121f410f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Hyper-parameters\n",
    "# n_iter = 10 # number of epochs\n",
    "# alpha = 1e-4 # learning_rate\n",
    "# mb_size = 256 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "# num_layers = 10 # depth \n",
    "# print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "# num_hidden_units = 10\n",
    "# p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# # build the model/NN and learn it: running session.\n",
    "# nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "# nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "#            n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# y_pred = nn.test(X_test)\n",
    "# accs = np.mean(y_pred == y_test)\n",
    "# print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # # Display the learning curve and losses for training, validation, and testing\n",
    "# # %matplotlib inline\n",
    "# # %config InlineBackend.figure_format = 'retina'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(nn.losses['train'], label='Train loss')\n",
    "# # plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
