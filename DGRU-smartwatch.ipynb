{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: time-serie data from smartwatch or smartwatch data\n",
    "# %matplotlib inline # for plt.show()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data reading\n",
    "# The smartwatch historical/time-seris data to visualize\n",
    "# data_path = 'data/smartwatch_data/experimental_data_analysis/Basis_Watch_Data.csv'\n",
    "# data_path = 'data/financial_data/USD_INR.csv'\n",
    "# data_path = 'data/bike_data/hour.csv'\n",
    "data_path = 'data/smartwatch_data/experimental_data_analysis/Basis_Watch_Data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Data: cleaning\n",
    "# Getting rid of NaN\n",
    "data = data.fillna(value=0.0)\n",
    "\n",
    "# Showing the data file csv or comma separated value\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting the smartwatch data before scaling/batch normalization\n",
    "# data[:10000]['Price'].plot()\n",
    "data[: 300].plot()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = np.array(data)\n",
    "data_array.shape, data_array.dtype\n",
    "data_main = np.array(data_array[:, 1:], dtype=float)\n",
    "data_main.shape, data_main.dtype\n",
    "\n",
    "plt.plot(data_main[:100])\n",
    "plt.show()\n",
    "data_main.shape, data_main.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data_main, axis=0)\n",
    "std = np.std(data_main, axis=0)\n",
    "std.shape, mean.shape, std.dtype, mean.dtype\n",
    "\n",
    "data_norm = (data_main - mean) / std\n",
    "plt.plot(data_norm[:1000])\n",
    "plt.show()\n",
    "data_norm.mean(), data_norm.std(), data_norm.var(), data_norm.shape, data_norm.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_norm[:1000000] # the last dim/variable/feature = 1,011,347 = 1,000,000 ONE million\n",
    "test_data = data_norm[1000000:] # the last dim/variable/feature\n",
    "train_data.shape, test_data.shape\n",
    "X_train = train_data[0:999999] # 999,999\n",
    "Y_train = train_data[1:1000000] # 1000,000\n",
    "X_train.shape, Y_train.shape\n",
    "\n",
    "plt.plot(X_train[:100])\n",
    "plt.plot(Y_train[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = test_data[0:11346]  # 11,347\n",
    "Y_valid = test_data[1:11347]\n",
    "X_valid.shape, Y_valid.shape\n",
    "plt.plot(X_valid[:100])\n",
    "plt.plot(Y_valid[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, p_dropout, lam):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.p_dropout = p_dropout\n",
    "        self.lam = lam\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid': []}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = m\n",
    "        \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, \n",
    "                 y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches, do_caches = [], [], []\n",
    "\n",
    "        for X in X_train:\n",
    "            X = X.reshape(1, -1) # X_1xn\n",
    "            y, h, cache = self.forward(X, h, self.model)\n",
    "            y, do_cache = l.dropout_forward(y, self.p_dropout)\n",
    "            caches.append(cache)\n",
    "            do_caches.append(do_cache)\n",
    "            ys.append(y)\n",
    "        \n",
    "        ys = np.array(ys, dtype=float).reshape(len(ys), -1) # ys_txn instead of ys_tx1xn\n",
    "        \n",
    "        return ys, caches, do_caches\n",
    "                                \n",
    "    def loss_function(self, y_pred, y_train): # , alpha alpha: learning rate\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y, Y in zip(y_pred, y_train):\n",
    "            loss += l2_regression_reg(model=self.model, y_pred=y, y_train=Y, lam=self.lam)\n",
    "            dy = dl2_regression(y_pred=y, y_train=Y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches, do_caches):\n",
    "        dh = np.zeros((1, self.H)) \n",
    "        grad = {key: np.zeros_like(val) for key, val in self.model.items()}\n",
    "        grads= {key: np.zeros_like(val) for key, val in self.model.items()}\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t].reshape(1, -1) # dy_1xn\n",
    "            dy = l.dropout_backward(dy, do_caches[t])\n",
    "            _, dh, grad = self.backward(dy, dh, caches[t])\n",
    "            for key in grad.keys():\n",
    "                grads[key] += grad[key]\n",
    "                \n",
    "        return grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        ys = []\n",
    "        X = X_seed.reshape(1, -1)\n",
    "        for _ in range(size):\n",
    "            y, h, _ = self.forward(X, h, self.model)\n",
    "            X = y.copy() # previous out for the next input for prediction\n",
    "            ys.append(y) # list array\n",
    "        \n",
    "        ys = np.array(ys, dtype=float).reshape(len(ys), -1) # ys_txn instead of ys_tx1xn\n",
    "        return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, XY_train, XY_valid, alpha, mb_size, n_iter, print_after):\n",
    "    X_train, y_train = XY_train\n",
    "    X_valid, y_valid = XY_valid\n",
    "\n",
    "    M= {key: np.zeros_like(val) for key, val in nn.model.items()}\n",
    "    R= {key: np.zeros_like(val) for key, val in nn.model.items()}\n",
    "    \n",
    "    # Learning decay: suggested by Justin Jhonson in Standford\n",
    "    beta1 = .9\n",
    "    beta2 = .99\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            \n",
    "            # Train the model\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches, do_caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_pred=ys, y_train=y_mini) #, alpha=alpha\n",
    "            grads = nn.train_backward(dys, caches, do_caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Update the model\n",
    "            for key in grads.keys(): #key, value: items\n",
    "                M[key] = l.exp_running_avg(M[key], grads[key], beta1)\n",
    "                R[key] = l.exp_running_avg(R[key], grads[key]**2, beta2)\n",
    "                m_k_hat = M[key] / (1. - (beta1** iter))\n",
    "                r_k_hat = R[key] / (1. - (beta2** iter))\n",
    "                nn.model[key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Validate the model (by testing)\n",
    "            ys = nn.test(X_seed=X_valid[0], h=state, size=X_valid.shape[0]) # ys_tx1xn\n",
    "            valid_loss, _ = nn.loss_function(y_pred=ys, y_train=Y_valid) #, alpha=alpha\n",
    "            nn.losses['valid'].append(valid_loss)\n",
    "\n",
    "        # Print the model loss/ error\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{}, train loss: {:.8f}, valid loss: {:.8f}'.format(iter, loss, valid_loss))\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 1024 # minibatch size: 32, 64, 128, or 256 Cache\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate: 1e-3, 5e-4, 1e-4 - default choices\n",
    "print_after = 1 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = X_train.shape[1] # X_txn, shape[0]==t, shape[1]=n\n",
    "keep_prob = 0.95 # p_dropout == keep_prob: keeping neurons/units - default 0.95 to 0.9 based on SELU-Dropout\n",
    "lam = 1e-4 # regularization\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "# def adam_rnn(nn, X_train, y_train, alpha=0.001, mb_size=256, n_iter=2000, print_after=100):\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, p_dropout=keep_prob, lam=lam) #, L=num_layers, p_dropout=p_dropout\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, XY_train=(X_train, Y_train), XY_valid=(X_valid, Y_valid), alpha=alpha, mb_size=time_step,\n",
    "         n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# % matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['valid'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_pred = net.test(X_seed=X_valid[0], h=net.initial_state(), size=X_valid.shape[0]) # ys_tx1xn\n",
    "y_pred.shape, Y_valid.shape\n",
    "\n",
    "plt.plot(y_pred[:300], label='y_pred')\n",
    "plt.plot(Y_valid[:300], label='Y_valid')\n",
    "# plt.plot(X_valid[:100], label='X_valid')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
