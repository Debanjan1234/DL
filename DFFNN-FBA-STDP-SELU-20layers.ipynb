{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, _ = l.selu_forward(X=y)\n",
    "#         if train:\n",
    "#             y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#             caches.append((fc_cache, do_cache)) # caches[0]\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.selu_forward(X=y)\n",
    "#             if train:\n",
    "#                 y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#                 do_caches.append(do_cache)\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "#         if train:\n",
    "#             caches.append((fc_caches, do_caches)) # caches[1]\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "#         if train:\n",
    "#             y, do_cache = l.dropout_forward(X=y, p_dropout=0.95) # poisson\n",
    "#             caches.append((fc_cache, do_cache)) # caches[2]\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        ys, ys_prev = self.ys, self.ys_prev # temporal diff instead of func diff\n",
    "        \n",
    "        # Output layer\n",
    "#         fc_cache, do_cache = caches[2]\n",
    "#         dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "#         fc_caches, do_caches = caches[1]\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.dropout_backward(cache=do_caches[layer], dout=dy)\n",
    "#             dy = l.selu_backward(cache=nl_caches[layer], dout=dy)\n",
    "#             dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # function derivative or dfunc\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "#         fc_cache, do_cache = caches[0]\n",
    "        fc_cache = caches[0]\n",
    "#         dy = l.dropout_backward(cache=do_cache, dout=dy)\n",
    "#         dy = l.selu_backward(cache=nl_cache, dout=dy)\n",
    "#         dy = np.exp(dy) #/ np.exp(dy).sum(axis=1).reshape(-1, 1) # txn\n",
    "        dy *= ys[0] - ys_prev[0]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        # dy = dX.copy()\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "#             print(self.ys[2].shape)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.3803 valid loss: 2.3137, valid accuracy: 0.1200\n",
      "Iter-20 train loss: 2.3191 valid loss: 2.3127, valid accuracy: 0.1214\n",
      "Iter-30 train loss: 2.3454 valid loss: 2.3116, valid accuracy: 0.1216\n",
      "Iter-40 train loss: 2.3218 valid loss: 2.3104, valid accuracy: 0.1220\n",
      "Iter-50 train loss: 2.3571 valid loss: 2.3094, valid accuracy: 0.1234\n",
      "Iter-60 train loss: 2.3384 valid loss: 2.3083, valid accuracy: 0.1240\n",
      "Iter-70 train loss: 2.2718 valid loss: 2.3073, valid accuracy: 0.1248\n",
      "Iter-80 train loss: 2.2633 valid loss: 2.3063, valid accuracy: 0.1248\n",
      "Iter-90 train loss: 2.2816 valid loss: 2.3053, valid accuracy: 0.1258\n",
      "Iter-100 train loss: 2.3391 valid loss: 2.3041, valid accuracy: 0.1280\n",
      "Iter-110 train loss: 2.3070 valid loss: 2.3031, valid accuracy: 0.1292\n",
      "Iter-120 train loss: 2.3578 valid loss: 2.3022, valid accuracy: 0.1302\n",
      "Iter-130 train loss: 2.1923 valid loss: 2.3012, valid accuracy: 0.1306\n",
      "Iter-140 train loss: 2.2815 valid loss: 2.3002, valid accuracy: 0.1308\n",
      "Iter-150 train loss: 2.2947 valid loss: 2.2991, valid accuracy: 0.1308\n",
      "Iter-160 train loss: 2.2667 valid loss: 2.2981, valid accuracy: 0.1314\n",
      "Iter-170 train loss: 2.2735 valid loss: 2.2972, valid accuracy: 0.1320\n",
      "Iter-180 train loss: 2.3051 valid loss: 2.2963, valid accuracy: 0.1324\n",
      "Iter-190 train loss: 2.2849 valid loss: 2.2953, valid accuracy: 0.1332\n",
      "Iter-200 train loss: 2.3480 valid loss: 2.2943, valid accuracy: 0.1336\n",
      "Iter-210 train loss: 2.3327 valid loss: 2.2934, valid accuracy: 0.1340\n",
      "Iter-220 train loss: 2.2050 valid loss: 2.2924, valid accuracy: 0.1350\n",
      "Iter-230 train loss: 2.2730 valid loss: 2.2914, valid accuracy: 0.1364\n",
      "Iter-240 train loss: 2.3159 valid loss: 2.2905, valid accuracy: 0.1368\n",
      "Iter-250 train loss: 2.2873 valid loss: 2.2894, valid accuracy: 0.1374\n",
      "Iter-260 train loss: 2.1829 valid loss: 2.2884, valid accuracy: 0.1384\n",
      "Iter-270 train loss: 2.2963 valid loss: 2.2875, valid accuracy: 0.1394\n",
      "Iter-280 train loss: 2.2759 valid loss: 2.2866, valid accuracy: 0.1406\n",
      "Iter-290 train loss: 2.3018 valid loss: 2.2857, valid accuracy: 0.1414\n",
      "Iter-300 train loss: 2.2724 valid loss: 2.2848, valid accuracy: 0.1428\n",
      "Iter-310 train loss: 2.2608 valid loss: 2.2838, valid accuracy: 0.1436\n",
      "Iter-320 train loss: 2.2932 valid loss: 2.2828, valid accuracy: 0.1448\n",
      "Iter-330 train loss: 2.2372 valid loss: 2.2819, valid accuracy: 0.1462\n",
      "Iter-340 train loss: 2.3583 valid loss: 2.2809, valid accuracy: 0.1470\n",
      "Iter-350 train loss: 2.2694 valid loss: 2.2799, valid accuracy: 0.1486\n",
      "Iter-360 train loss: 2.2072 valid loss: 2.2790, valid accuracy: 0.1482\n",
      "Iter-370 train loss: 2.3446 valid loss: 2.2782, valid accuracy: 0.1490\n",
      "Iter-380 train loss: 2.3090 valid loss: 2.2773, valid accuracy: 0.1494\n",
      "Iter-390 train loss: 2.2933 valid loss: 2.2763, valid accuracy: 0.1502\n",
      "Iter-400 train loss: 2.3038 valid loss: 2.2754, valid accuracy: 0.1514\n",
      "Iter-410 train loss: 2.2467 valid loss: 2.2746, valid accuracy: 0.1522\n",
      "Iter-420 train loss: 2.1742 valid loss: 2.2737, valid accuracy: 0.1518\n",
      "Iter-430 train loss: 2.2785 valid loss: 2.2728, valid accuracy: 0.1534\n",
      "Iter-440 train loss: 2.2765 valid loss: 2.2718, valid accuracy: 0.1544\n",
      "Iter-450 train loss: 2.2884 valid loss: 2.2710, valid accuracy: 0.1544\n",
      "Iter-460 train loss: 2.2439 valid loss: 2.2701, valid accuracy: 0.1556\n",
      "Iter-470 train loss: 2.2284 valid loss: 2.2692, valid accuracy: 0.1564\n",
      "Iter-480 train loss: 2.2171 valid loss: 2.2683, valid accuracy: 0.1582\n",
      "Iter-490 train loss: 2.2503 valid loss: 2.2676, valid accuracy: 0.1590\n",
      "Iter-500 train loss: 2.2278 valid loss: 2.2667, valid accuracy: 0.1590\n",
      "Iter-510 train loss: 2.2564 valid loss: 2.2659, valid accuracy: 0.1596\n",
      "Iter-520 train loss: 2.2848 valid loss: 2.2650, valid accuracy: 0.1620\n",
      "Iter-530 train loss: 2.2484 valid loss: 2.2641, valid accuracy: 0.1626\n",
      "Iter-540 train loss: 2.2565 valid loss: 2.2631, valid accuracy: 0.1638\n",
      "Iter-550 train loss: 2.1990 valid loss: 2.2622, valid accuracy: 0.1648\n",
      "Iter-560 train loss: 2.2491 valid loss: 2.2614, valid accuracy: 0.1658\n",
      "Iter-570 train loss: 2.2941 valid loss: 2.2605, valid accuracy: 0.1670\n",
      "Iter-580 train loss: 2.2848 valid loss: 2.2596, valid accuracy: 0.1672\n",
      "Iter-590 train loss: 2.2155 valid loss: 2.2587, valid accuracy: 0.1680\n",
      "Iter-600 train loss: 2.2406 valid loss: 2.2577, valid accuracy: 0.1686\n",
      "Iter-610 train loss: 2.2474 valid loss: 2.2568, valid accuracy: 0.1690\n",
      "Iter-620 train loss: 2.2688 valid loss: 2.2559, valid accuracy: 0.1708\n",
      "Iter-630 train loss: 2.2677 valid loss: 2.2550, valid accuracy: 0.1712\n",
      "Iter-640 train loss: 2.2537 valid loss: 2.2542, valid accuracy: 0.1720\n",
      "Iter-650 train loss: 2.2592 valid loss: 2.2533, valid accuracy: 0.1732\n",
      "Iter-660 train loss: 2.2454 valid loss: 2.2525, valid accuracy: 0.1734\n",
      "Iter-670 train loss: 2.2406 valid loss: 2.2516, valid accuracy: 0.1752\n",
      "Iter-680 train loss: 2.2097 valid loss: 2.2508, valid accuracy: 0.1752\n",
      "Iter-690 train loss: 2.2638 valid loss: 2.2499, valid accuracy: 0.1760\n",
      "Iter-700 train loss: 2.2357 valid loss: 2.2491, valid accuracy: 0.1764\n",
      "Iter-710 train loss: 2.2607 valid loss: 2.2483, valid accuracy: 0.1762\n",
      "Iter-720 train loss: 2.2485 valid loss: 2.2474, valid accuracy: 0.1772\n",
      "Iter-730 train loss: 2.2092 valid loss: 2.2465, valid accuracy: 0.1776\n",
      "Iter-740 train loss: 2.2165 valid loss: 2.2457, valid accuracy: 0.1790\n",
      "Iter-750 train loss: 2.3406 valid loss: 2.2450, valid accuracy: 0.1794\n",
      "Iter-760 train loss: 2.1576 valid loss: 2.2441, valid accuracy: 0.1818\n",
      "Iter-770 train loss: 2.2429 valid loss: 2.2432, valid accuracy: 0.1828\n",
      "Iter-780 train loss: 2.2162 valid loss: 2.2424, valid accuracy: 0.1840\n",
      "Iter-790 train loss: 2.3527 valid loss: 2.2415, valid accuracy: 0.1844\n",
      "Iter-800 train loss: 2.2313 valid loss: 2.2409, valid accuracy: 0.1844\n",
      "Iter-810 train loss: 2.2269 valid loss: 2.2402, valid accuracy: 0.1846\n",
      "Iter-820 train loss: 2.2261 valid loss: 2.2393, valid accuracy: 0.1850\n",
      "Iter-830 train loss: 2.2465 valid loss: 2.2386, valid accuracy: 0.1858\n",
      "Iter-840 train loss: 2.2763 valid loss: 2.2378, valid accuracy: 0.1862\n",
      "Iter-850 train loss: 2.2513 valid loss: 2.2370, valid accuracy: 0.1876\n",
      "Iter-860 train loss: 2.2605 valid loss: 2.2362, valid accuracy: 0.1878\n",
      "Iter-870 train loss: 2.1871 valid loss: 2.2354, valid accuracy: 0.1888\n",
      "Iter-880 train loss: 2.2439 valid loss: 2.2346, valid accuracy: 0.1888\n",
      "Iter-890 train loss: 2.2856 valid loss: 2.2338, valid accuracy: 0.1896\n",
      "Iter-900 train loss: 2.1894 valid loss: 2.2330, valid accuracy: 0.1910\n",
      "Iter-910 train loss: 2.1574 valid loss: 2.2322, valid accuracy: 0.1918\n",
      "Iter-920 train loss: 2.2349 valid loss: 2.2313, valid accuracy: 0.1922\n",
      "Iter-930 train loss: 2.2370 valid loss: 2.2305, valid accuracy: 0.1928\n",
      "Iter-940 train loss: 2.2555 valid loss: 2.2298, valid accuracy: 0.1930\n",
      "Iter-950 train loss: 2.2122 valid loss: 2.2290, valid accuracy: 0.1934\n",
      "Iter-960 train loss: 2.2157 valid loss: 2.2282, valid accuracy: 0.1942\n",
      "Iter-970 train loss: 2.1877 valid loss: 2.2275, valid accuracy: 0.1948\n",
      "Iter-980 train loss: 2.2600 valid loss: 2.2268, valid accuracy: 0.1954\n",
      "Iter-990 train loss: 2.1871 valid loss: 2.2260, valid accuracy: 0.1960\n",
      "Iter-1000 train loss: 2.2565 valid loss: 2.2253, valid accuracy: 0.1972\n",
      "Iter-1010 train loss: 2.2294 valid loss: 2.2246, valid accuracy: 0.1972\n",
      "Iter-1020 train loss: 2.1895 valid loss: 2.2238, valid accuracy: 0.1966\n",
      "Iter-1030 train loss: 2.2071 valid loss: 2.2231, valid accuracy: 0.1974\n",
      "Iter-1040 train loss: 2.1627 valid loss: 2.2223, valid accuracy: 0.1978\n",
      "Iter-1050 train loss: 2.2137 valid loss: 2.2215, valid accuracy: 0.1974\n",
      "Iter-1060 train loss: 2.1677 valid loss: 2.2207, valid accuracy: 0.1982\n",
      "Iter-1070 train loss: 2.1926 valid loss: 2.2199, valid accuracy: 0.1990\n",
      "Iter-1080 train loss: 2.2297 valid loss: 2.2192, valid accuracy: 0.1990\n",
      "Iter-1090 train loss: 2.1533 valid loss: 2.2183, valid accuracy: 0.1992\n",
      "Iter-1100 train loss: 2.2821 valid loss: 2.2176, valid accuracy: 0.2000\n",
      "Iter-1110 train loss: 2.1836 valid loss: 2.2168, valid accuracy: 0.2018\n",
      "Iter-1120 train loss: 2.1709 valid loss: 2.2162, valid accuracy: 0.2020\n",
      "Iter-1130 train loss: 2.2299 valid loss: 2.2154, valid accuracy: 0.2018\n",
      "Iter-1140 train loss: 2.2798 valid loss: 2.2146, valid accuracy: 0.2024\n",
      "Iter-1150 train loss: 2.2329 valid loss: 2.2139, valid accuracy: 0.2032\n",
      "Iter-1160 train loss: 2.2585 valid loss: 2.2131, valid accuracy: 0.2036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1170 train loss: 2.1851 valid loss: 2.2124, valid accuracy: 0.2036\n",
      "Iter-1180 train loss: 2.2350 valid loss: 2.2117, valid accuracy: 0.2046\n",
      "Iter-1190 train loss: 2.2435 valid loss: 2.2110, valid accuracy: 0.2046\n",
      "Iter-1200 train loss: 2.1583 valid loss: 2.2104, valid accuracy: 0.2044\n",
      "Iter-1210 train loss: 2.2452 valid loss: 2.2096, valid accuracy: 0.2050\n",
      "Iter-1220 train loss: 2.2943 valid loss: 2.2089, valid accuracy: 0.2052\n",
      "Iter-1230 train loss: 2.2823 valid loss: 2.2082, valid accuracy: 0.2050\n",
      "Iter-1240 train loss: 2.2549 valid loss: 2.2074, valid accuracy: 0.2052\n",
      "Iter-1250 train loss: 2.2309 valid loss: 2.2066, valid accuracy: 0.2052\n",
      "Iter-1260 train loss: 2.1946 valid loss: 2.2059, valid accuracy: 0.2056\n",
      "Iter-1270 train loss: 2.2734 valid loss: 2.2052, valid accuracy: 0.2064\n",
      "Iter-1280 train loss: 2.1977 valid loss: 2.2045, valid accuracy: 0.2066\n",
      "Iter-1290 train loss: 2.2075 valid loss: 2.2038, valid accuracy: 0.2072\n",
      "Iter-1300 train loss: 2.2277 valid loss: 2.2032, valid accuracy: 0.2072\n",
      "Iter-1310 train loss: 2.2693 valid loss: 2.2025, valid accuracy: 0.2072\n",
      "Iter-1320 train loss: 2.2798 valid loss: 2.2019, valid accuracy: 0.2076\n",
      "Iter-1330 train loss: 2.1853 valid loss: 2.2011, valid accuracy: 0.2080\n",
      "Iter-1340 train loss: 2.1953 valid loss: 2.2004, valid accuracy: 0.2084\n",
      "Iter-1350 train loss: 2.2878 valid loss: 2.1998, valid accuracy: 0.2090\n",
      "Iter-1360 train loss: 2.2279 valid loss: 2.1991, valid accuracy: 0.2094\n",
      "Iter-1370 train loss: 2.1923 valid loss: 2.1984, valid accuracy: 0.2092\n",
      "Iter-1380 train loss: 2.2172 valid loss: 2.1977, valid accuracy: 0.2094\n",
      "Iter-1390 train loss: 2.2833 valid loss: 2.1971, valid accuracy: 0.2094\n",
      "Iter-1400 train loss: 2.1352 valid loss: 2.1964, valid accuracy: 0.2096\n",
      "Iter-1410 train loss: 2.2500 valid loss: 2.1958, valid accuracy: 0.2100\n",
      "Iter-1420 train loss: 2.1883 valid loss: 2.1951, valid accuracy: 0.2100\n",
      "Iter-1430 train loss: 2.2252 valid loss: 2.1943, valid accuracy: 0.2096\n",
      "Iter-1440 train loss: 2.2069 valid loss: 2.1937, valid accuracy: 0.2112\n",
      "Iter-1450 train loss: 2.1532 valid loss: 2.1930, valid accuracy: 0.2112\n",
      "Iter-1460 train loss: 2.1980 valid loss: 2.1924, valid accuracy: 0.2108\n",
      "Iter-1470 train loss: 2.1672 valid loss: 2.1918, valid accuracy: 0.2108\n",
      "Iter-1480 train loss: 2.1733 valid loss: 2.1911, valid accuracy: 0.2108\n",
      "Iter-1490 train loss: 2.2167 valid loss: 2.1904, valid accuracy: 0.2110\n",
      "Iter-1500 train loss: 2.1249 valid loss: 2.1897, valid accuracy: 0.2106\n",
      "Iter-1510 train loss: 2.1803 valid loss: 2.1891, valid accuracy: 0.2108\n",
      "Iter-1520 train loss: 2.1229 valid loss: 2.1884, valid accuracy: 0.2112\n",
      "Iter-1530 train loss: 2.1371 valid loss: 2.1878, valid accuracy: 0.2112\n",
      "Iter-1540 train loss: 2.2576 valid loss: 2.1871, valid accuracy: 0.2120\n",
      "Iter-1550 train loss: 2.2110 valid loss: 2.1863, valid accuracy: 0.2122\n",
      "Iter-1560 train loss: 2.1301 valid loss: 2.1856, valid accuracy: 0.2126\n",
      "Iter-1570 train loss: 2.2264 valid loss: 2.1850, valid accuracy: 0.2132\n",
      "Iter-1580 train loss: 2.1614 valid loss: 2.1843, valid accuracy: 0.2134\n",
      "Iter-1590 train loss: 2.1661 valid loss: 2.1836, valid accuracy: 0.2146\n",
      "Iter-1600 train loss: 2.2101 valid loss: 2.1830, valid accuracy: 0.2150\n",
      "Iter-1610 train loss: 2.1985 valid loss: 2.1823, valid accuracy: 0.2158\n",
      "Iter-1620 train loss: 2.2756 valid loss: 2.1816, valid accuracy: 0.2158\n",
      "Iter-1630 train loss: 2.1078 valid loss: 2.1810, valid accuracy: 0.2158\n",
      "Iter-1640 train loss: 2.2260 valid loss: 2.1804, valid accuracy: 0.2154\n",
      "Iter-1650 train loss: 2.1861 valid loss: 2.1798, valid accuracy: 0.2156\n",
      "Iter-1660 train loss: 2.2098 valid loss: 2.1791, valid accuracy: 0.2154\n",
      "Iter-1670 train loss: 2.2195 valid loss: 2.1786, valid accuracy: 0.2152\n",
      "Iter-1680 train loss: 2.1606 valid loss: 2.1779, valid accuracy: 0.2158\n",
      "Iter-1690 train loss: 2.1761 valid loss: 2.1773, valid accuracy: 0.2156\n",
      "Iter-1700 train loss: 2.2387 valid loss: 2.1767, valid accuracy: 0.2162\n",
      "Iter-1710 train loss: 2.1543 valid loss: 2.1762, valid accuracy: 0.2166\n",
      "Iter-1720 train loss: 2.1203 valid loss: 2.1756, valid accuracy: 0.2166\n",
      "Iter-1730 train loss: 2.1562 valid loss: 2.1750, valid accuracy: 0.2158\n",
      "Iter-1740 train loss: 2.0596 valid loss: 2.1744, valid accuracy: 0.2160\n",
      "Iter-1750 train loss: 2.2045 valid loss: 2.1738, valid accuracy: 0.2166\n",
      "Iter-1760 train loss: 2.1842 valid loss: 2.1732, valid accuracy: 0.2164\n",
      "Iter-1770 train loss: 2.2117 valid loss: 2.1727, valid accuracy: 0.2172\n",
      "Iter-1780 train loss: 2.1736 valid loss: 2.1721, valid accuracy: 0.2172\n",
      "Iter-1790 train loss: 2.1972 valid loss: 2.1715, valid accuracy: 0.2174\n",
      "Iter-1800 train loss: 2.1706 valid loss: 2.1709, valid accuracy: 0.2178\n",
      "Iter-1810 train loss: 2.2570 valid loss: 2.1703, valid accuracy: 0.2178\n",
      "Iter-1820 train loss: 2.0884 valid loss: 2.1697, valid accuracy: 0.2176\n",
      "Iter-1830 train loss: 2.2314 valid loss: 2.1692, valid accuracy: 0.2176\n",
      "Iter-1840 train loss: 2.1254 valid loss: 2.1685, valid accuracy: 0.2178\n",
      "Iter-1850 train loss: 2.1662 valid loss: 2.1679, valid accuracy: 0.2178\n",
      "Iter-1860 train loss: 2.2416 valid loss: 2.1673, valid accuracy: 0.2176\n",
      "Iter-1870 train loss: 2.1990 valid loss: 2.1667, valid accuracy: 0.2178\n",
      "Iter-1880 train loss: 2.1995 valid loss: 2.1661, valid accuracy: 0.2190\n",
      "Iter-1890 train loss: 2.1992 valid loss: 2.1656, valid accuracy: 0.2190\n",
      "Iter-1900 train loss: 2.1220 valid loss: 2.1650, valid accuracy: 0.2194\n",
      "Iter-1910 train loss: 2.0512 valid loss: 2.1644, valid accuracy: 0.2196\n",
      "Iter-1920 train loss: 2.1325 valid loss: 2.1639, valid accuracy: 0.2192\n",
      "Iter-1930 train loss: 2.2195 valid loss: 2.1633, valid accuracy: 0.2196\n",
      "Iter-1940 train loss: 2.2928 valid loss: 2.1627, valid accuracy: 0.2196\n",
      "Iter-1950 train loss: 2.1251 valid loss: 2.1621, valid accuracy: 0.2192\n",
      "Iter-1960 train loss: 2.2812 valid loss: 2.1616, valid accuracy: 0.2198\n",
      "Iter-1970 train loss: 2.2025 valid loss: 2.1610, valid accuracy: 0.2210\n",
      "Iter-1980 train loss: 2.2799 valid loss: 2.1604, valid accuracy: 0.2214\n",
      "Iter-1990 train loss: 2.2248 valid loss: 2.1598, valid accuracy: 0.2212\n",
      "Iter-2000 train loss: 2.2084 valid loss: 2.1592, valid accuracy: 0.2214\n",
      "Iter-2010 train loss: 2.1378 valid loss: 2.1586, valid accuracy: 0.2216\n",
      "Iter-2020 train loss: 2.0853 valid loss: 2.1581, valid accuracy: 0.2216\n",
      "Iter-2030 train loss: 2.1355 valid loss: 2.1575, valid accuracy: 0.2216\n",
      "Iter-2040 train loss: 2.2622 valid loss: 2.1570, valid accuracy: 0.2216\n",
      "Iter-2050 train loss: 2.1522 valid loss: 2.1564, valid accuracy: 0.2218\n",
      "Iter-2060 train loss: 2.1325 valid loss: 2.1559, valid accuracy: 0.2220\n",
      "Iter-2070 train loss: 2.1854 valid loss: 2.1552, valid accuracy: 0.2232\n",
      "Iter-2080 train loss: 2.1908 valid loss: 2.1547, valid accuracy: 0.2238\n",
      "Iter-2090 train loss: 2.1889 valid loss: 2.1541, valid accuracy: 0.2240\n",
      "Iter-2100 train loss: 2.1576 valid loss: 2.1537, valid accuracy: 0.2242\n",
      "Iter-2110 train loss: 2.0623 valid loss: 2.1531, valid accuracy: 0.2244\n",
      "Iter-2120 train loss: 2.2578 valid loss: 2.1525, valid accuracy: 0.2252\n",
      "Iter-2130 train loss: 2.2007 valid loss: 2.1519, valid accuracy: 0.2254\n",
      "Iter-2140 train loss: 2.1558 valid loss: 2.1513, valid accuracy: 0.2264\n",
      "Iter-2150 train loss: 2.2202 valid loss: 2.1509, valid accuracy: 0.2266\n",
      "Iter-2160 train loss: 2.1477 valid loss: 2.1503, valid accuracy: 0.2264\n",
      "Iter-2170 train loss: 2.2662 valid loss: 2.1497, valid accuracy: 0.2268\n",
      "Iter-2180 train loss: 2.1265 valid loss: 2.1491, valid accuracy: 0.2272\n",
      "Iter-2190 train loss: 2.1728 valid loss: 2.1486, valid accuracy: 0.2274\n",
      "Iter-2200 train loss: 2.1053 valid loss: 2.1480, valid accuracy: 0.2278\n",
      "Iter-2210 train loss: 2.0942 valid loss: 2.1475, valid accuracy: 0.2276\n",
      "Iter-2220 train loss: 2.1974 valid loss: 2.1470, valid accuracy: 0.2280\n",
      "Iter-2230 train loss: 2.1750 valid loss: 2.1464, valid accuracy: 0.2288\n",
      "Iter-2240 train loss: 2.1606 valid loss: 2.1459, valid accuracy: 0.2286\n",
      "Iter-2250 train loss: 2.1554 valid loss: 2.1454, valid accuracy: 0.2300\n",
      "Iter-2260 train loss: 2.1399 valid loss: 2.1448, valid accuracy: 0.2300\n",
      "Iter-2270 train loss: 2.1370 valid loss: 2.1443, valid accuracy: 0.2318\n",
      "Iter-2280 train loss: 2.1346 valid loss: 2.1437, valid accuracy: 0.2330\n",
      "Iter-2290 train loss: 2.1075 valid loss: 2.1432, valid accuracy: 0.2334\n",
      "Iter-2300 train loss: 2.0929 valid loss: 2.1426, valid accuracy: 0.2348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2310 train loss: 2.1246 valid loss: 2.1421, valid accuracy: 0.2352\n",
      "Iter-2320 train loss: 2.1911 valid loss: 2.1416, valid accuracy: 0.2354\n",
      "Iter-2330 train loss: 2.2404 valid loss: 2.1411, valid accuracy: 0.2360\n",
      "Iter-2340 train loss: 2.1187 valid loss: 2.1406, valid accuracy: 0.2368\n",
      "Iter-2350 train loss: 2.2237 valid loss: 2.1402, valid accuracy: 0.2372\n",
      "Iter-2360 train loss: 2.2172 valid loss: 2.1397, valid accuracy: 0.2380\n",
      "Iter-2370 train loss: 2.2212 valid loss: 2.1392, valid accuracy: 0.2386\n",
      "Iter-2380 train loss: 2.1751 valid loss: 2.1387, valid accuracy: 0.2396\n",
      "Iter-2390 train loss: 2.1961 valid loss: 2.1383, valid accuracy: 0.2400\n",
      "Iter-2400 train loss: 2.1340 valid loss: 2.1378, valid accuracy: 0.2406\n",
      "Iter-2410 train loss: 2.2241 valid loss: 2.1373, valid accuracy: 0.2414\n",
      "Iter-2420 train loss: 2.0127 valid loss: 2.1367, valid accuracy: 0.2428\n",
      "Iter-2430 train loss: 2.1158 valid loss: 2.1363, valid accuracy: 0.2436\n",
      "Iter-2440 train loss: 2.1126 valid loss: 2.1358, valid accuracy: 0.2438\n",
      "Iter-2450 train loss: 2.1625 valid loss: 2.1353, valid accuracy: 0.2442\n",
      "Iter-2460 train loss: 2.0733 valid loss: 2.1349, valid accuracy: 0.2440\n",
      "Iter-2470 train loss: 2.2109 valid loss: 2.1344, valid accuracy: 0.2438\n",
      "Iter-2480 train loss: 2.1484 valid loss: 2.1339, valid accuracy: 0.2440\n",
      "Iter-2490 train loss: 2.1026 valid loss: 2.1334, valid accuracy: 0.2446\n",
      "Iter-2500 train loss: 2.1544 valid loss: 2.1329, valid accuracy: 0.2448\n",
      "Iter-2510 train loss: 2.1408 valid loss: 2.1324, valid accuracy: 0.2446\n",
      "Iter-2520 train loss: 2.0550 valid loss: 2.1319, valid accuracy: 0.2450\n",
      "Iter-2530 train loss: 2.1989 valid loss: 2.1314, valid accuracy: 0.2456\n",
      "Iter-2540 train loss: 2.1369 valid loss: 2.1309, valid accuracy: 0.2460\n",
      "Iter-2550 train loss: 2.1804 valid loss: 2.1304, valid accuracy: 0.2470\n",
      "Iter-2560 train loss: 2.1349 valid loss: 2.1300, valid accuracy: 0.2476\n",
      "Iter-2570 train loss: 2.1281 valid loss: 2.1295, valid accuracy: 0.2486\n",
      "Iter-2580 train loss: 2.2403 valid loss: 2.1291, valid accuracy: 0.2484\n",
      "Iter-2590 train loss: 2.1682 valid loss: 2.1286, valid accuracy: 0.2490\n",
      "Iter-2600 train loss: 2.2274 valid loss: 2.1283, valid accuracy: 0.2490\n",
      "Iter-2610 train loss: 2.0759 valid loss: 2.1278, valid accuracy: 0.2490\n",
      "Iter-2620 train loss: 2.0690 valid loss: 2.1273, valid accuracy: 0.2498\n",
      "Iter-2630 train loss: 2.1092 valid loss: 2.1269, valid accuracy: 0.2504\n",
      "Iter-2640 train loss: 2.1875 valid loss: 2.1264, valid accuracy: 0.2512\n",
      "Iter-2650 train loss: 2.1009 valid loss: 2.1259, valid accuracy: 0.2520\n",
      "Iter-2660 train loss: 2.1126 valid loss: 2.1254, valid accuracy: 0.2524\n",
      "Iter-2670 train loss: 2.2205 valid loss: 2.1251, valid accuracy: 0.2520\n",
      "Iter-2680 train loss: 2.1222 valid loss: 2.1246, valid accuracy: 0.2526\n",
      "Iter-2690 train loss: 2.0951 valid loss: 2.1241, valid accuracy: 0.2538\n",
      "Iter-2700 train loss: 2.1274 valid loss: 2.1237, valid accuracy: 0.2544\n",
      "Iter-2710 train loss: 2.1208 valid loss: 2.1232, valid accuracy: 0.2554\n",
      "Iter-2720 train loss: 2.1387 valid loss: 2.1228, valid accuracy: 0.2556\n",
      "Iter-2730 train loss: 2.1233 valid loss: 2.1223, valid accuracy: 0.2562\n",
      "Iter-2740 train loss: 2.1229 valid loss: 2.1219, valid accuracy: 0.2564\n",
      "Iter-2750 train loss: 2.0908 valid loss: 2.1215, valid accuracy: 0.2566\n",
      "Iter-2760 train loss: 2.2497 valid loss: 2.1210, valid accuracy: 0.2572\n",
      "Iter-2770 train loss: 2.1233 valid loss: 2.1205, valid accuracy: 0.2582\n",
      "Iter-2780 train loss: 2.1264 valid loss: 2.1201, valid accuracy: 0.2586\n",
      "Iter-2790 train loss: 2.1448 valid loss: 2.1197, valid accuracy: 0.2592\n",
      "Iter-2800 train loss: 2.2273 valid loss: 2.1193, valid accuracy: 0.2598\n",
      "Iter-2810 train loss: 2.1491 valid loss: 2.1188, valid accuracy: 0.2596\n",
      "Iter-2820 train loss: 2.1425 valid loss: 2.1183, valid accuracy: 0.2604\n",
      "Iter-2830 train loss: 2.1686 valid loss: 2.1179, valid accuracy: 0.2602\n",
      "Iter-2840 train loss: 2.1157 valid loss: 2.1175, valid accuracy: 0.2606\n",
      "Iter-2850 train loss: 2.1748 valid loss: 2.1170, valid accuracy: 0.2604\n",
      "Iter-2860 train loss: 2.1455 valid loss: 2.1166, valid accuracy: 0.2614\n",
      "Iter-2870 train loss: 2.1145 valid loss: 2.1161, valid accuracy: 0.2622\n",
      "Iter-2880 train loss: 2.1489 valid loss: 2.1156, valid accuracy: 0.2632\n",
      "Iter-2890 train loss: 2.1900 valid loss: 2.1152, valid accuracy: 0.2640\n",
      "Iter-2900 train loss: 2.0627 valid loss: 2.1148, valid accuracy: 0.2638\n",
      "Iter-2910 train loss: 2.1418 valid loss: 2.1144, valid accuracy: 0.2638\n",
      "Iter-2920 train loss: 2.1481 valid loss: 2.1141, valid accuracy: 0.2640\n",
      "Iter-2930 train loss: 2.0670 valid loss: 2.1136, valid accuracy: 0.2642\n",
      "Iter-2940 train loss: 2.1460 valid loss: 2.1131, valid accuracy: 0.2646\n",
      "Iter-2950 train loss: 2.0552 valid loss: 2.1127, valid accuracy: 0.2652\n",
      "Iter-2960 train loss: 2.1659 valid loss: 2.1123, valid accuracy: 0.2650\n",
      "Iter-2970 train loss: 2.1049 valid loss: 2.1119, valid accuracy: 0.2658\n",
      "Iter-2980 train loss: 2.1445 valid loss: 2.1115, valid accuracy: 0.2664\n",
      "Iter-2990 train loss: 2.1297 valid loss: 2.1111, valid accuracy: 0.2668\n",
      "Iter-3000 train loss: 2.1891 valid loss: 2.1107, valid accuracy: 0.2674\n",
      "Iter-3010 train loss: 2.1309 valid loss: 2.1103, valid accuracy: 0.2680\n",
      "Iter-3020 train loss: 2.1439 valid loss: 2.1099, valid accuracy: 0.2690\n",
      "Iter-3030 train loss: 2.0109 valid loss: 2.1095, valid accuracy: 0.2690\n",
      "Iter-3040 train loss: 2.1593 valid loss: 2.1091, valid accuracy: 0.2694\n",
      "Iter-3050 train loss: 2.0166 valid loss: 2.1087, valid accuracy: 0.2702\n",
      "Iter-3060 train loss: 2.1163 valid loss: 2.1082, valid accuracy: 0.2700\n",
      "Iter-3070 train loss: 2.1272 valid loss: 2.1078, valid accuracy: 0.2704\n",
      "Iter-3080 train loss: 2.0356 valid loss: 2.1074, valid accuracy: 0.2704\n",
      "Iter-3090 train loss: 2.0540 valid loss: 2.1070, valid accuracy: 0.2706\n",
      "Iter-3100 train loss: 2.1526 valid loss: 2.1067, valid accuracy: 0.2702\n",
      "Iter-3110 train loss: 2.1553 valid loss: 2.1063, valid accuracy: 0.2706\n",
      "Iter-3120 train loss: 2.0431 valid loss: 2.1059, valid accuracy: 0.2706\n",
      "Iter-3130 train loss: 2.1991 valid loss: 2.1055, valid accuracy: 0.2704\n",
      "Iter-3140 train loss: 2.1135 valid loss: 2.1050, valid accuracy: 0.2708\n",
      "Iter-3150 train loss: 2.0744 valid loss: 2.1046, valid accuracy: 0.2710\n",
      "Iter-3160 train loss: 2.1060 valid loss: 2.1042, valid accuracy: 0.2712\n",
      "Iter-3170 train loss: 2.0307 valid loss: 2.1037, valid accuracy: 0.2714\n",
      "Iter-3180 train loss: 2.1616 valid loss: 2.1033, valid accuracy: 0.2724\n",
      "Iter-3190 train loss: 2.1185 valid loss: 2.1029, valid accuracy: 0.2724\n",
      "Iter-3200 train loss: 2.2146 valid loss: 2.1026, valid accuracy: 0.2728\n",
      "Iter-3210 train loss: 2.1561 valid loss: 2.1022, valid accuracy: 0.2734\n",
      "Iter-3220 train loss: 2.0818 valid loss: 2.1018, valid accuracy: 0.2736\n",
      "Iter-3230 train loss: 2.0621 valid loss: 2.1014, valid accuracy: 0.2736\n",
      "Iter-3240 train loss: 1.9772 valid loss: 2.1010, valid accuracy: 0.2734\n",
      "Iter-3250 train loss: 2.0755 valid loss: 2.1005, valid accuracy: 0.2738\n",
      "Iter-3260 train loss: 2.1296 valid loss: 2.1001, valid accuracy: 0.2740\n",
      "Iter-3270 train loss: 2.0885 valid loss: 2.0997, valid accuracy: 0.2740\n",
      "Iter-3280 train loss: 2.1078 valid loss: 2.0992, valid accuracy: 0.2736\n",
      "Iter-3290 train loss: 2.1363 valid loss: 2.0989, valid accuracy: 0.2742\n",
      "Iter-3300 train loss: 2.0885 valid loss: 2.0985, valid accuracy: 0.2744\n",
      "Iter-3310 train loss: 2.1900 valid loss: 2.0981, valid accuracy: 0.2752\n",
      "Iter-3320 train loss: 2.2186 valid loss: 2.0978, valid accuracy: 0.2748\n",
      "Iter-3330 train loss: 2.1691 valid loss: 2.0975, valid accuracy: 0.2752\n",
      "Iter-3340 train loss: 2.1604 valid loss: 2.0972, valid accuracy: 0.2758\n",
      "Iter-3350 train loss: 2.0941 valid loss: 2.0968, valid accuracy: 0.2756\n",
      "Iter-3360 train loss: 2.0735 valid loss: 2.0965, valid accuracy: 0.2764\n",
      "Iter-3370 train loss: 2.0678 valid loss: 2.0961, valid accuracy: 0.2768\n",
      "Iter-3380 train loss: 2.0545 valid loss: 2.0957, valid accuracy: 0.2770\n",
      "Iter-3390 train loss: 2.0531 valid loss: 2.0953, valid accuracy: 0.2770\n",
      "Iter-3400 train loss: 2.0791 valid loss: 2.0949, valid accuracy: 0.2774\n",
      "Iter-3410 train loss: 2.1753 valid loss: 2.0945, valid accuracy: 0.2774\n",
      "Iter-3420 train loss: 1.9607 valid loss: 2.0941, valid accuracy: 0.2772\n",
      "Iter-3430 train loss: 2.0722 valid loss: 2.0938, valid accuracy: 0.2778\n",
      "Iter-3440 train loss: 2.1135 valid loss: 2.0934, valid accuracy: 0.2784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3450 train loss: 2.1168 valid loss: 2.0929, valid accuracy: 0.2784\n",
      "Iter-3460 train loss: 2.0472 valid loss: 2.0925, valid accuracy: 0.2788\n",
      "Iter-3470 train loss: 2.0881 valid loss: 2.0920, valid accuracy: 0.2786\n",
      "Iter-3480 train loss: 2.0829 valid loss: 2.0917, valid accuracy: 0.2792\n",
      "Iter-3490 train loss: 2.2418 valid loss: 2.0913, valid accuracy: 0.2792\n",
      "Iter-3500 train loss: 2.0648 valid loss: 2.0910, valid accuracy: 0.2794\n",
      "Iter-3510 train loss: 2.1958 valid loss: 2.0906, valid accuracy: 0.2798\n",
      "Iter-3520 train loss: 2.0639 valid loss: 2.0902, valid accuracy: 0.2798\n",
      "Iter-3530 train loss: 2.0241 valid loss: 2.0898, valid accuracy: 0.2796\n",
      "Iter-3540 train loss: 2.1103 valid loss: 2.0894, valid accuracy: 0.2802\n",
      "Iter-3550 train loss: 2.0655 valid loss: 2.0891, valid accuracy: 0.2808\n",
      "Iter-3560 train loss: 2.0037 valid loss: 2.0887, valid accuracy: 0.2812\n",
      "Iter-3570 train loss: 2.0357 valid loss: 2.0884, valid accuracy: 0.2818\n",
      "Iter-3580 train loss: 2.1510 valid loss: 2.0880, valid accuracy: 0.2818\n",
      "Iter-3590 train loss: 1.9714 valid loss: 2.0877, valid accuracy: 0.2816\n",
      "Iter-3600 train loss: 2.0499 valid loss: 2.0873, valid accuracy: 0.2820\n",
      "Iter-3610 train loss: 2.0512 valid loss: 2.0869, valid accuracy: 0.2818\n",
      "Iter-3620 train loss: 2.1882 valid loss: 2.0865, valid accuracy: 0.2818\n",
      "Iter-3630 train loss: 2.0983 valid loss: 2.0861, valid accuracy: 0.2822\n",
      "Iter-3640 train loss: 1.9346 valid loss: 2.0859, valid accuracy: 0.2820\n",
      "Iter-3650 train loss: 2.1171 valid loss: 2.0855, valid accuracy: 0.2814\n",
      "Iter-3660 train loss: 2.0535 valid loss: 2.0851, valid accuracy: 0.2822\n",
      "Iter-3670 train loss: 2.1378 valid loss: 2.0848, valid accuracy: 0.2824\n",
      "Iter-3680 train loss: 2.0319 valid loss: 2.0844, valid accuracy: 0.2826\n",
      "Iter-3690 train loss: 2.0395 valid loss: 2.0840, valid accuracy: 0.2830\n",
      "Iter-3700 train loss: 2.2075 valid loss: 2.0837, valid accuracy: 0.2836\n",
      "Iter-3710 train loss: 2.1563 valid loss: 2.0834, valid accuracy: 0.2838\n",
      "Iter-3720 train loss: 2.1507 valid loss: 2.0830, valid accuracy: 0.2842\n",
      "Iter-3730 train loss: 2.1625 valid loss: 2.0826, valid accuracy: 0.2846\n",
      "Iter-3740 train loss: 2.0910 valid loss: 2.0823, valid accuracy: 0.2842\n",
      "Iter-3750 train loss: 2.0753 valid loss: 2.0819, valid accuracy: 0.2842\n",
      "Iter-3760 train loss: 2.0397 valid loss: 2.0816, valid accuracy: 0.2846\n",
      "Iter-3770 train loss: 2.1661 valid loss: 2.0812, valid accuracy: 0.2846\n",
      "Iter-3780 train loss: 2.0318 valid loss: 2.0809, valid accuracy: 0.2850\n",
      "Iter-3790 train loss: 2.1554 valid loss: 2.0806, valid accuracy: 0.2848\n",
      "Iter-3800 train loss: 2.0420 valid loss: 2.0803, valid accuracy: 0.2848\n",
      "Iter-3810 train loss: 2.1649 valid loss: 2.0799, valid accuracy: 0.2848\n",
      "Iter-3820 train loss: 2.1737 valid loss: 2.0796, valid accuracy: 0.2850\n",
      "Iter-3830 train loss: 2.1136 valid loss: 2.0793, valid accuracy: 0.2854\n",
      "Iter-3840 train loss: 2.0492 valid loss: 2.0791, valid accuracy: 0.2852\n",
      "Iter-3850 train loss: 2.0384 valid loss: 2.0787, valid accuracy: 0.2852\n",
      "Iter-3860 train loss: 2.2087 valid loss: 2.0784, valid accuracy: 0.2852\n",
      "Iter-3870 train loss: 1.9931 valid loss: 2.0781, valid accuracy: 0.2852\n",
      "Iter-3880 train loss: 2.1871 valid loss: 2.0778, valid accuracy: 0.2860\n",
      "Iter-3890 train loss: 2.0808 valid loss: 2.0775, valid accuracy: 0.2860\n",
      "Iter-3900 train loss: 2.0267 valid loss: 2.0772, valid accuracy: 0.2860\n",
      "Iter-3910 train loss: 2.1548 valid loss: 2.0769, valid accuracy: 0.2858\n",
      "Iter-3920 train loss: 2.1515 valid loss: 2.0766, valid accuracy: 0.2858\n",
      "Iter-3930 train loss: 2.0241 valid loss: 2.0762, valid accuracy: 0.2856\n",
      "Iter-3940 train loss: 2.1875 valid loss: 2.0759, valid accuracy: 0.2856\n",
      "Iter-3950 train loss: 2.1538 valid loss: 2.0756, valid accuracy: 0.2854\n",
      "Iter-3960 train loss: 2.1039 valid loss: 2.0752, valid accuracy: 0.2854\n",
      "Iter-3970 train loss: 2.1070 valid loss: 2.0749, valid accuracy: 0.2854\n",
      "Iter-3980 train loss: 2.1536 valid loss: 2.0746, valid accuracy: 0.2854\n",
      "Iter-3990 train loss: 2.2012 valid loss: 2.0743, valid accuracy: 0.2854\n",
      "Iter-4000 train loss: 1.9907 valid loss: 2.0740, valid accuracy: 0.2854\n",
      "Iter-4010 train loss: 2.0996 valid loss: 2.0736, valid accuracy: 0.2854\n",
      "Iter-4020 train loss: 2.0435 valid loss: 2.0733, valid accuracy: 0.2846\n",
      "Iter-4030 train loss: 2.0503 valid loss: 2.0729, valid accuracy: 0.2844\n",
      "Iter-4040 train loss: 2.0849 valid loss: 2.0726, valid accuracy: 0.2842\n",
      "Iter-4050 train loss: 1.9417 valid loss: 2.0724, valid accuracy: 0.2846\n",
      "Iter-4060 train loss: 2.1151 valid loss: 2.0720, valid accuracy: 0.2848\n",
      "Iter-4070 train loss: 2.1182 valid loss: 2.0717, valid accuracy: 0.2848\n",
      "Iter-4080 train loss: 2.1378 valid loss: 2.0714, valid accuracy: 0.2850\n",
      "Iter-4090 train loss: 2.0290 valid loss: 2.0711, valid accuracy: 0.2852\n",
      "Iter-4100 train loss: 2.1524 valid loss: 2.0708, valid accuracy: 0.2850\n",
      "Iter-4110 train loss: 2.0413 valid loss: 2.0704, valid accuracy: 0.2852\n",
      "Iter-4120 train loss: 2.1706 valid loss: 2.0701, valid accuracy: 0.2854\n",
      "Iter-4130 train loss: 2.0794 valid loss: 2.0697, valid accuracy: 0.2864\n",
      "Iter-4140 train loss: 1.9335 valid loss: 2.0694, valid accuracy: 0.2862\n",
      "Iter-4150 train loss: 2.0504 valid loss: 2.0691, valid accuracy: 0.2862\n",
      "Iter-4160 train loss: 2.1091 valid loss: 2.0688, valid accuracy: 0.2862\n",
      "Iter-4170 train loss: 2.1325 valid loss: 2.0685, valid accuracy: 0.2858\n",
      "Iter-4180 train loss: 2.1566 valid loss: 2.0681, valid accuracy: 0.2854\n",
      "Iter-4190 train loss: 2.1715 valid loss: 2.0679, valid accuracy: 0.2860\n",
      "Iter-4200 train loss: 2.0182 valid loss: 2.0676, valid accuracy: 0.2858\n",
      "Iter-4210 train loss: 2.0486 valid loss: 2.0672, valid accuracy: 0.2858\n",
      "Iter-4220 train loss: 2.1638 valid loss: 2.0669, valid accuracy: 0.2856\n",
      "Iter-4230 train loss: 1.9831 valid loss: 2.0666, valid accuracy: 0.2864\n",
      "Iter-4240 train loss: 2.1425 valid loss: 2.0663, valid accuracy: 0.2864\n",
      "Iter-4250 train loss: 2.1439 valid loss: 2.0660, valid accuracy: 0.2872\n",
      "Iter-4260 train loss: 2.1760 valid loss: 2.0658, valid accuracy: 0.2866\n",
      "Iter-4270 train loss: 2.1127 valid loss: 2.0654, valid accuracy: 0.2872\n",
      "Iter-4280 train loss: 2.1144 valid loss: 2.0652, valid accuracy: 0.2866\n",
      "Iter-4290 train loss: 2.1738 valid loss: 2.0649, valid accuracy: 0.2866\n",
      "Iter-4300 train loss: 2.1250 valid loss: 2.0645, valid accuracy: 0.2868\n",
      "Iter-4310 train loss: 2.0198 valid loss: 2.0642, valid accuracy: 0.2866\n",
      "Iter-4320 train loss: 2.0476 valid loss: 2.0639, valid accuracy: 0.2862\n",
      "Iter-4330 train loss: 2.1058 valid loss: 2.0636, valid accuracy: 0.2862\n",
      "Iter-4340 train loss: 2.0428 valid loss: 2.0633, valid accuracy: 0.2860\n",
      "Iter-4350 train loss: 2.1312 valid loss: 2.0630, valid accuracy: 0.2864\n",
      "Iter-4360 train loss: 2.0969 valid loss: 2.0627, valid accuracy: 0.2878\n",
      "Iter-4370 train loss: 1.9772 valid loss: 2.0625, valid accuracy: 0.2876\n",
      "Iter-4380 train loss: 2.0136 valid loss: 2.0622, valid accuracy: 0.2876\n",
      "Iter-4390 train loss: 2.1427 valid loss: 2.0619, valid accuracy: 0.2884\n",
      "Iter-4400 train loss: 2.0031 valid loss: 2.0616, valid accuracy: 0.2878\n",
      "Iter-4410 train loss: 2.0497 valid loss: 2.0613, valid accuracy: 0.2878\n",
      "Iter-4420 train loss: 2.0472 valid loss: 2.0610, valid accuracy: 0.2888\n",
      "Iter-4430 train loss: 2.0347 valid loss: 2.0607, valid accuracy: 0.2888\n",
      "Iter-4440 train loss: 2.1410 valid loss: 2.0604, valid accuracy: 0.2892\n",
      "Iter-4450 train loss: 2.0669 valid loss: 2.0601, valid accuracy: 0.2888\n",
      "Iter-4460 train loss: 2.1631 valid loss: 2.0598, valid accuracy: 0.2890\n",
      "Iter-4470 train loss: 2.0484 valid loss: 2.0595, valid accuracy: 0.2890\n",
      "Iter-4480 train loss: 2.0779 valid loss: 2.0592, valid accuracy: 0.2888\n",
      "Iter-4490 train loss: 2.0643 valid loss: 2.0589, valid accuracy: 0.2890\n",
      "Iter-4500 train loss: 2.0602 valid loss: 2.0586, valid accuracy: 0.2890\n",
      "Iter-4510 train loss: 2.0357 valid loss: 2.0584, valid accuracy: 0.2892\n",
      "Iter-4520 train loss: 2.0807 valid loss: 2.0581, valid accuracy: 0.2892\n",
      "Iter-4530 train loss: 2.0116 valid loss: 2.0578, valid accuracy: 0.2890\n",
      "Iter-4540 train loss: 2.0874 valid loss: 2.0576, valid accuracy: 0.2890\n",
      "Iter-4550 train loss: 2.2100 valid loss: 2.0574, valid accuracy: 0.2888\n",
      "Iter-4560 train loss: 2.0701 valid loss: 2.0571, valid accuracy: 0.2886\n",
      "Iter-4570 train loss: 1.9192 valid loss: 2.0568, valid accuracy: 0.2888\n",
      "Iter-4580 train loss: 2.0339 valid loss: 2.0565, valid accuracy: 0.2884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4590 train loss: 2.1248 valid loss: 2.0562, valid accuracy: 0.2886\n",
      "Iter-4600 train loss: 2.0842 valid loss: 2.0560, valid accuracy: 0.2884\n",
      "Iter-4610 train loss: 2.1001 valid loss: 2.0557, valid accuracy: 0.2876\n",
      "Iter-4620 train loss: 2.1148 valid loss: 2.0555, valid accuracy: 0.2880\n",
      "Iter-4630 train loss: 2.1386 valid loss: 2.0552, valid accuracy: 0.2880\n",
      "Iter-4640 train loss: 2.1019 valid loss: 2.0549, valid accuracy: 0.2882\n",
      "Iter-4650 train loss: 2.1362 valid loss: 2.0546, valid accuracy: 0.2888\n",
      "Iter-4660 train loss: 1.9737 valid loss: 2.0543, valid accuracy: 0.2890\n",
      "Iter-4670 train loss: 2.1354 valid loss: 2.0541, valid accuracy: 0.2890\n",
      "Iter-4680 train loss: 2.0985 valid loss: 2.0538, valid accuracy: 0.2886\n",
      "Iter-4690 train loss: 2.0913 valid loss: 2.0536, valid accuracy: 0.2890\n",
      "Iter-4700 train loss: 2.0098 valid loss: 2.0533, valid accuracy: 0.2886\n",
      "Iter-4710 train loss: 2.0792 valid loss: 2.0530, valid accuracy: 0.2892\n",
      "Iter-4720 train loss: 2.2361 valid loss: 2.0527, valid accuracy: 0.2886\n",
      "Iter-4730 train loss: 1.9971 valid loss: 2.0525, valid accuracy: 0.2886\n",
      "Iter-4740 train loss: 2.1756 valid loss: 2.0522, valid accuracy: 0.2888\n",
      "Iter-4750 train loss: 2.1633 valid loss: 2.0519, valid accuracy: 0.2884\n",
      "Iter-4760 train loss: 1.9315 valid loss: 2.0516, valid accuracy: 0.2888\n",
      "Iter-4770 train loss: 1.9970 valid loss: 2.0513, valid accuracy: 0.2898\n",
      "Iter-4780 train loss: 2.1161 valid loss: 2.0510, valid accuracy: 0.2896\n",
      "Iter-4790 train loss: 2.0931 valid loss: 2.0508, valid accuracy: 0.2900\n",
      "Iter-4800 train loss: 2.1431 valid loss: 2.0505, valid accuracy: 0.2900\n",
      "Iter-4810 train loss: 2.0961 valid loss: 2.0503, valid accuracy: 0.2894\n",
      "Iter-4820 train loss: 2.2036 valid loss: 2.0500, valid accuracy: 0.2894\n",
      "Iter-4830 train loss: 2.0317 valid loss: 2.0498, valid accuracy: 0.2892\n",
      "Iter-4840 train loss: 2.0634 valid loss: 2.0495, valid accuracy: 0.2898\n",
      "Iter-4850 train loss: 2.1380 valid loss: 2.0492, valid accuracy: 0.2900\n",
      "Iter-4860 train loss: 2.0247 valid loss: 2.0490, valid accuracy: 0.2900\n",
      "Iter-4870 train loss: 2.0070 valid loss: 2.0487, valid accuracy: 0.2900\n",
      "Iter-4880 train loss: 2.1026 valid loss: 2.0485, valid accuracy: 0.2902\n",
      "Iter-4890 train loss: 2.0611 valid loss: 2.0483, valid accuracy: 0.2902\n",
      "Iter-4900 train loss: 1.9816 valid loss: 2.0480, valid accuracy: 0.2902\n",
      "Iter-4910 train loss: 1.9966 valid loss: 2.0477, valid accuracy: 0.2898\n",
      "Iter-4920 train loss: 1.9590 valid loss: 2.0475, valid accuracy: 0.2900\n",
      "Iter-4930 train loss: 2.0372 valid loss: 2.0472, valid accuracy: 0.2900\n",
      "Iter-4940 train loss: 2.0634 valid loss: 2.0470, valid accuracy: 0.2902\n",
      "Iter-4950 train loss: 2.0697 valid loss: 2.0467, valid accuracy: 0.2902\n",
      "Iter-4960 train loss: 1.9615 valid loss: 2.0465, valid accuracy: 0.2904\n",
      "Iter-4970 train loss: 2.0899 valid loss: 2.0463, valid accuracy: 0.2908\n",
      "Iter-4980 train loss: 2.1068 valid loss: 2.0460, valid accuracy: 0.2906\n",
      "Iter-4990 train loss: 1.9988 valid loss: 2.0457, valid accuracy: 0.2908\n",
      "Iter-5000 train loss: 2.0611 valid loss: 2.0454, valid accuracy: 0.2910\n",
      "Iter-5010 train loss: 2.0380 valid loss: 2.0452, valid accuracy: 0.2918\n",
      "Iter-5020 train loss: 2.0223 valid loss: 2.0449, valid accuracy: 0.2926\n",
      "Iter-5030 train loss: 2.0686 valid loss: 2.0446, valid accuracy: 0.2928\n",
      "Iter-5040 train loss: 2.0144 valid loss: 2.0443, valid accuracy: 0.2930\n",
      "Iter-5050 train loss: 2.0618 valid loss: 2.0440, valid accuracy: 0.2930\n",
      "Iter-5060 train loss: 2.0956 valid loss: 2.0437, valid accuracy: 0.2934\n",
      "Iter-5070 train loss: 1.8738 valid loss: 2.0435, valid accuracy: 0.2934\n",
      "Iter-5080 train loss: 2.1989 valid loss: 2.0433, valid accuracy: 0.2936\n",
      "Iter-5090 train loss: 2.0036 valid loss: 2.0430, valid accuracy: 0.2938\n",
      "Iter-5100 train loss: 1.9980 valid loss: 2.0428, valid accuracy: 0.2936\n",
      "Iter-5110 train loss: 2.0871 valid loss: 2.0426, valid accuracy: 0.2934\n",
      "Iter-5120 train loss: 2.0346 valid loss: 2.0424, valid accuracy: 0.2936\n",
      "Iter-5130 train loss: 2.0852 valid loss: 2.0422, valid accuracy: 0.2940\n",
      "Iter-5140 train loss: 2.1280 valid loss: 2.0419, valid accuracy: 0.2942\n",
      "Iter-5150 train loss: 2.0563 valid loss: 2.0417, valid accuracy: 0.2940\n",
      "Iter-5160 train loss: 1.9600 valid loss: 2.0414, valid accuracy: 0.2938\n",
      "Iter-5170 train loss: 2.1392 valid loss: 2.0412, valid accuracy: 0.2940\n",
      "Iter-5180 train loss: 2.1145 valid loss: 2.0409, valid accuracy: 0.2942\n",
      "Iter-5190 train loss: 2.1303 valid loss: 2.0407, valid accuracy: 0.2940\n",
      "Iter-5200 train loss: 1.9869 valid loss: 2.0405, valid accuracy: 0.2940\n",
      "Iter-5210 train loss: 2.1272 valid loss: 2.0402, valid accuracy: 0.2938\n",
      "Iter-5220 train loss: 2.0212 valid loss: 2.0400, valid accuracy: 0.2936\n",
      "Iter-5230 train loss: 2.1462 valid loss: 2.0398, valid accuracy: 0.2936\n",
      "Iter-5240 train loss: 2.0911 valid loss: 2.0395, valid accuracy: 0.2936\n",
      "Iter-5250 train loss: 2.0629 valid loss: 2.0393, valid accuracy: 0.2940\n",
      "Iter-5260 train loss: 2.0484 valid loss: 2.0391, valid accuracy: 0.2938\n",
      "Iter-5270 train loss: 2.0758 valid loss: 2.0389, valid accuracy: 0.2936\n",
      "Iter-5280 train loss: 2.0738 valid loss: 2.0386, valid accuracy: 0.2934\n",
      "Iter-5290 train loss: 2.0566 valid loss: 2.0384, valid accuracy: 0.2938\n",
      "Iter-5300 train loss: 2.0957 valid loss: 2.0382, valid accuracy: 0.2940\n",
      "Iter-5310 train loss: 2.0571 valid loss: 2.0380, valid accuracy: 0.2940\n",
      "Iter-5320 train loss: 2.0421 valid loss: 2.0378, valid accuracy: 0.2938\n",
      "Iter-5330 train loss: 2.1296 valid loss: 2.0376, valid accuracy: 0.2940\n",
      "Iter-5340 train loss: 2.0333 valid loss: 2.0374, valid accuracy: 0.2940\n",
      "Iter-5350 train loss: 2.1362 valid loss: 2.0372, valid accuracy: 0.2940\n",
      "Iter-5360 train loss: 1.9764 valid loss: 2.0370, valid accuracy: 0.2942\n",
      "Iter-5370 train loss: 2.0535 valid loss: 2.0368, valid accuracy: 0.2944\n",
      "Iter-5380 train loss: 2.0058 valid loss: 2.0365, valid accuracy: 0.2948\n",
      "Iter-5390 train loss: 2.0634 valid loss: 2.0363, valid accuracy: 0.2948\n",
      "Iter-5400 train loss: 2.0861 valid loss: 2.0361, valid accuracy: 0.2942\n",
      "Iter-5410 train loss: 2.1432 valid loss: 2.0358, valid accuracy: 0.2948\n",
      "Iter-5420 train loss: 2.0188 valid loss: 2.0356, valid accuracy: 0.2948\n",
      "Iter-5430 train loss: 2.0544 valid loss: 2.0354, valid accuracy: 0.2948\n",
      "Iter-5440 train loss: 2.0844 valid loss: 2.0352, valid accuracy: 0.2944\n",
      "Iter-5450 train loss: 2.0740 valid loss: 2.0349, valid accuracy: 0.2946\n",
      "Iter-5460 train loss: 2.0212 valid loss: 2.0347, valid accuracy: 0.2944\n",
      "Iter-5470 train loss: 2.0983 valid loss: 2.0345, valid accuracy: 0.2944\n",
      "Iter-5480 train loss: 2.0926 valid loss: 2.0342, valid accuracy: 0.2944\n",
      "Iter-5490 train loss: 2.0388 valid loss: 2.0340, valid accuracy: 0.2944\n",
      "Iter-5500 train loss: 2.1289 valid loss: 2.0337, valid accuracy: 0.2936\n",
      "Iter-5510 train loss: 1.9551 valid loss: 2.0335, valid accuracy: 0.2938\n",
      "Iter-5520 train loss: 1.9630 valid loss: 2.0333, valid accuracy: 0.2934\n",
      "Iter-5530 train loss: 2.0630 valid loss: 2.0330, valid accuracy: 0.2938\n",
      "Iter-5540 train loss: 2.1992 valid loss: 2.0329, valid accuracy: 0.2934\n",
      "Iter-5550 train loss: 2.1086 valid loss: 2.0326, valid accuracy: 0.2938\n",
      "Iter-5560 train loss: 2.0354 valid loss: 2.0324, valid accuracy: 0.2942\n",
      "Iter-5570 train loss: 2.0132 valid loss: 2.0322, valid accuracy: 0.2944\n",
      "Iter-5580 train loss: 2.1447 valid loss: 2.0320, valid accuracy: 0.2940\n",
      "Iter-5590 train loss: 2.1428 valid loss: 2.0317, valid accuracy: 0.2942\n",
      "Iter-5600 train loss: 2.0871 valid loss: 2.0315, valid accuracy: 0.2944\n",
      "Iter-5610 train loss: 2.1287 valid loss: 2.0313, valid accuracy: 0.2944\n",
      "Iter-5620 train loss: 1.9670 valid loss: 2.0310, valid accuracy: 0.2946\n",
      "Iter-5630 train loss: 2.1437 valid loss: 2.0308, valid accuracy: 0.2944\n",
      "Iter-5640 train loss: 1.8923 valid loss: 2.0306, valid accuracy: 0.2940\n",
      "Iter-5650 train loss: 2.0729 valid loss: 2.0304, valid accuracy: 0.2938\n",
      "Iter-5660 train loss: 1.9978 valid loss: 2.0302, valid accuracy: 0.2942\n",
      "Iter-5670 train loss: 2.1498 valid loss: 2.0299, valid accuracy: 0.2944\n",
      "Iter-5680 train loss: 2.0234 valid loss: 2.0297, valid accuracy: 0.2944\n",
      "Iter-5690 train loss: 2.0209 valid loss: 2.0295, valid accuracy: 0.2944\n",
      "Iter-5700 train loss: 2.0227 valid loss: 2.0292, valid accuracy: 0.2942\n",
      "Iter-5710 train loss: 2.0824 valid loss: 2.0290, valid accuracy: 0.2942\n",
      "Iter-5720 train loss: 2.2067 valid loss: 2.0288, valid accuracy: 0.2942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5730 train loss: 2.0634 valid loss: 2.0286, valid accuracy: 0.2938\n",
      "Iter-5740 train loss: 2.1244 valid loss: 2.0284, valid accuracy: 0.2940\n",
      "Iter-5750 train loss: 1.9383 valid loss: 2.0282, valid accuracy: 0.2944\n",
      "Iter-5760 train loss: 2.0423 valid loss: 2.0280, valid accuracy: 0.2946\n",
      "Iter-5770 train loss: 2.0134 valid loss: 2.0278, valid accuracy: 0.2944\n",
      "Iter-5780 train loss: 2.1217 valid loss: 2.0276, valid accuracy: 0.2942\n",
      "Iter-5790 train loss: 2.0149 valid loss: 2.0274, valid accuracy: 0.2940\n",
      "Iter-5800 train loss: 1.9943 valid loss: 2.0272, valid accuracy: 0.2946\n",
      "Iter-5810 train loss: 2.0829 valid loss: 2.0270, valid accuracy: 0.2948\n",
      "Iter-5820 train loss: 2.0955 valid loss: 2.0268, valid accuracy: 0.2948\n",
      "Iter-5830 train loss: 2.0025 valid loss: 2.0267, valid accuracy: 0.2948\n",
      "Iter-5840 train loss: 2.0044 valid loss: 2.0265, valid accuracy: 0.2946\n",
      "Iter-5850 train loss: 1.9987 valid loss: 2.0263, valid accuracy: 0.2950\n",
      "Iter-5860 train loss: 2.1796 valid loss: 2.0261, valid accuracy: 0.2950\n",
      "Iter-5870 train loss: 2.0434 valid loss: 2.0259, valid accuracy: 0.2948\n",
      "Iter-5880 train loss: 2.0096 valid loss: 2.0257, valid accuracy: 0.2950\n",
      "Iter-5890 train loss: 2.0253 valid loss: 2.0254, valid accuracy: 0.2946\n",
      "Iter-5900 train loss: 1.8752 valid loss: 2.0252, valid accuracy: 0.2944\n",
      "Iter-5910 train loss: 2.1402 valid loss: 2.0251, valid accuracy: 0.2946\n",
      "Iter-5920 train loss: 2.0782 valid loss: 2.0249, valid accuracy: 0.2944\n",
      "Iter-5930 train loss: 2.1367 valid loss: 2.0247, valid accuracy: 0.2944\n",
      "Iter-5940 train loss: 1.9890 valid loss: 2.0245, valid accuracy: 0.2942\n",
      "Iter-5950 train loss: 2.1595 valid loss: 2.0243, valid accuracy: 0.2944\n",
      "Iter-5960 train loss: 1.9693 valid loss: 2.0242, valid accuracy: 0.2946\n",
      "Iter-5970 train loss: 1.9822 valid loss: 2.0240, valid accuracy: 0.2946\n",
      "Iter-5980 train loss: 2.0686 valid loss: 2.0238, valid accuracy: 0.2946\n",
      "Iter-5990 train loss: 2.1140 valid loss: 2.0235, valid accuracy: 0.2946\n",
      "Iter-6000 train loss: 1.9958 valid loss: 2.0234, valid accuracy: 0.2952\n",
      "Iter-6010 train loss: 1.9880 valid loss: 2.0231, valid accuracy: 0.2952\n",
      "Iter-6020 train loss: 2.1195 valid loss: 2.0229, valid accuracy: 0.2952\n",
      "Iter-6030 train loss: 2.1514 valid loss: 2.0227, valid accuracy: 0.2956\n",
      "Iter-6040 train loss: 2.0283 valid loss: 2.0225, valid accuracy: 0.2954\n",
      "Iter-6050 train loss: 1.9458 valid loss: 2.0223, valid accuracy: 0.2958\n",
      "Iter-6060 train loss: 2.0938 valid loss: 2.0221, valid accuracy: 0.2958\n",
      "Iter-6070 train loss: 2.0947 valid loss: 2.0219, valid accuracy: 0.2956\n",
      "Iter-6080 train loss: 2.0766 valid loss: 2.0217, valid accuracy: 0.2956\n",
      "Iter-6090 train loss: 1.9510 valid loss: 2.0215, valid accuracy: 0.2958\n",
      "Iter-6100 train loss: 2.1571 valid loss: 2.0214, valid accuracy: 0.2958\n",
      "Iter-6110 train loss: 2.0244 valid loss: 2.0212, valid accuracy: 0.2958\n",
      "Iter-6120 train loss: 2.0841 valid loss: 2.0210, valid accuracy: 0.2958\n",
      "Iter-6130 train loss: 2.0260 valid loss: 2.0208, valid accuracy: 0.2958\n",
      "Iter-6140 train loss: 2.0839 valid loss: 2.0207, valid accuracy: 0.2960\n",
      "Iter-6150 train loss: 2.0250 valid loss: 2.0205, valid accuracy: 0.2960\n",
      "Iter-6160 train loss: 2.0954 valid loss: 2.0203, valid accuracy: 0.2958\n",
      "Iter-6170 train loss: 2.0614 valid loss: 2.0201, valid accuracy: 0.2958\n",
      "Iter-6180 train loss: 2.0596 valid loss: 2.0199, valid accuracy: 0.2958\n",
      "Iter-6190 train loss: 2.0908 valid loss: 2.0198, valid accuracy: 0.2960\n",
      "Iter-6200 train loss: 2.0417 valid loss: 2.0196, valid accuracy: 0.2960\n",
      "Iter-6210 train loss: 2.1139 valid loss: 2.0194, valid accuracy: 0.2960\n",
      "Iter-6220 train loss: 2.1211 valid loss: 2.0192, valid accuracy: 0.2960\n",
      "Iter-6230 train loss: 2.1959 valid loss: 2.0191, valid accuracy: 0.2962\n",
      "Iter-6240 train loss: 2.1599 valid loss: 2.0189, valid accuracy: 0.2958\n",
      "Iter-6250 train loss: 2.0439 valid loss: 2.0187, valid accuracy: 0.2958\n",
      "Iter-6260 train loss: 1.8939 valid loss: 2.0186, valid accuracy: 0.2958\n",
      "Iter-6270 train loss: 2.0165 valid loss: 2.0184, valid accuracy: 0.2958\n",
      "Iter-6280 train loss: 1.9255 valid loss: 2.0182, valid accuracy: 0.2962\n",
      "Iter-6290 train loss: 2.0894 valid loss: 2.0180, valid accuracy: 0.2958\n",
      "Iter-6300 train loss: 2.0219 valid loss: 2.0179, valid accuracy: 0.2964\n",
      "Iter-6310 train loss: 2.0823 valid loss: 2.0177, valid accuracy: 0.2960\n",
      "Iter-6320 train loss: 1.8869 valid loss: 2.0175, valid accuracy: 0.2962\n",
      "Iter-6330 train loss: 1.9213 valid loss: 2.0173, valid accuracy: 0.2964\n",
      "Iter-6340 train loss: 1.9094 valid loss: 2.0171, valid accuracy: 0.2960\n",
      "Iter-6350 train loss: 1.9671 valid loss: 2.0170, valid accuracy: 0.2962\n",
      "Iter-6360 train loss: 1.8681 valid loss: 2.0168, valid accuracy: 0.2960\n",
      "Iter-6370 train loss: 2.0726 valid loss: 2.0166, valid accuracy: 0.2960\n",
      "Iter-6380 train loss: 2.0027 valid loss: 2.0164, valid accuracy: 0.2960\n",
      "Iter-6390 train loss: 2.0374 valid loss: 2.0163, valid accuracy: 0.2962\n",
      "Iter-6400 train loss: 2.0585 valid loss: 2.0161, valid accuracy: 0.2960\n",
      "Iter-6410 train loss: 1.9365 valid loss: 2.0158, valid accuracy: 0.2960\n",
      "Iter-6420 train loss: 2.1677 valid loss: 2.0157, valid accuracy: 0.2958\n",
      "Iter-6430 train loss: 2.0808 valid loss: 2.0155, valid accuracy: 0.2960\n",
      "Iter-6440 train loss: 1.9856 valid loss: 2.0153, valid accuracy: 0.2960\n",
      "Iter-6450 train loss: 1.9810 valid loss: 2.0151, valid accuracy: 0.2960\n",
      "Iter-6460 train loss: 2.0392 valid loss: 2.0149, valid accuracy: 0.2960\n",
      "Iter-6470 train loss: 1.9789 valid loss: 2.0147, valid accuracy: 0.2960\n",
      "Iter-6480 train loss: 2.1548 valid loss: 2.0146, valid accuracy: 0.2960\n",
      "Iter-6490 train loss: 2.1973 valid loss: 2.0144, valid accuracy: 0.2956\n",
      "Iter-6500 train loss: 2.0106 valid loss: 2.0143, valid accuracy: 0.2956\n",
      "Iter-6510 train loss: 2.0099 valid loss: 2.0141, valid accuracy: 0.2958\n",
      "Iter-6520 train loss: 1.9814 valid loss: 2.0139, valid accuracy: 0.2952\n",
      "Iter-6530 train loss: 1.9716 valid loss: 2.0137, valid accuracy: 0.2948\n",
      "Iter-6540 train loss: 2.0973 valid loss: 2.0136, valid accuracy: 0.2950\n",
      "Iter-6550 train loss: 1.9290 valid loss: 2.0134, valid accuracy: 0.2946\n",
      "Iter-6560 train loss: 1.8593 valid loss: 2.0133, valid accuracy: 0.2952\n",
      "Iter-6570 train loss: 1.9480 valid loss: 2.0131, valid accuracy: 0.2948\n",
      "Iter-6580 train loss: 2.0404 valid loss: 2.0129, valid accuracy: 0.2952\n",
      "Iter-6590 train loss: 2.0167 valid loss: 2.0127, valid accuracy: 0.2950\n",
      "Iter-6600 train loss: 1.9823 valid loss: 2.0125, valid accuracy: 0.2948\n",
      "Iter-6610 train loss: 2.0366 valid loss: 2.0123, valid accuracy: 0.2950\n",
      "Iter-6620 train loss: 1.9905 valid loss: 2.0122, valid accuracy: 0.2950\n",
      "Iter-6630 train loss: 1.9111 valid loss: 2.0120, valid accuracy: 0.2946\n",
      "Iter-6640 train loss: 1.9841 valid loss: 2.0118, valid accuracy: 0.2942\n",
      "Iter-6650 train loss: 2.1914 valid loss: 2.0117, valid accuracy: 0.2942\n",
      "Iter-6660 train loss: 1.9117 valid loss: 2.0115, valid accuracy: 0.2942\n",
      "Iter-6670 train loss: 2.0187 valid loss: 2.0113, valid accuracy: 0.2942\n",
      "Iter-6680 train loss: 2.0586 valid loss: 2.0112, valid accuracy: 0.2946\n",
      "Iter-6690 train loss: 1.9390 valid loss: 2.0110, valid accuracy: 0.2944\n",
      "Iter-6700 train loss: 2.0320 valid loss: 2.0108, valid accuracy: 0.2944\n",
      "Iter-6710 train loss: 1.9897 valid loss: 2.0107, valid accuracy: 0.2946\n",
      "Iter-6720 train loss: 2.0009 valid loss: 2.0105, valid accuracy: 0.2944\n",
      "Iter-6730 train loss: 1.9846 valid loss: 2.0103, valid accuracy: 0.2944\n",
      "Iter-6740 train loss: 1.9414 valid loss: 2.0101, valid accuracy: 0.2944\n",
      "Iter-6750 train loss: 2.1859 valid loss: 2.0099, valid accuracy: 0.2944\n",
      "Iter-6760 train loss: 1.9683 valid loss: 2.0098, valid accuracy: 0.2946\n",
      "Iter-6770 train loss: 2.1780 valid loss: 2.0096, valid accuracy: 0.2948\n",
      "Iter-6780 train loss: 2.1280 valid loss: 2.0095, valid accuracy: 0.2946\n",
      "Iter-6790 train loss: 2.0496 valid loss: 2.0093, valid accuracy: 0.2946\n",
      "Iter-6800 train loss: 1.9963 valid loss: 2.0092, valid accuracy: 0.2950\n",
      "Iter-6810 train loss: 1.8974 valid loss: 2.0090, valid accuracy: 0.2948\n",
      "Iter-6820 train loss: 1.9538 valid loss: 2.0088, valid accuracy: 0.2946\n",
      "Iter-6830 train loss: 2.0076 valid loss: 2.0087, valid accuracy: 0.2944\n",
      "Iter-6840 train loss: 2.0270 valid loss: 2.0085, valid accuracy: 0.2944\n",
      "Iter-6850 train loss: 1.9764 valid loss: 2.0084, valid accuracy: 0.2948\n",
      "Iter-6860 train loss: 2.0829 valid loss: 2.0082, valid accuracy: 0.2946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6870 train loss: 2.0968 valid loss: 2.0080, valid accuracy: 0.2946\n",
      "Iter-6880 train loss: 2.0379 valid loss: 2.0079, valid accuracy: 0.2948\n",
      "Iter-6890 train loss: 1.8961 valid loss: 2.0077, valid accuracy: 0.2944\n",
      "Iter-6900 train loss: 2.0299 valid loss: 2.0076, valid accuracy: 0.2944\n",
      "Iter-6910 train loss: 2.0638 valid loss: 2.0074, valid accuracy: 0.2946\n",
      "Iter-6920 train loss: 2.1104 valid loss: 2.0073, valid accuracy: 0.2944\n",
      "Iter-6930 train loss: 1.9736 valid loss: 2.0071, valid accuracy: 0.2946\n",
      "Iter-6940 train loss: 2.1613 valid loss: 2.0070, valid accuracy: 0.2946\n",
      "Iter-6950 train loss: 2.1469 valid loss: 2.0068, valid accuracy: 0.2946\n",
      "Iter-6960 train loss: 1.9916 valid loss: 2.0066, valid accuracy: 0.2946\n",
      "Iter-6970 train loss: 1.8807 valid loss: 2.0065, valid accuracy: 0.2946\n",
      "Iter-6980 train loss: 2.0009 valid loss: 2.0063, valid accuracy: 0.2944\n",
      "Iter-6990 train loss: 2.0369 valid loss: 2.0061, valid accuracy: 0.2946\n",
      "Iter-7000 train loss: 1.9756 valid loss: 2.0060, valid accuracy: 0.2946\n",
      "Iter-7010 train loss: 1.9918 valid loss: 2.0058, valid accuracy: 0.2946\n",
      "Iter-7020 train loss: 1.9241 valid loss: 2.0057, valid accuracy: 0.2946\n",
      "Iter-7030 train loss: 1.9917 valid loss: 2.0055, valid accuracy: 0.2946\n",
      "Iter-7040 train loss: 1.9965 valid loss: 2.0053, valid accuracy: 0.2946\n",
      "Iter-7050 train loss: 2.0075 valid loss: 2.0051, valid accuracy: 0.2946\n",
      "Iter-7060 train loss: 2.1599 valid loss: 2.0050, valid accuracy: 0.2944\n",
      "Iter-7070 train loss: 2.0228 valid loss: 2.0048, valid accuracy: 0.2948\n",
      "Iter-7080 train loss: 2.0425 valid loss: 2.0047, valid accuracy: 0.2944\n",
      "Iter-7090 train loss: 1.9995 valid loss: 2.0045, valid accuracy: 0.2944\n",
      "Iter-7100 train loss: 1.8585 valid loss: 2.0044, valid accuracy: 0.2944\n",
      "Iter-7110 train loss: 2.1410 valid loss: 2.0043, valid accuracy: 0.2944\n",
      "Iter-7120 train loss: 1.9440 valid loss: 2.0041, valid accuracy: 0.2944\n",
      "Iter-7130 train loss: 1.9347 valid loss: 2.0039, valid accuracy: 0.2944\n",
      "Iter-7140 train loss: 1.9062 valid loss: 2.0037, valid accuracy: 0.2944\n",
      "Iter-7150 train loss: 2.0221 valid loss: 2.0036, valid accuracy: 0.2946\n",
      "Iter-7160 train loss: 1.9987 valid loss: 2.0034, valid accuracy: 0.2944\n",
      "Iter-7170 train loss: 2.0617 valid loss: 2.0032, valid accuracy: 0.2946\n",
      "Iter-7180 train loss: 2.0247 valid loss: 2.0030, valid accuracy: 0.2946\n",
      "Iter-7190 train loss: 2.0720 valid loss: 2.0029, valid accuracy: 0.2944\n",
      "Iter-7200 train loss: 2.0890 valid loss: 2.0028, valid accuracy: 0.2944\n",
      "Iter-7210 train loss: 2.0524 valid loss: 2.0026, valid accuracy: 0.2944\n",
      "Iter-7220 train loss: 1.9278 valid loss: 2.0025, valid accuracy: 0.2944\n",
      "Iter-7230 train loss: 2.0026 valid loss: 2.0023, valid accuracy: 0.2944\n",
      "Iter-7240 train loss: 1.9005 valid loss: 2.0022, valid accuracy: 0.2942\n",
      "Iter-7250 train loss: 1.9390 valid loss: 2.0020, valid accuracy: 0.2942\n",
      "Iter-7260 train loss: 2.1320 valid loss: 2.0019, valid accuracy: 0.2942\n",
      "Iter-7270 train loss: 2.0776 valid loss: 2.0018, valid accuracy: 0.2940\n",
      "Iter-7280 train loss: 1.9710 valid loss: 2.0016, valid accuracy: 0.2940\n",
      "Iter-7290 train loss: 2.0052 valid loss: 2.0015, valid accuracy: 0.2942\n",
      "Iter-7300 train loss: 2.0195 valid loss: 2.0013, valid accuracy: 0.2944\n",
      "Iter-7310 train loss: 2.0586 valid loss: 2.0012, valid accuracy: 0.2944\n",
      "Iter-7320 train loss: 2.0369 valid loss: 2.0010, valid accuracy: 0.2952\n",
      "Iter-7330 train loss: 2.0189 valid loss: 2.0009, valid accuracy: 0.2952\n",
      "Iter-7340 train loss: 1.9934 valid loss: 2.0008, valid accuracy: 0.2950\n",
      "Iter-7350 train loss: 2.0610 valid loss: 2.0007, valid accuracy: 0.2952\n",
      "Iter-7360 train loss: 2.0246 valid loss: 2.0005, valid accuracy: 0.2954\n",
      "Iter-7370 train loss: 1.9673 valid loss: 2.0004, valid accuracy: 0.2954\n",
      "Iter-7380 train loss: 1.9741 valid loss: 2.0002, valid accuracy: 0.2954\n",
      "Iter-7390 train loss: 1.9667 valid loss: 2.0001, valid accuracy: 0.2954\n",
      "Iter-7400 train loss: 1.9350 valid loss: 1.9999, valid accuracy: 0.2954\n",
      "Iter-7410 train loss: 2.0233 valid loss: 1.9998, valid accuracy: 0.2956\n",
      "Iter-7420 train loss: 1.9436 valid loss: 1.9996, valid accuracy: 0.2954\n",
      "Iter-7430 train loss: 2.0137 valid loss: 1.9994, valid accuracy: 0.2954\n",
      "Iter-7440 train loss: 2.1235 valid loss: 1.9993, valid accuracy: 0.2954\n",
      "Iter-7450 train loss: 2.0322 valid loss: 1.9992, valid accuracy: 0.2954\n",
      "Iter-7460 train loss: 2.1364 valid loss: 1.9990, valid accuracy: 0.2958\n",
      "Iter-7470 train loss: 2.0968 valid loss: 1.9989, valid accuracy: 0.2952\n",
      "Iter-7480 train loss: 2.0871 valid loss: 1.9987, valid accuracy: 0.2954\n",
      "Iter-7490 train loss: 2.0223 valid loss: 1.9986, valid accuracy: 0.2956\n",
      "Iter-7500 train loss: 2.0092 valid loss: 1.9984, valid accuracy: 0.2956\n",
      "Iter-7510 train loss: 1.8548 valid loss: 1.9983, valid accuracy: 0.2958\n",
      "Iter-7520 train loss: 2.0324 valid loss: 1.9981, valid accuracy: 0.2958\n",
      "Iter-7530 train loss: 2.0414 valid loss: 1.9980, valid accuracy: 0.2960\n",
      "Iter-7540 train loss: 2.1227 valid loss: 1.9978, valid accuracy: 0.2960\n",
      "Iter-7550 train loss: 2.0066 valid loss: 1.9977, valid accuracy: 0.2958\n",
      "Iter-7560 train loss: 2.0932 valid loss: 1.9976, valid accuracy: 0.2962\n",
      "Iter-7570 train loss: 2.0651 valid loss: 1.9974, valid accuracy: 0.2960\n",
      "Iter-7580 train loss: 2.0477 valid loss: 1.9973, valid accuracy: 0.2962\n",
      "Iter-7590 train loss: 1.9790 valid loss: 1.9971, valid accuracy: 0.2964\n",
      "Iter-7600 train loss: 1.9108 valid loss: 1.9970, valid accuracy: 0.2964\n",
      "Iter-7610 train loss: 1.9055 valid loss: 1.9968, valid accuracy: 0.2968\n",
      "Iter-7620 train loss: 2.0543 valid loss: 1.9967, valid accuracy: 0.2966\n",
      "Iter-7630 train loss: 1.9805 valid loss: 1.9965, valid accuracy: 0.2962\n",
      "Iter-7640 train loss: 2.1036 valid loss: 1.9964, valid accuracy: 0.2966\n",
      "Iter-7650 train loss: 2.0169 valid loss: 1.9963, valid accuracy: 0.2966\n",
      "Iter-7660 train loss: 1.9932 valid loss: 1.9961, valid accuracy: 0.2964\n",
      "Iter-7670 train loss: 1.9922 valid loss: 1.9960, valid accuracy: 0.2964\n",
      "Iter-7680 train loss: 1.9493 valid loss: 1.9958, valid accuracy: 0.2966\n",
      "Iter-7690 train loss: 1.9855 valid loss: 1.9957, valid accuracy: 0.2964\n",
      "Iter-7700 train loss: 2.0166 valid loss: 1.9956, valid accuracy: 0.2968\n",
      "Iter-7710 train loss: 2.1131 valid loss: 1.9954, valid accuracy: 0.2970\n",
      "Iter-7720 train loss: 1.8984 valid loss: 1.9953, valid accuracy: 0.2970\n",
      "Iter-7730 train loss: 1.9916 valid loss: 1.9952, valid accuracy: 0.2970\n",
      "Iter-7740 train loss: 1.9298 valid loss: 1.9950, valid accuracy: 0.2970\n",
      "Iter-7750 train loss: 2.1546 valid loss: 1.9949, valid accuracy: 0.2968\n",
      "Iter-7760 train loss: 2.0321 valid loss: 1.9948, valid accuracy: 0.2972\n",
      "Iter-7770 train loss: 2.1225 valid loss: 1.9947, valid accuracy: 0.2968\n",
      "Iter-7780 train loss: 1.9997 valid loss: 1.9946, valid accuracy: 0.2968\n",
      "Iter-7790 train loss: 1.9741 valid loss: 1.9944, valid accuracy: 0.2966\n",
      "Iter-7800 train loss: 2.0398 valid loss: 1.9943, valid accuracy: 0.2966\n",
      "Iter-7810 train loss: 1.9028 valid loss: 1.9941, valid accuracy: 0.2962\n",
      "Iter-7820 train loss: 1.9618 valid loss: 1.9940, valid accuracy: 0.2962\n",
      "Iter-7830 train loss: 1.8638 valid loss: 1.9939, valid accuracy: 0.2962\n",
      "Iter-7840 train loss: 2.0087 valid loss: 1.9938, valid accuracy: 0.2962\n",
      "Iter-7850 train loss: 2.1109 valid loss: 1.9936, valid accuracy: 0.2962\n",
      "Iter-7860 train loss: 2.1490 valid loss: 1.9935, valid accuracy: 0.2962\n",
      "Iter-7870 train loss: 2.1397 valid loss: 1.9934, valid accuracy: 0.2960\n",
      "Iter-7880 train loss: 1.9835 valid loss: 1.9933, valid accuracy: 0.2960\n",
      "Iter-7890 train loss: 2.0153 valid loss: 1.9931, valid accuracy: 0.2962\n",
      "Iter-7900 train loss: 1.9627 valid loss: 1.9930, valid accuracy: 0.2960\n",
      "Iter-7910 train loss: 2.1374 valid loss: 1.9929, valid accuracy: 0.2962\n",
      "Iter-7920 train loss: 2.0607 valid loss: 1.9927, valid accuracy: 0.2962\n",
      "Iter-7930 train loss: 2.0295 valid loss: 1.9926, valid accuracy: 0.2962\n",
      "Iter-7940 train loss: 2.0422 valid loss: 1.9925, valid accuracy: 0.2964\n",
      "Iter-7950 train loss: 2.0828 valid loss: 1.9924, valid accuracy: 0.2962\n",
      "Iter-7960 train loss: 1.9972 valid loss: 1.9922, valid accuracy: 0.2960\n",
      "Iter-7970 train loss: 2.0622 valid loss: 1.9921, valid accuracy: 0.2962\n",
      "Iter-7980 train loss: 2.0706 valid loss: 1.9919, valid accuracy: 0.2964\n",
      "Iter-7990 train loss: 1.8630 valid loss: 1.9918, valid accuracy: 0.2964\n",
      "Iter-8000 train loss: 2.0014 valid loss: 1.9916, valid accuracy: 0.2962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8010 train loss: 1.8861 valid loss: 1.9915, valid accuracy: 0.2962\n",
      "Iter-8020 train loss: 2.0141 valid loss: 1.9914, valid accuracy: 0.2962\n",
      "Iter-8030 train loss: 2.1348 valid loss: 1.9912, valid accuracy: 0.2960\n",
      "Iter-8040 train loss: 2.0543 valid loss: 1.9911, valid accuracy: 0.2958\n",
      "Iter-8050 train loss: 1.9346 valid loss: 1.9910, valid accuracy: 0.2960\n",
      "Iter-8060 train loss: 1.9583 valid loss: 1.9909, valid accuracy: 0.2960\n",
      "Iter-8070 train loss: 2.1651 valid loss: 1.9908, valid accuracy: 0.2960\n",
      "Iter-8080 train loss: 2.0323 valid loss: 1.9906, valid accuracy: 0.2960\n",
      "Iter-8090 train loss: 1.8967 valid loss: 1.9905, valid accuracy: 0.2962\n",
      "Iter-8100 train loss: 2.0395 valid loss: 1.9904, valid accuracy: 0.2962\n",
      "Iter-8110 train loss: 2.1320 valid loss: 1.9902, valid accuracy: 0.2962\n",
      "Iter-8120 train loss: 2.0239 valid loss: 1.9901, valid accuracy: 0.2962\n",
      "Iter-8130 train loss: 1.9522 valid loss: 1.9900, valid accuracy: 0.2960\n",
      "Iter-8140 train loss: 2.0708 valid loss: 1.9898, valid accuracy: 0.2960\n",
      "Iter-8150 train loss: 1.9957 valid loss: 1.9897, valid accuracy: 0.2962\n",
      "Iter-8160 train loss: 2.0537 valid loss: 1.9896, valid accuracy: 0.2964\n",
      "Iter-8170 train loss: 1.8560 valid loss: 1.9895, valid accuracy: 0.2962\n",
      "Iter-8180 train loss: 1.9684 valid loss: 1.9893, valid accuracy: 0.2962\n",
      "Iter-8190 train loss: 2.1780 valid loss: 1.9892, valid accuracy: 0.2962\n",
      "Iter-8200 train loss: 1.8932 valid loss: 1.9891, valid accuracy: 0.2966\n",
      "Iter-8210 train loss: 2.0692 valid loss: 1.9890, valid accuracy: 0.2966\n",
      "Iter-8220 train loss: 2.0616 valid loss: 1.9888, valid accuracy: 0.2966\n",
      "Iter-8230 train loss: 2.1186 valid loss: 1.9887, valid accuracy: 0.2968\n",
      "Iter-8240 train loss: 1.9920 valid loss: 1.9886, valid accuracy: 0.2970\n",
      "Iter-8250 train loss: 2.1077 valid loss: 1.9885, valid accuracy: 0.2970\n",
      "Iter-8260 train loss: 1.9644 valid loss: 1.9883, valid accuracy: 0.2972\n",
      "Iter-8270 train loss: 1.9694 valid loss: 1.9882, valid accuracy: 0.2972\n",
      "Iter-8280 train loss: 1.9740 valid loss: 1.9881, valid accuracy: 0.2972\n",
      "Iter-8290 train loss: 1.9677 valid loss: 1.9880, valid accuracy: 0.2968\n",
      "Iter-8300 train loss: 2.0445 valid loss: 1.9879, valid accuracy: 0.2972\n",
      "Iter-8310 train loss: 1.9766 valid loss: 1.9877, valid accuracy: 0.2974\n",
      "Iter-8320 train loss: 2.0761 valid loss: 1.9876, valid accuracy: 0.2978\n",
      "Iter-8330 train loss: 2.0289 valid loss: 1.9875, valid accuracy: 0.2976\n",
      "Iter-8340 train loss: 2.0466 valid loss: 1.9874, valid accuracy: 0.2976\n",
      "Iter-8350 train loss: 1.9200 valid loss: 1.9872, valid accuracy: 0.2976\n",
      "Iter-8360 train loss: 1.9916 valid loss: 1.9872, valid accuracy: 0.2970\n",
      "Iter-8370 train loss: 1.9675 valid loss: 1.9870, valid accuracy: 0.2970\n",
      "Iter-8380 train loss: 1.9804 valid loss: 1.9869, valid accuracy: 0.2970\n",
      "Iter-8390 train loss: 2.2396 valid loss: 1.9868, valid accuracy: 0.2970\n",
      "Iter-8400 train loss: 1.9041 valid loss: 1.9867, valid accuracy: 0.2972\n",
      "Iter-8410 train loss: 1.9468 valid loss: 1.9866, valid accuracy: 0.2970\n",
      "Iter-8420 train loss: 2.0684 valid loss: 1.9865, valid accuracy: 0.2970\n",
      "Iter-8430 train loss: 1.9770 valid loss: 1.9863, valid accuracy: 0.2970\n",
      "Iter-8440 train loss: 1.9891 valid loss: 1.9862, valid accuracy: 0.2966\n",
      "Iter-8450 train loss: 2.1857 valid loss: 1.9861, valid accuracy: 0.2964\n",
      "Iter-8460 train loss: 2.0507 valid loss: 1.9860, valid accuracy: 0.2966\n",
      "Iter-8470 train loss: 2.0388 valid loss: 1.9859, valid accuracy: 0.2966\n",
      "Iter-8480 train loss: 1.9444 valid loss: 1.9857, valid accuracy: 0.2964\n",
      "Iter-8490 train loss: 1.8933 valid loss: 1.9856, valid accuracy: 0.2966\n",
      "Iter-8500 train loss: 1.9164 valid loss: 1.9855, valid accuracy: 0.2966\n",
      "Iter-8510 train loss: 1.9686 valid loss: 1.9853, valid accuracy: 0.2966\n",
      "Iter-8520 train loss: 1.9407 valid loss: 1.9852, valid accuracy: 0.2966\n",
      "Iter-8530 train loss: 2.0351 valid loss: 1.9851, valid accuracy: 0.2966\n",
      "Iter-8540 train loss: 2.1537 valid loss: 1.9850, valid accuracy: 0.2968\n",
      "Iter-8550 train loss: 2.0173 valid loss: 1.9849, valid accuracy: 0.2966\n",
      "Iter-8560 train loss: 2.0733 valid loss: 1.9848, valid accuracy: 0.2966\n",
      "Iter-8570 train loss: 1.9491 valid loss: 1.9846, valid accuracy: 0.2964\n",
      "Iter-8580 train loss: 2.0491 valid loss: 1.9845, valid accuracy: 0.2966\n",
      "Iter-8590 train loss: 2.0427 valid loss: 1.9844, valid accuracy: 0.2966\n",
      "Iter-8600 train loss: 2.0495 valid loss: 1.9843, valid accuracy: 0.2964\n",
      "Iter-8610 train loss: 1.9885 valid loss: 1.9842, valid accuracy: 0.2964\n",
      "Iter-8620 train loss: 1.9108 valid loss: 1.9840, valid accuracy: 0.2966\n",
      "Iter-8630 train loss: 2.0529 valid loss: 1.9839, valid accuracy: 0.2970\n",
      "Iter-8640 train loss: 2.0677 valid loss: 1.9838, valid accuracy: 0.2970\n",
      "Iter-8650 train loss: 2.0305 valid loss: 1.9837, valid accuracy: 0.2968\n",
      "Iter-8660 train loss: 1.8623 valid loss: 1.9835, valid accuracy: 0.2966\n",
      "Iter-8670 train loss: 2.1165 valid loss: 1.9834, valid accuracy: 0.2970\n",
      "Iter-8680 train loss: 2.0675 valid loss: 1.9834, valid accuracy: 0.2968\n",
      "Iter-8690 train loss: 2.0309 valid loss: 1.9832, valid accuracy: 0.2968\n",
      "Iter-8700 train loss: 1.9139 valid loss: 1.9831, valid accuracy: 0.2968\n",
      "Iter-8710 train loss: 1.9677 valid loss: 1.9830, valid accuracy: 0.2968\n",
      "Iter-8720 train loss: 1.9782 valid loss: 1.9829, valid accuracy: 0.2966\n",
      "Iter-8730 train loss: 2.0444 valid loss: 1.9828, valid accuracy: 0.2968\n",
      "Iter-8740 train loss: 2.0388 valid loss: 1.9827, valid accuracy: 0.2970\n",
      "Iter-8750 train loss: 1.9283 valid loss: 1.9826, valid accuracy: 0.2968\n",
      "Iter-8760 train loss: 2.1200 valid loss: 1.9824, valid accuracy: 0.2970\n",
      "Iter-8770 train loss: 1.9534 valid loss: 1.9823, valid accuracy: 0.2970\n",
      "Iter-8780 train loss: 1.8580 valid loss: 1.9822, valid accuracy: 0.2966\n",
      "Iter-8790 train loss: 2.0677 valid loss: 1.9821, valid accuracy: 0.2970\n",
      "Iter-8800 train loss: 1.9226 valid loss: 1.9820, valid accuracy: 0.2972\n",
      "Iter-8810 train loss: 1.9776 valid loss: 1.9819, valid accuracy: 0.2972\n",
      "Iter-8820 train loss: 1.8959 valid loss: 1.9818, valid accuracy: 0.2968\n",
      "Iter-8830 train loss: 1.8851 valid loss: 1.9817, valid accuracy: 0.2968\n",
      "Iter-8840 train loss: 1.9318 valid loss: 1.9816, valid accuracy: 0.2968\n",
      "Iter-8850 train loss: 1.9247 valid loss: 1.9814, valid accuracy: 0.2968\n",
      "Iter-8860 train loss: 1.9987 valid loss: 1.9813, valid accuracy: 0.2966\n",
      "Iter-8870 train loss: 2.0091 valid loss: 1.9812, valid accuracy: 0.2968\n",
      "Iter-8880 train loss: 1.9930 valid loss: 1.9811, valid accuracy: 0.2968\n",
      "Iter-8890 train loss: 2.0531 valid loss: 1.9810, valid accuracy: 0.2970\n",
      "Iter-8900 train loss: 2.0593 valid loss: 1.9809, valid accuracy: 0.2968\n",
      "Iter-8910 train loss: 1.8447 valid loss: 1.9807, valid accuracy: 0.2970\n",
      "Iter-8920 train loss: 1.9775 valid loss: 1.9806, valid accuracy: 0.2968\n",
      "Iter-8930 train loss: 1.9923 valid loss: 1.9805, valid accuracy: 0.2974\n",
      "Iter-8940 train loss: 2.0634 valid loss: 1.9804, valid accuracy: 0.2974\n",
      "Iter-8950 train loss: 2.0099 valid loss: 1.9803, valid accuracy: 0.2974\n",
      "Iter-8960 train loss: 2.0121 valid loss: 1.9802, valid accuracy: 0.2974\n",
      "Iter-8970 train loss: 2.0663 valid loss: 1.9801, valid accuracy: 0.2976\n",
      "Iter-8980 train loss: 2.0661 valid loss: 1.9800, valid accuracy: 0.2976\n",
      "Iter-8990 train loss: 2.0826 valid loss: 1.9799, valid accuracy: 0.2978\n",
      "Iter-9000 train loss: 2.0200 valid loss: 1.9798, valid accuracy: 0.2978\n",
      "Iter-9010 train loss: 2.0858 valid loss: 1.9797, valid accuracy: 0.2980\n",
      "Iter-9020 train loss: 2.0169 valid loss: 1.9796, valid accuracy: 0.2980\n",
      "Iter-9030 train loss: 2.0204 valid loss: 1.9795, valid accuracy: 0.2980\n",
      "Iter-9040 train loss: 2.0419 valid loss: 1.9793, valid accuracy: 0.2980\n",
      "Iter-9050 train loss: 1.8801 valid loss: 1.9792, valid accuracy: 0.2980\n",
      "Iter-9060 train loss: 2.0050 valid loss: 1.9791, valid accuracy: 0.2980\n",
      "Iter-9070 train loss: 1.8830 valid loss: 1.9790, valid accuracy: 0.2980\n",
      "Iter-9080 train loss: 2.0206 valid loss: 1.9789, valid accuracy: 0.2980\n",
      "Iter-9090 train loss: 2.0000 valid loss: 1.9787, valid accuracy: 0.2980\n",
      "Iter-9100 train loss: 2.0286 valid loss: 1.9786, valid accuracy: 0.2978\n",
      "Iter-9110 train loss: 2.0932 valid loss: 1.9785, valid accuracy: 0.2978\n",
      "Iter-9120 train loss: 2.0728 valid loss: 1.9784, valid accuracy: 0.2978\n",
      "Iter-9130 train loss: 1.9517 valid loss: 1.9783, valid accuracy: 0.2976\n",
      "Iter-9140 train loss: 1.8084 valid loss: 1.9781, valid accuracy: 0.2976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9150 train loss: 2.0178 valid loss: 1.9781, valid accuracy: 0.2976\n",
      "Iter-9160 train loss: 2.1233 valid loss: 1.9780, valid accuracy: 0.2976\n",
      "Iter-9170 train loss: 1.9300 valid loss: 1.9778, valid accuracy: 0.2976\n",
      "Iter-9180 train loss: 2.0141 valid loss: 1.9777, valid accuracy: 0.2976\n",
      "Iter-9190 train loss: 1.9876 valid loss: 1.9776, valid accuracy: 0.2978\n",
      "Iter-9200 train loss: 2.1200 valid loss: 1.9775, valid accuracy: 0.2980\n",
      "Iter-9210 train loss: 2.0696 valid loss: 1.9773, valid accuracy: 0.2980\n",
      "Iter-9220 train loss: 2.0354 valid loss: 1.9772, valid accuracy: 0.2980\n",
      "Iter-9230 train loss: 1.8050 valid loss: 1.9771, valid accuracy: 0.2980\n",
      "Iter-9240 train loss: 1.9512 valid loss: 1.9770, valid accuracy: 0.2978\n",
      "Iter-9250 train loss: 1.9841 valid loss: 1.9769, valid accuracy: 0.2982\n",
      "Iter-9260 train loss: 2.0197 valid loss: 1.9768, valid accuracy: 0.2982\n",
      "Iter-9270 train loss: 2.0418 valid loss: 1.9767, valid accuracy: 0.2982\n",
      "Iter-9280 train loss: 2.1022 valid loss: 1.9766, valid accuracy: 0.2984\n",
      "Iter-9290 train loss: 2.0958 valid loss: 1.9765, valid accuracy: 0.2984\n",
      "Iter-9300 train loss: 2.1250 valid loss: 1.9764, valid accuracy: 0.2986\n",
      "Iter-9310 train loss: 2.0520 valid loss: 1.9763, valid accuracy: 0.2986\n",
      "Iter-9320 train loss: 1.9093 valid loss: 1.9762, valid accuracy: 0.2986\n",
      "Iter-9330 train loss: 1.9181 valid loss: 1.9761, valid accuracy: 0.2982\n",
      "Iter-9340 train loss: 1.8826 valid loss: 1.9760, valid accuracy: 0.2984\n",
      "Iter-9350 train loss: 1.9197 valid loss: 1.9759, valid accuracy: 0.2982\n",
      "Iter-9360 train loss: 2.0400 valid loss: 1.9757, valid accuracy: 0.2982\n",
      "Iter-9370 train loss: 1.9574 valid loss: 1.9756, valid accuracy: 0.2984\n",
      "Iter-9380 train loss: 1.9709 valid loss: 1.9755, valid accuracy: 0.2984\n",
      "Iter-9390 train loss: 1.9695 valid loss: 1.9754, valid accuracy: 0.2984\n",
      "Iter-9400 train loss: 1.8974 valid loss: 1.9754, valid accuracy: 0.2984\n",
      "Iter-9410 train loss: 1.9853 valid loss: 1.9753, valid accuracy: 0.2984\n",
      "Iter-9420 train loss: 2.1244 valid loss: 1.9751, valid accuracy: 0.2984\n",
      "Iter-9430 train loss: 1.9445 valid loss: 1.9750, valid accuracy: 0.2984\n",
      "Iter-9440 train loss: 2.0046 valid loss: 1.9750, valid accuracy: 0.2984\n",
      "Iter-9450 train loss: 1.9599 valid loss: 1.9748, valid accuracy: 0.2984\n",
      "Iter-9460 train loss: 1.9203 valid loss: 1.9747, valid accuracy: 0.2986\n",
      "Iter-9470 train loss: 1.9481 valid loss: 1.9747, valid accuracy: 0.2986\n",
      "Iter-9480 train loss: 1.9229 valid loss: 1.9745, valid accuracy: 0.2986\n",
      "Iter-9490 train loss: 2.0027 valid loss: 1.9744, valid accuracy: 0.2986\n",
      "Iter-9500 train loss: 1.9933 valid loss: 1.9743, valid accuracy: 0.2984\n",
      "Iter-9510 train loss: 1.9582 valid loss: 1.9743, valid accuracy: 0.2984\n",
      "Iter-9520 train loss: 2.0775 valid loss: 1.9742, valid accuracy: 0.2982\n",
      "Iter-9530 train loss: 2.0035 valid loss: 1.9741, valid accuracy: 0.2982\n",
      "Iter-9540 train loss: 2.0116 valid loss: 1.9740, valid accuracy: 0.2984\n",
      "Iter-9550 train loss: 2.1208 valid loss: 1.9739, valid accuracy: 0.2984\n",
      "Iter-9560 train loss: 1.9869 valid loss: 1.9737, valid accuracy: 0.2986\n",
      "Iter-9570 train loss: 2.0928 valid loss: 1.9736, valid accuracy: 0.2986\n",
      "Iter-9580 train loss: 2.0008 valid loss: 1.9736, valid accuracy: 0.2988\n",
      "Iter-9590 train loss: 1.9862 valid loss: 1.9735, valid accuracy: 0.2988\n",
      "Iter-9600 train loss: 2.0484 valid loss: 1.9734, valid accuracy: 0.2988\n",
      "Iter-9610 train loss: 1.9533 valid loss: 1.9732, valid accuracy: 0.2986\n",
      "Iter-9620 train loss: 2.0435 valid loss: 1.9732, valid accuracy: 0.2988\n",
      "Iter-9630 train loss: 1.9197 valid loss: 1.9731, valid accuracy: 0.2990\n",
      "Iter-9640 train loss: 2.0216 valid loss: 1.9730, valid accuracy: 0.2990\n",
      "Iter-9650 train loss: 1.9630 valid loss: 1.9728, valid accuracy: 0.2990\n",
      "Iter-9660 train loss: 1.9011 valid loss: 1.9728, valid accuracy: 0.2990\n",
      "Iter-9670 train loss: 2.1144 valid loss: 1.9727, valid accuracy: 0.2990\n",
      "Iter-9680 train loss: 2.0543 valid loss: 1.9726, valid accuracy: 0.2988\n",
      "Iter-9690 train loss: 1.9903 valid loss: 1.9725, valid accuracy: 0.2988\n",
      "Iter-9700 train loss: 1.9226 valid loss: 1.9724, valid accuracy: 0.2988\n",
      "Iter-9710 train loss: 2.0753 valid loss: 1.9723, valid accuracy: 0.2988\n",
      "Iter-9720 train loss: 2.0618 valid loss: 1.9722, valid accuracy: 0.2988\n",
      "Iter-9730 train loss: 2.0132 valid loss: 1.9721, valid accuracy: 0.2986\n",
      "Iter-9740 train loss: 2.0167 valid loss: 1.9720, valid accuracy: 0.2990\n",
      "Iter-9750 train loss: 2.0063 valid loss: 1.9719, valid accuracy: 0.2992\n",
      "Iter-9760 train loss: 2.1177 valid loss: 1.9718, valid accuracy: 0.2990\n",
      "Iter-9770 train loss: 2.0482 valid loss: 1.9717, valid accuracy: 0.2990\n",
      "Iter-9780 train loss: 1.9080 valid loss: 1.9716, valid accuracy: 0.2992\n",
      "Iter-9790 train loss: 1.9691 valid loss: 1.9715, valid accuracy: 0.2996\n",
      "Iter-9800 train loss: 2.0787 valid loss: 1.9714, valid accuracy: 0.2998\n",
      "Iter-9810 train loss: 1.8772 valid loss: 1.9714, valid accuracy: 0.2996\n",
      "Iter-9820 train loss: 2.0458 valid loss: 1.9713, valid accuracy: 0.2996\n",
      "Iter-9830 train loss: 1.9799 valid loss: 1.9712, valid accuracy: 0.2994\n",
      "Iter-9840 train loss: 1.9433 valid loss: 1.9711, valid accuracy: 0.2994\n",
      "Iter-9850 train loss: 1.9086 valid loss: 1.9710, valid accuracy: 0.2994\n",
      "Iter-9860 train loss: 2.0191 valid loss: 1.9709, valid accuracy: 0.2994\n",
      "Iter-9870 train loss: 1.9548 valid loss: 1.9708, valid accuracy: 0.2996\n",
      "Iter-9880 train loss: 2.0502 valid loss: 1.9707, valid accuracy: 0.3000\n",
      "Iter-9890 train loss: 2.0361 valid loss: 1.9706, valid accuracy: 0.3000\n",
      "Iter-9900 train loss: 1.9077 valid loss: 1.9705, valid accuracy: 0.2994\n",
      "Iter-9910 train loss: 2.1190 valid loss: 1.9705, valid accuracy: 0.2996\n",
      "Iter-9920 train loss: 1.9320 valid loss: 1.9704, valid accuracy: 0.2998\n",
      "Iter-9930 train loss: 2.0643 valid loss: 1.9703, valid accuracy: 0.2998\n",
      "Iter-9940 train loss: 2.0982 valid loss: 1.9702, valid accuracy: 0.2998\n",
      "Iter-9950 train loss: 1.9864 valid loss: 1.9701, valid accuracy: 0.3000\n",
      "Iter-9960 train loss: 1.8590 valid loss: 1.9700, valid accuracy: 0.3000\n",
      "Iter-9970 train loss: 2.0887 valid loss: 1.9699, valid accuracy: 0.2998\n",
      "Iter-9980 train loss: 2.0303 valid loss: 1.9699, valid accuracy: 0.2998\n",
      "Iter-9990 train loss: 2.0223 valid loss: 1.9698, valid accuracy: 0.2996\n",
      "Iter-10000 train loss: 2.0215 valid loss: 1.9697, valid accuracy: 0.2996\n",
      "Last iteration - Test accuracy mean: 0.2909, std: 0.0000, loss: 1.9780\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 20 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ea9748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICIiSf4/0zoPFt4+gSAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118ea97b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
