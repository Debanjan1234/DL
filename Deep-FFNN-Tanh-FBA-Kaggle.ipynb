{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # MNIST Data\n",
    "# import numpy as np\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import impl.layer as l\n",
    "\n",
    "# # Dataset preparation and pre-processing\n",
    "# mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "# X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "# X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "# X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Country'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# # Count the freq of words in the text/collection of words\n",
    "# word_counts = Counter(text)\n",
    "# # Having counted the frequency of the words in collection, sort them from most to least/top to bottom/descendng\n",
    "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "# # first enumerating for vocab to int\n",
    "# vocab_to_int = {words: ii for ii, words in enumerate(sorted_vocab)}\n",
    "# # into_to_vocab after enumerating through the sorted vocab\n",
    "# int_to_vocab = {ii: words for words, ii in vocab_to_int.items()}\n",
    "\n",
    "counted_labels = Counter(train_Y)\n",
    "key_to_val = {key: val for val, key in enumerate(counted_labels)}\n",
    "key_to_val['Country']\n",
    "\n",
    "val_to_key = {val: key for val, key in enumerate(counted_labels)}\n",
    "val_to_key[1]\n",
    "\n",
    "# labels = []\n",
    "# for val, key in enumerate(counted_labels):\n",
    "#     print(val, key)\n",
    "#     labels.append(val)\n",
    "  \n",
    "# labels = np.array(labels, dtype=int)\n",
    "# labels.size, np.max(labels), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from impl.layer import onehot\n",
    "\n",
    "# labels_onehot = onehot(labels)\n",
    "\n",
    "# labels, labels_onehot, counted_labels.keys()\n",
    "# key_to_vec = {key: vec for key, vec in zip(counted_labels.keys(), labels_onehot)}\n",
    "# key_to_vec, key_to_vec['Vocal']\n",
    "\n",
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.5145 valid loss: 2.5407, valid accuracy: 0.0985\n",
      "Iter-200 train loss: 2.5212 valid loss: 2.5393, valid accuracy: 0.0992\n",
      "Iter-300 train loss: 2.5643 valid loss: 2.5379, valid accuracy: 0.1015\n",
      "Iter-400 train loss: 2.5651 valid loss: 2.5366, valid accuracy: 0.1046\n",
      "Iter-500 train loss: 2.5689 valid loss: 2.5353, valid accuracy: 0.1100\n",
      "Iter-600 train loss: 2.5491 valid loss: 2.5340, valid accuracy: 0.1100\n",
      "Iter-700 train loss: 2.5520 valid loss: 2.5326, valid accuracy: 0.1146\n",
      "Iter-800 train loss: 2.5524 valid loss: 2.5314, valid accuracy: 0.1162\n",
      "Iter-900 train loss: 2.5784 valid loss: 2.5302, valid accuracy: 0.1177\n",
      "Iter-1000 train loss: 2.5133 valid loss: 2.5289, valid accuracy: 0.1185\n",
      "Iter-1100 train loss: 2.5146 valid loss: 2.5277, valid accuracy: 0.1208\n",
      "Iter-1200 train loss: 2.5425 valid loss: 2.5264, valid accuracy: 0.1215\n",
      "Iter-1300 train loss: 2.5087 valid loss: 2.5251, valid accuracy: 0.1262\n",
      "Iter-1400 train loss: 2.5477 valid loss: 2.5239, valid accuracy: 0.1262\n",
      "Iter-1500 train loss: 2.5396 valid loss: 2.5226, valid accuracy: 0.1277\n",
      "Iter-1600 train loss: 2.4958 valid loss: 2.5214, valid accuracy: 0.1300\n",
      "Iter-1700 train loss: 2.5445 valid loss: 2.5203, valid accuracy: 0.1300\n",
      "Iter-1800 train loss: 2.5517 valid loss: 2.5191, valid accuracy: 0.1323\n",
      "Iter-1900 train loss: 2.5514 valid loss: 2.5177, valid accuracy: 0.1346\n",
      "Iter-2000 train loss: 2.5301 valid loss: 2.5164, valid accuracy: 0.1354\n",
      "Iter-2100 train loss: 2.5620 valid loss: 2.5152, valid accuracy: 0.1354\n",
      "Iter-2200 train loss: 2.5371 valid loss: 2.5139, valid accuracy: 0.1369\n",
      "Iter-2300 train loss: 2.5136 valid loss: 2.5127, valid accuracy: 0.1369\n",
      "Iter-2400 train loss: 2.5694 valid loss: 2.5115, valid accuracy: 0.1369\n",
      "Iter-2500 train loss: 2.5346 valid loss: 2.5103, valid accuracy: 0.1392\n",
      "Iter-2600 train loss: 2.5650 valid loss: 2.5090, valid accuracy: 0.1400\n",
      "Iter-2700 train loss: 2.5043 valid loss: 2.5078, valid accuracy: 0.1400\n",
      "Iter-2800 train loss: 2.4868 valid loss: 2.5066, valid accuracy: 0.1408\n",
      "Iter-2900 train loss: 2.5086 valid loss: 2.5053, valid accuracy: 0.1423\n",
      "Iter-3000 train loss: 2.4669 valid loss: 2.5039, valid accuracy: 0.1415\n",
      "Iter-3100 train loss: 2.4809 valid loss: 2.5027, valid accuracy: 0.1415\n",
      "Iter-3200 train loss: 2.4726 valid loss: 2.5014, valid accuracy: 0.1392\n",
      "Iter-3300 train loss: 2.5366 valid loss: 2.5000, valid accuracy: 0.1423\n",
      "Iter-3400 train loss: 2.5226 valid loss: 2.4988, valid accuracy: 0.1385\n",
      "Iter-3500 train loss: 2.4911 valid loss: 2.4975, valid accuracy: 0.1400\n",
      "Iter-3600 train loss: 2.4501 valid loss: 2.4961, valid accuracy: 0.1423\n",
      "Iter-3700 train loss: 2.4832 valid loss: 2.4948, valid accuracy: 0.1454\n",
      "Iter-3800 train loss: 2.4861 valid loss: 2.4933, valid accuracy: 0.1477\n",
      "Iter-3900 train loss: 2.4897 valid loss: 2.4921, valid accuracy: 0.1485\n",
      "Iter-4000 train loss: 2.4899 valid loss: 2.4908, valid accuracy: 0.1485\n",
      "Iter-4100 train loss: 2.5312 valid loss: 2.4894, valid accuracy: 0.1500\n",
      "Iter-4200 train loss: 2.4905 valid loss: 2.4880, valid accuracy: 0.1500\n",
      "Iter-4300 train loss: 2.4295 valid loss: 2.4866, valid accuracy: 0.1492\n",
      "Iter-4400 train loss: 2.5042 valid loss: 2.4852, valid accuracy: 0.1485\n",
      "Iter-4500 train loss: 2.4912 valid loss: 2.4838, valid accuracy: 0.1508\n",
      "Iter-4600 train loss: 2.5422 valid loss: 2.4823, valid accuracy: 0.1515\n",
      "Iter-4700 train loss: 2.4476 valid loss: 2.4809, valid accuracy: 0.1500\n",
      "Iter-4800 train loss: 2.4377 valid loss: 2.4795, valid accuracy: 0.1492\n",
      "Iter-4900 train loss: 2.4561 valid loss: 2.4780, valid accuracy: 0.1508\n",
      "Iter-5000 train loss: 2.4469 valid loss: 2.4765, valid accuracy: 0.1515\n",
      "Iter-5100 train loss: 2.5154 valid loss: 2.4750, valid accuracy: 0.1523\n",
      "Iter-5200 train loss: 2.4985 valid loss: 2.4734, valid accuracy: 0.1562\n",
      "Iter-5300 train loss: 2.5277 valid loss: 2.4719, valid accuracy: 0.1546\n",
      "Iter-5400 train loss: 2.4414 valid loss: 2.4702, valid accuracy: 0.1562\n",
      "Iter-5500 train loss: 2.4354 valid loss: 2.4686, valid accuracy: 0.1577\n",
      "Iter-5600 train loss: 2.4568 valid loss: 2.4670, valid accuracy: 0.1585\n",
      "Iter-5700 train loss: 2.4628 valid loss: 2.4654, valid accuracy: 0.1592\n",
      "Iter-5800 train loss: 2.4658 valid loss: 2.4637, valid accuracy: 0.1608\n",
      "Iter-5900 train loss: 2.4687 valid loss: 2.4620, valid accuracy: 0.1600\n",
      "Iter-6000 train loss: 2.4435 valid loss: 2.4603, valid accuracy: 0.1600\n",
      "Iter-6100 train loss: 2.4469 valid loss: 2.4587, valid accuracy: 0.1600\n",
      "Iter-6200 train loss: 2.4724 valid loss: 2.4570, valid accuracy: 0.1600\n",
      "Iter-6300 train loss: 2.5609 valid loss: 2.4553, valid accuracy: 0.1600\n",
      "Iter-6400 train loss: 2.4259 valid loss: 2.4536, valid accuracy: 0.1615\n",
      "Iter-6500 train loss: 2.4384 valid loss: 2.4518, valid accuracy: 0.1638\n",
      "Iter-6600 train loss: 2.4732 valid loss: 2.4501, valid accuracy: 0.1638\n",
      "Iter-6700 train loss: 2.3727 valid loss: 2.4483, valid accuracy: 0.1654\n",
      "Iter-6800 train loss: 2.3788 valid loss: 2.4466, valid accuracy: 0.1669\n",
      "Iter-6900 train loss: 2.5011 valid loss: 2.4447, valid accuracy: 0.1677\n",
      "Iter-7000 train loss: 2.3920 valid loss: 2.4429, valid accuracy: 0.1685\n",
      "Iter-7100 train loss: 2.3829 valid loss: 2.4411, valid accuracy: 0.1692\n",
      "Iter-7200 train loss: 2.4112 valid loss: 2.4392, valid accuracy: 0.1708\n",
      "Iter-7300 train loss: 2.4239 valid loss: 2.4374, valid accuracy: 0.1700\n",
      "Iter-7400 train loss: 2.4545 valid loss: 2.4355, valid accuracy: 0.1708\n",
      "Iter-7500 train loss: 2.4141 valid loss: 2.4335, valid accuracy: 0.1692\n",
      "Iter-7600 train loss: 2.3914 valid loss: 2.4317, valid accuracy: 0.1723\n",
      "Iter-7700 train loss: 2.4339 valid loss: 2.4298, valid accuracy: 0.1738\n",
      "Iter-7800 train loss: 2.4076 valid loss: 2.4280, valid accuracy: 0.1738\n",
      "Iter-7900 train loss: 2.4224 valid loss: 2.4260, valid accuracy: 0.1738\n",
      "Iter-8000 train loss: 2.4926 valid loss: 2.4241, valid accuracy: 0.1738\n",
      "Iter-8100 train loss: 2.4636 valid loss: 2.4221, valid accuracy: 0.1738\n",
      "Iter-8200 train loss: 2.3917 valid loss: 2.4201, valid accuracy: 0.1762\n",
      "Iter-8300 train loss: 2.3622 valid loss: 2.4181, valid accuracy: 0.1769\n",
      "Iter-8400 train loss: 2.4631 valid loss: 2.4162, valid accuracy: 0.1762\n",
      "Iter-8500 train loss: 2.4388 valid loss: 2.4142, valid accuracy: 0.1792\n",
      "Iter-8600 train loss: 2.4003 valid loss: 2.4122, valid accuracy: 0.1823\n",
      "Iter-8700 train loss: 2.3847 valid loss: 2.4103, valid accuracy: 0.1815\n",
      "Iter-8800 train loss: 2.4311 valid loss: 2.4083, valid accuracy: 0.1815\n",
      "Iter-8900 train loss: 2.4145 valid loss: 2.4061, valid accuracy: 0.1815\n",
      "Iter-9000 train loss: 2.3502 valid loss: 2.4041, valid accuracy: 0.1808\n",
      "Iter-9100 train loss: 2.3715 valid loss: 2.4021, valid accuracy: 0.1792\n",
      "Iter-9200 train loss: 2.5078 valid loss: 2.4001, valid accuracy: 0.1831\n",
      "Iter-9300 train loss: 2.4494 valid loss: 2.3981, valid accuracy: 0.1838\n",
      "Iter-9400 train loss: 2.3839 valid loss: 2.3962, valid accuracy: 0.1854\n",
      "Iter-9500 train loss: 2.3677 valid loss: 2.3942, valid accuracy: 0.1877\n",
      "Iter-9600 train loss: 2.4298 valid loss: 2.3923, valid accuracy: 0.1877\n",
      "Iter-9700 train loss: 2.4232 valid loss: 2.3903, valid accuracy: 0.1854\n",
      "Iter-9800 train loss: 2.3434 valid loss: 2.3882, valid accuracy: 0.1877\n",
      "Iter-9900 train loss: 2.3246 valid loss: 2.3864, valid accuracy: 0.1854\n",
      "Iter-10000 train loss: 2.3686 valid loss: 2.3844, valid accuracy: 0.1862\n",
      "Iter-10100 train loss: 2.3551 valid loss: 2.3826, valid accuracy: 0.1854\n",
      "Iter-10200 train loss: 2.4720 valid loss: 2.3807, valid accuracy: 0.1885\n",
      "Iter-10300 train loss: 2.3106 valid loss: 2.3790, valid accuracy: 0.1892\n",
      "Iter-10400 train loss: 2.3363 valid loss: 2.3771, valid accuracy: 0.1908\n",
      "Iter-10500 train loss: 2.3566 valid loss: 2.3752, valid accuracy: 0.1900\n",
      "Iter-10600 train loss: 2.4121 valid loss: 2.3734, valid accuracy: 0.1908\n",
      "Iter-10700 train loss: 2.3653 valid loss: 2.3716, valid accuracy: 0.1915\n",
      "Iter-10800 train loss: 2.3751 valid loss: 2.3699, valid accuracy: 0.1908\n",
      "Iter-10900 train loss: 2.4191 valid loss: 2.3680, valid accuracy: 0.1915\n",
      "Iter-11000 train loss: 2.3613 valid loss: 2.3661, valid accuracy: 0.1908\n",
      "Iter-11100 train loss: 2.4449 valid loss: 2.3642, valid accuracy: 0.1885\n",
      "Iter-11200 train loss: 2.3206 valid loss: 2.3625, valid accuracy: 0.1908\n",
      "Iter-11300 train loss: 2.3939 valid loss: 2.3606, valid accuracy: 0.1923\n",
      "Iter-11400 train loss: 2.3927 valid loss: 2.3590, valid accuracy: 0.1938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.3632 valid loss: 2.3572, valid accuracy: 0.1969\n",
      "Iter-11600 train loss: 2.4374 valid loss: 2.3555, valid accuracy: 0.1969\n",
      "Iter-11700 train loss: 2.3188 valid loss: 2.3538, valid accuracy: 0.1962\n",
      "Iter-11800 train loss: 2.3838 valid loss: 2.3521, valid accuracy: 0.1962\n",
      "Iter-11900 train loss: 2.2406 valid loss: 2.3505, valid accuracy: 0.1977\n",
      "Iter-12000 train loss: 2.4700 valid loss: 2.3488, valid accuracy: 0.1985\n",
      "Iter-12100 train loss: 2.3926 valid loss: 2.3473, valid accuracy: 0.1962\n",
      "Iter-12200 train loss: 2.3665 valid loss: 2.3455, valid accuracy: 0.1969\n",
      "Iter-12300 train loss: 2.4054 valid loss: 2.3439, valid accuracy: 0.1992\n",
      "Iter-12400 train loss: 2.3627 valid loss: 2.3423, valid accuracy: 0.1969\n",
      "Iter-12500 train loss: 2.3063 valid loss: 2.3407, valid accuracy: 0.1985\n",
      "Iter-12600 train loss: 2.4194 valid loss: 2.3391, valid accuracy: 0.1992\n",
      "Iter-12700 train loss: 2.3346 valid loss: 2.3375, valid accuracy: 0.1985\n",
      "Iter-12800 train loss: 2.2981 valid loss: 2.3359, valid accuracy: 0.2000\n",
      "Iter-12900 train loss: 2.2973 valid loss: 2.3345, valid accuracy: 0.2015\n",
      "Iter-13000 train loss: 2.4584 valid loss: 2.3330, valid accuracy: 0.2023\n",
      "Iter-13100 train loss: 2.3652 valid loss: 2.3315, valid accuracy: 0.2038\n",
      "Iter-13200 train loss: 2.3652 valid loss: 2.3300, valid accuracy: 0.2015\n",
      "Iter-13300 train loss: 2.2609 valid loss: 2.3285, valid accuracy: 0.2031\n",
      "Iter-13400 train loss: 2.3051 valid loss: 2.3270, valid accuracy: 0.2038\n",
      "Iter-13500 train loss: 2.2376 valid loss: 2.3256, valid accuracy: 0.2046\n",
      "Iter-13600 train loss: 2.3526 valid loss: 2.3242, valid accuracy: 0.2046\n",
      "Iter-13700 train loss: 2.2349 valid loss: 2.3228, valid accuracy: 0.2023\n",
      "Iter-13800 train loss: 2.1555 valid loss: 2.3214, valid accuracy: 0.2031\n",
      "Iter-13900 train loss: 2.3546 valid loss: 2.3201, valid accuracy: 0.2031\n",
      "Iter-14000 train loss: 2.3608 valid loss: 2.3188, valid accuracy: 0.2031\n",
      "Iter-14100 train loss: 2.3213 valid loss: 2.3174, valid accuracy: 0.2023\n",
      "Iter-14200 train loss: 2.2360 valid loss: 2.3161, valid accuracy: 0.2023\n",
      "Iter-14300 train loss: 2.1657 valid loss: 2.3147, valid accuracy: 0.2038\n",
      "Iter-14400 train loss: 2.3745 valid loss: 2.3134, valid accuracy: 0.2023\n",
      "Iter-14500 train loss: 2.3110 valid loss: 2.3120, valid accuracy: 0.2031\n",
      "Iter-14600 train loss: 2.3139 valid loss: 2.3108, valid accuracy: 0.2038\n",
      "Iter-14700 train loss: 2.3085 valid loss: 2.3095, valid accuracy: 0.2046\n",
      "Iter-14800 train loss: 2.3520 valid loss: 2.3082, valid accuracy: 0.2046\n",
      "Iter-14900 train loss: 2.3005 valid loss: 2.3069, valid accuracy: 0.2038\n",
      "Iter-15000 train loss: 2.3620 valid loss: 2.3057, valid accuracy: 0.2038\n",
      "Iter-15100 train loss: 2.4030 valid loss: 2.3046, valid accuracy: 0.2031\n",
      "Iter-15200 train loss: 2.2068 valid loss: 2.3034, valid accuracy: 0.2023\n",
      "Iter-15300 train loss: 2.2948 valid loss: 2.3023, valid accuracy: 0.2046\n",
      "Iter-15400 train loss: 2.3204 valid loss: 2.3011, valid accuracy: 0.2046\n",
      "Iter-15500 train loss: 2.2264 valid loss: 2.2999, valid accuracy: 0.2046\n",
      "Iter-15600 train loss: 2.4005 valid loss: 2.2988, valid accuracy: 0.2038\n",
      "Iter-15700 train loss: 2.2772 valid loss: 2.2977, valid accuracy: 0.2015\n",
      "Iter-15800 train loss: 2.3199 valid loss: 2.2966, valid accuracy: 0.2015\n",
      "Iter-15900 train loss: 2.3341 valid loss: 2.2955, valid accuracy: 0.2008\n",
      "Iter-16000 train loss: 2.3273 valid loss: 2.2945, valid accuracy: 0.2000\n",
      "Iter-16100 train loss: 2.2539 valid loss: 2.2935, valid accuracy: 0.2000\n",
      "Iter-16200 train loss: 2.3990 valid loss: 2.2924, valid accuracy: 0.2000\n",
      "Iter-16300 train loss: 2.3560 valid loss: 2.2915, valid accuracy: 0.2023\n",
      "Iter-16400 train loss: 2.2641 valid loss: 2.2906, valid accuracy: 0.2038\n",
      "Iter-16500 train loss: 2.2517 valid loss: 2.2895, valid accuracy: 0.2046\n",
      "Iter-16600 train loss: 2.2146 valid loss: 2.2885, valid accuracy: 0.2054\n",
      "Iter-16700 train loss: 2.3349 valid loss: 2.2876, valid accuracy: 0.2069\n",
      "Iter-16800 train loss: 2.3824 valid loss: 2.2866, valid accuracy: 0.2077\n",
      "Iter-16900 train loss: 2.2483 valid loss: 2.2856, valid accuracy: 0.2069\n",
      "Iter-17000 train loss: 2.3689 valid loss: 2.2845, valid accuracy: 0.2077\n",
      "Iter-17100 train loss: 2.2502 valid loss: 2.2836, valid accuracy: 0.2077\n",
      "Iter-17200 train loss: 2.2973 valid loss: 2.2825, valid accuracy: 0.2100\n",
      "Iter-17300 train loss: 2.1618 valid loss: 2.2816, valid accuracy: 0.2092\n",
      "Iter-17400 train loss: 2.4394 valid loss: 2.2806, valid accuracy: 0.2100\n",
      "Iter-17500 train loss: 2.3383 valid loss: 2.2797, valid accuracy: 0.2108\n",
      "Iter-17600 train loss: 2.3491 valid loss: 2.2787, valid accuracy: 0.2123\n",
      "Iter-17700 train loss: 2.3467 valid loss: 2.2779, valid accuracy: 0.2123\n",
      "Iter-17800 train loss: 2.2982 valid loss: 2.2769, valid accuracy: 0.2138\n",
      "Iter-17900 train loss: 2.2710 valid loss: 2.2761, valid accuracy: 0.2138\n",
      "Iter-18000 train loss: 2.2975 valid loss: 2.2752, valid accuracy: 0.2138\n",
      "Iter-18100 train loss: 2.3733 valid loss: 2.2742, valid accuracy: 0.2123\n",
      "Iter-18200 train loss: 2.2282 valid loss: 2.2734, valid accuracy: 0.2123\n",
      "Iter-18300 train loss: 2.2788 valid loss: 2.2728, valid accuracy: 0.2108\n",
      "Iter-18400 train loss: 2.3106 valid loss: 2.2719, valid accuracy: 0.2138\n",
      "Iter-18500 train loss: 2.2598 valid loss: 2.2710, valid accuracy: 0.2131\n",
      "Iter-18600 train loss: 2.3067 valid loss: 2.2702, valid accuracy: 0.2108\n",
      "Iter-18700 train loss: 2.2453 valid loss: 2.2695, valid accuracy: 0.2100\n",
      "Iter-18800 train loss: 2.4569 valid loss: 2.2687, valid accuracy: 0.2100\n",
      "Iter-18900 train loss: 2.2936 valid loss: 2.2679, valid accuracy: 0.2100\n",
      "Iter-19000 train loss: 2.2383 valid loss: 2.2671, valid accuracy: 0.2092\n",
      "Iter-19100 train loss: 2.2863 valid loss: 2.2662, valid accuracy: 0.2092\n",
      "Iter-19200 train loss: 2.1510 valid loss: 2.2654, valid accuracy: 0.2108\n",
      "Iter-19300 train loss: 2.2198 valid loss: 2.2646, valid accuracy: 0.2100\n",
      "Iter-19400 train loss: 2.2195 valid loss: 2.2638, valid accuracy: 0.2108\n",
      "Iter-19500 train loss: 2.2635 valid loss: 2.2631, valid accuracy: 0.2115\n",
      "Iter-19600 train loss: 2.3357 valid loss: 2.2624, valid accuracy: 0.2115\n",
      "Iter-19700 train loss: 2.2297 valid loss: 2.2617, valid accuracy: 0.2115\n",
      "Iter-19800 train loss: 2.3527 valid loss: 2.2610, valid accuracy: 0.2131\n",
      "Iter-19900 train loss: 2.2759 valid loss: 2.2603, valid accuracy: 0.2131\n",
      "Iter-20000 train loss: 2.2294 valid loss: 2.2596, valid accuracy: 0.2154\n",
      "Iter-20100 train loss: 2.2650 valid loss: 2.2590, valid accuracy: 0.2146\n",
      "Iter-20200 train loss: 2.3170 valid loss: 2.2584, valid accuracy: 0.2146\n",
      "Iter-20300 train loss: 2.2586 valid loss: 2.2577, valid accuracy: 0.2169\n",
      "Iter-20400 train loss: 2.3005 valid loss: 2.2570, valid accuracy: 0.2177\n",
      "Iter-20500 train loss: 2.3582 valid loss: 2.2564, valid accuracy: 0.2177\n",
      "Iter-20600 train loss: 2.2286 valid loss: 2.2557, valid accuracy: 0.2185\n",
      "Iter-20700 train loss: 2.3996 valid loss: 2.2551, valid accuracy: 0.2177\n",
      "Iter-20800 train loss: 2.1797 valid loss: 2.2545, valid accuracy: 0.2185\n",
      "Iter-20900 train loss: 2.3201 valid loss: 2.2538, valid accuracy: 0.2192\n",
      "Iter-21000 train loss: 2.2896 valid loss: 2.2531, valid accuracy: 0.2192\n",
      "Iter-21100 train loss: 2.2702 valid loss: 2.2525, valid accuracy: 0.2185\n",
      "Iter-21200 train loss: 2.2022 valid loss: 2.2519, valid accuracy: 0.2185\n",
      "Iter-21300 train loss: 2.1862 valid loss: 2.2513, valid accuracy: 0.2169\n",
      "Iter-21400 train loss: 2.3226 valid loss: 2.2506, valid accuracy: 0.2185\n",
      "Iter-21500 train loss: 2.1630 valid loss: 2.2500, valid accuracy: 0.2185\n",
      "Iter-21600 train loss: 2.3729 valid loss: 2.2495, valid accuracy: 0.2177\n",
      "Iter-21700 train loss: 2.1312 valid loss: 2.2490, valid accuracy: 0.2192\n",
      "Iter-21800 train loss: 2.1571 valid loss: 2.2484, valid accuracy: 0.2200\n",
      "Iter-21900 train loss: 2.1537 valid loss: 2.2477, valid accuracy: 0.2200\n",
      "Iter-22000 train loss: 2.2155 valid loss: 2.2471, valid accuracy: 0.2223\n",
      "Iter-22100 train loss: 2.2099 valid loss: 2.2465, valid accuracy: 0.2223\n",
      "Iter-22200 train loss: 2.2340 valid loss: 2.2459, valid accuracy: 0.2215\n",
      "Iter-22300 train loss: 2.1882 valid loss: 2.2453, valid accuracy: 0.2238\n",
      "Iter-22400 train loss: 2.1232 valid loss: 2.2447, valid accuracy: 0.2238\n",
      "Iter-22500 train loss: 2.1871 valid loss: 2.2441, valid accuracy: 0.2254\n",
      "Iter-22600 train loss: 2.2213 valid loss: 2.2433, valid accuracy: 0.2269\n",
      "Iter-22700 train loss: 2.3794 valid loss: 2.2429, valid accuracy: 0.2269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.3108 valid loss: 2.2423, valid accuracy: 0.2254\n",
      "Iter-22900 train loss: 2.2506 valid loss: 2.2418, valid accuracy: 0.2246\n",
      "Iter-23000 train loss: 2.0558 valid loss: 2.2411, valid accuracy: 0.2246\n",
      "Iter-23100 train loss: 2.2791 valid loss: 2.2405, valid accuracy: 0.2246\n",
      "Iter-23200 train loss: 2.3364 valid loss: 2.2400, valid accuracy: 0.2262\n",
      "Iter-23300 train loss: 2.2475 valid loss: 2.2393, valid accuracy: 0.2277\n",
      "Iter-23400 train loss: 2.2019 valid loss: 2.2387, valid accuracy: 0.2269\n",
      "Iter-23500 train loss: 2.0790 valid loss: 2.2381, valid accuracy: 0.2262\n",
      "Iter-23600 train loss: 2.2918 valid loss: 2.2375, valid accuracy: 0.2262\n",
      "Iter-23700 train loss: 2.1507 valid loss: 2.2369, valid accuracy: 0.2277\n",
      "Iter-23800 train loss: 2.2283 valid loss: 2.2364, valid accuracy: 0.2277\n",
      "Iter-23900 train loss: 2.1246 valid loss: 2.2359, valid accuracy: 0.2277\n",
      "Iter-24000 train loss: 2.2553 valid loss: 2.2354, valid accuracy: 0.2292\n",
      "Iter-24100 train loss: 2.2548 valid loss: 2.2350, valid accuracy: 0.2285\n",
      "Iter-24200 train loss: 2.2031 valid loss: 2.2343, valid accuracy: 0.2292\n",
      "Iter-24300 train loss: 2.2234 valid loss: 2.2339, valid accuracy: 0.2292\n",
      "Iter-24400 train loss: 2.1789 valid loss: 2.2333, valid accuracy: 0.2315\n",
      "Iter-24500 train loss: 2.2541 valid loss: 2.2328, valid accuracy: 0.2315\n",
      "Iter-24600 train loss: 2.3206 valid loss: 2.2323, valid accuracy: 0.2308\n",
      "Iter-24700 train loss: 2.2876 valid loss: 2.2318, valid accuracy: 0.2323\n",
      "Iter-24800 train loss: 2.4121 valid loss: 2.2313, valid accuracy: 0.2315\n",
      "Iter-24900 train loss: 2.2426 valid loss: 2.2308, valid accuracy: 0.2315\n",
      "Iter-25000 train loss: 2.3160 valid loss: 2.2304, valid accuracy: 0.2323\n",
      "Iter-25100 train loss: 2.1132 valid loss: 2.2298, valid accuracy: 0.2338\n",
      "Iter-25200 train loss: 2.3836 valid loss: 2.2293, valid accuracy: 0.2338\n",
      "Iter-25300 train loss: 2.3105 valid loss: 2.2288, valid accuracy: 0.2338\n",
      "Iter-25400 train loss: 2.2266 valid loss: 2.2284, valid accuracy: 0.2331\n",
      "Iter-25500 train loss: 2.2282 valid loss: 2.2278, valid accuracy: 0.2308\n",
      "Iter-25600 train loss: 2.2982 valid loss: 2.2273, valid accuracy: 0.2315\n",
      "Iter-25700 train loss: 2.2426 valid loss: 2.2268, valid accuracy: 0.2315\n",
      "Iter-25800 train loss: 2.2084 valid loss: 2.2263, valid accuracy: 0.2331\n",
      "Iter-25900 train loss: 2.3832 valid loss: 2.2258, valid accuracy: 0.2331\n",
      "Iter-26000 train loss: 2.2646 valid loss: 2.2253, valid accuracy: 0.2331\n",
      "Iter-26100 train loss: 2.4755 valid loss: 2.2248, valid accuracy: 0.2323\n",
      "Iter-26200 train loss: 2.2038 valid loss: 2.2242, valid accuracy: 0.2323\n",
      "Iter-26300 train loss: 2.1292 valid loss: 2.2237, valid accuracy: 0.2308\n",
      "Iter-26400 train loss: 2.1993 valid loss: 2.2232, valid accuracy: 0.2292\n",
      "Iter-26500 train loss: 2.1779 valid loss: 2.2228, valid accuracy: 0.2292\n",
      "Iter-26600 train loss: 2.0232 valid loss: 2.2223, valid accuracy: 0.2292\n",
      "Iter-26700 train loss: 2.1702 valid loss: 2.2218, valid accuracy: 0.2285\n",
      "Iter-26800 train loss: 2.2486 valid loss: 2.2213, valid accuracy: 0.2300\n",
      "Iter-26900 train loss: 2.2844 valid loss: 2.2208, valid accuracy: 0.2300\n",
      "Iter-27000 train loss: 2.2905 valid loss: 2.2203, valid accuracy: 0.2323\n",
      "Iter-27100 train loss: 1.9863 valid loss: 2.2198, valid accuracy: 0.2331\n",
      "Iter-27200 train loss: 2.1410 valid loss: 2.2192, valid accuracy: 0.2331\n",
      "Iter-27300 train loss: 2.1577 valid loss: 2.2187, valid accuracy: 0.2338\n",
      "Iter-27400 train loss: 2.2065 valid loss: 2.2182, valid accuracy: 0.2338\n",
      "Iter-27500 train loss: 2.2157 valid loss: 2.2177, valid accuracy: 0.2331\n",
      "Iter-27600 train loss: 2.1814 valid loss: 2.2173, valid accuracy: 0.2338\n",
      "Iter-27700 train loss: 2.0598 valid loss: 2.2168, valid accuracy: 0.2346\n",
      "Iter-27800 train loss: 2.2343 valid loss: 2.2164, valid accuracy: 0.2338\n",
      "Iter-27900 train loss: 2.2682 valid loss: 2.2160, valid accuracy: 0.2346\n",
      "Iter-28000 train loss: 2.0174 valid loss: 2.2155, valid accuracy: 0.2346\n",
      "Iter-28100 train loss: 2.2969 valid loss: 2.2150, valid accuracy: 0.2331\n",
      "Iter-28200 train loss: 2.1106 valid loss: 2.2145, valid accuracy: 0.2354\n",
      "Iter-28300 train loss: 2.2645 valid loss: 2.2141, valid accuracy: 0.2354\n",
      "Iter-28400 train loss: 2.1448 valid loss: 2.2137, valid accuracy: 0.2362\n",
      "Iter-28500 train loss: 2.2227 valid loss: 2.2131, valid accuracy: 0.2354\n",
      "Iter-28600 train loss: 2.2712 valid loss: 2.2127, valid accuracy: 0.2362\n",
      "Iter-28700 train loss: 2.2262 valid loss: 2.2123, valid accuracy: 0.2362\n",
      "Iter-28800 train loss: 2.1214 valid loss: 2.2118, valid accuracy: 0.2354\n",
      "Iter-28900 train loss: 2.2073 valid loss: 2.2114, valid accuracy: 0.2362\n",
      "Iter-29000 train loss: 2.2483 valid loss: 2.2110, valid accuracy: 0.2369\n",
      "Iter-29100 train loss: 1.9762 valid loss: 2.2106, valid accuracy: 0.2385\n",
      "Iter-29200 train loss: 2.2547 valid loss: 2.2102, valid accuracy: 0.2385\n",
      "Iter-29300 train loss: 2.3206 valid loss: 2.2098, valid accuracy: 0.2392\n",
      "Iter-29400 train loss: 2.1631 valid loss: 2.2092, valid accuracy: 0.2392\n",
      "Iter-29500 train loss: 2.2236 valid loss: 2.2088, valid accuracy: 0.2385\n",
      "Iter-29600 train loss: 2.2574 valid loss: 2.2083, valid accuracy: 0.2392\n",
      "Iter-29700 train loss: 2.2989 valid loss: 2.2079, valid accuracy: 0.2385\n",
      "Iter-29800 train loss: 2.1163 valid loss: 2.2074, valid accuracy: 0.2377\n",
      "Iter-29900 train loss: 2.1564 valid loss: 2.2070, valid accuracy: 0.2377\n",
      "Iter-30000 train loss: 2.1348 valid loss: 2.2066, valid accuracy: 0.2385\n",
      "Iter-30100 train loss: 2.3090 valid loss: 2.2062, valid accuracy: 0.2377\n",
      "Iter-30200 train loss: 2.0759 valid loss: 2.2058, valid accuracy: 0.2392\n",
      "Iter-30300 train loss: 2.4452 valid loss: 2.2053, valid accuracy: 0.2385\n",
      "Iter-30400 train loss: 2.2577 valid loss: 2.2048, valid accuracy: 0.2377\n",
      "Iter-30500 train loss: 2.1684 valid loss: 2.2044, valid accuracy: 0.2362\n",
      "Iter-30600 train loss: 2.4288 valid loss: 2.2039, valid accuracy: 0.2369\n",
      "Iter-30700 train loss: 2.4625 valid loss: 2.2035, valid accuracy: 0.2369\n",
      "Iter-30800 train loss: 2.1915 valid loss: 2.2030, valid accuracy: 0.2369\n",
      "Iter-30900 train loss: 2.2020 valid loss: 2.2027, valid accuracy: 0.2369\n",
      "Iter-31000 train loss: 2.3038 valid loss: 2.2023, valid accuracy: 0.2369\n",
      "Iter-31100 train loss: 2.2793 valid loss: 2.2018, valid accuracy: 0.2354\n",
      "Iter-31200 train loss: 2.1694 valid loss: 2.2014, valid accuracy: 0.2354\n",
      "Iter-31300 train loss: 2.3895 valid loss: 2.2009, valid accuracy: 0.2362\n",
      "Iter-31400 train loss: 2.1005 valid loss: 2.2004, valid accuracy: 0.2369\n",
      "Iter-31500 train loss: 2.1252 valid loss: 2.2000, valid accuracy: 0.2377\n",
      "Iter-31600 train loss: 2.3843 valid loss: 2.1995, valid accuracy: 0.2408\n",
      "Iter-31700 train loss: 2.4647 valid loss: 2.1991, valid accuracy: 0.2400\n",
      "Iter-31800 train loss: 2.2133 valid loss: 2.1985, valid accuracy: 0.2415\n",
      "Iter-31900 train loss: 2.1957 valid loss: 2.1981, valid accuracy: 0.2423\n",
      "Iter-32000 train loss: 1.9135 valid loss: 2.1977, valid accuracy: 0.2423\n",
      "Iter-32100 train loss: 2.2098 valid loss: 2.1974, valid accuracy: 0.2423\n",
      "Iter-32200 train loss: 2.2809 valid loss: 2.1968, valid accuracy: 0.2423\n",
      "Iter-32300 train loss: 2.1951 valid loss: 2.1964, valid accuracy: 0.2438\n",
      "Iter-32400 train loss: 2.2146 valid loss: 2.1959, valid accuracy: 0.2446\n",
      "Iter-32500 train loss: 2.1103 valid loss: 2.1956, valid accuracy: 0.2446\n",
      "Iter-32600 train loss: 2.1239 valid loss: 2.1951, valid accuracy: 0.2446\n",
      "Iter-32700 train loss: 2.2491 valid loss: 2.1947, valid accuracy: 0.2446\n",
      "Iter-32800 train loss: 2.1821 valid loss: 2.1942, valid accuracy: 0.2438\n",
      "Iter-32900 train loss: 2.1694 valid loss: 2.1939, valid accuracy: 0.2446\n",
      "Iter-33000 train loss: 2.3419 valid loss: 2.1935, valid accuracy: 0.2446\n",
      "Iter-33100 train loss: 2.1425 valid loss: 2.1932, valid accuracy: 0.2446\n",
      "Iter-33200 train loss: 2.1419 valid loss: 2.1928, valid accuracy: 0.2431\n",
      "Iter-33300 train loss: 1.9970 valid loss: 2.1924, valid accuracy: 0.2454\n",
      "Iter-33400 train loss: 2.2233 valid loss: 2.1920, valid accuracy: 0.2446\n",
      "Iter-33500 train loss: 2.1573 valid loss: 2.1916, valid accuracy: 0.2454\n",
      "Iter-33600 train loss: 2.0724 valid loss: 2.1912, valid accuracy: 0.2462\n",
      "Iter-33700 train loss: 2.2694 valid loss: 2.1909, valid accuracy: 0.2446\n",
      "Iter-33800 train loss: 2.1518 valid loss: 2.1905, valid accuracy: 0.2431\n",
      "Iter-33900 train loss: 2.2647 valid loss: 2.1902, valid accuracy: 0.2438\n",
      "Iter-34000 train loss: 2.0928 valid loss: 2.1898, valid accuracy: 0.2446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.2074 valid loss: 2.1894, valid accuracy: 0.2446\n",
      "Iter-34200 train loss: 2.1720 valid loss: 2.1890, valid accuracy: 0.2446\n",
      "Iter-34300 train loss: 2.2134 valid loss: 2.1886, valid accuracy: 0.2454\n",
      "Iter-34400 train loss: 2.2689 valid loss: 2.1883, valid accuracy: 0.2454\n",
      "Iter-34500 train loss: 2.1187 valid loss: 2.1880, valid accuracy: 0.2462\n",
      "Iter-34600 train loss: 2.1742 valid loss: 2.1876, valid accuracy: 0.2454\n",
      "Iter-34700 train loss: 2.1813 valid loss: 2.1873, valid accuracy: 0.2454\n",
      "Iter-34800 train loss: 2.3095 valid loss: 2.1868, valid accuracy: 0.2454\n",
      "Iter-34900 train loss: 2.2439 valid loss: 2.1864, valid accuracy: 0.2454\n",
      "Iter-35000 train loss: 2.0507 valid loss: 2.1861, valid accuracy: 0.2454\n",
      "Iter-35100 train loss: 2.1195 valid loss: 2.1857, valid accuracy: 0.2454\n",
      "Iter-35200 train loss: 2.1769 valid loss: 2.1852, valid accuracy: 0.2469\n",
      "Iter-35300 train loss: 2.1655 valid loss: 2.1849, valid accuracy: 0.2477\n",
      "Iter-35400 train loss: 2.3210 valid loss: 2.1846, valid accuracy: 0.2485\n",
      "Iter-35500 train loss: 2.0778 valid loss: 2.1843, valid accuracy: 0.2477\n",
      "Iter-35600 train loss: 2.0581 valid loss: 2.1839, valid accuracy: 0.2500\n",
      "Iter-35700 train loss: 2.0713 valid loss: 2.1835, valid accuracy: 0.2492\n",
      "Iter-35800 train loss: 2.2403 valid loss: 2.1831, valid accuracy: 0.2492\n",
      "Iter-35900 train loss: 1.9940 valid loss: 2.1828, valid accuracy: 0.2477\n",
      "Iter-36000 train loss: 2.1982 valid loss: 2.1825, valid accuracy: 0.2477\n",
      "Iter-36100 train loss: 2.2619 valid loss: 2.1822, valid accuracy: 0.2485\n",
      "Iter-36200 train loss: 2.1532 valid loss: 2.1818, valid accuracy: 0.2485\n",
      "Iter-36300 train loss: 2.2601 valid loss: 2.1814, valid accuracy: 0.2492\n",
      "Iter-36400 train loss: 2.2081 valid loss: 2.1811, valid accuracy: 0.2500\n",
      "Iter-36500 train loss: 2.1066 valid loss: 2.1807, valid accuracy: 0.2508\n",
      "Iter-36600 train loss: 2.1777 valid loss: 2.1804, valid accuracy: 0.2500\n",
      "Iter-36700 train loss: 2.1559 valid loss: 2.1801, valid accuracy: 0.2508\n",
      "Iter-36800 train loss: 2.2645 valid loss: 2.1797, valid accuracy: 0.2523\n",
      "Iter-36900 train loss: 2.1367 valid loss: 2.1793, valid accuracy: 0.2523\n",
      "Iter-37000 train loss: 2.1132 valid loss: 2.1790, valid accuracy: 0.2515\n",
      "Iter-37100 train loss: 1.9804 valid loss: 2.1787, valid accuracy: 0.2492\n",
      "Iter-37200 train loss: 2.1979 valid loss: 2.1784, valid accuracy: 0.2492\n",
      "Iter-37300 train loss: 2.1288 valid loss: 2.1781, valid accuracy: 0.2485\n",
      "Iter-37400 train loss: 2.3358 valid loss: 2.1778, valid accuracy: 0.2485\n",
      "Iter-37500 train loss: 2.2161 valid loss: 2.1774, valid accuracy: 0.2492\n",
      "Iter-37600 train loss: 2.1021 valid loss: 2.1772, valid accuracy: 0.2485\n",
      "Iter-37700 train loss: 2.3778 valid loss: 2.1768, valid accuracy: 0.2485\n",
      "Iter-37800 train loss: 1.9582 valid loss: 2.1765, valid accuracy: 0.2477\n",
      "Iter-37900 train loss: 2.1156 valid loss: 2.1761, valid accuracy: 0.2492\n",
      "Iter-38000 train loss: 2.1474 valid loss: 2.1758, valid accuracy: 0.2500\n",
      "Iter-38100 train loss: 2.1237 valid loss: 2.1754, valid accuracy: 0.2492\n",
      "Iter-38200 train loss: 2.1525 valid loss: 2.1750, valid accuracy: 0.2492\n",
      "Iter-38300 train loss: 2.1480 valid loss: 2.1747, valid accuracy: 0.2485\n",
      "Iter-38400 train loss: 2.1590 valid loss: 2.1743, valid accuracy: 0.2469\n",
      "Iter-38500 train loss: 2.1265 valid loss: 2.1740, valid accuracy: 0.2469\n",
      "Iter-38600 train loss: 2.1669 valid loss: 2.1736, valid accuracy: 0.2454\n",
      "Iter-38700 train loss: 2.3072 valid loss: 2.1733, valid accuracy: 0.2477\n",
      "Iter-38800 train loss: 2.2056 valid loss: 2.1730, valid accuracy: 0.2469\n",
      "Iter-38900 train loss: 2.1387 valid loss: 2.1726, valid accuracy: 0.2477\n",
      "Iter-39000 train loss: 2.0840 valid loss: 2.1724, valid accuracy: 0.2462\n",
      "Iter-39100 train loss: 2.2140 valid loss: 2.1721, valid accuracy: 0.2477\n",
      "Iter-39200 train loss: 2.1189 valid loss: 2.1717, valid accuracy: 0.2485\n",
      "Iter-39300 train loss: 2.1193 valid loss: 2.1714, valid accuracy: 0.2492\n",
      "Iter-39400 train loss: 2.1909 valid loss: 2.1710, valid accuracy: 0.2492\n",
      "Iter-39500 train loss: 2.1386 valid loss: 2.1707, valid accuracy: 0.2500\n",
      "Iter-39600 train loss: 1.9971 valid loss: 2.1705, valid accuracy: 0.2515\n",
      "Iter-39700 train loss: 2.3105 valid loss: 2.1703, valid accuracy: 0.2523\n",
      "Iter-39800 train loss: 2.0764 valid loss: 2.1700, valid accuracy: 0.2523\n",
      "Iter-39900 train loss: 2.1703 valid loss: 2.1697, valid accuracy: 0.2538\n",
      "Iter-40000 train loss: 2.0459 valid loss: 2.1694, valid accuracy: 0.2523\n",
      "Iter-40100 train loss: 2.1008 valid loss: 2.1691, valid accuracy: 0.2531\n",
      "Iter-40200 train loss: 2.1688 valid loss: 2.1688, valid accuracy: 0.2531\n",
      "Iter-40300 train loss: 2.1707 valid loss: 2.1685, valid accuracy: 0.2531\n",
      "Iter-40400 train loss: 2.1236 valid loss: 2.1682, valid accuracy: 0.2538\n",
      "Iter-40500 train loss: 2.0723 valid loss: 2.1679, valid accuracy: 0.2531\n",
      "Iter-40600 train loss: 1.9093 valid loss: 2.1676, valid accuracy: 0.2523\n",
      "Iter-40700 train loss: 2.0952 valid loss: 2.1674, valid accuracy: 0.2531\n",
      "Iter-40800 train loss: 2.1984 valid loss: 2.1671, valid accuracy: 0.2523\n",
      "Iter-40900 train loss: 2.1214 valid loss: 2.1669, valid accuracy: 0.2523\n",
      "Iter-41000 train loss: 2.2310 valid loss: 2.1667, valid accuracy: 0.2492\n",
      "Iter-41100 train loss: 2.2385 valid loss: 2.1663, valid accuracy: 0.2500\n",
      "Iter-41200 train loss: 2.1812 valid loss: 2.1661, valid accuracy: 0.2500\n",
      "Iter-41300 train loss: 2.3369 valid loss: 2.1658, valid accuracy: 0.2500\n",
      "Iter-41400 train loss: 2.0616 valid loss: 2.1655, valid accuracy: 0.2492\n",
      "Iter-41500 train loss: 2.2110 valid loss: 2.1651, valid accuracy: 0.2500\n",
      "Iter-41600 train loss: 2.3251 valid loss: 2.1649, valid accuracy: 0.2508\n",
      "Iter-41700 train loss: 2.0823 valid loss: 2.1645, valid accuracy: 0.2515\n",
      "Iter-41800 train loss: 2.2035 valid loss: 2.1643, valid accuracy: 0.2500\n",
      "Iter-41900 train loss: 2.1158 valid loss: 2.1639, valid accuracy: 0.2492\n",
      "Iter-42000 train loss: 2.1392 valid loss: 2.1636, valid accuracy: 0.2500\n",
      "Iter-42100 train loss: 2.0628 valid loss: 2.1633, valid accuracy: 0.2485\n",
      "Iter-42200 train loss: 1.9920 valid loss: 2.1630, valid accuracy: 0.2500\n",
      "Iter-42300 train loss: 2.2367 valid loss: 2.1627, valid accuracy: 0.2500\n",
      "Iter-42400 train loss: 2.2918 valid loss: 2.1624, valid accuracy: 0.2515\n",
      "Iter-42500 train loss: 1.9940 valid loss: 2.1622, valid accuracy: 0.2515\n",
      "Iter-42600 train loss: 2.1312 valid loss: 2.1619, valid accuracy: 0.2531\n",
      "Iter-42700 train loss: 2.1223 valid loss: 2.1615, valid accuracy: 0.2523\n",
      "Iter-42800 train loss: 2.0511 valid loss: 2.1613, valid accuracy: 0.2523\n",
      "Iter-42900 train loss: 2.1124 valid loss: 2.1609, valid accuracy: 0.2515\n",
      "Iter-43000 train loss: 1.9691 valid loss: 2.1607, valid accuracy: 0.2523\n",
      "Iter-43100 train loss: 2.0150 valid loss: 2.1605, valid accuracy: 0.2508\n",
      "Iter-43200 train loss: 2.2217 valid loss: 2.1602, valid accuracy: 0.2508\n",
      "Iter-43300 train loss: 2.1183 valid loss: 2.1599, valid accuracy: 0.2508\n",
      "Iter-43400 train loss: 2.0795 valid loss: 2.1597, valid accuracy: 0.2508\n",
      "Iter-43500 train loss: 2.0395 valid loss: 2.1594, valid accuracy: 0.2508\n",
      "Iter-43600 train loss: 2.0953 valid loss: 2.1591, valid accuracy: 0.2515\n",
      "Iter-43700 train loss: 2.3267 valid loss: 2.1588, valid accuracy: 0.2515\n",
      "Iter-43800 train loss: 2.2461 valid loss: 2.1586, valid accuracy: 0.2515\n",
      "Iter-43900 train loss: 2.0247 valid loss: 2.1583, valid accuracy: 0.2515\n",
      "Iter-44000 train loss: 2.1867 valid loss: 2.1580, valid accuracy: 0.2523\n",
      "Iter-44100 train loss: 2.3341 valid loss: 2.1578, valid accuracy: 0.2531\n",
      "Iter-44200 train loss: 2.3743 valid loss: 2.1575, valid accuracy: 0.2523\n",
      "Iter-44300 train loss: 2.1258 valid loss: 2.1573, valid accuracy: 0.2531\n",
      "Iter-44400 train loss: 2.0328 valid loss: 2.1571, valid accuracy: 0.2523\n",
      "Iter-44500 train loss: 2.2038 valid loss: 2.1569, valid accuracy: 0.2523\n",
      "Iter-44600 train loss: 2.1887 valid loss: 2.1566, valid accuracy: 0.2531\n",
      "Iter-44700 train loss: 2.1781 valid loss: 2.1564, valid accuracy: 0.2531\n",
      "Iter-44800 train loss: 2.0794 valid loss: 2.1562, valid accuracy: 0.2531\n",
      "Iter-44900 train loss: 2.1960 valid loss: 2.1559, valid accuracy: 0.2538\n",
      "Iter-45000 train loss: 2.2168 valid loss: 2.1557, valid accuracy: 0.2538\n",
      "Iter-45100 train loss: 2.1464 valid loss: 2.1555, valid accuracy: 0.2554\n",
      "Iter-45200 train loss: 1.9406 valid loss: 2.1553, valid accuracy: 0.2554\n",
      "Iter-45300 train loss: 2.1088 valid loss: 2.1551, valid accuracy: 0.2538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 2.3465 valid loss: 2.1548, valid accuracy: 0.2554\n",
      "Iter-45500 train loss: 2.1831 valid loss: 2.1546, valid accuracy: 0.2562\n",
      "Iter-45600 train loss: 2.1993 valid loss: 2.1543, valid accuracy: 0.2546\n",
      "Iter-45700 train loss: 2.0088 valid loss: 2.1541, valid accuracy: 0.2554\n",
      "Iter-45800 train loss: 2.1764 valid loss: 2.1538, valid accuracy: 0.2577\n",
      "Iter-45900 train loss: 2.0216 valid loss: 2.1535, valid accuracy: 0.2585\n",
      "Iter-46000 train loss: 1.9101 valid loss: 2.1533, valid accuracy: 0.2585\n",
      "Iter-46100 train loss: 1.9898 valid loss: 2.1531, valid accuracy: 0.2585\n",
      "Iter-46200 train loss: 2.0433 valid loss: 2.1529, valid accuracy: 0.2585\n",
      "Iter-46300 train loss: 2.1910 valid loss: 2.1526, valid accuracy: 0.2569\n",
      "Iter-46400 train loss: 2.3352 valid loss: 2.1523, valid accuracy: 0.2569\n",
      "Iter-46500 train loss: 2.0738 valid loss: 2.1522, valid accuracy: 0.2569\n",
      "Iter-46600 train loss: 2.1841 valid loss: 2.1519, valid accuracy: 0.2577\n",
      "Iter-46700 train loss: 2.2194 valid loss: 2.1516, valid accuracy: 0.2577\n",
      "Iter-46800 train loss: 2.0380 valid loss: 2.1515, valid accuracy: 0.2600\n",
      "Iter-46900 train loss: 2.1209 valid loss: 2.1512, valid accuracy: 0.2600\n",
      "Iter-47000 train loss: 2.1275 valid loss: 2.1510, valid accuracy: 0.2585\n",
      "Iter-47100 train loss: 2.0501 valid loss: 2.1507, valid accuracy: 0.2592\n",
      "Iter-47200 train loss: 2.0387 valid loss: 2.1506, valid accuracy: 0.2592\n",
      "Iter-47300 train loss: 2.1566 valid loss: 2.1503, valid accuracy: 0.2608\n",
      "Iter-47400 train loss: 2.1995 valid loss: 2.1500, valid accuracy: 0.2592\n",
      "Iter-47500 train loss: 2.1199 valid loss: 2.1498, valid accuracy: 0.2585\n",
      "Iter-47600 train loss: 2.1216 valid loss: 2.1496, valid accuracy: 0.2592\n",
      "Iter-47700 train loss: 2.1347 valid loss: 2.1494, valid accuracy: 0.2608\n",
      "Iter-47800 train loss: 1.9631 valid loss: 2.1492, valid accuracy: 0.2608\n",
      "Iter-47900 train loss: 1.9758 valid loss: 2.1490, valid accuracy: 0.2600\n",
      "Iter-48000 train loss: 2.0493 valid loss: 2.1488, valid accuracy: 0.2608\n",
      "Iter-48100 train loss: 1.9998 valid loss: 2.1487, valid accuracy: 0.2592\n",
      "Iter-48200 train loss: 2.0597 valid loss: 2.1485, valid accuracy: 0.2600\n",
      "Iter-48300 train loss: 2.2386 valid loss: 2.1482, valid accuracy: 0.2600\n",
      "Iter-48400 train loss: 2.1409 valid loss: 2.1480, valid accuracy: 0.2585\n",
      "Iter-48500 train loss: 2.1196 valid loss: 2.1479, valid accuracy: 0.2569\n",
      "Iter-48600 train loss: 2.2067 valid loss: 2.1477, valid accuracy: 0.2569\n",
      "Iter-48700 train loss: 2.0952 valid loss: 2.1475, valid accuracy: 0.2585\n",
      "Iter-48800 train loss: 1.9658 valid loss: 2.1473, valid accuracy: 0.2577\n",
      "Iter-48900 train loss: 2.2163 valid loss: 2.1470, valid accuracy: 0.2585\n",
      "Iter-49000 train loss: 2.1150 valid loss: 2.1467, valid accuracy: 0.2577\n",
      "Iter-49100 train loss: 2.0270 valid loss: 2.1464, valid accuracy: 0.2569\n",
      "Iter-49200 train loss: 2.1820 valid loss: 2.1463, valid accuracy: 0.2569\n",
      "Iter-49300 train loss: 2.2826 valid loss: 2.1462, valid accuracy: 0.2585\n",
      "Iter-49400 train loss: 2.1551 valid loss: 2.1460, valid accuracy: 0.2615\n",
      "Iter-49500 train loss: 2.1356 valid loss: 2.1459, valid accuracy: 0.2600\n",
      "Iter-49600 train loss: 1.9987 valid loss: 2.1458, valid accuracy: 0.2600\n",
      "Iter-49700 train loss: 2.1607 valid loss: 2.1456, valid accuracy: 0.2600\n",
      "Iter-49800 train loss: 2.1330 valid loss: 2.1453, valid accuracy: 0.2631\n",
      "Iter-49900 train loss: 2.1842 valid loss: 2.1452, valid accuracy: 0.2638\n",
      "Iter-50000 train loss: 1.9446 valid loss: 2.1450, valid accuracy: 0.2631\n",
      "Iter-50100 train loss: 2.1400 valid loss: 2.1448, valid accuracy: 0.2646\n",
      "Iter-50200 train loss: 2.1021 valid loss: 2.1445, valid accuracy: 0.2623\n",
      "Iter-50300 train loss: 2.2019 valid loss: 2.1443, valid accuracy: 0.2638\n",
      "Iter-50400 train loss: 2.2021 valid loss: 2.1440, valid accuracy: 0.2615\n",
      "Iter-50500 train loss: 2.4172 valid loss: 2.1439, valid accuracy: 0.2623\n",
      "Iter-50600 train loss: 2.1230 valid loss: 2.1437, valid accuracy: 0.2623\n",
      "Iter-50700 train loss: 2.1157 valid loss: 2.1435, valid accuracy: 0.2638\n",
      "Iter-50800 train loss: 2.0378 valid loss: 2.1433, valid accuracy: 0.2646\n",
      "Iter-50900 train loss: 1.9081 valid loss: 2.1432, valid accuracy: 0.2631\n",
      "Iter-51000 train loss: 2.3602 valid loss: 2.1430, valid accuracy: 0.2646\n",
      "Iter-51100 train loss: 2.0901 valid loss: 2.1428, valid accuracy: 0.2631\n",
      "Iter-51200 train loss: 2.0357 valid loss: 2.1426, valid accuracy: 0.2638\n",
      "Iter-51300 train loss: 2.0832 valid loss: 2.1425, valid accuracy: 0.2646\n",
      "Iter-51400 train loss: 1.9970 valid loss: 2.1422, valid accuracy: 0.2638\n",
      "Iter-51500 train loss: 2.3038 valid loss: 2.1420, valid accuracy: 0.2631\n",
      "Iter-51600 train loss: 2.0715 valid loss: 2.1418, valid accuracy: 0.2631\n",
      "Iter-51700 train loss: 2.3006 valid loss: 2.1416, valid accuracy: 0.2631\n",
      "Iter-51800 train loss: 2.2455 valid loss: 2.1414, valid accuracy: 0.2623\n",
      "Iter-51900 train loss: 1.9583 valid loss: 2.1412, valid accuracy: 0.2654\n",
      "Iter-52000 train loss: 2.1856 valid loss: 2.1411, valid accuracy: 0.2669\n",
      "Iter-52100 train loss: 1.9971 valid loss: 2.1411, valid accuracy: 0.2662\n",
      "Iter-52200 train loss: 2.2179 valid loss: 2.1409, valid accuracy: 0.2662\n",
      "Iter-52300 train loss: 2.2283 valid loss: 2.1407, valid accuracy: 0.2662\n",
      "Iter-52400 train loss: 2.0397 valid loss: 2.1405, valid accuracy: 0.2662\n",
      "Iter-52500 train loss: 1.9864 valid loss: 2.1403, valid accuracy: 0.2646\n",
      "Iter-52600 train loss: 2.1797 valid loss: 2.1401, valid accuracy: 0.2654\n",
      "Iter-52700 train loss: 2.1381 valid loss: 2.1398, valid accuracy: 0.2654\n",
      "Iter-52800 train loss: 1.9320 valid loss: 2.1397, valid accuracy: 0.2646\n",
      "Iter-52900 train loss: 2.2273 valid loss: 2.1396, valid accuracy: 0.2654\n",
      "Iter-53000 train loss: 2.1631 valid loss: 2.1395, valid accuracy: 0.2654\n",
      "Iter-53100 train loss: 2.1528 valid loss: 2.1392, valid accuracy: 0.2662\n",
      "Iter-53200 train loss: 1.9362 valid loss: 2.1390, valid accuracy: 0.2677\n",
      "Iter-53300 train loss: 2.0590 valid loss: 2.1389, valid accuracy: 0.2685\n",
      "Iter-53400 train loss: 2.2350 valid loss: 2.1387, valid accuracy: 0.2662\n",
      "Iter-53500 train loss: 2.1389 valid loss: 2.1386, valid accuracy: 0.2662\n",
      "Iter-53600 train loss: 2.1385 valid loss: 2.1385, valid accuracy: 0.2662\n",
      "Iter-53700 train loss: 2.0361 valid loss: 2.1382, valid accuracy: 0.2662\n",
      "Iter-53800 train loss: 1.9478 valid loss: 2.1381, valid accuracy: 0.2654\n",
      "Iter-53900 train loss: 2.1141 valid loss: 2.1379, valid accuracy: 0.2669\n",
      "Iter-54000 train loss: 2.1171 valid loss: 2.1378, valid accuracy: 0.2654\n",
      "Iter-54100 train loss: 2.0070 valid loss: 2.1376, valid accuracy: 0.2654\n",
      "Iter-54200 train loss: 2.2006 valid loss: 2.1374, valid accuracy: 0.2677\n",
      "Iter-54300 train loss: 2.0984 valid loss: 2.1371, valid accuracy: 0.2677\n",
      "Iter-54400 train loss: 2.0530 valid loss: 2.1370, valid accuracy: 0.2692\n",
      "Iter-54500 train loss: 2.0817 valid loss: 2.1367, valid accuracy: 0.2692\n",
      "Iter-54600 train loss: 2.0678 valid loss: 2.1366, valid accuracy: 0.2685\n",
      "Iter-54700 train loss: 2.1217 valid loss: 2.1365, valid accuracy: 0.2692\n",
      "Iter-54800 train loss: 2.1571 valid loss: 2.1363, valid accuracy: 0.2692\n",
      "Iter-54900 train loss: 2.0639 valid loss: 2.1362, valid accuracy: 0.2692\n",
      "Iter-55000 train loss: 2.1626 valid loss: 2.1361, valid accuracy: 0.2700\n",
      "Iter-55100 train loss: 2.3551 valid loss: 2.1359, valid accuracy: 0.2685\n",
      "Iter-55200 train loss: 2.1601 valid loss: 2.1358, valid accuracy: 0.2685\n",
      "Iter-55300 train loss: 2.1200 valid loss: 2.1357, valid accuracy: 0.2692\n",
      "Iter-55400 train loss: 2.2304 valid loss: 2.1355, valid accuracy: 0.2685\n",
      "Iter-55500 train loss: 2.2677 valid loss: 2.1354, valid accuracy: 0.2685\n",
      "Iter-55600 train loss: 2.2779 valid loss: 2.1352, valid accuracy: 0.2677\n",
      "Iter-55700 train loss: 2.1166 valid loss: 2.1350, valid accuracy: 0.2677\n",
      "Iter-55800 train loss: 2.1531 valid loss: 2.1348, valid accuracy: 0.2685\n",
      "Iter-55900 train loss: 1.9948 valid loss: 2.1348, valid accuracy: 0.2669\n",
      "Iter-56000 train loss: 1.8645 valid loss: 2.1346, valid accuracy: 0.2685\n",
      "Iter-56100 train loss: 2.0300 valid loss: 2.1345, valid accuracy: 0.2700\n",
      "Iter-56200 train loss: 2.1364 valid loss: 2.1343, valid accuracy: 0.2692\n",
      "Iter-56300 train loss: 2.1892 valid loss: 2.1342, valid accuracy: 0.2700\n",
      "Iter-56400 train loss: 2.0622 valid loss: 2.1341, valid accuracy: 0.2700\n",
      "Iter-56500 train loss: 2.0158 valid loss: 2.1339, valid accuracy: 0.2700\n",
      "Iter-56600 train loss: 2.1721 valid loss: 2.1336, valid accuracy: 0.2692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 2.2827 valid loss: 2.1333, valid accuracy: 0.2700\n",
      "Iter-56800 train loss: 1.9777 valid loss: 2.1332, valid accuracy: 0.2700\n",
      "Iter-56900 train loss: 1.8644 valid loss: 2.1330, valid accuracy: 0.2692\n",
      "Iter-57000 train loss: 1.9069 valid loss: 2.1329, valid accuracy: 0.2708\n",
      "Iter-57100 train loss: 2.3243 valid loss: 2.1326, valid accuracy: 0.2715\n",
      "Iter-57200 train loss: 1.8500 valid loss: 2.1325, valid accuracy: 0.2700\n",
      "Iter-57300 train loss: 1.9907 valid loss: 2.1323, valid accuracy: 0.2700\n",
      "Iter-57400 train loss: 2.0844 valid loss: 2.1321, valid accuracy: 0.2708\n",
      "Iter-57500 train loss: 2.2656 valid loss: 2.1320, valid accuracy: 0.2715\n",
      "Iter-57600 train loss: 2.0594 valid loss: 2.1318, valid accuracy: 0.2708\n",
      "Iter-57700 train loss: 2.2317 valid loss: 2.1318, valid accuracy: 0.2708\n",
      "Iter-57800 train loss: 2.0968 valid loss: 2.1317, valid accuracy: 0.2708\n",
      "Iter-57900 train loss: 2.2421 valid loss: 2.1316, valid accuracy: 0.2715\n",
      "Iter-58000 train loss: 2.1611 valid loss: 2.1314, valid accuracy: 0.2746\n",
      "Iter-58100 train loss: 2.1555 valid loss: 2.1312, valid accuracy: 0.2738\n",
      "Iter-58200 train loss: 2.0653 valid loss: 2.1310, valid accuracy: 0.2738\n",
      "Iter-58300 train loss: 2.0620 valid loss: 2.1308, valid accuracy: 0.2738\n",
      "Iter-58400 train loss: 2.1697 valid loss: 2.1308, valid accuracy: 0.2723\n",
      "Iter-58500 train loss: 1.9918 valid loss: 2.1306, valid accuracy: 0.2723\n",
      "Iter-58600 train loss: 2.0126 valid loss: 2.1306, valid accuracy: 0.2723\n",
      "Iter-58700 train loss: 2.0423 valid loss: 2.1303, valid accuracy: 0.2723\n",
      "Iter-58800 train loss: 2.2404 valid loss: 2.1301, valid accuracy: 0.2746\n",
      "Iter-58900 train loss: 2.2229 valid loss: 2.1299, valid accuracy: 0.2738\n",
      "Iter-59000 train loss: 2.1294 valid loss: 2.1298, valid accuracy: 0.2754\n",
      "Iter-59100 train loss: 2.4594 valid loss: 2.1296, valid accuracy: 0.2762\n",
      "Iter-59200 train loss: 2.1240 valid loss: 2.1295, valid accuracy: 0.2762\n",
      "Iter-59300 train loss: 2.0964 valid loss: 2.1295, valid accuracy: 0.2769\n",
      "Iter-59400 train loss: 1.8781 valid loss: 2.1294, valid accuracy: 0.2762\n",
      "Iter-59500 train loss: 2.0659 valid loss: 2.1291, valid accuracy: 0.2769\n",
      "Iter-59600 train loss: 2.1558 valid loss: 2.1290, valid accuracy: 0.2769\n",
      "Iter-59700 train loss: 2.2453 valid loss: 2.1288, valid accuracy: 0.2762\n",
      "Iter-59800 train loss: 2.1179 valid loss: 2.1286, valid accuracy: 0.2769\n",
      "Iter-59900 train loss: 1.9692 valid loss: 2.1284, valid accuracy: 0.2762\n",
      "Iter-60000 train loss: 2.1298 valid loss: 2.1283, valid accuracy: 0.2769\n",
      "Iter-60100 train loss: 1.8823 valid loss: 2.1283, valid accuracy: 0.2754\n",
      "Iter-60200 train loss: 2.3522 valid loss: 2.1282, valid accuracy: 0.2746\n",
      "Iter-60300 train loss: 2.0337 valid loss: 2.1281, valid accuracy: 0.2762\n",
      "Iter-60400 train loss: 1.9505 valid loss: 2.1279, valid accuracy: 0.2762\n",
      "Iter-60500 train loss: 2.2935 valid loss: 2.1277, valid accuracy: 0.2762\n",
      "Iter-60600 train loss: 2.0694 valid loss: 2.1275, valid accuracy: 0.2762\n",
      "Iter-60700 train loss: 2.0200 valid loss: 2.1274, valid accuracy: 0.2777\n",
      "Iter-60800 train loss: 2.1790 valid loss: 2.1273, valid accuracy: 0.2792\n",
      "Iter-60900 train loss: 2.1738 valid loss: 2.1271, valid accuracy: 0.2792\n",
      "Iter-61000 train loss: 2.2954 valid loss: 2.1271, valid accuracy: 0.2792\n",
      "Iter-61100 train loss: 2.3082 valid loss: 2.1270, valid accuracy: 0.2800\n",
      "Iter-61200 train loss: 2.0677 valid loss: 2.1269, valid accuracy: 0.2792\n",
      "Iter-61300 train loss: 2.1411 valid loss: 2.1268, valid accuracy: 0.2792\n",
      "Iter-61400 train loss: 2.1795 valid loss: 2.1267, valid accuracy: 0.2792\n",
      "Iter-61500 train loss: 2.2136 valid loss: 2.1266, valid accuracy: 0.2792\n",
      "Iter-61600 train loss: 2.2499 valid loss: 2.1265, valid accuracy: 0.2808\n",
      "Iter-61700 train loss: 1.8562 valid loss: 2.1263, valid accuracy: 0.2800\n",
      "Iter-61800 train loss: 2.2521 valid loss: 2.1262, valid accuracy: 0.2792\n",
      "Iter-61900 train loss: 2.1634 valid loss: 2.1260, valid accuracy: 0.2792\n",
      "Iter-62000 train loss: 1.8947 valid loss: 2.1260, valid accuracy: 0.2792\n",
      "Iter-62100 train loss: 2.1702 valid loss: 2.1258, valid accuracy: 0.2800\n",
      "Iter-62200 train loss: 2.1132 valid loss: 2.1257, valid accuracy: 0.2800\n",
      "Iter-62300 train loss: 1.9913 valid loss: 2.1255, valid accuracy: 0.2815\n",
      "Iter-62400 train loss: 2.0367 valid loss: 2.1254, valid accuracy: 0.2808\n",
      "Iter-62500 train loss: 2.0502 valid loss: 2.1253, valid accuracy: 0.2792\n",
      "Iter-62600 train loss: 2.1551 valid loss: 2.1251, valid accuracy: 0.2808\n",
      "Iter-62700 train loss: 2.2152 valid loss: 2.1249, valid accuracy: 0.2815\n",
      "Iter-62800 train loss: 2.0156 valid loss: 2.1248, valid accuracy: 0.2815\n",
      "Iter-62900 train loss: 1.8349 valid loss: 2.1247, valid accuracy: 0.2815\n",
      "Iter-63000 train loss: 2.1381 valid loss: 2.1246, valid accuracy: 0.2800\n",
      "Iter-63100 train loss: 2.0591 valid loss: 2.1245, valid accuracy: 0.2823\n",
      "Iter-63200 train loss: 2.1216 valid loss: 2.1245, valid accuracy: 0.2823\n",
      "Iter-63300 train loss: 2.1638 valid loss: 2.1243, valid accuracy: 0.2815\n",
      "Iter-63400 train loss: 1.8790 valid loss: 2.1242, valid accuracy: 0.2815\n",
      "Iter-63500 train loss: 2.0366 valid loss: 2.1241, valid accuracy: 0.2831\n",
      "Iter-63600 train loss: 2.0171 valid loss: 2.1239, valid accuracy: 0.2838\n",
      "Iter-63700 train loss: 2.1648 valid loss: 2.1238, valid accuracy: 0.2815\n",
      "Iter-63800 train loss: 2.1440 valid loss: 2.1237, valid accuracy: 0.2808\n",
      "Iter-63900 train loss: 2.0299 valid loss: 2.1236, valid accuracy: 0.2808\n",
      "Iter-64000 train loss: 1.8112 valid loss: 2.1235, valid accuracy: 0.2792\n",
      "Iter-64100 train loss: 1.9792 valid loss: 2.1234, valid accuracy: 0.2800\n",
      "Iter-64200 train loss: 1.9387 valid loss: 2.1233, valid accuracy: 0.2823\n",
      "Iter-64300 train loss: 1.9145 valid loss: 2.1233, valid accuracy: 0.2838\n",
      "Iter-64400 train loss: 1.9598 valid loss: 2.1232, valid accuracy: 0.2823\n",
      "Iter-64500 train loss: 2.0416 valid loss: 2.1231, valid accuracy: 0.2831\n",
      "Iter-64600 train loss: 1.9772 valid loss: 2.1230, valid accuracy: 0.2838\n",
      "Iter-64700 train loss: 2.0658 valid loss: 2.1229, valid accuracy: 0.2823\n",
      "Iter-64800 train loss: 2.1735 valid loss: 2.1228, valid accuracy: 0.2838\n",
      "Iter-64900 train loss: 2.0716 valid loss: 2.1227, valid accuracy: 0.2831\n",
      "Iter-65000 train loss: 1.8968 valid loss: 2.1226, valid accuracy: 0.2831\n",
      "Iter-65100 train loss: 2.0570 valid loss: 2.1226, valid accuracy: 0.2815\n",
      "Iter-65200 train loss: 2.0101 valid loss: 2.1223, valid accuracy: 0.2823\n",
      "Iter-65300 train loss: 2.0523 valid loss: 2.1222, valid accuracy: 0.2808\n",
      "Iter-65400 train loss: 2.2448 valid loss: 2.1220, valid accuracy: 0.2808\n",
      "Iter-65500 train loss: 1.9815 valid loss: 2.1219, valid accuracy: 0.2808\n",
      "Iter-65600 train loss: 2.2274 valid loss: 2.1218, valid accuracy: 0.2808\n",
      "Iter-65700 train loss: 2.1732 valid loss: 2.1217, valid accuracy: 0.2800\n",
      "Iter-65800 train loss: 2.1354 valid loss: 2.1216, valid accuracy: 0.2815\n",
      "Iter-65900 train loss: 1.8645 valid loss: 2.1214, valid accuracy: 0.2823\n",
      "Iter-66000 train loss: 2.0453 valid loss: 2.1213, valid accuracy: 0.2823\n",
      "Iter-66100 train loss: 2.2018 valid loss: 2.1212, valid accuracy: 0.2838\n",
      "Iter-66200 train loss: 2.3671 valid loss: 2.1210, valid accuracy: 0.2846\n",
      "Iter-66300 train loss: 2.1386 valid loss: 2.1209, valid accuracy: 0.2831\n",
      "Iter-66400 train loss: 2.1665 valid loss: 2.1208, valid accuracy: 0.2869\n",
      "Iter-66500 train loss: 2.0224 valid loss: 2.1205, valid accuracy: 0.2862\n",
      "Iter-66600 train loss: 1.8468 valid loss: 2.1205, valid accuracy: 0.2869\n",
      "Iter-66700 train loss: 2.1188 valid loss: 2.1204, valid accuracy: 0.2854\n",
      "Iter-66800 train loss: 1.9193 valid loss: 2.1203, valid accuracy: 0.2846\n",
      "Iter-66900 train loss: 2.0581 valid loss: 2.1203, valid accuracy: 0.2854\n",
      "Iter-67000 train loss: 2.0678 valid loss: 2.1201, valid accuracy: 0.2862\n",
      "Iter-67100 train loss: 2.1302 valid loss: 2.1201, valid accuracy: 0.2862\n",
      "Iter-67200 train loss: 2.0117 valid loss: 2.1201, valid accuracy: 0.2854\n",
      "Iter-67300 train loss: 2.1105 valid loss: 2.1200, valid accuracy: 0.2854\n",
      "Iter-67400 train loss: 2.0324 valid loss: 2.1199, valid accuracy: 0.2869\n",
      "Iter-67500 train loss: 1.9618 valid loss: 2.1197, valid accuracy: 0.2854\n",
      "Iter-67600 train loss: 2.0371 valid loss: 2.1196, valid accuracy: 0.2854\n",
      "Iter-67700 train loss: 1.9723 valid loss: 2.1194, valid accuracy: 0.2862\n",
      "Iter-67800 train loss: 2.1178 valid loss: 2.1194, valid accuracy: 0.2854\n",
      "Iter-67900 train loss: 2.1310 valid loss: 2.1192, valid accuracy: 0.2846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 2.0499 valid loss: 2.1191, valid accuracy: 0.2831\n",
      "Iter-68100 train loss: 1.9417 valid loss: 2.1190, valid accuracy: 0.2846\n",
      "Iter-68200 train loss: 1.9388 valid loss: 2.1189, valid accuracy: 0.2838\n",
      "Iter-68300 train loss: 1.9895 valid loss: 2.1189, valid accuracy: 0.2838\n",
      "Iter-68400 train loss: 1.9695 valid loss: 2.1188, valid accuracy: 0.2846\n",
      "Iter-68500 train loss: 1.9329 valid loss: 2.1187, valid accuracy: 0.2838\n",
      "Iter-68600 train loss: 2.3963 valid loss: 2.1185, valid accuracy: 0.2838\n",
      "Iter-68700 train loss: 1.9667 valid loss: 2.1183, valid accuracy: 0.2869\n",
      "Iter-68800 train loss: 2.2295 valid loss: 2.1182, valid accuracy: 0.2862\n",
      "Iter-68900 train loss: 2.1757 valid loss: 2.1180, valid accuracy: 0.2854\n",
      "Iter-69000 train loss: 2.1568 valid loss: 2.1179, valid accuracy: 0.2892\n",
      "Iter-69100 train loss: 2.1423 valid loss: 2.1178, valid accuracy: 0.2877\n",
      "Iter-69200 train loss: 2.1778 valid loss: 2.1176, valid accuracy: 0.2885\n",
      "Iter-69300 train loss: 2.0940 valid loss: 2.1176, valid accuracy: 0.2908\n",
      "Iter-69400 train loss: 2.1352 valid loss: 2.1174, valid accuracy: 0.2915\n",
      "Iter-69500 train loss: 2.0286 valid loss: 2.1173, valid accuracy: 0.2908\n",
      "Iter-69600 train loss: 1.8747 valid loss: 2.1172, valid accuracy: 0.2900\n",
      "Iter-69700 train loss: 2.1187 valid loss: 2.1171, valid accuracy: 0.2908\n",
      "Iter-69800 train loss: 2.1707 valid loss: 2.1169, valid accuracy: 0.2900\n",
      "Iter-69900 train loss: 2.0604 valid loss: 2.1168, valid accuracy: 0.2900\n",
      "Iter-70000 train loss: 1.8913 valid loss: 2.1168, valid accuracy: 0.2892\n",
      "Iter-70100 train loss: 2.0835 valid loss: 2.1168, valid accuracy: 0.2892\n",
      "Iter-70200 train loss: 2.0932 valid loss: 2.1167, valid accuracy: 0.2892\n",
      "Iter-70300 train loss: 2.0628 valid loss: 2.1167, valid accuracy: 0.2885\n",
      "Iter-70400 train loss: 1.9835 valid loss: 2.1166, valid accuracy: 0.2892\n",
      "Iter-70500 train loss: 2.1289 valid loss: 2.1165, valid accuracy: 0.2885\n",
      "Iter-70600 train loss: 2.1002 valid loss: 2.1164, valid accuracy: 0.2885\n",
      "Iter-70700 train loss: 2.1926 valid loss: 2.1164, valid accuracy: 0.2885\n",
      "Iter-70800 train loss: 1.9703 valid loss: 2.1163, valid accuracy: 0.2885\n",
      "Iter-70900 train loss: 1.9992 valid loss: 2.1161, valid accuracy: 0.2877\n",
      "Iter-71000 train loss: 2.0345 valid loss: 2.1160, valid accuracy: 0.2877\n",
      "Iter-71100 train loss: 2.2812 valid loss: 2.1159, valid accuracy: 0.2885\n",
      "Iter-71200 train loss: 2.0715 valid loss: 2.1158, valid accuracy: 0.2877\n",
      "Iter-71300 train loss: 2.1922 valid loss: 2.1157, valid accuracy: 0.2892\n",
      "Iter-71400 train loss: 1.8102 valid loss: 2.1155, valid accuracy: 0.2892\n",
      "Iter-71500 train loss: 2.0845 valid loss: 2.1153, valid accuracy: 0.2892\n",
      "Iter-71600 train loss: 1.8226 valid loss: 2.1153, valid accuracy: 0.2892\n",
      "Iter-71700 train loss: 2.2292 valid loss: 2.1152, valid accuracy: 0.2885\n",
      "Iter-71800 train loss: 2.0924 valid loss: 2.1151, valid accuracy: 0.2885\n",
      "Iter-71900 train loss: 2.1286 valid loss: 2.1149, valid accuracy: 0.2885\n",
      "Iter-72000 train loss: 2.1146 valid loss: 2.1149, valid accuracy: 0.2877\n",
      "Iter-72100 train loss: 1.9980 valid loss: 2.1147, valid accuracy: 0.2892\n",
      "Iter-72200 train loss: 2.2562 valid loss: 2.1146, valid accuracy: 0.2900\n",
      "Iter-72300 train loss: 2.0499 valid loss: 2.1145, valid accuracy: 0.2892\n",
      "Iter-72400 train loss: 2.1722 valid loss: 2.1144, valid accuracy: 0.2892\n",
      "Iter-72500 train loss: 2.0715 valid loss: 2.1143, valid accuracy: 0.2885\n",
      "Iter-72600 train loss: 2.3407 valid loss: 2.1143, valid accuracy: 0.2892\n",
      "Iter-72700 train loss: 2.1212 valid loss: 2.1143, valid accuracy: 0.2869\n",
      "Iter-72800 train loss: 2.2028 valid loss: 2.1142, valid accuracy: 0.2854\n",
      "Iter-72900 train loss: 2.0740 valid loss: 2.1141, valid accuracy: 0.2877\n",
      "Iter-73000 train loss: 2.2591 valid loss: 2.1141, valid accuracy: 0.2877\n",
      "Iter-73100 train loss: 2.0582 valid loss: 2.1140, valid accuracy: 0.2885\n",
      "Iter-73200 train loss: 2.1849 valid loss: 2.1139, valid accuracy: 0.2877\n",
      "Iter-73300 train loss: 1.6796 valid loss: 2.1138, valid accuracy: 0.2877\n",
      "Iter-73400 train loss: 2.0664 valid loss: 2.1137, valid accuracy: 0.2877\n",
      "Iter-73500 train loss: 2.0205 valid loss: 2.1136, valid accuracy: 0.2877\n",
      "Iter-73600 train loss: 2.1327 valid loss: 2.1136, valid accuracy: 0.2892\n",
      "Iter-73700 train loss: 1.8667 valid loss: 2.1135, valid accuracy: 0.2885\n",
      "Iter-73800 train loss: 2.1658 valid loss: 2.1134, valid accuracy: 0.2869\n",
      "Iter-73900 train loss: 1.9721 valid loss: 2.1133, valid accuracy: 0.2869\n",
      "Iter-74000 train loss: 1.9691 valid loss: 2.1133, valid accuracy: 0.2869\n",
      "Iter-74100 train loss: 2.3033 valid loss: 2.1132, valid accuracy: 0.2854\n",
      "Iter-74200 train loss: 1.8782 valid loss: 2.1131, valid accuracy: 0.2854\n",
      "Iter-74300 train loss: 1.9820 valid loss: 2.1131, valid accuracy: 0.2854\n",
      "Iter-74400 train loss: 2.0953 valid loss: 2.1130, valid accuracy: 0.2846\n",
      "Iter-74500 train loss: 2.1130 valid loss: 2.1131, valid accuracy: 0.2838\n",
      "Iter-74600 train loss: 2.0748 valid loss: 2.1130, valid accuracy: 0.2838\n",
      "Iter-74700 train loss: 2.1171 valid loss: 2.1129, valid accuracy: 0.2838\n",
      "Iter-74800 train loss: 2.1972 valid loss: 2.1127, valid accuracy: 0.2838\n",
      "Iter-74900 train loss: 2.0266 valid loss: 2.1126, valid accuracy: 0.2838\n",
      "Iter-75000 train loss: 2.1841 valid loss: 2.1126, valid accuracy: 0.2838\n",
      "Iter-75100 train loss: 2.1585 valid loss: 2.1126, valid accuracy: 0.2838\n",
      "Iter-75200 train loss: 2.3519 valid loss: 2.1124, valid accuracy: 0.2831\n",
      "Iter-75300 train loss: 2.0708 valid loss: 2.1123, valid accuracy: 0.2838\n",
      "Iter-75400 train loss: 2.0410 valid loss: 2.1123, valid accuracy: 0.2846\n",
      "Iter-75500 train loss: 1.8630 valid loss: 2.1122, valid accuracy: 0.2854\n",
      "Iter-75600 train loss: 1.8720 valid loss: 2.1120, valid accuracy: 0.2846\n",
      "Iter-75700 train loss: 2.0994 valid loss: 2.1120, valid accuracy: 0.2838\n",
      "Iter-75800 train loss: 2.2229 valid loss: 2.1119, valid accuracy: 0.2846\n",
      "Iter-75900 train loss: 1.8217 valid loss: 2.1118, valid accuracy: 0.2846\n",
      "Iter-76000 train loss: 2.1200 valid loss: 2.1116, valid accuracy: 0.2846\n",
      "Iter-76100 train loss: 2.0612 valid loss: 2.1114, valid accuracy: 0.2846\n",
      "Iter-76200 train loss: 2.2069 valid loss: 2.1113, valid accuracy: 0.2838\n",
      "Iter-76300 train loss: 1.8300 valid loss: 2.1113, valid accuracy: 0.2838\n",
      "Iter-76400 train loss: 2.0449 valid loss: 2.1111, valid accuracy: 0.2838\n",
      "Iter-76500 train loss: 2.1510 valid loss: 2.1111, valid accuracy: 0.2838\n",
      "Iter-76600 train loss: 2.0316 valid loss: 2.1110, valid accuracy: 0.2838\n",
      "Iter-76700 train loss: 2.1183 valid loss: 2.1109, valid accuracy: 0.2838\n",
      "Iter-76800 train loss: 1.8102 valid loss: 2.1108, valid accuracy: 0.2831\n",
      "Iter-76900 train loss: 2.0654 valid loss: 2.1107, valid accuracy: 0.2838\n",
      "Iter-77000 train loss: 2.2129 valid loss: 2.1107, valid accuracy: 0.2831\n",
      "Iter-77100 train loss: 1.9674 valid loss: 2.1105, valid accuracy: 0.2846\n",
      "Iter-77200 train loss: 2.0882 valid loss: 2.1104, valid accuracy: 0.2854\n",
      "Iter-77300 train loss: 2.0810 valid loss: 2.1103, valid accuracy: 0.2854\n",
      "Iter-77400 train loss: 1.9981 valid loss: 2.1101, valid accuracy: 0.2869\n",
      "Iter-77500 train loss: 1.8239 valid loss: 2.1099, valid accuracy: 0.2877\n",
      "Iter-77600 train loss: 1.9962 valid loss: 2.1098, valid accuracy: 0.2869\n",
      "Iter-77700 train loss: 1.7598 valid loss: 2.1097, valid accuracy: 0.2877\n",
      "Iter-77800 train loss: 1.9870 valid loss: 2.1095, valid accuracy: 0.2877\n",
      "Iter-77900 train loss: 2.0065 valid loss: 2.1095, valid accuracy: 0.2877\n",
      "Iter-78000 train loss: 2.0620 valid loss: 2.1093, valid accuracy: 0.2877\n",
      "Iter-78100 train loss: 1.9389 valid loss: 2.1092, valid accuracy: 0.2869\n",
      "Iter-78200 train loss: 2.0174 valid loss: 2.1092, valid accuracy: 0.2854\n",
      "Iter-78300 train loss: 1.9637 valid loss: 2.1092, valid accuracy: 0.2854\n",
      "Iter-78400 train loss: 2.0684 valid loss: 2.1091, valid accuracy: 0.2846\n",
      "Iter-78500 train loss: 1.9398 valid loss: 2.1090, valid accuracy: 0.2854\n",
      "Iter-78600 train loss: 2.0166 valid loss: 2.1090, valid accuracy: 0.2854\n",
      "Iter-78700 train loss: 2.1776 valid loss: 2.1089, valid accuracy: 0.2877\n",
      "Iter-78800 train loss: 2.0716 valid loss: 2.1087, valid accuracy: 0.2869\n",
      "Iter-78900 train loss: 1.9281 valid loss: 2.1086, valid accuracy: 0.2854\n",
      "Iter-79000 train loss: 2.0859 valid loss: 2.1085, valid accuracy: 0.2862\n",
      "Iter-79100 train loss: 2.0247 valid loss: 2.1085, valid accuracy: 0.2854\n",
      "Iter-79200 train loss: 2.2610 valid loss: 2.1084, valid accuracy: 0.2854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 2.0531 valid loss: 2.1083, valid accuracy: 0.2846\n",
      "Iter-79400 train loss: 2.1940 valid loss: 2.1083, valid accuracy: 0.2846\n",
      "Iter-79500 train loss: 1.8194 valid loss: 2.1083, valid accuracy: 0.2838\n",
      "Iter-79600 train loss: 1.9143 valid loss: 2.1082, valid accuracy: 0.2838\n",
      "Iter-79700 train loss: 2.1801 valid loss: 2.1081, valid accuracy: 0.2838\n",
      "Iter-79800 train loss: 1.9055 valid loss: 2.1079, valid accuracy: 0.2838\n",
      "Iter-79900 train loss: 2.1344 valid loss: 2.1077, valid accuracy: 0.2846\n",
      "Iter-80000 train loss: 1.8093 valid loss: 2.1075, valid accuracy: 0.2838\n",
      "Iter-80100 train loss: 2.1819 valid loss: 2.1075, valid accuracy: 0.2846\n",
      "Iter-80200 train loss: 2.1172 valid loss: 2.1073, valid accuracy: 0.2846\n",
      "Iter-80300 train loss: 2.1336 valid loss: 2.1072, valid accuracy: 0.2846\n",
      "Iter-80400 train loss: 2.1112 valid loss: 2.1071, valid accuracy: 0.2846\n",
      "Iter-80500 train loss: 2.3366 valid loss: 2.1072, valid accuracy: 0.2854\n",
      "Iter-80600 train loss: 2.1587 valid loss: 2.1070, valid accuracy: 0.2869\n",
      "Iter-80700 train loss: 1.9218 valid loss: 2.1069, valid accuracy: 0.2869\n",
      "Iter-80800 train loss: 2.0225 valid loss: 2.1068, valid accuracy: 0.2862\n",
      "Iter-80900 train loss: 2.0252 valid loss: 2.1067, valid accuracy: 0.2854\n",
      "Iter-81000 train loss: 2.0784 valid loss: 2.1066, valid accuracy: 0.2862\n",
      "Iter-81100 train loss: 2.0094 valid loss: 2.1065, valid accuracy: 0.2869\n",
      "Iter-81200 train loss: 2.0288 valid loss: 2.1064, valid accuracy: 0.2862\n",
      "Iter-81300 train loss: 2.0350 valid loss: 2.1063, valid accuracy: 0.2862\n",
      "Iter-81400 train loss: 2.0561 valid loss: 2.1062, valid accuracy: 0.2862\n",
      "Iter-81500 train loss: 2.2388 valid loss: 2.1061, valid accuracy: 0.2862\n",
      "Iter-81600 train loss: 2.1811 valid loss: 2.1059, valid accuracy: 0.2862\n",
      "Iter-81700 train loss: 2.1190 valid loss: 2.1059, valid accuracy: 0.2854\n",
      "Iter-81800 train loss: 2.1520 valid loss: 2.1058, valid accuracy: 0.2862\n",
      "Iter-81900 train loss: 1.8860 valid loss: 2.1057, valid accuracy: 0.2854\n",
      "Iter-82000 train loss: 2.0009 valid loss: 2.1056, valid accuracy: 0.2854\n",
      "Iter-82100 train loss: 2.0762 valid loss: 2.1055, valid accuracy: 0.2862\n",
      "Iter-82200 train loss: 1.7844 valid loss: 2.1053, valid accuracy: 0.2854\n",
      "Iter-82300 train loss: 2.1246 valid loss: 2.1053, valid accuracy: 0.2862\n",
      "Iter-82400 train loss: 2.0287 valid loss: 2.1053, valid accuracy: 0.2862\n",
      "Iter-82500 train loss: 2.0156 valid loss: 2.1052, valid accuracy: 0.2862\n",
      "Iter-82600 train loss: 2.1850 valid loss: 2.1051, valid accuracy: 0.2862\n",
      "Iter-82700 train loss: 2.2007 valid loss: 2.1049, valid accuracy: 0.2877\n",
      "Iter-82800 train loss: 1.9835 valid loss: 2.1048, valid accuracy: 0.2862\n",
      "Iter-82900 train loss: 2.0897 valid loss: 2.1048, valid accuracy: 0.2869\n",
      "Iter-83000 train loss: 2.0707 valid loss: 2.1047, valid accuracy: 0.2862\n",
      "Iter-83100 train loss: 2.0596 valid loss: 2.1047, valid accuracy: 0.2877\n",
      "Iter-83200 train loss: 2.2570 valid loss: 2.1045, valid accuracy: 0.2885\n",
      "Iter-83300 train loss: 1.9913 valid loss: 2.1044, valid accuracy: 0.2892\n",
      "Iter-83400 train loss: 2.2105 valid loss: 2.1043, valid accuracy: 0.2885\n",
      "Iter-83500 train loss: 2.0082 valid loss: 2.1042, valid accuracy: 0.2885\n",
      "Iter-83600 train loss: 2.0603 valid loss: 2.1042, valid accuracy: 0.2869\n",
      "Iter-83700 train loss: 2.0323 valid loss: 2.1041, valid accuracy: 0.2862\n",
      "Iter-83800 train loss: 1.8592 valid loss: 2.1040, valid accuracy: 0.2869\n",
      "Iter-83900 train loss: 1.9605 valid loss: 2.1038, valid accuracy: 0.2869\n",
      "Iter-84000 train loss: 1.9746 valid loss: 2.1038, valid accuracy: 0.2869\n",
      "Iter-84100 train loss: 2.1141 valid loss: 2.1038, valid accuracy: 0.2862\n",
      "Iter-84200 train loss: 2.2618 valid loss: 2.1036, valid accuracy: 0.2869\n",
      "Iter-84300 train loss: 1.9971 valid loss: 2.1036, valid accuracy: 0.2862\n",
      "Iter-84400 train loss: 2.3382 valid loss: 2.1035, valid accuracy: 0.2862\n",
      "Iter-84500 train loss: 1.9365 valid loss: 2.1035, valid accuracy: 0.2854\n",
      "Iter-84600 train loss: 2.1675 valid loss: 2.1034, valid accuracy: 0.2862\n",
      "Iter-84700 train loss: 1.9912 valid loss: 2.1033, valid accuracy: 0.2862\n",
      "Iter-84800 train loss: 1.9237 valid loss: 2.1032, valid accuracy: 0.2862\n",
      "Iter-84900 train loss: 2.4067 valid loss: 2.1032, valid accuracy: 0.2854\n",
      "Iter-85000 train loss: 2.1465 valid loss: 2.1032, valid accuracy: 0.2869\n",
      "Iter-85100 train loss: 2.1852 valid loss: 2.1031, valid accuracy: 0.2869\n",
      "Iter-85200 train loss: 2.1391 valid loss: 2.1030, valid accuracy: 0.2862\n",
      "Iter-85300 train loss: 2.0312 valid loss: 2.1029, valid accuracy: 0.2862\n",
      "Iter-85400 train loss: 2.1594 valid loss: 2.1028, valid accuracy: 0.2854\n",
      "Iter-85500 train loss: 1.8823 valid loss: 2.1027, valid accuracy: 0.2838\n",
      "Iter-85600 train loss: 1.8542 valid loss: 2.1027, valid accuracy: 0.2838\n",
      "Iter-85700 train loss: 2.2084 valid loss: 2.1026, valid accuracy: 0.2846\n",
      "Iter-85800 train loss: 1.8598 valid loss: 2.1025, valid accuracy: 0.2862\n",
      "Iter-85900 train loss: 2.2168 valid loss: 2.1026, valid accuracy: 0.2854\n",
      "Iter-86000 train loss: 2.1057 valid loss: 2.1025, valid accuracy: 0.2846\n",
      "Iter-86100 train loss: 2.1560 valid loss: 2.1025, valid accuracy: 0.2838\n",
      "Iter-86200 train loss: 1.9290 valid loss: 2.1025, valid accuracy: 0.2846\n",
      "Iter-86300 train loss: 2.0440 valid loss: 2.1025, valid accuracy: 0.2846\n",
      "Iter-86400 train loss: 2.0911 valid loss: 2.1025, valid accuracy: 0.2838\n",
      "Iter-86500 train loss: 2.0041 valid loss: 2.1025, valid accuracy: 0.2862\n",
      "Iter-86600 train loss: 2.0422 valid loss: 2.1024, valid accuracy: 0.2846\n",
      "Iter-86700 train loss: 2.0493 valid loss: 2.1024, valid accuracy: 0.2846\n",
      "Iter-86800 train loss: 1.8228 valid loss: 2.1023, valid accuracy: 0.2838\n",
      "Iter-86900 train loss: 1.8808 valid loss: 2.1023, valid accuracy: 0.2838\n",
      "Iter-87000 train loss: 2.3225 valid loss: 2.1022, valid accuracy: 0.2838\n",
      "Iter-87100 train loss: 2.0986 valid loss: 2.1021, valid accuracy: 0.2815\n",
      "Iter-87200 train loss: 1.9182 valid loss: 2.1021, valid accuracy: 0.2823\n",
      "Iter-87300 train loss: 2.0660 valid loss: 2.1020, valid accuracy: 0.2831\n",
      "Iter-87400 train loss: 1.8676 valid loss: 2.1018, valid accuracy: 0.2831\n",
      "Iter-87500 train loss: 2.0270 valid loss: 2.1017, valid accuracy: 0.2838\n",
      "Iter-87600 train loss: 2.0595 valid loss: 2.1017, valid accuracy: 0.2831\n",
      "Iter-87700 train loss: 1.8461 valid loss: 2.1016, valid accuracy: 0.2831\n",
      "Iter-87800 train loss: 2.2698 valid loss: 2.1015, valid accuracy: 0.2823\n",
      "Iter-87900 train loss: 1.9965 valid loss: 2.1015, valid accuracy: 0.2823\n",
      "Iter-88000 train loss: 1.8784 valid loss: 2.1013, valid accuracy: 0.2831\n",
      "Iter-88100 train loss: 2.2153 valid loss: 2.1012, valid accuracy: 0.2815\n",
      "Iter-88200 train loss: 1.9221 valid loss: 2.1011, valid accuracy: 0.2808\n",
      "Iter-88300 train loss: 1.9614 valid loss: 2.1010, valid accuracy: 0.2808\n",
      "Iter-88400 train loss: 2.0005 valid loss: 2.1009, valid accuracy: 0.2808\n",
      "Iter-88500 train loss: 2.0962 valid loss: 2.1008, valid accuracy: 0.2831\n",
      "Iter-88600 train loss: 2.0132 valid loss: 2.1007, valid accuracy: 0.2831\n",
      "Iter-88700 train loss: 2.0202 valid loss: 2.1006, valid accuracy: 0.2831\n",
      "Iter-88800 train loss: 1.9216 valid loss: 2.1006, valid accuracy: 0.2846\n",
      "Iter-88900 train loss: 2.2782 valid loss: 2.1005, valid accuracy: 0.2846\n",
      "Iter-89000 train loss: 1.9673 valid loss: 2.1006, valid accuracy: 0.2838\n",
      "Iter-89100 train loss: 2.0612 valid loss: 2.1005, valid accuracy: 0.2831\n",
      "Iter-89200 train loss: 1.9456 valid loss: 2.1004, valid accuracy: 0.2846\n",
      "Iter-89300 train loss: 1.9181 valid loss: 2.1003, valid accuracy: 0.2831\n",
      "Iter-89400 train loss: 1.8662 valid loss: 2.1002, valid accuracy: 0.2831\n",
      "Iter-89500 train loss: 2.1909 valid loss: 2.1002, valid accuracy: 0.2831\n",
      "Iter-89600 train loss: 2.3297 valid loss: 2.1002, valid accuracy: 0.2831\n",
      "Iter-89700 train loss: 1.8544 valid loss: 2.1001, valid accuracy: 0.2823\n",
      "Iter-89800 train loss: 2.2256 valid loss: 2.1000, valid accuracy: 0.2838\n",
      "Iter-89900 train loss: 2.1015 valid loss: 2.0999, valid accuracy: 0.2831\n",
      "Iter-90000 train loss: 1.9740 valid loss: 2.0997, valid accuracy: 0.2831\n",
      "Iter-90100 train loss: 1.9172 valid loss: 2.0997, valid accuracy: 0.2846\n",
      "Iter-90200 train loss: 2.0619 valid loss: 2.0996, valid accuracy: 0.2838\n",
      "Iter-90300 train loss: 2.0418 valid loss: 2.0995, valid accuracy: 0.2846\n",
      "Iter-90400 train loss: 1.9746 valid loss: 2.0995, valid accuracy: 0.2846\n",
      "Iter-90500 train loss: 2.2193 valid loss: 2.0995, valid accuracy: 0.2846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 2.3103 valid loss: 2.0995, valid accuracy: 0.2846\n",
      "Iter-90700 train loss: 1.8422 valid loss: 2.0994, valid accuracy: 0.2854\n",
      "Iter-90800 train loss: 2.1436 valid loss: 2.0994, valid accuracy: 0.2869\n",
      "Iter-90900 train loss: 1.9917 valid loss: 2.0994, valid accuracy: 0.2862\n",
      "Iter-91000 train loss: 2.1588 valid loss: 2.0992, valid accuracy: 0.2862\n",
      "Iter-91100 train loss: 2.1413 valid loss: 2.0992, valid accuracy: 0.2854\n",
      "Iter-91200 train loss: 2.0760 valid loss: 2.0990, valid accuracy: 0.2877\n",
      "Iter-91300 train loss: 2.2033 valid loss: 2.0990, valid accuracy: 0.2862\n",
      "Iter-91400 train loss: 1.9995 valid loss: 2.0988, valid accuracy: 0.2862\n",
      "Iter-91500 train loss: 1.9930 valid loss: 2.0988, valid accuracy: 0.2854\n",
      "Iter-91600 train loss: 2.0030 valid loss: 2.0987, valid accuracy: 0.2854\n",
      "Iter-91700 train loss: 1.9325 valid loss: 2.0986, valid accuracy: 0.2862\n",
      "Iter-91800 train loss: 1.9379 valid loss: 2.0986, valid accuracy: 0.2854\n",
      "Iter-91900 train loss: 1.9680 valid loss: 2.0986, valid accuracy: 0.2854\n",
      "Iter-92000 train loss: 1.9389 valid loss: 2.0985, valid accuracy: 0.2854\n",
      "Iter-92100 train loss: 1.9044 valid loss: 2.0983, valid accuracy: 0.2854\n",
      "Iter-92200 train loss: 1.9842 valid loss: 2.0984, valid accuracy: 0.2862\n",
      "Iter-92300 train loss: 1.9824 valid loss: 2.0982, valid accuracy: 0.2862\n",
      "Iter-92400 train loss: 1.9971 valid loss: 2.0982, valid accuracy: 0.2862\n",
      "Iter-92500 train loss: 1.7161 valid loss: 2.0982, valid accuracy: 0.2869\n",
      "Iter-92600 train loss: 1.9626 valid loss: 2.0980, valid accuracy: 0.2869\n",
      "Iter-92700 train loss: 2.1129 valid loss: 2.0981, valid accuracy: 0.2854\n",
      "Iter-92800 train loss: 2.1210 valid loss: 2.0980, valid accuracy: 0.2854\n",
      "Iter-92900 train loss: 2.0679 valid loss: 2.0978, valid accuracy: 0.2862\n",
      "Iter-93000 train loss: 2.0588 valid loss: 2.0978, valid accuracy: 0.2862\n",
      "Iter-93100 train loss: 1.8434 valid loss: 2.0979, valid accuracy: 0.2854\n",
      "Iter-93200 train loss: 2.0981 valid loss: 2.0979, valid accuracy: 0.2838\n",
      "Iter-93300 train loss: 1.9455 valid loss: 2.0979, valid accuracy: 0.2831\n",
      "Iter-93400 train loss: 2.1045 valid loss: 2.0977, valid accuracy: 0.2823\n",
      "Iter-93500 train loss: 1.8736 valid loss: 2.0977, valid accuracy: 0.2838\n",
      "Iter-93600 train loss: 1.9700 valid loss: 2.0977, valid accuracy: 0.2831\n",
      "Iter-93700 train loss: 2.0476 valid loss: 2.0976, valid accuracy: 0.2846\n",
      "Iter-93800 train loss: 1.9383 valid loss: 2.0975, valid accuracy: 0.2838\n",
      "Iter-93900 train loss: 2.1238 valid loss: 2.0974, valid accuracy: 0.2846\n",
      "Iter-94000 train loss: 2.0405 valid loss: 2.0973, valid accuracy: 0.2862\n",
      "Iter-94100 train loss: 2.2916 valid loss: 2.0972, valid accuracy: 0.2831\n",
      "Iter-94200 train loss: 1.9239 valid loss: 2.0971, valid accuracy: 0.2831\n",
      "Iter-94300 train loss: 2.1686 valid loss: 2.0970, valid accuracy: 0.2831\n",
      "Iter-94400 train loss: 2.2568 valid loss: 2.0969, valid accuracy: 0.2838\n",
      "Iter-94500 train loss: 1.8588 valid loss: 2.0967, valid accuracy: 0.2831\n",
      "Iter-94600 train loss: 1.8834 valid loss: 2.0967, valid accuracy: 0.2838\n",
      "Iter-94700 train loss: 2.1641 valid loss: 2.0967, valid accuracy: 0.2823\n",
      "Iter-94800 train loss: 1.9997 valid loss: 2.0966, valid accuracy: 0.2823\n",
      "Iter-94900 train loss: 1.9037 valid loss: 2.0965, valid accuracy: 0.2831\n",
      "Iter-95000 train loss: 2.3536 valid loss: 2.0965, valid accuracy: 0.2823\n",
      "Iter-95100 train loss: 2.1136 valid loss: 2.0964, valid accuracy: 0.2838\n",
      "Iter-95200 train loss: 1.9082 valid loss: 2.0963, valid accuracy: 0.2838\n",
      "Iter-95300 train loss: 1.8966 valid loss: 2.0962, valid accuracy: 0.2831\n",
      "Iter-95400 train loss: 2.0177 valid loss: 2.0961, valid accuracy: 0.2831\n",
      "Iter-95500 train loss: 2.1441 valid loss: 2.0960, valid accuracy: 0.2854\n",
      "Iter-95600 train loss: 2.1376 valid loss: 2.0958, valid accuracy: 0.2854\n",
      "Iter-95700 train loss: 2.3971 valid loss: 2.0957, valid accuracy: 0.2846\n",
      "Iter-95800 train loss: 1.9542 valid loss: 2.0955, valid accuracy: 0.2846\n",
      "Iter-95900 train loss: 2.1457 valid loss: 2.0954, valid accuracy: 0.2846\n",
      "Iter-96000 train loss: 2.1802 valid loss: 2.0952, valid accuracy: 0.2869\n",
      "Iter-96100 train loss: 2.0260 valid loss: 2.0951, valid accuracy: 0.2869\n",
      "Iter-96200 train loss: 2.2908 valid loss: 2.0951, valid accuracy: 0.2869\n",
      "Iter-96300 train loss: 2.1401 valid loss: 2.0950, valid accuracy: 0.2862\n",
      "Iter-96400 train loss: 1.9683 valid loss: 2.0950, valid accuracy: 0.2854\n",
      "Iter-96500 train loss: 1.9629 valid loss: 2.0949, valid accuracy: 0.2877\n",
      "Iter-96600 train loss: 2.0961 valid loss: 2.0948, valid accuracy: 0.2877\n",
      "Iter-96700 train loss: 1.8393 valid loss: 2.0948, valid accuracy: 0.2877\n",
      "Iter-96800 train loss: 2.0409 valid loss: 2.0948, valid accuracy: 0.2892\n",
      "Iter-96900 train loss: 1.8996 valid loss: 2.0946, valid accuracy: 0.2885\n",
      "Iter-97000 train loss: 2.1953 valid loss: 2.0946, valid accuracy: 0.2885\n",
      "Iter-97100 train loss: 1.9973 valid loss: 2.0945, valid accuracy: 0.2885\n",
      "Iter-97200 train loss: 2.0406 valid loss: 2.0944, valid accuracy: 0.2885\n",
      "Iter-97300 train loss: 2.0053 valid loss: 2.0944, valid accuracy: 0.2892\n",
      "Iter-97400 train loss: 1.9877 valid loss: 2.0943, valid accuracy: 0.2892\n",
      "Iter-97500 train loss: 2.0553 valid loss: 2.0943, valid accuracy: 0.2892\n",
      "Iter-97600 train loss: 1.9799 valid loss: 2.0942, valid accuracy: 0.2885\n",
      "Iter-97700 train loss: 2.0893 valid loss: 2.0940, valid accuracy: 0.2885\n",
      "Iter-97800 train loss: 1.9559 valid loss: 2.0940, valid accuracy: 0.2877\n",
      "Iter-97900 train loss: 2.0658 valid loss: 2.0939, valid accuracy: 0.2885\n",
      "Iter-98000 train loss: 1.9072 valid loss: 2.0938, valid accuracy: 0.2877\n",
      "Iter-98100 train loss: 2.0348 valid loss: 2.0937, valid accuracy: 0.2869\n",
      "Iter-98200 train loss: 1.9589 valid loss: 2.0936, valid accuracy: 0.2885\n",
      "Iter-98300 train loss: 2.0464 valid loss: 2.0936, valid accuracy: 0.2885\n",
      "Iter-98400 train loss: 1.7937 valid loss: 2.0935, valid accuracy: 0.2854\n",
      "Iter-98500 train loss: 2.2477 valid loss: 2.0935, valid accuracy: 0.2885\n",
      "Iter-98600 train loss: 2.1693 valid loss: 2.0934, valid accuracy: 0.2869\n",
      "Iter-98700 train loss: 2.0230 valid loss: 2.0933, valid accuracy: 0.2869\n",
      "Iter-98800 train loss: 2.1533 valid loss: 2.0935, valid accuracy: 0.2877\n",
      "Iter-98900 train loss: 1.9799 valid loss: 2.0934, valid accuracy: 0.2869\n",
      "Iter-99000 train loss: 2.0089 valid loss: 2.0932, valid accuracy: 0.2885\n",
      "Iter-99100 train loss: 2.1006 valid loss: 2.0931, valid accuracy: 0.2862\n",
      "Iter-99200 train loss: 1.9209 valid loss: 2.0930, valid accuracy: 0.2885\n",
      "Iter-99300 train loss: 1.9396 valid loss: 2.0930, valid accuracy: 0.2877\n",
      "Iter-99400 train loss: 2.0277 valid loss: 2.0929, valid accuracy: 0.2885\n",
      "Iter-99500 train loss: 2.0444 valid loss: 2.0929, valid accuracy: 0.2869\n",
      "Iter-99600 train loss: 2.3360 valid loss: 2.0930, valid accuracy: 0.2892\n",
      "Iter-99700 train loss: 1.8896 valid loss: 2.0930, valid accuracy: 0.2892\n",
      "Iter-99800 train loss: 2.0566 valid loss: 2.0930, valid accuracy: 0.2885\n",
      "Iter-99900 train loss: 2.0576 valid loss: 2.0929, valid accuracy: 0.2908\n",
      "Iter-100000 train loss: 2.0593 valid loss: 2.0928, valid accuracy: 0.2908\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0a151f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUqIQEZGUlChERCQlJQoREUnp/wNyiwj0J4iDHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb0a151f3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "# y_pred, y_logit = nn.test(X_test)\n",
    "# loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "# acc = np.mean(y_pred == y_test)\n",
    "# print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#     acc.mean(), acc.std(), loss))\n",
    "y_pred, _ = nn.test(X_test)\n",
    "# mplot.imsave(y_pred)\n",
    "# pd.DataFrame.to_csv(y_pred)\n",
    "# y_pred.shape\n",
    "# import numpy\n",
    "# a = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "# numpy.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "np.savetxt(X=y_pred, delimiter=\",\", fname='y_predddddddddddddddddd2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBCI-HW1\u001b[0m/                                 \u001b[01;34mDGRUs\u001b[0m/\r\n",
      "\u001b[01;34mBCI-HW2\u001b[0m/                                 \u001b[01;34mDGRUs_old\u001b[0m/\r\n",
      "\u001b[01;34mbin\u001b[0m/                                     environment.yml\r\n",
      "\u001b[01;34mbio-bp-dl\u001b[0m/                               gradient_descent.py\r\n",
      "confusion_mat_cov.ipynb                  \u001b[01;34mimpl\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/                                    \u001b[01;34mimpl_imagernn_karpathy\u001b[0m/\r\n",
      "DCNN.ipynb                               LICENSE\r\n",
      "Deep-FFNN-Tanh-FBA.ipynb                 minimal_net.ipynb\r\n",
      "Deep-FFNN-Tanh-FBA-ITD.ipynb             \u001b[01;34mmisc\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD-Kaggle.ipynb      \u001b[01;34mmisc2\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-Kaggle.ipynb          NOTES\r\n",
      "Deep-FFNN-Tanh-Vanilla.ipynb             numba-cuda-gpu-example.ipynb\r\n",
      "Deep-FFNN-Tanh-Vanilla-Kaggle.ipynb      README.md\r\n",
      "DFFNN-FBA-STDP-Tanh-diff.ipynb           \u001b[01;34mrnn-testing-platform\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-diff-no_FBA.ipynb    \u001b[01;34mtf-based\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff2.ipynb  y_predddddddddddddddddd2.csv\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff.ipynb   y_predddddddddddddddddd3.csv\r\n",
      "\u001b[01;34mDFFNNs\u001b[0m/                                  y_predddddddddddddddddd.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
