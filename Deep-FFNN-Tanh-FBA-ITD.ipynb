{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.2902 valid loss: 2.2901, valid accuracy: 0.1360\n",
      "Iter-200 train loss: 2.2858 valid loss: 2.2894, valid accuracy: 0.1370\n",
      "Iter-300 train loss: 2.2759 valid loss: 2.2886, valid accuracy: 0.1390\n",
      "Iter-400 train loss: 2.3064 valid loss: 2.2879, valid accuracy: 0.1402\n",
      "Iter-500 train loss: 2.2864 valid loss: 2.2871, valid accuracy: 0.1422\n",
      "Iter-600 train loss: 2.2663 valid loss: 2.2864, valid accuracy: 0.1430\n",
      "Iter-700 train loss: 2.2861 valid loss: 2.2856, valid accuracy: 0.1452\n",
      "Iter-800 train loss: 2.2894 valid loss: 2.2849, valid accuracy: 0.1464\n",
      "Iter-900 train loss: 2.2895 valid loss: 2.2842, valid accuracy: 0.1468\n",
      "Iter-1000 train loss: 2.2675 valid loss: 2.2834, valid accuracy: 0.1488\n",
      "Iter-1100 train loss: 2.3248 valid loss: 2.2826, valid accuracy: 0.1506\n",
      "Iter-1200 train loss: 2.2905 valid loss: 2.2819, valid accuracy: 0.1512\n",
      "Iter-1300 train loss: 2.2785 valid loss: 2.2811, valid accuracy: 0.1526\n",
      "Iter-1400 train loss: 2.2842 valid loss: 2.2804, valid accuracy: 0.1534\n",
      "Iter-1500 train loss: 2.2794 valid loss: 2.2797, valid accuracy: 0.1544\n",
      "Iter-1600 train loss: 2.2830 valid loss: 2.2789, valid accuracy: 0.1550\n",
      "Iter-1700 train loss: 2.2785 valid loss: 2.2782, valid accuracy: 0.1558\n",
      "Iter-1800 train loss: 2.2959 valid loss: 2.2775, valid accuracy: 0.1572\n",
      "Iter-1900 train loss: 2.2687 valid loss: 2.2767, valid accuracy: 0.1584\n",
      "Iter-2000 train loss: 2.2753 valid loss: 2.2760, valid accuracy: 0.1616\n",
      "Iter-2100 train loss: 2.2677 valid loss: 2.2752, valid accuracy: 0.1620\n",
      "Iter-2200 train loss: 2.2677 valid loss: 2.2745, valid accuracy: 0.1634\n",
      "Iter-2300 train loss: 2.2752 valid loss: 2.2738, valid accuracy: 0.1654\n",
      "Iter-2400 train loss: 2.2695 valid loss: 2.2730, valid accuracy: 0.1670\n",
      "Iter-2500 train loss: 2.2970 valid loss: 2.2723, valid accuracy: 0.1700\n",
      "Iter-2600 train loss: 2.2535 valid loss: 2.2716, valid accuracy: 0.1714\n",
      "Iter-2700 train loss: 2.2828 valid loss: 2.2708, valid accuracy: 0.1718\n",
      "Iter-2800 train loss: 2.2485 valid loss: 2.2701, valid accuracy: 0.1728\n",
      "Iter-2900 train loss: 2.2693 valid loss: 2.2694, valid accuracy: 0.1746\n",
      "Iter-3000 train loss: 2.2585 valid loss: 2.2687, valid accuracy: 0.1764\n",
      "Iter-3100 train loss: 2.2631 valid loss: 2.2679, valid accuracy: 0.1778\n",
      "Iter-3200 train loss: 2.2768 valid loss: 2.2672, valid accuracy: 0.1796\n",
      "Iter-3300 train loss: 2.2533 valid loss: 2.2665, valid accuracy: 0.1806\n",
      "Iter-3400 train loss: 2.2712 valid loss: 2.2657, valid accuracy: 0.1822\n",
      "Iter-3500 train loss: 2.2675 valid loss: 2.2650, valid accuracy: 0.1836\n",
      "Iter-3600 train loss: 2.2932 valid loss: 2.2643, valid accuracy: 0.1846\n",
      "Iter-3700 train loss: 2.2804 valid loss: 2.2636, valid accuracy: 0.1852\n",
      "Iter-3800 train loss: 2.2732 valid loss: 2.2629, valid accuracy: 0.1880\n",
      "Iter-3900 train loss: 2.2876 valid loss: 2.2621, valid accuracy: 0.1890\n",
      "Iter-4000 train loss: 2.2458 valid loss: 2.2614, valid accuracy: 0.1924\n",
      "Iter-4100 train loss: 2.2606 valid loss: 2.2607, valid accuracy: 0.1944\n",
      "Iter-4200 train loss: 2.2634 valid loss: 2.2600, valid accuracy: 0.1962\n",
      "Iter-4300 train loss: 2.2754 valid loss: 2.2593, valid accuracy: 0.1968\n",
      "Iter-4400 train loss: 2.2821 valid loss: 2.2585, valid accuracy: 0.1998\n",
      "Iter-4500 train loss: 2.2696 valid loss: 2.2578, valid accuracy: 0.2010\n",
      "Iter-4600 train loss: 2.2745 valid loss: 2.2571, valid accuracy: 0.2028\n",
      "Iter-4700 train loss: 2.2477 valid loss: 2.2564, valid accuracy: 0.2038\n",
      "Iter-4800 train loss: 2.2617 valid loss: 2.2557, valid accuracy: 0.2074\n",
      "Iter-4900 train loss: 2.2563 valid loss: 2.2550, valid accuracy: 0.2094\n",
      "Iter-5000 train loss: 2.2531 valid loss: 2.2542, valid accuracy: 0.2118\n",
      "Iter-5100 train loss: 2.2530 valid loss: 2.2536, valid accuracy: 0.2146\n",
      "Iter-5200 train loss: 2.2373 valid loss: 2.2528, valid accuracy: 0.2158\n",
      "Iter-5300 train loss: 2.2587 valid loss: 2.2521, valid accuracy: 0.2172\n",
      "Iter-5400 train loss: 2.2373 valid loss: 2.2514, valid accuracy: 0.2190\n",
      "Iter-5500 train loss: 2.2622 valid loss: 2.2507, valid accuracy: 0.2202\n",
      "Iter-5600 train loss: 2.2499 valid loss: 2.2500, valid accuracy: 0.2210\n",
      "Iter-5700 train loss: 2.2641 valid loss: 2.2493, valid accuracy: 0.2222\n",
      "Iter-5800 train loss: 2.2550 valid loss: 2.2486, valid accuracy: 0.2230\n",
      "Iter-5900 train loss: 2.2311 valid loss: 2.2479, valid accuracy: 0.2260\n",
      "Iter-6000 train loss: 2.2723 valid loss: 2.2472, valid accuracy: 0.2276\n",
      "Iter-6100 train loss: 2.2543 valid loss: 2.2465, valid accuracy: 0.2294\n",
      "Iter-6200 train loss: 2.2285 valid loss: 2.2457, valid accuracy: 0.2310\n",
      "Iter-6300 train loss: 2.2201 valid loss: 2.2451, valid accuracy: 0.2324\n",
      "Iter-6400 train loss: 2.2415 valid loss: 2.2443, valid accuracy: 0.2338\n",
      "Iter-6500 train loss: 2.2458 valid loss: 2.2437, valid accuracy: 0.2352\n",
      "Iter-6600 train loss: 2.2564 valid loss: 2.2430, valid accuracy: 0.2378\n",
      "Iter-6700 train loss: 2.2704 valid loss: 2.2423, valid accuracy: 0.2390\n",
      "Iter-6800 train loss: 2.2527 valid loss: 2.2416, valid accuracy: 0.2424\n",
      "Iter-6900 train loss: 2.2530 valid loss: 2.2408, valid accuracy: 0.2434\n",
      "Iter-7000 train loss: 2.2318 valid loss: 2.2402, valid accuracy: 0.2458\n",
      "Iter-7100 train loss: 2.2550 valid loss: 2.2395, valid accuracy: 0.2480\n",
      "Iter-7200 train loss: 2.2398 valid loss: 2.2388, valid accuracy: 0.2498\n",
      "Iter-7300 train loss: 2.2238 valid loss: 2.2381, valid accuracy: 0.2516\n",
      "Iter-7400 train loss: 2.2353 valid loss: 2.2374, valid accuracy: 0.2524\n",
      "Iter-7500 train loss: 2.2350 valid loss: 2.2367, valid accuracy: 0.2558\n",
      "Iter-7600 train loss: 2.2295 valid loss: 2.2360, valid accuracy: 0.2572\n",
      "Iter-7700 train loss: 2.2124 valid loss: 2.2353, valid accuracy: 0.2590\n",
      "Iter-7800 train loss: 2.2008 valid loss: 2.2346, valid accuracy: 0.2592\n",
      "Iter-7900 train loss: 2.2219 valid loss: 2.2339, valid accuracy: 0.2606\n",
      "Iter-8000 train loss: 2.2153 valid loss: 2.2332, valid accuracy: 0.2620\n",
      "Iter-8100 train loss: 2.2442 valid loss: 2.2325, valid accuracy: 0.2628\n",
      "Iter-8200 train loss: 2.2132 valid loss: 2.2318, valid accuracy: 0.2648\n",
      "Iter-8300 train loss: 2.2383 valid loss: 2.2312, valid accuracy: 0.2662\n",
      "Iter-8400 train loss: 2.2433 valid loss: 2.2305, valid accuracy: 0.2668\n",
      "Iter-8500 train loss: 2.2063 valid loss: 2.2298, valid accuracy: 0.2674\n",
      "Iter-8600 train loss: 2.2490 valid loss: 2.2291, valid accuracy: 0.2690\n",
      "Iter-8700 train loss: 2.2327 valid loss: 2.2285, valid accuracy: 0.2722\n",
      "Iter-8800 train loss: 2.2367 valid loss: 2.2278, valid accuracy: 0.2728\n",
      "Iter-8900 train loss: 2.2517 valid loss: 2.2271, valid accuracy: 0.2740\n",
      "Iter-9000 train loss: 2.1925 valid loss: 2.2264, valid accuracy: 0.2750\n",
      "Iter-9100 train loss: 2.2341 valid loss: 2.2257, valid accuracy: 0.2766\n",
      "Iter-9200 train loss: 2.2023 valid loss: 2.2251, valid accuracy: 0.2786\n",
      "Iter-9300 train loss: 2.2311 valid loss: 2.2244, valid accuracy: 0.2800\n",
      "Iter-9400 train loss: 2.2179 valid loss: 2.2237, valid accuracy: 0.2812\n",
      "Iter-9500 train loss: 2.2467 valid loss: 2.2230, valid accuracy: 0.2828\n",
      "Iter-9600 train loss: 2.2498 valid loss: 2.2224, valid accuracy: 0.2838\n",
      "Iter-9700 train loss: 2.2456 valid loss: 2.2217, valid accuracy: 0.2866\n",
      "Iter-9800 train loss: 2.2559 valid loss: 2.2211, valid accuracy: 0.2876\n",
      "Iter-9900 train loss: 2.1948 valid loss: 2.2204, valid accuracy: 0.2888\n",
      "Iter-10000 train loss: 2.2075 valid loss: 2.2197, valid accuracy: 0.2896\n",
      "Iter-10100 train loss: 2.2267 valid loss: 2.2191, valid accuracy: 0.2910\n",
      "Iter-10200 train loss: 2.2558 valid loss: 2.2184, valid accuracy: 0.2918\n",
      "Iter-10300 train loss: 2.2093 valid loss: 2.2177, valid accuracy: 0.2932\n",
      "Iter-10400 train loss: 2.2134 valid loss: 2.2170, valid accuracy: 0.2936\n",
      "Iter-10500 train loss: 2.2346 valid loss: 2.2163, valid accuracy: 0.2962\n",
      "Iter-10600 train loss: 2.2349 valid loss: 2.2156, valid accuracy: 0.2978\n",
      "Iter-10700 train loss: 2.2087 valid loss: 2.2150, valid accuracy: 0.2996\n",
      "Iter-10800 train loss: 2.1792 valid loss: 2.2143, valid accuracy: 0.3016\n",
      "Iter-10900 train loss: 2.2195 valid loss: 2.2136, valid accuracy: 0.3024\n",
      "Iter-11000 train loss: 2.2188 valid loss: 2.2129, valid accuracy: 0.3042\n",
      "Iter-11100 train loss: 2.2122 valid loss: 2.2122, valid accuracy: 0.3058\n",
      "Iter-11200 train loss: 2.2309 valid loss: 2.2116, valid accuracy: 0.3064\n",
      "Iter-11300 train loss: 2.1967 valid loss: 2.2109, valid accuracy: 0.3078\n",
      "Iter-11400 train loss: 2.1978 valid loss: 2.2103, valid accuracy: 0.3086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.2216 valid loss: 2.2096, valid accuracy: 0.3096\n",
      "Iter-11600 train loss: 2.2263 valid loss: 2.2089, valid accuracy: 0.3108\n",
      "Iter-11700 train loss: 2.2379 valid loss: 2.2083, valid accuracy: 0.3116\n",
      "Iter-11800 train loss: 2.1800 valid loss: 2.2076, valid accuracy: 0.3136\n",
      "Iter-11900 train loss: 2.2168 valid loss: 2.2070, valid accuracy: 0.3150\n",
      "Iter-12000 train loss: 2.2151 valid loss: 2.2063, valid accuracy: 0.3164\n",
      "Iter-12100 train loss: 2.1815 valid loss: 2.2056, valid accuracy: 0.3178\n",
      "Iter-12200 train loss: 2.2177 valid loss: 2.2050, valid accuracy: 0.3208\n",
      "Iter-12300 train loss: 2.1925 valid loss: 2.2043, valid accuracy: 0.3216\n",
      "Iter-12400 train loss: 2.2278 valid loss: 2.2036, valid accuracy: 0.3226\n",
      "Iter-12500 train loss: 2.2039 valid loss: 2.2030, valid accuracy: 0.3242\n",
      "Iter-12600 train loss: 2.2221 valid loss: 2.2023, valid accuracy: 0.3250\n",
      "Iter-12700 train loss: 2.2446 valid loss: 2.2017, valid accuracy: 0.3250\n",
      "Iter-12800 train loss: 2.1821 valid loss: 2.2010, valid accuracy: 0.3268\n",
      "Iter-12900 train loss: 2.1975 valid loss: 2.2004, valid accuracy: 0.3274\n",
      "Iter-13000 train loss: 2.1709 valid loss: 2.1997, valid accuracy: 0.3294\n",
      "Iter-13100 train loss: 2.1529 valid loss: 2.1990, valid accuracy: 0.3310\n",
      "Iter-13200 train loss: 2.1925 valid loss: 2.1984, valid accuracy: 0.3316\n",
      "Iter-13300 train loss: 2.1818 valid loss: 2.1978, valid accuracy: 0.3322\n",
      "Iter-13400 train loss: 2.1862 valid loss: 2.1971, valid accuracy: 0.3326\n",
      "Iter-13500 train loss: 2.2011 valid loss: 2.1964, valid accuracy: 0.3340\n",
      "Iter-13600 train loss: 2.1959 valid loss: 2.1958, valid accuracy: 0.3350\n",
      "Iter-13700 train loss: 2.2437 valid loss: 2.1951, valid accuracy: 0.3354\n",
      "Iter-13800 train loss: 2.1854 valid loss: 2.1945, valid accuracy: 0.3370\n",
      "Iter-13900 train loss: 2.2320 valid loss: 2.1938, valid accuracy: 0.3384\n",
      "Iter-14000 train loss: 2.1709 valid loss: 2.1932, valid accuracy: 0.3390\n",
      "Iter-14100 train loss: 2.1490 valid loss: 2.1925, valid accuracy: 0.3404\n",
      "Iter-14200 train loss: 2.1981 valid loss: 2.1919, valid accuracy: 0.3414\n",
      "Iter-14300 train loss: 2.2226 valid loss: 2.1912, valid accuracy: 0.3418\n",
      "Iter-14400 train loss: 2.1735 valid loss: 2.1906, valid accuracy: 0.3418\n",
      "Iter-14500 train loss: 2.2042 valid loss: 2.1899, valid accuracy: 0.3418\n",
      "Iter-14600 train loss: 2.1916 valid loss: 2.1893, valid accuracy: 0.3426\n",
      "Iter-14700 train loss: 2.1912 valid loss: 2.1887, valid accuracy: 0.3436\n",
      "Iter-14800 train loss: 2.2022 valid loss: 2.1880, valid accuracy: 0.3440\n",
      "Iter-14900 train loss: 2.1694 valid loss: 2.1874, valid accuracy: 0.3460\n",
      "Iter-15000 train loss: 2.1871 valid loss: 2.1867, valid accuracy: 0.3472\n",
      "Iter-15100 train loss: 2.2304 valid loss: 2.1861, valid accuracy: 0.3488\n",
      "Iter-15200 train loss: 2.1640 valid loss: 2.1855, valid accuracy: 0.3488\n",
      "Iter-15300 train loss: 2.2204 valid loss: 2.1849, valid accuracy: 0.3502\n",
      "Iter-15400 train loss: 2.2016 valid loss: 2.1842, valid accuracy: 0.3506\n",
      "Iter-15500 train loss: 2.1617 valid loss: 2.1835, valid accuracy: 0.3526\n",
      "Iter-15600 train loss: 2.2025 valid loss: 2.1829, valid accuracy: 0.3536\n",
      "Iter-15700 train loss: 2.2115 valid loss: 2.1823, valid accuracy: 0.3544\n",
      "Iter-15800 train loss: 2.1703 valid loss: 2.1816, valid accuracy: 0.3552\n",
      "Iter-15900 train loss: 2.1999 valid loss: 2.1810, valid accuracy: 0.3558\n",
      "Iter-16000 train loss: 2.1941 valid loss: 2.1804, valid accuracy: 0.3560\n",
      "Iter-16100 train loss: 2.2052 valid loss: 2.1797, valid accuracy: 0.3564\n",
      "Iter-16200 train loss: 2.1638 valid loss: 2.1791, valid accuracy: 0.3568\n",
      "Iter-16300 train loss: 2.1826 valid loss: 2.1785, valid accuracy: 0.3574\n",
      "Iter-16400 train loss: 2.1884 valid loss: 2.1778, valid accuracy: 0.3578\n",
      "Iter-16500 train loss: 2.1618 valid loss: 2.1772, valid accuracy: 0.3586\n",
      "Iter-16600 train loss: 2.1758 valid loss: 2.1766, valid accuracy: 0.3592\n",
      "Iter-16700 train loss: 2.2075 valid loss: 2.1759, valid accuracy: 0.3600\n",
      "Iter-16800 train loss: 2.1986 valid loss: 2.1753, valid accuracy: 0.3614\n",
      "Iter-16900 train loss: 2.2385 valid loss: 2.1747, valid accuracy: 0.3628\n",
      "Iter-17000 train loss: 2.2273 valid loss: 2.1740, valid accuracy: 0.3620\n",
      "Iter-17100 train loss: 2.1753 valid loss: 2.1734, valid accuracy: 0.3622\n",
      "Iter-17200 train loss: 2.1695 valid loss: 2.1728, valid accuracy: 0.3632\n",
      "Iter-17300 train loss: 2.1693 valid loss: 2.1722, valid accuracy: 0.3642\n",
      "Iter-17400 train loss: 2.2150 valid loss: 2.1715, valid accuracy: 0.3650\n",
      "Iter-17500 train loss: 2.1815 valid loss: 2.1709, valid accuracy: 0.3648\n",
      "Iter-17600 train loss: 2.1673 valid loss: 2.1703, valid accuracy: 0.3654\n",
      "Iter-17700 train loss: 2.1765 valid loss: 2.1696, valid accuracy: 0.3662\n",
      "Iter-17800 train loss: 2.1519 valid loss: 2.1690, valid accuracy: 0.3666\n",
      "Iter-17900 train loss: 2.1271 valid loss: 2.1684, valid accuracy: 0.3664\n",
      "Iter-18000 train loss: 2.1478 valid loss: 2.1678, valid accuracy: 0.3664\n",
      "Iter-18100 train loss: 2.2146 valid loss: 2.1672, valid accuracy: 0.3666\n",
      "Iter-18200 train loss: 2.1777 valid loss: 2.1665, valid accuracy: 0.3664\n",
      "Iter-18300 train loss: 2.2177 valid loss: 2.1659, valid accuracy: 0.3660\n",
      "Iter-18400 train loss: 2.1802 valid loss: 2.1653, valid accuracy: 0.3666\n",
      "Iter-18500 train loss: 2.1664 valid loss: 2.1646, valid accuracy: 0.3666\n",
      "Iter-18600 train loss: 2.1642 valid loss: 2.1640, valid accuracy: 0.3672\n",
      "Iter-18700 train loss: 2.1708 valid loss: 2.1634, valid accuracy: 0.3684\n",
      "Iter-18800 train loss: 2.2033 valid loss: 2.1628, valid accuracy: 0.3698\n",
      "Iter-18900 train loss: 2.1735 valid loss: 2.1622, valid accuracy: 0.3704\n",
      "Iter-19000 train loss: 2.1761 valid loss: 2.1615, valid accuracy: 0.3712\n",
      "Iter-19100 train loss: 2.1591 valid loss: 2.1609, valid accuracy: 0.3720\n",
      "Iter-19200 train loss: 2.1767 valid loss: 2.1603, valid accuracy: 0.3726\n",
      "Iter-19300 train loss: 2.1482 valid loss: 2.1597, valid accuracy: 0.3742\n",
      "Iter-19400 train loss: 2.1705 valid loss: 2.1591, valid accuracy: 0.3750\n",
      "Iter-19500 train loss: 2.1873 valid loss: 2.1584, valid accuracy: 0.3756\n",
      "Iter-19600 train loss: 2.1618 valid loss: 2.1578, valid accuracy: 0.3762\n",
      "Iter-19700 train loss: 2.1916 valid loss: 2.1572, valid accuracy: 0.3766\n",
      "Iter-19800 train loss: 2.1553 valid loss: 2.1566, valid accuracy: 0.3770\n",
      "Iter-19900 train loss: 2.1376 valid loss: 2.1559, valid accuracy: 0.3774\n",
      "Iter-20000 train loss: 2.1859 valid loss: 2.1553, valid accuracy: 0.3778\n",
      "Iter-20100 train loss: 2.1522 valid loss: 2.1547, valid accuracy: 0.3788\n",
      "Iter-20200 train loss: 2.1028 valid loss: 2.1541, valid accuracy: 0.3794\n",
      "Iter-20300 train loss: 2.1539 valid loss: 2.1535, valid accuracy: 0.3798\n",
      "Iter-20400 train loss: 2.2007 valid loss: 2.1528, valid accuracy: 0.3808\n",
      "Iter-20500 train loss: 2.1476 valid loss: 2.1522, valid accuracy: 0.3812\n",
      "Iter-20600 train loss: 2.1320 valid loss: 2.1516, valid accuracy: 0.3820\n",
      "Iter-20700 train loss: 2.1489 valid loss: 2.1510, valid accuracy: 0.3826\n",
      "Iter-20800 train loss: 2.1927 valid loss: 2.1504, valid accuracy: 0.3828\n",
      "Iter-20900 train loss: 2.1514 valid loss: 2.1498, valid accuracy: 0.3836\n",
      "Iter-21000 train loss: 2.1504 valid loss: 2.1491, valid accuracy: 0.3854\n",
      "Iter-21100 train loss: 2.1345 valid loss: 2.1485, valid accuracy: 0.3850\n",
      "Iter-21200 train loss: 2.1529 valid loss: 2.1479, valid accuracy: 0.3868\n",
      "Iter-21300 train loss: 2.1272 valid loss: 2.1473, valid accuracy: 0.3872\n",
      "Iter-21400 train loss: 2.1859 valid loss: 2.1467, valid accuracy: 0.3886\n",
      "Iter-21500 train loss: 2.1563 valid loss: 2.1461, valid accuracy: 0.3888\n",
      "Iter-21600 train loss: 2.1055 valid loss: 2.1455, valid accuracy: 0.3890\n",
      "Iter-21700 train loss: 2.1502 valid loss: 2.1449, valid accuracy: 0.3896\n",
      "Iter-21800 train loss: 2.1879 valid loss: 2.1443, valid accuracy: 0.3896\n",
      "Iter-21900 train loss: 2.1697 valid loss: 2.1437, valid accuracy: 0.3908\n",
      "Iter-22000 train loss: 2.1396 valid loss: 2.1431, valid accuracy: 0.3922\n",
      "Iter-22100 train loss: 2.1253 valid loss: 2.1425, valid accuracy: 0.3932\n",
      "Iter-22200 train loss: 2.1013 valid loss: 2.1418, valid accuracy: 0.3932\n",
      "Iter-22300 train loss: 2.1219 valid loss: 2.1412, valid accuracy: 0.3944\n",
      "Iter-22400 train loss: 2.1771 valid loss: 2.1406, valid accuracy: 0.3950\n",
      "Iter-22500 train loss: 2.1346 valid loss: 2.1400, valid accuracy: 0.3950\n",
      "Iter-22600 train loss: 2.1009 valid loss: 2.1394, valid accuracy: 0.3954\n",
      "Iter-22700 train loss: 2.1650 valid loss: 2.1388, valid accuracy: 0.3970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.0936 valid loss: 2.1382, valid accuracy: 0.3970\n",
      "Iter-22900 train loss: 2.1216 valid loss: 2.1376, valid accuracy: 0.3968\n",
      "Iter-23000 train loss: 2.0918 valid loss: 2.1371, valid accuracy: 0.3972\n",
      "Iter-23100 train loss: 2.1565 valid loss: 2.1365, valid accuracy: 0.3976\n",
      "Iter-23200 train loss: 2.1508 valid loss: 2.1358, valid accuracy: 0.3974\n",
      "Iter-23300 train loss: 2.1325 valid loss: 2.1352, valid accuracy: 0.3978\n",
      "Iter-23400 train loss: 2.1525 valid loss: 2.1346, valid accuracy: 0.3974\n",
      "Iter-23500 train loss: 2.1428 valid loss: 2.1340, valid accuracy: 0.3972\n",
      "Iter-23600 train loss: 2.1173 valid loss: 2.1334, valid accuracy: 0.3978\n",
      "Iter-23700 train loss: 2.1691 valid loss: 2.1328, valid accuracy: 0.3976\n",
      "Iter-23800 train loss: 2.1253 valid loss: 2.1322, valid accuracy: 0.3984\n",
      "Iter-23900 train loss: 2.1064 valid loss: 2.1316, valid accuracy: 0.3984\n",
      "Iter-24000 train loss: 2.1544 valid loss: 2.1310, valid accuracy: 0.3986\n",
      "Iter-24100 train loss: 2.0923 valid loss: 2.1304, valid accuracy: 0.3996\n",
      "Iter-24200 train loss: 2.1657 valid loss: 2.1298, valid accuracy: 0.3996\n",
      "Iter-24300 train loss: 2.0769 valid loss: 2.1292, valid accuracy: 0.4012\n",
      "Iter-24400 train loss: 2.1254 valid loss: 2.1286, valid accuracy: 0.4020\n",
      "Iter-24500 train loss: 2.0986 valid loss: 2.1280, valid accuracy: 0.4026\n",
      "Iter-24600 train loss: 2.0753 valid loss: 2.1274, valid accuracy: 0.4036\n",
      "Iter-24700 train loss: 2.1468 valid loss: 2.1268, valid accuracy: 0.4046\n",
      "Iter-24800 train loss: 2.1111 valid loss: 2.1262, valid accuracy: 0.4052\n",
      "Iter-24900 train loss: 2.1512 valid loss: 2.1256, valid accuracy: 0.4052\n",
      "Iter-25000 train loss: 2.1605 valid loss: 2.1250, valid accuracy: 0.4056\n",
      "Iter-25100 train loss: 2.1222 valid loss: 2.1244, valid accuracy: 0.4054\n",
      "Iter-25200 train loss: 2.1447 valid loss: 2.1238, valid accuracy: 0.4054\n",
      "Iter-25300 train loss: 2.1586 valid loss: 2.1233, valid accuracy: 0.4050\n",
      "Iter-25400 train loss: 2.1463 valid loss: 2.1227, valid accuracy: 0.4056\n",
      "Iter-25500 train loss: 2.1173 valid loss: 2.1221, valid accuracy: 0.4054\n",
      "Iter-25600 train loss: 2.1269 valid loss: 2.1215, valid accuracy: 0.4058\n",
      "Iter-25700 train loss: 2.1454 valid loss: 2.1209, valid accuracy: 0.4060\n",
      "Iter-25800 train loss: 2.1176 valid loss: 2.1203, valid accuracy: 0.4060\n",
      "Iter-25900 train loss: 2.1411 valid loss: 2.1197, valid accuracy: 0.4062\n",
      "Iter-26000 train loss: 2.1036 valid loss: 2.1192, valid accuracy: 0.4064\n",
      "Iter-26100 train loss: 2.0933 valid loss: 2.1186, valid accuracy: 0.4068\n",
      "Iter-26200 train loss: 2.1070 valid loss: 2.1180, valid accuracy: 0.4076\n",
      "Iter-26300 train loss: 2.1179 valid loss: 2.1174, valid accuracy: 0.4086\n",
      "Iter-26400 train loss: 2.0888 valid loss: 2.1168, valid accuracy: 0.4084\n",
      "Iter-26500 train loss: 2.0926 valid loss: 2.1162, valid accuracy: 0.4092\n",
      "Iter-26600 train loss: 2.1196 valid loss: 2.1157, valid accuracy: 0.4098\n",
      "Iter-26700 train loss: 2.1081 valid loss: 2.1151, valid accuracy: 0.4100\n",
      "Iter-26800 train loss: 2.1665 valid loss: 2.1145, valid accuracy: 0.4102\n",
      "Iter-26900 train loss: 2.1754 valid loss: 2.1139, valid accuracy: 0.4102\n",
      "Iter-27000 train loss: 2.1214 valid loss: 2.1133, valid accuracy: 0.4098\n",
      "Iter-27100 train loss: 2.1135 valid loss: 2.1128, valid accuracy: 0.4108\n",
      "Iter-27200 train loss: 2.0671 valid loss: 2.1122, valid accuracy: 0.4110\n",
      "Iter-27300 train loss: 2.1318 valid loss: 2.1116, valid accuracy: 0.4108\n",
      "Iter-27400 train loss: 2.0702 valid loss: 2.1110, valid accuracy: 0.4112\n",
      "Iter-27500 train loss: 2.1017 valid loss: 2.1105, valid accuracy: 0.4112\n",
      "Iter-27600 train loss: 2.1572 valid loss: 2.1099, valid accuracy: 0.4118\n",
      "Iter-27700 train loss: 2.0797 valid loss: 2.1093, valid accuracy: 0.4128\n",
      "Iter-27800 train loss: 2.1477 valid loss: 2.1087, valid accuracy: 0.4132\n",
      "Iter-27900 train loss: 2.1268 valid loss: 2.1081, valid accuracy: 0.4136\n",
      "Iter-28000 train loss: 2.1457 valid loss: 2.1076, valid accuracy: 0.4136\n",
      "Iter-28100 train loss: 2.0621 valid loss: 2.1070, valid accuracy: 0.4138\n",
      "Iter-28200 train loss: 2.1250 valid loss: 2.1064, valid accuracy: 0.4142\n",
      "Iter-28300 train loss: 2.1101 valid loss: 2.1059, valid accuracy: 0.4144\n",
      "Iter-28400 train loss: 2.1207 valid loss: 2.1053, valid accuracy: 0.4156\n",
      "Iter-28500 train loss: 2.0520 valid loss: 2.1047, valid accuracy: 0.4160\n",
      "Iter-28600 train loss: 2.0999 valid loss: 2.1041, valid accuracy: 0.4156\n",
      "Iter-28700 train loss: 2.1335 valid loss: 2.1035, valid accuracy: 0.4150\n",
      "Iter-28800 train loss: 2.1143 valid loss: 2.1030, valid accuracy: 0.4150\n",
      "Iter-28900 train loss: 2.1191 valid loss: 2.1024, valid accuracy: 0.4148\n",
      "Iter-29000 train loss: 2.1125 valid loss: 2.1018, valid accuracy: 0.4156\n",
      "Iter-29100 train loss: 2.1256 valid loss: 2.1012, valid accuracy: 0.4160\n",
      "Iter-29200 train loss: 2.1444 valid loss: 2.1007, valid accuracy: 0.4166\n",
      "Iter-29300 train loss: 2.0963 valid loss: 2.1001, valid accuracy: 0.4164\n",
      "Iter-29400 train loss: 2.1426 valid loss: 2.0995, valid accuracy: 0.4164\n",
      "Iter-29500 train loss: 2.0998 valid loss: 2.0989, valid accuracy: 0.4160\n",
      "Iter-29600 train loss: 2.0819 valid loss: 2.0984, valid accuracy: 0.4162\n",
      "Iter-29700 train loss: 2.0688 valid loss: 2.0978, valid accuracy: 0.4164\n",
      "Iter-29800 train loss: 2.1295 valid loss: 2.0972, valid accuracy: 0.4166\n",
      "Iter-29900 train loss: 2.0421 valid loss: 2.0966, valid accuracy: 0.4168\n",
      "Iter-30000 train loss: 2.1270 valid loss: 2.0960, valid accuracy: 0.4166\n",
      "Iter-30100 train loss: 2.0801 valid loss: 2.0955, valid accuracy: 0.4174\n",
      "Iter-30200 train loss: 2.0999 valid loss: 2.0949, valid accuracy: 0.4170\n",
      "Iter-30300 train loss: 2.0946 valid loss: 2.0943, valid accuracy: 0.4170\n",
      "Iter-30400 train loss: 2.0854 valid loss: 2.0937, valid accuracy: 0.4170\n",
      "Iter-30500 train loss: 2.0845 valid loss: 2.0932, valid accuracy: 0.4170\n",
      "Iter-30600 train loss: 2.0762 valid loss: 2.0926, valid accuracy: 0.4178\n",
      "Iter-30700 train loss: 2.0898 valid loss: 2.0920, valid accuracy: 0.4178\n",
      "Iter-30800 train loss: 2.0785 valid loss: 2.0915, valid accuracy: 0.4172\n",
      "Iter-30900 train loss: 2.1495 valid loss: 2.0909, valid accuracy: 0.4176\n",
      "Iter-31000 train loss: 2.0975 valid loss: 2.0903, valid accuracy: 0.4188\n",
      "Iter-31100 train loss: 2.1255 valid loss: 2.0897, valid accuracy: 0.4188\n",
      "Iter-31200 train loss: 2.0777 valid loss: 2.0892, valid accuracy: 0.4198\n",
      "Iter-31300 train loss: 2.0734 valid loss: 2.0886, valid accuracy: 0.4198\n",
      "Iter-31400 train loss: 2.0943 valid loss: 2.0880, valid accuracy: 0.4200\n",
      "Iter-31500 train loss: 2.0833 valid loss: 2.0874, valid accuracy: 0.4196\n",
      "Iter-31600 train loss: 2.0533 valid loss: 2.0869, valid accuracy: 0.4200\n",
      "Iter-31700 train loss: 2.1221 valid loss: 2.0863, valid accuracy: 0.4206\n",
      "Iter-31800 train loss: 2.0968 valid loss: 2.0857, valid accuracy: 0.4204\n",
      "Iter-31900 train loss: 2.0514 valid loss: 2.0852, valid accuracy: 0.4202\n",
      "Iter-32000 train loss: 2.1051 valid loss: 2.0846, valid accuracy: 0.4210\n",
      "Iter-32100 train loss: 2.1462 valid loss: 2.0840, valid accuracy: 0.4208\n",
      "Iter-32200 train loss: 2.0420 valid loss: 2.0835, valid accuracy: 0.4210\n",
      "Iter-32300 train loss: 2.1341 valid loss: 2.0829, valid accuracy: 0.4214\n",
      "Iter-32400 train loss: 2.0625 valid loss: 2.0824, valid accuracy: 0.4214\n",
      "Iter-32500 train loss: 2.0844 valid loss: 2.0818, valid accuracy: 0.4216\n",
      "Iter-32600 train loss: 2.0630 valid loss: 2.0812, valid accuracy: 0.4214\n",
      "Iter-32700 train loss: 2.0441 valid loss: 2.0807, valid accuracy: 0.4218\n",
      "Iter-32800 train loss: 2.0852 valid loss: 2.0801, valid accuracy: 0.4216\n",
      "Iter-32900 train loss: 2.0700 valid loss: 2.0795, valid accuracy: 0.4218\n",
      "Iter-33000 train loss: 2.0589 valid loss: 2.0790, valid accuracy: 0.4220\n",
      "Iter-33100 train loss: 2.1089 valid loss: 2.0784, valid accuracy: 0.4224\n",
      "Iter-33200 train loss: 2.0763 valid loss: 2.0778, valid accuracy: 0.4226\n",
      "Iter-33300 train loss: 2.0871 valid loss: 2.0773, valid accuracy: 0.4230\n",
      "Iter-33400 train loss: 2.0897 valid loss: 2.0767, valid accuracy: 0.4238\n",
      "Iter-33500 train loss: 2.0821 valid loss: 2.0762, valid accuracy: 0.4236\n",
      "Iter-33600 train loss: 2.1677 valid loss: 2.0756, valid accuracy: 0.4234\n",
      "Iter-33700 train loss: 2.1319 valid loss: 2.0750, valid accuracy: 0.4238\n",
      "Iter-33800 train loss: 2.0914 valid loss: 2.0745, valid accuracy: 0.4244\n",
      "Iter-33900 train loss: 2.0944 valid loss: 2.0739, valid accuracy: 0.4250\n",
      "Iter-34000 train loss: 2.0917 valid loss: 2.0733, valid accuracy: 0.4246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 2.0646 valid loss: 2.0728, valid accuracy: 0.4252\n",
      "Iter-34200 train loss: 2.0723 valid loss: 2.0722, valid accuracy: 0.4256\n",
      "Iter-34300 train loss: 2.1272 valid loss: 2.0717, valid accuracy: 0.4258\n",
      "Iter-34400 train loss: 2.0824 valid loss: 2.0711, valid accuracy: 0.4262\n",
      "Iter-34500 train loss: 2.1188 valid loss: 2.0705, valid accuracy: 0.4268\n",
      "Iter-34600 train loss: 2.1308 valid loss: 2.0700, valid accuracy: 0.4264\n",
      "Iter-34700 train loss: 2.0626 valid loss: 2.0694, valid accuracy: 0.4266\n",
      "Iter-34800 train loss: 2.0804 valid loss: 2.0688, valid accuracy: 0.4268\n",
      "Iter-34900 train loss: 2.0647 valid loss: 2.0683, valid accuracy: 0.4266\n",
      "Iter-35000 train loss: 2.0479 valid loss: 2.0677, valid accuracy: 0.4278\n",
      "Iter-35100 train loss: 2.1354 valid loss: 2.0672, valid accuracy: 0.4280\n",
      "Iter-35200 train loss: 2.0695 valid loss: 2.0666, valid accuracy: 0.4280\n",
      "Iter-35300 train loss: 2.0874 valid loss: 2.0660, valid accuracy: 0.4280\n",
      "Iter-35400 train loss: 2.0230 valid loss: 2.0655, valid accuracy: 0.4278\n",
      "Iter-35500 train loss: 2.0872 valid loss: 2.0649, valid accuracy: 0.4286\n",
      "Iter-35600 train loss: 2.0683 valid loss: 2.0644, valid accuracy: 0.4294\n",
      "Iter-35700 train loss: 2.0400 valid loss: 2.0638, valid accuracy: 0.4286\n",
      "Iter-35800 train loss: 2.0798 valid loss: 2.0633, valid accuracy: 0.4288\n",
      "Iter-35900 train loss: 2.1288 valid loss: 2.0627, valid accuracy: 0.4296\n",
      "Iter-36000 train loss: 2.0464 valid loss: 2.0621, valid accuracy: 0.4296\n",
      "Iter-36100 train loss: 2.0199 valid loss: 2.0616, valid accuracy: 0.4298\n",
      "Iter-36200 train loss: 2.0598 valid loss: 2.0611, valid accuracy: 0.4302\n",
      "Iter-36300 train loss: 2.1293 valid loss: 2.0605, valid accuracy: 0.4302\n",
      "Iter-36400 train loss: 2.0405 valid loss: 2.0599, valid accuracy: 0.4308\n",
      "Iter-36500 train loss: 2.1195 valid loss: 2.0594, valid accuracy: 0.4310\n",
      "Iter-36600 train loss: 2.0503 valid loss: 2.0589, valid accuracy: 0.4316\n",
      "Iter-36700 train loss: 2.0835 valid loss: 2.0583, valid accuracy: 0.4314\n",
      "Iter-36800 train loss: 2.0875 valid loss: 2.0578, valid accuracy: 0.4312\n",
      "Iter-36900 train loss: 2.0769 valid loss: 2.0572, valid accuracy: 0.4314\n",
      "Iter-37000 train loss: 2.0379 valid loss: 2.0567, valid accuracy: 0.4328\n",
      "Iter-37100 train loss: 2.0631 valid loss: 2.0561, valid accuracy: 0.4326\n",
      "Iter-37200 train loss: 2.0685 valid loss: 2.0556, valid accuracy: 0.4324\n",
      "Iter-37300 train loss: 2.0841 valid loss: 2.0550, valid accuracy: 0.4328\n",
      "Iter-37400 train loss: 2.0828 valid loss: 2.0545, valid accuracy: 0.4332\n",
      "Iter-37500 train loss: 2.0775 valid loss: 2.0539, valid accuracy: 0.4340\n",
      "Iter-37600 train loss: 2.0395 valid loss: 2.0534, valid accuracy: 0.4342\n",
      "Iter-37700 train loss: 2.0713 valid loss: 2.0528, valid accuracy: 0.4346\n",
      "Iter-37800 train loss: 2.0497 valid loss: 2.0523, valid accuracy: 0.4346\n",
      "Iter-37900 train loss: 2.0243 valid loss: 2.0517, valid accuracy: 0.4352\n",
      "Iter-38000 train loss: 2.0619 valid loss: 2.0512, valid accuracy: 0.4356\n",
      "Iter-38100 train loss: 2.0604 valid loss: 2.0507, valid accuracy: 0.4358\n",
      "Iter-38200 train loss: 2.0380 valid loss: 2.0501, valid accuracy: 0.4366\n",
      "Iter-38300 train loss: 1.9996 valid loss: 2.0496, valid accuracy: 0.4366\n",
      "Iter-38400 train loss: 2.0435 valid loss: 2.0490, valid accuracy: 0.4368\n",
      "Iter-38500 train loss: 2.0822 valid loss: 2.0485, valid accuracy: 0.4368\n",
      "Iter-38600 train loss: 2.0824 valid loss: 2.0480, valid accuracy: 0.4362\n",
      "Iter-38700 train loss: 2.0730 valid loss: 2.0474, valid accuracy: 0.4360\n",
      "Iter-38800 train loss: 2.0638 valid loss: 2.0469, valid accuracy: 0.4354\n",
      "Iter-38900 train loss: 2.0486 valid loss: 2.0464, valid accuracy: 0.4364\n",
      "Iter-39000 train loss: 2.0785 valid loss: 2.0458, valid accuracy: 0.4366\n",
      "Iter-39100 train loss: 2.0570 valid loss: 2.0453, valid accuracy: 0.4374\n",
      "Iter-39200 train loss: 2.0524 valid loss: 2.0448, valid accuracy: 0.4366\n",
      "Iter-39300 train loss: 2.0352 valid loss: 2.0442, valid accuracy: 0.4366\n",
      "Iter-39400 train loss: 2.0436 valid loss: 2.0437, valid accuracy: 0.4362\n",
      "Iter-39500 train loss: 2.0422 valid loss: 2.0432, valid accuracy: 0.4364\n",
      "Iter-39600 train loss: 2.1052 valid loss: 2.0426, valid accuracy: 0.4360\n",
      "Iter-39700 train loss: 2.1632 valid loss: 2.0421, valid accuracy: 0.4360\n",
      "Iter-39800 train loss: 2.0653 valid loss: 2.0416, valid accuracy: 0.4364\n",
      "Iter-39900 train loss: 1.9305 valid loss: 2.0411, valid accuracy: 0.4366\n",
      "Iter-40000 train loss: 2.0438 valid loss: 2.0405, valid accuracy: 0.4366\n",
      "Iter-40100 train loss: 2.0029 valid loss: 2.0400, valid accuracy: 0.4364\n",
      "Iter-40200 train loss: 2.0407 valid loss: 2.0394, valid accuracy: 0.4366\n",
      "Iter-40300 train loss: 2.0966 valid loss: 2.0389, valid accuracy: 0.4374\n",
      "Iter-40400 train loss: 2.0386 valid loss: 2.0383, valid accuracy: 0.4380\n",
      "Iter-40500 train loss: 2.0177 valid loss: 2.0378, valid accuracy: 0.4384\n",
      "Iter-40600 train loss: 2.0051 valid loss: 2.0373, valid accuracy: 0.4388\n",
      "Iter-40700 train loss: 2.0490 valid loss: 2.0367, valid accuracy: 0.4386\n",
      "Iter-40800 train loss: 2.1096 valid loss: 2.0362, valid accuracy: 0.4392\n",
      "Iter-40900 train loss: 2.0074 valid loss: 2.0357, valid accuracy: 0.4394\n",
      "Iter-41000 train loss: 2.0405 valid loss: 2.0351, valid accuracy: 0.4396\n",
      "Iter-41100 train loss: 1.9628 valid loss: 2.0346, valid accuracy: 0.4394\n",
      "Iter-41200 train loss: 2.0417 valid loss: 2.0341, valid accuracy: 0.4398\n",
      "Iter-41300 train loss: 2.0206 valid loss: 2.0335, valid accuracy: 0.4396\n",
      "Iter-41400 train loss: 2.0283 valid loss: 2.0330, valid accuracy: 0.4396\n",
      "Iter-41500 train loss: 1.9910 valid loss: 2.0325, valid accuracy: 0.4394\n",
      "Iter-41600 train loss: 1.9669 valid loss: 2.0319, valid accuracy: 0.4398\n",
      "Iter-41700 train loss: 2.0497 valid loss: 2.0314, valid accuracy: 0.4396\n",
      "Iter-41800 train loss: 2.1051 valid loss: 2.0309, valid accuracy: 0.4394\n",
      "Iter-41900 train loss: 2.0007 valid loss: 2.0303, valid accuracy: 0.4392\n",
      "Iter-42000 train loss: 2.0325 valid loss: 2.0298, valid accuracy: 0.4394\n",
      "Iter-42100 train loss: 1.9805 valid loss: 2.0293, valid accuracy: 0.4396\n",
      "Iter-42200 train loss: 2.0258 valid loss: 2.0288, valid accuracy: 0.4396\n",
      "Iter-42300 train loss: 1.9930 valid loss: 2.0283, valid accuracy: 0.4400\n",
      "Iter-42400 train loss: 2.0418 valid loss: 2.0277, valid accuracy: 0.4400\n",
      "Iter-42500 train loss: 2.0792 valid loss: 2.0272, valid accuracy: 0.4406\n",
      "Iter-42600 train loss: 2.0555 valid loss: 2.0267, valid accuracy: 0.4404\n",
      "Iter-42700 train loss: 2.0814 valid loss: 2.0261, valid accuracy: 0.4410\n",
      "Iter-42800 train loss: 2.0760 valid loss: 2.0256, valid accuracy: 0.4412\n",
      "Iter-42900 train loss: 2.0108 valid loss: 2.0251, valid accuracy: 0.4410\n",
      "Iter-43000 train loss: 2.0267 valid loss: 2.0246, valid accuracy: 0.4414\n",
      "Iter-43100 train loss: 1.9833 valid loss: 2.0241, valid accuracy: 0.4410\n",
      "Iter-43200 train loss: 2.0270 valid loss: 2.0236, valid accuracy: 0.4412\n",
      "Iter-43300 train loss: 2.0689 valid loss: 2.0231, valid accuracy: 0.4418\n",
      "Iter-43400 train loss: 2.0317 valid loss: 2.0225, valid accuracy: 0.4422\n",
      "Iter-43500 train loss: 2.0519 valid loss: 2.0220, valid accuracy: 0.4432\n",
      "Iter-43600 train loss: 2.0288 valid loss: 2.0215, valid accuracy: 0.4430\n",
      "Iter-43700 train loss: 2.0503 valid loss: 2.0209, valid accuracy: 0.4434\n",
      "Iter-43800 train loss: 2.0547 valid loss: 2.0204, valid accuracy: 0.4438\n",
      "Iter-43900 train loss: 2.0772 valid loss: 2.0199, valid accuracy: 0.4436\n",
      "Iter-44000 train loss: 2.0599 valid loss: 2.0194, valid accuracy: 0.4434\n",
      "Iter-44100 train loss: 2.0471 valid loss: 2.0189, valid accuracy: 0.4436\n",
      "Iter-44200 train loss: 1.9664 valid loss: 2.0183, valid accuracy: 0.4432\n",
      "Iter-44300 train loss: 2.0239 valid loss: 2.0178, valid accuracy: 0.4432\n",
      "Iter-44400 train loss: 2.0055 valid loss: 2.0173, valid accuracy: 0.4430\n",
      "Iter-44500 train loss: 2.0590 valid loss: 2.0168, valid accuracy: 0.4430\n",
      "Iter-44600 train loss: 2.0026 valid loss: 2.0162, valid accuracy: 0.4440\n",
      "Iter-44700 train loss: 2.0266 valid loss: 2.0157, valid accuracy: 0.4436\n",
      "Iter-44800 train loss: 2.0851 valid loss: 2.0151, valid accuracy: 0.4434\n",
      "Iter-44900 train loss: 2.0581 valid loss: 2.0146, valid accuracy: 0.4434\n",
      "Iter-45000 train loss: 2.0842 valid loss: 2.0141, valid accuracy: 0.4442\n",
      "Iter-45100 train loss: 2.0689 valid loss: 2.0136, valid accuracy: 0.4444\n",
      "Iter-45200 train loss: 2.0727 valid loss: 2.0131, valid accuracy: 0.4440\n",
      "Iter-45300 train loss: 2.1114 valid loss: 2.0126, valid accuracy: 0.4442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 1.9669 valid loss: 2.0120, valid accuracy: 0.4442\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 1000000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
