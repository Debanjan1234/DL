{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'train2':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "#             Wxh_res=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "#             Whh_res=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "#             bh_res=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "#         Wxh, Whh, Wxh_res, Whh_res, Why = m['Wxh'], m['Whh'], m['Wxh_res'], m['Whh_res'], m['Why']\n",
    "#         bh, bh_res, by = m['bh'], m['bh_res'], m['by']\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = (X @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "#         h, h_cache = self.selu_forward(h)\n",
    "\n",
    "#         # h_res for residual connection or skip connection for gradients\n",
    "#         # Residual connection to avoid vanishing gradients\n",
    "#         # SELU act_function to avoid exploding gradients\n",
    "#         # x+ f(x)\n",
    "#         h_res = (X @ Wxh_res) + (hprev @ Whh_res) + bh_res\n",
    "#         h += h_res\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "#         y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "        y, nl_cache = self.selu_forward(y)\n",
    "        y, do_cache = self.alpha_dropout_fwd(h=y, q=1.0-self.p_dropout) # q=1-p, 1=keep_prob\n",
    "\n",
    "#         cache = (X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, nl_cache, do_cache)\n",
    "#         cache = (X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, do_cache)\n",
    "        cache = (X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache)\n",
    "#         cache = (X, hprev, Wxh, Whh, h_cache, y_cache, do_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "#         X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, nl_cache, do_cache = cache\n",
    "#         X, hprev, Wxh, Whh, Wxh_res, Whh_res, h_cache, y_cache, do_cache = cache\n",
    "        X, hprev, Wxh, Whh, h_cache, y_cache, nl_cache, do_cache = cache\n",
    "#         X, hprev, Wxh, Whh, h_cache, y_cache, do_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "#         dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        dy = self.alpha_dropout_bwd(dout=dy, cache=do_cache)\n",
    "        dy = self.selu_backward(dy, nl_cache)\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "#         dh_res = dh.copy()\n",
    "#         dbh_res = dh_res * 1.0\n",
    "#         dWhh_res = hprev.T @ dh_res\n",
    "#         dWxh_res = X.T @ dh_res\n",
    "#         dX_res = dh_res @ Wxh_res.T\n",
    "#         dh_res = dh_res @ Whh_res.T\n",
    "\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "#         dh = self.selu_backward(dh, h_cache)\n",
    "        dbh = dh * 1.0\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "#         dX += dX_res\n",
    "#         dh += dh_res\n",
    "\n",
    "#         grad = dict(Wxh=dWxh, Whh=dWhh, Wxh_res=dWxh_res, Whh_res=dWhh_res, Why=dWhy, bh=dbh, bh_res=dbh_res, \n",
    "#                     by=dby)\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    # keep_prob = 1 - p_dropout, q = 1 - p\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        u = cache\n",
    "        dX = dout * u\n",
    "        return dX\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "    \n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "\n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for k in grad[0].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size): # range(start, stop, step)\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle=True):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()}) # dict={items, key:val, word:ID}\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99 # 0.9 to 0.99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    \n",
    "    #     import impl.constant as c, c.eps\n",
    "    eps = 1e-8 # constant\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1): # range(start, stop, step=1 by default)\n",
    "\n",
    "        # No batches or other files available\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items for dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print training loss and predicted samping for testing the model\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} training loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 training loss: 3.2124\n",
      "erten As DK Cs\n",
      "x% NU% tH 1L0'n yd int de G本itye zent5 ent日 Ller, \"weol, 4n R\n",
      "A, r1 PETKekMe0tl par(tK\n",
      "Iter-26 training loss: 3.3472\n",
      "ed LR natita avECarec7nWedios SitlanArser Herlan3 5k .st Na7 LiserUz-:P )KintiTJlt:tTlOhe K(–potLiUlm\n",
      "Iter-39 training loss: 3.5148\n",
      "e8la an porz\n",
      "fe fU. ;HrbaliDWaiL –-6\"rTlb1ne in or smas swse 本inGt\"Robanl pas i本-Dto–(Jas opDwanA6 tm\n",
      "Iter-52 training loss: 3.1716\n",
      "e   parlE's RoOS8本g 5iA\" tba, tliol日 a wis)本Mtw–zorverieeLin Nar1 unita3( ent WaN6ukeNta29ole 47n8une\n",
      "Iter-65 training loss: 3.5394\n",
      "en HIn SoMia oestis FiOBon Pins t Nar–De7 o\"1t2tla to5inx gap algipmper tpen yJ5pin Ts age本 t, ra aGl\n",
      "Iter-78 training loss: 3.0717\n",
      "eAPeaoin6 in\"rJ6r pirs Kont Kare poEFi日a Ty Japast haGanlo6 -warl7 4oOMeyFf Kist -om, an EoriEn B-an \n",
      "Iter-91 training loss: 3.1428\n",
      "ezele anIn0ozest din7t1 elola5y rore t6eiDkop t本 yore xFoEnsy 31silen larkf:KLHeMta fo日nzins\"TNJ itiH\n",
      "Iter-104 training loss: 3.2252\n",
      "er arCn日Ttr8;win 4as o) o%:y p5DjyrOanle; t9e) o,a5R in u:CGre. iS 8Poan9en日antx'4 te ef to BlPian: 日\n",
      "Iter-117 training loss: 3.2258\n",
      "e Fer ont Na–nitinLeHinareleg o1 ePcanizeAhewpor TDTrenRsn(per onla OU\"Et In 3) o\n",
      "9eHjartat Japki21Ni\n",
      "Iter-130 training loss: 2.9877\n",
      "e Iy is elitat7o本 日iS 'r7 lo5HoperinagemtolecenhLkLenJama\"y re(ian ;oHlastoORogtD ) meWiSI aSls sian \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RNN at 0x7f63f0c8d588>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameters\n",
    "n_iter = 130 # epochs\n",
    "print_after = n_iter//10 # print loss, valid, and test\n",
    "time_step = 100 # width\n",
    "alpha = 1/time_step #1e-3 # learning_rate\n",
    "num_layers = 1 # depth\n",
    "num_hidden_units = 64 # hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "p_dropout = 0.10 # keep_prob=1.0-p_dropout, q=1-p # 5% to 10% noise is recommanded! as p_dropout\n",
    "\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f63d0962828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "# plt.plot(net.losses['train2'], label='Train loss 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
