{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        # q = 1-p_dropout\n",
    "        # u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "        \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "        # equal to\n",
    "        # h = (1. - hz) * h_in + hz * hh\n",
    "        # or\n",
    "        # h = h_in + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz\n",
    "        # or\n",
    "        # h = h_in + hh\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train: # with Dropout\n",
    "            y, y_do_cache = self.dropout_forward(y, self.p_dropout)\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_do_cache)\n",
    "        else: # no Dropout: testing and validation\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=y_do_cache)\n",
    "        else:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dy_out = dy.copy()\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        \n",
    "        dh += dh_out\n",
    "        dh_in1 = dh * (1. - hz)\n",
    "\n",
    "        dhh =  dh * hz\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "        \n",
    "        dhz = dh * (hh - h_in)\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "        \n",
    "        dX = dXz + dXh\n",
    "        \n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dh = dh_in1 + dh_in2\n",
    "        \n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, Xs, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for t in range(len(Xs)):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[Xs[t]] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t], train=True)                \n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X_seed] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "         \n",
    "        # Hidden layers\n",
    "        for layer in range(nn.L):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                sample = self.test(X_mini[0], state, size=100)\n",
    "                print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 40.1720\n",
      "clfiarnspb ,rgtiswogois ree  dsnra3dia'e-s oono  to,elspaparr,eudaasIdr. ria w 2uspeo o hfn  wa i dto\n",
      "Iter-2 loss: 38.7454\n",
      "cdBvsuefJufJitse d a'p r de 3ltbrhrfhegtne C.tda1teuaac  slan soro9eaolosd rd2 ih a sareim ct t  h7r \n",
      "Iter-3 loss: 37.6296\n",
      "cp,f8aeaoapiesru wl iiiputcat6.cg l srpudcehkrgt mce ssdolnmP  6ekit adinTni.drAemirgiuSenEsciGs  ipc\n",
      "Iter-4 loss: 38.7449\n",
      "c7we  otned  c  di2hio,cr,tlto imouo gli.ni cnliicihrtoiyr9rnt NTneo.diis noeree wnluddliot h tttal n\n",
      "Iter-5 loss: 35.4462\n",
      "cI idbed ihl ueui  iii  ut Spii dpat1eslJsisacnicalegi prnn7  wt iaifr aOArl td agea2rsxuis rcteaigt,\n",
      "Iter-6 loss: 38.2912\n",
      "c日.rstolffnpSi anatatep1ntrpeet,itielo m tesE ngooeeaabwiNniaah idcp m  otsvn ell trsline asrebi,e tn\n",
      "Iter-7 loss: 36.1148\n",
      "c  totsaot parretwprfcnlIk0 nh hiCpRlu,1tgscte9i t  erirbhee o uahp rltsmAson nsefr34ins .3hei n7san.\n",
      "Iter-8 loss: 35.6561\n",
      "cwno a8an umevp NnnesoeEelee rneoidoneidowghl tlAat aghm,i csfeoiur  ilnp oelsene lchlptpnt cthdoonti\n",
      "Iter-9 loss: 35.5873\n",
      "cn hiis elcodtnoEhrbaoJexaieapirnoaomo rirwvdPie ta iCfiiierrxeTeioiohn cpccMitit, eniortis,'8u tt0he\n",
      "Iter-10 loss: 37.3187\n",
      "cCeeaealrsotr i gusrracaeonsnoEteai8wnplns nrlG7n.speJienuaetelissictttalcortr ouyhte tey6ssele ikt c\n",
      "Iter-11 loss: 35.1546\n",
      "ctecoaiJo  r,u ttnlieaddtooenntiiiplietzptpnremo a atieodesiy,taneknn yeAsd2rasriegtnitseiirt NieWMlo\n",
      "Iter-12 loss: 34.5983\n",
      "cp8d iacaiiapsapmtn1vyicst icoe tdnoelonncdoc本hal1karrulspnnr  ine'1mi x7asgseotoaio opsct aol -nrita\n",
      "Iter-13 loss: 34.7599\n",
      "cfGva4no,hrh1 edsrua odSa ua',8aifrueoiJBniJj 5 opdrson\"oetaem at tophrfltc,xrcGEiphuftedemmninouJrOe\n",
      "Iter-14 loss: 33.7938\n",
      "cta eJ crnuoalpcncotnpgfaet0,oanplirmnst0npeasiuitortuanaepwegJnepEoluntncmislstrrenitteor1itsposoiie\n",
      "Iter-15 loss: 34.1358\n",
      "ch toirronio araauiaaroorlaiectsnehclenottaolasnoog9rtedltIsristta1oatanahure teipar samtltaat,niasan\n",
      "Iter-16 loss: 34.1696\n",
      "c hoonceol wnkftoieoJsalslP,almsnonttrtccitl,lnn4xoutltw;GeeMianiovtelCTzseliisti;prdn.ia f-t-tnindhn\n",
      "Iter-17 loss: 34.0541\n",
      "caorehfi,ll\"'lIwoasabmslailoaolelsoouaiiaoopeoogream,o tJ stuaslW tmAealrtpnlpiiunmnT,thrlgatipape,ua\n",
      "Iter-18 loss: 33.9173\n",
      "cGenlirJle  inailbaelrdt licinmnmnhtptanysthOaoomaxhnatctgagaeotrhtl0hoihesacbretkmwr\"nxtn pdtCedn9oa\n",
      "Iter-19 loss: 35.6161\n",
      "cppltdoalgJitl2ntaaScaol1ticchillliaelbnoadnsrwawsaleneeaspnolaohtitiaokn.ldttrnso ebptounncoafktmech\n",
      "Iter-20 loss: 36.0158\n",
      "cthgubnesnhteldsiaeousinotlnnafteksotu.ilpEegaptadheruetxJnteTnsintthoohimefitt15urlc8lprs75eraelsaht\n",
      "Iter-21 loss: 33.3829\n",
      "c aiptnel arnWcoenilooeaawoioretafnrtint dWna.pJTuoyreitobnhlanoahkmt.nl,aeyouibtiftxarttUguibelpNaol\n",
      "Iter-22 loss: 33.3863\n",
      "clteinUChgndttri8iesl a5oaortepiwlenabtaadaWSlaioomgyim-aaiatoiBe5rerocoanrlnuohputnnoeawotsitan itsJ\n",
      "Iter-23 loss: 33.7108\n",
      "cSa-P1oalyoonodrbttnncndorclvoryiiiraoeoieooeosertnerloiel lnttiinnlleecdaeinnsoiopttdaanidyioopF6raa\n",
      "Iter-24 loss: 33.0178\n",
      "cDtwosltoe6tttnkauaeaprr1oirs\n",
      "tyal,hatliyetltsysenireaeanbaaoa-hogiooahaatnianynttoniTyrecb5atsiten,a\n",
      "Iter-25 loss: 32.9370\n",
      "cAleiiastduFldootaihmot:lheiuaelaeleiuilatdaiiaueeIou\"nlpanitfieaiuaanh9aao.iea\"aiasttasooisiaxctnnel\n",
      "Iter-26 loss: 32.4502\n",
      "chaatsnoSrracfetdstireamsnotano latcoalnbltr neoaaataerp-4anaalay8o1ts'ilcdietacaoaonnoyshibseuerrlrl\n",
      "Iter-27 loss: 33.8602\n",
      "chuiNaenmdoeJFseopapolloatcmetsrocwncepaJotds1aosdreatorbldnlt-ddsurotlulaahscheoalsmcoigiorfombaEcha\n",
      "Iter-28 loss: 32.4247\n",
      "c6rrlillaitooearttitons7laaomtieSlepoareiet8hduonien9ntoiieit5rie5iRcwriytlncglWtbasMrr2ixGaGraoaoiiy\n",
      "Iter-29 loss: 34.3848\n",
      "ctohltDioettpdeegce.toilop aaatflsnopoht,ielroiatualitiaaatiusmaetde\"glialinNii2vealuoaoaaeonuntfmfop\n",
      "Iter-30 loss: 32.4359\n",
      "cf ocLpiyonorbfcntcycfmanlntmltll, chotratetyeoh2ehroiabtatdLarasnooiylaaeant.apeeiniaolaadntuexlello\n",
      "Iter-31 loss: 32.4349\n",
      "cIdxeoiiiafahairhedGeuroenelCupohtoietlbiiecabpnatoiecoiollbttrcaloeo,onrtpnoxlaoilteoPubiioaooomthot\n",
      "Iter-32 loss: 31.7443\n",
      "c eiyainintbbp tettanharlo2csninoiarieanaanehap\"lotneShaionhroltloai daoaeiieianeie tiarumt.rnpceleia\n",
      "Iter-33 loss: 34.5655\n",
      "cHanorlauvfafolGypeoottaauneeoosayaoe iooa'luua.hxbaoofnnltiliptaaehmcliurioadRlmetauttiena vaati)tdm\n",
      "Iter-34 loss: 31.8174\n",
      "ci piaJeemcontetapih,rto3iodxait\n",
      "Nrdtaooese1atauoaotDhlWoaao  eorttatdkblroeooiiGuiignlteaWoooaGwsiaa\n",
      "Iter-35 loss: 32.8176\n",
      "cgiopooitlileancliomera5aalilaoaetieaaoiPllroattanolteinntancdonaeoogaelamLputeholioltatlole'odlOenoo\n",
      "Iter-36 loss: 30.1869\n",
      "cioiarxaiotb7taeatacrhncUSnnarairlaemhngpouwynilrantiuoaaieaclstmuaaueareolaciaaaienvbecoasteuosat8ot\n",
      "Iter-37 loss: 31.5822\n",
      "caahplofoitneoptaeohcntimtoephhPislnsuinpJatotiaenoorermcoata n8ppaoueoatioainmatooouionfiluhll.iaiml\n",
      "Iter-38 loss: 31.8036\n",
      "cnaopolrosaiotlonaaUaofaeacioionteote5pppasitolaoontboaSnoioupooueaebilroririaahlfnxfbmoiooapen8p8anp\n",
      "Iter-39 loss: 31.7183\n",
      "ceieaaaoolhocildnonooamtiataeoetsiTplcoataRi,iu5itolrthathneoritltloglyioombnbpecaoiooepiobotesiiorun\n",
      "Iter-40 loss: 31.7939\n",
      "cahfnontoeooJeciol'ipaoatlo CoelfaaamoalSeiolioaioi5tehiecpuelaootnooloaoaaoolauopotieettsleollraaeno\n",
      "Iter-41 loss: 32.2093\n",
      "cte-apefiiloconancoikolearingatarnteeoeidEtowltsofiaatlfnaecipeusutfpuasaotinfnmsmootsrfaonlcttonnota\n",
      "Iter-42 loss: 30.9320\n",
      "cN eorwntanRscnrnaapnotaftbillanpposatoraoettctaonpelpaaiSita9rt\"oapjnenene\"icabamotw eoci9iehoai,n5n\n",
      "Iter-43 loss: 31.0830\n",
      "cOiurtdclirhotaJliyblaecmtookanniitotpaoonffiiatfietaioaelateaoao nftanaiaiaeian8loobolatnpaoooaateie\n",
      "Iter-44 loss: 31.4748\n",
      "cgnoaclt6rpuotfobolnlooaiehoeorntotitdioeoniase-p5aiponptosota1niensnaeeerprcriRabEittoTttrauawnseoht\n",
      "Iter-45 loss: 31.4367\n",
      "cnuoretegeionorJiideieanioonaoapStolaortLtifieuaniachacrrl,attoor oelutilkaojCnoamcbotaoceltenaetaa8r\n",
      "Iter-46 loss: 32.9343\n",
      "ccettflaultensoaaaidaatyrieosyilopiarhailtoittlplippNtanutaaiehpbmhaofnusoieateeeaoaoalaAaoilbheieont\n",
      "Iter-47 loss: 31.9735\n",
      "cHsoaoaouoatiu1oemepneaoluiaulaiintnnaogeaeioctaol7saealoeCalihemoeadn.a-ttiootoenainaJnaooJtaanoSnae\n",
      "Iter-48 loss: 31.9568\n",
      "ch peonmitoltgppGgacnoaslistiiaewiohoeaaPpeeaarar,aoibalsetiaierseeo5epoaoiLrnollgltoppaattoceutaneat\n",
      "Iter-49 loss: 30.9679\n",
      "ci oaraotaiacotonintnNeNteuaoohiaaiioNooiionilntusteauantteotttxllilitinpohah1nnoootltItaoatoIipralhi\n",
      "Iter-50 loss: 31.7909\n",
      "c  toaheottonoieooinefbrsavlniamaoolLcapiixlloattctiWkaeiaansiotataaloetenaaaeieiopieetie,ptsdohiofta\n",
      "Iter-51 loss: 30.8994\n",
      "ci teramtthiaaeaoocacanoipoemtiieeooiahpapetedoanltooaaalshreuinarttnlonbaoaaucoaiotanotiaaoxioJaeita\n",
      "Iter-52 loss: 32.8660\n",
      "cKprtaotatiinaaopcoaiiarnoaoslpaocsepneeioWrtosnaniioanntdaaoaesdoaaootuoaaoebisnollaioolauaoolupoepi\n",
      "Iter-53 loss: 30.7738\n",
      "cliramentaoammofooseitlnatsploaonehaop'iluiucesnioeopcoiiolteUtlnciiaeoincaalemaliloftliaynopttpmnooi\n",
      "Iter-54 loss: 30.5991\n",
      "cbueoetetlccatiotool5iloleeshiwp,iaa,aaannoamotolba,aceooaoiot9icuiotaooipabrieospeatappoatyarutotleo\n",
      "Iter-55 loss: 28.4110\n",
      "colcapooisrabieioooeemelinhiantrioaepgienalinlroanuwotilpee-,nstanaasbppeelgt5floaoeealtirntreotdmiog\n",
      "Iter-56 loss: 31.4688\n",
      "ce-aearenarpilgeosraopehztnisnnpoptinooooaahteeannllcpaoaocitaveageronaacnaaapaPppoeseenapaonpoiirgai\n",
      "Iter-57 loss: 30.6185\n",
      "cslasoAeaoelilaorooo,hroolrtooloapiomuapaeinaooaloenoleoeottieee oilgrPnpieioonaeopnoplotaiclaretaooi\n",
      "Iter-58 loss: 30.2364\n",
      "caashiitcaoa9otitlaooSoTanaaaiiaritaitiopcoactiloonotaanrnioealobnloo,aaeoheoboidareceoyeonosn8otluon\n",
      "Iter-59 loss: 30.3992\n",
      "caoooMattaialaepaoiatitmhrnioeciooeoelttcio -oaabraIlsteaultsapiiternooooatsoaihoinrlc8ad,netiaeallae\n",
      "Iter-60 loss: 30.3623\n",
      "cetioceniadlanobiecnaierloltohnnasooaalJoCnoalinoaaocienoo4tourabttcaiiiitaloeeGnarhcttrooaaokanotnra\n",
      "Iter-61 loss: 30.8620\n",
      "colnseoitmuintohoailioorohheootecarlrernlowihonltsetrmcpacplhmoailiotoaogoaeytrorelaoaaoloaercaoocuao\n",
      "Iter-62 loss: 30.2832\n",
      "caoopeloa ttnooileapootnoartltoeatttoaoptnrolaa iaaonaooorlc,aetoDaa.iSatcotanectetmoiocatuocrupornoa\n",
      "Iter-63 loss: 30.0662\n",
      "c eooluoldnwtlaanhatatapal\"aaoicpiaaynlicaetnleInoaaernaotnItonatogeroesptcactoilsnhauoeoouepmaoriioa\n",
      "Iter-64 loss: 29.8085\n",
      "cmraotestelcieoialaniropaoonootoyotsaoalnoiottlotntiintporeenlntanoEdcafttonpaoncianeouealpoaelaNbata\n",
      "Iter-65 loss: 28.6190\n",
      "ccipoilennttcmioaooeaiuWarcaolseaaaaiiabanaocitir5iioiieaoaaeoeonhcopoultllpcoiioieoeEeltetnenrhfaipt\n",
      "Iter-66 loss: 29.1424\n",
      "cM oaintocioienrnralpliaisnotxonnapnneitisttnlaatciaactlsetaralaa\"iEooanlelaoooeeoootmutlecnfaaaaaabt\n",
      "Iter-67 loss: 32.2949\n",
      "cAaeotnecoaeJsompocntoelhoetloatlalttraooimapoeolceeueloloaariiWGalicetoiaiJoonltmaaboaiaaltaaearmpoa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68 loss: 29.8081\n",
      "cipoomaheetoflutiiiantlooooctodioocopoiefoa iaapooeoateomsasiaeootuibneloraloroaoiwlpo,oieiptdchrrbko\n",
      "Iter-69 loss: 29.0414\n",
      "coetohclioloaNibamsrtpteroleiooaotoeErsnoeeUlfhllrtoiaiooattoaaixtnaeooaaictapatliaromiueeeoaoearenad\n",
      "Iter-70 loss: 29.2155\n",
      "cuiiobunoaoatiplalToaepoaeeartaotoxEtalorrflsisyueceoaaatoeiponlnnhllacaholaeiteaatonnonemst8aottocai\n",
      "Iter-71 loss: 29.4198\n",
      "caoiaaaoavkaitonekeTosntlEolmdelelpxebtnttooarleeodaapioimluanmeiWacocaautaicutiinsloteeroalopotrharo\n",
      "Iter-72 loss: 29.3827\n",
      "ciiarncinaositaoniouuoawaltiuotnorbinstraiatorleoneootapRsbahoieetnpruucstrpoobauaaieaaoouetoeieatimo\n",
      "Iter-73 loss: 29.2376\n",
      "cadlotntptiaoeelnJoremhetmeiloirbboiaaloimao aonelahaaeakeasaSDoucnlooecmalalnpiagoapiaoltnilaaoupois\n",
      "Iter-74 loss: 28.6525\n",
      "c oeeuaaaeawiioloaroaneaaanaircnnaeohtnopilicmoeaolkeoapleeantlanibeobioaeaoclaaolnicaiciorealveto\"ae\n",
      "Iter-75 loss: 29.6496\n",
      "cwfluiiooitipniitaatpatnocimikeliooisotioaeoyotieiirtptaroaisiroreaottaoaeaaatorlleaceopsiuotoanttina\n",
      "Iter-76 loss: 31.5726\n",
      "cHcoboooolobaaooomirobaoecrwnnnrtitieoolilewaia aaeaaolenaoaaoteaaayracnailaoaoolahrraeipatia-hooaott\n",
      "Iter-77 loss: 32.8700\n",
      "chtiooo5oieolianntciidlspoctpacohtaiiierxlustactoitiaiaapllnnautaitaiaotensaneaNioaieoeonmaupeihaoSha\n",
      "Iter-78 loss: 29.1070\n",
      "cGliaalii taeltoceeearinohlraiaaitoomaSireettnianloiraiiaaligostahaonaaapaoatiotalbloeatlogtasnoapaao\n",
      "Iter-79 loss: 29.9128\n",
      "cjnalnaonotulahaooonaetlauhro1taeraopeuaalaaiaoaae\"oaaelaeconrlneNLteonoapaeanaooohatplilaltiaHrlinic\n",
      "Iter-80 loss: 29.8942\n",
      "cb aAateiiioosoatriioraoaoanianooaonaanttaioiaentoeaosaalttcueoaititonlneantsnieaianbolaenlutaoasiiou\n",
      "Iter-81 loss: 29.4755\n",
      "c roiolotchoaoriatenNsaolaeoloorioeoenciarlttieoirnounotoooeirsepooolaauwaaeeaaoeJtphtietaoatiaidsoaa\n",
      "Iter-82 loss: 28.5198\n",
      "ci8morapallbibroaororonsoeaooeiaoianaoaoitlhitnocieanaiaooiteaihetntrmoleaatsaitsaorosalaoallaaappooo\n",
      "Iter-83 loss: 32.8520\n",
      "ciaholataopoiraoshilioaaatiiotluareilaacnonoanaforiahoibcueaeperiahat8emiipootloaoEcrornoaoatooalltna\n",
      "Iter-84 loss: 31.3166\n",
      "ce.oalrutnlaraoatplilnonioiatolepooaolcltolitoeaeta anhoutaaonieltEdponifoeleaouloroaoaoupclooaatltdi\n",
      "Iter-85 loss: 28.0479\n",
      "c aieCoocaloonltytooi9araao-oaominiEnohaaiyuicaieeoaoehiluipaoteoaopatdtcrioltalroiii5taoopanaaietsaa\n",
      "Iter-86 loss: 27.3467\n",
      "cataaotocayaritaooimonrlociutctiolnhaeinpaaeluraaialitoteleeetinaoepaoalegcpartootluonataciiataalaeil\n",
      "Iter-87 loss: 30.3175\n",
      "c.oespiltdoiGeamteloatotaptocpaaiaoaeaniohacoioreoninitootmpiicoirteceooohmlaeleaaeitaoiaaotor.aaatao\n",
      "Iter-88 loss: 28.3609\n",
      "cctooloaathlauoaotoollaupemnidpoiyaoetaloaoaoerttalcnnncoooleaniprtoptlaeiuiiaaoaepvaoifoaompeoaonoat\n",
      "Iter-89 loss: 28.5176\n",
      "ccoarottanlcirlcaptntiaommnauntoaooptraaarleoooepaSoehsaaeroepielllliloiooeorptr'eaiopooanoloinoiEila\n",
      "Iter-90 loss: 28.5301\n",
      "ck)ecmoooatucoepoaatldaeaoaooomiiataaoGoammonecpeowaoe1pretorpllaoooacoeo1eoataprnoecpanoepoatenaoeao\n",
      "Iter-91 loss: 28.2930\n",
      "cyiuoaonialhoioabieilaaeiocaaplcacpoaoaaioootoabttaaeoaoliaealaaaatrioeenltaailyesemooaaenieoonaetato\n",
      "Iter-92 loss: 27.8139\n",
      "cslooattciloilotiptan laitsooaralaoaouaooaauaaonoataranenaauleiplnioaelfoatntatnpiaoraeaiaoinnaeaanti\n",
      "Iter-93 loss: 30.0651\n",
      "c toealortaoaonohaioootoeoienaatnnttpleiloopohoaceaciintaiaponpahlorancraaTnaoalicaoaeoonotarelepouat\n",
      "Iter-94 loss: 25.9750\n",
      "c,ilaiocotoriiapuieloornaitiapituoaoSnaantooemocpolotioooIttaeoaaaharpoaitouipatnaeyoeaonaineenaoroio\n",
      "Iter-95 loss: 29.9006\n",
      "cuieuioeoodyn\"tootaatohtpaoobauntetoaainacatoubaianfntoampcooaioiiaiirooaainwlhemaanlooalaioaoaaUoppe\n",
      "Iter-96 loss: 27.7865\n",
      "chloiiaariotnitnoptad.lcbanooaleaoanamonieaetisaitloipasiaaoerollaaatainoliaioonnonoaatooatooelpnrlty\n",
      "Iter-97 loss: 27.7311\n",
      "ckllniponeomtroecipeauoeasoiuconanauidefleolioaaaacoaniofopitperaanpautlcttahethtploaaatitanpaamiooeo\n",
      "Iter-98 loss: 26.2606\n",
      "cuaIellpsancaaeiiioorttneatbeanoaaeauhorrnaceiannaaneoollioaierooleiearnoaltsaifalioeeeilaaenhapralor\n",
      "Iter-99 loss: 27.6019\n",
      "ceteiamrllolncoattraeiopoaauuua8aoiaioeaoEoleialwucaoTialoiaaiinJmooalbaltaeateooltrhcalxiaacuyenaaen\n",
      "Iter-100 loss: 27.5607\n",
      "cdliapnelfceontoafiaotoanTaeiaonaliioittalopop nasaaorauooaiea0lnatobtliialeymtaaitotilioooionmaonlnu\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKH1BEIQ4QpS+IAhCHPH/kIFftPBd0OcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10bf2e5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 100 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//100 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, p_dropout=p_dropout, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
