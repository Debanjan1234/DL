{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        dX = dout @ W.T # VBP\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            #             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            #             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.2383, acc-0.2300, valid loss-2.2657, acc-0.1696, test loss-2.2644, acc-0.1708\n",
      "Iter-20, train loss-2.2434, acc-0.2400, valid loss-2.2388, acc-0.2356, test loss-2.2373, acc-0.2411\n",
      "Iter-30, train loss-2.2180, acc-0.2800, valid loss-2.2121, acc-0.3100, test loss-2.2103, acc-0.3161\n",
      "Iter-40, train loss-2.1881, acc-0.4200, valid loss-2.1841, acc-0.3760, test loss-2.1821, acc-0.3811\n",
      "Iter-50, train loss-2.1686, acc-0.3700, valid loss-2.1579, acc-0.4120, test loss-2.1556, acc-0.4158\n",
      "Iter-60, train loss-2.1401, acc-0.4000, valid loss-2.1310, acc-0.4368, test loss-2.1286, acc-0.4449\n",
      "Iter-70, train loss-2.0914, acc-0.5100, valid loss-2.1053, acc-0.4644, test loss-2.1027, acc-0.4688\n",
      "Iter-80, train loss-2.0525, acc-0.5300, valid loss-2.0775, acc-0.4826, test loss-2.0746, acc-0.4861\n",
      "Iter-90, train loss-2.0402, acc-0.4800, valid loss-2.0494, acc-0.4980, test loss-2.0464, acc-0.5024\n",
      "Iter-100, train loss-2.0130, acc-0.5500, valid loss-2.0225, acc-0.5140, test loss-2.0194, acc-0.5200\n",
      "Iter-110, train loss-1.9675, acc-0.5100, valid loss-1.9942, acc-0.5210, test loss-1.9910, acc-0.5310\n",
      "Iter-120, train loss-1.9862, acc-0.5100, valid loss-1.9667, acc-0.5352, test loss-1.9637, acc-0.5387\n",
      "Iter-130, train loss-1.9366, acc-0.4900, valid loss-1.9387, acc-0.5410, test loss-1.9355, acc-0.5483\n",
      "Iter-140, train loss-1.9186, acc-0.5800, valid loss-1.9114, acc-0.5548, test loss-1.9086, acc-0.5612\n",
      "Iter-150, train loss-1.9378, acc-0.4600, valid loss-1.8832, acc-0.5578, test loss-1.8805, acc-0.5657\n",
      "Iter-160, train loss-1.8624, acc-0.5300, valid loss-1.8553, acc-0.5662, test loss-1.8528, acc-0.5725\n",
      "Iter-170, train loss-1.8424, acc-0.5900, valid loss-1.8267, acc-0.5694, test loss-1.8245, acc-0.5758\n",
      "Iter-180, train loss-1.7618, acc-0.6200, valid loss-1.7986, acc-0.5732, test loss-1.7964, acc-0.5809\n",
      "Iter-190, train loss-1.7384, acc-0.6400, valid loss-1.7721, acc-0.5794, test loss-1.7701, acc-0.5878\n",
      "Iter-200, train loss-1.7108, acc-0.5900, valid loss-1.7452, acc-0.5874, test loss-1.7436, acc-0.5930\n",
      "Iter-210, train loss-1.7779, acc-0.6000, valid loss-1.7185, acc-0.5894, test loss-1.7169, acc-0.5980\n",
      "Iter-220, train loss-1.6166, acc-0.6800, valid loss-1.6919, acc-0.5978, test loss-1.6905, acc-0.6024\n",
      "Iter-230, train loss-1.6855, acc-0.5700, valid loss-1.6657, acc-0.6010, test loss-1.6645, acc-0.6071\n",
      "Iter-240, train loss-1.5784, acc-0.6900, valid loss-1.6397, acc-0.6070, test loss-1.6387, acc-0.6111\n",
      "Iter-250, train loss-1.5339, acc-0.6800, valid loss-1.6137, acc-0.6106, test loss-1.6129, acc-0.6143\n",
      "Iter-260, train loss-1.5767, acc-0.6400, valid loss-1.5880, acc-0.6194, test loss-1.5879, acc-0.6214\n",
      "Iter-270, train loss-1.5100, acc-0.6600, valid loss-1.5631, acc-0.6264, test loss-1.5634, acc-0.6269\n",
      "Iter-280, train loss-1.5180, acc-0.6100, valid loss-1.5384, acc-0.6382, test loss-1.5393, acc-0.6355\n",
      "Iter-290, train loss-1.5168, acc-0.6400, valid loss-1.5143, acc-0.6428, test loss-1.5157, acc-0.6390\n",
      "Iter-300, train loss-1.5320, acc-0.5900, valid loss-1.4910, acc-0.6548, test loss-1.4929, acc-0.6484\n",
      "Iter-310, train loss-1.5012, acc-0.6700, valid loss-1.4681, acc-0.6618, test loss-1.4703, acc-0.6557\n",
      "Iter-320, train loss-1.4094, acc-0.7100, valid loss-1.4455, acc-0.6700, test loss-1.4480, acc-0.6614\n",
      "Iter-330, train loss-1.4412, acc-0.7000, valid loss-1.4228, acc-0.6780, test loss-1.4258, acc-0.6699\n",
      "Iter-340, train loss-1.3708, acc-0.7100, valid loss-1.4004, acc-0.6864, test loss-1.4038, acc-0.6770\n",
      "Iter-350, train loss-1.3789, acc-0.6600, valid loss-1.3782, acc-0.6906, test loss-1.3820, acc-0.6859\n",
      "Iter-360, train loss-1.3970, acc-0.7000, valid loss-1.3576, acc-0.6982, test loss-1.3613, acc-0.6931\n",
      "Iter-370, train loss-1.3724, acc-0.6600, valid loss-1.3360, acc-0.7086, test loss-1.3403, acc-0.7044\n",
      "Iter-380, train loss-1.3016, acc-0.7100, valid loss-1.3159, acc-0.7132, test loss-1.3204, acc-0.7093\n",
      "Iter-390, train loss-1.2981, acc-0.7200, valid loss-1.2963, acc-0.7194, test loss-1.3013, acc-0.7152\n",
      "Iter-400, train loss-1.3166, acc-0.6600, valid loss-1.2765, acc-0.7306, test loss-1.2819, acc-0.7282\n",
      "Iter-410, train loss-1.2752, acc-0.7400, valid loss-1.2571, acc-0.7344, test loss-1.2625, acc-0.7313\n",
      "Iter-420, train loss-1.2773, acc-0.6900, valid loss-1.2380, acc-0.7392, test loss-1.2440, acc-0.7354\n",
      "Iter-430, train loss-1.2421, acc-0.7600, valid loss-1.2195, acc-0.7452, test loss-1.2258, acc-0.7401\n",
      "Iter-440, train loss-1.2411, acc-0.7200, valid loss-1.2012, acc-0.7506, test loss-1.2080, acc-0.7460\n",
      "Iter-450, train loss-1.1805, acc-0.7800, valid loss-1.1828, acc-0.7564, test loss-1.1901, acc-0.7512\n",
      "Iter-460, train loss-1.2015, acc-0.7900, valid loss-1.1652, acc-0.7606, test loss-1.1727, acc-0.7562\n",
      "Iter-470, train loss-1.0868, acc-0.7900, valid loss-1.1475, acc-0.7644, test loss-1.1555, acc-0.7587\n",
      "Iter-480, train loss-1.0035, acc-0.8800, valid loss-1.1315, acc-0.7662, test loss-1.1398, acc-0.7602\n",
      "Iter-490, train loss-1.1050, acc-0.7700, valid loss-1.1153, acc-0.7690, test loss-1.1238, acc-0.7635\n",
      "Iter-500, train loss-1.0612, acc-0.7800, valid loss-1.0993, acc-0.7736, test loss-1.1083, acc-0.7679\n",
      "Iter-510, train loss-1.1354, acc-0.7400, valid loss-1.0838, acc-0.7754, test loss-1.0929, acc-0.7712\n",
      "Iter-520, train loss-1.0376, acc-0.8500, valid loss-1.0685, acc-0.7794, test loss-1.0782, acc-0.7724\n",
      "Iter-530, train loss-1.1052, acc-0.7900, valid loss-1.0543, acc-0.7814, test loss-1.0640, acc-0.7742\n",
      "Iter-540, train loss-1.1051, acc-0.7300, valid loss-1.0401, acc-0.7844, test loss-1.0500, acc-0.7778\n",
      "Iter-550, train loss-1.1695, acc-0.6900, valid loss-1.0263, acc-0.7892, test loss-1.0366, acc-0.7813\n",
      "Iter-560, train loss-1.0372, acc-0.7700, valid loss-1.0128, acc-0.7926, test loss-1.0234, acc-0.7849\n",
      "Iter-570, train loss-1.0212, acc-0.7800, valid loss-1.0005, acc-0.7938, test loss-1.0112, acc-0.7866\n",
      "Iter-580, train loss-1.0532, acc-0.7200, valid loss-0.9879, acc-0.7968, test loss-0.9984, acc-0.7893\n",
      "Iter-590, train loss-1.0406, acc-0.8100, valid loss-0.9757, acc-0.7986, test loss-0.9863, acc-0.7908\n",
      "Iter-600, train loss-1.1321, acc-0.6900, valid loss-0.9640, acc-0.8014, test loss-0.9747, acc-0.7934\n",
      "Iter-610, train loss-0.9357, acc-0.8400, valid loss-0.9520, acc-0.8036, test loss-0.9630, acc-0.7956\n",
      "Iter-620, train loss-0.9805, acc-0.7900, valid loss-0.9405, acc-0.8056, test loss-0.9521, acc-0.7970\n",
      "Iter-630, train loss-0.9038, acc-0.8100, valid loss-0.9299, acc-0.8082, test loss-0.9414, acc-0.7978\n",
      "Iter-640, train loss-0.9687, acc-0.8000, valid loss-0.9194, acc-0.8092, test loss-0.9310, acc-0.7987\n",
      "Iter-650, train loss-0.8961, acc-0.8400, valid loss-0.9087, acc-0.8116, test loss-0.9200, acc-0.8010\n",
      "Iter-660, train loss-0.8734, acc-0.8500, valid loss-0.8981, acc-0.8142, test loss-0.9095, acc-0.8028\n",
      "Iter-670, train loss-0.9145, acc-0.8300, valid loss-0.8876, acc-0.8164, test loss-0.8992, acc-0.8047\n",
      "Iter-680, train loss-0.9469, acc-0.7700, valid loss-0.8776, acc-0.8188, test loss-0.8894, acc-0.8073\n",
      "Iter-690, train loss-0.8849, acc-0.7900, valid loss-0.8677, acc-0.8208, test loss-0.8795, acc-0.8091\n",
      "Iter-700, train loss-0.7906, acc-0.8400, valid loss-0.8581, acc-0.8220, test loss-0.8702, acc-0.8116\n",
      "Iter-710, train loss-0.9122, acc-0.7900, valid loss-0.8488, acc-0.8230, test loss-0.8609, acc-0.8139\n",
      "Iter-720, train loss-0.8132, acc-0.8100, valid loss-0.8398, acc-0.8256, test loss-0.8523, acc-0.8147\n",
      "Iter-730, train loss-0.8110, acc-0.8400, valid loss-0.8310, acc-0.8260, test loss-0.8437, acc-0.8157\n",
      "Iter-740, train loss-0.7919, acc-0.8100, valid loss-0.8229, acc-0.8276, test loss-0.8355, acc-0.8175\n",
      "Iter-750, train loss-0.8057, acc-0.8000, valid loss-0.8137, acc-0.8290, test loss-0.8267, acc-0.8211\n",
      "Iter-760, train loss-0.8419, acc-0.7700, valid loss-0.8059, acc-0.8316, test loss-0.8186, acc-0.8219\n",
      "Iter-770, train loss-0.7760, acc-0.8700, valid loss-0.7978, acc-0.8328, test loss-0.8108, acc-0.8244\n",
      "Iter-780, train loss-0.8641, acc-0.8000, valid loss-0.7900, acc-0.8326, test loss-0.8033, acc-0.8246\n",
      "Iter-790, train loss-0.9171, acc-0.7900, valid loss-0.7829, acc-0.8352, test loss-0.7956, acc-0.8266\n",
      "Iter-800, train loss-0.7372, acc-0.8800, valid loss-0.7752, acc-0.8356, test loss-0.7881, acc-0.8280\n",
      "Iter-810, train loss-0.8196, acc-0.8100, valid loss-0.7678, acc-0.8362, test loss-0.7807, acc-0.8289\n",
      "Iter-820, train loss-0.9287, acc-0.7600, valid loss-0.7609, acc-0.8388, test loss-0.7736, acc-0.8306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-0.7080, acc-0.8600, valid loss-0.7537, acc-0.8412, test loss-0.7668, acc-0.8319\n",
      "Iter-840, train loss-0.7542, acc-0.8400, valid loss-0.7468, acc-0.8422, test loss-0.7605, acc-0.8331\n",
      "Iter-850, train loss-0.6882, acc-0.8200, valid loss-0.7403, acc-0.8440, test loss-0.7542, acc-0.8333\n",
      "Iter-860, train loss-0.6522, acc-0.8900, valid loss-0.7340, acc-0.8448, test loss-0.7480, acc-0.8349\n",
      "Iter-870, train loss-0.7034, acc-0.8600, valid loss-0.7285, acc-0.8464, test loss-0.7419, acc-0.8356\n",
      "Iter-880, train loss-0.7364, acc-0.8500, valid loss-0.7222, acc-0.8460, test loss-0.7355, acc-0.8371\n",
      "Iter-890, train loss-0.8321, acc-0.7700, valid loss-0.7162, acc-0.8478, test loss-0.7297, acc-0.8369\n",
      "Iter-900, train loss-0.6840, acc-0.8500, valid loss-0.7104, acc-0.8480, test loss-0.7237, acc-0.8388\n",
      "Iter-910, train loss-0.6146, acc-0.8700, valid loss-0.7045, acc-0.8490, test loss-0.7177, acc-0.8397\n",
      "Iter-920, train loss-0.7421, acc-0.7800, valid loss-0.6989, acc-0.8492, test loss-0.7119, acc-0.8406\n",
      "Iter-930, train loss-0.6575, acc-0.8900, valid loss-0.6932, acc-0.8496, test loss-0.7067, acc-0.8409\n",
      "Iter-940, train loss-0.7105, acc-0.8700, valid loss-0.6881, acc-0.8508, test loss-0.7019, acc-0.8410\n",
      "Iter-950, train loss-0.6898, acc-0.8300, valid loss-0.6826, acc-0.8514, test loss-0.6964, acc-0.8419\n",
      "Iter-960, train loss-0.7059, acc-0.7900, valid loss-0.6778, acc-0.8524, test loss-0.6912, acc-0.8429\n",
      "Iter-970, train loss-0.6991, acc-0.8300, valid loss-0.6727, acc-0.8528, test loss-0.6858, acc-0.8450\n",
      "Iter-980, train loss-0.6507, acc-0.8700, valid loss-0.6677, acc-0.8534, test loss-0.6809, acc-0.8451\n",
      "Iter-990, train loss-0.7716, acc-0.8000, valid loss-0.6625, acc-0.8550, test loss-0.6757, acc-0.8463\n",
      "Iter-1000, train loss-0.6722, acc-0.8300, valid loss-0.6575, acc-0.8578, test loss-0.6710, acc-0.8468\n",
      "Iter-1010, train loss-0.7280, acc-0.8200, valid loss-0.6528, acc-0.8576, test loss-0.6663, acc-0.8479\n",
      "Iter-1020, train loss-0.7854, acc-0.8000, valid loss-0.6480, acc-0.8574, test loss-0.6616, acc-0.8479\n",
      "Iter-1030, train loss-0.6363, acc-0.9000, valid loss-0.6436, acc-0.8592, test loss-0.6568, acc-0.8480\n",
      "Iter-1040, train loss-0.7246, acc-0.8600, valid loss-0.6390, acc-0.8600, test loss-0.6524, acc-0.8490\n",
      "Iter-1050, train loss-0.6167, acc-0.8800, valid loss-0.6346, acc-0.8602, test loss-0.6476, acc-0.8487\n",
      "Iter-1060, train loss-0.5266, acc-0.9100, valid loss-0.6302, acc-0.8614, test loss-0.6434, acc-0.8490\n",
      "Iter-1070, train loss-0.6853, acc-0.8400, valid loss-0.6263, acc-0.8626, test loss-0.6392, acc-0.8522\n",
      "Iter-1080, train loss-0.6598, acc-0.8000, valid loss-0.6225, acc-0.8626, test loss-0.6349, acc-0.8517\n",
      "Iter-1090, train loss-0.6362, acc-0.8600, valid loss-0.6184, acc-0.8638, test loss-0.6309, acc-0.8530\n",
      "Iter-1100, train loss-0.6060, acc-0.8800, valid loss-0.6146, acc-0.8630, test loss-0.6273, acc-0.8542\n",
      "Iter-1110, train loss-0.6602, acc-0.8400, valid loss-0.6104, acc-0.8654, test loss-0.6236, acc-0.8544\n",
      "Iter-1120, train loss-0.5926, acc-0.8900, valid loss-0.6068, acc-0.8672, test loss-0.6198, acc-0.8552\n",
      "Iter-1130, train loss-0.6176, acc-0.8700, valid loss-0.6031, acc-0.8668, test loss-0.6159, acc-0.8561\n",
      "Iter-1140, train loss-0.6551, acc-0.9100, valid loss-0.5995, acc-0.8682, test loss-0.6126, acc-0.8560\n",
      "Iter-1150, train loss-0.6360, acc-0.8700, valid loss-0.5957, acc-0.8674, test loss-0.6086, acc-0.8564\n",
      "Iter-1160, train loss-0.4864, acc-0.9100, valid loss-0.5918, acc-0.8680, test loss-0.6047, acc-0.8578\n",
      "Iter-1170, train loss-0.6899, acc-0.8600, valid loss-0.5883, acc-0.8688, test loss-0.6013, acc-0.8580\n",
      "Iter-1180, train loss-0.5806, acc-0.8900, valid loss-0.5847, acc-0.8686, test loss-0.5979, acc-0.8577\n",
      "Iter-1190, train loss-0.5976, acc-0.8500, valid loss-0.5818, acc-0.8680, test loss-0.5948, acc-0.8591\n",
      "Iter-1200, train loss-0.5558, acc-0.8800, valid loss-0.5787, acc-0.8684, test loss-0.5918, acc-0.8595\n",
      "Iter-1210, train loss-0.7378, acc-0.8400, valid loss-0.5756, acc-0.8688, test loss-0.5888, acc-0.8610\n",
      "Iter-1220, train loss-0.5969, acc-0.8800, valid loss-0.5724, acc-0.8696, test loss-0.5855, acc-0.8614\n",
      "Iter-1230, train loss-0.6614, acc-0.8300, valid loss-0.5693, acc-0.8698, test loss-0.5822, acc-0.8618\n",
      "Iter-1240, train loss-0.6168, acc-0.8600, valid loss-0.5662, acc-0.8702, test loss-0.5789, acc-0.8625\n",
      "Iter-1250, train loss-0.6356, acc-0.8500, valid loss-0.5628, acc-0.8706, test loss-0.5757, acc-0.8635\n",
      "Iter-1260, train loss-0.5015, acc-0.8600, valid loss-0.5600, acc-0.8714, test loss-0.5729, acc-0.8632\n",
      "Iter-1270, train loss-0.5937, acc-0.8600, valid loss-0.5569, acc-0.8726, test loss-0.5695, acc-0.8637\n",
      "Iter-1280, train loss-0.4992, acc-0.8700, valid loss-0.5537, acc-0.8724, test loss-0.5664, acc-0.8648\n",
      "Iter-1290, train loss-0.6691, acc-0.8000, valid loss-0.5514, acc-0.8736, test loss-0.5634, acc-0.8658\n",
      "Iter-1300, train loss-0.5512, acc-0.8700, valid loss-0.5488, acc-0.8738, test loss-0.5609, acc-0.8666\n",
      "Iter-1310, train loss-0.5070, acc-0.8800, valid loss-0.5462, acc-0.8748, test loss-0.5581, acc-0.8665\n",
      "Iter-1320, train loss-0.5527, acc-0.8800, valid loss-0.5438, acc-0.8744, test loss-0.5556, acc-0.8662\n",
      "Iter-1330, train loss-0.5700, acc-0.8400, valid loss-0.5408, acc-0.8740, test loss-0.5530, acc-0.8669\n",
      "Iter-1340, train loss-0.5642, acc-0.8600, valid loss-0.5385, acc-0.8744, test loss-0.5505, acc-0.8681\n",
      "Iter-1350, train loss-0.3859, acc-0.9500, valid loss-0.5357, acc-0.8750, test loss-0.5477, acc-0.8680\n",
      "Iter-1360, train loss-0.4982, acc-0.8800, valid loss-0.5334, acc-0.8752, test loss-0.5452, acc-0.8681\n",
      "Iter-1370, train loss-0.6712, acc-0.8500, valid loss-0.5311, acc-0.8752, test loss-0.5426, acc-0.8682\n",
      "Iter-1380, train loss-0.6511, acc-0.8500, valid loss-0.5289, acc-0.8768, test loss-0.5400, acc-0.8681\n",
      "Iter-1390, train loss-0.5844, acc-0.8200, valid loss-0.5265, acc-0.8774, test loss-0.5379, acc-0.8685\n",
      "Iter-1400, train loss-0.5051, acc-0.8800, valid loss-0.5241, acc-0.8776, test loss-0.5355, acc-0.8690\n",
      "Iter-1410, train loss-0.5335, acc-0.8800, valid loss-0.5218, acc-0.8780, test loss-0.5330, acc-0.8701\n",
      "Iter-1420, train loss-0.4841, acc-0.8800, valid loss-0.5198, acc-0.8780, test loss-0.5308, acc-0.8704\n",
      "Iter-1430, train loss-0.5148, acc-0.8700, valid loss-0.5172, acc-0.8792, test loss-0.5286, acc-0.8706\n",
      "Iter-1440, train loss-0.6769, acc-0.8600, valid loss-0.5148, acc-0.8788, test loss-0.5264, acc-0.8711\n",
      "Iter-1450, train loss-0.5698, acc-0.8300, valid loss-0.5129, acc-0.8798, test loss-0.5242, acc-0.8715\n",
      "Iter-1460, train loss-0.4897, acc-0.9100, valid loss-0.5109, acc-0.8784, test loss-0.5222, acc-0.8708\n",
      "Iter-1470, train loss-0.4945, acc-0.8700, valid loss-0.5087, acc-0.8788, test loss-0.5199, acc-0.8722\n",
      "Iter-1480, train loss-0.5003, acc-0.9100, valid loss-0.5067, acc-0.8792, test loss-0.5182, acc-0.8734\n",
      "Iter-1490, train loss-0.5878, acc-0.8200, valid loss-0.5045, acc-0.8792, test loss-0.5160, acc-0.8728\n",
      "Iter-1500, train loss-0.5134, acc-0.8700, valid loss-0.5020, acc-0.8810, test loss-0.5140, acc-0.8737\n",
      "Iter-1510, train loss-0.5606, acc-0.8800, valid loss-0.4995, acc-0.8820, test loss-0.5120, acc-0.8740\n",
      "Iter-1520, train loss-0.5253, acc-0.8600, valid loss-0.4977, acc-0.8824, test loss-0.5098, acc-0.8746\n",
      "Iter-1530, train loss-0.5330, acc-0.8400, valid loss-0.4962, acc-0.8828, test loss-0.5079, acc-0.8747\n",
      "Iter-1540, train loss-0.4187, acc-0.9400, valid loss-0.4941, acc-0.8824, test loss-0.5061, acc-0.8746\n",
      "Iter-1550, train loss-0.4515, acc-0.9000, valid loss-0.4922, acc-0.8814, test loss-0.5040, acc-0.8752\n",
      "Iter-1560, train loss-0.4619, acc-0.9000, valid loss-0.4907, acc-0.8806, test loss-0.5021, acc-0.8751\n",
      "Iter-1570, train loss-0.3615, acc-0.9500, valid loss-0.4887, acc-0.8812, test loss-0.5003, acc-0.8756\n",
      "Iter-1580, train loss-0.4045, acc-0.9200, valid loss-0.4865, acc-0.8822, test loss-0.4986, acc-0.8751\n",
      "Iter-1590, train loss-0.4163, acc-0.9000, valid loss-0.4843, acc-0.8834, test loss-0.4969, acc-0.8757\n",
      "Iter-1600, train loss-0.3845, acc-0.9200, valid loss-0.4824, acc-0.8836, test loss-0.4950, acc-0.8753\n",
      "Iter-1610, train loss-0.4481, acc-0.8800, valid loss-0.4808, acc-0.8832, test loss-0.4930, acc-0.8767\n",
      "Iter-1620, train loss-0.6215, acc-0.8200, valid loss-0.4792, acc-0.8832, test loss-0.4914, acc-0.8763\n",
      "Iter-1630, train loss-0.4393, acc-0.8900, valid loss-0.4772, acc-0.8838, test loss-0.4896, acc-0.8765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.3984, acc-0.9300, valid loss-0.4754, acc-0.8834, test loss-0.4877, acc-0.8772\n",
      "Iter-1650, train loss-0.5143, acc-0.8800, valid loss-0.4733, acc-0.8842, test loss-0.4861, acc-0.8760\n",
      "Iter-1660, train loss-0.4441, acc-0.8700, valid loss-0.4720, acc-0.8850, test loss-0.4847, acc-0.8773\n",
      "Iter-1670, train loss-0.4181, acc-0.9200, valid loss-0.4703, acc-0.8856, test loss-0.4829, acc-0.8773\n",
      "Iter-1680, train loss-0.3916, acc-0.9200, valid loss-0.4685, acc-0.8860, test loss-0.4812, acc-0.8776\n",
      "Iter-1690, train loss-0.5326, acc-0.9000, valid loss-0.4672, acc-0.8844, test loss-0.4794, acc-0.8781\n",
      "Iter-1700, train loss-0.5300, acc-0.8400, valid loss-0.4655, acc-0.8854, test loss-0.4779, acc-0.8790\n",
      "Iter-1710, train loss-0.6238, acc-0.8300, valid loss-0.4640, acc-0.8850, test loss-0.4762, acc-0.8788\n",
      "Iter-1720, train loss-0.5380, acc-0.8600, valid loss-0.4625, acc-0.8846, test loss-0.4749, acc-0.8795\n",
      "Iter-1730, train loss-0.5249, acc-0.8400, valid loss-0.4607, acc-0.8846, test loss-0.4733, acc-0.8796\n",
      "Iter-1740, train loss-0.4539, acc-0.9000, valid loss-0.4595, acc-0.8856, test loss-0.4719, acc-0.8802\n",
      "Iter-1750, train loss-0.3866, acc-0.9200, valid loss-0.4583, acc-0.8854, test loss-0.4705, acc-0.8814\n",
      "Iter-1760, train loss-0.5701, acc-0.9100, valid loss-0.4572, acc-0.8854, test loss-0.4692, acc-0.8819\n",
      "Iter-1770, train loss-0.4889, acc-0.8800, valid loss-0.4561, acc-0.8854, test loss-0.4680, acc-0.8812\n",
      "Iter-1780, train loss-0.5685, acc-0.8400, valid loss-0.4548, acc-0.8864, test loss-0.4668, acc-0.8820\n",
      "Iter-1790, train loss-0.3744, acc-0.9300, valid loss-0.4534, acc-0.8864, test loss-0.4653, acc-0.8822\n",
      "Iter-1800, train loss-0.3667, acc-0.9100, valid loss-0.4516, acc-0.8858, test loss-0.4637, acc-0.8814\n",
      "Iter-1810, train loss-0.5362, acc-0.8600, valid loss-0.4504, acc-0.8860, test loss-0.4625, acc-0.8817\n",
      "Iter-1820, train loss-0.4849, acc-0.8800, valid loss-0.4492, acc-0.8860, test loss-0.4613, acc-0.8817\n",
      "Iter-1830, train loss-0.3979, acc-0.9200, valid loss-0.4479, acc-0.8874, test loss-0.4604, acc-0.8823\n",
      "Iter-1840, train loss-0.4957, acc-0.8500, valid loss-0.4467, acc-0.8864, test loss-0.4590, acc-0.8825\n",
      "Iter-1850, train loss-0.4204, acc-0.9000, valid loss-0.4451, acc-0.8872, test loss-0.4575, acc-0.8829\n",
      "Iter-1860, train loss-0.3746, acc-0.9200, valid loss-0.4438, acc-0.8870, test loss-0.4562, acc-0.8827\n",
      "Iter-1870, train loss-0.4990, acc-0.8800, valid loss-0.4425, acc-0.8876, test loss-0.4553, acc-0.8829\n",
      "Iter-1880, train loss-0.4368, acc-0.8900, valid loss-0.4408, acc-0.8884, test loss-0.4535, acc-0.8830\n",
      "Iter-1890, train loss-0.4108, acc-0.9200, valid loss-0.4399, acc-0.8876, test loss-0.4527, acc-0.8830\n",
      "Iter-1900, train loss-0.4129, acc-0.9100, valid loss-0.4390, acc-0.8878, test loss-0.4517, acc-0.8832\n",
      "Iter-1910, train loss-0.3715, acc-0.9400, valid loss-0.4378, acc-0.8880, test loss-0.4500, acc-0.8839\n",
      "Iter-1920, train loss-0.5135, acc-0.8600, valid loss-0.4370, acc-0.8882, test loss-0.4488, acc-0.8856\n",
      "Iter-1930, train loss-0.3647, acc-0.9400, valid loss-0.4367, acc-0.8876, test loss-0.4476, acc-0.8850\n",
      "Iter-1940, train loss-0.5279, acc-0.8600, valid loss-0.4348, acc-0.8884, test loss-0.4463, acc-0.8858\n",
      "Iter-1950, train loss-0.4625, acc-0.8600, valid loss-0.4333, acc-0.8884, test loss-0.4449, acc-0.8862\n",
      "Iter-1960, train loss-0.5540, acc-0.8200, valid loss-0.4322, acc-0.8892, test loss-0.4436, acc-0.8863\n",
      "Iter-1970, train loss-0.6787, acc-0.7700, valid loss-0.4313, acc-0.8908, test loss-0.4423, acc-0.8860\n",
      "Iter-1980, train loss-0.4128, acc-0.8900, valid loss-0.4303, acc-0.8904, test loss-0.4412, acc-0.8868\n",
      "Iter-1990, train loss-0.4041, acc-0.9100, valid loss-0.4296, acc-0.8904, test loss-0.4403, acc-0.8872\n",
      "Iter-2000, train loss-0.3377, acc-0.9400, valid loss-0.4285, acc-0.8898, test loss-0.4396, acc-0.8871\n",
      "Iter-2010, train loss-0.4181, acc-0.9000, valid loss-0.4276, acc-0.8912, test loss-0.4390, acc-0.8877\n",
      "Iter-2020, train loss-0.3812, acc-0.9000, valid loss-0.4263, acc-0.8912, test loss-0.4378, acc-0.8871\n",
      "Iter-2030, train loss-0.4142, acc-0.8900, valid loss-0.4258, acc-0.8914, test loss-0.4369, acc-0.8880\n",
      "Iter-2040, train loss-0.5800, acc-0.8200, valid loss-0.4253, acc-0.8900, test loss-0.4360, acc-0.8886\n",
      "Iter-2050, train loss-0.3472, acc-0.9000, valid loss-0.4239, acc-0.8906, test loss-0.4347, acc-0.8893\n",
      "Iter-2060, train loss-0.4680, acc-0.8700, valid loss-0.4224, acc-0.8912, test loss-0.4333, acc-0.8893\n",
      "Iter-2070, train loss-0.5547, acc-0.8700, valid loss-0.4215, acc-0.8922, test loss-0.4322, acc-0.8902\n",
      "Iter-2080, train loss-0.3879, acc-0.9000, valid loss-0.4204, acc-0.8914, test loss-0.4308, acc-0.8897\n",
      "Iter-2090, train loss-0.4460, acc-0.9000, valid loss-0.4194, acc-0.8910, test loss-0.4298, acc-0.8908\n",
      "Iter-2100, train loss-0.2973, acc-0.9500, valid loss-0.4183, acc-0.8912, test loss-0.4290, acc-0.8906\n",
      "Iter-2110, train loss-0.5018, acc-0.8700, valid loss-0.4174, acc-0.8918, test loss-0.4278, acc-0.8904\n",
      "Iter-2120, train loss-0.4748, acc-0.8800, valid loss-0.4161, acc-0.8918, test loss-0.4265, acc-0.8905\n",
      "Iter-2130, train loss-0.5402, acc-0.8600, valid loss-0.4151, acc-0.8918, test loss-0.4252, acc-0.8916\n",
      "Iter-2140, train loss-0.4025, acc-0.9000, valid loss-0.4136, acc-0.8924, test loss-0.4240, acc-0.8915\n",
      "Iter-2150, train loss-0.6207, acc-0.8600, valid loss-0.4125, acc-0.8924, test loss-0.4231, acc-0.8913\n",
      "Iter-2160, train loss-0.5081, acc-0.8700, valid loss-0.4117, acc-0.8928, test loss-0.4225, acc-0.8916\n",
      "Iter-2170, train loss-0.3002, acc-0.9400, valid loss-0.4113, acc-0.8930, test loss-0.4219, acc-0.8917\n",
      "Iter-2180, train loss-0.4082, acc-0.8900, valid loss-0.4104, acc-0.8938, test loss-0.4206, acc-0.8916\n",
      "Iter-2190, train loss-0.4807, acc-0.9000, valid loss-0.4099, acc-0.8938, test loss-0.4198, acc-0.8920\n",
      "Iter-2200, train loss-0.5085, acc-0.8400, valid loss-0.4090, acc-0.8926, test loss-0.4188, acc-0.8921\n",
      "Iter-2210, train loss-0.4446, acc-0.8600, valid loss-0.4082, acc-0.8932, test loss-0.4181, acc-0.8926\n",
      "Iter-2220, train loss-0.4349, acc-0.9000, valid loss-0.4069, acc-0.8930, test loss-0.4172, acc-0.8919\n",
      "Iter-2230, train loss-0.4670, acc-0.8700, valid loss-0.4057, acc-0.8938, test loss-0.4164, acc-0.8928\n",
      "Iter-2240, train loss-0.3657, acc-0.9300, valid loss-0.4048, acc-0.8952, test loss-0.4156, acc-0.8932\n",
      "Iter-2250, train loss-0.3724, acc-0.9100, valid loss-0.4042, acc-0.8948, test loss-0.4147, acc-0.8925\n",
      "Iter-2260, train loss-0.3713, acc-0.8900, valid loss-0.4032, acc-0.8944, test loss-0.4138, acc-0.8933\n",
      "Iter-2270, train loss-0.3542, acc-0.9100, valid loss-0.4028, acc-0.8946, test loss-0.4130, acc-0.8935\n",
      "Iter-2280, train loss-0.4350, acc-0.8900, valid loss-0.4021, acc-0.8942, test loss-0.4123, acc-0.8931\n",
      "Iter-2290, train loss-0.4133, acc-0.9000, valid loss-0.4013, acc-0.8948, test loss-0.4115, acc-0.8937\n",
      "Iter-2300, train loss-0.3990, acc-0.9000, valid loss-0.4007, acc-0.8948, test loss-0.4109, acc-0.8936\n",
      "Iter-2310, train loss-0.3079, acc-0.9100, valid loss-0.3995, acc-0.8944, test loss-0.4101, acc-0.8938\n",
      "Iter-2320, train loss-0.4000, acc-0.8800, valid loss-0.3987, acc-0.8950, test loss-0.4094, acc-0.8934\n",
      "Iter-2330, train loss-0.5118, acc-0.8400, valid loss-0.3982, acc-0.8952, test loss-0.4088, acc-0.8936\n",
      "Iter-2340, train loss-0.4092, acc-0.9300, valid loss-0.3979, acc-0.8954, test loss-0.4082, acc-0.8938\n",
      "Iter-2350, train loss-0.3480, acc-0.8800, valid loss-0.3970, acc-0.8952, test loss-0.4073, acc-0.8941\n",
      "Iter-2360, train loss-0.3389, acc-0.9200, valid loss-0.3963, acc-0.8952, test loss-0.4066, acc-0.8944\n",
      "Iter-2370, train loss-0.4561, acc-0.8800, valid loss-0.3950, acc-0.8948, test loss-0.4056, acc-0.8952\n",
      "Iter-2380, train loss-0.5199, acc-0.8500, valid loss-0.3941, acc-0.8956, test loss-0.4048, acc-0.8944\n",
      "Iter-2390, train loss-0.4022, acc-0.9000, valid loss-0.3938, acc-0.8962, test loss-0.4038, acc-0.8946\n",
      "Iter-2400, train loss-0.3006, acc-0.9400, valid loss-0.3929, acc-0.8956, test loss-0.4029, acc-0.8951\n",
      "Iter-2410, train loss-0.4356, acc-0.8900, valid loss-0.3923, acc-0.8960, test loss-0.4021, acc-0.8947\n",
      "Iter-2420, train loss-0.4478, acc-0.8900, valid loss-0.3916, acc-0.8956, test loss-0.4011, acc-0.8952\n",
      "Iter-2430, train loss-0.3764, acc-0.9000, valid loss-0.3908, acc-0.8954, test loss-0.4006, acc-0.8946\n",
      "Iter-2440, train loss-0.4680, acc-0.8700, valid loss-0.3897, acc-0.8954, test loss-0.3996, acc-0.8964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.3673, acc-0.9300, valid loss-0.3885, acc-0.8964, test loss-0.3991, acc-0.8958\n",
      "Iter-2460, train loss-0.4703, acc-0.8800, valid loss-0.3879, acc-0.8974, test loss-0.3987, acc-0.8960\n",
      "Iter-2470, train loss-0.5147, acc-0.8000, valid loss-0.3876, acc-0.8972, test loss-0.3977, acc-0.8966\n",
      "Iter-2480, train loss-0.4257, acc-0.8900, valid loss-0.3868, acc-0.8966, test loss-0.3971, acc-0.8973\n",
      "Iter-2490, train loss-0.4531, acc-0.8800, valid loss-0.3856, acc-0.8968, test loss-0.3963, acc-0.8968\n",
      "Iter-2500, train loss-0.4953, acc-0.8700, valid loss-0.3845, acc-0.8970, test loss-0.3957, acc-0.8967\n",
      "Iter-2510, train loss-0.4681, acc-0.8700, valid loss-0.3839, acc-0.8980, test loss-0.3951, acc-0.8969\n",
      "Iter-2520, train loss-0.4991, acc-0.8700, valid loss-0.3828, acc-0.8982, test loss-0.3943, acc-0.8967\n",
      "Iter-2530, train loss-0.4210, acc-0.8700, valid loss-0.3823, acc-0.8988, test loss-0.3935, acc-0.8966\n",
      "Iter-2540, train loss-0.2510, acc-0.9500, valid loss-0.3818, acc-0.8984, test loss-0.3930, acc-0.8970\n",
      "Iter-2550, train loss-0.4128, acc-0.9000, valid loss-0.3814, acc-0.8990, test loss-0.3923, acc-0.8964\n",
      "Iter-2560, train loss-0.4891, acc-0.8700, valid loss-0.3808, acc-0.8982, test loss-0.3919, acc-0.8966\n",
      "Iter-2570, train loss-0.2390, acc-0.9600, valid loss-0.3797, acc-0.8994, test loss-0.3911, acc-0.8967\n",
      "Iter-2580, train loss-0.3635, acc-0.8800, valid loss-0.3791, acc-0.8984, test loss-0.3903, acc-0.8972\n",
      "Iter-2590, train loss-0.3822, acc-0.8600, valid loss-0.3783, acc-0.8988, test loss-0.3896, acc-0.8969\n",
      "Iter-2600, train loss-0.2743, acc-0.9400, valid loss-0.3780, acc-0.8988, test loss-0.3891, acc-0.8973\n",
      "Iter-2610, train loss-0.3317, acc-0.9300, valid loss-0.3772, acc-0.8994, test loss-0.3887, acc-0.8967\n",
      "Iter-2620, train loss-0.3678, acc-0.8800, valid loss-0.3764, acc-0.8994, test loss-0.3881, acc-0.8966\n",
      "Iter-2630, train loss-0.3639, acc-0.9100, valid loss-0.3757, acc-0.8998, test loss-0.3877, acc-0.8978\n",
      "Iter-2640, train loss-0.2833, acc-0.9200, valid loss-0.3754, acc-0.8996, test loss-0.3868, acc-0.8975\n",
      "Iter-2650, train loss-0.3356, acc-0.8900, valid loss-0.3749, acc-0.8992, test loss-0.3865, acc-0.8971\n",
      "Iter-2660, train loss-0.4351, acc-0.8900, valid loss-0.3741, acc-0.9004, test loss-0.3857, acc-0.8966\n",
      "Iter-2670, train loss-0.3842, acc-0.9000, valid loss-0.3734, acc-0.9004, test loss-0.3853, acc-0.8977\n",
      "Iter-2680, train loss-0.4728, acc-0.8600, valid loss-0.3734, acc-0.9002, test loss-0.3852, acc-0.8973\n",
      "Iter-2690, train loss-0.3554, acc-0.9300, valid loss-0.3725, acc-0.9002, test loss-0.3847, acc-0.8967\n",
      "Iter-2700, train loss-0.3394, acc-0.9000, valid loss-0.3717, acc-0.9010, test loss-0.3844, acc-0.8975\n",
      "Iter-2710, train loss-0.4149, acc-0.9100, valid loss-0.3707, acc-0.9006, test loss-0.3838, acc-0.8978\n",
      "Iter-2720, train loss-0.3203, acc-0.9200, valid loss-0.3702, acc-0.9016, test loss-0.3831, acc-0.8980\n",
      "Iter-2730, train loss-0.4116, acc-0.8900, valid loss-0.3699, acc-0.9018, test loss-0.3830, acc-0.8975\n",
      "Iter-2740, train loss-0.3372, acc-0.9000, valid loss-0.3694, acc-0.9026, test loss-0.3824, acc-0.8979\n",
      "Iter-2750, train loss-0.2673, acc-0.9500, valid loss-0.3689, acc-0.9014, test loss-0.3816, acc-0.8977\n",
      "Iter-2760, train loss-0.3914, acc-0.8900, valid loss-0.3685, acc-0.9018, test loss-0.3802, acc-0.8980\n",
      "Iter-2770, train loss-0.2679, acc-0.9400, valid loss-0.3682, acc-0.9010, test loss-0.3796, acc-0.8987\n",
      "Iter-2780, train loss-0.5341, acc-0.8800, valid loss-0.3677, acc-0.9020, test loss-0.3792, acc-0.8981\n",
      "Iter-2790, train loss-0.3534, acc-0.8900, valid loss-0.3667, acc-0.9020, test loss-0.3786, acc-0.8979\n",
      "Iter-2800, train loss-0.3980, acc-0.8700, valid loss-0.3661, acc-0.9020, test loss-0.3780, acc-0.8987\n",
      "Iter-2810, train loss-0.4220, acc-0.8600, valid loss-0.3654, acc-0.9016, test loss-0.3771, acc-0.8989\n",
      "Iter-2820, train loss-0.2535, acc-0.9400, valid loss-0.3645, acc-0.9026, test loss-0.3765, acc-0.8997\n",
      "Iter-2830, train loss-0.3919, acc-0.8900, valid loss-0.3637, acc-0.9030, test loss-0.3759, acc-0.8988\n",
      "Iter-2840, train loss-0.2943, acc-0.9000, valid loss-0.3631, acc-0.9032, test loss-0.3752, acc-0.8991\n",
      "Iter-2850, train loss-0.3846, acc-0.8900, valid loss-0.3624, acc-0.9030, test loss-0.3749, acc-0.9000\n",
      "Iter-2860, train loss-0.2182, acc-0.9700, valid loss-0.3615, acc-0.9032, test loss-0.3744, acc-0.8998\n",
      "Iter-2870, train loss-0.4186, acc-0.9000, valid loss-0.3608, acc-0.9038, test loss-0.3739, acc-0.8997\n",
      "Iter-2880, train loss-0.3751, acc-0.9100, valid loss-0.3598, acc-0.9042, test loss-0.3735, acc-0.8999\n",
      "Iter-2890, train loss-0.4926, acc-0.8600, valid loss-0.3592, acc-0.9048, test loss-0.3731, acc-0.9000\n",
      "Iter-2900, train loss-0.3228, acc-0.9100, valid loss-0.3587, acc-0.9038, test loss-0.3728, acc-0.9005\n",
      "Iter-2910, train loss-0.4760, acc-0.8200, valid loss-0.3585, acc-0.9048, test loss-0.3723, acc-0.9002\n",
      "Iter-2920, train loss-0.3991, acc-0.8800, valid loss-0.3578, acc-0.9052, test loss-0.3718, acc-0.9003\n",
      "Iter-2930, train loss-0.2791, acc-0.9500, valid loss-0.3574, acc-0.9056, test loss-0.3706, acc-0.9008\n",
      "Iter-2940, train loss-0.3654, acc-0.9300, valid loss-0.3566, acc-0.9048, test loss-0.3698, acc-0.9013\n",
      "Iter-2950, train loss-0.4078, acc-0.9200, valid loss-0.3560, acc-0.9046, test loss-0.3694, acc-0.9016\n",
      "Iter-2960, train loss-0.4608, acc-0.8700, valid loss-0.3560, acc-0.9052, test loss-0.3694, acc-0.9014\n",
      "Iter-2970, train loss-0.2365, acc-0.9500, valid loss-0.3557, acc-0.9046, test loss-0.3689, acc-0.9012\n",
      "Iter-2980, train loss-0.4238, acc-0.9000, valid loss-0.3551, acc-0.9048, test loss-0.3687, acc-0.9008\n",
      "Iter-2990, train loss-0.3391, acc-0.9300, valid loss-0.3545, acc-0.9036, test loss-0.3683, acc-0.9013\n",
      "Iter-3000, train loss-0.3618, acc-0.9100, valid loss-0.3538, acc-0.9036, test loss-0.3675, acc-0.9021\n",
      "Iter-3010, train loss-0.4279, acc-0.8900, valid loss-0.3532, acc-0.9038, test loss-0.3667, acc-0.9024\n",
      "Iter-3020, train loss-0.2574, acc-0.9400, valid loss-0.3526, acc-0.9044, test loss-0.3664, acc-0.9027\n",
      "Iter-3030, train loss-0.4326, acc-0.8800, valid loss-0.3525, acc-0.9044, test loss-0.3663, acc-0.9025\n",
      "Iter-3040, train loss-0.4048, acc-0.8900, valid loss-0.3520, acc-0.9050, test loss-0.3656, acc-0.9025\n",
      "Iter-3050, train loss-0.3508, acc-0.9200, valid loss-0.3513, acc-0.9048, test loss-0.3651, acc-0.9026\n",
      "Iter-3060, train loss-0.3427, acc-0.9200, valid loss-0.3514, acc-0.9048, test loss-0.3643, acc-0.9024\n",
      "Iter-3070, train loss-0.3812, acc-0.8900, valid loss-0.3512, acc-0.9046, test loss-0.3635, acc-0.9019\n",
      "Iter-3080, train loss-0.5111, acc-0.8700, valid loss-0.3506, acc-0.9058, test loss-0.3632, acc-0.9022\n",
      "Iter-3090, train loss-0.4551, acc-0.8800, valid loss-0.3496, acc-0.9050, test loss-0.3631, acc-0.9022\n",
      "Iter-3100, train loss-0.3860, acc-0.8800, valid loss-0.3496, acc-0.9056, test loss-0.3622, acc-0.9021\n",
      "Iter-3110, train loss-0.5415, acc-0.8600, valid loss-0.3494, acc-0.9048, test loss-0.3620, acc-0.9023\n",
      "Iter-3120, train loss-0.4116, acc-0.8700, valid loss-0.3487, acc-0.9044, test loss-0.3612, acc-0.9026\n",
      "Iter-3130, train loss-0.2026, acc-0.9600, valid loss-0.3484, acc-0.9042, test loss-0.3609, acc-0.9031\n",
      "Iter-3140, train loss-0.3235, acc-0.9100, valid loss-0.3476, acc-0.9048, test loss-0.3602, acc-0.9028\n",
      "Iter-3150, train loss-0.3133, acc-0.9200, valid loss-0.3472, acc-0.9052, test loss-0.3601, acc-0.9035\n",
      "Iter-3160, train loss-0.3978, acc-0.9100, valid loss-0.3467, acc-0.9052, test loss-0.3597, acc-0.9025\n",
      "Iter-3170, train loss-0.3649, acc-0.9400, valid loss-0.3461, acc-0.9060, test loss-0.3591, acc-0.9030\n",
      "Iter-3180, train loss-0.3729, acc-0.9000, valid loss-0.3456, acc-0.9064, test loss-0.3587, acc-0.9027\n",
      "Iter-3190, train loss-0.3725, acc-0.8600, valid loss-0.3455, acc-0.9060, test loss-0.3587, acc-0.9030\n",
      "Iter-3200, train loss-0.2965, acc-0.9200, valid loss-0.3443, acc-0.9048, test loss-0.3587, acc-0.9028\n",
      "Iter-3210, train loss-0.2494, acc-0.9400, valid loss-0.3440, acc-0.9048, test loss-0.3580, acc-0.9032\n",
      "Iter-3220, train loss-0.3326, acc-0.9000, valid loss-0.3437, acc-0.9058, test loss-0.3581, acc-0.9029\n",
      "Iter-3230, train loss-0.2544, acc-0.9300, valid loss-0.3433, acc-0.9064, test loss-0.3575, acc-0.9032\n",
      "Iter-3240, train loss-0.2986, acc-0.9000, valid loss-0.3429, acc-0.9060, test loss-0.3568, acc-0.9033\n",
      "Iter-3250, train loss-0.2896, acc-0.9000, valid loss-0.3421, acc-0.9062, test loss-0.3561, acc-0.9039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.4636, acc-0.8800, valid loss-0.3421, acc-0.9062, test loss-0.3554, acc-0.9036\n",
      "Iter-3270, train loss-0.4076, acc-0.8900, valid loss-0.3422, acc-0.9056, test loss-0.3550, acc-0.9038\n",
      "Iter-3280, train loss-0.3851, acc-0.9000, valid loss-0.3416, acc-0.9058, test loss-0.3546, acc-0.9042\n",
      "Iter-3290, train loss-0.3357, acc-0.9200, valid loss-0.3415, acc-0.9060, test loss-0.3541, acc-0.9043\n",
      "Iter-3300, train loss-0.5658, acc-0.8300, valid loss-0.3405, acc-0.9060, test loss-0.3534, acc-0.9048\n",
      "Iter-3310, train loss-0.4388, acc-0.9300, valid loss-0.3403, acc-0.9060, test loss-0.3531, acc-0.9048\n",
      "Iter-3320, train loss-0.3354, acc-0.9100, valid loss-0.3399, acc-0.9064, test loss-0.3526, acc-0.9045\n",
      "Iter-3330, train loss-0.2226, acc-0.9500, valid loss-0.3395, acc-0.9066, test loss-0.3518, acc-0.9049\n",
      "Iter-3340, train loss-0.2881, acc-0.9300, valid loss-0.3392, acc-0.9058, test loss-0.3512, acc-0.9049\n",
      "Iter-3350, train loss-0.2247, acc-0.9300, valid loss-0.3386, acc-0.9062, test loss-0.3509, acc-0.9051\n",
      "Iter-3360, train loss-0.3415, acc-0.9200, valid loss-0.3380, acc-0.9066, test loss-0.3506, acc-0.9049\n",
      "Iter-3370, train loss-0.2955, acc-0.9300, valid loss-0.3382, acc-0.9062, test loss-0.3510, acc-0.9040\n",
      "Iter-3380, train loss-0.2767, acc-0.9600, valid loss-0.3378, acc-0.9064, test loss-0.3504, acc-0.9046\n",
      "Iter-3390, train loss-0.2492, acc-0.9300, valid loss-0.3373, acc-0.9068, test loss-0.3500, acc-0.9047\n",
      "Iter-3400, train loss-0.3790, acc-0.9000, valid loss-0.3366, acc-0.9072, test loss-0.3493, acc-0.9050\n",
      "Iter-3410, train loss-0.3008, acc-0.9300, valid loss-0.3360, acc-0.9072, test loss-0.3488, acc-0.9050\n",
      "Iter-3420, train loss-0.3275, acc-0.9500, valid loss-0.3353, acc-0.9068, test loss-0.3485, acc-0.9052\n",
      "Iter-3430, train loss-0.2905, acc-0.9200, valid loss-0.3351, acc-0.9074, test loss-0.3479, acc-0.9053\n",
      "Iter-3440, train loss-0.3572, acc-0.9000, valid loss-0.3346, acc-0.9078, test loss-0.3478, acc-0.9052\n",
      "Iter-3450, train loss-0.3769, acc-0.9300, valid loss-0.3341, acc-0.9084, test loss-0.3474, acc-0.9053\n",
      "Iter-3460, train loss-0.4327, acc-0.9000, valid loss-0.3338, acc-0.9080, test loss-0.3471, acc-0.9057\n",
      "Iter-3470, train loss-0.4408, acc-0.8300, valid loss-0.3336, acc-0.9084, test loss-0.3470, acc-0.9058\n",
      "Iter-3480, train loss-0.3380, acc-0.9100, valid loss-0.3328, acc-0.9094, test loss-0.3464, acc-0.9049\n",
      "Iter-3490, train loss-0.3609, acc-0.9200, valid loss-0.3323, acc-0.9092, test loss-0.3460, acc-0.9053\n",
      "Iter-3500, train loss-0.2945, acc-0.9200, valid loss-0.3324, acc-0.9084, test loss-0.3460, acc-0.9060\n",
      "Iter-3510, train loss-0.3723, acc-0.9100, valid loss-0.3319, acc-0.9086, test loss-0.3453, acc-0.9056\n",
      "Iter-3520, train loss-0.2684, acc-0.9400, valid loss-0.3317, acc-0.9084, test loss-0.3448, acc-0.9057\n",
      "Iter-3530, train loss-0.2458, acc-0.9500, valid loss-0.3310, acc-0.9098, test loss-0.3444, acc-0.9056\n",
      "Iter-3540, train loss-0.2498, acc-0.9100, valid loss-0.3306, acc-0.9102, test loss-0.3439, acc-0.9060\n",
      "Iter-3550, train loss-0.4286, acc-0.8900, valid loss-0.3304, acc-0.9098, test loss-0.3434, acc-0.9058\n",
      "Iter-3560, train loss-0.3150, acc-0.8600, valid loss-0.3297, acc-0.9090, test loss-0.3431, acc-0.9052\n",
      "Iter-3570, train loss-0.2311, acc-0.9300, valid loss-0.3293, acc-0.9100, test loss-0.3427, acc-0.9050\n",
      "Iter-3580, train loss-0.2500, acc-0.9200, valid loss-0.3293, acc-0.9096, test loss-0.3422, acc-0.9051\n",
      "Iter-3590, train loss-0.3716, acc-0.9300, valid loss-0.3287, acc-0.9100, test loss-0.3417, acc-0.9057\n",
      "Iter-3600, train loss-0.3053, acc-0.9200, valid loss-0.3283, acc-0.9098, test loss-0.3416, acc-0.9050\n",
      "Iter-3610, train loss-0.3622, acc-0.8900, valid loss-0.3281, acc-0.9092, test loss-0.3411, acc-0.9051\n",
      "Iter-3620, train loss-0.2706, acc-0.9100, valid loss-0.3278, acc-0.9092, test loss-0.3409, acc-0.9053\n",
      "Iter-3630, train loss-0.3063, acc-0.9000, valid loss-0.3272, acc-0.9104, test loss-0.3405, acc-0.9056\n",
      "Iter-3640, train loss-0.3603, acc-0.8900, valid loss-0.3268, acc-0.9100, test loss-0.3398, acc-0.9056\n",
      "Iter-3650, train loss-0.3892, acc-0.9100, valid loss-0.3262, acc-0.9094, test loss-0.3395, acc-0.9053\n",
      "Iter-3660, train loss-0.2255, acc-0.9400, valid loss-0.3260, acc-0.9086, test loss-0.3393, acc-0.9056\n",
      "Iter-3670, train loss-0.4754, acc-0.8600, valid loss-0.3257, acc-0.9092, test loss-0.3393, acc-0.9052\n",
      "Iter-3680, train loss-0.3950, acc-0.8800, valid loss-0.3254, acc-0.9088, test loss-0.3388, acc-0.9056\n",
      "Iter-3690, train loss-0.2795, acc-0.9300, valid loss-0.3251, acc-0.9098, test loss-0.3388, acc-0.9059\n",
      "Iter-3700, train loss-0.3310, acc-0.9200, valid loss-0.3251, acc-0.9102, test loss-0.3383, acc-0.9059\n",
      "Iter-3710, train loss-0.2466, acc-0.9500, valid loss-0.3257, acc-0.9100, test loss-0.3381, acc-0.9059\n",
      "Iter-3720, train loss-0.3426, acc-0.8900, valid loss-0.3253, acc-0.9106, test loss-0.3380, acc-0.9058\n",
      "Iter-3730, train loss-0.3268, acc-0.9200, valid loss-0.3248, acc-0.9096, test loss-0.3371, acc-0.9052\n",
      "Iter-3740, train loss-0.4010, acc-0.8900, valid loss-0.3246, acc-0.9100, test loss-0.3368, acc-0.9058\n",
      "Iter-3750, train loss-0.2896, acc-0.8900, valid loss-0.3249, acc-0.9106, test loss-0.3368, acc-0.9059\n",
      "Iter-3760, train loss-0.3981, acc-0.8700, valid loss-0.3246, acc-0.9118, test loss-0.3368, acc-0.9062\n",
      "Iter-3770, train loss-0.3242, acc-0.9000, valid loss-0.3238, acc-0.9120, test loss-0.3364, acc-0.9066\n",
      "Iter-3780, train loss-0.2781, acc-0.9400, valid loss-0.3232, acc-0.9122, test loss-0.3360, acc-0.9061\n",
      "Iter-3790, train loss-0.3263, acc-0.9300, valid loss-0.3225, acc-0.9114, test loss-0.3357, acc-0.9056\n",
      "Iter-3800, train loss-0.3458, acc-0.9200, valid loss-0.3229, acc-0.9110, test loss-0.3356, acc-0.9060\n",
      "Iter-3810, train loss-0.2829, acc-0.9200, valid loss-0.3219, acc-0.9116, test loss-0.3352, acc-0.9065\n",
      "Iter-3820, train loss-0.3855, acc-0.8900, valid loss-0.3214, acc-0.9120, test loss-0.3353, acc-0.9061\n",
      "Iter-3830, train loss-0.4877, acc-0.8600, valid loss-0.3216, acc-0.9120, test loss-0.3346, acc-0.9063\n",
      "Iter-3840, train loss-0.2881, acc-0.9300, valid loss-0.3215, acc-0.9126, test loss-0.3344, acc-0.9066\n",
      "Iter-3850, train loss-0.3185, acc-0.9100, valid loss-0.3208, acc-0.9132, test loss-0.3342, acc-0.9072\n",
      "Iter-3860, train loss-0.3200, acc-0.9100, valid loss-0.3206, acc-0.9118, test loss-0.3344, acc-0.9058\n",
      "Iter-3870, train loss-0.3690, acc-0.8900, valid loss-0.3202, acc-0.9110, test loss-0.3340, acc-0.9068\n",
      "Iter-3880, train loss-0.2673, acc-0.9200, valid loss-0.3199, acc-0.9114, test loss-0.3337, acc-0.9067\n",
      "Iter-3890, train loss-0.2531, acc-0.9200, valid loss-0.3194, acc-0.9120, test loss-0.3334, acc-0.9071\n",
      "Iter-3900, train loss-0.3692, acc-0.9000, valid loss-0.3187, acc-0.9124, test loss-0.3328, acc-0.9070\n",
      "Iter-3910, train loss-0.5184, acc-0.8800, valid loss-0.3181, acc-0.9126, test loss-0.3324, acc-0.9072\n",
      "Iter-3920, train loss-0.2885, acc-0.9000, valid loss-0.3182, acc-0.9122, test loss-0.3322, acc-0.9078\n",
      "Iter-3930, train loss-0.3885, acc-0.8900, valid loss-0.3173, acc-0.9128, test loss-0.3314, acc-0.9078\n",
      "Iter-3940, train loss-0.2759, acc-0.8800, valid loss-0.3174, acc-0.9126, test loss-0.3312, acc-0.9076\n",
      "Iter-3950, train loss-0.2788, acc-0.9200, valid loss-0.3172, acc-0.9118, test loss-0.3308, acc-0.9069\n",
      "Iter-3960, train loss-0.3099, acc-0.8900, valid loss-0.3161, acc-0.9114, test loss-0.3300, acc-0.9071\n",
      "Iter-3970, train loss-0.4627, acc-0.8900, valid loss-0.3164, acc-0.9124, test loss-0.3301, acc-0.9077\n",
      "Iter-3980, train loss-0.3772, acc-0.9000, valid loss-0.3162, acc-0.9108, test loss-0.3296, acc-0.9070\n",
      "Iter-3990, train loss-0.2333, acc-0.9400, valid loss-0.3162, acc-0.9112, test loss-0.3293, acc-0.9064\n",
      "Iter-4000, train loss-0.3394, acc-0.8900, valid loss-0.3157, acc-0.9118, test loss-0.3292, acc-0.9078\n",
      "Iter-4010, train loss-0.2432, acc-0.9400, valid loss-0.3148, acc-0.9120, test loss-0.3288, acc-0.9081\n",
      "Iter-4020, train loss-0.4593, acc-0.9000, valid loss-0.3143, acc-0.9110, test loss-0.3275, acc-0.9077\n",
      "Iter-4030, train loss-0.2465, acc-0.9200, valid loss-0.3136, acc-0.9124, test loss-0.3274, acc-0.9084\n",
      "Iter-4040, train loss-0.2982, acc-0.9300, valid loss-0.3134, acc-0.9124, test loss-0.3270, acc-0.9085\n",
      "Iter-4050, train loss-0.3307, acc-0.9200, valid loss-0.3129, acc-0.9134, test loss-0.3269, acc-0.9074\n",
      "Iter-4060, train loss-0.3116, acc-0.9100, valid loss-0.3124, acc-0.9134, test loss-0.3267, acc-0.9080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.2751, acc-0.9500, valid loss-0.3118, acc-0.9134, test loss-0.3264, acc-0.9084\n",
      "Iter-4080, train loss-0.3460, acc-0.8700, valid loss-0.3117, acc-0.9130, test loss-0.3262, acc-0.9081\n",
      "Iter-4090, train loss-0.3268, acc-0.9400, valid loss-0.3115, acc-0.9136, test loss-0.3258, acc-0.9083\n",
      "Iter-4100, train loss-0.3712, acc-0.9000, valid loss-0.3116, acc-0.9124, test loss-0.3250, acc-0.9084\n",
      "Iter-4110, train loss-0.4051, acc-0.9100, valid loss-0.3112, acc-0.9134, test loss-0.3247, acc-0.9085\n",
      "Iter-4120, train loss-0.3223, acc-0.8800, valid loss-0.3113, acc-0.9126, test loss-0.3245, acc-0.9078\n",
      "Iter-4130, train loss-0.2753, acc-0.9300, valid loss-0.3113, acc-0.9126, test loss-0.3245, acc-0.9086\n",
      "Iter-4140, train loss-0.2065, acc-0.9700, valid loss-0.3107, acc-0.9126, test loss-0.3244, acc-0.9087\n",
      "Iter-4150, train loss-0.3287, acc-0.8900, valid loss-0.3106, acc-0.9126, test loss-0.3241, acc-0.9095\n",
      "Iter-4160, train loss-0.2931, acc-0.8900, valid loss-0.3101, acc-0.9136, test loss-0.3239, acc-0.9094\n",
      "Iter-4170, train loss-0.3009, acc-0.9200, valid loss-0.3098, acc-0.9138, test loss-0.3234, acc-0.9099\n",
      "Iter-4180, train loss-0.3521, acc-0.8900, valid loss-0.3092, acc-0.9138, test loss-0.3229, acc-0.9095\n",
      "Iter-4190, train loss-0.3013, acc-0.9200, valid loss-0.3089, acc-0.9142, test loss-0.3224, acc-0.9093\n",
      "Iter-4200, train loss-0.3267, acc-0.8900, valid loss-0.3086, acc-0.9142, test loss-0.3220, acc-0.9092\n",
      "Iter-4210, train loss-0.3240, acc-0.9100, valid loss-0.3083, acc-0.9146, test loss-0.3215, acc-0.9095\n",
      "Iter-4220, train loss-0.3089, acc-0.9300, valid loss-0.3074, acc-0.9156, test loss-0.3215, acc-0.9098\n",
      "Iter-4230, train loss-0.3375, acc-0.9200, valid loss-0.3069, acc-0.9156, test loss-0.3213, acc-0.9103\n",
      "Iter-4240, train loss-0.3591, acc-0.8500, valid loss-0.3067, acc-0.9154, test loss-0.3208, acc-0.9097\n",
      "Iter-4250, train loss-0.3718, acc-0.8800, valid loss-0.3061, acc-0.9160, test loss-0.3206, acc-0.9101\n",
      "Iter-4260, train loss-0.2248, acc-0.9600, valid loss-0.3062, acc-0.9142, test loss-0.3197, acc-0.9101\n",
      "Iter-4270, train loss-0.2708, acc-0.9400, valid loss-0.3059, acc-0.9148, test loss-0.3195, acc-0.9105\n",
      "Iter-4280, train loss-0.2722, acc-0.9200, valid loss-0.3055, acc-0.9158, test loss-0.3193, acc-0.9106\n",
      "Iter-4290, train loss-0.3215, acc-0.9300, valid loss-0.3051, acc-0.9166, test loss-0.3195, acc-0.9113\n",
      "Iter-4300, train loss-0.2378, acc-0.9400, valid loss-0.3044, acc-0.9168, test loss-0.3192, acc-0.9106\n",
      "Iter-4310, train loss-0.2889, acc-0.9000, valid loss-0.3046, acc-0.9166, test loss-0.3186, acc-0.9111\n",
      "Iter-4320, train loss-0.3260, acc-0.8900, valid loss-0.3041, acc-0.9162, test loss-0.3187, acc-0.9110\n",
      "Iter-4330, train loss-0.3721, acc-0.9000, valid loss-0.3040, acc-0.9162, test loss-0.3181, acc-0.9116\n",
      "Iter-4340, train loss-0.3759, acc-0.8900, valid loss-0.3039, acc-0.9168, test loss-0.3176, acc-0.9116\n",
      "Iter-4350, train loss-0.3739, acc-0.8900, valid loss-0.3035, acc-0.9158, test loss-0.3171, acc-0.9111\n",
      "Iter-4360, train loss-0.2775, acc-0.9300, valid loss-0.3036, acc-0.9152, test loss-0.3173, acc-0.9111\n",
      "Iter-4370, train loss-0.3949, acc-0.8400, valid loss-0.3030, acc-0.9158, test loss-0.3171, acc-0.9104\n",
      "Iter-4380, train loss-0.1894, acc-0.9400, valid loss-0.3031, acc-0.9176, test loss-0.3170, acc-0.9116\n",
      "Iter-4390, train loss-0.3306, acc-0.9100, valid loss-0.3031, acc-0.9162, test loss-0.3166, acc-0.9112\n",
      "Iter-4400, train loss-0.2698, acc-0.9200, valid loss-0.3030, acc-0.9160, test loss-0.3160, acc-0.9115\n",
      "Iter-4410, train loss-0.2591, acc-0.9400, valid loss-0.3025, acc-0.9160, test loss-0.3160, acc-0.9117\n",
      "Iter-4420, train loss-0.3959, acc-0.8900, valid loss-0.3023, acc-0.9166, test loss-0.3157, acc-0.9122\n",
      "Iter-4430, train loss-0.3161, acc-0.9300, valid loss-0.3018, acc-0.9168, test loss-0.3153, acc-0.9117\n",
      "Iter-4440, train loss-0.5057, acc-0.8500, valid loss-0.3016, acc-0.9156, test loss-0.3152, acc-0.9114\n",
      "Iter-4450, train loss-0.2268, acc-0.9300, valid loss-0.3010, acc-0.9152, test loss-0.3150, acc-0.9118\n",
      "Iter-4460, train loss-0.2820, acc-0.9200, valid loss-0.3003, acc-0.9162, test loss-0.3150, acc-0.9121\n",
      "Iter-4470, train loss-0.3709, acc-0.9000, valid loss-0.3000, acc-0.9164, test loss-0.3151, acc-0.9117\n",
      "Iter-4480, train loss-0.3185, acc-0.9100, valid loss-0.3001, acc-0.9166, test loss-0.3148, acc-0.9119\n",
      "Iter-4490, train loss-0.3931, acc-0.9100, valid loss-0.2998, acc-0.9166, test loss-0.3145, acc-0.9121\n",
      "Iter-4500, train loss-0.3361, acc-0.9100, valid loss-0.2999, acc-0.9154, test loss-0.3144, acc-0.9118\n",
      "Iter-4510, train loss-0.3457, acc-0.8900, valid loss-0.2996, acc-0.9160, test loss-0.3145, acc-0.9115\n",
      "Iter-4520, train loss-0.2248, acc-0.9500, valid loss-0.3000, acc-0.9152, test loss-0.3145, acc-0.9109\n",
      "Iter-4530, train loss-0.3478, acc-0.9100, valid loss-0.3001, acc-0.9148, test loss-0.3140, acc-0.9115\n",
      "Iter-4540, train loss-0.4601, acc-0.8600, valid loss-0.3000, acc-0.9152, test loss-0.3142, acc-0.9116\n",
      "Iter-4550, train loss-0.5039, acc-0.8700, valid loss-0.2998, acc-0.9154, test loss-0.3138, acc-0.9115\n",
      "Iter-4560, train loss-0.1715, acc-0.9400, valid loss-0.2990, acc-0.9162, test loss-0.3130, acc-0.9120\n",
      "Iter-4570, train loss-0.1682, acc-0.9600, valid loss-0.2983, acc-0.9158, test loss-0.3128, acc-0.9117\n",
      "Iter-4580, train loss-0.3854, acc-0.9000, valid loss-0.2985, acc-0.9156, test loss-0.3125, acc-0.9120\n",
      "Iter-4590, train loss-0.2790, acc-0.9200, valid loss-0.2980, acc-0.9158, test loss-0.3119, acc-0.9127\n",
      "Iter-4600, train loss-0.2110, acc-0.9400, valid loss-0.2978, acc-0.9162, test loss-0.3116, acc-0.9131\n",
      "Iter-4610, train loss-0.2297, acc-0.9500, valid loss-0.2976, acc-0.9166, test loss-0.3115, acc-0.9122\n",
      "Iter-4620, train loss-0.3269, acc-0.9300, valid loss-0.2981, acc-0.9160, test loss-0.3113, acc-0.9120\n",
      "Iter-4630, train loss-0.3546, acc-0.8800, valid loss-0.2974, acc-0.9158, test loss-0.3111, acc-0.9114\n",
      "Iter-4640, train loss-0.3793, acc-0.9100, valid loss-0.2971, acc-0.9166, test loss-0.3107, acc-0.9123\n",
      "Iter-4650, train loss-0.3513, acc-0.9000, valid loss-0.2968, acc-0.9164, test loss-0.3104, acc-0.9124\n",
      "Iter-4660, train loss-0.3556, acc-0.8900, valid loss-0.2965, acc-0.9172, test loss-0.3103, acc-0.9124\n",
      "Iter-4670, train loss-0.2176, acc-0.9500, valid loss-0.2960, acc-0.9182, test loss-0.3103, acc-0.9129\n",
      "Iter-4680, train loss-0.4280, acc-0.8700, valid loss-0.2956, acc-0.9188, test loss-0.3103, acc-0.9135\n",
      "Iter-4690, train loss-0.4528, acc-0.9200, valid loss-0.2955, acc-0.9184, test loss-0.3101, acc-0.9133\n",
      "Iter-4700, train loss-0.3020, acc-0.9100, valid loss-0.2950, acc-0.9176, test loss-0.3094, acc-0.9138\n",
      "Iter-4710, train loss-0.2904, acc-0.9000, valid loss-0.2947, acc-0.9192, test loss-0.3097, acc-0.9135\n",
      "Iter-4720, train loss-0.3810, acc-0.9100, valid loss-0.2946, acc-0.9192, test loss-0.3097, acc-0.9142\n",
      "Iter-4730, train loss-0.3124, acc-0.9200, valid loss-0.2942, acc-0.9198, test loss-0.3091, acc-0.9137\n",
      "Iter-4740, train loss-0.3519, acc-0.8900, valid loss-0.2943, acc-0.9180, test loss-0.3091, acc-0.9130\n",
      "Iter-4750, train loss-0.3480, acc-0.9300, valid loss-0.2944, acc-0.9178, test loss-0.3094, acc-0.9127\n",
      "Iter-4760, train loss-0.4300, acc-0.8800, valid loss-0.2938, acc-0.9178, test loss-0.3090, acc-0.9128\n",
      "Iter-4770, train loss-0.2558, acc-0.9300, valid loss-0.2940, acc-0.9172, test loss-0.3085, acc-0.9136\n",
      "Iter-4780, train loss-0.3610, acc-0.9100, valid loss-0.2941, acc-0.9178, test loss-0.3085, acc-0.9140\n",
      "Iter-4790, train loss-0.4122, acc-0.8800, valid loss-0.2943, acc-0.9172, test loss-0.3086, acc-0.9140\n",
      "Iter-4800, train loss-0.2501, acc-0.9200, valid loss-0.2940, acc-0.9176, test loss-0.3079, acc-0.9144\n",
      "Iter-4810, train loss-0.4651, acc-0.8600, valid loss-0.2935, acc-0.9172, test loss-0.3071, acc-0.9139\n",
      "Iter-4820, train loss-0.3392, acc-0.8900, valid loss-0.2933, acc-0.9176, test loss-0.3065, acc-0.9147\n",
      "Iter-4830, train loss-0.3141, acc-0.9100, valid loss-0.2931, acc-0.9180, test loss-0.3068, acc-0.9146\n",
      "Iter-4840, train loss-0.3091, acc-0.9200, valid loss-0.2928, acc-0.9162, test loss-0.3064, acc-0.9144\n",
      "Iter-4850, train loss-0.2338, acc-0.9400, valid loss-0.2928, acc-0.9170, test loss-0.3062, acc-0.9145\n",
      "Iter-4860, train loss-0.3458, acc-0.8900, valid loss-0.2920, acc-0.9168, test loss-0.3057, acc-0.9146\n",
      "Iter-4870, train loss-0.2264, acc-0.9400, valid loss-0.2916, acc-0.9168, test loss-0.3056, acc-0.9146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.3254, acc-0.9100, valid loss-0.2915, acc-0.9184, test loss-0.3055, acc-0.9143\n",
      "Iter-4890, train loss-0.3014, acc-0.9300, valid loss-0.2912, acc-0.9180, test loss-0.3051, acc-0.9137\n",
      "Iter-4900, train loss-0.1952, acc-0.9300, valid loss-0.2908, acc-0.9178, test loss-0.3047, acc-0.9144\n",
      "Iter-4910, train loss-0.3366, acc-0.9100, valid loss-0.2909, acc-0.9172, test loss-0.3042, acc-0.9144\n",
      "Iter-4920, train loss-0.4117, acc-0.8900, valid loss-0.2907, acc-0.9178, test loss-0.3041, acc-0.9147\n",
      "Iter-4930, train loss-0.3533, acc-0.8700, valid loss-0.2902, acc-0.9186, test loss-0.3040, acc-0.9146\n",
      "Iter-4940, train loss-0.3266, acc-0.9000, valid loss-0.2898, acc-0.9190, test loss-0.3041, acc-0.9146\n",
      "Iter-4950, train loss-0.3384, acc-0.9100, valid loss-0.2899, acc-0.9178, test loss-0.3037, acc-0.9152\n",
      "Iter-4960, train loss-0.1811, acc-0.9700, valid loss-0.2891, acc-0.9182, test loss-0.3031, acc-0.9148\n",
      "Iter-4970, train loss-0.3343, acc-0.9000, valid loss-0.2891, acc-0.9180, test loss-0.3027, acc-0.9148\n",
      "Iter-4980, train loss-0.3649, acc-0.8600, valid loss-0.2892, acc-0.9194, test loss-0.3024, acc-0.9149\n",
      "Iter-4990, train loss-0.3207, acc-0.9300, valid loss-0.2891, acc-0.9198, test loss-0.3024, acc-0.9151\n",
      "Iter-5000, train loss-0.2535, acc-0.9400, valid loss-0.2882, acc-0.9198, test loss-0.3019, acc-0.9150\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 5000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEpE5HwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1193af978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncsQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1199a46a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
