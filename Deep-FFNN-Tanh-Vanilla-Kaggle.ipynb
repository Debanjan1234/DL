{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # MNIST Data\n",
    "# import numpy as np\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import impl.layer as l\n",
    "\n",
    "# # Dataset preparation and pre-processing\n",
    "# mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "# X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "# X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "# X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vocal'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# # Count the freq of words in the text/collection of words\n",
    "# word_counts = Counter(text)\n",
    "# # Having counted the frequency of the words in collection, sort them from most to least/top to bottom/descendng\n",
    "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "# # first enumerating for vocab to int\n",
    "# vocab_to_int = {words: ii for ii, words in enumerate(sorted_vocab)}\n",
    "# # into_to_vocab after enumerating through the sorted vocab\n",
    "# int_to_vocab = {ii: words for words, ii in vocab_to_int.items()}\n",
    "\n",
    "counted_labels = Counter(train_Y)\n",
    "key_to_val = {key: val for val, key in enumerate(counted_labels)}\n",
    "key_to_val['Country']\n",
    "\n",
    "val_to_key = {val: key for val, key in enumerate(counted_labels)}\n",
    "val_to_key[1]\n",
    "\n",
    "# labels = []\n",
    "# for val, key in enumerate(counted_labels):\n",
    "#     print(val, key)\n",
    "#     labels.append(val)\n",
    "  \n",
    "# labels = np.array(labels, dtype=int)\n",
    "# labels.size, np.max(labels), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from impl.layer import onehot\n",
    "\n",
    "# labels_onehot = onehot(labels)\n",
    "\n",
    "# labels, labels_onehot, counted_labels.keys()\n",
    "# key_to_vec = {key: vec for key, vec in zip(counted_labels.keys(), labels_onehot)}\n",
    "# key_to_vec, key_to_vec['Vocal']\n",
    "\n",
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.5897 valid loss: 2.5683, valid accuracy: 0.0931\n",
      "Iter-200 train loss: 2.5537 valid loss: 2.5631, valid accuracy: 0.0969\n",
      "Iter-300 train loss: 2.5550 valid loss: 2.5577, valid accuracy: 0.1000\n",
      "Iter-400 train loss: 2.6012 valid loss: 2.5526, valid accuracy: 0.1046\n",
      "Iter-500 train loss: 2.5234 valid loss: 2.5474, valid accuracy: 0.1100\n",
      "Iter-600 train loss: 2.5492 valid loss: 2.5424, valid accuracy: 0.1115\n",
      "Iter-700 train loss: 2.4950 valid loss: 2.5373, valid accuracy: 0.1146\n",
      "Iter-800 train loss: 2.5337 valid loss: 2.5324, valid accuracy: 0.1177\n",
      "Iter-900 train loss: 2.5303 valid loss: 2.5275, valid accuracy: 0.1223\n",
      "Iter-1000 train loss: 2.5531 valid loss: 2.5225, valid accuracy: 0.1262\n",
      "Iter-1100 train loss: 2.5206 valid loss: 2.5177, valid accuracy: 0.1308\n",
      "Iter-1200 train loss: 2.5923 valid loss: 2.5130, valid accuracy: 0.1331\n",
      "Iter-1300 train loss: 2.5346 valid loss: 2.5082, valid accuracy: 0.1415\n",
      "Iter-1400 train loss: 2.4967 valid loss: 2.5035, valid accuracy: 0.1446\n",
      "Iter-1500 train loss: 2.4298 valid loss: 2.4987, valid accuracy: 0.1485\n",
      "Iter-1600 train loss: 2.4360 valid loss: 2.4940, valid accuracy: 0.1515\n",
      "Iter-1700 train loss: 2.4893 valid loss: 2.4893, valid accuracy: 0.1546\n",
      "Iter-1800 train loss: 2.5513 valid loss: 2.4844, valid accuracy: 0.1562\n",
      "Iter-1900 train loss: 2.5468 valid loss: 2.4798, valid accuracy: 0.1554\n",
      "Iter-2000 train loss: 2.4742 valid loss: 2.4748, valid accuracy: 0.1592\n",
      "Iter-2100 train loss: 2.4533 valid loss: 2.4701, valid accuracy: 0.1623\n",
      "Iter-2200 train loss: 2.5080 valid loss: 2.4654, valid accuracy: 0.1646\n",
      "Iter-2300 train loss: 2.5115 valid loss: 2.4606, valid accuracy: 0.1654\n",
      "Iter-2400 train loss: 2.4099 valid loss: 2.4559, valid accuracy: 0.1677\n",
      "Iter-2500 train loss: 2.4250 valid loss: 2.4512, valid accuracy: 0.1723\n",
      "Iter-2600 train loss: 2.4678 valid loss: 2.4463, valid accuracy: 0.1838\n",
      "Iter-2700 train loss: 2.3974 valid loss: 2.4415, valid accuracy: 0.1885\n",
      "Iter-2800 train loss: 2.5174 valid loss: 2.4369, valid accuracy: 0.1923\n",
      "Iter-2900 train loss: 2.4469 valid loss: 2.4321, valid accuracy: 0.1969\n",
      "Iter-3000 train loss: 2.4384 valid loss: 2.4274, valid accuracy: 0.1992\n",
      "Iter-3100 train loss: 2.4174 valid loss: 2.4227, valid accuracy: 0.1985\n",
      "Iter-3200 train loss: 2.4478 valid loss: 2.4181, valid accuracy: 0.2008\n",
      "Iter-3300 train loss: 2.4395 valid loss: 2.4133, valid accuracy: 0.2008\n",
      "Iter-3400 train loss: 2.3677 valid loss: 2.4086, valid accuracy: 0.2023\n",
      "Iter-3500 train loss: 2.3415 valid loss: 2.4042, valid accuracy: 0.2046\n",
      "Iter-3600 train loss: 2.3909 valid loss: 2.3996, valid accuracy: 0.2031\n",
      "Iter-3700 train loss: 2.3830 valid loss: 2.3954, valid accuracy: 0.2054\n",
      "Iter-3800 train loss: 2.3114 valid loss: 2.3909, valid accuracy: 0.2062\n",
      "Iter-3900 train loss: 2.3746 valid loss: 2.3864, valid accuracy: 0.2054\n",
      "Iter-4000 train loss: 2.3663 valid loss: 2.3820, valid accuracy: 0.2077\n",
      "Iter-4100 train loss: 2.3906 valid loss: 2.3778, valid accuracy: 0.2077\n",
      "Iter-4200 train loss: 2.4065 valid loss: 2.3734, valid accuracy: 0.2077\n",
      "Iter-4300 train loss: 2.3815 valid loss: 2.3693, valid accuracy: 0.2085\n",
      "Iter-4400 train loss: 2.3891 valid loss: 2.3650, valid accuracy: 0.2077\n",
      "Iter-4500 train loss: 2.3914 valid loss: 2.3612, valid accuracy: 0.2069\n",
      "Iter-4600 train loss: 2.4023 valid loss: 2.3572, valid accuracy: 0.2069\n",
      "Iter-4700 train loss: 2.3112 valid loss: 2.3533, valid accuracy: 0.2085\n",
      "Iter-4800 train loss: 2.4394 valid loss: 2.3495, valid accuracy: 0.2115\n",
      "Iter-4900 train loss: 2.3805 valid loss: 2.3457, valid accuracy: 0.2115\n",
      "Iter-5000 train loss: 2.4730 valid loss: 2.3420, valid accuracy: 0.2138\n",
      "Iter-5100 train loss: 2.3600 valid loss: 2.3384, valid accuracy: 0.2131\n",
      "Iter-5200 train loss: 2.2727 valid loss: 2.3347, valid accuracy: 0.2138\n",
      "Iter-5300 train loss: 2.3517 valid loss: 2.3313, valid accuracy: 0.2123\n",
      "Iter-5400 train loss: 2.2179 valid loss: 2.3279, valid accuracy: 0.2146\n",
      "Iter-5500 train loss: 2.3266 valid loss: 2.3246, valid accuracy: 0.2162\n",
      "Iter-5600 train loss: 2.3670 valid loss: 2.3213, valid accuracy: 0.2177\n",
      "Iter-5700 train loss: 2.2803 valid loss: 2.3182, valid accuracy: 0.2185\n",
      "Iter-5800 train loss: 2.2394 valid loss: 2.3151, valid accuracy: 0.2177\n",
      "Iter-5900 train loss: 2.2986 valid loss: 2.3122, valid accuracy: 0.2185\n",
      "Iter-6000 train loss: 2.4055 valid loss: 2.3094, valid accuracy: 0.2200\n",
      "Iter-6100 train loss: 2.2670 valid loss: 2.3063, valid accuracy: 0.2192\n",
      "Iter-6200 train loss: 2.2485 valid loss: 2.3033, valid accuracy: 0.2215\n",
      "Iter-6300 train loss: 2.3663 valid loss: 2.3005, valid accuracy: 0.2208\n",
      "Iter-6400 train loss: 2.2807 valid loss: 2.2977, valid accuracy: 0.2185\n",
      "Iter-6500 train loss: 2.3562 valid loss: 2.2950, valid accuracy: 0.2185\n",
      "Iter-6600 train loss: 2.3759 valid loss: 2.2924, valid accuracy: 0.2215\n",
      "Iter-6700 train loss: 2.4042 valid loss: 2.2901, valid accuracy: 0.2223\n",
      "Iter-6800 train loss: 2.1912 valid loss: 2.2876, valid accuracy: 0.2269\n",
      "Iter-6900 train loss: 2.3305 valid loss: 2.2851, valid accuracy: 0.2300\n",
      "Iter-7000 train loss: 2.2561 valid loss: 2.2826, valid accuracy: 0.2300\n",
      "Iter-7100 train loss: 2.1896 valid loss: 2.2802, valid accuracy: 0.2292\n",
      "Iter-7200 train loss: 2.2163 valid loss: 2.2778, valid accuracy: 0.2300\n",
      "Iter-7300 train loss: 2.2563 valid loss: 2.2753, valid accuracy: 0.2308\n",
      "Iter-7400 train loss: 2.1577 valid loss: 2.2731, valid accuracy: 0.2308\n",
      "Iter-7500 train loss: 2.2860 valid loss: 2.2709, valid accuracy: 0.2308\n",
      "Iter-7600 train loss: 2.1383 valid loss: 2.2688, valid accuracy: 0.2308\n",
      "Iter-7700 train loss: 2.2132 valid loss: 2.2665, valid accuracy: 0.2315\n",
      "Iter-7800 train loss: 2.3001 valid loss: 2.2645, valid accuracy: 0.2323\n",
      "Iter-7900 train loss: 2.1715 valid loss: 2.2624, valid accuracy: 0.2323\n",
      "Iter-8000 train loss: 2.3156 valid loss: 2.2603, valid accuracy: 0.2331\n",
      "Iter-8100 train loss: 2.2379 valid loss: 2.2584, valid accuracy: 0.2346\n",
      "Iter-8200 train loss: 2.2140 valid loss: 2.2564, valid accuracy: 0.2346\n",
      "Iter-8300 train loss: 2.2059 valid loss: 2.2545, valid accuracy: 0.2362\n",
      "Iter-8400 train loss: 2.4382 valid loss: 2.2527, valid accuracy: 0.2346\n",
      "Iter-8500 train loss: 2.1561 valid loss: 2.2509, valid accuracy: 0.2369\n",
      "Iter-8600 train loss: 2.2229 valid loss: 2.2491, valid accuracy: 0.2369\n",
      "Iter-8700 train loss: 2.1421 valid loss: 2.2475, valid accuracy: 0.2392\n",
      "Iter-8800 train loss: 2.2997 valid loss: 2.2458, valid accuracy: 0.2408\n",
      "Iter-8900 train loss: 2.1429 valid loss: 2.2443, valid accuracy: 0.2400\n",
      "Iter-9000 train loss: 2.2782 valid loss: 2.2426, valid accuracy: 0.2408\n",
      "Iter-9100 train loss: 2.1873 valid loss: 2.2409, valid accuracy: 0.2423\n",
      "Iter-9200 train loss: 2.1320 valid loss: 2.2393, valid accuracy: 0.2446\n",
      "Iter-9300 train loss: 2.3046 valid loss: 2.2377, valid accuracy: 0.2454\n",
      "Iter-9400 train loss: 2.2241 valid loss: 2.2360, valid accuracy: 0.2438\n",
      "Iter-9500 train loss: 2.3835 valid loss: 2.2346, valid accuracy: 0.2446\n",
      "Iter-9600 train loss: 2.3194 valid loss: 2.2331, valid accuracy: 0.2446\n",
      "Iter-9700 train loss: 2.1433 valid loss: 2.2315, valid accuracy: 0.2454\n",
      "Iter-9800 train loss: 2.1587 valid loss: 2.2301, valid accuracy: 0.2446\n",
      "Iter-9900 train loss: 2.2232 valid loss: 2.2285, valid accuracy: 0.2477\n",
      "Iter-10000 train loss: 2.2877 valid loss: 2.2269, valid accuracy: 0.2477\n",
      "Iter-10100 train loss: 2.0622 valid loss: 2.2254, valid accuracy: 0.2485\n",
      "Iter-10200 train loss: 2.3000 valid loss: 2.2240, valid accuracy: 0.2485\n",
      "Iter-10300 train loss: 2.1271 valid loss: 2.2226, valid accuracy: 0.2492\n",
      "Iter-10400 train loss: 2.1262 valid loss: 2.2210, valid accuracy: 0.2508\n",
      "Iter-10500 train loss: 2.1266 valid loss: 2.2197, valid accuracy: 0.2523\n",
      "Iter-10600 train loss: 2.3332 valid loss: 2.2185, valid accuracy: 0.2538\n",
      "Iter-10700 train loss: 2.2516 valid loss: 2.2172, valid accuracy: 0.2546\n",
      "Iter-10800 train loss: 2.2536 valid loss: 2.2160, valid accuracy: 0.2531\n",
      "Iter-10900 train loss: 2.2771 valid loss: 2.2145, valid accuracy: 0.2538\n",
      "Iter-11000 train loss: 2.1114 valid loss: 2.2133, valid accuracy: 0.2546\n",
      "Iter-11100 train loss: 2.1321 valid loss: 2.2119, valid accuracy: 0.2554\n",
      "Iter-11200 train loss: 2.2962 valid loss: 2.2106, valid accuracy: 0.2538\n",
      "Iter-11300 train loss: 2.2714 valid loss: 2.2093, valid accuracy: 0.2531\n",
      "Iter-11400 train loss: 2.2447 valid loss: 2.2082, valid accuracy: 0.2531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.1253 valid loss: 2.2069, valid accuracy: 0.2531\n",
      "Iter-11600 train loss: 2.0155 valid loss: 2.2058, valid accuracy: 0.2562\n",
      "Iter-11700 train loss: 2.1839 valid loss: 2.2046, valid accuracy: 0.2538\n",
      "Iter-11800 train loss: 2.1695 valid loss: 2.2031, valid accuracy: 0.2538\n",
      "Iter-11900 train loss: 2.2686 valid loss: 2.2018, valid accuracy: 0.2538\n",
      "Iter-12000 train loss: 2.3379 valid loss: 2.2006, valid accuracy: 0.2577\n",
      "Iter-12100 train loss: 2.1797 valid loss: 2.1994, valid accuracy: 0.2577\n",
      "Iter-12200 train loss: 2.2479 valid loss: 2.1982, valid accuracy: 0.2562\n",
      "Iter-12300 train loss: 2.2075 valid loss: 2.1970, valid accuracy: 0.2562\n",
      "Iter-12400 train loss: 2.2111 valid loss: 2.1958, valid accuracy: 0.2554\n",
      "Iter-12500 train loss: 2.1747 valid loss: 2.1947, valid accuracy: 0.2562\n",
      "Iter-12600 train loss: 1.9998 valid loss: 2.1934, valid accuracy: 0.2562\n",
      "Iter-12700 train loss: 2.0771 valid loss: 2.1922, valid accuracy: 0.2546\n",
      "Iter-12800 train loss: 2.1354 valid loss: 2.1910, valid accuracy: 0.2546\n",
      "Iter-12900 train loss: 2.1373 valid loss: 2.1898, valid accuracy: 0.2592\n",
      "Iter-13000 train loss: 2.1814 valid loss: 2.1888, valid accuracy: 0.2569\n",
      "Iter-13100 train loss: 2.1427 valid loss: 2.1875, valid accuracy: 0.2592\n",
      "Iter-13200 train loss: 2.3133 valid loss: 2.1865, valid accuracy: 0.2600\n",
      "Iter-13300 train loss: 2.1346 valid loss: 2.1855, valid accuracy: 0.2615\n",
      "Iter-13400 train loss: 2.1377 valid loss: 2.1845, valid accuracy: 0.2600\n",
      "Iter-13500 train loss: 2.0418 valid loss: 2.1836, valid accuracy: 0.2615\n",
      "Iter-13600 train loss: 2.1935 valid loss: 2.1825, valid accuracy: 0.2608\n",
      "Iter-13700 train loss: 2.2289 valid loss: 2.1814, valid accuracy: 0.2608\n",
      "Iter-13800 train loss: 1.9944 valid loss: 2.1803, valid accuracy: 0.2600\n",
      "Iter-13900 train loss: 1.9412 valid loss: 2.1792, valid accuracy: 0.2608\n",
      "Iter-14000 train loss: 2.1709 valid loss: 2.1781, valid accuracy: 0.2615\n",
      "Iter-14100 train loss: 2.1126 valid loss: 2.1770, valid accuracy: 0.2646\n",
      "Iter-14200 train loss: 2.1243 valid loss: 2.1761, valid accuracy: 0.2631\n",
      "Iter-14300 train loss: 1.9456 valid loss: 2.1750, valid accuracy: 0.2646\n",
      "Iter-14400 train loss: 2.1834 valid loss: 2.1743, valid accuracy: 0.2654\n",
      "Iter-14500 train loss: 1.9976 valid loss: 2.1732, valid accuracy: 0.2631\n",
      "Iter-14600 train loss: 2.0554 valid loss: 2.1721, valid accuracy: 0.2685\n",
      "Iter-14700 train loss: 2.0355 valid loss: 2.1712, valid accuracy: 0.2715\n",
      "Iter-14800 train loss: 2.2700 valid loss: 2.1701, valid accuracy: 0.2708\n",
      "Iter-14900 train loss: 2.1451 valid loss: 2.1691, valid accuracy: 0.2731\n",
      "Iter-15000 train loss: 2.0002 valid loss: 2.1683, valid accuracy: 0.2731\n",
      "Iter-15100 train loss: 2.1094 valid loss: 2.1675, valid accuracy: 0.2708\n",
      "Iter-15200 train loss: 2.1505 valid loss: 2.1665, valid accuracy: 0.2731\n",
      "Iter-15300 train loss: 2.1405 valid loss: 2.1656, valid accuracy: 0.2723\n",
      "Iter-15400 train loss: 2.1050 valid loss: 2.1648, valid accuracy: 0.2738\n",
      "Iter-15500 train loss: 1.9722 valid loss: 2.1641, valid accuracy: 0.2708\n",
      "Iter-15600 train loss: 2.2386 valid loss: 2.1632, valid accuracy: 0.2708\n",
      "Iter-15700 train loss: 2.2092 valid loss: 2.1621, valid accuracy: 0.2708\n",
      "Iter-15800 train loss: 2.0779 valid loss: 2.1610, valid accuracy: 0.2715\n",
      "Iter-15900 train loss: 2.2760 valid loss: 2.1600, valid accuracy: 0.2738\n",
      "Iter-16000 train loss: 2.2400 valid loss: 2.1590, valid accuracy: 0.2746\n",
      "Iter-16100 train loss: 2.0808 valid loss: 2.1583, valid accuracy: 0.2762\n",
      "Iter-16200 train loss: 2.1209 valid loss: 2.1574, valid accuracy: 0.2777\n",
      "Iter-16300 train loss: 1.9916 valid loss: 2.1564, valid accuracy: 0.2800\n",
      "Iter-16400 train loss: 2.0529 valid loss: 2.1555, valid accuracy: 0.2808\n",
      "Iter-16500 train loss: 2.0102 valid loss: 2.1545, valid accuracy: 0.2800\n",
      "Iter-16600 train loss: 2.1072 valid loss: 2.1537, valid accuracy: 0.2792\n",
      "Iter-16700 train loss: 2.1665 valid loss: 2.1527, valid accuracy: 0.2808\n",
      "Iter-16800 train loss: 2.1323 valid loss: 2.1519, valid accuracy: 0.2800\n",
      "Iter-16900 train loss: 2.1204 valid loss: 2.1512, valid accuracy: 0.2792\n",
      "Iter-17000 train loss: 2.1185 valid loss: 2.1502, valid accuracy: 0.2808\n",
      "Iter-17100 train loss: 1.9116 valid loss: 2.1496, valid accuracy: 0.2808\n",
      "Iter-17200 train loss: 2.0422 valid loss: 2.1487, valid accuracy: 0.2815\n",
      "Iter-17300 train loss: 2.1269 valid loss: 2.1479, valid accuracy: 0.2815\n",
      "Iter-17400 train loss: 2.2506 valid loss: 2.1470, valid accuracy: 0.2831\n",
      "Iter-17500 train loss: 2.0452 valid loss: 2.1463, valid accuracy: 0.2831\n",
      "Iter-17600 train loss: 1.9433 valid loss: 2.1455, valid accuracy: 0.2815\n",
      "Iter-17700 train loss: 2.1959 valid loss: 2.1447, valid accuracy: 0.2831\n",
      "Iter-17800 train loss: 2.1145 valid loss: 2.1439, valid accuracy: 0.2823\n",
      "Iter-17900 train loss: 1.9968 valid loss: 2.1430, valid accuracy: 0.2823\n",
      "Iter-18000 train loss: 1.9832 valid loss: 2.1425, valid accuracy: 0.2815\n",
      "Iter-18100 train loss: 2.1934 valid loss: 2.1417, valid accuracy: 0.2823\n",
      "Iter-18200 train loss: 2.1272 valid loss: 2.1408, valid accuracy: 0.2823\n",
      "Iter-18300 train loss: 2.0517 valid loss: 2.1400, valid accuracy: 0.2815\n",
      "Iter-18400 train loss: 2.2303 valid loss: 2.1390, valid accuracy: 0.2823\n",
      "Iter-18500 train loss: 2.2045 valid loss: 2.1380, valid accuracy: 0.2831\n",
      "Iter-18600 train loss: 1.9798 valid loss: 2.1374, valid accuracy: 0.2831\n",
      "Iter-18700 train loss: 2.1262 valid loss: 2.1367, valid accuracy: 0.2823\n",
      "Iter-18800 train loss: 2.0786 valid loss: 2.1361, valid accuracy: 0.2831\n",
      "Iter-18900 train loss: 1.9310 valid loss: 2.1354, valid accuracy: 0.2815\n",
      "Iter-19000 train loss: 2.1006 valid loss: 2.1345, valid accuracy: 0.2831\n",
      "Iter-19100 train loss: 2.0615 valid loss: 2.1336, valid accuracy: 0.2831\n",
      "Iter-19200 train loss: 2.0792 valid loss: 2.1329, valid accuracy: 0.2831\n",
      "Iter-19300 train loss: 2.2278 valid loss: 2.1320, valid accuracy: 0.2815\n",
      "Iter-19400 train loss: 2.2081 valid loss: 2.1313, valid accuracy: 0.2831\n",
      "Iter-19500 train loss: 1.9819 valid loss: 2.1306, valid accuracy: 0.2815\n",
      "Iter-19600 train loss: 2.1451 valid loss: 2.1299, valid accuracy: 0.2815\n",
      "Iter-19700 train loss: 2.2572 valid loss: 2.1291, valid accuracy: 0.2854\n",
      "Iter-19800 train loss: 2.0865 valid loss: 2.1285, valid accuracy: 0.2815\n",
      "Iter-19900 train loss: 2.0966 valid loss: 2.1278, valid accuracy: 0.2862\n",
      "Iter-20000 train loss: 2.0211 valid loss: 2.1273, valid accuracy: 0.2877\n",
      "Iter-20100 train loss: 2.0943 valid loss: 2.1267, valid accuracy: 0.2869\n",
      "Iter-20200 train loss: 1.9552 valid loss: 2.1260, valid accuracy: 0.2869\n",
      "Iter-20300 train loss: 2.0579 valid loss: 2.1251, valid accuracy: 0.2854\n",
      "Iter-20400 train loss: 2.1429 valid loss: 2.1242, valid accuracy: 0.2869\n",
      "Iter-20500 train loss: 2.1385 valid loss: 2.1235, valid accuracy: 0.2877\n",
      "Iter-20600 train loss: 2.1752 valid loss: 2.1230, valid accuracy: 0.2885\n",
      "Iter-20700 train loss: 2.1345 valid loss: 2.1222, valid accuracy: 0.2908\n",
      "Iter-20800 train loss: 2.0720 valid loss: 2.1216, valid accuracy: 0.2877\n",
      "Iter-20900 train loss: 1.8717 valid loss: 2.1210, valid accuracy: 0.2877\n",
      "Iter-21000 train loss: 2.0656 valid loss: 2.1204, valid accuracy: 0.2862\n",
      "Iter-21100 train loss: 2.1245 valid loss: 2.1198, valid accuracy: 0.2862\n",
      "Iter-21200 train loss: 1.8350 valid loss: 2.1192, valid accuracy: 0.2869\n",
      "Iter-21300 train loss: 2.1245 valid loss: 2.1187, valid accuracy: 0.2862\n",
      "Iter-21400 train loss: 1.9986 valid loss: 2.1182, valid accuracy: 0.2892\n",
      "Iter-21500 train loss: 1.9749 valid loss: 2.1176, valid accuracy: 0.2892\n",
      "Iter-21600 train loss: 2.0332 valid loss: 2.1169, valid accuracy: 0.2923\n",
      "Iter-21700 train loss: 2.2738 valid loss: 2.1163, valid accuracy: 0.2946\n",
      "Iter-21800 train loss: 1.9929 valid loss: 2.1157, valid accuracy: 0.2954\n",
      "Iter-21900 train loss: 2.2900 valid loss: 2.1153, valid accuracy: 0.2938\n",
      "Iter-22000 train loss: 2.0733 valid loss: 2.1147, valid accuracy: 0.2923\n",
      "Iter-22100 train loss: 2.2255 valid loss: 2.1139, valid accuracy: 0.2931\n",
      "Iter-22200 train loss: 1.9535 valid loss: 2.1133, valid accuracy: 0.2931\n",
      "Iter-22300 train loss: 2.1224 valid loss: 2.1126, valid accuracy: 0.2938\n",
      "Iter-22400 train loss: 2.0682 valid loss: 2.1117, valid accuracy: 0.2962\n",
      "Iter-22500 train loss: 2.0724 valid loss: 2.1113, valid accuracy: 0.2938\n",
      "Iter-22600 train loss: 2.1366 valid loss: 2.1106, valid accuracy: 0.2954\n",
      "Iter-22700 train loss: 2.2686 valid loss: 2.1101, valid accuracy: 0.2977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.2123 valid loss: 2.1095, valid accuracy: 0.2969\n",
      "Iter-22900 train loss: 2.1989 valid loss: 2.1090, valid accuracy: 0.2985\n",
      "Iter-23000 train loss: 2.0893 valid loss: 2.1082, valid accuracy: 0.2992\n",
      "Iter-23100 train loss: 1.9585 valid loss: 2.1074, valid accuracy: 0.3008\n",
      "Iter-23200 train loss: 2.1065 valid loss: 2.1069, valid accuracy: 0.3038\n",
      "Iter-23300 train loss: 2.2066 valid loss: 2.1065, valid accuracy: 0.3031\n",
      "Iter-23400 train loss: 2.0239 valid loss: 2.1058, valid accuracy: 0.3031\n",
      "Iter-23500 train loss: 2.1755 valid loss: 2.1055, valid accuracy: 0.3031\n",
      "Iter-23600 train loss: 2.0116 valid loss: 2.1049, valid accuracy: 0.3008\n",
      "Iter-23700 train loss: 1.9082 valid loss: 2.1046, valid accuracy: 0.2992\n",
      "Iter-23800 train loss: 2.1300 valid loss: 2.1040, valid accuracy: 0.3008\n",
      "Iter-23900 train loss: 2.0557 valid loss: 2.1033, valid accuracy: 0.3054\n",
      "Iter-24000 train loss: 1.9915 valid loss: 2.1027, valid accuracy: 0.3038\n",
      "Iter-24100 train loss: 1.8546 valid loss: 2.1021, valid accuracy: 0.3023\n",
      "Iter-24200 train loss: 1.9310 valid loss: 2.1014, valid accuracy: 0.3000\n",
      "Iter-24300 train loss: 2.0667 valid loss: 2.1008, valid accuracy: 0.3023\n",
      "Iter-24400 train loss: 2.0920 valid loss: 2.1004, valid accuracy: 0.3000\n",
      "Iter-24500 train loss: 2.0836 valid loss: 2.0997, valid accuracy: 0.3023\n",
      "Iter-24600 train loss: 2.1266 valid loss: 2.0992, valid accuracy: 0.3008\n",
      "Iter-24700 train loss: 2.0472 valid loss: 2.0985, valid accuracy: 0.3023\n",
      "Iter-24800 train loss: 2.0390 valid loss: 2.0979, valid accuracy: 0.3023\n",
      "Iter-24900 train loss: 2.3097 valid loss: 2.0972, valid accuracy: 0.3038\n",
      "Iter-25000 train loss: 2.1430 valid loss: 2.0968, valid accuracy: 0.3023\n",
      "Iter-25100 train loss: 1.9964 valid loss: 2.0964, valid accuracy: 0.3054\n",
      "Iter-25200 train loss: 2.0202 valid loss: 2.0957, valid accuracy: 0.3062\n",
      "Iter-25300 train loss: 2.0620 valid loss: 2.0950, valid accuracy: 0.3069\n",
      "Iter-25400 train loss: 1.9887 valid loss: 2.0944, valid accuracy: 0.3054\n",
      "Iter-25500 train loss: 2.2363 valid loss: 2.0941, valid accuracy: 0.3062\n",
      "Iter-25600 train loss: 1.9018 valid loss: 2.0938, valid accuracy: 0.3038\n",
      "Iter-25700 train loss: 1.9028 valid loss: 2.0933, valid accuracy: 0.3054\n",
      "Iter-25800 train loss: 1.9603 valid loss: 2.0926, valid accuracy: 0.3069\n",
      "Iter-25900 train loss: 2.1789 valid loss: 2.0918, valid accuracy: 0.3069\n",
      "Iter-26000 train loss: 1.9669 valid loss: 2.0914, valid accuracy: 0.3069\n",
      "Iter-26100 train loss: 1.9586 valid loss: 2.0912, valid accuracy: 0.3054\n",
      "Iter-26200 train loss: 1.8353 valid loss: 2.0906, valid accuracy: 0.3054\n",
      "Iter-26300 train loss: 1.9579 valid loss: 2.0902, valid accuracy: 0.3062\n",
      "Iter-26400 train loss: 2.1594 valid loss: 2.0896, valid accuracy: 0.3054\n",
      "Iter-26500 train loss: 2.0222 valid loss: 2.0892, valid accuracy: 0.3085\n",
      "Iter-26600 train loss: 1.9188 valid loss: 2.0887, valid accuracy: 0.3092\n",
      "Iter-26700 train loss: 1.8220 valid loss: 2.0880, valid accuracy: 0.3092\n",
      "Iter-26800 train loss: 1.9641 valid loss: 2.0875, valid accuracy: 0.3092\n",
      "Iter-26900 train loss: 1.7907 valid loss: 2.0873, valid accuracy: 0.3054\n",
      "Iter-27000 train loss: 2.0622 valid loss: 2.0869, valid accuracy: 0.3046\n",
      "Iter-27100 train loss: 1.9910 valid loss: 2.0865, valid accuracy: 0.3015\n",
      "Iter-27200 train loss: 2.2265 valid loss: 2.0860, valid accuracy: 0.3015\n",
      "Iter-27300 train loss: 2.2085 valid loss: 2.0854, valid accuracy: 0.3023\n",
      "Iter-27400 train loss: 2.1294 valid loss: 2.0848, valid accuracy: 0.3062\n",
      "Iter-27500 train loss: 2.2200 valid loss: 2.0845, valid accuracy: 0.3038\n",
      "Iter-27600 train loss: 1.9175 valid loss: 2.0839, valid accuracy: 0.3062\n",
      "Iter-27700 train loss: 1.9845 valid loss: 2.0836, valid accuracy: 0.3046\n",
      "Iter-27800 train loss: 2.0966 valid loss: 2.0832, valid accuracy: 0.3038\n",
      "Iter-27900 train loss: 1.9326 valid loss: 2.0829, valid accuracy: 0.3069\n",
      "Iter-28000 train loss: 1.9398 valid loss: 2.0825, valid accuracy: 0.3054\n",
      "Iter-28100 train loss: 2.1888 valid loss: 2.0821, valid accuracy: 0.3062\n",
      "Iter-28200 train loss: 2.0977 valid loss: 2.0819, valid accuracy: 0.3062\n",
      "Iter-28300 train loss: 2.1016 valid loss: 2.0815, valid accuracy: 0.3046\n",
      "Iter-28400 train loss: 1.8846 valid loss: 2.0810, valid accuracy: 0.3069\n",
      "Iter-28500 train loss: 1.9200 valid loss: 2.0807, valid accuracy: 0.3054\n",
      "Iter-28600 train loss: 2.2601 valid loss: 2.0801, valid accuracy: 0.3062\n",
      "Iter-28700 train loss: 2.0499 valid loss: 2.0799, valid accuracy: 0.3069\n",
      "Iter-28800 train loss: 2.0955 valid loss: 2.0791, valid accuracy: 0.3092\n",
      "Iter-28900 train loss: 2.0071 valid loss: 2.0788, valid accuracy: 0.3077\n",
      "Iter-29000 train loss: 2.0119 valid loss: 2.0783, valid accuracy: 0.3085\n",
      "Iter-29100 train loss: 1.9183 valid loss: 2.0781, valid accuracy: 0.3092\n",
      "Iter-29200 train loss: 2.1731 valid loss: 2.0774, valid accuracy: 0.3092\n",
      "Iter-29300 train loss: 2.0725 valid loss: 2.0771, valid accuracy: 0.3108\n",
      "Iter-29400 train loss: 2.0307 valid loss: 2.0770, valid accuracy: 0.3100\n",
      "Iter-29500 train loss: 2.0995 valid loss: 2.0765, valid accuracy: 0.3108\n",
      "Iter-29600 train loss: 1.9451 valid loss: 2.0760, valid accuracy: 0.3069\n",
      "Iter-29700 train loss: 1.9186 valid loss: 2.0754, valid accuracy: 0.3092\n",
      "Iter-29800 train loss: 1.7307 valid loss: 2.0751, valid accuracy: 0.3108\n",
      "Iter-29900 train loss: 1.8980 valid loss: 2.0747, valid accuracy: 0.3108\n",
      "Iter-30000 train loss: 2.1408 valid loss: 2.0744, valid accuracy: 0.3115\n",
      "Iter-30100 train loss: 1.7174 valid loss: 2.0741, valid accuracy: 0.3108\n",
      "Iter-30200 train loss: 1.8641 valid loss: 2.0738, valid accuracy: 0.3115\n",
      "Iter-30300 train loss: 2.0485 valid loss: 2.0735, valid accuracy: 0.3115\n",
      "Iter-30400 train loss: 1.9777 valid loss: 2.0731, valid accuracy: 0.3115\n",
      "Iter-30500 train loss: 2.0823 valid loss: 2.0728, valid accuracy: 0.3108\n",
      "Iter-30600 train loss: 1.9014 valid loss: 2.0724, valid accuracy: 0.3108\n",
      "Iter-30700 train loss: 1.8995 valid loss: 2.0723, valid accuracy: 0.3092\n",
      "Iter-30800 train loss: 2.2324 valid loss: 2.0722, valid accuracy: 0.3092\n",
      "Iter-30900 train loss: 1.9465 valid loss: 2.0716, valid accuracy: 0.3108\n",
      "Iter-31000 train loss: 2.0142 valid loss: 2.0712, valid accuracy: 0.3108\n",
      "Iter-31100 train loss: 1.9193 valid loss: 2.0710, valid accuracy: 0.3108\n",
      "Iter-31200 train loss: 1.8642 valid loss: 2.0707, valid accuracy: 0.3123\n",
      "Iter-31300 train loss: 2.1955 valid loss: 2.0705, valid accuracy: 0.3123\n",
      "Iter-31400 train loss: 2.0314 valid loss: 2.0701, valid accuracy: 0.3138\n",
      "Iter-31500 train loss: 1.9793 valid loss: 2.0698, valid accuracy: 0.3138\n",
      "Iter-31600 train loss: 1.8740 valid loss: 2.0696, valid accuracy: 0.3131\n",
      "Iter-31700 train loss: 1.9723 valid loss: 2.0691, valid accuracy: 0.3131\n",
      "Iter-31800 train loss: 2.1706 valid loss: 2.0690, valid accuracy: 0.3115\n",
      "Iter-31900 train loss: 2.0615 valid loss: 2.0685, valid accuracy: 0.3123\n",
      "Iter-32000 train loss: 1.9646 valid loss: 2.0681, valid accuracy: 0.3131\n",
      "Iter-32100 train loss: 1.9205 valid loss: 2.0676, valid accuracy: 0.3123\n",
      "Iter-32200 train loss: 1.9783 valid loss: 2.0675, valid accuracy: 0.3154\n",
      "Iter-32300 train loss: 2.2014 valid loss: 2.0669, valid accuracy: 0.3138\n",
      "Iter-32400 train loss: 1.8839 valid loss: 2.0665, valid accuracy: 0.3146\n",
      "Iter-32500 train loss: 2.1527 valid loss: 2.0661, valid accuracy: 0.3146\n",
      "Iter-32600 train loss: 1.7809 valid loss: 2.0661, valid accuracy: 0.3154\n",
      "Iter-32700 train loss: 2.0099 valid loss: 2.0658, valid accuracy: 0.3162\n",
      "Iter-32800 train loss: 2.0264 valid loss: 2.0656, valid accuracy: 0.3177\n",
      "Iter-32900 train loss: 1.9861 valid loss: 2.0651, valid accuracy: 0.3169\n",
      "Iter-33000 train loss: 2.1429 valid loss: 2.0650, valid accuracy: 0.3177\n",
      "Iter-33100 train loss: 1.9519 valid loss: 2.0647, valid accuracy: 0.3200\n",
      "Iter-33200 train loss: 2.1464 valid loss: 2.0645, valid accuracy: 0.3169\n",
      "Iter-33300 train loss: 2.0660 valid loss: 2.0642, valid accuracy: 0.3169\n",
      "Iter-33400 train loss: 1.8880 valid loss: 2.0640, valid accuracy: 0.3162\n",
      "Iter-33500 train loss: 2.0110 valid loss: 2.0635, valid accuracy: 0.3162\n",
      "Iter-33600 train loss: 2.1250 valid loss: 2.0630, valid accuracy: 0.3169\n",
      "Iter-33700 train loss: 2.1997 valid loss: 2.0627, valid accuracy: 0.3200\n",
      "Iter-33800 train loss: 2.0313 valid loss: 2.0619, valid accuracy: 0.3192\n",
      "Iter-33900 train loss: 2.0147 valid loss: 2.0619, valid accuracy: 0.3192\n",
      "Iter-34000 train loss: 2.0113 valid loss: 2.0617, valid accuracy: 0.3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 1.8828 valid loss: 2.0612, valid accuracy: 0.3200\n",
      "Iter-34200 train loss: 1.9178 valid loss: 2.0611, valid accuracy: 0.3192\n",
      "Iter-34300 train loss: 2.1090 valid loss: 2.0606, valid accuracy: 0.3208\n",
      "Iter-34400 train loss: 2.0991 valid loss: 2.0602, valid accuracy: 0.3208\n",
      "Iter-34500 train loss: 1.7571 valid loss: 2.0600, valid accuracy: 0.3223\n",
      "Iter-34600 train loss: 1.8803 valid loss: 2.0597, valid accuracy: 0.3192\n",
      "Iter-34700 train loss: 2.0300 valid loss: 2.0595, valid accuracy: 0.3208\n",
      "Iter-34800 train loss: 1.9922 valid loss: 2.0592, valid accuracy: 0.3192\n",
      "Iter-34900 train loss: 2.3007 valid loss: 2.0589, valid accuracy: 0.3185\n",
      "Iter-35000 train loss: 2.0619 valid loss: 2.0585, valid accuracy: 0.3200\n",
      "Iter-35100 train loss: 1.9501 valid loss: 2.0581, valid accuracy: 0.3192\n",
      "Iter-35200 train loss: 2.0452 valid loss: 2.0578, valid accuracy: 0.3215\n",
      "Iter-35300 train loss: 2.0881 valid loss: 2.0576, valid accuracy: 0.3200\n",
      "Iter-35400 train loss: 2.2670 valid loss: 2.0574, valid accuracy: 0.3200\n",
      "Iter-35500 train loss: 2.1207 valid loss: 2.0571, valid accuracy: 0.3192\n",
      "Iter-35600 train loss: 2.1122 valid loss: 2.0568, valid accuracy: 0.3192\n",
      "Iter-35700 train loss: 1.9954 valid loss: 2.0562, valid accuracy: 0.3200\n",
      "Iter-35800 train loss: 1.7580 valid loss: 2.0561, valid accuracy: 0.3200\n",
      "Iter-35900 train loss: 1.8950 valid loss: 2.0559, valid accuracy: 0.3208\n",
      "Iter-36000 train loss: 1.7769 valid loss: 2.0557, valid accuracy: 0.3208\n",
      "Iter-36100 train loss: 1.9873 valid loss: 2.0554, valid accuracy: 0.3192\n",
      "Iter-36200 train loss: 2.0768 valid loss: 2.0551, valid accuracy: 0.3215\n",
      "Iter-36300 train loss: 2.0988 valid loss: 2.0547, valid accuracy: 0.3208\n",
      "Iter-36400 train loss: 2.0140 valid loss: 2.0545, valid accuracy: 0.3215\n",
      "Iter-36500 train loss: 2.0101 valid loss: 2.0546, valid accuracy: 0.3215\n",
      "Iter-36600 train loss: 1.9704 valid loss: 2.0542, valid accuracy: 0.3223\n",
      "Iter-36700 train loss: 2.0751 valid loss: 2.0540, valid accuracy: 0.3215\n",
      "Iter-36800 train loss: 2.0710 valid loss: 2.0539, valid accuracy: 0.3192\n",
      "Iter-36900 train loss: 2.0237 valid loss: 2.0535, valid accuracy: 0.3192\n",
      "Iter-37000 train loss: 1.9697 valid loss: 2.0533, valid accuracy: 0.3185\n",
      "Iter-37100 train loss: 2.1148 valid loss: 2.0532, valid accuracy: 0.3200\n",
      "Iter-37200 train loss: 2.1234 valid loss: 2.0531, valid accuracy: 0.3185\n",
      "Iter-37300 train loss: 1.9922 valid loss: 2.0527, valid accuracy: 0.3192\n",
      "Iter-37400 train loss: 1.8913 valid loss: 2.0524, valid accuracy: 0.3208\n",
      "Iter-37500 train loss: 2.0121 valid loss: 2.0523, valid accuracy: 0.3208\n",
      "Iter-37600 train loss: 2.1339 valid loss: 2.0521, valid accuracy: 0.3223\n",
      "Iter-37700 train loss: 2.2391 valid loss: 2.0517, valid accuracy: 0.3238\n",
      "Iter-37800 train loss: 2.0694 valid loss: 2.0517, valid accuracy: 0.3231\n",
      "Iter-37900 train loss: 1.8052 valid loss: 2.0516, valid accuracy: 0.3231\n",
      "Iter-38000 train loss: 2.2629 valid loss: 2.0513, valid accuracy: 0.3215\n",
      "Iter-38100 train loss: 2.1030 valid loss: 2.0512, valid accuracy: 0.3231\n",
      "Iter-38200 train loss: 2.1072 valid loss: 2.0508, valid accuracy: 0.3238\n",
      "Iter-38300 train loss: 2.2473 valid loss: 2.0505, valid accuracy: 0.3238\n",
      "Iter-38400 train loss: 2.0404 valid loss: 2.0502, valid accuracy: 0.3238\n",
      "Iter-38500 train loss: 2.1465 valid loss: 2.0498, valid accuracy: 0.3238\n",
      "Iter-38600 train loss: 2.0669 valid loss: 2.0498, valid accuracy: 0.3246\n",
      "Iter-38700 train loss: 1.8331 valid loss: 2.0493, valid accuracy: 0.3246\n",
      "Iter-38800 train loss: 1.8345 valid loss: 2.0495, valid accuracy: 0.3238\n",
      "Iter-38900 train loss: 2.0594 valid loss: 2.0493, valid accuracy: 0.3254\n",
      "Iter-39000 train loss: 1.8809 valid loss: 2.0493, valid accuracy: 0.3246\n",
      "Iter-39100 train loss: 1.9629 valid loss: 2.0494, valid accuracy: 0.3231\n",
      "Iter-39200 train loss: 2.0093 valid loss: 2.0492, valid accuracy: 0.3246\n",
      "Iter-39300 train loss: 2.1229 valid loss: 2.0490, valid accuracy: 0.3246\n",
      "Iter-39400 train loss: 2.0368 valid loss: 2.0487, valid accuracy: 0.3262\n",
      "Iter-39500 train loss: 1.7832 valid loss: 2.0483, valid accuracy: 0.3254\n",
      "Iter-39600 train loss: 2.1132 valid loss: 2.0482, valid accuracy: 0.3246\n",
      "Iter-39700 train loss: 2.0138 valid loss: 2.0479, valid accuracy: 0.3254\n",
      "Iter-39800 train loss: 1.9636 valid loss: 2.0478, valid accuracy: 0.3254\n",
      "Iter-39900 train loss: 2.1184 valid loss: 2.0476, valid accuracy: 0.3238\n",
      "Iter-40000 train loss: 1.9701 valid loss: 2.0471, valid accuracy: 0.3238\n",
      "Iter-40100 train loss: 2.0001 valid loss: 2.0469, valid accuracy: 0.3238\n",
      "Iter-40200 train loss: 1.7294 valid loss: 2.0464, valid accuracy: 0.3246\n",
      "Iter-40300 train loss: 2.0547 valid loss: 2.0461, valid accuracy: 0.3277\n",
      "Iter-40400 train loss: 2.0517 valid loss: 2.0459, valid accuracy: 0.3246\n",
      "Iter-40500 train loss: 1.8541 valid loss: 2.0458, valid accuracy: 0.3246\n",
      "Iter-40600 train loss: 2.0346 valid loss: 2.0457, valid accuracy: 0.3262\n",
      "Iter-40700 train loss: 1.8550 valid loss: 2.0455, valid accuracy: 0.3269\n",
      "Iter-40800 train loss: 1.9119 valid loss: 2.0450, valid accuracy: 0.3262\n",
      "Iter-40900 train loss: 1.6881 valid loss: 2.0448, valid accuracy: 0.3285\n",
      "Iter-41000 train loss: 1.9479 valid loss: 2.0449, valid accuracy: 0.3262\n",
      "Iter-41100 train loss: 1.8895 valid loss: 2.0448, valid accuracy: 0.3246\n",
      "Iter-41200 train loss: 2.0488 valid loss: 2.0448, valid accuracy: 0.3238\n",
      "Iter-41300 train loss: 2.0031 valid loss: 2.0447, valid accuracy: 0.3254\n",
      "Iter-41400 train loss: 1.9620 valid loss: 2.0441, valid accuracy: 0.3246\n",
      "Iter-41500 train loss: 1.8653 valid loss: 2.0438, valid accuracy: 0.3223\n",
      "Iter-41600 train loss: 2.1767 valid loss: 2.0436, valid accuracy: 0.3254\n",
      "Iter-41700 train loss: 2.0240 valid loss: 2.0434, valid accuracy: 0.3238\n",
      "Iter-41800 train loss: 1.9558 valid loss: 2.0432, valid accuracy: 0.3238\n",
      "Iter-41900 train loss: 1.8791 valid loss: 2.0430, valid accuracy: 0.3238\n",
      "Iter-42000 train loss: 1.9898 valid loss: 2.0427, valid accuracy: 0.3254\n",
      "Iter-42100 train loss: 2.0536 valid loss: 2.0425, valid accuracy: 0.3231\n",
      "Iter-42200 train loss: 1.8439 valid loss: 2.0426, valid accuracy: 0.3238\n",
      "Iter-42300 train loss: 1.8922 valid loss: 2.0424, valid accuracy: 0.3238\n",
      "Iter-42400 train loss: 1.9331 valid loss: 2.0421, valid accuracy: 0.3246\n",
      "Iter-42500 train loss: 1.7898 valid loss: 2.0418, valid accuracy: 0.3254\n",
      "Iter-42600 train loss: 1.8660 valid loss: 2.0415, valid accuracy: 0.3246\n",
      "Iter-42700 train loss: 1.9964 valid loss: 2.0415, valid accuracy: 0.3238\n",
      "Iter-42800 train loss: 2.0992 valid loss: 2.0413, valid accuracy: 0.3238\n",
      "Iter-42900 train loss: 1.7896 valid loss: 2.0411, valid accuracy: 0.3246\n",
      "Iter-43000 train loss: 2.2393 valid loss: 2.0409, valid accuracy: 0.3231\n",
      "Iter-43100 train loss: 1.9143 valid loss: 2.0408, valid accuracy: 0.3246\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "# y_pred, y_logit = nn.test(X_test)\n",
    "# loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "# acc = np.mean(y_pred == y_test)\n",
    "# print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#     acc.mean(), acc.std(), loss))\n",
    "y_pred, _ = nn.test(X_test)\n",
    "# mplot.imsave(y_pred)\n",
    "# pd.DataFrame.to_csv(y_pred)\n",
    "# y_pred.shape\n",
    "# import numpy\n",
    "# a = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "# numpy.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "np.savetxt(X=y_pred, delimiter=\",\", fname='y_predddddddddddddddddd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
