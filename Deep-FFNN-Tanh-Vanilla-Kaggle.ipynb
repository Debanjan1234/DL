{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # MNIST Data\n",
    "# import numpy as np\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# import impl.layer as l\n",
    "\n",
    "# # Dataset preparation and pre-processing\n",
    "# mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "# X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "# X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "# X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd # to read CSV files (Comma Separated Values)\n",
    "\n",
    "train_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.x.csv')\n",
    "train_y = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/train.y.csv')\n",
    "test_x = pd.read_csv(filepath_or_buffer='data/kaggle-music-genre/test.x.csv')\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X = np.array(train_x)\n",
    "train_Y = np.array(train_y[:]['class_label'])\n",
    "test_X = np.array(test_x)\n",
    "\n",
    "X_train_val = np.array(train_X[:, 1:-1], dtype=float)\n",
    "X_test = np.array(test_X[:, 1:], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vocal'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# # Count the freq of words in the text/collection of words\n",
    "# word_counts = Counter(text)\n",
    "# # Having counted the frequency of the words in collection, sort them from most to least/top to bottom/descendng\n",
    "# sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "# # first enumerating for vocab to int\n",
    "# vocab_to_int = {words: ii for ii, words in enumerate(sorted_vocab)}\n",
    "# # into_to_vocab after enumerating through the sorted vocab\n",
    "# int_to_vocab = {ii: words for words, ii in vocab_to_int.items()}\n",
    "\n",
    "counted_labels = Counter(train_Y)\n",
    "key_to_val = {key: val for val, key in enumerate(counted_labels)}\n",
    "key_to_val['Country']\n",
    "\n",
    "val_to_key = {val: key for val, key in enumerate(counted_labels)}\n",
    "val_to_key[1]\n",
    "\n",
    "# labels = []\n",
    "# for val, key in enumerate(counted_labels):\n",
    "#     print(val, key)\n",
    "#     labels.append(val)\n",
    "  \n",
    "# labels = np.array(labels, dtype=int)\n",
    "# labels.size, np.max(labels), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from impl.layer import onehot\n",
    "\n",
    "# labels_onehot = onehot(labels)\n",
    "\n",
    "# labels, labels_onehot, counted_labels.keys()\n",
    "# key_to_vec = {key: vec for key, vec in zip(counted_labels.keys(), labels_onehot)}\n",
    "# key_to_vec, key_to_vec['Vocal']\n",
    "\n",
    "Y_train_vec = []\n",
    "for each in train_y[:]['class_label']:\n",
    "#     print(each, key_to_val[each])\n",
    "    Y_train_vec.append(key_to_val[each])\n",
    "\n",
    "Y_train_val = np.array(Y_train_vec)\n",
    "Y_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pre-processing: normalizing\n",
    "# def normalize(X):\n",
    "#     # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "#     return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "# X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)\n",
    "\n",
    "# Preprocessing: normalizing the data based on the training set\n",
    "mean = X_train_val.mean(axis=0)\n",
    "std = X_train_val.std(axis=0)\n",
    "\n",
    "X_train_val, X_test = (X_train_val - mean)/ std, (X_test - mean)/ std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700, 26), (1300, 26), (10400, 26), (1300,), (11700,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating validation set: 10% or 1/10 of the training set or whatever dataset with labels/ annotation\n",
    "valid_size = X_train_val.shape[0]//10\n",
    "valid_size\n",
    "X_val = X_train_val[-valid_size:]\n",
    "Y_val = Y_train_val[-valid_size:]\n",
    "X_train = X_train_val[: -valid_size]\n",
    "Y_train = Y_train_val[: -valid_size]\n",
    "X_train_val.shape, \n",
    "X_train.shape, X_val.shape, X_test.shape, Y_val.shape, Y_train.shape \n",
    "# X_train.dtype, X_val.dtype\n",
    "# Y_train.dtype, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # Backprop\n",
    "#         dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches, ys_L = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, ys):\n",
    "        grads, ys_prev = self.grads, self.ys_prev # initialized by Zero in every iteration/epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, ys) # ys[0], ys[1] and ys_prev are used for backprop\n",
    "            self.ys_prev = ys # for next iteration or epoch learning dW and db\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "#         # Test the final model\n",
    "#         y_pred, y_logit = nn.test(X_test)\n",
    "#         loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "#         acc = np.mean(y_pred == y_test)\n",
    "#         print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#             acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11700,), (11700, 26), (1300, 26), (1300,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, X_train.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.5897 valid loss: 2.5683, valid accuracy: 0.0931\n",
      "Iter-200 train loss: 2.5537 valid loss: 2.5631, valid accuracy: 0.0969\n",
      "Iter-300 train loss: 2.5550 valid loss: 2.5577, valid accuracy: 0.1000\n",
      "Iter-400 train loss: 2.6012 valid loss: 2.5526, valid accuracy: 0.1046\n",
      "Iter-500 train loss: 2.5234 valid loss: 2.5474, valid accuracy: 0.1100\n",
      "Iter-600 train loss: 2.5492 valid loss: 2.5424, valid accuracy: 0.1115\n",
      "Iter-700 train loss: 2.4950 valid loss: 2.5373, valid accuracy: 0.1146\n",
      "Iter-800 train loss: 2.5337 valid loss: 2.5324, valid accuracy: 0.1177\n",
      "Iter-900 train loss: 2.5303 valid loss: 2.5275, valid accuracy: 0.1223\n",
      "Iter-1000 train loss: 2.5531 valid loss: 2.5225, valid accuracy: 0.1262\n",
      "Iter-1100 train loss: 2.5206 valid loss: 2.5177, valid accuracy: 0.1308\n",
      "Iter-1200 train loss: 2.5923 valid loss: 2.5130, valid accuracy: 0.1331\n",
      "Iter-1300 train loss: 2.5346 valid loss: 2.5082, valid accuracy: 0.1415\n",
      "Iter-1400 train loss: 2.4967 valid loss: 2.5035, valid accuracy: 0.1446\n",
      "Iter-1500 train loss: 2.4298 valid loss: 2.4987, valid accuracy: 0.1485\n",
      "Iter-1600 train loss: 2.4360 valid loss: 2.4940, valid accuracy: 0.1515\n",
      "Iter-1700 train loss: 2.4893 valid loss: 2.4893, valid accuracy: 0.1546\n",
      "Iter-1800 train loss: 2.5513 valid loss: 2.4844, valid accuracy: 0.1562\n",
      "Iter-1900 train loss: 2.5468 valid loss: 2.4798, valid accuracy: 0.1554\n",
      "Iter-2000 train loss: 2.4742 valid loss: 2.4748, valid accuracy: 0.1592\n",
      "Iter-2100 train loss: 2.4533 valid loss: 2.4701, valid accuracy: 0.1623\n",
      "Iter-2200 train loss: 2.5080 valid loss: 2.4654, valid accuracy: 0.1646\n",
      "Iter-2300 train loss: 2.5115 valid loss: 2.4606, valid accuracy: 0.1654\n",
      "Iter-2400 train loss: 2.4099 valid loss: 2.4559, valid accuracy: 0.1677\n",
      "Iter-2500 train loss: 2.4250 valid loss: 2.4512, valid accuracy: 0.1723\n",
      "Iter-2600 train loss: 2.4678 valid loss: 2.4463, valid accuracy: 0.1838\n",
      "Iter-2700 train loss: 2.3974 valid loss: 2.4415, valid accuracy: 0.1885\n",
      "Iter-2800 train loss: 2.5174 valid loss: 2.4369, valid accuracy: 0.1923\n",
      "Iter-2900 train loss: 2.4469 valid loss: 2.4321, valid accuracy: 0.1969\n",
      "Iter-3000 train loss: 2.4384 valid loss: 2.4274, valid accuracy: 0.1992\n",
      "Iter-3100 train loss: 2.4174 valid loss: 2.4227, valid accuracy: 0.1985\n",
      "Iter-3200 train loss: 2.4478 valid loss: 2.4181, valid accuracy: 0.2008\n",
      "Iter-3300 train loss: 2.4395 valid loss: 2.4133, valid accuracy: 0.2008\n",
      "Iter-3400 train loss: 2.3677 valid loss: 2.4086, valid accuracy: 0.2023\n",
      "Iter-3500 train loss: 2.3415 valid loss: 2.4042, valid accuracy: 0.2046\n",
      "Iter-3600 train loss: 2.3909 valid loss: 2.3996, valid accuracy: 0.2031\n",
      "Iter-3700 train loss: 2.3830 valid loss: 2.3954, valid accuracy: 0.2054\n",
      "Iter-3800 train loss: 2.3114 valid loss: 2.3909, valid accuracy: 0.2062\n",
      "Iter-3900 train loss: 2.3746 valid loss: 2.3864, valid accuracy: 0.2054\n",
      "Iter-4000 train loss: 2.3663 valid loss: 2.3820, valid accuracy: 0.2077\n",
      "Iter-4100 train loss: 2.3906 valid loss: 2.3778, valid accuracy: 0.2077\n",
      "Iter-4200 train loss: 2.4065 valid loss: 2.3734, valid accuracy: 0.2077\n",
      "Iter-4300 train loss: 2.3815 valid loss: 2.3693, valid accuracy: 0.2085\n",
      "Iter-4400 train loss: 2.3891 valid loss: 2.3650, valid accuracy: 0.2077\n",
      "Iter-4500 train loss: 2.3914 valid loss: 2.3612, valid accuracy: 0.2069\n",
      "Iter-4600 train loss: 2.4023 valid loss: 2.3572, valid accuracy: 0.2069\n",
      "Iter-4700 train loss: 2.3112 valid loss: 2.3533, valid accuracy: 0.2085\n",
      "Iter-4800 train loss: 2.4394 valid loss: 2.3495, valid accuracy: 0.2115\n",
      "Iter-4900 train loss: 2.3805 valid loss: 2.3457, valid accuracy: 0.2115\n",
      "Iter-5000 train loss: 2.4730 valid loss: 2.3420, valid accuracy: 0.2138\n",
      "Iter-5100 train loss: 2.3600 valid loss: 2.3384, valid accuracy: 0.2131\n",
      "Iter-5200 train loss: 2.2727 valid loss: 2.3347, valid accuracy: 0.2138\n",
      "Iter-5300 train loss: 2.3517 valid loss: 2.3313, valid accuracy: 0.2123\n",
      "Iter-5400 train loss: 2.2179 valid loss: 2.3279, valid accuracy: 0.2146\n",
      "Iter-5500 train loss: 2.3266 valid loss: 2.3246, valid accuracy: 0.2162\n",
      "Iter-5600 train loss: 2.3670 valid loss: 2.3213, valid accuracy: 0.2177\n",
      "Iter-5700 train loss: 2.2803 valid loss: 2.3182, valid accuracy: 0.2185\n",
      "Iter-5800 train loss: 2.2394 valid loss: 2.3151, valid accuracy: 0.2177\n",
      "Iter-5900 train loss: 2.2986 valid loss: 2.3122, valid accuracy: 0.2185\n",
      "Iter-6000 train loss: 2.4055 valid loss: 2.3094, valid accuracy: 0.2200\n",
      "Iter-6100 train loss: 2.2670 valid loss: 2.3063, valid accuracy: 0.2192\n",
      "Iter-6200 train loss: 2.2485 valid loss: 2.3033, valid accuracy: 0.2215\n",
      "Iter-6300 train loss: 2.3663 valid loss: 2.3005, valid accuracy: 0.2208\n",
      "Iter-6400 train loss: 2.2807 valid loss: 2.2977, valid accuracy: 0.2185\n",
      "Iter-6500 train loss: 2.3562 valid loss: 2.2950, valid accuracy: 0.2185\n",
      "Iter-6600 train loss: 2.3759 valid loss: 2.2924, valid accuracy: 0.2215\n",
      "Iter-6700 train loss: 2.4042 valid loss: 2.2901, valid accuracy: 0.2223\n",
      "Iter-6800 train loss: 2.1912 valid loss: 2.2876, valid accuracy: 0.2269\n",
      "Iter-6900 train loss: 2.3305 valid loss: 2.2851, valid accuracy: 0.2300\n",
      "Iter-7000 train loss: 2.2561 valid loss: 2.2826, valid accuracy: 0.2300\n",
      "Iter-7100 train loss: 2.1896 valid loss: 2.2802, valid accuracy: 0.2292\n",
      "Iter-7200 train loss: 2.2163 valid loss: 2.2778, valid accuracy: 0.2300\n",
      "Iter-7300 train loss: 2.2563 valid loss: 2.2753, valid accuracy: 0.2308\n",
      "Iter-7400 train loss: 2.1577 valid loss: 2.2731, valid accuracy: 0.2308\n",
      "Iter-7500 train loss: 2.2860 valid loss: 2.2709, valid accuracy: 0.2308\n",
      "Iter-7600 train loss: 2.1383 valid loss: 2.2688, valid accuracy: 0.2308\n",
      "Iter-7700 train loss: 2.2132 valid loss: 2.2665, valid accuracy: 0.2315\n",
      "Iter-7800 train loss: 2.3001 valid loss: 2.2645, valid accuracy: 0.2323\n",
      "Iter-7900 train loss: 2.1715 valid loss: 2.2624, valid accuracy: 0.2323\n",
      "Iter-8000 train loss: 2.3156 valid loss: 2.2603, valid accuracy: 0.2331\n",
      "Iter-8100 train loss: 2.2379 valid loss: 2.2584, valid accuracy: 0.2346\n",
      "Iter-8200 train loss: 2.2140 valid loss: 2.2564, valid accuracy: 0.2346\n",
      "Iter-8300 train loss: 2.2059 valid loss: 2.2545, valid accuracy: 0.2362\n",
      "Iter-8400 train loss: 2.4382 valid loss: 2.2527, valid accuracy: 0.2346\n",
      "Iter-8500 train loss: 2.1561 valid loss: 2.2509, valid accuracy: 0.2369\n",
      "Iter-8600 train loss: 2.2229 valid loss: 2.2491, valid accuracy: 0.2369\n",
      "Iter-8700 train loss: 2.1421 valid loss: 2.2475, valid accuracy: 0.2392\n",
      "Iter-8800 train loss: 2.2997 valid loss: 2.2458, valid accuracy: 0.2408\n",
      "Iter-8900 train loss: 2.1429 valid loss: 2.2443, valid accuracy: 0.2400\n",
      "Iter-9000 train loss: 2.2782 valid loss: 2.2426, valid accuracy: 0.2408\n",
      "Iter-9100 train loss: 2.1873 valid loss: 2.2409, valid accuracy: 0.2423\n",
      "Iter-9200 train loss: 2.1320 valid loss: 2.2393, valid accuracy: 0.2446\n",
      "Iter-9300 train loss: 2.3046 valid loss: 2.2377, valid accuracy: 0.2454\n",
      "Iter-9400 train loss: 2.2241 valid loss: 2.2360, valid accuracy: 0.2438\n",
      "Iter-9500 train loss: 2.3835 valid loss: 2.2346, valid accuracy: 0.2446\n",
      "Iter-9600 train loss: 2.3194 valid loss: 2.2331, valid accuracy: 0.2446\n",
      "Iter-9700 train loss: 2.1433 valid loss: 2.2315, valid accuracy: 0.2454\n",
      "Iter-9800 train loss: 2.1587 valid loss: 2.2301, valid accuracy: 0.2446\n",
      "Iter-9900 train loss: 2.2232 valid loss: 2.2285, valid accuracy: 0.2477\n",
      "Iter-10000 train loss: 2.2877 valid loss: 2.2269, valid accuracy: 0.2477\n",
      "Iter-10100 train loss: 2.0622 valid loss: 2.2254, valid accuracy: 0.2485\n",
      "Iter-10200 train loss: 2.3000 valid loss: 2.2240, valid accuracy: 0.2485\n",
      "Iter-10300 train loss: 2.1271 valid loss: 2.2226, valid accuracy: 0.2492\n",
      "Iter-10400 train loss: 2.1262 valid loss: 2.2210, valid accuracy: 0.2508\n",
      "Iter-10500 train loss: 2.1266 valid loss: 2.2197, valid accuracy: 0.2523\n",
      "Iter-10600 train loss: 2.3332 valid loss: 2.2185, valid accuracy: 0.2538\n",
      "Iter-10700 train loss: 2.2516 valid loss: 2.2172, valid accuracy: 0.2546\n",
      "Iter-10800 train loss: 2.2536 valid loss: 2.2160, valid accuracy: 0.2531\n",
      "Iter-10900 train loss: 2.2771 valid loss: 2.2145, valid accuracy: 0.2538\n",
      "Iter-11000 train loss: 2.1114 valid loss: 2.2133, valid accuracy: 0.2546\n",
      "Iter-11100 train loss: 2.1321 valid loss: 2.2119, valid accuracy: 0.2554\n",
      "Iter-11200 train loss: 2.2962 valid loss: 2.2106, valid accuracy: 0.2538\n",
      "Iter-11300 train loss: 2.2714 valid loss: 2.2093, valid accuracy: 0.2531\n",
      "Iter-11400 train loss: 2.2447 valid loss: 2.2082, valid accuracy: 0.2531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-11500 train loss: 2.1253 valid loss: 2.2069, valid accuracy: 0.2531\n",
      "Iter-11600 train loss: 2.0155 valid loss: 2.2058, valid accuracy: 0.2562\n",
      "Iter-11700 train loss: 2.1839 valid loss: 2.2046, valid accuracy: 0.2538\n",
      "Iter-11800 train loss: 2.1695 valid loss: 2.2031, valid accuracy: 0.2538\n",
      "Iter-11900 train loss: 2.2686 valid loss: 2.2018, valid accuracy: 0.2538\n",
      "Iter-12000 train loss: 2.3379 valid loss: 2.2006, valid accuracy: 0.2577\n",
      "Iter-12100 train loss: 2.1797 valid loss: 2.1994, valid accuracy: 0.2577\n",
      "Iter-12200 train loss: 2.2479 valid loss: 2.1982, valid accuracy: 0.2562\n",
      "Iter-12300 train loss: 2.2075 valid loss: 2.1970, valid accuracy: 0.2562\n",
      "Iter-12400 train loss: 2.2111 valid loss: 2.1958, valid accuracy: 0.2554\n",
      "Iter-12500 train loss: 2.1747 valid loss: 2.1947, valid accuracy: 0.2562\n",
      "Iter-12600 train loss: 1.9998 valid loss: 2.1934, valid accuracy: 0.2562\n",
      "Iter-12700 train loss: 2.0771 valid loss: 2.1922, valid accuracy: 0.2546\n",
      "Iter-12800 train loss: 2.1354 valid loss: 2.1910, valid accuracy: 0.2546\n",
      "Iter-12900 train loss: 2.1373 valid loss: 2.1898, valid accuracy: 0.2592\n",
      "Iter-13000 train loss: 2.1814 valid loss: 2.1888, valid accuracy: 0.2569\n",
      "Iter-13100 train loss: 2.1427 valid loss: 2.1875, valid accuracy: 0.2592\n",
      "Iter-13200 train loss: 2.3133 valid loss: 2.1865, valid accuracy: 0.2600\n",
      "Iter-13300 train loss: 2.1346 valid loss: 2.1855, valid accuracy: 0.2615\n",
      "Iter-13400 train loss: 2.1377 valid loss: 2.1845, valid accuracy: 0.2600\n",
      "Iter-13500 train loss: 2.0418 valid loss: 2.1836, valid accuracy: 0.2615\n",
      "Iter-13600 train loss: 2.1935 valid loss: 2.1825, valid accuracy: 0.2608\n",
      "Iter-13700 train loss: 2.2289 valid loss: 2.1814, valid accuracy: 0.2608\n",
      "Iter-13800 train loss: 1.9944 valid loss: 2.1803, valid accuracy: 0.2600\n",
      "Iter-13900 train loss: 1.9412 valid loss: 2.1792, valid accuracy: 0.2608\n",
      "Iter-14000 train loss: 2.1709 valid loss: 2.1781, valid accuracy: 0.2615\n",
      "Iter-14100 train loss: 2.1126 valid loss: 2.1770, valid accuracy: 0.2646\n",
      "Iter-14200 train loss: 2.1243 valid loss: 2.1761, valid accuracy: 0.2631\n",
      "Iter-14300 train loss: 1.9456 valid loss: 2.1750, valid accuracy: 0.2646\n",
      "Iter-14400 train loss: 2.1834 valid loss: 2.1743, valid accuracy: 0.2654\n",
      "Iter-14500 train loss: 1.9976 valid loss: 2.1732, valid accuracy: 0.2631\n",
      "Iter-14600 train loss: 2.0554 valid loss: 2.1721, valid accuracy: 0.2685\n",
      "Iter-14700 train loss: 2.0355 valid loss: 2.1712, valid accuracy: 0.2715\n",
      "Iter-14800 train loss: 2.2700 valid loss: 2.1701, valid accuracy: 0.2708\n",
      "Iter-14900 train loss: 2.1451 valid loss: 2.1691, valid accuracy: 0.2731\n",
      "Iter-15000 train loss: 2.0002 valid loss: 2.1683, valid accuracy: 0.2731\n",
      "Iter-15100 train loss: 2.1094 valid loss: 2.1675, valid accuracy: 0.2708\n",
      "Iter-15200 train loss: 2.1505 valid loss: 2.1665, valid accuracy: 0.2731\n",
      "Iter-15300 train loss: 2.1405 valid loss: 2.1656, valid accuracy: 0.2723\n",
      "Iter-15400 train loss: 2.1050 valid loss: 2.1648, valid accuracy: 0.2738\n",
      "Iter-15500 train loss: 1.9722 valid loss: 2.1641, valid accuracy: 0.2708\n",
      "Iter-15600 train loss: 2.2386 valid loss: 2.1632, valid accuracy: 0.2708\n",
      "Iter-15700 train loss: 2.2092 valid loss: 2.1621, valid accuracy: 0.2708\n",
      "Iter-15800 train loss: 2.0779 valid loss: 2.1610, valid accuracy: 0.2715\n",
      "Iter-15900 train loss: 2.2760 valid loss: 2.1600, valid accuracy: 0.2738\n",
      "Iter-16000 train loss: 2.2400 valid loss: 2.1590, valid accuracy: 0.2746\n",
      "Iter-16100 train loss: 2.0808 valid loss: 2.1583, valid accuracy: 0.2762\n",
      "Iter-16200 train loss: 2.1209 valid loss: 2.1574, valid accuracy: 0.2777\n",
      "Iter-16300 train loss: 1.9916 valid loss: 2.1564, valid accuracy: 0.2800\n",
      "Iter-16400 train loss: 2.0529 valid loss: 2.1555, valid accuracy: 0.2808\n",
      "Iter-16500 train loss: 2.0102 valid loss: 2.1545, valid accuracy: 0.2800\n",
      "Iter-16600 train loss: 2.1072 valid loss: 2.1537, valid accuracy: 0.2792\n",
      "Iter-16700 train loss: 2.1665 valid loss: 2.1527, valid accuracy: 0.2808\n",
      "Iter-16800 train loss: 2.1323 valid loss: 2.1519, valid accuracy: 0.2800\n",
      "Iter-16900 train loss: 2.1204 valid loss: 2.1512, valid accuracy: 0.2792\n",
      "Iter-17000 train loss: 2.1185 valid loss: 2.1502, valid accuracy: 0.2808\n",
      "Iter-17100 train loss: 1.9116 valid loss: 2.1496, valid accuracy: 0.2808\n",
      "Iter-17200 train loss: 2.0422 valid loss: 2.1487, valid accuracy: 0.2815\n",
      "Iter-17300 train loss: 2.1269 valid loss: 2.1479, valid accuracy: 0.2815\n",
      "Iter-17400 train loss: 2.2506 valid loss: 2.1470, valid accuracy: 0.2831\n",
      "Iter-17500 train loss: 2.0452 valid loss: 2.1463, valid accuracy: 0.2831\n",
      "Iter-17600 train loss: 1.9433 valid loss: 2.1455, valid accuracy: 0.2815\n",
      "Iter-17700 train loss: 2.1959 valid loss: 2.1447, valid accuracy: 0.2831\n",
      "Iter-17800 train loss: 2.1145 valid loss: 2.1439, valid accuracy: 0.2823\n",
      "Iter-17900 train loss: 1.9968 valid loss: 2.1430, valid accuracy: 0.2823\n",
      "Iter-18000 train loss: 1.9832 valid loss: 2.1425, valid accuracy: 0.2815\n",
      "Iter-18100 train loss: 2.1934 valid loss: 2.1417, valid accuracy: 0.2823\n",
      "Iter-18200 train loss: 2.1272 valid loss: 2.1408, valid accuracy: 0.2823\n",
      "Iter-18300 train loss: 2.0517 valid loss: 2.1400, valid accuracy: 0.2815\n",
      "Iter-18400 train loss: 2.2303 valid loss: 2.1390, valid accuracy: 0.2823\n",
      "Iter-18500 train loss: 2.2045 valid loss: 2.1380, valid accuracy: 0.2831\n",
      "Iter-18600 train loss: 1.9798 valid loss: 2.1374, valid accuracy: 0.2831\n",
      "Iter-18700 train loss: 2.1262 valid loss: 2.1367, valid accuracy: 0.2823\n",
      "Iter-18800 train loss: 2.0786 valid loss: 2.1361, valid accuracy: 0.2831\n",
      "Iter-18900 train loss: 1.9310 valid loss: 2.1354, valid accuracy: 0.2815\n",
      "Iter-19000 train loss: 2.1006 valid loss: 2.1345, valid accuracy: 0.2831\n",
      "Iter-19100 train loss: 2.0615 valid loss: 2.1336, valid accuracy: 0.2831\n",
      "Iter-19200 train loss: 2.0792 valid loss: 2.1329, valid accuracy: 0.2831\n",
      "Iter-19300 train loss: 2.2278 valid loss: 2.1320, valid accuracy: 0.2815\n",
      "Iter-19400 train loss: 2.2081 valid loss: 2.1313, valid accuracy: 0.2831\n",
      "Iter-19500 train loss: 1.9819 valid loss: 2.1306, valid accuracy: 0.2815\n",
      "Iter-19600 train loss: 2.1451 valid loss: 2.1299, valid accuracy: 0.2815\n",
      "Iter-19700 train loss: 2.2572 valid loss: 2.1291, valid accuracy: 0.2854\n",
      "Iter-19800 train loss: 2.0865 valid loss: 2.1285, valid accuracy: 0.2815\n",
      "Iter-19900 train loss: 2.0966 valid loss: 2.1278, valid accuracy: 0.2862\n",
      "Iter-20000 train loss: 2.0211 valid loss: 2.1273, valid accuracy: 0.2877\n",
      "Iter-20100 train loss: 2.0943 valid loss: 2.1267, valid accuracy: 0.2869\n",
      "Iter-20200 train loss: 1.9552 valid loss: 2.1260, valid accuracy: 0.2869\n",
      "Iter-20300 train loss: 2.0579 valid loss: 2.1251, valid accuracy: 0.2854\n",
      "Iter-20400 train loss: 2.1429 valid loss: 2.1242, valid accuracy: 0.2869\n",
      "Iter-20500 train loss: 2.1385 valid loss: 2.1235, valid accuracy: 0.2877\n",
      "Iter-20600 train loss: 2.1752 valid loss: 2.1230, valid accuracy: 0.2885\n",
      "Iter-20700 train loss: 2.1345 valid loss: 2.1222, valid accuracy: 0.2908\n",
      "Iter-20800 train loss: 2.0720 valid loss: 2.1216, valid accuracy: 0.2877\n",
      "Iter-20900 train loss: 1.8717 valid loss: 2.1210, valid accuracy: 0.2877\n",
      "Iter-21000 train loss: 2.0656 valid loss: 2.1204, valid accuracy: 0.2862\n",
      "Iter-21100 train loss: 2.1245 valid loss: 2.1198, valid accuracy: 0.2862\n",
      "Iter-21200 train loss: 1.8350 valid loss: 2.1192, valid accuracy: 0.2869\n",
      "Iter-21300 train loss: 2.1245 valid loss: 2.1187, valid accuracy: 0.2862\n",
      "Iter-21400 train loss: 1.9986 valid loss: 2.1182, valid accuracy: 0.2892\n",
      "Iter-21500 train loss: 1.9749 valid loss: 2.1176, valid accuracy: 0.2892\n",
      "Iter-21600 train loss: 2.0332 valid loss: 2.1169, valid accuracy: 0.2923\n",
      "Iter-21700 train loss: 2.2738 valid loss: 2.1163, valid accuracy: 0.2946\n",
      "Iter-21800 train loss: 1.9929 valid loss: 2.1157, valid accuracy: 0.2954\n",
      "Iter-21900 train loss: 2.2900 valid loss: 2.1153, valid accuracy: 0.2938\n",
      "Iter-22000 train loss: 2.0733 valid loss: 2.1147, valid accuracy: 0.2923\n",
      "Iter-22100 train loss: 2.2255 valid loss: 2.1139, valid accuracy: 0.2931\n",
      "Iter-22200 train loss: 1.9535 valid loss: 2.1133, valid accuracy: 0.2931\n",
      "Iter-22300 train loss: 2.1224 valid loss: 2.1126, valid accuracy: 0.2938\n",
      "Iter-22400 train loss: 2.0682 valid loss: 2.1117, valid accuracy: 0.2962\n",
      "Iter-22500 train loss: 2.0724 valid loss: 2.1113, valid accuracy: 0.2938\n",
      "Iter-22600 train loss: 2.1366 valid loss: 2.1106, valid accuracy: 0.2954\n",
      "Iter-22700 train loss: 2.2686 valid loss: 2.1101, valid accuracy: 0.2977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-22800 train loss: 2.2123 valid loss: 2.1095, valid accuracy: 0.2969\n",
      "Iter-22900 train loss: 2.1989 valid loss: 2.1090, valid accuracy: 0.2985\n",
      "Iter-23000 train loss: 2.0893 valid loss: 2.1082, valid accuracy: 0.2992\n",
      "Iter-23100 train loss: 1.9585 valid loss: 2.1074, valid accuracy: 0.3008\n",
      "Iter-23200 train loss: 2.1065 valid loss: 2.1069, valid accuracy: 0.3038\n",
      "Iter-23300 train loss: 2.2066 valid loss: 2.1065, valid accuracy: 0.3031\n",
      "Iter-23400 train loss: 2.0239 valid loss: 2.1058, valid accuracy: 0.3031\n",
      "Iter-23500 train loss: 2.1755 valid loss: 2.1055, valid accuracy: 0.3031\n",
      "Iter-23600 train loss: 2.0116 valid loss: 2.1049, valid accuracy: 0.3008\n",
      "Iter-23700 train loss: 1.9082 valid loss: 2.1046, valid accuracy: 0.2992\n",
      "Iter-23800 train loss: 2.1300 valid loss: 2.1040, valid accuracy: 0.3008\n",
      "Iter-23900 train loss: 2.0557 valid loss: 2.1033, valid accuracy: 0.3054\n",
      "Iter-24000 train loss: 1.9915 valid loss: 2.1027, valid accuracy: 0.3038\n",
      "Iter-24100 train loss: 1.8546 valid loss: 2.1021, valid accuracy: 0.3023\n",
      "Iter-24200 train loss: 1.9310 valid loss: 2.1014, valid accuracy: 0.3000\n",
      "Iter-24300 train loss: 2.0667 valid loss: 2.1008, valid accuracy: 0.3023\n",
      "Iter-24400 train loss: 2.0920 valid loss: 2.1004, valid accuracy: 0.3000\n",
      "Iter-24500 train loss: 2.0836 valid loss: 2.0997, valid accuracy: 0.3023\n",
      "Iter-24600 train loss: 2.1266 valid loss: 2.0992, valid accuracy: 0.3008\n",
      "Iter-24700 train loss: 2.0472 valid loss: 2.0985, valid accuracy: 0.3023\n",
      "Iter-24800 train loss: 2.0390 valid loss: 2.0979, valid accuracy: 0.3023\n",
      "Iter-24900 train loss: 2.3097 valid loss: 2.0972, valid accuracy: 0.3038\n",
      "Iter-25000 train loss: 2.1430 valid loss: 2.0968, valid accuracy: 0.3023\n",
      "Iter-25100 train loss: 1.9964 valid loss: 2.0964, valid accuracy: 0.3054\n",
      "Iter-25200 train loss: 2.0202 valid loss: 2.0957, valid accuracy: 0.3062\n",
      "Iter-25300 train loss: 2.0620 valid loss: 2.0950, valid accuracy: 0.3069\n",
      "Iter-25400 train loss: 1.9887 valid loss: 2.0944, valid accuracy: 0.3054\n",
      "Iter-25500 train loss: 2.2363 valid loss: 2.0941, valid accuracy: 0.3062\n",
      "Iter-25600 train loss: 1.9018 valid loss: 2.0938, valid accuracy: 0.3038\n",
      "Iter-25700 train loss: 1.9028 valid loss: 2.0933, valid accuracy: 0.3054\n",
      "Iter-25800 train loss: 1.9603 valid loss: 2.0926, valid accuracy: 0.3069\n",
      "Iter-25900 train loss: 2.1789 valid loss: 2.0918, valid accuracy: 0.3069\n",
      "Iter-26000 train loss: 1.9669 valid loss: 2.0914, valid accuracy: 0.3069\n",
      "Iter-26100 train loss: 1.9586 valid loss: 2.0912, valid accuracy: 0.3054\n",
      "Iter-26200 train loss: 1.8353 valid loss: 2.0906, valid accuracy: 0.3054\n",
      "Iter-26300 train loss: 1.9579 valid loss: 2.0902, valid accuracy: 0.3062\n",
      "Iter-26400 train loss: 2.1594 valid loss: 2.0896, valid accuracy: 0.3054\n",
      "Iter-26500 train loss: 2.0222 valid loss: 2.0892, valid accuracy: 0.3085\n",
      "Iter-26600 train loss: 1.9188 valid loss: 2.0887, valid accuracy: 0.3092\n",
      "Iter-26700 train loss: 1.8220 valid loss: 2.0880, valid accuracy: 0.3092\n",
      "Iter-26800 train loss: 1.9641 valid loss: 2.0875, valid accuracy: 0.3092\n",
      "Iter-26900 train loss: 1.7907 valid loss: 2.0873, valid accuracy: 0.3054\n",
      "Iter-27000 train loss: 2.0622 valid loss: 2.0869, valid accuracy: 0.3046\n",
      "Iter-27100 train loss: 1.9910 valid loss: 2.0865, valid accuracy: 0.3015\n",
      "Iter-27200 train loss: 2.2265 valid loss: 2.0860, valid accuracy: 0.3015\n",
      "Iter-27300 train loss: 2.2085 valid loss: 2.0854, valid accuracy: 0.3023\n",
      "Iter-27400 train loss: 2.1294 valid loss: 2.0848, valid accuracy: 0.3062\n",
      "Iter-27500 train loss: 2.2200 valid loss: 2.0845, valid accuracy: 0.3038\n",
      "Iter-27600 train loss: 1.9175 valid loss: 2.0839, valid accuracy: 0.3062\n",
      "Iter-27700 train loss: 1.9845 valid loss: 2.0836, valid accuracy: 0.3046\n",
      "Iter-27800 train loss: 2.0966 valid loss: 2.0832, valid accuracy: 0.3038\n",
      "Iter-27900 train loss: 1.9326 valid loss: 2.0829, valid accuracy: 0.3069\n",
      "Iter-28000 train loss: 1.9398 valid loss: 2.0825, valid accuracy: 0.3054\n",
      "Iter-28100 train loss: 2.1888 valid loss: 2.0821, valid accuracy: 0.3062\n",
      "Iter-28200 train loss: 2.0977 valid loss: 2.0819, valid accuracy: 0.3062\n",
      "Iter-28300 train loss: 2.1016 valid loss: 2.0815, valid accuracy: 0.3046\n",
      "Iter-28400 train loss: 1.8846 valid loss: 2.0810, valid accuracy: 0.3069\n",
      "Iter-28500 train loss: 1.9200 valid loss: 2.0807, valid accuracy: 0.3054\n",
      "Iter-28600 train loss: 2.2601 valid loss: 2.0801, valid accuracy: 0.3062\n",
      "Iter-28700 train loss: 2.0499 valid loss: 2.0799, valid accuracy: 0.3069\n",
      "Iter-28800 train loss: 2.0955 valid loss: 2.0791, valid accuracy: 0.3092\n",
      "Iter-28900 train loss: 2.0071 valid loss: 2.0788, valid accuracy: 0.3077\n",
      "Iter-29000 train loss: 2.0119 valid loss: 2.0783, valid accuracy: 0.3085\n",
      "Iter-29100 train loss: 1.9183 valid loss: 2.0781, valid accuracy: 0.3092\n",
      "Iter-29200 train loss: 2.1731 valid loss: 2.0774, valid accuracy: 0.3092\n",
      "Iter-29300 train loss: 2.0725 valid loss: 2.0771, valid accuracy: 0.3108\n",
      "Iter-29400 train loss: 2.0307 valid loss: 2.0770, valid accuracy: 0.3100\n",
      "Iter-29500 train loss: 2.0995 valid loss: 2.0765, valid accuracy: 0.3108\n",
      "Iter-29600 train loss: 1.9451 valid loss: 2.0760, valid accuracy: 0.3069\n",
      "Iter-29700 train loss: 1.9186 valid loss: 2.0754, valid accuracy: 0.3092\n",
      "Iter-29800 train loss: 1.7307 valid loss: 2.0751, valid accuracy: 0.3108\n",
      "Iter-29900 train loss: 1.8980 valid loss: 2.0747, valid accuracy: 0.3108\n",
      "Iter-30000 train loss: 2.1408 valid loss: 2.0744, valid accuracy: 0.3115\n",
      "Iter-30100 train loss: 1.7174 valid loss: 2.0741, valid accuracy: 0.3108\n",
      "Iter-30200 train loss: 1.8641 valid loss: 2.0738, valid accuracy: 0.3115\n",
      "Iter-30300 train loss: 2.0485 valid loss: 2.0735, valid accuracy: 0.3115\n",
      "Iter-30400 train loss: 1.9777 valid loss: 2.0731, valid accuracy: 0.3115\n",
      "Iter-30500 train loss: 2.0823 valid loss: 2.0728, valid accuracy: 0.3108\n",
      "Iter-30600 train loss: 1.9014 valid loss: 2.0724, valid accuracy: 0.3108\n",
      "Iter-30700 train loss: 1.8995 valid loss: 2.0723, valid accuracy: 0.3092\n",
      "Iter-30800 train loss: 2.2324 valid loss: 2.0722, valid accuracy: 0.3092\n",
      "Iter-30900 train loss: 1.9465 valid loss: 2.0716, valid accuracy: 0.3108\n",
      "Iter-31000 train loss: 2.0142 valid loss: 2.0712, valid accuracy: 0.3108\n",
      "Iter-31100 train loss: 1.9193 valid loss: 2.0710, valid accuracy: 0.3108\n",
      "Iter-31200 train loss: 1.8642 valid loss: 2.0707, valid accuracy: 0.3123\n",
      "Iter-31300 train loss: 2.1955 valid loss: 2.0705, valid accuracy: 0.3123\n",
      "Iter-31400 train loss: 2.0314 valid loss: 2.0701, valid accuracy: 0.3138\n",
      "Iter-31500 train loss: 1.9793 valid loss: 2.0698, valid accuracy: 0.3138\n",
      "Iter-31600 train loss: 1.8740 valid loss: 2.0696, valid accuracy: 0.3131\n",
      "Iter-31700 train loss: 1.9723 valid loss: 2.0691, valid accuracy: 0.3131\n",
      "Iter-31800 train loss: 2.1706 valid loss: 2.0690, valid accuracy: 0.3115\n",
      "Iter-31900 train loss: 2.0615 valid loss: 2.0685, valid accuracy: 0.3123\n",
      "Iter-32000 train loss: 1.9646 valid loss: 2.0681, valid accuracy: 0.3131\n",
      "Iter-32100 train loss: 1.9205 valid loss: 2.0676, valid accuracy: 0.3123\n",
      "Iter-32200 train loss: 1.9783 valid loss: 2.0675, valid accuracy: 0.3154\n",
      "Iter-32300 train loss: 2.2014 valid loss: 2.0669, valid accuracy: 0.3138\n",
      "Iter-32400 train loss: 1.8839 valid loss: 2.0665, valid accuracy: 0.3146\n",
      "Iter-32500 train loss: 2.1527 valid loss: 2.0661, valid accuracy: 0.3146\n",
      "Iter-32600 train loss: 1.7809 valid loss: 2.0661, valid accuracy: 0.3154\n",
      "Iter-32700 train loss: 2.0099 valid loss: 2.0658, valid accuracy: 0.3162\n",
      "Iter-32800 train loss: 2.0264 valid loss: 2.0656, valid accuracy: 0.3177\n",
      "Iter-32900 train loss: 1.9861 valid loss: 2.0651, valid accuracy: 0.3169\n",
      "Iter-33000 train loss: 2.1429 valid loss: 2.0650, valid accuracy: 0.3177\n",
      "Iter-33100 train loss: 1.9519 valid loss: 2.0647, valid accuracy: 0.3200\n",
      "Iter-33200 train loss: 2.1464 valid loss: 2.0645, valid accuracy: 0.3169\n",
      "Iter-33300 train loss: 2.0660 valid loss: 2.0642, valid accuracy: 0.3169\n",
      "Iter-33400 train loss: 1.8880 valid loss: 2.0640, valid accuracy: 0.3162\n",
      "Iter-33500 train loss: 2.0110 valid loss: 2.0635, valid accuracy: 0.3162\n",
      "Iter-33600 train loss: 2.1250 valid loss: 2.0630, valid accuracy: 0.3169\n",
      "Iter-33700 train loss: 2.1997 valid loss: 2.0627, valid accuracy: 0.3200\n",
      "Iter-33800 train loss: 2.0313 valid loss: 2.0619, valid accuracy: 0.3192\n",
      "Iter-33900 train loss: 2.0147 valid loss: 2.0619, valid accuracy: 0.3192\n",
      "Iter-34000 train loss: 2.0113 valid loss: 2.0617, valid accuracy: 0.3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-34100 train loss: 1.8828 valid loss: 2.0612, valid accuracy: 0.3200\n",
      "Iter-34200 train loss: 1.9178 valid loss: 2.0611, valid accuracy: 0.3192\n",
      "Iter-34300 train loss: 2.1090 valid loss: 2.0606, valid accuracy: 0.3208\n",
      "Iter-34400 train loss: 2.0991 valid loss: 2.0602, valid accuracy: 0.3208\n",
      "Iter-34500 train loss: 1.7571 valid loss: 2.0600, valid accuracy: 0.3223\n",
      "Iter-34600 train loss: 1.8803 valid loss: 2.0597, valid accuracy: 0.3192\n",
      "Iter-34700 train loss: 2.0300 valid loss: 2.0595, valid accuracy: 0.3208\n",
      "Iter-34800 train loss: 1.9922 valid loss: 2.0592, valid accuracy: 0.3192\n",
      "Iter-34900 train loss: 2.3007 valid loss: 2.0589, valid accuracy: 0.3185\n",
      "Iter-35000 train loss: 2.0619 valid loss: 2.0585, valid accuracy: 0.3200\n",
      "Iter-35100 train loss: 1.9501 valid loss: 2.0581, valid accuracy: 0.3192\n",
      "Iter-35200 train loss: 2.0452 valid loss: 2.0578, valid accuracy: 0.3215\n",
      "Iter-35300 train loss: 2.0881 valid loss: 2.0576, valid accuracy: 0.3200\n",
      "Iter-35400 train loss: 2.2670 valid loss: 2.0574, valid accuracy: 0.3200\n",
      "Iter-35500 train loss: 2.1207 valid loss: 2.0571, valid accuracy: 0.3192\n",
      "Iter-35600 train loss: 2.1122 valid loss: 2.0568, valid accuracy: 0.3192\n",
      "Iter-35700 train loss: 1.9954 valid loss: 2.0562, valid accuracy: 0.3200\n",
      "Iter-35800 train loss: 1.7580 valid loss: 2.0561, valid accuracy: 0.3200\n",
      "Iter-35900 train loss: 1.8950 valid loss: 2.0559, valid accuracy: 0.3208\n",
      "Iter-36000 train loss: 1.7769 valid loss: 2.0557, valid accuracy: 0.3208\n",
      "Iter-36100 train loss: 1.9873 valid loss: 2.0554, valid accuracy: 0.3192\n",
      "Iter-36200 train loss: 2.0768 valid loss: 2.0551, valid accuracy: 0.3215\n",
      "Iter-36300 train loss: 2.0988 valid loss: 2.0547, valid accuracy: 0.3208\n",
      "Iter-36400 train loss: 2.0140 valid loss: 2.0545, valid accuracy: 0.3215\n",
      "Iter-36500 train loss: 2.0101 valid loss: 2.0546, valid accuracy: 0.3215\n",
      "Iter-36600 train loss: 1.9704 valid loss: 2.0542, valid accuracy: 0.3223\n",
      "Iter-36700 train loss: 2.0751 valid loss: 2.0540, valid accuracy: 0.3215\n",
      "Iter-36800 train loss: 2.0710 valid loss: 2.0539, valid accuracy: 0.3192\n",
      "Iter-36900 train loss: 2.0237 valid loss: 2.0535, valid accuracy: 0.3192\n",
      "Iter-37000 train loss: 1.9697 valid loss: 2.0533, valid accuracy: 0.3185\n",
      "Iter-37100 train loss: 2.1148 valid loss: 2.0532, valid accuracy: 0.3200\n",
      "Iter-37200 train loss: 2.1234 valid loss: 2.0531, valid accuracy: 0.3185\n",
      "Iter-37300 train loss: 1.9922 valid loss: 2.0527, valid accuracy: 0.3192\n",
      "Iter-37400 train loss: 1.8913 valid loss: 2.0524, valid accuracy: 0.3208\n",
      "Iter-37500 train loss: 2.0121 valid loss: 2.0523, valid accuracy: 0.3208\n",
      "Iter-37600 train loss: 2.1339 valid loss: 2.0521, valid accuracy: 0.3223\n",
      "Iter-37700 train loss: 2.2391 valid loss: 2.0517, valid accuracy: 0.3238\n",
      "Iter-37800 train loss: 2.0694 valid loss: 2.0517, valid accuracy: 0.3231\n",
      "Iter-37900 train loss: 1.8052 valid loss: 2.0516, valid accuracy: 0.3231\n",
      "Iter-38000 train loss: 2.2629 valid loss: 2.0513, valid accuracy: 0.3215\n",
      "Iter-38100 train loss: 2.1030 valid loss: 2.0512, valid accuracy: 0.3231\n",
      "Iter-38200 train loss: 2.1072 valid loss: 2.0508, valid accuracy: 0.3238\n",
      "Iter-38300 train loss: 2.2473 valid loss: 2.0505, valid accuracy: 0.3238\n",
      "Iter-38400 train loss: 2.0404 valid loss: 2.0502, valid accuracy: 0.3238\n",
      "Iter-38500 train loss: 2.1465 valid loss: 2.0498, valid accuracy: 0.3238\n",
      "Iter-38600 train loss: 2.0669 valid loss: 2.0498, valid accuracy: 0.3246\n",
      "Iter-38700 train loss: 1.8331 valid loss: 2.0493, valid accuracy: 0.3246\n",
      "Iter-38800 train loss: 1.8345 valid loss: 2.0495, valid accuracy: 0.3238\n",
      "Iter-38900 train loss: 2.0594 valid loss: 2.0493, valid accuracy: 0.3254\n",
      "Iter-39000 train loss: 1.8809 valid loss: 2.0493, valid accuracy: 0.3246\n",
      "Iter-39100 train loss: 1.9629 valid loss: 2.0494, valid accuracy: 0.3231\n",
      "Iter-39200 train loss: 2.0093 valid loss: 2.0492, valid accuracy: 0.3246\n",
      "Iter-39300 train loss: 2.1229 valid loss: 2.0490, valid accuracy: 0.3246\n",
      "Iter-39400 train loss: 2.0368 valid loss: 2.0487, valid accuracy: 0.3262\n",
      "Iter-39500 train loss: 1.7832 valid loss: 2.0483, valid accuracy: 0.3254\n",
      "Iter-39600 train loss: 2.1132 valid loss: 2.0482, valid accuracy: 0.3246\n",
      "Iter-39700 train loss: 2.0138 valid loss: 2.0479, valid accuracy: 0.3254\n",
      "Iter-39800 train loss: 1.9636 valid loss: 2.0478, valid accuracy: 0.3254\n",
      "Iter-39900 train loss: 2.1184 valid loss: 2.0476, valid accuracy: 0.3238\n",
      "Iter-40000 train loss: 1.9701 valid loss: 2.0471, valid accuracy: 0.3238\n",
      "Iter-40100 train loss: 2.0001 valid loss: 2.0469, valid accuracy: 0.3238\n",
      "Iter-40200 train loss: 1.7294 valid loss: 2.0464, valid accuracy: 0.3246\n",
      "Iter-40300 train loss: 2.0547 valid loss: 2.0461, valid accuracy: 0.3277\n",
      "Iter-40400 train loss: 2.0517 valid loss: 2.0459, valid accuracy: 0.3246\n",
      "Iter-40500 train loss: 1.8541 valid loss: 2.0458, valid accuracy: 0.3246\n",
      "Iter-40600 train loss: 2.0346 valid loss: 2.0457, valid accuracy: 0.3262\n",
      "Iter-40700 train loss: 1.8550 valid loss: 2.0455, valid accuracy: 0.3269\n",
      "Iter-40800 train loss: 1.9119 valid loss: 2.0450, valid accuracy: 0.3262\n",
      "Iter-40900 train loss: 1.6881 valid loss: 2.0448, valid accuracy: 0.3285\n",
      "Iter-41000 train loss: 1.9479 valid loss: 2.0449, valid accuracy: 0.3262\n",
      "Iter-41100 train loss: 1.8895 valid loss: 2.0448, valid accuracy: 0.3246\n",
      "Iter-41200 train loss: 2.0488 valid loss: 2.0448, valid accuracy: 0.3238\n",
      "Iter-41300 train loss: 2.0031 valid loss: 2.0447, valid accuracy: 0.3254\n",
      "Iter-41400 train loss: 1.9620 valid loss: 2.0441, valid accuracy: 0.3246\n",
      "Iter-41500 train loss: 1.8653 valid loss: 2.0438, valid accuracy: 0.3223\n",
      "Iter-41600 train loss: 2.1767 valid loss: 2.0436, valid accuracy: 0.3254\n",
      "Iter-41700 train loss: 2.0240 valid loss: 2.0434, valid accuracy: 0.3238\n",
      "Iter-41800 train loss: 1.9558 valid loss: 2.0432, valid accuracy: 0.3238\n",
      "Iter-41900 train loss: 1.8791 valid loss: 2.0430, valid accuracy: 0.3238\n",
      "Iter-42000 train loss: 1.9898 valid loss: 2.0427, valid accuracy: 0.3254\n",
      "Iter-42100 train loss: 2.0536 valid loss: 2.0425, valid accuracy: 0.3231\n",
      "Iter-42200 train loss: 1.8439 valid loss: 2.0426, valid accuracy: 0.3238\n",
      "Iter-42300 train loss: 1.8922 valid loss: 2.0424, valid accuracy: 0.3238\n",
      "Iter-42400 train loss: 1.9331 valid loss: 2.0421, valid accuracy: 0.3246\n",
      "Iter-42500 train loss: 1.7898 valid loss: 2.0418, valid accuracy: 0.3254\n",
      "Iter-42600 train loss: 1.8660 valid loss: 2.0415, valid accuracy: 0.3246\n",
      "Iter-42700 train loss: 1.9964 valid loss: 2.0415, valid accuracy: 0.3238\n",
      "Iter-42800 train loss: 2.0992 valid loss: 2.0413, valid accuracy: 0.3238\n",
      "Iter-42900 train loss: 1.7896 valid loss: 2.0411, valid accuracy: 0.3246\n",
      "Iter-43000 train loss: 2.2393 valid loss: 2.0409, valid accuracy: 0.3231\n",
      "Iter-43100 train loss: 1.9143 valid loss: 2.0408, valid accuracy: 0.3246\n",
      "Iter-43200 train loss: 2.0256 valid loss: 2.0404, valid accuracy: 0.3254\n",
      "Iter-43300 train loss: 2.0166 valid loss: 2.0399, valid accuracy: 0.3238\n",
      "Iter-43400 train loss: 2.2359 valid loss: 2.0397, valid accuracy: 0.3238\n",
      "Iter-43500 train loss: 1.9061 valid loss: 2.0395, valid accuracy: 0.3231\n",
      "Iter-43600 train loss: 1.9960 valid loss: 2.0393, valid accuracy: 0.3246\n",
      "Iter-43700 train loss: 1.6614 valid loss: 2.0391, valid accuracy: 0.3254\n",
      "Iter-43800 train loss: 1.9523 valid loss: 2.0389, valid accuracy: 0.3262\n",
      "Iter-43900 train loss: 2.1173 valid loss: 2.0390, valid accuracy: 0.3277\n",
      "Iter-44000 train loss: 1.9197 valid loss: 2.0387, valid accuracy: 0.3285\n",
      "Iter-44100 train loss: 1.8660 valid loss: 2.0384, valid accuracy: 0.3292\n",
      "Iter-44200 train loss: 2.2178 valid loss: 2.0383, valid accuracy: 0.3308\n",
      "Iter-44300 train loss: 1.8872 valid loss: 2.0384, valid accuracy: 0.3300\n",
      "Iter-44400 train loss: 1.9071 valid loss: 2.0380, valid accuracy: 0.3300\n",
      "Iter-44500 train loss: 1.7905 valid loss: 2.0379, valid accuracy: 0.3285\n",
      "Iter-44600 train loss: 2.0897 valid loss: 2.0379, valid accuracy: 0.3285\n",
      "Iter-44700 train loss: 1.9219 valid loss: 2.0378, valid accuracy: 0.3292\n",
      "Iter-44800 train loss: 2.0168 valid loss: 2.0377, valid accuracy: 0.3300\n",
      "Iter-44900 train loss: 2.0634 valid loss: 2.0373, valid accuracy: 0.3277\n",
      "Iter-45000 train loss: 2.0459 valid loss: 2.0374, valid accuracy: 0.3308\n",
      "Iter-45100 train loss: 1.9782 valid loss: 2.0375, valid accuracy: 0.3262\n",
      "Iter-45200 train loss: 1.8141 valid loss: 2.0372, valid accuracy: 0.3246\n",
      "Iter-45300 train loss: 1.8612 valid loss: 2.0371, valid accuracy: 0.3246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-45400 train loss: 1.9869 valid loss: 2.0369, valid accuracy: 0.3277\n",
      "Iter-45500 train loss: 1.8522 valid loss: 2.0368, valid accuracy: 0.3254\n",
      "Iter-45600 train loss: 2.1515 valid loss: 2.0367, valid accuracy: 0.3269\n",
      "Iter-45700 train loss: 1.9967 valid loss: 2.0364, valid accuracy: 0.3277\n",
      "Iter-45800 train loss: 2.0322 valid loss: 2.0361, valid accuracy: 0.3277\n",
      "Iter-45900 train loss: 2.0782 valid loss: 2.0362, valid accuracy: 0.3285\n",
      "Iter-46000 train loss: 2.0655 valid loss: 2.0359, valid accuracy: 0.3285\n",
      "Iter-46100 train loss: 1.9845 valid loss: 2.0359, valid accuracy: 0.3277\n",
      "Iter-46200 train loss: 2.0143 valid loss: 2.0362, valid accuracy: 0.3285\n",
      "Iter-46300 train loss: 1.7571 valid loss: 2.0361, valid accuracy: 0.3292\n",
      "Iter-46400 train loss: 1.9806 valid loss: 2.0361, valid accuracy: 0.3285\n",
      "Iter-46500 train loss: 1.9609 valid loss: 2.0359, valid accuracy: 0.3277\n",
      "Iter-46600 train loss: 1.9240 valid loss: 2.0357, valid accuracy: 0.3277\n",
      "Iter-46700 train loss: 1.6956 valid loss: 2.0359, valid accuracy: 0.3277\n",
      "Iter-46800 train loss: 1.9168 valid loss: 2.0356, valid accuracy: 0.3285\n",
      "Iter-46900 train loss: 1.9092 valid loss: 2.0353, valid accuracy: 0.3285\n",
      "Iter-47000 train loss: 2.0374 valid loss: 2.0353, valid accuracy: 0.3285\n",
      "Iter-47100 train loss: 1.8457 valid loss: 2.0347, valid accuracy: 0.3285\n",
      "Iter-47200 train loss: 1.9663 valid loss: 2.0346, valid accuracy: 0.3262\n",
      "Iter-47300 train loss: 1.8225 valid loss: 2.0341, valid accuracy: 0.3285\n",
      "Iter-47400 train loss: 2.0013 valid loss: 2.0340, valid accuracy: 0.3285\n",
      "Iter-47500 train loss: 2.0080 valid loss: 2.0334, valid accuracy: 0.3277\n",
      "Iter-47600 train loss: 1.9993 valid loss: 2.0331, valid accuracy: 0.3238\n",
      "Iter-47700 train loss: 1.8563 valid loss: 2.0331, valid accuracy: 0.3246\n",
      "Iter-47800 train loss: 1.9222 valid loss: 2.0333, valid accuracy: 0.3246\n",
      "Iter-47900 train loss: 2.1074 valid loss: 2.0331, valid accuracy: 0.3254\n",
      "Iter-48000 train loss: 1.9542 valid loss: 2.0332, valid accuracy: 0.3277\n",
      "Iter-48100 train loss: 1.9715 valid loss: 2.0331, valid accuracy: 0.3269\n",
      "Iter-48200 train loss: 2.1196 valid loss: 2.0330, valid accuracy: 0.3277\n",
      "Iter-48300 train loss: 2.0079 valid loss: 2.0331, valid accuracy: 0.3269\n",
      "Iter-48400 train loss: 1.7557 valid loss: 2.0330, valid accuracy: 0.3285\n",
      "Iter-48500 train loss: 1.9003 valid loss: 2.0328, valid accuracy: 0.3292\n",
      "Iter-48600 train loss: 1.9447 valid loss: 2.0327, valid accuracy: 0.3269\n",
      "Iter-48700 train loss: 2.0662 valid loss: 2.0326, valid accuracy: 0.3269\n",
      "Iter-48800 train loss: 2.1269 valid loss: 2.0322, valid accuracy: 0.3269\n",
      "Iter-48900 train loss: 2.1041 valid loss: 2.0318, valid accuracy: 0.3285\n",
      "Iter-49000 train loss: 1.8944 valid loss: 2.0318, valid accuracy: 0.3300\n",
      "Iter-49100 train loss: 2.0174 valid loss: 2.0316, valid accuracy: 0.3277\n",
      "Iter-49200 train loss: 1.6438 valid loss: 2.0312, valid accuracy: 0.3277\n",
      "Iter-49300 train loss: 2.0719 valid loss: 2.0312, valid accuracy: 0.3285\n",
      "Iter-49400 train loss: 1.9432 valid loss: 2.0311, valid accuracy: 0.3285\n",
      "Iter-49500 train loss: 2.0466 valid loss: 2.0310, valid accuracy: 0.3292\n",
      "Iter-49600 train loss: 1.9624 valid loss: 2.0310, valid accuracy: 0.3292\n",
      "Iter-49700 train loss: 1.7816 valid loss: 2.0308, valid accuracy: 0.3277\n",
      "Iter-49800 train loss: 2.0422 valid loss: 2.0305, valid accuracy: 0.3292\n",
      "Iter-49900 train loss: 1.9085 valid loss: 2.0303, valid accuracy: 0.3308\n",
      "Iter-50000 train loss: 1.8412 valid loss: 2.0301, valid accuracy: 0.3292\n",
      "Iter-50100 train loss: 1.9654 valid loss: 2.0301, valid accuracy: 0.3262\n",
      "Iter-50200 train loss: 2.2592 valid loss: 2.0303, valid accuracy: 0.3269\n",
      "Iter-50300 train loss: 1.8053 valid loss: 2.0301, valid accuracy: 0.3254\n",
      "Iter-50400 train loss: 1.8425 valid loss: 2.0299, valid accuracy: 0.3285\n",
      "Iter-50500 train loss: 1.8764 valid loss: 2.0297, valid accuracy: 0.3285\n",
      "Iter-50600 train loss: 1.9298 valid loss: 2.0296, valid accuracy: 0.3285\n",
      "Iter-50700 train loss: 2.1430 valid loss: 2.0294, valid accuracy: 0.3277\n",
      "Iter-50800 train loss: 1.9999 valid loss: 2.0294, valid accuracy: 0.3308\n",
      "Iter-50900 train loss: 2.0490 valid loss: 2.0293, valid accuracy: 0.3292\n",
      "Iter-51000 train loss: 2.1208 valid loss: 2.0291, valid accuracy: 0.3285\n",
      "Iter-51100 train loss: 2.2128 valid loss: 2.0293, valid accuracy: 0.3285\n",
      "Iter-51200 train loss: 2.2327 valid loss: 2.0290, valid accuracy: 0.3277\n",
      "Iter-51300 train loss: 2.1195 valid loss: 2.0287, valid accuracy: 0.3277\n",
      "Iter-51400 train loss: 1.9785 valid loss: 2.0286, valid accuracy: 0.3285\n",
      "Iter-51500 train loss: 2.0089 valid loss: 2.0285, valid accuracy: 0.3277\n",
      "Iter-51600 train loss: 2.0287 valid loss: 2.0282, valid accuracy: 0.3292\n",
      "Iter-51700 train loss: 1.8519 valid loss: 2.0279, valid accuracy: 0.3285\n",
      "Iter-51800 train loss: 2.0667 valid loss: 2.0279, valid accuracy: 0.3292\n",
      "Iter-51900 train loss: 1.9664 valid loss: 2.0276, valid accuracy: 0.3300\n",
      "Iter-52000 train loss: 1.6827 valid loss: 2.0273, valid accuracy: 0.3292\n",
      "Iter-52100 train loss: 1.7950 valid loss: 2.0268, valid accuracy: 0.3292\n",
      "Iter-52200 train loss: 2.0468 valid loss: 2.0267, valid accuracy: 0.3300\n",
      "Iter-52300 train loss: 1.8198 valid loss: 2.0264, valid accuracy: 0.3308\n",
      "Iter-52400 train loss: 2.0649 valid loss: 2.0265, valid accuracy: 0.3315\n",
      "Iter-52500 train loss: 1.8868 valid loss: 2.0264, valid accuracy: 0.3308\n",
      "Iter-52600 train loss: 2.0153 valid loss: 2.0265, valid accuracy: 0.3323\n",
      "Iter-52700 train loss: 2.1417 valid loss: 2.0261, valid accuracy: 0.3315\n",
      "Iter-52800 train loss: 1.9846 valid loss: 2.0262, valid accuracy: 0.3315\n",
      "Iter-52900 train loss: 1.7341 valid loss: 2.0260, valid accuracy: 0.3323\n",
      "Iter-53000 train loss: 1.9498 valid loss: 2.0260, valid accuracy: 0.3323\n",
      "Iter-53100 train loss: 2.0078 valid loss: 2.0264, valid accuracy: 0.3331\n",
      "Iter-53200 train loss: 2.0230 valid loss: 2.0267, valid accuracy: 0.3315\n",
      "Iter-53300 train loss: 1.8725 valid loss: 2.0268, valid accuracy: 0.3315\n",
      "Iter-53400 train loss: 1.6524 valid loss: 2.0265, valid accuracy: 0.3315\n",
      "Iter-53500 train loss: 1.9872 valid loss: 2.0265, valid accuracy: 0.3300\n",
      "Iter-53600 train loss: 1.9654 valid loss: 2.0265, valid accuracy: 0.3308\n",
      "Iter-53700 train loss: 1.9057 valid loss: 2.0264, valid accuracy: 0.3300\n",
      "Iter-53800 train loss: 2.0061 valid loss: 2.0266, valid accuracy: 0.3315\n",
      "Iter-53900 train loss: 1.9620 valid loss: 2.0265, valid accuracy: 0.3308\n",
      "Iter-54000 train loss: 2.0259 valid loss: 2.0266, valid accuracy: 0.3308\n",
      "Iter-54100 train loss: 1.8107 valid loss: 2.0262, valid accuracy: 0.3285\n",
      "Iter-54200 train loss: 1.6973 valid loss: 2.0263, valid accuracy: 0.3277\n",
      "Iter-54300 train loss: 1.9532 valid loss: 2.0260, valid accuracy: 0.3277\n",
      "Iter-54400 train loss: 1.8227 valid loss: 2.0262, valid accuracy: 0.3300\n",
      "Iter-54500 train loss: 1.9912 valid loss: 2.0261, valid accuracy: 0.3292\n",
      "Iter-54600 train loss: 1.9981 valid loss: 2.0258, valid accuracy: 0.3315\n",
      "Iter-54700 train loss: 1.8207 valid loss: 2.0256, valid accuracy: 0.3285\n",
      "Iter-54800 train loss: 1.9493 valid loss: 2.0255, valid accuracy: 0.3315\n",
      "Iter-54900 train loss: 1.8691 valid loss: 2.0253, valid accuracy: 0.3308\n",
      "Iter-55000 train loss: 1.9982 valid loss: 2.0249, valid accuracy: 0.3323\n",
      "Iter-55100 train loss: 1.8301 valid loss: 2.0249, valid accuracy: 0.3323\n",
      "Iter-55200 train loss: 1.7433 valid loss: 2.0246, valid accuracy: 0.3331\n",
      "Iter-55300 train loss: 2.0211 valid loss: 2.0243, valid accuracy: 0.3323\n",
      "Iter-55400 train loss: 1.9239 valid loss: 2.0243, valid accuracy: 0.3315\n",
      "Iter-55500 train loss: 1.8149 valid loss: 2.0242, valid accuracy: 0.3315\n",
      "Iter-55600 train loss: 2.0844 valid loss: 2.0242, valid accuracy: 0.3323\n",
      "Iter-55700 train loss: 1.7589 valid loss: 2.0241, valid accuracy: 0.3323\n",
      "Iter-55800 train loss: 2.0155 valid loss: 2.0239, valid accuracy: 0.3331\n",
      "Iter-55900 train loss: 2.0265 valid loss: 2.0237, valid accuracy: 0.3331\n",
      "Iter-56000 train loss: 1.9100 valid loss: 2.0234, valid accuracy: 0.3331\n",
      "Iter-56100 train loss: 1.8896 valid loss: 2.0234, valid accuracy: 0.3338\n",
      "Iter-56200 train loss: 1.9124 valid loss: 2.0232, valid accuracy: 0.3338\n",
      "Iter-56300 train loss: 1.8447 valid loss: 2.0232, valid accuracy: 0.3338\n",
      "Iter-56400 train loss: 1.8625 valid loss: 2.0228, valid accuracy: 0.3346\n",
      "Iter-56500 train loss: 2.1747 valid loss: 2.0227, valid accuracy: 0.3338\n",
      "Iter-56600 train loss: 2.0571 valid loss: 2.0226, valid accuracy: 0.3346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-56700 train loss: 1.7650 valid loss: 2.0221, valid accuracy: 0.3346\n",
      "Iter-56800 train loss: 1.9571 valid loss: 2.0218, valid accuracy: 0.3346\n",
      "Iter-56900 train loss: 2.0383 valid loss: 2.0217, valid accuracy: 0.3346\n",
      "Iter-57000 train loss: 1.9173 valid loss: 2.0215, valid accuracy: 0.3362\n",
      "Iter-57100 train loss: 2.1130 valid loss: 2.0216, valid accuracy: 0.3354\n",
      "Iter-57200 train loss: 1.9784 valid loss: 2.0215, valid accuracy: 0.3346\n",
      "Iter-57300 train loss: 2.1093 valid loss: 2.0214, valid accuracy: 0.3346\n",
      "Iter-57400 train loss: 1.8961 valid loss: 2.0213, valid accuracy: 0.3346\n",
      "Iter-57500 train loss: 1.8306 valid loss: 2.0211, valid accuracy: 0.3338\n",
      "Iter-57600 train loss: 2.0198 valid loss: 2.0212, valid accuracy: 0.3338\n",
      "Iter-57700 train loss: 1.9476 valid loss: 2.0211, valid accuracy: 0.3338\n",
      "Iter-57800 train loss: 1.9770 valid loss: 2.0213, valid accuracy: 0.3323\n",
      "Iter-57900 train loss: 1.6934 valid loss: 2.0214, valid accuracy: 0.3346\n",
      "Iter-58000 train loss: 2.1548 valid loss: 2.0213, valid accuracy: 0.3338\n",
      "Iter-58100 train loss: 1.8897 valid loss: 2.0214, valid accuracy: 0.3338\n",
      "Iter-58200 train loss: 1.9622 valid loss: 2.0214, valid accuracy: 0.3315\n",
      "Iter-58300 train loss: 2.1568 valid loss: 2.0212, valid accuracy: 0.3346\n",
      "Iter-58400 train loss: 1.8132 valid loss: 2.0210, valid accuracy: 0.3331\n",
      "Iter-58500 train loss: 1.9442 valid loss: 2.0213, valid accuracy: 0.3338\n",
      "Iter-58600 train loss: 1.9131 valid loss: 2.0210, valid accuracy: 0.3323\n",
      "Iter-58700 train loss: 1.8996 valid loss: 2.0208, valid accuracy: 0.3331\n",
      "Iter-58800 train loss: 1.9822 valid loss: 2.0206, valid accuracy: 0.3323\n",
      "Iter-58900 train loss: 1.8446 valid loss: 2.0205, valid accuracy: 0.3331\n",
      "Iter-59000 train loss: 1.6428 valid loss: 2.0205, valid accuracy: 0.3338\n",
      "Iter-59100 train loss: 1.8930 valid loss: 2.0206, valid accuracy: 0.3354\n",
      "Iter-59200 train loss: 2.0559 valid loss: 2.0207, valid accuracy: 0.3338\n",
      "Iter-59300 train loss: 2.1856 valid loss: 2.0207, valid accuracy: 0.3346\n",
      "Iter-59400 train loss: 1.9015 valid loss: 2.0207, valid accuracy: 0.3331\n",
      "Iter-59500 train loss: 1.7528 valid loss: 2.0204, valid accuracy: 0.3331\n",
      "Iter-59600 train loss: 1.6863 valid loss: 2.0204, valid accuracy: 0.3331\n",
      "Iter-59700 train loss: 2.1039 valid loss: 2.0200, valid accuracy: 0.3331\n",
      "Iter-59800 train loss: 1.9336 valid loss: 2.0198, valid accuracy: 0.3338\n",
      "Iter-59900 train loss: 2.0122 valid loss: 2.0197, valid accuracy: 0.3354\n",
      "Iter-60000 train loss: 2.0493 valid loss: 2.0194, valid accuracy: 0.3338\n",
      "Iter-60100 train loss: 1.8450 valid loss: 2.0193, valid accuracy: 0.3331\n",
      "Iter-60200 train loss: 1.8299 valid loss: 2.0190, valid accuracy: 0.3315\n",
      "Iter-60300 train loss: 1.9967 valid loss: 2.0187, valid accuracy: 0.3346\n",
      "Iter-60400 train loss: 2.2315 valid loss: 2.0186, valid accuracy: 0.3362\n",
      "Iter-60500 train loss: 1.8972 valid loss: 2.0186, valid accuracy: 0.3362\n",
      "Iter-60600 train loss: 2.0647 valid loss: 2.0186, valid accuracy: 0.3354\n",
      "Iter-60700 train loss: 2.0124 valid loss: 2.0185, valid accuracy: 0.3331\n",
      "Iter-60800 train loss: 2.1066 valid loss: 2.0185, valid accuracy: 0.3331\n",
      "Iter-60900 train loss: 1.8022 valid loss: 2.0183, valid accuracy: 0.3331\n",
      "Iter-61000 train loss: 1.9264 valid loss: 2.0180, valid accuracy: 0.3362\n",
      "Iter-61100 train loss: 2.0779 valid loss: 2.0180, valid accuracy: 0.3377\n",
      "Iter-61200 train loss: 2.1484 valid loss: 2.0179, valid accuracy: 0.3362\n",
      "Iter-61300 train loss: 2.0659 valid loss: 2.0180, valid accuracy: 0.3346\n",
      "Iter-61400 train loss: 2.0321 valid loss: 2.0177, valid accuracy: 0.3354\n",
      "Iter-61500 train loss: 2.1814 valid loss: 2.0176, valid accuracy: 0.3369\n",
      "Iter-61600 train loss: 2.1109 valid loss: 2.0175, valid accuracy: 0.3338\n",
      "Iter-61700 train loss: 2.0094 valid loss: 2.0175, valid accuracy: 0.3346\n",
      "Iter-61800 train loss: 1.8055 valid loss: 2.0176, valid accuracy: 0.3331\n",
      "Iter-61900 train loss: 2.0888 valid loss: 2.0177, valid accuracy: 0.3315\n",
      "Iter-62000 train loss: 1.8813 valid loss: 2.0174, valid accuracy: 0.3323\n",
      "Iter-62100 train loss: 1.8172 valid loss: 2.0174, valid accuracy: 0.3323\n",
      "Iter-62200 train loss: 2.0292 valid loss: 2.0175, valid accuracy: 0.3308\n",
      "Iter-62300 train loss: 1.9486 valid loss: 2.0174, valid accuracy: 0.3315\n",
      "Iter-62400 train loss: 1.8290 valid loss: 2.0173, valid accuracy: 0.3331\n",
      "Iter-62500 train loss: 1.7405 valid loss: 2.0172, valid accuracy: 0.3323\n",
      "Iter-62600 train loss: 1.8623 valid loss: 2.0170, valid accuracy: 0.3315\n",
      "Iter-62700 train loss: 1.8569 valid loss: 2.0171, valid accuracy: 0.3338\n",
      "Iter-62800 train loss: 1.8698 valid loss: 2.0168, valid accuracy: 0.3331\n",
      "Iter-62900 train loss: 1.8271 valid loss: 2.0167, valid accuracy: 0.3338\n",
      "Iter-63000 train loss: 1.8010 valid loss: 2.0167, valid accuracy: 0.3323\n",
      "Iter-63100 train loss: 1.8556 valid loss: 2.0166, valid accuracy: 0.3331\n",
      "Iter-63200 train loss: 1.9642 valid loss: 2.0169, valid accuracy: 0.3354\n",
      "Iter-63300 train loss: 1.7728 valid loss: 2.0170, valid accuracy: 0.3369\n",
      "Iter-63400 train loss: 1.8503 valid loss: 2.0169, valid accuracy: 0.3369\n",
      "Iter-63500 train loss: 1.9575 valid loss: 2.0168, valid accuracy: 0.3354\n",
      "Iter-63600 train loss: 2.0434 valid loss: 2.0164, valid accuracy: 0.3354\n",
      "Iter-63700 train loss: 2.0772 valid loss: 2.0164, valid accuracy: 0.3354\n",
      "Iter-63800 train loss: 2.1545 valid loss: 2.0166, valid accuracy: 0.3362\n",
      "Iter-63900 train loss: 1.6786 valid loss: 2.0165, valid accuracy: 0.3346\n",
      "Iter-64000 train loss: 1.6890 valid loss: 2.0165, valid accuracy: 0.3338\n",
      "Iter-64100 train loss: 2.1787 valid loss: 2.0164, valid accuracy: 0.3338\n",
      "Iter-64200 train loss: 1.8589 valid loss: 2.0161, valid accuracy: 0.3354\n",
      "Iter-64300 train loss: 1.7647 valid loss: 2.0162, valid accuracy: 0.3385\n",
      "Iter-64400 train loss: 1.8323 valid loss: 2.0161, valid accuracy: 0.3362\n",
      "Iter-64500 train loss: 2.0285 valid loss: 2.0161, valid accuracy: 0.3362\n",
      "Iter-64600 train loss: 2.2330 valid loss: 2.0159, valid accuracy: 0.3369\n",
      "Iter-64700 train loss: 1.7960 valid loss: 2.0157, valid accuracy: 0.3377\n",
      "Iter-64800 train loss: 2.0649 valid loss: 2.0157, valid accuracy: 0.3385\n",
      "Iter-64900 train loss: 2.2129 valid loss: 2.0154, valid accuracy: 0.3369\n",
      "Iter-65000 train loss: 1.7100 valid loss: 2.0153, valid accuracy: 0.3369\n",
      "Iter-65100 train loss: 2.0169 valid loss: 2.0151, valid accuracy: 0.3385\n",
      "Iter-65200 train loss: 2.1785 valid loss: 2.0149, valid accuracy: 0.3385\n",
      "Iter-65300 train loss: 1.8456 valid loss: 2.0148, valid accuracy: 0.3369\n",
      "Iter-65400 train loss: 1.9023 valid loss: 2.0147, valid accuracy: 0.3362\n",
      "Iter-65500 train loss: 2.0696 valid loss: 2.0148, valid accuracy: 0.3369\n",
      "Iter-65600 train loss: 1.9608 valid loss: 2.0146, valid accuracy: 0.3369\n",
      "Iter-65700 train loss: 1.9818 valid loss: 2.0146, valid accuracy: 0.3362\n",
      "Iter-65800 train loss: 1.9437 valid loss: 2.0149, valid accuracy: 0.3362\n",
      "Iter-65900 train loss: 1.8313 valid loss: 2.0145, valid accuracy: 0.3369\n",
      "Iter-66000 train loss: 1.8038 valid loss: 2.0143, valid accuracy: 0.3369\n",
      "Iter-66100 train loss: 1.8545 valid loss: 2.0138, valid accuracy: 0.3369\n",
      "Iter-66200 train loss: 1.9355 valid loss: 2.0138, valid accuracy: 0.3362\n",
      "Iter-66300 train loss: 1.8346 valid loss: 2.0138, valid accuracy: 0.3362\n",
      "Iter-66400 train loss: 1.9118 valid loss: 2.0140, valid accuracy: 0.3362\n",
      "Iter-66500 train loss: 2.0782 valid loss: 2.0137, valid accuracy: 0.3377\n",
      "Iter-66600 train loss: 2.0625 valid loss: 2.0135, valid accuracy: 0.3385\n",
      "Iter-66700 train loss: 1.8325 valid loss: 2.0135, valid accuracy: 0.3385\n",
      "Iter-66800 train loss: 1.9645 valid loss: 2.0133, valid accuracy: 0.3392\n",
      "Iter-66900 train loss: 1.9411 valid loss: 2.0134, valid accuracy: 0.3385\n",
      "Iter-67000 train loss: 1.8030 valid loss: 2.0131, valid accuracy: 0.3377\n",
      "Iter-67100 train loss: 1.8323 valid loss: 2.0128, valid accuracy: 0.3400\n",
      "Iter-67200 train loss: 1.7776 valid loss: 2.0127, valid accuracy: 0.3385\n",
      "Iter-67300 train loss: 1.7647 valid loss: 2.0128, valid accuracy: 0.3369\n",
      "Iter-67400 train loss: 1.9931 valid loss: 2.0127, valid accuracy: 0.3400\n",
      "Iter-67500 train loss: 1.8540 valid loss: 2.0127, valid accuracy: 0.3415\n",
      "Iter-67600 train loss: 1.7385 valid loss: 2.0129, valid accuracy: 0.3408\n",
      "Iter-67700 train loss: 1.7480 valid loss: 2.0130, valid accuracy: 0.3392\n",
      "Iter-67800 train loss: 1.8718 valid loss: 2.0128, valid accuracy: 0.3385\n",
      "Iter-67900 train loss: 2.0061 valid loss: 2.0129, valid accuracy: 0.3385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68000 train loss: 2.0268 valid loss: 2.0129, valid accuracy: 0.3385\n",
      "Iter-68100 train loss: 1.8299 valid loss: 2.0125, valid accuracy: 0.3408\n",
      "Iter-68200 train loss: 1.9055 valid loss: 2.0125, valid accuracy: 0.3408\n",
      "Iter-68300 train loss: 1.9774 valid loss: 2.0125, valid accuracy: 0.3392\n",
      "Iter-68400 train loss: 2.0613 valid loss: 2.0122, valid accuracy: 0.3392\n",
      "Iter-68500 train loss: 1.9266 valid loss: 2.0122, valid accuracy: 0.3400\n",
      "Iter-68600 train loss: 2.1044 valid loss: 2.0120, valid accuracy: 0.3408\n",
      "Iter-68700 train loss: 1.9038 valid loss: 2.0120, valid accuracy: 0.3400\n",
      "Iter-68800 train loss: 2.2392 valid loss: 2.0119, valid accuracy: 0.3369\n",
      "Iter-68900 train loss: 2.0981 valid loss: 2.0120, valid accuracy: 0.3377\n",
      "Iter-69000 train loss: 1.8121 valid loss: 2.0119, valid accuracy: 0.3392\n",
      "Iter-69100 train loss: 2.0279 valid loss: 2.0116, valid accuracy: 0.3385\n",
      "Iter-69200 train loss: 1.6406 valid loss: 2.0116, valid accuracy: 0.3392\n",
      "Iter-69300 train loss: 2.0755 valid loss: 2.0117, valid accuracy: 0.3385\n",
      "Iter-69400 train loss: 2.0751 valid loss: 2.0117, valid accuracy: 0.3377\n",
      "Iter-69500 train loss: 1.8383 valid loss: 2.0113, valid accuracy: 0.3392\n",
      "Iter-69600 train loss: 1.8354 valid loss: 2.0110, valid accuracy: 0.3385\n",
      "Iter-69700 train loss: 2.0403 valid loss: 2.0109, valid accuracy: 0.3392\n",
      "Iter-69800 train loss: 2.0126 valid loss: 2.0112, valid accuracy: 0.3431\n",
      "Iter-69900 train loss: 1.7372 valid loss: 2.0112, valid accuracy: 0.3415\n",
      "Iter-70000 train loss: 2.5056 valid loss: 2.0113, valid accuracy: 0.3431\n",
      "Iter-70100 train loss: 1.9162 valid loss: 2.0112, valid accuracy: 0.3431\n",
      "Iter-70200 train loss: 2.0496 valid loss: 2.0112, valid accuracy: 0.3454\n",
      "Iter-70300 train loss: 2.1365 valid loss: 2.0111, valid accuracy: 0.3431\n",
      "Iter-70400 train loss: 1.9790 valid loss: 2.0106, valid accuracy: 0.3415\n",
      "Iter-70500 train loss: 1.9235 valid loss: 2.0105, valid accuracy: 0.3423\n",
      "Iter-70600 train loss: 1.9471 valid loss: 2.0104, valid accuracy: 0.3438\n",
      "Iter-70700 train loss: 1.8039 valid loss: 2.0105, valid accuracy: 0.3446\n",
      "Iter-70800 train loss: 1.9117 valid loss: 2.0108, valid accuracy: 0.3438\n",
      "Iter-70900 train loss: 1.9313 valid loss: 2.0108, valid accuracy: 0.3454\n",
      "Iter-71000 train loss: 2.0003 valid loss: 2.0107, valid accuracy: 0.3454\n",
      "Iter-71100 train loss: 2.1104 valid loss: 2.0104, valid accuracy: 0.3438\n",
      "Iter-71200 train loss: 1.5997 valid loss: 2.0100, valid accuracy: 0.3423\n",
      "Iter-71300 train loss: 1.9185 valid loss: 2.0096, valid accuracy: 0.3400\n",
      "Iter-71400 train loss: 2.1603 valid loss: 2.0093, valid accuracy: 0.3415\n",
      "Iter-71500 train loss: 2.0240 valid loss: 2.0093, valid accuracy: 0.3415\n",
      "Iter-71600 train loss: 1.9738 valid loss: 2.0094, valid accuracy: 0.3423\n",
      "Iter-71700 train loss: 1.8609 valid loss: 2.0091, valid accuracy: 0.3415\n",
      "Iter-71800 train loss: 1.8991 valid loss: 2.0094, valid accuracy: 0.3431\n",
      "Iter-71900 train loss: 1.9579 valid loss: 2.0090, valid accuracy: 0.3446\n",
      "Iter-72000 train loss: 1.7555 valid loss: 2.0091, valid accuracy: 0.3431\n",
      "Iter-72100 train loss: 1.8009 valid loss: 2.0093, valid accuracy: 0.3454\n",
      "Iter-72200 train loss: 1.8158 valid loss: 2.0091, valid accuracy: 0.3454\n",
      "Iter-72300 train loss: 1.8858 valid loss: 2.0090, valid accuracy: 0.3438\n",
      "Iter-72400 train loss: 2.1392 valid loss: 2.0088, valid accuracy: 0.3431\n",
      "Iter-72500 train loss: 2.0401 valid loss: 2.0088, valid accuracy: 0.3431\n",
      "Iter-72600 train loss: 1.8362 valid loss: 2.0089, valid accuracy: 0.3408\n",
      "Iter-72700 train loss: 1.8760 valid loss: 2.0085, valid accuracy: 0.3415\n",
      "Iter-72800 train loss: 1.9243 valid loss: 2.0084, valid accuracy: 0.3423\n",
      "Iter-72900 train loss: 1.9779 valid loss: 2.0082, valid accuracy: 0.3423\n",
      "Iter-73000 train loss: 1.8768 valid loss: 2.0081, valid accuracy: 0.3415\n",
      "Iter-73100 train loss: 1.9702 valid loss: 2.0078, valid accuracy: 0.3408\n",
      "Iter-73200 train loss: 1.9149 valid loss: 2.0076, valid accuracy: 0.3423\n",
      "Iter-73300 train loss: 1.8644 valid loss: 2.0076, valid accuracy: 0.3446\n",
      "Iter-73400 train loss: 1.9930 valid loss: 2.0078, valid accuracy: 0.3454\n",
      "Iter-73500 train loss: 1.8582 valid loss: 2.0077, valid accuracy: 0.3469\n",
      "Iter-73600 train loss: 1.7013 valid loss: 2.0077, valid accuracy: 0.3438\n",
      "Iter-73700 train loss: 1.8555 valid loss: 2.0078, valid accuracy: 0.3454\n",
      "Iter-73800 train loss: 2.0416 valid loss: 2.0079, valid accuracy: 0.3446\n",
      "Iter-73900 train loss: 1.7336 valid loss: 2.0081, valid accuracy: 0.3446\n",
      "Iter-74000 train loss: 1.7568 valid loss: 2.0078, valid accuracy: 0.3431\n",
      "Iter-74100 train loss: 2.0088 valid loss: 2.0078, valid accuracy: 0.3438\n",
      "Iter-74200 train loss: 1.8880 valid loss: 2.0078, valid accuracy: 0.3446\n",
      "Iter-74300 train loss: 1.9920 valid loss: 2.0078, valid accuracy: 0.3454\n",
      "Iter-74400 train loss: 1.9810 valid loss: 2.0078, valid accuracy: 0.3438\n",
      "Iter-74500 train loss: 1.8964 valid loss: 2.0075, valid accuracy: 0.3415\n",
      "Iter-74600 train loss: 1.7412 valid loss: 2.0071, valid accuracy: 0.3415\n",
      "Iter-74700 train loss: 1.7924 valid loss: 2.0070, valid accuracy: 0.3400\n",
      "Iter-74800 train loss: 1.9593 valid loss: 2.0070, valid accuracy: 0.3415\n",
      "Iter-74900 train loss: 1.7760 valid loss: 2.0069, valid accuracy: 0.3415\n",
      "Iter-75000 train loss: 1.6239 valid loss: 2.0069, valid accuracy: 0.3423\n",
      "Iter-75100 train loss: 1.7757 valid loss: 2.0073, valid accuracy: 0.3431\n",
      "Iter-75200 train loss: 1.8235 valid loss: 2.0069, valid accuracy: 0.3446\n",
      "Iter-75300 train loss: 1.7929 valid loss: 2.0072, valid accuracy: 0.3431\n",
      "Iter-75400 train loss: 1.7664 valid loss: 2.0074, valid accuracy: 0.3415\n",
      "Iter-75500 train loss: 1.8254 valid loss: 2.0073, valid accuracy: 0.3408\n",
      "Iter-75600 train loss: 1.8876 valid loss: 2.0072, valid accuracy: 0.3408\n",
      "Iter-75700 train loss: 2.0614 valid loss: 2.0068, valid accuracy: 0.3385\n",
      "Iter-75800 train loss: 2.0157 valid loss: 2.0068, valid accuracy: 0.3408\n",
      "Iter-75900 train loss: 2.0221 valid loss: 2.0063, valid accuracy: 0.3415\n",
      "Iter-76000 train loss: 1.7226 valid loss: 2.0061, valid accuracy: 0.3415\n",
      "Iter-76100 train loss: 1.9279 valid loss: 2.0058, valid accuracy: 0.3408\n",
      "Iter-76200 train loss: 1.9419 valid loss: 2.0055, valid accuracy: 0.3408\n",
      "Iter-76300 train loss: 1.6808 valid loss: 2.0053, valid accuracy: 0.3408\n",
      "Iter-76400 train loss: 1.7919 valid loss: 2.0053, valid accuracy: 0.3408\n",
      "Iter-76500 train loss: 1.8451 valid loss: 2.0053, valid accuracy: 0.3408\n",
      "Iter-76600 train loss: 1.6841 valid loss: 2.0049, valid accuracy: 0.3415\n",
      "Iter-76700 train loss: 1.9178 valid loss: 2.0047, valid accuracy: 0.3431\n",
      "Iter-76800 train loss: 1.9409 valid loss: 2.0049, valid accuracy: 0.3438\n",
      "Iter-76900 train loss: 1.7797 valid loss: 2.0048, valid accuracy: 0.3423\n",
      "Iter-77000 train loss: 1.9158 valid loss: 2.0047, valid accuracy: 0.3431\n",
      "Iter-77100 train loss: 1.9210 valid loss: 2.0045, valid accuracy: 0.3446\n",
      "Iter-77200 train loss: 1.7377 valid loss: 2.0044, valid accuracy: 0.3438\n",
      "Iter-77300 train loss: 1.7769 valid loss: 2.0043, valid accuracy: 0.3446\n",
      "Iter-77400 train loss: 1.8653 valid loss: 2.0044, valid accuracy: 0.3438\n",
      "Iter-77500 train loss: 2.1557 valid loss: 2.0043, valid accuracy: 0.3431\n",
      "Iter-77600 train loss: 2.0629 valid loss: 2.0039, valid accuracy: 0.3462\n",
      "Iter-77700 train loss: 1.7958 valid loss: 2.0037, valid accuracy: 0.3485\n",
      "Iter-77800 train loss: 1.8116 valid loss: 2.0038, valid accuracy: 0.3485\n",
      "Iter-77900 train loss: 2.0736 valid loss: 2.0039, valid accuracy: 0.3485\n",
      "Iter-78000 train loss: 1.8741 valid loss: 2.0041, valid accuracy: 0.3462\n",
      "Iter-78100 train loss: 1.6601 valid loss: 2.0041, valid accuracy: 0.3462\n",
      "Iter-78200 train loss: 1.9416 valid loss: 2.0043, valid accuracy: 0.3477\n",
      "Iter-78300 train loss: 1.6782 valid loss: 2.0042, valid accuracy: 0.3485\n",
      "Iter-78400 train loss: 1.8975 valid loss: 2.0042, valid accuracy: 0.3477\n",
      "Iter-78500 train loss: 2.0838 valid loss: 2.0041, valid accuracy: 0.3492\n",
      "Iter-78600 train loss: 1.7451 valid loss: 2.0037, valid accuracy: 0.3492\n",
      "Iter-78700 train loss: 1.7738 valid loss: 2.0038, valid accuracy: 0.3485\n",
      "Iter-78800 train loss: 2.0181 valid loss: 2.0038, valid accuracy: 0.3469\n",
      "Iter-78900 train loss: 1.8020 valid loss: 2.0039, valid accuracy: 0.3454\n",
      "Iter-79000 train loss: 1.8590 valid loss: 2.0039, valid accuracy: 0.3492\n",
      "Iter-79100 train loss: 1.9145 valid loss: 2.0036, valid accuracy: 0.3492\n",
      "Iter-79200 train loss: 1.7771 valid loss: 2.0038, valid accuracy: 0.3492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-79300 train loss: 1.8793 valid loss: 2.0036, valid accuracy: 0.3477\n",
      "Iter-79400 train loss: 1.7556 valid loss: 2.0031, valid accuracy: 0.3469\n",
      "Iter-79500 train loss: 2.1988 valid loss: 2.0030, valid accuracy: 0.3469\n",
      "Iter-79600 train loss: 1.9650 valid loss: 2.0031, valid accuracy: 0.3469\n",
      "Iter-79700 train loss: 1.8963 valid loss: 2.0031, valid accuracy: 0.3469\n",
      "Iter-79800 train loss: 1.9435 valid loss: 2.0031, valid accuracy: 0.3477\n",
      "Iter-79900 train loss: 1.9230 valid loss: 2.0029, valid accuracy: 0.3469\n",
      "Iter-80000 train loss: 1.8260 valid loss: 2.0029, valid accuracy: 0.3477\n",
      "Iter-80100 train loss: 1.9495 valid loss: 2.0030, valid accuracy: 0.3462\n",
      "Iter-80200 train loss: 1.7477 valid loss: 2.0030, valid accuracy: 0.3462\n",
      "Iter-80300 train loss: 1.9556 valid loss: 2.0029, valid accuracy: 0.3438\n",
      "Iter-80400 train loss: 2.0053 valid loss: 2.0029, valid accuracy: 0.3446\n",
      "Iter-80500 train loss: 2.3430 valid loss: 2.0026, valid accuracy: 0.3477\n",
      "Iter-80600 train loss: 2.0001 valid loss: 2.0025, valid accuracy: 0.3469\n",
      "Iter-80700 train loss: 2.0477 valid loss: 2.0026, valid accuracy: 0.3462\n",
      "Iter-80800 train loss: 1.9895 valid loss: 2.0022, valid accuracy: 0.3469\n",
      "Iter-80900 train loss: 1.7493 valid loss: 2.0019, valid accuracy: 0.3477\n",
      "Iter-81000 train loss: 1.9205 valid loss: 2.0023, valid accuracy: 0.3454\n",
      "Iter-81100 train loss: 2.2342 valid loss: 2.0023, valid accuracy: 0.3462\n",
      "Iter-81200 train loss: 1.8982 valid loss: 2.0021, valid accuracy: 0.3462\n",
      "Iter-81300 train loss: 1.5605 valid loss: 2.0023, valid accuracy: 0.3454\n",
      "Iter-81400 train loss: 1.9022 valid loss: 2.0023, valid accuracy: 0.3454\n",
      "Iter-81500 train loss: 2.2506 valid loss: 2.0024, valid accuracy: 0.3462\n",
      "Iter-81600 train loss: 1.8219 valid loss: 2.0020, valid accuracy: 0.3454\n",
      "Iter-81700 train loss: 2.0747 valid loss: 2.0023, valid accuracy: 0.3454\n",
      "Iter-81800 train loss: 2.1885 valid loss: 2.0022, valid accuracy: 0.3454\n",
      "Iter-81900 train loss: 1.8754 valid loss: 2.0022, valid accuracy: 0.3462\n",
      "Iter-82000 train loss: 1.9198 valid loss: 2.0020, valid accuracy: 0.3462\n",
      "Iter-82100 train loss: 1.9392 valid loss: 2.0021, valid accuracy: 0.3469\n",
      "Iter-82200 train loss: 2.0899 valid loss: 2.0020, valid accuracy: 0.3477\n",
      "Iter-82300 train loss: 1.8694 valid loss: 2.0019, valid accuracy: 0.3462\n",
      "Iter-82400 train loss: 1.9059 valid loss: 2.0018, valid accuracy: 0.3469\n",
      "Iter-82500 train loss: 1.9903 valid loss: 2.0014, valid accuracy: 0.3477\n",
      "Iter-82600 train loss: 2.0543 valid loss: 2.0011, valid accuracy: 0.3485\n",
      "Iter-82700 train loss: 1.8879 valid loss: 2.0014, valid accuracy: 0.3469\n",
      "Iter-82800 train loss: 2.1037 valid loss: 2.0013, valid accuracy: 0.3469\n",
      "Iter-82900 train loss: 1.9341 valid loss: 2.0014, valid accuracy: 0.3462\n",
      "Iter-83000 train loss: 1.8329 valid loss: 2.0013, valid accuracy: 0.3469\n",
      "Iter-83100 train loss: 1.9407 valid loss: 2.0014, valid accuracy: 0.3454\n",
      "Iter-83200 train loss: 1.9442 valid loss: 2.0011, valid accuracy: 0.3431\n",
      "Iter-83300 train loss: 1.9186 valid loss: 2.0009, valid accuracy: 0.3462\n",
      "Iter-83400 train loss: 1.8689 valid loss: 2.0009, valid accuracy: 0.3477\n",
      "Iter-83500 train loss: 1.9556 valid loss: 2.0010, valid accuracy: 0.3485\n",
      "Iter-83600 train loss: 2.2155 valid loss: 2.0008, valid accuracy: 0.3469\n",
      "Iter-83700 train loss: 1.8649 valid loss: 2.0007, valid accuracy: 0.3469\n",
      "Iter-83800 train loss: 1.7704 valid loss: 2.0007, valid accuracy: 0.3477\n",
      "Iter-83900 train loss: 2.2743 valid loss: 2.0006, valid accuracy: 0.3469\n",
      "Iter-84000 train loss: 2.1030 valid loss: 2.0005, valid accuracy: 0.3454\n",
      "Iter-84100 train loss: 2.0445 valid loss: 2.0004, valid accuracy: 0.3462\n",
      "Iter-84200 train loss: 1.9848 valid loss: 2.0002, valid accuracy: 0.3454\n",
      "Iter-84300 train loss: 2.0760 valid loss: 2.0001, valid accuracy: 0.3446\n",
      "Iter-84400 train loss: 1.8403 valid loss: 2.0001, valid accuracy: 0.3454\n",
      "Iter-84500 train loss: 1.5832 valid loss: 2.0000, valid accuracy: 0.3469\n",
      "Iter-84600 train loss: 2.1701 valid loss: 1.9998, valid accuracy: 0.3477\n",
      "Iter-84700 train loss: 1.8279 valid loss: 1.9997, valid accuracy: 0.3462\n",
      "Iter-84800 train loss: 1.9593 valid loss: 1.9998, valid accuracy: 0.3462\n",
      "Iter-84900 train loss: 1.9053 valid loss: 1.9998, valid accuracy: 0.3477\n",
      "Iter-85000 train loss: 1.7298 valid loss: 1.9996, valid accuracy: 0.3469\n",
      "Iter-85100 train loss: 2.0584 valid loss: 1.9995, valid accuracy: 0.3462\n",
      "Iter-85200 train loss: 1.8045 valid loss: 1.9996, valid accuracy: 0.3454\n",
      "Iter-85300 train loss: 1.7029 valid loss: 1.9997, valid accuracy: 0.3438\n",
      "Iter-85400 train loss: 1.8012 valid loss: 1.9995, valid accuracy: 0.3446\n",
      "Iter-85500 train loss: 1.6664 valid loss: 1.9997, valid accuracy: 0.3462\n",
      "Iter-85600 train loss: 2.0284 valid loss: 1.9997, valid accuracy: 0.3469\n",
      "Iter-85700 train loss: 1.9207 valid loss: 1.9997, valid accuracy: 0.3462\n",
      "Iter-85800 train loss: 2.0208 valid loss: 2.0000, valid accuracy: 0.3477\n",
      "Iter-85900 train loss: 1.6800 valid loss: 1.9999, valid accuracy: 0.3469\n",
      "Iter-86000 train loss: 2.1650 valid loss: 2.0003, valid accuracy: 0.3469\n",
      "Iter-86100 train loss: 1.7660 valid loss: 2.0002, valid accuracy: 0.3469\n",
      "Iter-86200 train loss: 2.0771 valid loss: 2.0005, valid accuracy: 0.3454\n",
      "Iter-86300 train loss: 2.2783 valid loss: 2.0004, valid accuracy: 0.3469\n",
      "Iter-86400 train loss: 1.9591 valid loss: 2.0006, valid accuracy: 0.3446\n",
      "Iter-86500 train loss: 1.8393 valid loss: 2.0007, valid accuracy: 0.3454\n",
      "Iter-86600 train loss: 1.7896 valid loss: 2.0004, valid accuracy: 0.3462\n",
      "Iter-86700 train loss: 1.8972 valid loss: 2.0004, valid accuracy: 0.3462\n",
      "Iter-86800 train loss: 1.8790 valid loss: 2.0005, valid accuracy: 0.3454\n",
      "Iter-86900 train loss: 1.8818 valid loss: 2.0000, valid accuracy: 0.3462\n",
      "Iter-87000 train loss: 1.8693 valid loss: 1.9998, valid accuracy: 0.3438\n",
      "Iter-87100 train loss: 1.6247 valid loss: 1.9999, valid accuracy: 0.3438\n",
      "Iter-87200 train loss: 1.9631 valid loss: 1.9998, valid accuracy: 0.3454\n",
      "Iter-87300 train loss: 1.9830 valid loss: 1.9995, valid accuracy: 0.3446\n",
      "Iter-87400 train loss: 1.6992 valid loss: 1.9993, valid accuracy: 0.3438\n",
      "Iter-87500 train loss: 1.5985 valid loss: 1.9991, valid accuracy: 0.3454\n",
      "Iter-87600 train loss: 2.1222 valid loss: 1.9990, valid accuracy: 0.3438\n",
      "Iter-87700 train loss: 1.9415 valid loss: 1.9990, valid accuracy: 0.3454\n",
      "Iter-87800 train loss: 2.1725 valid loss: 1.9991, valid accuracy: 0.3462\n",
      "Iter-87900 train loss: 1.9298 valid loss: 1.9991, valid accuracy: 0.3454\n",
      "Iter-88000 train loss: 2.2523 valid loss: 1.9991, valid accuracy: 0.3462\n",
      "Iter-88100 train loss: 1.8837 valid loss: 1.9988, valid accuracy: 0.3462\n",
      "Iter-88200 train loss: 2.1039 valid loss: 1.9987, valid accuracy: 0.3454\n",
      "Iter-88300 train loss: 1.5943 valid loss: 1.9987, valid accuracy: 0.3438\n",
      "Iter-88400 train loss: 2.1226 valid loss: 1.9988, valid accuracy: 0.3454\n",
      "Iter-88500 train loss: 1.6342 valid loss: 1.9985, valid accuracy: 0.3454\n",
      "Iter-88600 train loss: 1.9557 valid loss: 1.9984, valid accuracy: 0.3469\n",
      "Iter-88700 train loss: 2.1892 valid loss: 1.9987, valid accuracy: 0.3438\n",
      "Iter-88800 train loss: 1.6453 valid loss: 1.9988, valid accuracy: 0.3438\n",
      "Iter-88900 train loss: 2.0653 valid loss: 1.9986, valid accuracy: 0.3438\n",
      "Iter-89000 train loss: 1.8386 valid loss: 1.9985, valid accuracy: 0.3462\n",
      "Iter-89100 train loss: 2.0070 valid loss: 1.9984, valid accuracy: 0.3462\n",
      "Iter-89200 train loss: 1.7784 valid loss: 1.9981, valid accuracy: 0.3469\n",
      "Iter-89300 train loss: 2.0278 valid loss: 1.9983, valid accuracy: 0.3462\n",
      "Iter-89400 train loss: 1.6885 valid loss: 1.9983, valid accuracy: 0.3454\n",
      "Iter-89500 train loss: 1.8819 valid loss: 1.9977, valid accuracy: 0.3454\n",
      "Iter-89600 train loss: 1.6303 valid loss: 1.9979, valid accuracy: 0.3469\n",
      "Iter-89700 train loss: 1.9500 valid loss: 1.9978, valid accuracy: 0.3454\n",
      "Iter-89800 train loss: 1.7773 valid loss: 1.9976, valid accuracy: 0.3469\n",
      "Iter-89900 train loss: 2.1646 valid loss: 1.9974, valid accuracy: 0.3446\n",
      "Iter-90000 train loss: 1.7253 valid loss: 1.9973, valid accuracy: 0.3446\n",
      "Iter-90100 train loss: 2.0197 valid loss: 1.9974, valid accuracy: 0.3469\n",
      "Iter-90200 train loss: 1.7698 valid loss: 1.9973, valid accuracy: 0.3446\n",
      "Iter-90300 train loss: 1.5226 valid loss: 1.9976, valid accuracy: 0.3462\n",
      "Iter-90400 train loss: 2.1674 valid loss: 1.9975, valid accuracy: 0.3462\n",
      "Iter-90500 train loss: 1.7965 valid loss: 1.9974, valid accuracy: 0.3446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-90600 train loss: 1.9422 valid loss: 1.9974, valid accuracy: 0.3438\n",
      "Iter-90700 train loss: 2.0892 valid loss: 1.9973, valid accuracy: 0.3415\n",
      "Iter-90800 train loss: 1.9383 valid loss: 1.9972, valid accuracy: 0.3438\n",
      "Iter-90900 train loss: 1.7170 valid loss: 1.9972, valid accuracy: 0.3454\n",
      "Iter-91000 train loss: 1.9684 valid loss: 1.9972, valid accuracy: 0.3431\n",
      "Iter-91100 train loss: 1.8422 valid loss: 1.9972, valid accuracy: 0.3454\n",
      "Iter-91200 train loss: 1.9010 valid loss: 1.9972, valid accuracy: 0.3446\n",
      "Iter-91300 train loss: 1.9168 valid loss: 1.9973, valid accuracy: 0.3423\n",
      "Iter-91400 train loss: 1.7891 valid loss: 1.9974, valid accuracy: 0.3446\n",
      "Iter-91500 train loss: 2.1152 valid loss: 1.9973, valid accuracy: 0.3400\n",
      "Iter-91600 train loss: 1.9728 valid loss: 1.9972, valid accuracy: 0.3431\n",
      "Iter-91700 train loss: 2.1179 valid loss: 1.9971, valid accuracy: 0.3423\n",
      "Iter-91800 train loss: 1.9754 valid loss: 1.9968, valid accuracy: 0.3423\n",
      "Iter-91900 train loss: 1.7734 valid loss: 1.9969, valid accuracy: 0.3423\n",
      "Iter-92000 train loss: 1.7548 valid loss: 1.9970, valid accuracy: 0.3400\n",
      "Iter-92100 train loss: 1.6502 valid loss: 1.9970, valid accuracy: 0.3415\n",
      "Iter-92200 train loss: 2.0795 valid loss: 1.9969, valid accuracy: 0.3408\n",
      "Iter-92300 train loss: 1.9751 valid loss: 1.9968, valid accuracy: 0.3423\n",
      "Iter-92400 train loss: 2.1055 valid loss: 1.9967, valid accuracy: 0.3423\n",
      "Iter-92500 train loss: 2.0335 valid loss: 1.9965, valid accuracy: 0.3431\n",
      "Iter-92600 train loss: 1.9562 valid loss: 1.9967, valid accuracy: 0.3415\n",
      "Iter-92700 train loss: 2.1412 valid loss: 1.9968, valid accuracy: 0.3431\n",
      "Iter-92800 train loss: 1.9590 valid loss: 1.9967, valid accuracy: 0.3446\n",
      "Iter-92900 train loss: 1.8029 valid loss: 1.9968, valid accuracy: 0.3438\n",
      "Iter-93000 train loss: 2.1217 valid loss: 1.9966, valid accuracy: 0.3438\n",
      "Iter-93100 train loss: 1.8650 valid loss: 1.9966, valid accuracy: 0.3438\n",
      "Iter-93200 train loss: 1.6847 valid loss: 1.9967, valid accuracy: 0.3438\n",
      "Iter-93300 train loss: 1.9909 valid loss: 1.9964, valid accuracy: 0.3415\n",
      "Iter-93400 train loss: 2.0894 valid loss: 1.9965, valid accuracy: 0.3423\n",
      "Iter-93500 train loss: 1.8720 valid loss: 1.9964, valid accuracy: 0.3446\n",
      "Iter-93600 train loss: 2.1213 valid loss: 1.9964, valid accuracy: 0.3431\n",
      "Iter-93700 train loss: 1.8523 valid loss: 1.9965, valid accuracy: 0.3423\n",
      "Iter-93800 train loss: 1.7835 valid loss: 1.9963, valid accuracy: 0.3438\n",
      "Iter-93900 train loss: 1.8969 valid loss: 1.9963, valid accuracy: 0.3423\n",
      "Iter-94000 train loss: 1.6229 valid loss: 1.9962, valid accuracy: 0.3431\n",
      "Iter-94100 train loss: 2.3263 valid loss: 1.9961, valid accuracy: 0.3423\n",
      "Iter-94200 train loss: 1.5269 valid loss: 1.9961, valid accuracy: 0.3438\n",
      "Iter-94300 train loss: 1.9649 valid loss: 1.9963, valid accuracy: 0.3431\n",
      "Iter-94400 train loss: 1.9719 valid loss: 1.9964, valid accuracy: 0.3408\n",
      "Iter-94500 train loss: 1.9624 valid loss: 1.9966, valid accuracy: 0.3423\n",
      "Iter-94600 train loss: 1.7836 valid loss: 1.9964, valid accuracy: 0.3423\n",
      "Iter-94700 train loss: 1.8972 valid loss: 1.9963, valid accuracy: 0.3415\n",
      "Iter-94800 train loss: 2.0469 valid loss: 1.9962, valid accuracy: 0.3438\n",
      "Iter-94900 train loss: 1.7624 valid loss: 1.9963, valid accuracy: 0.3438\n",
      "Iter-95000 train loss: 2.1709 valid loss: 1.9963, valid accuracy: 0.3415\n",
      "Iter-95100 train loss: 1.8966 valid loss: 1.9962, valid accuracy: 0.3415\n",
      "Iter-95200 train loss: 1.9221 valid loss: 1.9961, valid accuracy: 0.3423\n",
      "Iter-95300 train loss: 1.9311 valid loss: 1.9961, valid accuracy: 0.3438\n",
      "Iter-95400 train loss: 1.7314 valid loss: 1.9961, valid accuracy: 0.3408\n",
      "Iter-95500 train loss: 1.7945 valid loss: 1.9959, valid accuracy: 0.3415\n",
      "Iter-95600 train loss: 2.0549 valid loss: 1.9960, valid accuracy: 0.3454\n",
      "Iter-95700 train loss: 2.2447 valid loss: 1.9962, valid accuracy: 0.3431\n",
      "Iter-95800 train loss: 2.0462 valid loss: 1.9962, valid accuracy: 0.3438\n",
      "Iter-95900 train loss: 1.8859 valid loss: 1.9961, valid accuracy: 0.3431\n",
      "Iter-96000 train loss: 2.3433 valid loss: 1.9959, valid accuracy: 0.3438\n",
      "Iter-96100 train loss: 1.9935 valid loss: 1.9959, valid accuracy: 0.3438\n",
      "Iter-96200 train loss: 1.9807 valid loss: 1.9958, valid accuracy: 0.3438\n",
      "Iter-96300 train loss: 1.8249 valid loss: 1.9956, valid accuracy: 0.3438\n",
      "Iter-96400 train loss: 1.7796 valid loss: 1.9955, valid accuracy: 0.3454\n",
      "Iter-96500 train loss: 1.7621 valid loss: 1.9957, valid accuracy: 0.3423\n",
      "Iter-96600 train loss: 2.1506 valid loss: 1.9953, valid accuracy: 0.3431\n",
      "Iter-96700 train loss: 2.0642 valid loss: 1.9955, valid accuracy: 0.3423\n",
      "Iter-96800 train loss: 1.7329 valid loss: 1.9952, valid accuracy: 0.3423\n",
      "Iter-96900 train loss: 2.0950 valid loss: 1.9954, valid accuracy: 0.3400\n",
      "Iter-97000 train loss: 1.8949 valid loss: 1.9955, valid accuracy: 0.3408\n",
      "Iter-97100 train loss: 1.8737 valid loss: 1.9952, valid accuracy: 0.3438\n",
      "Iter-97200 train loss: 1.8436 valid loss: 1.9950, valid accuracy: 0.3446\n",
      "Iter-97300 train loss: 1.9078 valid loss: 1.9949, valid accuracy: 0.3423\n",
      "Iter-97400 train loss: 2.0597 valid loss: 1.9942, valid accuracy: 0.3423\n",
      "Iter-97500 train loss: 2.1644 valid loss: 1.9940, valid accuracy: 0.3431\n",
      "Iter-97600 train loss: 1.8686 valid loss: 1.9939, valid accuracy: 0.3438\n",
      "Iter-97700 train loss: 1.5766 valid loss: 1.9939, valid accuracy: 0.3431\n",
      "Iter-97800 train loss: 2.0463 valid loss: 1.9940, valid accuracy: 0.3469\n",
      "Iter-97900 train loss: 1.7209 valid loss: 1.9942, valid accuracy: 0.3462\n",
      "Iter-98000 train loss: 1.8680 valid loss: 1.9941, valid accuracy: 0.3454\n",
      "Iter-98100 train loss: 1.6451 valid loss: 1.9943, valid accuracy: 0.3477\n",
      "Iter-98200 train loss: 1.7533 valid loss: 1.9942, valid accuracy: 0.3492\n",
      "Iter-98300 train loss: 2.1029 valid loss: 1.9940, valid accuracy: 0.3454\n",
      "Iter-98400 train loss: 1.7886 valid loss: 1.9939, valid accuracy: 0.3462\n",
      "Iter-98500 train loss: 1.9144 valid loss: 1.9938, valid accuracy: 0.3446\n",
      "Iter-98600 train loss: 1.9293 valid loss: 1.9938, valid accuracy: 0.3446\n",
      "Iter-98700 train loss: 1.8059 valid loss: 1.9937, valid accuracy: 0.3454\n",
      "Iter-98800 train loss: 1.8788 valid loss: 1.9935, valid accuracy: 0.3469\n",
      "Iter-98900 train loss: 1.8359 valid loss: 1.9938, valid accuracy: 0.3477\n",
      "Iter-99000 train loss: 2.1744 valid loss: 1.9937, valid accuracy: 0.3469\n",
      "Iter-99100 train loss: 1.8129 valid loss: 1.9938, valid accuracy: 0.3477\n",
      "Iter-99200 train loss: 1.7677 valid loss: 1.9937, valid accuracy: 0.3446\n",
      "Iter-99300 train loss: 1.7960 valid loss: 1.9937, valid accuracy: 0.3469\n",
      "Iter-99400 train loss: 1.9344 valid loss: 1.9934, valid accuracy: 0.3469\n",
      "Iter-99500 train loss: 1.5880 valid loss: 1.9932, valid accuracy: 0.3454\n",
      "Iter-99600 train loss: 1.8547 valid loss: 1.9929, valid accuracy: 0.3462\n",
      "Iter-99700 train loss: 1.8750 valid loss: 1.9932, valid accuracy: 0.3454\n",
      "Iter-99800 train loss: 1.9300 valid loss: 1.9928, valid accuracy: 0.3454\n",
      "Iter-99900 train loss: 1.8648 valid loss: 1.9930, valid accuracy: 0.3469\n",
      "Iter-100000 train loss: 1.9010 valid loss: 1.9928, valid accuracy: 0.3477\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = Y_train.max() + 1 # number of classes in this classification problem\n",
    "# num_output_units = Y_train.shape[1] # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, Y_train), val_set=(X_val, Y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUQAVCIqiKIoPFQiKoigKAP8PkofuEghOXEEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cb3252438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEACAYAAACtVTGuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhSChQiIpKUAoWIiCSlQCEiIkn9fxewsU3OVNLaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8cb3252ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the final model\n",
    "# y_pred, y_logit = nn.test(X_test)\n",
    "# loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "# acc = np.mean(y_pred == y_test)\n",
    "# print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "#     acc.mean(), acc.std(), loss))\n",
    "# y_pred, y_logits = nn.test(X_test)\n",
    "# mplot.imsave(y_pred)\n",
    "# pd.DataFrame.to_csv(y_pred)\n",
    "# y_pred.shape\n",
    "# import numpy\n",
    "# a = numpy.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "# numpy.savetxt(\"foo.csv\", a, delimiter=\",\")\n",
    "# np.savetxt(X=y_pred, delimiter=\",\", fname='y_predddddddddddddddddd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBCI-HW1\u001b[0m/                                 environment.yml\r\n",
      "\u001b[01;34mBCI-HW2\u001b[0m/                                 foo.csv\r\n",
      "\u001b[01;34mbin\u001b[0m/                                     gradient_descent.py\r\n",
      "\u001b[01;34mbio-bp-dl\u001b[0m/                               \u001b[01;34mimpl\u001b[0m/\r\n",
      "confusion_mat_cov.ipynb                  \u001b[01;34mimpl_imagernn_karpathy\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/                                    LICENSE\r\n",
      "DCNN.ipynb                               minimal_net.ipynb\r\n",
      "Deep-FFNN-Tanh-FBA.ipynb                 \u001b[01;34mmisc\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD.ipynb             \u001b[01;34mmisc2\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD-Kaggle.ipynb      NOTES\r\n",
      "Deep-FFNN-Tanh-FBA-Kaggle.ipynb          numba-cuda-gpu-example.ipynb\r\n",
      "Deep-FFNN-Tanh-Vanilla.ipynb             output.csv\r\n",
      "Deep-FFNN-Tanh-Vanilla-Kaggle.ipynb      README.md\r\n",
      "DFFNN-FBA-STDP-Tanh-diff.ipynb           \u001b[01;34mrnn-testing-platform\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-diff-no_FBA.ipynb    submission.csv\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff2.ipynb  \u001b[01;34mtf-based\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff.ipynb   y_predddddddddddddddddd2.csv\r\n",
      "\u001b[01;34mDFFNNs\u001b[0m/                                  y_predddddddddddddddddd3.csv\r\n",
      "\u001b[01;34mDGRUs\u001b[0m/                                   y_predddddddddddddddddd.csv\r\n",
      "\u001b[01;34mDGRUs_old\u001b[0m/                               y_prob.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Latin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vocal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id    class_label\n",
       "0   1  International\n",
       "1   2          Vocal\n",
       "2   3          Latin\n",
       "3   4          Blues\n",
       "4   5          Vocal"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = []\n",
    "# labels.append(('Id', 'class_label')) # Id starts with 1\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(y_pred.shape[0]): # y_txn\n",
    "# # for idx in range(10): # y_txn\n",
    "# # for each in y_pred[1000:1100]:\n",
    "# #     print(val_to_key[y_pred[idx]])\n",
    "#     labels.append((idx+1, val_to_key[y_pred[idx]]))\n",
    "# #     labels.append([idx+1, val_to_key[y_pred[idx]]])\n",
    "# #     labels.append(val_to_key[y_pred[idx]])\n",
    "\n",
    "# len(labels), X_test.shape, labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels = np.array(labels)\n",
    "# pred_labels.shape, pred_labels.dtype, pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import numpy as np\n",
    "# # a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "# pred_labels.tofile('foo.csv',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Writing down the the prediction into the 'csv' file\n",
    "# # np.savetxt(delimiter=',', fname='submission.csv', X=pred_labels) \n",
    "# # import csv\n",
    "\n",
    "# # Finding the corresponding point to the time equivalent\n",
    "# import csv\n",
    "# RESULT = ['apple','cherry','orange','pineapple','strawberry']\n",
    "# with open(\"output.csv\",'wb') as resultFile:\n",
    "#     wr = csv.writer(resultFile, dialect='excel')\n",
    "#     wr.writerow(RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define Data\n",
    "# RESULTS = labels #['apple','cherry','orange','pineapple','strawberry']\n",
    "\n",
    "# # Open File\n",
    "# resultFyle = open(\"output.csv\",'w')\n",
    "\n",
    "# # Write data to file\n",
    "# for r in RESULTS:\n",
    "#     resultFyle.write(r + \"\\n\")\n",
    "# resultFyle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBCI-HW1\u001b[0m/                                 environment.yml\r\n",
      "\u001b[01;34mBCI-HW2\u001b[0m/                                 foo.csv\r\n",
      "\u001b[01;34mbin\u001b[0m/                                     gradient_descent.py\r\n",
      "\u001b[01;34mbio-bp-dl\u001b[0m/                               \u001b[01;34mimpl\u001b[0m/\r\n",
      "confusion_mat_cov.ipynb                  \u001b[01;34mimpl_imagernn_karpathy\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/                                    LICENSE\r\n",
      "DCNN.ipynb                               minimal_net.ipynb\r\n",
      "Deep-FFNN-Tanh-FBA.ipynb                 \u001b[01;34mmisc\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD.ipynb             \u001b[01;34mmisc2\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD-Kaggle.ipynb      NOTES\r\n",
      "Deep-FFNN-Tanh-FBA-Kaggle.ipynb          numba-cuda-gpu-example.ipynb\r\n",
      "Deep-FFNN-Tanh-Vanilla.ipynb             output.csv\r\n",
      "Deep-FFNN-Tanh-Vanilla-Kaggle.ipynb      README.md\r\n",
      "DFFNN-FBA-STDP-Tanh-diff.ipynb           \u001b[01;34mrnn-testing-platform\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-diff-no_FBA.ipynb    submission.csv\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff2.ipynb  \u001b[01;34mtf-based\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff.ipynb   y_predddddddddddddddddd2.csv\r\n",
      "\u001b[01;34mDFFNNs\u001b[0m/                                  y_predddddddddddddddddd3.csv\r\n",
      "\u001b[01;34mDGRUs\u001b[0m/                                   y_predddddddddddddddddd.csv\r\n",
      "\u001b[01;34mDGRUs_old\u001b[0m/                               y_prob.csv\r\n"
     ]
    }
   ],
   "source": [
    "% ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10400, 13), (10400, 26))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_logits = nn.test(X_test)\n",
    "y_logits.shape\n",
    "y_prob = l.softmax(y_logits)\n",
    "y_prob.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.shape, y_prob.dtype, y_prob[:2]\n",
    "y_prob.tofile(file='y_prob.csv', sep=',', format='%5.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mBCI-HW1\u001b[0m/                                 environment.yml\r\n",
      "\u001b[01;34mBCI-HW2\u001b[0m/                                 foo.csv\r\n",
      "\u001b[01;34mbin\u001b[0m/                                     gradient_descent.py\r\n",
      "\u001b[01;34mbio-bp-dl\u001b[0m/                               \u001b[01;34mimpl\u001b[0m/\r\n",
      "confusion_mat_cov.ipynb                  \u001b[01;34mimpl_imagernn_karpathy\u001b[0m/\r\n",
      "\u001b[01;34mdata\u001b[0m/                                    LICENSE\r\n",
      "DCNN.ipynb                               minimal_net.ipynb\r\n",
      "Deep-FFNN-Tanh-FBA.ipynb                 \u001b[01;34mmisc\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD.ipynb             \u001b[01;34mmisc2\u001b[0m/\r\n",
      "Deep-FFNN-Tanh-FBA-ITD-Kaggle.ipynb      NOTES\r\n",
      "Deep-FFNN-Tanh-FBA-Kaggle.ipynb          numba-cuda-gpu-example.ipynb\r\n",
      "Deep-FFNN-Tanh-Vanilla.ipynb             output.csv\r\n",
      "Deep-FFNN-Tanh-Vanilla-Kaggle.ipynb      README.md\r\n",
      "DFFNN-FBA-STDP-Tanh-diff.ipynb           \u001b[01;34mrnn-testing-platform\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-diff-no_FBA.ipynb    submission.csv\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff2.ipynb  \u001b[01;34mtf-based\u001b[0m/\r\n",
      "DFFNN-FBA-STDP-Tanh-TemporalDiff.ipynb   y_predddddddddddddddddd2.csv\r\n",
      "\u001b[01;34mDFFNNs\u001b[0m/                                  y_predddddddddddddddddd3.csv\r\n",
      "\u001b[01;34mDGRUs\u001b[0m/                                   y_predddddddddddddddddd.csv\r\n",
      "\u001b[01;34mDGRUs_old\u001b[0m/                               y_prob.csv\r\n"
     ]
    }
   ],
   "source": [
    "%ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample =  np.fromfile('../Downloads/submission-random.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(filepath_or_buffer='../Downloads/submission-random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Blues</th>\n",
       "      <th>Country</th>\n",
       "      <th>Electronic</th>\n",
       "      <th>Folk</th>\n",
       "      <th>International</th>\n",
       "      <th>Jazz</th>\n",
       "      <th>Latin</th>\n",
       "      <th>New_Age</th>\n",
       "      <th>Pop_Rock</th>\n",
       "      <th>Rap</th>\n",
       "      <th>Reggae</th>\n",
       "      <th>RnB</th>\n",
       "      <th>Vocal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.1400</td>\n",
       "      <td>0.0073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>0.0691</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.1283</td>\n",
       "      <td>0.0238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0627</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0812</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0667</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.1251</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.1012</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   Blues  Country  Electronic    Folk  International    Jazz   Latin  \\\n",
       "0   1  0.0964   0.0884      0.0121  0.1004         0.0137  0.1214  0.0883   \n",
       "1   2  0.0121   0.0804      0.0376  0.0289         0.1310  0.0684  0.1044   \n",
       "2   3  0.1291   0.0985      0.0691  0.0356         0.0788  0.0529  0.1185   \n",
       "3   4  0.0453   0.1234      0.0931  0.0126         0.1224  0.0627  0.0269   \n",
       "4   5  0.0600   0.0915      0.0667  0.0947         0.0509  0.0335  0.1251   \n",
       "\n",
       "   New_Age  Pop_Rock     Rap  Reggae     RnB   Vocal  \n",
       "0   0.0765    0.0332  0.0445  0.1193  0.1019  0.1038  \n",
       "1   0.0118    0.1562  0.0585  0.1633  0.1400  0.0073  \n",
       "2   0.1057    0.1041  0.0075  0.0481  0.1283  0.0238  \n",
       "3   0.0764    0.0812  0.1337  0.0357  0.0937  0.0930  \n",
       "4   0.0202    0.1012  0.0365  0.1310  0.0898  0.0991  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Pop_Rock', 5)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_to_key[0], key_to_val['Blues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = pd.read_csv(filepath_or_buffer='y_prob.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.00317</th>\n",
       "      <th>0.01518</th>\n",
       "      <th>0.46551</th>\n",
       "      <th>0.01597</th>\n",
       "      <th>0.00730</th>\n",
       "      <th>0.01441</th>\n",
       "      <th>0.07149</th>\n",
       "      <th>0.01446</th>\n",
       "      <th>0.32313</th>\n",
       "      <th>0.00510</th>\n",
       "      <th>...</th>\n",
       "      <th>0.01342.18</th>\n",
       "      <th>0.22801.1</th>\n",
       "      <th>0.14515.2</th>\n",
       "      <th>0.02259.11</th>\n",
       "      <th>0.05781.9</th>\n",
       "      <th>0.01140.21</th>\n",
       "      <th>0.16361.1</th>\n",
       "      <th>0.02778.14</th>\n",
       "      <th>0.02164.8</th>\n",
       "      <th>0.01738.18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 135200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0.00317, 0.01518, 0.46551, 0.01597, 0.00730, 0.01441, 0.07149, 0.01446, 0.32313, 0.00510, 0.00107, 0.04692, 0.01629, 0.00823, 0.06148, 0.14456, 0.01124, 0.02515, 0.02763, 0.05439, 0.05128, 0.48560, 0.02344, 0.00241, 0.09850, 0.00607, 0.03383, 0.00601, 0.25815, 0.06245, 0.01236, 0.01418, 0.08862, 0.03961, 0.41734, 0.00725, 0.00141, 0.05672, 0.00207, 0.01632, 0.10631, 0.05016, 0.13406, 0.02545, 0.04688, 0.09564, 0.04325, 0.37643, 0.06109, 0.00697, 0.02396, 0.01349, 0.00958, 0.00387, 0.47537, 0.02084, 0.00701, 0.01177, 0.02561, 0.01122, 0.39854, 0.00310, 0.00040, 0.03130, 0.00139, 0.03053, 0.00324, 0.39947, 0.14309, 0.00804, 0.01427, 0.05040, 0.01927, 0.28808, 0.00475, 0.00060, 0.03590, 0.00236, 0.02770, 0.04806, 0.14266, 0.02327, 0.04844, 0.02872, 0.18310, 0.07437, 0.26269, 0.02435, 0.00271, 0.12976, 0.00418, 0.00522, 0.02864, 0.09106, 0.04066, 0.01372, 0.05629, 0.03943, 0.01530, 0.65218, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 135200 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
