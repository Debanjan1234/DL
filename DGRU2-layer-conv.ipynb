{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3629,),\n",
       " (3629,),\n",
       " array([38, 40, 70, 40, 65,  9, 32, 38, 40, 70]),\n",
       " array([40, 70, 40, 65,  9, 32, 38, 40, 70, 40]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# Looking at the X, y\n",
    "X.shape, y.shape, X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.), \n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "        # Conv model parameters\n",
    "        kernel_size = 9\n",
    "        # y = X @ W + b # X_txn.T @ W_tx1 + b_nx1 = y_nx1\n",
    "        m = dict(\n",
    "            W = np.random.randn(kernel_size, 1) / np.sqrt(kernel_size / 2.),\n",
    "            b = np.random.randn(1, 1) / np.sqrt(1 / 2.)\n",
    "        )\n",
    "        self.model_conv = []\n",
    "        for _ in range(self.L):\n",
    "            self.model_conv.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        # h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        \n",
    "        dhh = hz * dh\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        # dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        dhz = (hh - h_in) * dh\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXh + dXz\n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "        \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches, caches_conv = [], [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            caches_conv.append([])\n",
    "        \n",
    "        # Embedding, Input layer, 1st layer\n",
    "        Xs = []\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            Xs.append(X)\n",
    "        \n",
    "        for layer in range(self.L):\n",
    "            ys = []\n",
    "            # Convolution RNN forward\n",
    "            # Preparation for convolutional layer\n",
    "            # np.pad for padding rows & cols and all dimensions/axis\n",
    "            # colstack/rowstack for i,j,k, ... stack\n",
    "            Xs = np.array(Xs).reshape(len(Xs), -1)\n",
    "#             print('Xs.shape', Xs.shape)\n",
    "            #             Xs_pad = np.pad(Xs, (kernel_size// 2, kernel_size// 2), 'constant', constant_values=(0., 0.))\n",
    "            #             print('Xs_pad.shape', Xs_pad.shape)\n",
    "            #             zero_pads = np.zeros((kernel_size// 2, Xs.shape[1])\n",
    "            #             print('zero_pads.shape', zero_pads.shape)\n",
    "            # Convolution layer requirements\n",
    "            kernel_size = 9\n",
    "            n = Xs.shape[1] # Xs_txn\n",
    "            pad = np.zeros((kernel_size//2, n))\n",
    "            Xs_pad = np.row_stack((pad, Xs, pad))\n",
    "#             print('Xs_pad.shape', Xs_pad.shape)\n",
    "\n",
    "            for i in range(0, len(Xs_pad) - kernel_size + 1, 1):\n",
    "                X = Xs_pad[i: i + kernel_size] # X_txn\n",
    "#                 print('X.shape', X.shape)\n",
    "                # y = X @ W + b # X_txn.T @ W_tx1 + b_nx1 = y_nx1\n",
    "                y, cache = l.fc_forward(X.T, self.model_conv[layer]['W'], self.model_conv[layer]['b'])\n",
    "                caches_conv[layer].append(cache)\n",
    "                X = y.reshape(1, -1).copy() # X_1xn\n",
    "#                 print('X.shape', X.shape)\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                ys.append(y)\n",
    "            Xs = ys.copy()\n",
    "\n",
    "        ys_caches = caches, caches_conv\n",
    "\n",
    "        return ys, ys_caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, ys_caches):\n",
    "        dh, grad, grads, grads_conv = [], [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads_conv.append({key: np.zeros_like(val) for key, val in self.model_conv[layer].items()})\n",
    "        \n",
    "        caches, caches_conv = ys_caches\n",
    "        \n",
    "        for layer in reversed(range(self.L)):\n",
    "            # Convolution RNN forward\n",
    "            kernel_size = 9\n",
    "#             n = Xs.shape[1] # Xs_txn\n",
    "            n = dys[0].shape[1] # y_1xn\n",
    "            t = len(dys)\n",
    "            dXs = np.zeros((t, n))\n",
    "            print('dXs.shape', dXs.shape)\n",
    "#             dXs_pad = np.pad(dXs, (kernel_size//2, kernel_size//2), 'constant', constant_values=(0., 0.))\n",
    "            pad = np.zeros((kernel_size//2, n))\n",
    "            dXs_pad = np.row_stack((pad, dXs, pad))\n",
    "            print('dXs_pad.shape', dXs_pad.shape)\n",
    "\n",
    "\n",
    "#             for t in reversed(range(len(dys))):\n",
    "#                 dy = dys[t]\n",
    "#                 dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "#                 for k in grad[layer].keys():\n",
    "#                     grads[layer][k] += grad[layer][k]\n",
    "#                 dy = dX.copy()\n",
    "#                 dX, dW, db = l.fc_backward(dy.T, caches_conv[layer][t])\n",
    "#                 grads_conv[layer]['W'] += dW\n",
    "#                 grads_conv[layer]['b'] += db\n",
    "#                 for i in range(t, t + kernel_size, 1):\n",
    "#                     np.add.at(dXs_pad, [i], dX[i-t].T)\n",
    "#             dXs = dXs_pad[kernel_size//2:-kernel_size//2]\n",
    "#             dys = dXs.copy()\n",
    "                \n",
    "        return dXs, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    #for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "#             nn.losses['train'].append(loss)\n",
    "#             smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "#             nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "#             for layer in range(nn.L):\n",
    "#                 for k in grads[layer].keys(): #key, value: items\n",
    "#                     M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "#                     R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "#                     m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "#                     r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "#                     nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "#         # Print loss and test sample\n",
    "#         if iter % print_after == 0:\n",
    "#             print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "#             #             sample = nn.test(X_mini[0], state, 100)\n",
    "#             #             print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1158bffd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 1 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
