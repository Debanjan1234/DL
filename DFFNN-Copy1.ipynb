{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10000,), (5000,), (55000,))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "y_test.shape, y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing\n",
    "def prepro(X_train, X_val, X_test):\n",
    "    mean = np.mean(X_train)\n",
    "    # scale = 255. - mean # std or sqrt(var), 255 == 2**8 or 8 bit grayscale\n",
    "    # return (X_train - mean)/ scale, (X_val - mean)/ scale, (X_test - mean) / scale\n",
    "    return X_train - mean, X_val - mean, X_test - mean\n",
    "\n",
    "def selu_forward(X):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "def selu_backward(dout, cache):\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    X = cache\n",
    "    dX_pos = dout.copy()\n",
    "    dX_pos[X<0] = 0\n",
    "    dX_neg = dout.copy()\n",
    "    dX_neg[X>0] = 0\n",
    "    dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "    return dX\n",
    "\n",
    "# p_dropout = keep_prob\n",
    "def selu_dropout_forward(h, q):\n",
    "    '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "    alpha = 1.6732632423543772848170429916717\n",
    "    scale = 1.0507009873554804934193349852946\n",
    "    alpha_p = -scale * alpha\n",
    "    mask = np.random.binomial(1, q, size=h.shape)\n",
    "    dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "    a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "    b = -a * (1 - q) * alpha_p\n",
    "    out = (a * dropped) + b\n",
    "    cache = (a, mask)\n",
    "    return out, cache\n",
    "\n",
    "def selu_dropout_backward(dout, cache):\n",
    "    a, mask = cache\n",
    "    d_dropped = dout * a\n",
    "    dh = d_dropped * mask\n",
    "    return dh\n",
    "\n",
    "X_train, X_val, X_test = prepro(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.13712648, -0.43424916,  0.23527308],\n",
       "        [ 0.21919256,  0.11550273,  0.9344346 ]]),\n",
       " array([[ 0.32579053, -0.32532503, -0.34606027],\n",
       "        [-0.45380438, -0.46950318,  0.09033982]]),\n",
       " array([[-0.50272774,  0.49629487,  0.60371473],\n",
       "        [ 0.42899527,  0.6532508 ,  0.126175  ]]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But, how do we initialize B, the fixed-random matrix?\n",
    "# Let me summarize both initializations for you:\n",
    "# Lillicrap’s B were sampled from a uniform distribution in the range of\n",
    "# -0.5 to 0.5.\n",
    "# Nøkland’s B were sampled from a uniform distribution in the range of\n",
    "# [−1/√fanout, 1/√fanout]\n",
    "# fanout: the number of inputs that can be connected to a specified output.\n",
    "D = 2\n",
    "H= 3\n",
    "np.random.randn(D, H) / np.sqrt(D / 2.), np.random.uniform(low=-0.5, high=0.5, size=(D, H)), np.random.uniform(low= (-1./np.sqrt(D)), high=(1./np.sqrt(D)), size=(D, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "#         self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "#         self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        \n",
    "        # Input layer\n",
    "        m = dict(W=np.random.randn(D, H) / np.sqrt(D / 2.), # ff weight - flexible/ adaptive/ learnable\n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        \n",
    "        # Input layer fixed weight for feedback\n",
    "#         m = np.random.randn(D, H) / np.sqrt(D / 2.) # fb weight - fixed\n",
    "        m = np.random.uniform(low= (-1./np.sqrt(D)), high=(1./np.sqrt(D)), size=(D, H))\n",
    "#         m = np.random.uniform(low=-0.5, high=0.5), size=(D, H))\n",
    "        self.W_fixed.append(m)\n",
    "\n",
    "        # Hidden layers\n",
    "        m = dict(W=np.random.randn(H, H) / np.sqrt(H / 2.), # ff weight - adaptive\n",
    "                 b=np.zeros((1, H)))\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        \n",
    "        # Hidden layers fixed weight feedback alignment\n",
    "#         m = np.random.randn(H, H) / np.sqrt(H / 2.)\n",
    "        m = np.random.uniform(low= (-1./np.sqrt(H)), high=(1./np.sqrt(H)), size=(H, H))\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "        # Output layer\n",
    "        m = dict(W=np.random.randn(H, C) / np.sqrt(H / 2.), # ff weight - adaptive\n",
    "                 b=np.zeros((1, C)),\n",
    "                 W_fixed=np.random.randn(H, C) / np.sqrt(H / 2.)) # fb weight - fixed\n",
    "        self.model.append(m)\n",
    "        \n",
    "        # Output layer fixed weight for feedback alignment\n",
    "#         m = np.random.randn(H, C) / np.sqrt(H / 2.)\n",
    "        m = np.random.uniform(low= (-1./np.sqrt(H)), high=(1./np.sqrt(H)), size=(H, C))\n",
    "        self.W_fixed.append(m)\n",
    "        \n",
    "    def fc_forward(self, X, model):\n",
    "        W, b = model['W'], model['b']\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "#         print('db.shape', db.shape)\n",
    "#         dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "#         y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, fc_cache = self.fc_forward(X=X, model=self.model[0]) # X_1xD, y_1xc\n",
    "        y, nl_cache = selu_forward(X=y)\n",
    "        X = y.copy() # pass the previous output to the next layer\n",
    "        caches.append((fc_cache, nl_cache)) # caches[0]\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "#             y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, fc_cache = self.fc_forward(X=X, model=self.model[1][layer])\n",
    "            y, nl_cache = selu_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            fc_caches.append(fc_cache)\n",
    "            nl_caches.append(nl_cache)\n",
    "        caches.append((fc_caches, nl_caches)) # caches[1]\n",
    "        \n",
    "        # Output layer\n",
    "#         y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y, fc_cache = self.fc_forward(X=X, model=self.model[2])\n",
    "        caches.append(fc_cache) # caches[2]\n",
    "\n",
    "        return y, caches\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "    \n",
    "    def train_backward(self, dy, caches):\n",
    "#         grads = self.model.copy()\n",
    "#         print('dy.shape', dy.shape)\n",
    "        grads = []\n",
    "\n",
    "        # Input layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        \n",
    "        # Hidden layer\n",
    "        grad = []\n",
    "        for layer in range(self.L):\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        grads.append(grad)\n",
    "\n",
    "        # Outout layer\n",
    "        grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy() # pass it to the previous layer\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "#         print('''grads[2]['W'].shape, grads[2]['b'].shape''', grads[2]['W'].shape, grads[2]['b'].shape)\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = selu_backward(dout=dy, cache=nl_caches[layer])\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "#             print('''grads[1][]layer['W'].shape, grads[1][layer]['b'].shape''', grads[1][layer]['W'].shape, \n",
    "#                   grads[1][layer]['b'].shape)\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = selu_backward(dout=dy, cache=nl_cache)\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "#         print('''grads[0]['W'].shape, grads[0]['b'].shape''', grads[0]['W'].shape, grads[0]['b'].shape)\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, _ = self.train_forward(X)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy== acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Momentums\n",
    "        M, R = [], []\n",
    "\n",
    "        # Input layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers momentum\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        # Output layer momentums\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    " \n",
    "        # Learning decay\n",
    "        beta1 = .9\n",
    "        beta2 = .99\n",
    "        smooth_train = 1.\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            #             for _ in range(10):\n",
    "            # Shuffle in every iteration\n",
    "            # The dataset is static and non-sequentiol: no time-dependency or temporal pattern\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y, caches = self.train_forward(X_mini)\n",
    "            loss, dy = self.loss_function(y, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches)\n",
    "            self.losses['train'].append(loss)\n",
    "            smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "            self.losses['smooth train'].append(smooth_train)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "#                 self.model[0][key] -= alpha * grads[0][key]\n",
    "                M[0][key] = l.exp_running_avg(M[0][key], grads[0][key], beta1)\n",
    "                R[0][key] = l.exp_running_avg(R[0][key], grads[0][key]**2, beta2)\n",
    "                m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "                self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "#                     self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "                    M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grads[1][layer][key], beta1)\n",
    "                    R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grads[1][layer][key]**2, beta2)\n",
    "                    m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "                    self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "#                 self.model[2][key] -= alpha * grads[2][key]\n",
    "                M[2][key] = l.exp_running_avg(M[2][key], grads[2][key], beta1)\n",
    "                R[2][key] = l.exp_running_avg(R[2][key], grads[2][key]**2, beta2)\n",
    "                m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "                self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val)\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 train loss: 2.7280 valid loss: 2.9452, valid accuracy: 0.0652\n",
      "Iter-20 train loss: 2.8783 valid loss: 2.9495, valid accuracy: 0.0720\n",
      "Iter-30 train loss: 2.8277 valid loss: 2.9536, valid accuracy: 0.0760\n",
      "Iter-40 train loss: 2.8175 valid loss: 2.9554, valid accuracy: 0.0788\n",
      "Iter-50 train loss: 3.0613 valid loss: 2.9581, valid accuracy: 0.0798\n",
      "Iter-60 train loss: 3.0868 valid loss: 2.9690, valid accuracy: 0.0806\n",
      "Iter-70 train loss: 2.8314 valid loss: 2.9824, valid accuracy: 0.0812\n",
      "Iter-80 train loss: 2.8415 valid loss: 3.0111, valid accuracy: 0.0758\n",
      "Iter-90 train loss: 3.1515 valid loss: 3.0604, valid accuracy: 0.0760\n",
      "Iter-100 train loss: 3.0726 valid loss: 3.1454, valid accuracy: 0.0732\n",
      "Iter-110 train loss: 3.3380 valid loss: 3.2678, valid accuracy: 0.0808\n",
      "Iter-120 train loss: 3.2625 valid loss: 3.4260, valid accuracy: 0.0864\n",
      "Iter-130 train loss: 3.4128 valid loss: 3.6017, valid accuracy: 0.0922\n",
      "Iter-140 train loss: 3.9737 valid loss: 3.7849, valid accuracy: 0.0948\n",
      "Iter-150 train loss: 4.0650 valid loss: 3.9769, valid accuracy: 0.0964\n",
      "Iter-160 train loss: 3.9422 valid loss: 4.1667, valid accuracy: 0.0970\n",
      "Iter-170 train loss: 4.6036 valid loss: 4.3467, valid accuracy: 0.0974\n",
      "Iter-180 train loss: 4.1152 valid loss: 4.5180, valid accuracy: 0.0976\n",
      "Iter-190 train loss: 4.4365 valid loss: 4.6753, valid accuracy: 0.0976\n",
      "Iter-200 train loss: 5.0414 valid loss: 4.8247, valid accuracy: 0.0976\n",
      "Iter-210 train loss: 5.0517 valid loss: 4.9629, valid accuracy: 0.0976\n",
      "Iter-220 train loss: 5.0629 valid loss: 5.0864, valid accuracy: 0.0976\n",
      "Iter-230 train loss: 4.8693 valid loss: 5.2019, valid accuracy: 0.0976\n",
      "Iter-240 train loss: 5.4293 valid loss: 5.2966, valid accuracy: 0.0976\n",
      "Iter-250 train loss: 6.0470 valid loss: 5.3716, valid accuracy: 0.0976\n",
      "Iter-260 train loss: 5.3543 valid loss: 5.4255, valid accuracy: 0.0976\n",
      "Iter-270 train loss: 5.3872 valid loss: 5.4680, valid accuracy: 0.0976\n",
      "Iter-280 train loss: 5.0613 valid loss: 5.5010, valid accuracy: 0.0974\n",
      "Iter-290 train loss: 5.6625 valid loss: 5.5279, valid accuracy: 0.0974\n",
      "Iter-300 train loss: 6.1947 valid loss: 5.5508, valid accuracy: 0.0972\n",
      "Iter-310 train loss: 5.2200 valid loss: 5.5694, valid accuracy: 0.0970\n",
      "Iter-320 train loss: 5.6892 valid loss: 5.5853, valid accuracy: 0.0970\n",
      "Iter-330 train loss: 4.8530 valid loss: 5.5933, valid accuracy: 0.0968\n",
      "Iter-340 train loss: 5.5344 valid loss: 5.5889, valid accuracy: 0.0968\n",
      "Iter-350 train loss: 5.9729 valid loss: 5.5819, valid accuracy: 0.0966\n",
      "Iter-360 train loss: 6.2310 valid loss: 5.5777, valid accuracy: 0.0962\n",
      "Iter-370 train loss: 5.2659 valid loss: 5.5823, valid accuracy: 0.0958\n",
      "Iter-380 train loss: 5.6092 valid loss: 5.5927, valid accuracy: 0.0958\n",
      "Iter-390 train loss: 5.2954 valid loss: 5.6069, valid accuracy: 0.0948\n",
      "Iter-400 train loss: 5.8867 valid loss: 5.6246, valid accuracy: 0.0942\n",
      "Iter-410 train loss: 6.3152 valid loss: 5.6392, valid accuracy: 0.0936\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 64 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 16 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 1 # depth \n",
    "\n",
    "# NOT used now\n",
    "p_dropout = 0.95 #  layer & unit noise: keep_prob = p_dropout, q = 1-p, 0.95 or 0.90 by default, noise at the network level or layers\n",
    "\n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
