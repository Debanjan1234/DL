{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_in, c_in = h\n",
    "        X_in = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = (hf * c_in) + (hi * hc)\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        h = ho * c\n",
    "        \n",
    "        h_ = (h, c)\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        cache = (c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache)\n",
    "        \n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        c_in, hf, hf_cache, hf_sigm_cache, hi, hi_cache, hi_sigm_cache, hc, hc_cache, hc_tanh_cache, c, c_tanh_cache, ho, ho_cache, ho_sigm_cache, y_cache = cache\n",
    "\n",
    "        dh_out, dc_out = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc += dc_out\n",
    "        dc_in = hf * dc\n",
    "\n",
    "        dhf = c_in * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_in = dX[:, :self.H]\n",
    "        dX_in = dX[:, self.H:]\n",
    "        \n",
    "        dh = (dh_in, dc_in)\n",
    "        dX = dX_in\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 106.0724\n",
      "elaWidth riphni tsetnlnrtN8 etoog oriisorawinw gen cuennesnt rlochs,i–  si lo i-emovwkolc eirwgaudc o\n",
      "Iter-20 loss: 105.7127\n",
      "e;.-twaGndselnueallipns 本  Eglti h8ik ia)rnshdsamsig  xrlno cs oreSivehe Ge a ghnActis n5 id  aa slog\n",
      "Iter-30 loss: 105.0185\n",
      "exgo'tygptuagmn rJtcailtps.seeoTfune llanho cir inw doedccsp iaoona t psohit   ads km  1noeno,afcJuJi\n",
      "Iter-40 loss: 104.2113\n",
      "eikeime  whnitrnry ashh  cosiycl tsatce7ofeneatsrhinmi,E l.rEpaE r aa dppisb e oJani: lrssd oapk 8cs'\n",
      "Iter-50 loss: 103.3396\n",
      "e zfrret5atteadna ih rrroe  u1t nlco sctsh acm9reauo ny  antnefeaeuinedj ah ie  i 1uf eint  4 dseaaut\n",
      "Iter-60 loss: 102.4589\n",
      "e74m s scoo2 ;couleti9hextin eon iin Ahiks enth sito rsaa  iantl frove s.oItutbwone feolvnrn dne rtf \n",
      "Iter-70 loss: 101.5791\n",
      "eBdftner Gryiotilon wCrath,m  ftea staiell elooe rSewf r chen nour.h7tbu.  eh ay twhetrome reptto9ank\n",
      "Iter-80 loss: 100.7204\n",
      "eLANito ve9f 7ap sonoethdescxpana ldomhaindewresylNneoitoregnrae,oileisv licstearpaa R8s fir'ipae  fo\n",
      "Iter-90 loss: 99.8953\n",
      "epotn taa dt n  Oror stgtanr aad urysndf pEa esk citkates oni l env 2nheisr,hIda ekh ti re nIe,w or a\n",
      "Iter-100 loss: 99.1116\n",
      "eLEs7teio en ansaapl'heea, pknGet eihn p9od th  gdlrrsest iatleWh coJm arDcamtyteanrysp ;ohee tanl io\n",
      "Iter-110 loss: 98.3698\n",
      "e5h 9int,eansk 2le woo wwrcl Isarunsjf 2spiace eke heis  iae ceoran ir h aag  ia yun 2mh- s  ign,SCop\n",
      "Iter-120 loss: 97.6670\n",
      "e,gswGnoJhintsofvksd o.lm iocaei- faai I hcaondin  ry whew so iinJanie tat teepanterifiosaain onx Tdl\n",
      "Iter-130 loss: 97.0003\n",
      "eaT%r ra rugal5mle (1mnh in Wenasnutatnsorupekeovtsisw is tpih .ae. lJaAinpineiWame foeapyn-smalyunyi\n",
      "Iter-140 loss: 96.3677\n",
      "eri7 art orv felt uonaA lCa ald neianiys inserhk bb atcdetane w ans se  hrCu2epe psre. fha inpdts lan\n",
      "Iter-150 loss: 95.7666\n",
      "eioS wurhses os iE apin ehSaons eia, ev odtaniro ey nwin et0ld  wan; as tK w,eon rpttN iraCc ipetiocm\n",
      "Iter-160 loss: 95.1945\n",
      "ecGtTlel Weuth  athTr iiac  atec,.lTdylai d lesare Jhiwan iS tpde ten wae upae Jtp astem- areanetmte \n",
      "Iter-170 loss: 94.6497\n",
      "eODe uptinstAnruNr chiud-ttnss ocaayd vs wo.nong p hasay  hedsil feomg lein8 ix ,inrthctee th  hhdige\n",
      "Iter-180 loss: 94.1317\n",
      "eCtlepinm in sh iy. f3y f9llosthesf stilegit tae Ja Sncpae wa8npenlTioscth. 6iun iouy fila est, eouCs\n",
      "Iter-190 loss: 93.6394\n",
      "eltsi moso Ganwremfkoirg the tyaon hd ug. fo1o Irim 2os  rhetoitnnf Jmare .n whis inthe ,o conebe hil\n",
      "Iter-200 loss: 93.1697\n",
      "efon. nantn di7 tnt lainthexs ,anc, g6ire ;giupmmis isdes Jnantpou hes asRlipdrao:-tins  osj ana, inc\n",
      "Iter-210 loss: 92.7163\n",
      "eLwrg iSne rith ton  f inJobal ptthicrsians e pappbapam 5e no, 5h end inltfidlWoarl eipfripmalin t 2e\n",
      "Iter-220 loss: 92.2714\n",
      "e0poFnC he2yh was \"htah s cas ph7aale oeh ifkxees bhl vas iantisr thxpg 1oroph 1cplmpinn on gsr echer\n",
      "Iter-230 loss: 91.8276\n",
      "eNiSt the fis anv lahmeitceged ih mir rpecwpeandiomNesod pagmyriny tIp7ul cs i7 hc7on ero og anrHeee \n",
      "Iter-240 loss: 91.3635\n",
      "eOe0Ivenass rfthisn Woleatst wioWro.  aECds eaan  rfy7e tor deipLh e Kbh i8w lvylu-of.c,'ciWoor  se k\n",
      "Iter-250 loss: 91.9225\n",
      "efchyeaf thu wtire gtin tony al girstlwenuior ani IsnIect clen oomd  Ged tain fan Ciandre 本ae Ctpan,i\n",
      "Iter-260 loss: 95.0057\n",
      "e7bstatanc argts orxcitir anisthletre tos inth wroree 1ada anjbrre onwweo , heonhtemo aohin  Wonlth g\n",
      "Iter-270 loss: 91.7990\n",
      "ednafia iisimolki atdtn sose ieala5ce eleinn wlaD artbe kiDmndeIgixmlh Jatiin martwgm ebthimic.uon Cy\n",
      "Iter-280 loss: 91.4778\n",
      "efcnn vfploppande choKhcwor prpinis 1r\" ary A.slpdis toice eate ir er vw,nh , eone the \"n tch anit es\n",
      "Iter-290 loss: 91.0859\n",
      "eryiye sottr dnou ;sm ,ar8 cyan raisr ep,tirir J nadpguiss toja miy pan altoe rey  iarkxe tiptkkw inv\n",
      "Iter-300 loss: 90.7199\n",
      "egmadtleaftetCtbourtary cifdenftthto4 rhis hetep.otmye, ien fiss Jaite fatin Itouppo-n7  pahen, Janpe\n",
      "Iter-310 loss: 91.7525\n",
      "ebhe  iouBeyEw' ectuah , anin Eotheo nmemtae fntmh,di hi  homa y th Uwo pesr8 Goun tued baue 8ots8pla\n",
      "Iter-320 loss: 91.7775\n",
      "eluc imolihd uonte hKeEColan t ebotto Japhh in wanind idned JkiuAly lias the oontppAareads ioandd Sak\n",
      "Iter-330 loss: 91.6112\n",
      "e2vuag cpiriurg isl llhelly atulomroJe are ren  ape Jonan in9l Gendul festvo,v4Tkwy lmelve gad, ia,tw\n",
      "Iter-340 loss: 91.8036\n",
      "e-mlearSe,  hutpic- mning5 Ana1inu on wiyregvrhe trt S9dar1e ex Com timeutan un.klepat-lotmeec,tilr i\n",
      "Iter-350 loss: 91.6017\n",
      "elap)Id tholcaot  rhos tepewyrfoo irhso xa, and Ranalot9os tim,ch,lapmmy IalirostanE.,yeed wre eaetu \n",
      "Iter-360 loss: 91.5323\n",
      "exs8.e isatHoslwioen7  \"6e. aly inera. thih \"re in.  apomilnd oAwEnousissilat in aa lher itpowtom cnd\n",
      "Iter-370 loss: 91.0504\n",
      "e. i本no ferporl itd  oupd nala fancwisrh, rint fout yetpiten Ohdaccans lo teoll pa eswod arrss  ter f\n",
      "Iter-380 loss: 90.9129\n",
      "e3f candgdosgts Gf ins 5yn pacicontswanosth 8yecd ,f wandtce pr1oraks an Jsstuwpadino6 Grel rinel oon\n",
      "Iter-390 loss: 90.8970\n",
      "ep panpf tee in tlalol si1elon Rnst6,er  hpr,cpans,  NoTes fory gur) nt8 ares ,a furi anormh \"agad fa\n",
      "Iter-400 loss: 93.3054\n",
      "e,.eod G. paJaaatavy. C rpl leeWg:t d th 1hepon  aporeletad th –i is1 Ito tind  eochn bhe C untmopind\n",
      "Iter-410 loss: 91.4980\n",
      "e-s the rhics umpxeatef , thiittgw ot ht aa. rvedseted ihab lond  Stherihe. talimodd trprlany cioo.pe\n",
      "Iter-420 loss: 91.6739\n",
      "e Waptearuilopmi,l,rs Wosm lod1o JaurD thel paorhe SUj  heche coutt ohh ao ar pcn -awostan turpsef at\n",
      "Iter-430 loss: 91.5239\n",
      "eurie srti  oh  he bbturs o8meit Giare ae epopy S uu thewpW8res, c,whsth ireu tte th.eert w rhhespad \n",
      "Iter-440 loss: 91.3421\n",
      "e2).an Gs rere. Sv thks h meaf  arurusjdil, ua anmthhtcentere ty fGsrostuaplamarnsseh  oape opatertin\n",
      "Iter-450 loss: 91.1763\n",
      "eJTale \"ed ,Iaur dissii s ad  te tom. 1hyf .fbe rfc sats  lias aumrW o  andaslrie t n fhrha redeprsbf\n",
      "Iter-460 loss: 91.2091\n",
      "ex8,he acalyil Eo ar.asnked isadi iFs uth  h ape t uetGietht cape pa, eorte ina–aa \"ind uarccisi worJ\n",
      "Iter-470 loss: 90.6891\n",
      "ei9arl anh lacud rinnLl'n, Tithidur fitO gcopthela, wis as to fyrhat schr Jgascwasplor ithcorerot lnr\n",
      "Iter-480 loss: 90.4845\n",
      "eidmhe\". rett arett  ililosiss thiarc n9t hs Tikkumtod spm preheon . tiare the aokea an Jaon Pteindbe\n",
      "Iter-490 loss: 90.3827\n",
      "edr aethios in Hde–n d earo as turCor isr ou,cio1s enapa.k,-io s,dpilptconog,ac bue5adwen eal oar Eth\n",
      "Iter-500 loss: 90.3843\n",
      "edan eCthe Fnttth  h日 af llhe vote tcert an8d he本Ss erin pent'ifd WtegWa, Ghelathre hy the Te eusitns\n",
      "Iter-510 loss: 91.3317\n",
      "eof,iapr th sian  e in the Ino fhaifnt  id Soun optan cCin o th tveiw, ia, tothe cre ,uxr ghconh ui6c\n",
      "Iter-520 loss: 91.4861\n",
      "ea';sf guyf hat s arearseaaotolgaeA wmrre polantee sin calccifecitpilj Knderyacoms wosrorio d fnt uHo\n",
      "Iter-530 loss: 90.9801\n",
      "eHM)tG8f 1el --vugepsntaktciniurate th Jam slEudis anddirsites heh Westounmef sor wpora te Jarud mpkr\n",
      "Iter-540 loss: 90.1056\n",
      "eMuta  panade aNeede uOalbon aro asindyet xe Iriave incan Woahel gcih ifcf pehid ilep rheR CoErian en\n",
      "Iter-550 loss: 89.3641\n",
      "e.s thimpeaf e. peDam 4l ond 9JaEy ealuon,iinsan ,T th \"8opgindetptyan a. Japon o.. L7 anip5othe ec p\n",
      "Iter-560 loss: 89.7495\n",
      "e,'rcotane fks sihlf u yopous onua7i fob7et.lfs I1aefo5 vhiy f thiarinpAexthat sepro  ans tte  af aua\n",
      "Iter-570 loss: 89.5131\n",
      "e:an t nvecee ains fth olitrs,ete anaEre tylc bhmah eh bferid mv enroy pant c8Ctew meorltalet ant eul\n",
      "Iter-580 loss: 89.3456\n",
      "eo\n",
      " theilsl omo ty O, an irs llalo en dirt anenungemJ 98nactiuf par oanti sotPlugeab lmed ,iche Nop e\n",
      "Iter-590 loss: 89.1962\n",
      "epci2d rE-onet Ouxed ia,, Cvtyteds Wor ind opltur rhev ap.etlhal Jan, mf ch bheas nres ar nn ninth md\n",
      "Iter-600 loss: 89.0503\n",
      "e onare l an 2f-G Calnise laikeclh An thh iip pacergisarsm a an, and an in iistasd Nhitusi toe ntare \n",
      "Iter-610 loss: 88.9141\n",
      "epasaree irsts oisd lpa7 Fre paranag roniv ortrast 49 th assapile reruor  ihe ta. wtgh  houmvancmushw\n",
      "Iter-620 loss: 88.7857\n",
      "eas.liad,  unea 5cftas  ana on- th pMiponn tofyiao theaisstydditt s9ii'nt aloe8'rtis ry einmdsege Con\n",
      "Iter-630 loss: 88.6635\n",
      "epancte 1tun nrge  ananitheis oand Jnulestiorg  pant6 fan cal. thed,hot4ono in tielinejid tges araR v\n",
      "Iter-640 loss: 88.5462\n",
      "e.geWr Srae mo tnu Jbmimtun fooeu esopsran  orbceSel ah o ghee care fengoesmlUacant part mhe loieaPG \n",
      "Iter-650 loss: 88.4323\n",
      "end Jarlie ir ssros an agincencab6 Ay i8 hande mheof the in. wsalh int  ins og 0ese Jaivxrctleeze ,rm\n",
      "Iter-660 loss: 88.3210\n",
      "e'osdxhde ans En ilis. one lafiCd, ter pyo lioat eelrh wano S aperi12 rndthvi5sg oetre uhePk pin roth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 88.2116\n",
      "eewl3Iemcoth To 5n nrhe ari.in ifm Janorls, arorsohe panRmae af Jcir(rme'gy anit , e roe m Jaitry. 2o\n",
      "Iter-680 loss: 88.1035\n",
      "es feafchp2okrhe wout lnst abeo-ti-r-th' wnedFHu pxtna mate, lniol indimducparary ted 2agisdofox ttt \n",
      "Iter-690 loss: 87.9964\n",
      "e 4hm opan thl1eam aner ery. th irion ndgc OhlD olcery is h1edaxs f 6er 1gves lripoynop eairrra vheT \n",
      "Iter-700 loss: 87.8901\n",
      "epsf whare. tuhek panv Adipe, loae an  tomy. an th tn tpalasmnir parvPeof soisn an roTe 1ran tingou t\n",
      "Iter-710 loss: 87.7846\n",
      "ey ourIci– iptopea  bom enc-licne T6 hetriagenecons leu, Ptin pat SSanHy Wand iog bon GltsEt6us Umyks\n",
      "Iter-720 loss: 87.6799\n",
      "e6 Jar usnd Sar0koal\"m toanresis ap te malate lrlin GIuhe dsgest unkioOtntoon ie corolbton  elwnR enD\n",
      "Iter-730 loss: 87.5759\n",
      "eT6s aoxora\"ssed-oanr at lanmu9 dind \n",
      "of ome unpess inr Tertis nd iertildepob8 onal, bhe wore oncreht\n",
      "Iter-740 loss: 87.4727\n",
      "evKf aens efr\n",
      " Jaken peto fnrsWvan todg alen wis sllirrthedes inryic peosben'E tiatac aMoiis che pon \n",
      "Iter-750 loss: 87.3704\n",
      "e SLkef ins ind 1olu oarhirlighiC f日n Japans Jakum in inpes waf in Cansi roun ,ikt thoe.e 'olprlh aed\n",
      "Iter-760 loss: 87.2691\n",
      "end Joweal fmo anJ paatt romtoptlyotem eisd oumle ro th wingtto iny Jined'sy J ad i5he foe9iy blnar m\n",
      "Iter-770 loss: 87.1686\n",
      "etoryie, thnt.dwisntsinc4rin tesit pect paneW't oul NJae Hrar il foe aryptlloNocm uni ophitloeg dtrat\n",
      "Iter-780 loss: 87.0691\n",
      "ecIan thl ro.aund sredmerass onHed the loultuks popan he inIep SDughkreuR ed iry ltus erftoof onstcur\n",
      "Iter-790 loss: 86.9705\n",
      "ei\"oidranko meank fhi o's rosteny esd gg-erges the te1 an hlofe itun-ideot .e wo 8ilano n IanpaU w\"ti\n",
      "Iter-800 loss: 86.8729\n",
      "enjhrkbeg fobit tuar. thedglofocisy mp-a2a-rhi  iIAr pangt ere weed-dlccnpooe oan,th ind widsd  urig \n",
      "Iter-810 loss: 86.7761\n",
      "e mesth condcd gglintged fandy xhe im e d. Sf N. ohdan an iete fest  .trs omalas fapmll wirl Eyth Gys\n",
      "Iter-820 loss: 86.6802\n",
      "ey \"tthe wh tf fhe weat fa toh ou, of Jarpir ocaincthL edeat es cop the incinsae lais WoJentonti M. d\n",
      "Iter-830 loss: 86.5850\n",
      "eFGto8 porin anI. wald ,Sfoi, hisv oumlsinI To  analogs areo ind mtes 'otireclEinic ihe is wes Ocuroy\n",
      "Iter-840 loss: 86.4906\n",
      "e 1opas- Che eofe foppmainc oay n\"meS opser ,dweesrwe Ulkup asdghe ar wstertin  alon  iWry oo and Jan\n",
      "Iter-850 loss: 86.3968\n",
      "ecETve df ur ftinthye inrd xof an th -e. ienelore pOmed oontte mad Feg thd do 5n indtof igs theine t,\n",
      "Iter-860 loss: 86.3034\n",
      "ee3Iunay ih thr Ipatpanan rdn fhe1d in amurean P \"eato win ned W6ox, pond \"cy o4ietas tiiing tio ans \n",
      "Iter-870 loss: 86.2104\n",
      "e17tyslne Eam6 an 1.Cehecir2 .,ebn ,lin mand hociye ti, \"ane st pha ono waro indt- lalyp-Naton. ald f\n",
      "Iter-880 loss: 86.1175\n",
      "erCla, S Jompelrao ftind Tee worud anmel ense 5tooulas and ond mir th.l kf w日sy'treye wese vof frf id\n",
      "Iter-890 loss: 86.0246\n",
      "ekG 1fivesouh gbxs 1th lehte onsalge ly vEoss ut4es arisanes iupl uzes mionila ys ,ouieddlyisltlhe gm\n",
      "Iter-900 loss: 85.9314\n",
      "e-drEar con onsd cafishe. an the Estan Ued in Ja ad oun oplannthee cirse t tecoitis Jare parere  orlo\n",
      "Iter-910 loss: 85.8376\n",
      "ectinreslek iah ,ic,gore fosiourg-on dy Rtes manr Spapor eo aulmhit m5y Ooxap,tih ariN,s sire tibind \n",
      "Iter-920 loss: 85.7430\n",
      "eb Jaruk area Cnor iond eEcirr Wond usl eolhiv chseg If tewin lllnd eont  9chind yarares xo e waslanJ\n",
      "Iter-930 loss: 85.6472\n",
      "eLipthe fafc aE fainresy oD -i.r cttkuiplrona of\n",
      "ge 1ls wapmepetokredy tiofe thiomtuJary incmt intdmi\n",
      "Iter-940 loss: 85.5497\n",
      "e日w6d Wasd Gr wa ofr ante ist ifIyy fwons syhe wapioe cawans tod Ah Gtn, lolarear.  his anr eod Jofun\n",
      "Iter-950 loss: 85.4503\n",
      "ek Smu1 un,dratiss Jasastos9un upcakuliDah Trheit lkef if 99rtot pulfcelicheaw ths panethe bslepef re\n",
      "Iter-960 loss: 85.3484\n",
      "eeW Bhe tafs eevdbry Suto E–palafeatetisthe ar Wiht rcgy aryd co'9dltirenlosid 2as eon phi the Wer th\n",
      "Iter-970 loss: 85.2437\n",
      "esara lufeos tisori ojfr-kind wailcun fsed  Jalr riptpin hhi\n",
      "d e anriyt csopopr)es uh JAf.the Th eari\n",
      "Iter-980 loss: 85.1358\n",
      "eoul the in einab t\"othe ser  rse Oen soga an Wan5ti resd come in te loland a0d toar, Apas thetin. Kc\n",
      "Iter-990 loss: 85.0245\n",
      "e5swiire trat lar iars sg.itt eand 'ild oar in dind cih Mh ms4Ilhecoklan2 are. of Jepon  cuolar Te 8e\n",
      "Iter-1000 loss: 84.9100\n",
      "ead Japarus repuropusundtys Skbdian cen baul lopal ptexiseangder,cissdoc perusralilo rugeucreI iargis\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VdW5//HPk4kpJIR5CKNaBrWiFhyAmtbWudVLFahD\nq9bWW297tdoq2FLsba3Yn3pre/X+7tX+qFoFba8DVq1KaZxaRQWuCogoBGSUmQAJkOT5/bF24BAT\nMp2TfRK+79drv7LPOnt4zk6yn7PW2nsvc3dEREQy4g5ARETSgxKCiIgASggiIhJRQhAREUAJQURE\nIkoIIiICNCIhmFmGmS0ws9nR62lmttrM5kfTWQnLTjGzZWa2xMzOSEXgIiKSXFmNWPZaYBGQl1B2\nl7vflbiQmQ0HJgDDgUJgjpkd5brhQUQkrTWohmBmhcA5wP0136pl8fOBWe5e4e4lwDJgdHOCFBGR\n1Gtok9G/Az8Can7L/56ZLTSz+80sPyrrB3ycsMyaqExERNJYvQnBzM4FNrj7Qg6uEdwLDHH3kcB6\n4M7UhCgiIi2hIX0IY4Cvmtk5QAegs5k96O7fSFjmPuDpaH4N0D/hvcKo7CBmpj4FEZEmcPfamuub\nrd4agrvf7O4D3H0IMAmY6+7fMLPeCYuNB96L5mcDk8wsx8wGA0cC8+rYdtpP06ZNiz0Gxak4W3Oc\nrSHG1hRnKjXmKqOafmVmI4EqoAS4GsDdF5vZY8BiYB9wjaf6U4iISLM1KiG4+0vAS9H8Nw6x3G3A\nbc0LTUREWpLuVK5HUVFR3CE0iOJMLsWZPK0hRmg9caaSxdWaY2ZqSRIRaSQzw1PUqdycPgQRAQYN\nGsTKlSvjDkPamIEDB1JSUtKi+1QNQaSZom9scYchbUxdf1eprCGoD0FERAAlBBERiSghiIgIoIQg\nIg1UVVVF586dWb16daPX/eijj8jI0Okm3ek3JNJGde7cmby8PPLy8sjMzKRjx477y2bOnNno7WVk\nZFBaWkphYWGT4jFLST+oJFGsl52uWwd9+sQZgUjbVVpaun9+yJAh/O53v+MLX/hCnctXVlaSmZnZ\nEqFJmoq1hrBzZ5x7Fzl81PZgtKlTpzJp0iQuvvhi8vPzefjhh3n99dc55ZRTKCgooF+/flx77bVU\nVlYCIWFkZGSwatUqAC677DKuvfZazjnnHPLy8hgzZkyD78dYs2YNX/nKV+jWrRtDhw5lxowZ+997\n4403OPHEE8nPz6dPnz7cdNNNAJSVlXHJJZfQvXt3CgoKOPnkk9myZUsyDo9EYk0I0d+ZiMTkySef\n5NJLL2X79u1MnDiR7OxsfvOb37BlyxZee+01nn/+ef7rv/5r//I1m31mzpzJrbfeytatW+nfvz9T\np05t0H4nTpzIEUccwfr165k1axY33ngjr7zyCgDf//73ufHGG9m+fTsffvghF154IQAzZsygrKyM\ntWvXsmXLFu69917at2+fpCMhEHNC+MlP4ty7SMswS86UCmPHjuWcc84BoF27dpx44omMGjUKM2PQ\noEF8+9vf5qWXXtq/fM1axoUXXsjxxx9PZmYml1xyCQsXLqx3nytWrODNN99k+vTpZGdnc/zxx3PF\nFVfw0EMPAZCTk8OyZcvYsmULnTp1YtSoUQBkZ2ezadMmPvjgA8yME044gY4dOybrUAgxJ4T/+Z84\n9y7SMtyTM6VC//79D3q9dOlSzjvvPPr06UN+fj7Tpk1j06ZNda7fu/eBYVE6duzIzga0A69bt47u\n3bsf9O1+4MCBrFkTxtGaMWMGixYtYujQoZx88sk899xzAFx++eV86UtfYsKECfTv35+bb76Zqqqq\nRn1eObRYE8JXvxrn3kWkZhPQ1VdfzbHHHsvy5cvZvn07P/vZz5L+WI6+ffuyadMmysrK9petWrWK\nfv3C0OtHHXUUM2fOZOPGjVx//fV87WtfY+/evWRnZ/PTn/6UxYsX8+qrr/L444/z8MMPJzW2w12s\nCUG1PZH0UlpaSn5+Ph06dGDJkiUH9R80V3ViGTRoEJ/73Oe4+eab2bt3LwsXLmTGjBlcdtllAPzh\nD39g8+bNAOTl5ZGRkUFGRgZ/+9vfWLRoEe5Obm4u2dnZurchyRp8NM0sw8zmm9ns6HWBmb1gZkvN\n7Hkzy09YdoqZLTOzJWZ2Rl3brKhoXvAi0jANvQfgzjvv5Pe//z15eXl897vfZdKkSXVup7H3FSQu\n/+ijj/LBBx/Qu3dvJkyYwPTp0xk3bhwAzz77LMOHDyc/P58bb7yRxx57jKysLNauXcv48ePJz8/n\n2GOP5YwzzuDiiy9uVAxyaA1+2qmZ/QA4Echz96+a2e3AZnf/lZndBBS4+2QzGwE8DIwCCoE5wFE1\nH21qZn7BBc4TTyTz44i0PD3tVFIhbZ92amaFwDnA/QnF5wMPRPMPABdE818FZrl7hbuXAMuA0bVt\nd/36JkQsIiIp0dAmo38HfgQkpqte7r4BwN3XAz2j8n7AxwnLrYnKPuX11xsVq4iIpFC9j64ws3OB\nDe6+0MyKDrFoE+rMt3DLLWGuqKhIY5qKiNRQXFxMcXFxi+yr3j4EM/slcClQAXQAOgNPAJ8Ditx9\ng5n1Bv7m7sPNbDLg7n57tP5fgGnu/kaN7Tp4yq6vFmkp6kOQVEjLPgR3v9ndB7j7EGASMNfdLwOe\nBi6PFvsm8FQ0PxuYZGY5ZjYYOBKYl/TIRUQkqZrztNPpwGNmdiWwEpgA4O6LzewxYDGwD7hGgyeL\niKS/Bl92mvQdq8lI2gg1GUkqpGWTkYiIHB6UEESkQZozhGa6GjduHA8++GCDlv3rX//K4MGDUxxR\nvJQQRNqodBtCM25Tp07lyiuvbNY22vowoLEOoSkiqaMhNKWxVEMQOQzEPYTmoYa/HDduHNOmTeOU\nU04hNzeX8ePHs2XLlv1xnXLKKQc1U7366quMGjVq/3bmzTtwVXtdQ3M+88wz/OpXv+Lhhx+mc+fO\n+wfdAVi+fDljxowhLy+Pc845h23btjXomC5evJiioiIKCgo47rjjePbZZ/e/9+c//5kRI0aQl5fH\ngAEDuPvuuwHYuHEj5557LgUFBXTr1i39bsat/kNp6QnwcP+aSOtGK/hDHjRokP/1r389qOwnP/mJ\nt2vXzp955hl3dy8vL/e33nrL582b51VVVb5ixQofOnSo33PPPe7uXlFR4RkZGb5y5Up3d7/00ku9\nR48ePn/+fK+oqPCJEyf6ZZddVuv+77nnHv+nf/on37Nnj1dVVfnbb7/tu3btcnf3sWPH+rBhw7yk\npMS3bdvmw4YN82HDhvlLL73klZWVfvHFF/t3vvMdd3fftGmT5+fn+6OPPuqVlZX+0EMPebdu3Xzb\ntm3u7j5mzBi/9tprfe/evT5//nzv3r27v/zyy/s/7xVXXHFQXGPHjvXPfOYz/tFHH3lZWZmPGzfO\np06dWutnmDNnjg8ePNjd3ffu3euDBw/2O+64wysqKnzOnDmem5vrH330kbu79+jRw19//XV3d9+6\ndasvWLDA3d1/9KMf+fe//32vrKz0ffv2+SuvvFLn76yuv6uoPCXnZTUZiaSY/Sw57c4+LfmXttY2\nhGa1xCE0r7nmmhBDHUNoAlxyySX8+Mc/rnU/icNfHnPMMZxwwgkHvX/llVcycOBAAM4880xWrFjB\n5z//eQAuuugifvnLXwLw9NNPc8wxxzBhwgQALr30Un7zm9/wzDPPcOqpp/Lmm28yZ86cTw3NWf1o\n7dp861vfYsiQIfv39eKLL9Z73F599VX27dvHDTfcAMDpp5/O2WefzaxZs7j55pvJyclh0aJFHH30\n0XTp0oWRI0fuPw7Lly+npKSEIUOGMHbs2Hr31ZKUEERSLBUn8mSpbQjNG264gbfffpvdu3dTWVnJ\nSSedVOf6DR1C84orrmDdunVMmDCB0tJSLr30Um699db9A9z06tVr/7IdOnT41Ovq7a5du3Z/4qhW\nPfzm2rVrax2ac9GiRYc8Bk0dBnTAgAG1xgHwxBNP8Itf/IIf/vCHjBw5kunTpzN69GimTJnCT3/6\nU04//XSysrK4+uqr+eEPf1jv/lqK+hBEDmMtNYRmVlbWQcNfPvHEE00a/rJv376UlJQcVFY9/GZ9\nQ3Mm8wqhvn378vHHHx9UlrivUaNG8dRTT+3vM6geaCg3N5e77rqLFStW8OSTT3L77bfzyiuvJC2u\n5lJCEJH9UjWEZm3DXzbliqbzzjuPxYsX88c//pHKykoeeeQRPvroI84999x6h+bs1avXp5JJU516\n6qlkZWVx1113UVFRwdy5c3nuueeYOHEi5eXlzJw5k9LSUjIzM8nNzd3/Wf/85z+zfPlyIFwWnJWV\nlVbDgKZPJCKSMnEPoVnb8Jdf//rXG72d7t27M3v2bKZPn0737t25++67eeaZZ8jPDyP4HmpozokT\nJ7Jnzx66du3KySef3Oh9J8rJyeHpp5/mySefpHv37lx33XXMnDmTI444AoAHHniAQYMG0aVLF2bM\nmLG/NrR06VK++MUv0rlzZ8aNG8d1113HmDFjmhRDKuhZRiLNpGcZSSroWUYiIhIbJQQREQGUEERE\nJBJ7Qogu2xURkZjVmxDMrJ2ZvWFmC8zsXTObFpVPM7PVZjY/ms5KWGeKmS0zsyVmdsahtl/jIgYR\nEYlJg64yMrOO7r7bzDKB14B/Bc4GSt39rhrLDgceAUYBhcAc4CivsaPqq4wAXWkkrZquMpJUiOMq\nowY9usLdd0ez7aJ1qqOsLajzgVnuXgGUmNkyYDTwRl3bf+aZ8PPccxsUs0haGThwYJt/Tr60vJqP\n6GgJDUoIZpYBvA0cAdzj7m+a2TnA98zsMuAt4AZ33w70A/6RsPqaqKxO550Xfh53HMyfD2l0455I\nvZJ196tI3BpaQ6gCjjezPOAJMxsB3Av8m7u7mf0CuBO4qnG7vyVhvoj//d8iqu9m37ABevZs3NZE\nRNqa4uJiiouLW2Rfjb5T2cymArsS+w7MbCDwtLt/1swmE57XfXv03l+Aae7+Ro3t+IGWp7q9/z4M\nHdqoEEVE2qxY71Q2s+5mlh/NdwC+DLxvZr0TFhsPvBfNzwYmmVmOmQ0GjgTm0UTDhoEZ3HdfU7cg\nIiINUW8NwcyOBR4gJI8M4FF3v9XMHgRGAlVACXC1u2+I1pkCfAvYB1zr7i/Ust0G1RBqGjMG5s6F\nnJxGryoi0uqlsoYQ+8PtmmPlSqgxRoWISJumh9vVYeDA0Jw0Y4buZRARaa5WXUOoqX9/WLIEOnVK\n6mZFRNKGaggN9PHHkJsbag2vvRZ3NCIirUubSgiJxo4NieHKK6GiIu5oRETSX5tqMqrPihUwaFCL\n7lJEJKnUZJQkgweHWsNdd6kTWkSkpsOqhlCb9euhV6+4oxARaRjVEFKod+9Qa/jNb6CqKu5oRETi\nc9jXEGqzalW4hFVEJN2ohtDCBgwItYbJk6GyMu5oRERahmoIDbRgAYwcGXcUInK4Uw0hDRx/fKg1\nfPnLsHt3/cuLiLQ2SgiNNGdOeDSGGTz6aNzRNN3kybBmTdxRiEg6UZNRkrSmjuiSknBPBqT2foz/\n/E+48ELo0SN1+xA53KjJqBWo7oj+3vfS51EZ7nDuuXDSSdC3b4ivvBzuvrtl9n/NNfCHP8COHWHf\nZrBxY8vsW0QaTzWEFJo7F77whfj2v3p1/bWWVP76LfoO85nPwAcfHCgvKQmPLk93K1bAkCFw+eXh\nEesi6SDuITTbmdkbZrbAzN41s2lReYGZvWBmS83s+ephNqP3ppjZMjNbYmZnpCLw1uCLXwwnxb59\nYevWlt//LbfUv0xlZZj27Quvy8th3brkXm6bmAwgPE/qm99M/xsBFy4MP3//+9bdXyTSUPUmBHff\nA3zB3Y8nDJl5tpmNBiYDc9x9KDAXmAJgZiOACcBw4GzgXjNLSTZrLdatg65dQ3K49dbmn2x37IDr\nrqv/2/3f/17/trKywpSTE+Lr0CEksKysA808I0eGzwAhcTSkVlFefuj3H3wQMjPD9gsK4P77w+dK\nJ9u3H5ifNOnA8ag5jR8fxvx+770DiVWkNWpUk5GZdQReBr4LPASc5u4bzKw3UOzuw8xsMuDufnu0\nznPALe7+Ro1ttfkmo/q8/TaccELj16tOr4WFYQyI+pZLtfPOC2NdDx8ORx8dOqwXL4bPfrZ5273h\nBjj99DC19Bjap50GL7+c/O0OGBCOyzHHwNCh4VgNHBiSsMYJl4aIfUxlM8sA3gaOAO5x9ylmttXd\nCxKW2eLuXc3st8A/3P2RqPx+4Fl3f7zGNg/7hFBt5EgoLob8/HoXZeNG6NnzwOtPPqn7Kp62WC/b\nsiXUKFLlqafgggtSt/1kaN8+JN+jjgrToEEh0fTrB336QJcukKHLRdqsVCaErIYs5O5VwPFmlgc8\nYWZH8+mzeRPO7rckzBdF0+Fn4cLwTwwwfTr86Ed1/0MnJoPE13l5cO+9ofliz57QBNMWde366bKp\nU+Hb307OZb9vvdX8baRaeXm4c37BguRsr1u30PF/5JEHkkthYUguvXqF97Ozk7Mvabzi4mKKi4tb\nZF+NvsrIzKYCu4GrgKKEJqO/ufvwWpqM/gJMU5NR4z36KFx00YFv+t/5TmirlkPr2hVmzQpXeGXV\n+MpTVgYdOx54fd114Sa96keg//Sn8POft1ysbVVOTkgwQ4aEBNO//4EaTK9eoVarRNM0sTYZmVl3\nYJ+7bzezDsDzwHTgNGCLu99uZjcBBe4+OepUfhg4CegHvAgc5TV2pIQgLW38eHj88brfP+KIkETa\nau2qrcjICL+rwYNDTaawMPTB9OkTasw9e4Zk07lz22w6izshHAs8QLgiKQN41N1vNbOuwGNAf2Al\nMMHdt0XrTAG+BewDrnX3F2rZrhKCiKSd3r1D09ngweFnde2mX7+QbHr0CP04cYm9UzklO1ZCEJE2\nZunS0B+TSnp0hYhIKzBnTtwRNI8SgohIksTU4JI0SggiIkmihCAiIkDrTwgNujGtTcjZCb0XQq93\nYG8n2DQc1h0PVboQWkSSQwkhXWWVwYj/gfGXhdcV7SBrT+3Lrj8O3r8A1n4uTDt7t1ycItJmKCGk\nm6xyOPtf4cT7YN1IWPhNmPcv4URPjSu1ssqgz3zIXR9+jv4tHBndMvHBubD5KHjlx7C7e4t/DBFp\nfZQQ0smAV+DKz4f5h56Hj+oZiqGiA3w8Jswv+Vr4mbEPjnoWBrwKY+6AU34dylcUwZzpsOaklIQu\nIq1fa08IbefGtN4L4J9PgDWj4L43+FRtoKm6lMAXfwLHzIKMaCCDFUXw7D2wcURy9iEibcKdd8L1\n16d2H7pTuT79X4NvjYVH/3Tgm34qZO+Gz/8CjpkJBSWwtyO8+S/wziWw4bjU7VdEWoU77gjjeKSS\nEsKhdNwE1w2El6fCq5Obv72Gsir43P+F0f8BPZZAZRa8fh28ezGsH0nSaigi0mrccENICqmkhFAn\nh6k5sPzL8PCzSYmryXGccD+c/GsoWB6uUlp8ESy6qPbObBFps1J9SlVCqMupd8CXboJbd0Nlu+QE\n1mwOfd+CEX+CYx4NrxdfGKbVJ6PkINK2KSE0ZcfNTQhZZfCTjvDHWbBoYvICSyqHfm+G5DDiT5C5\nFxZ/LfRzrBoDnhl3gCKSZEoITdlxcxPCpWfCoJfgF+XJCyqlHPosgOH/E5JDh62w5J9gyXgo+QJU\naoR1kbagTScEMysEHgR6AVXAf7v7b81sGvBt4JNo0Zvd/S/ROlOAK4EKUjFATlYZXF8ID86B9cc3\nbRtx67EIhj8epoLlsOzc0Ofw0Zmwr2P966daRgW03xYe+dFxY6jddKr+VRt4BpR3gT2dYV+nML+v\nY7gjvCoLNY3J4aqtJ4TeQG93X2hmucDbwPnARKDU3e+qsfxw4BFgFFAIzCHZQ2ie+F/wmT/DzKeb\ntn666bIi1BqGPRXumC45DT48OzQvlfZL0U49XB3V651w2W7ftyB3AxSsqH3xT44ON+1ZFZiHhNBh\nM+TsqvuRINW2DYTyfOj9TnhdmQ2Z+w68v2ko7OsAezvDtkFhP+VdQrK3StjwWSjtCzv7QEWMQ1WJ\nNECbTgi1BPMk8FtgLLDT3e+s8f5kwN399uj1c8At7v5GjeWalhCsCr43DGbfBytPa/z66S53PRz5\nXEh4Q5+GLUeEWsPSr0BJUdTv4OH9k38NQ+YevH5lNuzJC7Wo8gLI3AMdN4eTOMDGYeEEXn3i//gU\n2N4/3GRX1hW2DYYNx8KOwqb1cVhVeHxIux0hYWSVh6kyB3b1DJNnhBg6bAnLAOStDkmiS0m436PL\nilAjydkVXueuh85roawgXECwamxYb/XJ4aquPZ3DAwt3dwufP1k1lIx9MPL3MOb/hJpPtw/CDYq7\nu4a4Ng0LNamtR4RjvqtXiCdzD3xyDGSXhSRXlQU7e4XHoJR1DQmvon3y4pS0cdgkBDMbBBQDxwA3\nAJcD24G3gBvcfbuZ/Rb4h7s/Eq1zP/Csuz9eY1tNSwifeRpO+3ly70ZOV1nlMHgunPjfcMQL4eSy\nt1M4SQL84zpYfUo44biFE2pGBeSUQt6acDJrVwqlfaITUu8wVbYLJ81dPWhVx9Aqw+fq+V5IJD0X\nhSatbktDssguC/elZJWHxLC7O5RFP6undttDrSunNJzIzYGoxuMZIQmWdYWuH0L+qtBPtWYULLgy\n3F9S3iUc44yK0IyWUQH5K0MizN4NXVaGpNx+e3g/b3XYV84u6LwmxNZp04HPVFYQYlo/MqzzydHh\nd7b+uNDPtHFE2Gfm3vBsrcqc8DnKC2BvbkiEujghrRwWCSFqLioGfu7uT5lZD2CTu7uZ/YLQrHRV\nyhPC+Etg1Th4658bv25r1/39cGLY0xm2D9CJoC7VtaIOm0OCqJ66lISfe/LDyX93t4Rj6OGbf0ZF\nWCZ7dzjxvn9BdLlwsnk48WfvCkkOD7W2qszoEe2dQzKBEHfm3pAQuy0LSbAyBzptgMwKKIv6b9rt\ngM2fCbWwvbnh0e5WBRuHh9d7c0N/T/V84rSrZ2i2a01fENJUa04IDXq4nZllAX8CHnL3pwDcfWPC\nIvcB1Q36a4D+Ce8VRmW1uCVhviiaDiFzT3jw3At3Hnq5tmrTsLgjaB0q24U+h9K+cUdyCBZqaXvy\nQt8IwNpR4WdjHr+SuSckr+zd4QtDZU5IGDk7Q6JpvzUkhc5rD5TXnLq/DxlVoZa5r2OYqjJDEtp4\ndLhQoF1pqL1k7Qm1mvKCkKCqm788I+r/qTjQ11OeHz5fZQ5KNE1XXFxMcXFxi+yrQTUEM3uQUBu4\nPqGst7uvj+Z/AIxy94vNbATwMHAS0A94kWR1Kh/1LIy9DWa80rj1RKR+VnkgubTfFmpZGftCzaP6\nCrO81eHk33ldeJ0b/cxbE5JItw+i2svGAw+DTFSVGWokFe1DMtndPTSxVWWHRFSeD7t7RGVZoWxv\n51Arrq7N7MkPSWh/wmkXYkoTbbqGYGZjgEuAd81sAeEsfjNwsZmNJFyKWgJcDeDui83sMWAxsA+4\npmYyaLLhj4fr9kUk+TwznHz3dg6d45uTtN2s8qg2EvWlZO8KFxTk7DzwOqMyNPG1Kw3LZe8OtY3O\na0NCytl1oEbTfnv9+9yTGz5HdY1n/9QhYb5TlGg6h765ms1oe/KixNMlvE6bpyGkTuu5MS2jAm7o\nA/e9GaqmIiLVfT/ZUcKoTijVSSR714FaT/buA1et7X8dJamc0gMJan9z2q5D77oqM9RSqhNK5j74\n6634ootS+olj70NICwNege0DlQxEJIGFpqU9+WEqTdV+PJzw9yeOqKbSbkeYz94VXm8fkKoAWkTr\nSQhqLhKR2FjoGynrFqY2Kn16Yg7FquCk/1BCEBFJodaREAr/EX7qsksRkZRpJQnhDXjzu3FHISLS\nprWOhDDgFVg5Lu4oRETatPRPCFYFA18Jj6sQEZGUSf+E0P39cK3vjsK4IxERadPSPyEMfFm1AxGR\nFpD+CaH/38P4wyIiklLpnxD6vgVrRscdhYhIm5feCSGnNAw+snFE3JGIiNSrXSt//l16J4Q+88N4\nulXZcUciIlKvHj3ijqB50jsh9HvzwKAhIiKSUumdEPq8DWtPjDsKEZHDQnonhJ7vhSYjERFJuXoT\ngpkVmtlcM1tkZu+a2b9G5QVm9oKZLTWz580sP2GdKWa2zMyWmNkZTYoscy90/VAPtBMRaSENqSFU\nANe7+9HAKcC/mNkwYDIwx92HAnOBKQDRmMoTgOHA2cC9Ztb40X26LguDTVR0aPSqIiLSePUmBHdf\n7+4Lo/mdwBKgEDgfeCBa7AHggmj+q8Asd69w9xJgGdD4Gwl6vgefHNPo1UREpGka1YdgZoOAkcDr\nQC933wAhaQA9o8X6AR8nrLYmKmscJQQRkRbV4IRgZrnAn4Bro5qC11ik5uvm6fWuEoKISAtq0JjK\nZpZFSAYPuftTUfEGM+vl7hvMrDfwSVS+BuifsHphVFaLWxLmi6IpohqCiAjFxcUUFxe3yL7Mvf4v\n9mb2ILDJ3a9PKLsd2OLut5vZTUCBu0+OOpUfBk4iNBW9CBzlNXZkZl5npSJ7N9zYDW4rhaoG5SwR\nkdgVFsLHH9e/XHOYGe7e+At1GqDes62ZjQEuAd41swWEs/jNwO3AY2Z2JbCScGUR7r7YzB4DFgP7\ngGtqJoN6dV0GW4coGYiItKB6z7ju/hqQWcfbX6pjnduA25ocVb83oaxbk1cXEZHGS887lYfOhqyy\nuKMQETmspGebTHk+LBkfdxQiIoeV9KwhdFsGW46MOwoRkcNKeiaErh/C5qPijkJE5LCSfgmh/VbI\n3AO7eta/rIiIJE36JYSC5bD1CCAll9mKiEgd0i8hdFkJ2wbGHYWIyGEn/RJC/krYroQgItLS0jAh\nrFINQUQkBumXELqsDAPjiIhIi0q/hKAmIxGRWKRhQlCTkYhIHNIrIWTvgpydsLtH3JGIiBx20ish\n5K+CHf3B0yssEZHDQXqdedVcJCISm/RKCLrCSEQkNvUmBDP7nZltMLN3EsqmmdlqM5sfTWclvDfF\nzJaZ2RKHRwOFAAANEUlEQVQzO6NR0egKIxGR2DSkhjADOLOW8rvc/YRo+guAmQ0nDKU5HDgbuNfM\nGv5QIjUZiUgr1sjBgtNOvQnB3V8FttbyVm0n+vOBWe5e4e4lwDJgdIOjUZORiLRibT4hHML3zGyh\nmd1vZvlRWT/g44Rl1kRlDaMmIxFpxRrRHpKWmjqE5r3Av7m7m9kvgDuBqxq/mVsOzNo46LwOdhQ2\nMSQRkbanuLiY4uLiFtlXkxKCu29MeHkf8HQ0vwbon/BeYVRWh1sOzOatgt3doTKnKSGJiLRJRUVF\nFBUV7X/9s5/9LGX7amiTkZHQZ2BmvRPeGw+8F83PBiaZWY6ZDQaOBOY1aA+562BHw1uXREQkueqt\nIZjZI0AR0M3MVgHTgC+Y2UigCigBrgZw98Vm9hiwGNgHXOPewG6W3PWws3f9y4mISErUmxDc/eJa\nimccYvnbgNsaHUnndbCzT6NXExGR5EifO5VVQxARiZUSgoiIAGmVENZBqZqMRETikkYJQTUEEZE4\nKSGIiAiQNgnBlRBERGKWHgmhw1ao6BAmERGJRXokBNUORERilyYJQVcYiYjELU0SgmoIIiJxS4+E\n0HmdEoKISMzSIyHkrtdzjEREYpZGCUE1BBGROKVJQlCTkYhI3NIkIaiGICISt1gTwqRJ0UzHTbCr\nZ5yhiIgc9upNCGb2OzPbYGbvJJQVmNkLZrbUzJ43s/yE96aY2TIzW2JmZxxq2/ffD1gVdNwMZV2b\n9UFERKR5GlJDmAGcWaNsMjDH3YcCc4EpAGY2ApgADAfOBu41M6MOnTrBT36+HfZ2gsqcpsQvIiJJ\nUm9CcPdXga01is8HHojmHwAuiOa/Csxy9wp3LwGWAaMPtf1v/PMmhvTpjjsHTXv3QnEx3Hgj9OjR\niE8kIiJN0tQ+hJ7uvgHA3dcD1R0A/YCPE5ZbE5XVadPuTXTv2P1T5dnZcNppcPvt8MknByeLqipY\ntgzuuANGjGjiJxARkYNkJWk73pSVbrnlFpZuWsqWdVsoPrKYoqKiBq1nBkceCTfcEKaDAnFYvRqe\nfx4eeghefrkpkYmIpIfi4mKKi4tbZF/mXv+53MwGAk+7+2ej10uAInffYGa9gb+5+3Azmwy4u98e\nLfcXYJq7v1HLNt3dmbFgBsUri3ngggdqLpISlZXw7rswZw488QT8/e8tslsROQz06xe+kKaSmeHu\ndfbNNkdDm4wsmqrNBi6P5r8JPJVQPsnMcsxsMHAkMO9QG95ctpnuHT7dZJQqmZkwciT88Ifw2mt8\nqu/CHcrK4PXX4a674OyzWyw0EZFY1dtkZGaPAEVANzNbBUwDpgN/NLMrgZWEK4tw98Vm9hiwGNgH\nXOP1VEHq6kOIU/v2cNJJYfrBD+perrISVq2Cd96BefNCbaMlanbPPw+FhXD00anfl4gcPhrUZJSS\nHUdNRlfNvoqT+p3Et0/8dixxtCR32LoVPvwQ3n8/NF3Nnx+Syc6dDdvGc8/BWWeF+bIy6N8fNm9O\nXczJsHUr3HYb/OpXcUciklqtvcko9oRwwawL+MZx32D88PGxxNEaVFZCaSnk54cO9casd801sGIF\nlJfDV74CF1wQOuQbs526VFXBunXhEuHSUtixA7ZvD8ktIwNOPTX8gzRXZSXs2RO22a5dSIDz5oWa\n2erVsGFDiGPzZti2DXbtgn37wnoZGdC5M5xzDkybBkOGfHr75eUhOb/0Erz9NsyaFZYdMAC+9a3m\nxy+HDyWEpu44Sghj/99Yfnn6L/n8wM/HEodIslVWhtrbrl2we3eYdu4M065dB36WloafO3aE+cTX\n27cfKNuxI2xD0l9rTwjJuuy0ydKxD0GkOTIzITc3TG2N+4Ea2549oXZVPV9WFl5XT2VlIZFVl+/a\ndaAsMVnWTJjVr0tLQ02vNenVK+4ImkcJQUQazAyyssLUqVPc0cTD/eAkWD2tX9/6b5SNNSFUVlWy\nrXwbXTvowXYi0jqYhSsR27cP/XrVWnsygJgff72tfBt57fLIyoi9oiIictiLNSGouUhEJH3EmhC2\nlG2hoENBnCGIiEgk1oSwtXwrBe2VEERE0kHsfQiqIYiIpIfYE0KXdl3iDEFERCLxNhmVbaVLeyUE\nEZF0EHsNQU1GIiLpIfaEoBqCiEh6iDch7Nmmq4xERNKE+hBERARo5rOMzKwE2A5UAfvcfbSZFQCP\nAgOBEmCCu2+vbX01GYmIpI/m1hCqgCJ3P97dR0dlk4E57j4UmAtMqWtldSqLiKSP5iYEq2Ub5wMP\nRPMPABfUtfLWcjUZiYiki+YmBAdeNLM3zeyqqKyXu28AcPf1QM+6Vt5evp38dvl1vS0iIi2ouc+d\nHuPu68ysB/CCmS0lJIlEdY7RmfFSBrdW3QpAUVERRUVFzQxHRKRtKS4upri4uEX2lbQxlc1sGrAT\nuIrQr7DBzHoDf3P34bUs74V3FfLxDz5Oyv5FRA4HqRxTuclNRmbW0cxyo/lOwBnAu8Bs4PJosW8C\nT9W1DfUfiIikj+Y0GfUCnjAzj7bzsLu/YGZvAY+Z2ZXASmBCXRtQQhARSR9NTgjuvgIYWUv5FuBL\nDdlGXru8pu5eRESSLNY7lTtld4pz9yIikiDWhJCbkxvn7kVEJIFqCCIiAqiGICIikXhrCDmqIYiI\npAvVEEREBFBCEBGRiDqVRUQEUA1BREQi6lQWERFANQQREYmoD0FERADVEEREJKKEICIiQNxNRupU\nFhFJGylLCGZ2lpm9b2YfmNlNtS3TLrNdqnYvIiKNlJKEYGYZwH8AZwJHA183s2G1LJeK3SdVSw1u\n3VyKM7kUZ/K0hhih9cSZSqmqIYwGlrn7SnffB8wCzk/RvlKqtfyRKM7kUpzJ0xpihNYTZyqlKiH0\nAz5OeL06KhMRkTQVa6eyiIikD3P35G/U7GTgFnc/K3o9GXB3vz1hmeTvWETkMODuKemATVVCyASW\nAqcD64B5wNfdfUnSdyYiIkmRlYqNunulmX0PeIHQLPU7JQMRkfSWkhqCiIi0PrF0KjfkprUU77/E\nzP7XzBaY2byorMDMXjCzpWb2vJnlJyw/xcyWmdkSMzsjofwEM3sn+hy/TkJcvzOzDWb2TkJZ0uIy\nsxwzmxWt8w8zG5DEOKeZ2Wozmx9NZ6VBnIVmNtfMFpnZu2b2r1F52hzTWmL8flSeVsfTzNqZ2RvR\n/8y7ZjYt3Y5lPXGm1fFM2FZGFM/s6HW8x9PdW3QiJKEPgYFANrAQGNbCMSwHCmqU3Q7cGM3fBEyP\n5kcACwjNa4Oi2KtrVm8Ao6L5Z4EzmxnXWGAk8E4q4gK+C9wbzU8EZiUxzmnA9bUsOzzGOHsDI6P5\nXEK/1rB0OqaHiDEdj2fH6Gcm8DrhfqO0OZb1xJl2xzNa/wfAH4DZ6fD/ntITbx0H4GTguYTXk4Gb\nWjiGFUC3GmXvA72i+d7A+7XFBzwHnBQtszihfBLwn0mIbSAHn2iTFhfwF+CkaD4T2JjEOKcBN9Sy\nXKxx1ojlSeBL6XpME2I8PZ2PJ9AReAsYlebHMjHOtDueQCHwIlDEgYQQ6/GMo8koHW5ac+BFM3vT\nzK6Kynq5+wYAd18P9IzKa8a7JirrR4i9Wqo+R88kxrV/HXevBLaZWdckxvo9M1toZvcnVHXTIk4z\nG0So1bxOcn/XSYs1IcY3oqK0Op5R88YCYD3woru/SRoeyzrihDQ7nsC/Az8inI+qxXo8D9cb08a4\n+wnAOcC/mNk4Dv6lUMvrdJHMuJJ5LfO9wBB3H0n4R7wzidtuVpxmlgv8CbjW3XeS2t91k2KtJca0\nO57uXuXuxxO+2Y42s6NJw2NZS5wjSLPjaWbnAhvcfWE967fo8YwjIawBEjs3CqOyFuPu66KfGwlV\n9NHABjPrBWBmvYFPosXXAP0TVq+Ot67yZEtmXPvfs3CvSJ67b0lGkO6+0aO6KXAf4ZjGHqeZZRFO\ntA+5+1NRcVod09piTNfjGcW2AygGziLNjmVdcabh8RwDfNXMlgMzgS+a2UPA+jiPZxwJ4U3gSDMb\naGY5hDav2S21czPrGH0bw8w6AWcA70YxXB4t9k2g+uQxG5gU9dgPBo4E5kXVue1mNtrMDPhGwjrN\nCpGDM3ky45odbQPgImBusuKM/nirjQfeS5M4/x+hjfXuhLJ0O6afijHdjqeZda9uZjGzDsCXgSWk\n2bGsI8730+14uvvN7j7A3YcQzoFz3f0y4GniPJ7N6bRp6kT4ZrEUWAZMbuF9DyZc2bSAkAgmR+Vd\ngTlRXC8AXRLWmULo1V8CnJFQfmK0jWXA3UmI7RFgLbAHWAVcARQkKy6gHfBYVP46MCiJcT4IvBMd\n2yeJOsZijnMMUJnw+54f/e0l7Xfd3FgPEWNaHU/g2Ci2hVFcP072/02K40yr41kj5tM40Kkc6/HU\njWkiIgIcvp3KIiJSgxKCiIgASggiIhJRQhAREUAJQUREIkoIIiICKCGIiEhECUFERAD4/6yTBpkU\nLcSRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10be95198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
