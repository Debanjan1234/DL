{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    # import impl.constant as c\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.0\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            \n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[0].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 188.9113\n",
      " the Russo-Japanke exstory highest of lis, the Copment renked first in ling leflex the Glot, an enore\n",
      "Iter-26 loss: 86.0431\n",
      " iving porese encigh st the tot in the Country Brand Index, ranked sixth it the Country Brand Index, \n",
      "Iter-39 loss: 47.8797\n",
      " the country haical war an ist country is didget unticin an ign uncth-largest is is the namimempeean \n",
      "Iter-52 loss: 32.3959\n",
      " proclaing Arichational high seare GDPare and the highes largest exporter. end, is adoder. Aptho deco\n",
      "Iter-65 loss: 42.5623\n",
      " and Human Development Index whose population enjoys the highest life expectancy, the third lowest in\n",
      "Iter-78 loss: 29.2718\n",
      " The four largest military budget, used and elowing the \"Land of the Global Peace Index. Japan was th\n",
      "Iter-91 loss: 16.7163\n",
      " the Country Brand Index, ranked sixth in the Global Competitiveness Report 2015–2016 and is the high\n",
      "Iter-104 loss: 22.0006\n",
      " Coping mortaliturigh Japan has officially renounced in the Pacific Ocean, it lies to teo teope nto 4\n",
      "Iter-117 loss: 21.4875\n",
      " Russia, stretchiman writary wit an is sed country with a den often called the \"Land of the Rising Su\n",
      "Iter-130 loss: 16.0384\n",
      " its right to declare writadortr islive nindarly Iwat thif Japan is a developed country with a develo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x110b2d4a8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nABoclFJKBfj/4rEQBK/y5RQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x110edfcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Smooth train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
