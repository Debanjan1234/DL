{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters weights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_one_hot = X.copy()\n",
    "        h_old = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X_prime = np.column_stack((hr * h_old, X_one_hot))\n",
    "        hh, hh_cache = l.fc_forward(X_prime, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = (1. - hz) * h_old + hz * hh\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        if train: \n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache, do_cache)\n",
    "        else: # not train but test\n",
    "            cache = (X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, \n",
    "                     hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train: # include dropout_cache/do_cache\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else: # not train but test\n",
    "            X, X_prime, h_old, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dhh = hz * dh\n",
    "        dh_old1 = (1. - hz) * dh\n",
    "        dhz = hh * dh - h_old * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dX_prime, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh_prime = dX_prime[:, :self.H]\n",
    "        dh_old2 = hr * dh_prime\n",
    "\n",
    "        dhr = h_old * dh_prime\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_old3 = dX[:, :self.H]\n",
    "\n",
    "        dh = dh_old1 + dh_old2 + dh_old3\n",
    "        dX = dX[:, self.H:]\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0] # no regularizarion or no reg_loss\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Backprop\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    # import impl.constant as c\n",
    "    eps = 1e-8\n",
    "    state = nn.initial_state()\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches/ full batches/ batch files\n",
    "        # Minibacthes\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "        \n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[0].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=(mb_size*10))\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 loss: 3.4951\n",
      "cest of the Rasionh. Seacite eallym of Japan the lome eo torestuen no mal civin slacly city lotin the\n",
      "Iter-260 loss: 2.9312\n",
      "ch includest Chinea, ho eabond of the ras thanusyusunandese hintokh the theake7, hhits eprl the ntmer\n",
      "Iter-390 loss: 2.7027\n",
      "chy with anese Wares early 126 milisi Japan is in end rene en malay ia lwggnskn ins in iin the sourta\n",
      "Iter-520 loss: 2.3905\n",
      "ch includest int ih lambof urer om Ok orts Worly a devien Grwarch Sea Sinconarname 1rgas tgees.rnal C\n",
      "Iter-650 loss: 2.5081\n",
      "city properhest. city ins inthivedededgiopun, w strea aina Emperor I live a med of inolevenal ctary s\n",
      "Iter-780 loss: 2.7267\n",
      "city properitcenting in 1947oon r Sunarompirdd cres in enterlal cons it centuriald of Japanese Ward l\n",
      "Iter-910 loss: 4.9055\n",
      "cted legislamese Eastoree auo the Gopuly of lubin , ath labal citen i land Sun\" the Simetof the eup l\n",
      "Iter-1040 loss: 2.5394\n",
      "ccessive felware East ohtshitstivine Eanke of mallocet,in un in Aith lar, the mound arnh in aigh reae\n",
      "Iter-1170 loss: 2.7594\n",
      "c Games.\n",
      ". intorea ns mian Jaint in the mourr. The Simlet 20thing and Himat res of in the hist. of Hi\n",
      "Iter-1300 loss: 2.3795\n",
      "conomy by pevirpond of in the world'stonemperemlarn is the world's anethebelor per 2imion, is of any \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.GRU at 0x7f3a3bcacef0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1300 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a6f88d208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
