{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache\n",
    "            )\n",
    "        else: # train=False\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache\n",
    "            )\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=(mb_size*10)) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 loss: 4.3165\n",
      "conomy by palionwited four eas n Shuserca aleaen cl enfrctanes inte wrk\"a an the country uh and Sanke\n",
      "Iter-260 loss: 2.9742\n",
      "ct. tht mortert in the sourt-,8, pnoule liefenfl econ expl NeNitaneciinse Uxme, wrollettnunt innaran \n",
      "Iter-390 loss: 6.1705\n",
      "conomy by ppofe counst Jana hh then cian Sea and the U7, tho plopanen isoun, San of 68eaines dasut en\n",
      "Iter-520 loss: 2.6468\n",
      "came to an siven \"sume fouller makl cod larrd of lamake usmallowy ih consmit Gld5ita luobtron millobe\n",
      "Iter-650 loss: 2.6670\n",
      "chy with and Hous the world Worldcioudgpinf in Rsicou,taroolothing iistrto pevporia, whisi Rusporo ch\n",
      "Iter-780 loss: 7.7952\n",
      "city proper . Japan u anol.. Ahing Japan sompertan is and Sen ead sevpered of the worldocteecea of 1s\n",
      "Iter-910 loss: 2.4233\n",
      "constitution in intaldeectict reagek e Sleots forsh Sintaren sixthe countehes-lawe frthe of peeicol c\n",
      "Iter-1040 loss: 6.3418\n",
      "ccessive feapno Japan eoplrthitnd in the nila in the world's lecg onstre Worthn a ontt Sen foret Sino\n",
      "Iter-1170 loss: 2.5815\n",
      "c Games.\n",
      ". Japan wat eepea en ean2os tha m ale rones ranal oa Sha anded singeg, ranked fourth-largest\n",
      "Iter-1300 loss: 2.5908\n",
      "constitution mnd inne-HHomat. honsouratur conest c cllltien-f HPmern--eorlar paree forent-ragesed a M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM at 0x7f5f2bf65ac8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1300 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5f2bf09c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
