{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    # P_dropout == keep_prob in this case!\n",
    "    # q = keep_prob and p_dropout = p ???\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache)\n",
    "        else: # train=False\n",
    "            cache = (X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, \n",
    "                     ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 39.2908\n",
      "iveportit unmge Netas, the Rectgted the otiunthe Sitys ans The iao-vinotira, the ci udentarivirs of m\n",
      "Iter-26 loss: 30.0147\n",
      "ilg of the wirndex, and ade sdd. red the nulolepoutth-lame2 laromac of the Cototh ol wed diteta of ir\n",
      "Iter-39 loss: 32.9972\n",
      "in. Aranken Epis the wor, centical rigth, pecheniod Cotar the namomy of count exrpin the soun Nimy's \n",
      "Iter-52 loss: 19.7579\n",
      "ing if aran bapi-xand NeJapited Comgest militari and in milion the world, and ary with Japan milight \n",
      "Iter-65 loss: 17.8051\n",
      "ing af wortal roppoution with ule agest of Wet fomess uran, with a Acth the Sich en to the Eith last \n",
      "Iter-78 loss: 14.4913\n",
      "inder of the six. livea) prope worldka tary moned highes, whith adht with rilolictaciy iginatimaticca\n",
      "Iter-91 loss: 17.7135\n",
      "isegy in the world's tenth largest e7th in in the laure. is the nuralich a cally by peacitary in as a\n",
      "Iter-104 loss: 18.2462\n",
      "ies, the worr whiapion, partitary in of 126 miltalatefolitary with the East in the centisinl poserrar\n",
      "Iter-117 loss: 18.3611\n",
      "ing P. The first country in follconji chy has ced the World latents liciet an isinncolympionikeet pou\n",
      "Iter-130 loss: 29.9901\n",
      "ised Japan in the Emperor. whhu Emperor Japan chalic of world's fourth-largest country it military sh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM at 0x7fec2f85a048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCFFGSEAXQogyQgK6EEKUEf8PZEOJJrJNkU0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec2f345160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
