{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache\n",
    "            )\n",
    "        else: # train=False\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache\n",
    "            )\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    #     for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "    smooth_loss = 1\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "            \n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=(mb_size*10)) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 43.4242\n",
      "in ranes, Japan the Napesphamac an in in the Watof wetend-and it randivemat ind and vioWanabe intexlx\n",
      "Iter-2 loss: 42.0389\n",
      "ist worldd enpimpectarin wiitt in exan to nNoun founuranked Hounts Asias the country worl fri bal hic\n",
      "Iter-3 loss: 40.6792\n",
      "in is enjeSt1r vustin the hinh stan I, hass in of laoned first piwest enth, encountry by up and rede.\n",
      "Iter-4 loss: 35.6667\n",
      "inest the micina an was the fourtry Chinan as and with the woruna, end the Ul1Dhosunion to the bonc f\n",
      "Iter-5 loss: 35.6970\n",
      "incy was thin was rulof the to udisic Chinal boset in the byatest inal GP91917190increfedts instiresi\n",
      "Iter-6 loss: 32.9916\n",
      "ing hag nurtale global Dume the Gloug for sures astares head ecorth Japan's whogentest cipponfe thit \n",
      "Iter-7 loss: 28.9322\n",
      "ic as to the eighth-largest econorna, 6Pate of the fourth ry mente fourthich of isolasled an rsour th\n",
      "Iter-8 loss: 25.7986\n",
      "iFrth the Sea of Noben divinly fumme largest eicing population wasidentandat inst in the highest-raba\n",
      "Iter-9 loss: 21.0189\n",
      "iji which a omomonanked Asian, thily-Inst nndex, expectany rulos oxing vile moinal DIminly Iumpering \n",
      "Iter-10 loss: 17.0495\n",
      "ic a of \"Sino-detly unsicly rano as devered fourth-largest-ranges an is fourth-largest in of inth cou\n",
      "Iter-11 loss: 15.4861\n",
      "illion hectures a developed to land eadounc, it liintaing as a sixth rbar 35 firth rrgest ation in th\n",
      "Iter-12 loss: 12.6893\n",
      "ituncy is ranked Asian, and rank givenast in the G20 the highest ihth largest indy, the highest of Ho\n",
      "Iter-13 loss: 13.5499\n",
      "ike hughast of la9ing  country in with the to the world, and surest tion regivenare vet-larged and it\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM at 0x10bf30a58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 13 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 1.0 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWYQIfUEQhCzi/wEzwjuZiV5yNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aae7320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
