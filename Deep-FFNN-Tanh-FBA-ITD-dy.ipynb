{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "#         dX = dout @ W.T # vanilla Backprop\n",
    "        dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "#         dy =  dy @ self.W_fixed[2].T # done\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "#             dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "            dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "#             dy =  dy @ self.W_fixed[2].T # done\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "#         dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "        dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3124, acc-0.1000, valid loss-2.3055, acc-0.1042, test loss-2.2996, acc-0.1094\n",
      "Iter-20, train loss-2.3156, acc-0.0800, valid loss-2.3054, acc-0.1040, test loss-2.2995, acc-0.1096\n",
      "Iter-30, train loss-2.3028, acc-0.1000, valid loss-2.3053, acc-0.1044, test loss-2.2994, acc-0.1097\n",
      "Iter-40, train loss-2.2974, acc-0.1200, valid loss-2.3052, acc-0.1048, test loss-2.2992, acc-0.1099\n",
      "Iter-50, train loss-2.2896, acc-0.1000, valid loss-2.3051, acc-0.1050, test loss-2.2991, acc-0.1099\n",
      "Iter-60, train loss-2.2824, acc-0.1000, valid loss-2.3050, acc-0.1056, test loss-2.2990, acc-0.1103\n",
      "Iter-70, train loss-2.3210, acc-0.1400, valid loss-2.3048, acc-0.1060, test loss-2.2989, acc-0.1103\n",
      "Iter-80, train loss-2.2946, acc-0.0600, valid loss-2.3047, acc-0.1060, test loss-2.2987, acc-0.1105\n",
      "Iter-90, train loss-2.2940, acc-0.1200, valid loss-2.3046, acc-0.1060, test loss-2.2986, acc-0.1106\n",
      "Iter-100, train loss-2.3170, acc-0.0800, valid loss-2.3045, acc-0.1062, test loss-2.2985, acc-0.1112\n",
      "Iter-110, train loss-2.3095, acc-0.1600, valid loss-2.3043, acc-0.1064, test loss-2.2984, acc-0.1117\n",
      "Iter-120, train loss-2.2958, acc-0.1800, valid loss-2.3042, acc-0.1070, test loss-2.2983, acc-0.1119\n",
      "Iter-130, train loss-2.2892, acc-0.1200, valid loss-2.3041, acc-0.1070, test loss-2.2981, acc-0.1120\n",
      "Iter-140, train loss-2.2865, acc-0.1400, valid loss-2.3040, acc-0.1072, test loss-2.2980, acc-0.1126\n",
      "Iter-150, train loss-2.3005, acc-0.1000, valid loss-2.3039, acc-0.1072, test loss-2.2979, acc-0.1131\n",
      "Iter-160, train loss-2.3291, acc-0.1000, valid loss-2.3037, acc-0.1074, test loss-2.2977, acc-0.1133\n",
      "Iter-170, train loss-2.2630, acc-0.1400, valid loss-2.3036, acc-0.1078, test loss-2.2976, acc-0.1136\n",
      "Iter-180, train loss-2.2946, acc-0.0400, valid loss-2.3035, acc-0.1078, test loss-2.2975, acc-0.1139\n",
      "Iter-190, train loss-2.3165, acc-0.0600, valid loss-2.3034, acc-0.1078, test loss-2.2974, acc-0.1146\n",
      "Iter-200, train loss-2.3027, acc-0.1000, valid loss-2.3033, acc-0.1080, test loss-2.2972, acc-0.1148\n",
      "Iter-210, train loss-2.2844, acc-0.1600, valid loss-2.3031, acc-0.1090, test loss-2.2971, acc-0.1148\n",
      "Iter-220, train loss-2.2992, acc-0.1400, valid loss-2.3030, acc-0.1094, test loss-2.2970, acc-0.1150\n",
      "Iter-230, train loss-2.3168, acc-0.0800, valid loss-2.3029, acc-0.1100, test loss-2.2969, acc-0.1155\n",
      "Iter-240, train loss-2.3138, acc-0.0800, valid loss-2.3028, acc-0.1104, test loss-2.2968, acc-0.1157\n",
      "Iter-250, train loss-2.3075, acc-0.1400, valid loss-2.3027, acc-0.1112, test loss-2.2966, acc-0.1159\n",
      "Iter-260, train loss-2.3204, acc-0.0600, valid loss-2.3026, acc-0.1112, test loss-2.2965, acc-0.1168\n",
      "Iter-270, train loss-2.2940, acc-0.0600, valid loss-2.3024, acc-0.1118, test loss-2.2964, acc-0.1174\n",
      "Iter-280, train loss-2.3067, acc-0.1400, valid loss-2.3023, acc-0.1118, test loss-2.2962, acc-0.1175\n",
      "Iter-290, train loss-2.3117, acc-0.0400, valid loss-2.3022, acc-0.1126, test loss-2.2961, acc-0.1180\n",
      "Iter-300, train loss-2.3154, acc-0.1800, valid loss-2.3021, acc-0.1126, test loss-2.2960, acc-0.1183\n",
      "Iter-310, train loss-2.2849, acc-0.1400, valid loss-2.3019, acc-0.1124, test loss-2.2959, acc-0.1185\n",
      "Iter-320, train loss-2.2920, acc-0.1200, valid loss-2.3018, acc-0.1126, test loss-2.2957, acc-0.1192\n",
      "Iter-330, train loss-2.2703, acc-0.1400, valid loss-2.3017, acc-0.1126, test loss-2.2956, acc-0.1192\n",
      "Iter-340, train loss-2.2992, acc-0.1000, valid loss-2.3016, acc-0.1128, test loss-2.2955, acc-0.1192\n",
      "Iter-350, train loss-2.3093, acc-0.0200, valid loss-2.3015, acc-0.1134, test loss-2.2954, acc-0.1194\n",
      "Iter-360, train loss-2.2988, acc-0.1400, valid loss-2.3013, acc-0.1136, test loss-2.2953, acc-0.1193\n",
      "Iter-370, train loss-2.3167, acc-0.1200, valid loss-2.3012, acc-0.1136, test loss-2.2951, acc-0.1191\n",
      "Iter-380, train loss-2.3155, acc-0.1200, valid loss-2.3011, acc-0.1138, test loss-2.2950, acc-0.1196\n",
      "Iter-390, train loss-2.3051, acc-0.0800, valid loss-2.3010, acc-0.1142, test loss-2.2949, acc-0.1195\n",
      "Iter-400, train loss-2.2938, acc-0.2000, valid loss-2.3009, acc-0.1144, test loss-2.2948, acc-0.1200\n",
      "Iter-410, train loss-2.3001, acc-0.0800, valid loss-2.3008, acc-0.1142, test loss-2.2946, acc-0.1204\n",
      "Iter-420, train loss-2.2965, acc-0.0400, valid loss-2.3006, acc-0.1142, test loss-2.2945, acc-0.1205\n",
      "Iter-430, train loss-2.2791, acc-0.1200, valid loss-2.3005, acc-0.1144, test loss-2.2944, acc-0.1208\n",
      "Iter-440, train loss-2.2964, acc-0.1200, valid loss-2.3004, acc-0.1144, test loss-2.2942, acc-0.1210\n",
      "Iter-450, train loss-2.2898, acc-0.0600, valid loss-2.3003, acc-0.1146, test loss-2.2941, acc-0.1213\n",
      "Iter-460, train loss-2.2608, acc-0.2600, valid loss-2.3002, acc-0.1146, test loss-2.2940, acc-0.1217\n",
      "Iter-470, train loss-2.2832, acc-0.2000, valid loss-2.3000, acc-0.1152, test loss-2.2939, acc-0.1223\n",
      "Iter-480, train loss-2.2727, acc-0.0600, valid loss-2.2999, acc-0.1156, test loss-2.2938, acc-0.1225\n",
      "Iter-490, train loss-2.2730, acc-0.1000, valid loss-2.2998, acc-0.1160, test loss-2.2936, acc-0.1227\n",
      "Iter-500, train loss-2.2884, acc-0.1000, valid loss-2.2997, acc-0.1162, test loss-2.2935, acc-0.1228\n",
      "Iter-510, train loss-2.3092, acc-0.1200, valid loss-2.2996, acc-0.1168, test loss-2.2934, acc-0.1228\n",
      "Iter-520, train loss-2.2957, acc-0.1600, valid loss-2.2995, acc-0.1166, test loss-2.2933, acc-0.1231\n",
      "Iter-530, train loss-2.3068, acc-0.1000, valid loss-2.2993, acc-0.1168, test loss-2.2932, acc-0.1231\n",
      "Iter-540, train loss-2.2716, acc-0.1800, valid loss-2.2992, acc-0.1176, test loss-2.2930, acc-0.1233\n",
      "Iter-550, train loss-2.3062, acc-0.1400, valid loss-2.2991, acc-0.1184, test loss-2.2929, acc-0.1235\n",
      "Iter-560, train loss-2.2945, acc-0.1200, valid loss-2.2990, acc-0.1186, test loss-2.2928, acc-0.1239\n",
      "Iter-570, train loss-2.3448, acc-0.0800, valid loss-2.2989, acc-0.1190, test loss-2.2927, acc-0.1241\n",
      "Iter-580, train loss-2.2881, acc-0.1800, valid loss-2.2988, acc-0.1190, test loss-2.2926, acc-0.1245\n",
      "Iter-590, train loss-2.3229, acc-0.0800, valid loss-2.2986, acc-0.1192, test loss-2.2924, acc-0.1247\n",
      "Iter-600, train loss-2.2767, acc-0.2000, valid loss-2.2985, acc-0.1200, test loss-2.2923, acc-0.1248\n",
      "Iter-610, train loss-2.2938, acc-0.1400, valid loss-2.2984, acc-0.1202, test loss-2.2922, acc-0.1250\n",
      "Iter-620, train loss-2.2964, acc-0.1200, valid loss-2.2983, acc-0.1202, test loss-2.2921, acc-0.1256\n",
      "Iter-630, train loss-2.3005, acc-0.1400, valid loss-2.2982, acc-0.1210, test loss-2.2920, acc-0.1257\n",
      "Iter-640, train loss-2.2797, acc-0.0600, valid loss-2.2980, acc-0.1210, test loss-2.2918, acc-0.1258\n",
      "Iter-650, train loss-2.2811, acc-0.1600, valid loss-2.2979, acc-0.1214, test loss-2.2917, acc-0.1260\n",
      "Iter-660, train loss-2.3138, acc-0.0400, valid loss-2.2978, acc-0.1218, test loss-2.2916, acc-0.1265\n",
      "Iter-670, train loss-2.2859, acc-0.1000, valid loss-2.2977, acc-0.1220, test loss-2.2915, acc-0.1265\n",
      "Iter-680, train loss-2.2791, acc-0.1800, valid loss-2.2976, acc-0.1230, test loss-2.2913, acc-0.1264\n",
      "Iter-690, train loss-2.2858, acc-0.0800, valid loss-2.2974, acc-0.1230, test loss-2.2912, acc-0.1270\n",
      "Iter-700, train loss-2.2907, acc-0.1200, valid loss-2.2973, acc-0.1230, test loss-2.2911, acc-0.1274\n",
      "Iter-710, train loss-2.2984, acc-0.0800, valid loss-2.2972, acc-0.1236, test loss-2.2909, acc-0.1277\n",
      "Iter-720, train loss-2.2966, acc-0.1000, valid loss-2.2971, acc-0.1238, test loss-2.2908, acc-0.1278\n",
      "Iter-730, train loss-2.2879, acc-0.2200, valid loss-2.2970, acc-0.1238, test loss-2.2907, acc-0.1281\n",
      "Iter-740, train loss-2.2862, acc-0.0800, valid loss-2.2969, acc-0.1238, test loss-2.2906, acc-0.1281\n",
      "Iter-750, train loss-2.2944, acc-0.1200, valid loss-2.2967, acc-0.1242, test loss-2.2905, acc-0.1282\n",
      "Iter-760, train loss-2.2881, acc-0.1200, valid loss-2.2966, acc-0.1250, test loss-2.2903, acc-0.1286\n",
      "Iter-770, train loss-2.3077, acc-0.0800, valid loss-2.2965, acc-0.1254, test loss-2.2902, acc-0.1286\n",
      "Iter-780, train loss-2.2661, acc-0.1800, valid loss-2.2964, acc-0.1254, test loss-2.2901, acc-0.1291\n",
      "Iter-790, train loss-2.2842, acc-0.1200, valid loss-2.2963, acc-0.1260, test loss-2.2900, acc-0.1293\n",
      "Iter-800, train loss-2.2661, acc-0.1800, valid loss-2.2961, acc-0.1266, test loss-2.2898, acc-0.1293\n",
      "Iter-810, train loss-2.2985, acc-0.1200, valid loss-2.2960, acc-0.1268, test loss-2.2897, acc-0.1296\n",
      "Iter-820, train loss-2.2869, acc-0.1800, valid loss-2.2959, acc-0.1272, test loss-2.2896, acc-0.1298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.2762, acc-0.2200, valid loss-2.2958, acc-0.1274, test loss-2.2895, acc-0.1304\n",
      "Iter-840, train loss-2.3264, acc-0.0600, valid loss-2.2957, acc-0.1276, test loss-2.2894, acc-0.1307\n",
      "Iter-850, train loss-2.2991, acc-0.0600, valid loss-2.2956, acc-0.1276, test loss-2.2893, acc-0.1312\n",
      "Iter-860, train loss-2.3071, acc-0.1000, valid loss-2.2954, acc-0.1278, test loss-2.2891, acc-0.1313\n",
      "Iter-870, train loss-2.2836, acc-0.1800, valid loss-2.2953, acc-0.1280, test loss-2.2890, acc-0.1317\n",
      "Iter-880, train loss-2.3156, acc-0.0800, valid loss-2.2952, acc-0.1284, test loss-2.2889, acc-0.1316\n",
      "Iter-890, train loss-2.2708, acc-0.2400, valid loss-2.2951, acc-0.1282, test loss-2.2888, acc-0.1318\n",
      "Iter-900, train loss-2.2823, acc-0.1800, valid loss-2.2950, acc-0.1284, test loss-2.2887, acc-0.1320\n",
      "Iter-910, train loss-2.2949, acc-0.1200, valid loss-2.2949, acc-0.1288, test loss-2.2885, acc-0.1323\n",
      "Iter-920, train loss-2.2873, acc-0.0600, valid loss-2.2947, acc-0.1290, test loss-2.2884, acc-0.1325\n",
      "Iter-930, train loss-2.3202, acc-0.1400, valid loss-2.2946, acc-0.1292, test loss-2.2883, acc-0.1334\n",
      "Iter-940, train loss-2.2751, acc-0.1400, valid loss-2.2945, acc-0.1300, test loss-2.2882, acc-0.1334\n",
      "Iter-950, train loss-2.2580, acc-0.2000, valid loss-2.2944, acc-0.1304, test loss-2.2880, acc-0.1336\n",
      "Iter-960, train loss-2.2506, acc-0.1800, valid loss-2.2943, acc-0.1310, test loss-2.2879, acc-0.1339\n",
      "Iter-970, train loss-2.2974, acc-0.1400, valid loss-2.2941, acc-0.1312, test loss-2.2878, acc-0.1339\n",
      "Iter-980, train loss-2.2774, acc-0.1000, valid loss-2.2940, acc-0.1314, test loss-2.2877, acc-0.1340\n",
      "Iter-990, train loss-2.2699, acc-0.1600, valid loss-2.2939, acc-0.1314, test loss-2.2876, acc-0.1341\n",
      "Iter-1000, train loss-2.2561, acc-0.2200, valid loss-2.2938, acc-0.1316, test loss-2.2874, acc-0.1345\n",
      "Iter-1010, train loss-2.3099, acc-0.1200, valid loss-2.2937, acc-0.1314, test loss-2.2873, acc-0.1348\n",
      "Iter-1020, train loss-2.2809, acc-0.2000, valid loss-2.2936, acc-0.1316, test loss-2.2872, acc-0.1351\n",
      "Iter-1030, train loss-2.2947, acc-0.1200, valid loss-2.2934, acc-0.1318, test loss-2.2871, acc-0.1355\n",
      "Iter-1040, train loss-2.2954, acc-0.1200, valid loss-2.2933, acc-0.1316, test loss-2.2870, acc-0.1355\n",
      "Iter-1050, train loss-2.2984, acc-0.1400, valid loss-2.2932, acc-0.1320, test loss-2.2868, acc-0.1356\n",
      "Iter-1060, train loss-2.2892, acc-0.0800, valid loss-2.2931, acc-0.1316, test loss-2.2867, acc-0.1359\n",
      "Iter-1070, train loss-2.2797, acc-0.2000, valid loss-2.2930, acc-0.1316, test loss-2.2866, acc-0.1361\n",
      "Iter-1080, train loss-2.2948, acc-0.1600, valid loss-2.2929, acc-0.1320, test loss-2.2865, acc-0.1361\n",
      "Iter-1090, train loss-2.2785, acc-0.2000, valid loss-2.2928, acc-0.1322, test loss-2.2864, acc-0.1361\n",
      "Iter-1100, train loss-2.2992, acc-0.1800, valid loss-2.2927, acc-0.1326, test loss-2.2862, acc-0.1367\n",
      "Iter-1110, train loss-2.2930, acc-0.1400, valid loss-2.2925, acc-0.1326, test loss-2.2861, acc-0.1371\n",
      "Iter-1120, train loss-2.3123, acc-0.1600, valid loss-2.2924, acc-0.1326, test loss-2.2860, acc-0.1369\n",
      "Iter-1130, train loss-2.3212, acc-0.1200, valid loss-2.2923, acc-0.1328, test loss-2.2859, acc-0.1373\n",
      "Iter-1140, train loss-2.2952, acc-0.1200, valid loss-2.2922, acc-0.1328, test loss-2.2858, acc-0.1376\n",
      "Iter-1150, train loss-2.2731, acc-0.0800, valid loss-2.2921, acc-0.1330, test loss-2.2857, acc-0.1375\n",
      "Iter-1160, train loss-2.2853, acc-0.1000, valid loss-2.2920, acc-0.1332, test loss-2.2855, acc-0.1376\n",
      "Iter-1170, train loss-2.2875, acc-0.1000, valid loss-2.2919, acc-0.1336, test loss-2.2854, acc-0.1381\n",
      "Iter-1180, train loss-2.3087, acc-0.1200, valid loss-2.2917, acc-0.1344, test loss-2.2853, acc-0.1382\n",
      "Iter-1190, train loss-2.2921, acc-0.0400, valid loss-2.2916, acc-0.1344, test loss-2.2852, acc-0.1385\n",
      "Iter-1200, train loss-2.2899, acc-0.1600, valid loss-2.2915, acc-0.1342, test loss-2.2851, acc-0.1385\n",
      "Iter-1210, train loss-2.2721, acc-0.1400, valid loss-2.2914, acc-0.1346, test loss-2.2849, acc-0.1393\n",
      "Iter-1220, train loss-2.2689, acc-0.1600, valid loss-2.2913, acc-0.1346, test loss-2.2848, acc-0.1397\n",
      "Iter-1230, train loss-2.2920, acc-0.1800, valid loss-2.2912, acc-0.1346, test loss-2.2847, acc-0.1398\n",
      "Iter-1240, train loss-2.3071, acc-0.0600, valid loss-2.2910, acc-0.1350, test loss-2.2846, acc-0.1399\n",
      "Iter-1250, train loss-2.2767, acc-0.1000, valid loss-2.2909, acc-0.1350, test loss-2.2845, acc-0.1399\n",
      "Iter-1260, train loss-2.2978, acc-0.2400, valid loss-2.2908, acc-0.1352, test loss-2.2844, acc-0.1404\n",
      "Iter-1270, train loss-2.2852, acc-0.1400, valid loss-2.2907, acc-0.1354, test loss-2.2843, acc-0.1406\n",
      "Iter-1280, train loss-2.2953, acc-0.1400, valid loss-2.2906, acc-0.1354, test loss-2.2841, acc-0.1407\n",
      "Iter-1290, train loss-2.2935, acc-0.0800, valid loss-2.2905, acc-0.1356, test loss-2.2840, acc-0.1410\n",
      "Iter-1300, train loss-2.2965, acc-0.1400, valid loss-2.2904, acc-0.1362, test loss-2.2839, acc-0.1413\n",
      "Iter-1310, train loss-2.2567, acc-0.2200, valid loss-2.2903, acc-0.1364, test loss-2.2838, acc-0.1418\n",
      "Iter-1320, train loss-2.2896, acc-0.0800, valid loss-2.2902, acc-0.1364, test loss-2.2837, acc-0.1420\n",
      "Iter-1330, train loss-2.3036, acc-0.1400, valid loss-2.2901, acc-0.1368, test loss-2.2835, acc-0.1419\n",
      "Iter-1340, train loss-2.3063, acc-0.1000, valid loss-2.2899, acc-0.1368, test loss-2.2834, acc-0.1419\n",
      "Iter-1350, train loss-2.2974, acc-0.0800, valid loss-2.2898, acc-0.1368, test loss-2.2833, acc-0.1424\n",
      "Iter-1360, train loss-2.2910, acc-0.1200, valid loss-2.2897, acc-0.1370, test loss-2.2832, acc-0.1427\n",
      "Iter-1370, train loss-2.2969, acc-0.1200, valid loss-2.2896, acc-0.1376, test loss-2.2831, acc-0.1431\n",
      "Iter-1380, train loss-2.2628, acc-0.1800, valid loss-2.2895, acc-0.1380, test loss-2.2829, acc-0.1433\n",
      "Iter-1390, train loss-2.2876, acc-0.1400, valid loss-2.2894, acc-0.1380, test loss-2.2828, acc-0.1435\n",
      "Iter-1400, train loss-2.3010, acc-0.1400, valid loss-2.2892, acc-0.1386, test loss-2.2827, acc-0.1440\n",
      "Iter-1410, train loss-2.3059, acc-0.1400, valid loss-2.2891, acc-0.1384, test loss-2.2826, acc-0.1445\n",
      "Iter-1420, train loss-2.2837, acc-0.1000, valid loss-2.2890, acc-0.1382, test loss-2.2825, acc-0.1444\n",
      "Iter-1430, train loss-2.3104, acc-0.1400, valid loss-2.2889, acc-0.1384, test loss-2.2823, acc-0.1445\n",
      "Iter-1440, train loss-2.2775, acc-0.1400, valid loss-2.2888, acc-0.1384, test loss-2.2822, acc-0.1447\n",
      "Iter-1450, train loss-2.2805, acc-0.1800, valid loss-2.2887, acc-0.1390, test loss-2.2821, acc-0.1451\n",
      "Iter-1460, train loss-2.3140, acc-0.1200, valid loss-2.2885, acc-0.1390, test loss-2.2820, acc-0.1452\n",
      "Iter-1470, train loss-2.2526, acc-0.2200, valid loss-2.2884, acc-0.1398, test loss-2.2819, acc-0.1455\n",
      "Iter-1480, train loss-2.2975, acc-0.1000, valid loss-2.2883, acc-0.1400, test loss-2.2817, acc-0.1458\n",
      "Iter-1490, train loss-2.2722, acc-0.1600, valid loss-2.2882, acc-0.1404, test loss-2.2816, acc-0.1463\n",
      "Iter-1500, train loss-2.2539, acc-0.1400, valid loss-2.2881, acc-0.1406, test loss-2.2815, acc-0.1465\n",
      "Iter-1510, train loss-2.2525, acc-0.2600, valid loss-2.2880, acc-0.1408, test loss-2.2814, acc-0.1473\n",
      "Iter-1520, train loss-2.2904, acc-0.1800, valid loss-2.2878, acc-0.1408, test loss-2.2813, acc-0.1473\n",
      "Iter-1530, train loss-2.2629, acc-0.2400, valid loss-2.2877, acc-0.1412, test loss-2.2811, acc-0.1477\n",
      "Iter-1540, train loss-2.2812, acc-0.1800, valid loss-2.2876, acc-0.1412, test loss-2.2810, acc-0.1480\n",
      "Iter-1550, train loss-2.2541, acc-0.1400, valid loss-2.2875, acc-0.1418, test loss-2.2809, acc-0.1481\n",
      "Iter-1560, train loss-2.2944, acc-0.1000, valid loss-2.2874, acc-0.1416, test loss-2.2808, acc-0.1483\n",
      "Iter-1570, train loss-2.2849, acc-0.1800, valid loss-2.2873, acc-0.1422, test loss-2.2806, acc-0.1485\n",
      "Iter-1580, train loss-2.2998, acc-0.1000, valid loss-2.2871, acc-0.1424, test loss-2.2805, acc-0.1486\n",
      "Iter-1590, train loss-2.3179, acc-0.0600, valid loss-2.2870, acc-0.1430, test loss-2.2804, acc-0.1487\n",
      "Iter-1600, train loss-2.2700, acc-0.1200, valid loss-2.2869, acc-0.1434, test loss-2.2803, acc-0.1488\n",
      "Iter-1610, train loss-2.2748, acc-0.1400, valid loss-2.2868, acc-0.1436, test loss-2.2802, acc-0.1495\n",
      "Iter-1620, train loss-2.3156, acc-0.0800, valid loss-2.2867, acc-0.1438, test loss-2.2801, acc-0.1497\n",
      "Iter-1630, train loss-2.2509, acc-0.2000, valid loss-2.2866, acc-0.1440, test loss-2.2799, acc-0.1502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-2.2805, acc-0.2200, valid loss-2.2865, acc-0.1446, test loss-2.2798, acc-0.1508\n",
      "Iter-1650, train loss-2.2727, acc-0.1200, valid loss-2.2864, acc-0.1448, test loss-2.2797, acc-0.1508\n",
      "Iter-1660, train loss-2.2608, acc-0.1200, valid loss-2.2863, acc-0.1450, test loss-2.2796, acc-0.1512\n",
      "Iter-1670, train loss-2.2898, acc-0.1400, valid loss-2.2862, acc-0.1452, test loss-2.2795, acc-0.1512\n",
      "Iter-1680, train loss-2.2604, acc-0.1600, valid loss-2.2860, acc-0.1456, test loss-2.2794, acc-0.1521\n",
      "Iter-1690, train loss-2.2758, acc-0.2200, valid loss-2.2859, acc-0.1456, test loss-2.2792, acc-0.1529\n",
      "Iter-1700, train loss-2.2775, acc-0.2200, valid loss-2.2858, acc-0.1464, test loss-2.2791, acc-0.1530\n",
      "Iter-1710, train loss-2.2442, acc-0.3000, valid loss-2.2857, acc-0.1466, test loss-2.2790, acc-0.1532\n",
      "Iter-1720, train loss-2.3109, acc-0.0800, valid loss-2.2856, acc-0.1466, test loss-2.2789, acc-0.1535\n",
      "Iter-1730, train loss-2.2763, acc-0.1400, valid loss-2.2855, acc-0.1464, test loss-2.2788, acc-0.1538\n",
      "Iter-1740, train loss-2.2607, acc-0.3200, valid loss-2.2853, acc-0.1472, test loss-2.2786, acc-0.1543\n",
      "Iter-1750, train loss-2.2854, acc-0.1200, valid loss-2.2852, acc-0.1464, test loss-2.2785, acc-0.1543\n",
      "Iter-1760, train loss-2.2892, acc-0.1800, valid loss-2.2851, acc-0.1466, test loss-2.2784, acc-0.1543\n",
      "Iter-1770, train loss-2.2714, acc-0.2200, valid loss-2.2850, acc-0.1470, test loss-2.2783, acc-0.1546\n",
      "Iter-1780, train loss-2.2881, acc-0.1000, valid loss-2.2849, acc-0.1474, test loss-2.2782, acc-0.1548\n",
      "Iter-1790, train loss-2.2857, acc-0.2200, valid loss-2.2848, acc-0.1474, test loss-2.2780, acc-0.1551\n",
      "Iter-1800, train loss-2.2556, acc-0.2400, valid loss-2.2847, acc-0.1478, test loss-2.2779, acc-0.1558\n",
      "Iter-1810, train loss-2.2902, acc-0.1000, valid loss-2.2846, acc-0.1478, test loss-2.2778, acc-0.1560\n",
      "Iter-1820, train loss-2.2826, acc-0.1600, valid loss-2.2844, acc-0.1480, test loss-2.2777, acc-0.1563\n",
      "Iter-1830, train loss-2.2543, acc-0.2000, valid loss-2.2843, acc-0.1484, test loss-2.2776, acc-0.1567\n",
      "Iter-1840, train loss-2.3014, acc-0.1400, valid loss-2.2842, acc-0.1490, test loss-2.2774, acc-0.1570\n",
      "Iter-1850, train loss-2.2814, acc-0.2000, valid loss-2.2841, acc-0.1502, test loss-2.2773, acc-0.1570\n",
      "Iter-1860, train loss-2.2620, acc-0.2000, valid loss-2.2840, acc-0.1502, test loss-2.2772, acc-0.1571\n",
      "Iter-1870, train loss-2.2961, acc-0.1400, valid loss-2.2839, acc-0.1504, test loss-2.2771, acc-0.1571\n",
      "Iter-1880, train loss-2.2328, acc-0.2000, valid loss-2.2837, acc-0.1504, test loss-2.2770, acc-0.1572\n",
      "Iter-1890, train loss-2.2720, acc-0.1600, valid loss-2.2836, acc-0.1508, test loss-2.2768, acc-0.1573\n",
      "Iter-1900, train loss-2.2859, acc-0.1200, valid loss-2.2835, acc-0.1512, test loss-2.2767, acc-0.1578\n",
      "Iter-1910, train loss-2.2777, acc-0.2000, valid loss-2.2834, acc-0.1522, test loss-2.2766, acc-0.1579\n",
      "Iter-1920, train loss-2.2995, acc-0.0800, valid loss-2.2833, acc-0.1528, test loss-2.2765, acc-0.1583\n",
      "Iter-1930, train loss-2.3039, acc-0.1200, valid loss-2.2832, acc-0.1532, test loss-2.2764, acc-0.1586\n",
      "Iter-1940, train loss-2.3071, acc-0.0600, valid loss-2.2830, acc-0.1534, test loss-2.2762, acc-0.1588\n",
      "Iter-1950, train loss-2.2971, acc-0.1200, valid loss-2.2829, acc-0.1538, test loss-2.2761, acc-0.1593\n",
      "Iter-1960, train loss-2.3028, acc-0.1800, valid loss-2.2828, acc-0.1542, test loss-2.2760, acc-0.1593\n",
      "Iter-1970, train loss-2.2654, acc-0.0800, valid loss-2.2827, acc-0.1540, test loss-2.2759, acc-0.1594\n",
      "Iter-1980, train loss-2.2435, acc-0.3200, valid loss-2.2826, acc-0.1546, test loss-2.2758, acc-0.1602\n",
      "Iter-1990, train loss-2.3187, acc-0.1000, valid loss-2.2824, acc-0.1554, test loss-2.2756, acc-0.1604\n",
      "Iter-2000, train loss-2.2774, acc-0.1000, valid loss-2.2823, acc-0.1560, test loss-2.2755, acc-0.1612\n",
      "Iter-2010, train loss-2.2689, acc-0.1800, valid loss-2.2822, acc-0.1564, test loss-2.2754, acc-0.1617\n",
      "Iter-2020, train loss-2.2851, acc-0.2000, valid loss-2.2821, acc-0.1566, test loss-2.2753, acc-0.1618\n",
      "Iter-2030, train loss-2.3060, acc-0.1000, valid loss-2.2820, acc-0.1568, test loss-2.2752, acc-0.1618\n",
      "Iter-2040, train loss-2.3024, acc-0.1000, valid loss-2.2819, acc-0.1570, test loss-2.2751, acc-0.1619\n",
      "Iter-2050, train loss-2.2746, acc-0.1200, valid loss-2.2818, acc-0.1570, test loss-2.2749, acc-0.1623\n",
      "Iter-2060, train loss-2.2825, acc-0.1200, valid loss-2.2817, acc-0.1570, test loss-2.2748, acc-0.1627\n",
      "Iter-2070, train loss-2.2921, acc-0.1400, valid loss-2.2816, acc-0.1574, test loss-2.2747, acc-0.1630\n",
      "Iter-2080, train loss-2.2456, acc-0.2000, valid loss-2.2814, acc-0.1574, test loss-2.2746, acc-0.1630\n",
      "Iter-2090, train loss-2.2808, acc-0.1600, valid loss-2.2813, acc-0.1576, test loss-2.2745, acc-0.1630\n",
      "Iter-2100, train loss-2.2782, acc-0.2800, valid loss-2.2812, acc-0.1578, test loss-2.2744, acc-0.1633\n",
      "Iter-2110, train loss-2.2577, acc-0.1600, valid loss-2.2811, acc-0.1590, test loss-2.2743, acc-0.1641\n",
      "Iter-2120, train loss-2.2384, acc-0.3000, valid loss-2.2810, acc-0.1592, test loss-2.2741, acc-0.1644\n",
      "Iter-2130, train loss-2.2749, acc-0.1400, valid loss-2.2809, acc-0.1598, test loss-2.2740, acc-0.1649\n",
      "Iter-2140, train loss-2.2820, acc-0.1600, valid loss-2.2808, acc-0.1602, test loss-2.2739, acc-0.1654\n",
      "Iter-2150, train loss-2.2740, acc-0.1400, valid loss-2.2806, acc-0.1604, test loss-2.2738, acc-0.1658\n",
      "Iter-2160, train loss-2.2884, acc-0.1600, valid loss-2.2805, acc-0.1604, test loss-2.2737, acc-0.1661\n",
      "Iter-2170, train loss-2.2764, acc-0.1400, valid loss-2.2804, acc-0.1608, test loss-2.2735, acc-0.1662\n",
      "Iter-2180, train loss-2.3043, acc-0.1000, valid loss-2.2803, acc-0.1608, test loss-2.2734, acc-0.1664\n",
      "Iter-2190, train loss-2.3217, acc-0.0800, valid loss-2.2802, acc-0.1608, test loss-2.2733, acc-0.1666\n",
      "Iter-2200, train loss-2.2742, acc-0.2400, valid loss-2.2801, acc-0.1616, test loss-2.2732, acc-0.1668\n",
      "Iter-2210, train loss-2.3057, acc-0.1400, valid loss-2.2800, acc-0.1624, test loss-2.2731, acc-0.1671\n",
      "Iter-2220, train loss-2.2962, acc-0.1200, valid loss-2.2798, acc-0.1624, test loss-2.2730, acc-0.1673\n",
      "Iter-2230, train loss-2.2767, acc-0.1400, valid loss-2.2797, acc-0.1628, test loss-2.2728, acc-0.1676\n",
      "Iter-2240, train loss-2.2938, acc-0.1400, valid loss-2.2796, acc-0.1630, test loss-2.2727, acc-0.1678\n",
      "Iter-2250, train loss-2.3318, acc-0.1000, valid loss-2.2795, acc-0.1636, test loss-2.2726, acc-0.1682\n",
      "Iter-2260, train loss-2.2900, acc-0.1200, valid loss-2.2794, acc-0.1642, test loss-2.2725, acc-0.1686\n",
      "Iter-2270, train loss-2.3063, acc-0.1200, valid loss-2.2793, acc-0.1646, test loss-2.2724, acc-0.1689\n",
      "Iter-2280, train loss-2.2620, acc-0.1800, valid loss-2.2792, acc-0.1664, test loss-2.2723, acc-0.1699\n",
      "Iter-2290, train loss-2.2993, acc-0.1000, valid loss-2.2791, acc-0.1670, test loss-2.2721, acc-0.1701\n",
      "Iter-2300, train loss-2.2547, acc-0.3200, valid loss-2.2789, acc-0.1676, test loss-2.2720, acc-0.1702\n",
      "Iter-2310, train loss-2.2548, acc-0.2000, valid loss-2.2788, acc-0.1684, test loss-2.2719, acc-0.1704\n",
      "Iter-2320, train loss-2.2840, acc-0.1400, valid loss-2.2787, acc-0.1688, test loss-2.2718, acc-0.1706\n",
      "Iter-2330, train loss-2.2673, acc-0.1600, valid loss-2.2786, acc-0.1686, test loss-2.2717, acc-0.1708\n",
      "Iter-2340, train loss-2.2913, acc-0.1000, valid loss-2.2785, acc-0.1690, test loss-2.2715, acc-0.1709\n",
      "Iter-2350, train loss-2.2840, acc-0.1600, valid loss-2.2784, acc-0.1692, test loss-2.2714, acc-0.1711\n",
      "Iter-2360, train loss-2.2805, acc-0.1800, valid loss-2.2783, acc-0.1696, test loss-2.2713, acc-0.1715\n",
      "Iter-2370, train loss-2.2810, acc-0.3400, valid loss-2.2782, acc-0.1698, test loss-2.2712, acc-0.1715\n",
      "Iter-2380, train loss-2.2554, acc-0.2800, valid loss-2.2780, acc-0.1704, test loss-2.2711, acc-0.1726\n",
      "Iter-2390, train loss-2.2726, acc-0.2000, valid loss-2.2779, acc-0.1712, test loss-2.2710, acc-0.1732\n",
      "Iter-2400, train loss-2.2563, acc-0.2000, valid loss-2.2778, acc-0.1716, test loss-2.2708, acc-0.1733\n",
      "Iter-2410, train loss-2.2846, acc-0.0800, valid loss-2.2777, acc-0.1718, test loss-2.2707, acc-0.1739\n",
      "Iter-2420, train loss-2.2429, acc-0.2000, valid loss-2.2776, acc-0.1728, test loss-2.2706, acc-0.1744\n",
      "Iter-2430, train loss-2.2752, acc-0.2400, valid loss-2.2775, acc-0.1726, test loss-2.2705, acc-0.1748\n",
      "Iter-2440, train loss-2.2692, acc-0.1400, valid loss-2.2773, acc-0.1736, test loss-2.2704, acc-0.1752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-2.2588, acc-0.1800, valid loss-2.2772, acc-0.1740, test loss-2.2702, acc-0.1762\n",
      "Iter-2460, train loss-2.2818, acc-0.1800, valid loss-2.2771, acc-0.1742, test loss-2.2701, acc-0.1764\n",
      "Iter-2470, train loss-2.2901, acc-0.1600, valid loss-2.2770, acc-0.1752, test loss-2.2700, acc-0.1770\n",
      "Iter-2480, train loss-2.2794, acc-0.1600, valid loss-2.2769, acc-0.1752, test loss-2.2699, acc-0.1773\n",
      "Iter-2490, train loss-2.2550, acc-0.2000, valid loss-2.2768, acc-0.1762, test loss-2.2698, acc-0.1778\n",
      "Iter-2500, train loss-2.2595, acc-0.2400, valid loss-2.2767, acc-0.1758, test loss-2.2696, acc-0.1785\n",
      "Iter-2510, train loss-2.2767, acc-0.2000, valid loss-2.2765, acc-0.1764, test loss-2.2695, acc-0.1792\n",
      "Iter-2520, train loss-2.2889, acc-0.2200, valid loss-2.2764, acc-0.1772, test loss-2.2694, acc-0.1802\n",
      "Iter-2530, train loss-2.2636, acc-0.1200, valid loss-2.2763, acc-0.1770, test loss-2.2693, acc-0.1806\n",
      "Iter-2540, train loss-2.2359, acc-0.3800, valid loss-2.2762, acc-0.1776, test loss-2.2692, acc-0.1811\n",
      "Iter-2550, train loss-2.2738, acc-0.1600, valid loss-2.2761, acc-0.1784, test loss-2.2690, acc-0.1812\n",
      "Iter-2560, train loss-2.3004, acc-0.1400, valid loss-2.2760, acc-0.1782, test loss-2.2689, acc-0.1815\n",
      "Iter-2570, train loss-2.2522, acc-0.1400, valid loss-2.2759, acc-0.1790, test loss-2.2688, acc-0.1817\n",
      "Iter-2580, train loss-2.2719, acc-0.2000, valid loss-2.2757, acc-0.1796, test loss-2.2687, acc-0.1826\n",
      "Iter-2590, train loss-2.2640, acc-0.2000, valid loss-2.2756, acc-0.1804, test loss-2.2686, acc-0.1829\n",
      "Iter-2600, train loss-2.2381, acc-0.3200, valid loss-2.2755, acc-0.1810, test loss-2.2685, acc-0.1827\n",
      "Iter-2610, train loss-2.2618, acc-0.1400, valid loss-2.2754, acc-0.1816, test loss-2.2683, acc-0.1829\n",
      "Iter-2620, train loss-2.2966, acc-0.1200, valid loss-2.2753, acc-0.1820, test loss-2.2682, acc-0.1835\n",
      "Iter-2630, train loss-2.2431, acc-0.1800, valid loss-2.2752, acc-0.1824, test loss-2.2681, acc-0.1840\n",
      "Iter-2640, train loss-2.2303, acc-0.2800, valid loss-2.2751, acc-0.1834, test loss-2.2680, acc-0.1850\n",
      "Iter-2650, train loss-2.2592, acc-0.2800, valid loss-2.2750, acc-0.1844, test loss-2.2679, acc-0.1858\n",
      "Iter-2660, train loss-2.2583, acc-0.2200, valid loss-2.2749, acc-0.1850, test loss-2.2678, acc-0.1862\n",
      "Iter-2670, train loss-2.2899, acc-0.1000, valid loss-2.2747, acc-0.1854, test loss-2.2676, acc-0.1869\n",
      "Iter-2680, train loss-2.2663, acc-0.2200, valid loss-2.2746, acc-0.1862, test loss-2.2675, acc-0.1873\n",
      "Iter-2690, train loss-2.2525, acc-0.1400, valid loss-2.2745, acc-0.1864, test loss-2.2674, acc-0.1874\n",
      "Iter-2700, train loss-2.2849, acc-0.1800, valid loss-2.2744, acc-0.1866, test loss-2.2673, acc-0.1879\n",
      "Iter-2710, train loss-2.2620, acc-0.1800, valid loss-2.2743, acc-0.1870, test loss-2.2672, acc-0.1883\n",
      "Iter-2720, train loss-2.2482, acc-0.2800, valid loss-2.2742, acc-0.1874, test loss-2.2671, acc-0.1890\n",
      "Iter-2730, train loss-2.2716, acc-0.1600, valid loss-2.2741, acc-0.1878, test loss-2.2669, acc-0.1891\n",
      "Iter-2740, train loss-2.3271, acc-0.1600, valid loss-2.2740, acc-0.1880, test loss-2.2668, acc-0.1894\n",
      "Iter-2750, train loss-2.2626, acc-0.1800, valid loss-2.2739, acc-0.1880, test loss-2.2667, acc-0.1902\n",
      "Iter-2760, train loss-2.2890, acc-0.1400, valid loss-2.2737, acc-0.1884, test loss-2.2666, acc-0.1905\n",
      "Iter-2770, train loss-2.2784, acc-0.1400, valid loss-2.2736, acc-0.1896, test loss-2.2665, acc-0.1915\n",
      "Iter-2780, train loss-2.2613, acc-0.2000, valid loss-2.2735, acc-0.1896, test loss-2.2664, acc-0.1919\n",
      "Iter-2790, train loss-2.3091, acc-0.0800, valid loss-2.2734, acc-0.1894, test loss-2.2663, acc-0.1923\n",
      "Iter-2800, train loss-2.2619, acc-0.2800, valid loss-2.2733, acc-0.1896, test loss-2.2662, acc-0.1928\n",
      "Iter-2810, train loss-2.2966, acc-0.1600, valid loss-2.2732, acc-0.1902, test loss-2.2660, acc-0.1932\n",
      "Iter-2820, train loss-2.2901, acc-0.1800, valid loss-2.2731, acc-0.1902, test loss-2.2659, acc-0.1938\n",
      "Iter-2830, train loss-2.2690, acc-0.1800, valid loss-2.2729, acc-0.1906, test loss-2.2658, acc-0.1944\n",
      "Iter-2840, train loss-2.2287, acc-0.2600, valid loss-2.2728, acc-0.1912, test loss-2.2657, acc-0.1947\n",
      "Iter-2850, train loss-2.2497, acc-0.1800, valid loss-2.2727, acc-0.1914, test loss-2.2656, acc-0.1949\n",
      "Iter-2860, train loss-2.2687, acc-0.2600, valid loss-2.2726, acc-0.1910, test loss-2.2654, acc-0.1954\n",
      "Iter-2870, train loss-2.2919, acc-0.1800, valid loss-2.2725, acc-0.1918, test loss-2.2653, acc-0.1959\n",
      "Iter-2880, train loss-2.2796, acc-0.1800, valid loss-2.2724, acc-0.1928, test loss-2.2652, acc-0.1967\n",
      "Iter-2890, train loss-2.2932, acc-0.1200, valid loss-2.2723, acc-0.1934, test loss-2.2651, acc-0.1978\n",
      "Iter-2900, train loss-2.2408, acc-0.2200, valid loss-2.2722, acc-0.1934, test loss-2.2650, acc-0.1981\n",
      "Iter-2910, train loss-2.2630, acc-0.2000, valid loss-2.2720, acc-0.1932, test loss-2.2649, acc-0.1985\n",
      "Iter-2920, train loss-2.2582, acc-0.1600, valid loss-2.2719, acc-0.1942, test loss-2.2647, acc-0.1990\n",
      "Iter-2930, train loss-2.2546, acc-0.2200, valid loss-2.2718, acc-0.1944, test loss-2.2646, acc-0.2004\n",
      "Iter-2940, train loss-2.2686, acc-0.1600, valid loss-2.2717, acc-0.1950, test loss-2.2645, acc-0.2006\n",
      "Iter-2950, train loss-2.2339, acc-0.3200, valid loss-2.2716, acc-0.1954, test loss-2.2644, acc-0.2007\n",
      "Iter-2960, train loss-2.2880, acc-0.1000, valid loss-2.2715, acc-0.1954, test loss-2.2643, acc-0.2011\n",
      "Iter-2970, train loss-2.2769, acc-0.2200, valid loss-2.2714, acc-0.1952, test loss-2.2642, acc-0.2018\n",
      "Iter-2980, train loss-2.2234, acc-0.2000, valid loss-2.2713, acc-0.1952, test loss-2.2641, acc-0.2020\n",
      "Iter-2990, train loss-2.2420, acc-0.2400, valid loss-2.2712, acc-0.1946, test loss-2.2639, acc-0.2027\n",
      "Iter-3000, train loss-2.2334, acc-0.3800, valid loss-2.2711, acc-0.1950, test loss-2.2638, acc-0.2029\n",
      "Iter-3010, train loss-2.2888, acc-0.1200, valid loss-2.2709, acc-0.1954, test loss-2.2637, acc-0.2034\n",
      "Iter-3020, train loss-2.2754, acc-0.2200, valid loss-2.2708, acc-0.1956, test loss-2.2636, acc-0.2035\n",
      "Iter-3030, train loss-2.2624, acc-0.1400, valid loss-2.2707, acc-0.1954, test loss-2.2635, acc-0.2041\n",
      "Iter-3040, train loss-2.2885, acc-0.1400, valid loss-2.2706, acc-0.1958, test loss-2.2634, acc-0.2041\n",
      "Iter-3050, train loss-2.2592, acc-0.2000, valid loss-2.2705, acc-0.1960, test loss-2.2632, acc-0.2045\n",
      "Iter-3060, train loss-2.2586, acc-0.2600, valid loss-2.2704, acc-0.1960, test loss-2.2631, acc-0.2051\n",
      "Iter-3070, train loss-2.2444, acc-0.2400, valid loss-2.2703, acc-0.1966, test loss-2.2630, acc-0.2056\n",
      "Iter-3080, train loss-2.2648, acc-0.2400, valid loss-2.2701, acc-0.1970, test loss-2.2629, acc-0.2063\n",
      "Iter-3090, train loss-2.2990, acc-0.1600, valid loss-2.2700, acc-0.1972, test loss-2.2628, acc-0.2066\n",
      "Iter-3100, train loss-2.3075, acc-0.0400, valid loss-2.2699, acc-0.1972, test loss-2.2627, acc-0.2067\n",
      "Iter-3110, train loss-2.2350, acc-0.2800, valid loss-2.2698, acc-0.1974, test loss-2.2626, acc-0.2069\n",
      "Iter-3120, train loss-2.2906, acc-0.0600, valid loss-2.2697, acc-0.1978, test loss-2.2624, acc-0.2070\n",
      "Iter-3130, train loss-2.2747, acc-0.1600, valid loss-2.2696, acc-0.1992, test loss-2.2623, acc-0.2076\n",
      "Iter-3140, train loss-2.2843, acc-0.2000, valid loss-2.2695, acc-0.1994, test loss-2.2622, acc-0.2084\n",
      "Iter-3150, train loss-2.2415, acc-0.1600, valid loss-2.2694, acc-0.1998, test loss-2.2621, acc-0.2085\n",
      "Iter-3160, train loss-2.2894, acc-0.0800, valid loss-2.2693, acc-0.2000, test loss-2.2620, acc-0.2088\n",
      "Iter-3170, train loss-2.2517, acc-0.2600, valid loss-2.2691, acc-0.2012, test loss-2.2618, acc-0.2093\n",
      "Iter-3180, train loss-2.2680, acc-0.2000, valid loss-2.2690, acc-0.2012, test loss-2.2617, acc-0.2095\n",
      "Iter-3190, train loss-2.2559, acc-0.1400, valid loss-2.2689, acc-0.2028, test loss-2.2616, acc-0.2097\n",
      "Iter-3200, train loss-2.2557, acc-0.2200, valid loss-2.2688, acc-0.2026, test loss-2.2615, acc-0.2101\n",
      "Iter-3210, train loss-2.2509, acc-0.2200, valid loss-2.2687, acc-0.2026, test loss-2.2614, acc-0.2110\n",
      "Iter-3220, train loss-2.2190, acc-0.3400, valid loss-2.2686, acc-0.2030, test loss-2.2613, acc-0.2115\n",
      "Iter-3230, train loss-2.2705, acc-0.1800, valid loss-2.2685, acc-0.2030, test loss-2.2611, acc-0.2122\n",
      "Iter-3240, train loss-2.2344, acc-0.2000, valid loss-2.2683, acc-0.2032, test loss-2.2610, acc-0.2132\n",
      "Iter-3250, train loss-2.2587, acc-0.2400, valid loss-2.2682, acc-0.2036, test loss-2.2609, acc-0.2135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-2.2499, acc-0.2200, valid loss-2.2681, acc-0.2036, test loss-2.2608, acc-0.2138\n",
      "Iter-3270, train loss-2.2662, acc-0.2200, valid loss-2.2680, acc-0.2042, test loss-2.2607, acc-0.2145\n",
      "Iter-3280, train loss-2.2559, acc-0.1800, valid loss-2.2679, acc-0.2044, test loss-2.2606, acc-0.2152\n",
      "Iter-3290, train loss-2.2601, acc-0.2400, valid loss-2.2678, acc-0.2044, test loss-2.2604, acc-0.2152\n",
      "Iter-3300, train loss-2.2827, acc-0.1600, valid loss-2.2677, acc-0.2052, test loss-2.2603, acc-0.2154\n",
      "Iter-3310, train loss-2.3037, acc-0.1600, valid loss-2.2676, acc-0.2054, test loss-2.2602, acc-0.2157\n",
      "Iter-3320, train loss-2.2246, acc-0.2400, valid loss-2.2674, acc-0.2054, test loss-2.2601, acc-0.2160\n",
      "Iter-3330, train loss-2.2563, acc-0.2400, valid loss-2.2673, acc-0.2058, test loss-2.2600, acc-0.2160\n",
      "Iter-3340, train loss-2.2612, acc-0.2400, valid loss-2.2672, acc-0.2066, test loss-2.2598, acc-0.2165\n",
      "Iter-3350, train loss-2.2510, acc-0.3000, valid loss-2.2671, acc-0.2064, test loss-2.2597, acc-0.2171\n",
      "Iter-3360, train loss-2.2482, acc-0.3000, valid loss-2.2670, acc-0.2066, test loss-2.2596, acc-0.2177\n",
      "Iter-3370, train loss-2.2485, acc-0.2800, valid loss-2.2669, acc-0.2070, test loss-2.2595, acc-0.2182\n",
      "Iter-3380, train loss-2.2778, acc-0.1800, valid loss-2.2668, acc-0.2070, test loss-2.2594, acc-0.2181\n",
      "Iter-3390, train loss-2.2481, acc-0.2600, valid loss-2.2667, acc-0.2076, test loss-2.2593, acc-0.2183\n",
      "Iter-3400, train loss-2.2700, acc-0.2200, valid loss-2.2666, acc-0.2080, test loss-2.2591, acc-0.2188\n",
      "Iter-3410, train loss-2.2582, acc-0.2400, valid loss-2.2664, acc-0.2086, test loss-2.2590, acc-0.2188\n",
      "Iter-3420, train loss-2.2416, acc-0.2800, valid loss-2.2663, acc-0.2090, test loss-2.2589, acc-0.2193\n",
      "Iter-3430, train loss-2.2492, acc-0.2000, valid loss-2.2662, acc-0.2100, test loss-2.2588, acc-0.2196\n",
      "Iter-3440, train loss-2.2870, acc-0.2000, valid loss-2.2661, acc-0.2110, test loss-2.2587, acc-0.2196\n",
      "Iter-3450, train loss-2.2747, acc-0.1200, valid loss-2.2660, acc-0.2120, test loss-2.2585, acc-0.2204\n",
      "Iter-3460, train loss-2.2782, acc-0.2400, valid loss-2.2659, acc-0.2122, test loss-2.2584, acc-0.2205\n",
      "Iter-3470, train loss-2.2680, acc-0.2000, valid loss-2.2658, acc-0.2128, test loss-2.2583, acc-0.2208\n",
      "Iter-3480, train loss-2.2206, acc-0.2800, valid loss-2.2657, acc-0.2128, test loss-2.2582, acc-0.2211\n",
      "Iter-3490, train loss-2.2317, acc-0.3000, valid loss-2.2655, acc-0.2128, test loss-2.2581, acc-0.2215\n",
      "Iter-3500, train loss-2.2585, acc-0.1800, valid loss-2.2654, acc-0.2130, test loss-2.2580, acc-0.2220\n",
      "Iter-3510, train loss-2.2663, acc-0.2600, valid loss-2.2653, acc-0.2130, test loss-2.2579, acc-0.2219\n",
      "Iter-3520, train loss-2.2457, acc-0.2600, valid loss-2.2652, acc-0.2134, test loss-2.2577, acc-0.2225\n",
      "Iter-3530, train loss-2.2534, acc-0.2400, valid loss-2.2651, acc-0.2138, test loss-2.2576, acc-0.2227\n",
      "Iter-3540, train loss-2.2559, acc-0.2200, valid loss-2.2650, acc-0.2138, test loss-2.2575, acc-0.2231\n",
      "Iter-3550, train loss-2.2609, acc-0.1600, valid loss-2.2649, acc-0.2140, test loss-2.2574, acc-0.2230\n",
      "Iter-3560, train loss-2.2835, acc-0.1600, valid loss-2.2648, acc-0.2140, test loss-2.2573, acc-0.2234\n",
      "Iter-3570, train loss-2.2796, acc-0.1600, valid loss-2.2647, acc-0.2148, test loss-2.2572, acc-0.2240\n",
      "Iter-3580, train loss-2.2470, acc-0.3400, valid loss-2.2646, acc-0.2144, test loss-2.2571, acc-0.2243\n",
      "Iter-3590, train loss-2.2752, acc-0.1800, valid loss-2.2645, acc-0.2146, test loss-2.2569, acc-0.2243\n",
      "Iter-3600, train loss-2.2188, acc-0.3000, valid loss-2.2644, acc-0.2156, test loss-2.2568, acc-0.2249\n",
      "Iter-3610, train loss-2.2426, acc-0.2800, valid loss-2.2642, acc-0.2156, test loss-2.2567, acc-0.2256\n",
      "Iter-3620, train loss-2.2554, acc-0.2400, valid loss-2.2641, acc-0.2154, test loss-2.2566, acc-0.2258\n",
      "Iter-3630, train loss-2.2590, acc-0.2800, valid loss-2.2640, acc-0.2156, test loss-2.2565, acc-0.2264\n",
      "Iter-3640, train loss-2.2553, acc-0.2000, valid loss-2.2639, acc-0.2158, test loss-2.2564, acc-0.2265\n",
      "Iter-3650, train loss-2.2418, acc-0.3400, valid loss-2.2638, acc-0.2160, test loss-2.2563, acc-0.2269\n",
      "Iter-3660, train loss-2.2856, acc-0.2000, valid loss-2.2637, acc-0.2162, test loss-2.2561, acc-0.2272\n",
      "Iter-3670, train loss-2.2681, acc-0.1800, valid loss-2.2636, acc-0.2166, test loss-2.2560, acc-0.2271\n",
      "Iter-3680, train loss-2.2661, acc-0.2200, valid loss-2.2635, acc-0.2172, test loss-2.2559, acc-0.2277\n",
      "Iter-3690, train loss-2.2567, acc-0.2800, valid loss-2.2633, acc-0.2174, test loss-2.2558, acc-0.2280\n",
      "Iter-3700, train loss-2.2511, acc-0.2000, valid loss-2.2632, acc-0.2180, test loss-2.2557, acc-0.2284\n",
      "Iter-3710, train loss-2.2663, acc-0.2600, valid loss-2.2631, acc-0.2186, test loss-2.2556, acc-0.2289\n",
      "Iter-3720, train loss-2.2450, acc-0.2200, valid loss-2.2630, acc-0.2190, test loss-2.2555, acc-0.2290\n",
      "Iter-3730, train loss-2.2603, acc-0.2400, valid loss-2.2629, acc-0.2198, test loss-2.2554, acc-0.2293\n",
      "Iter-3740, train loss-2.2597, acc-0.2000, valid loss-2.2628, acc-0.2200, test loss-2.2552, acc-0.2295\n",
      "Iter-3750, train loss-2.2538, acc-0.1800, valid loss-2.2627, acc-0.2200, test loss-2.2551, acc-0.2300\n",
      "Iter-3760, train loss-2.2421, acc-0.2800, valid loss-2.2626, acc-0.2204, test loss-2.2550, acc-0.2299\n",
      "Iter-3770, train loss-2.2148, acc-0.4000, valid loss-2.2625, acc-0.2212, test loss-2.2549, acc-0.2300\n",
      "Iter-3780, train loss-2.2520, acc-0.2600, valid loss-2.2624, acc-0.2214, test loss-2.2548, acc-0.2300\n",
      "Iter-3790, train loss-2.2448, acc-0.2600, valid loss-2.2623, acc-0.2218, test loss-2.2547, acc-0.2307\n",
      "Iter-3800, train loss-2.2801, acc-0.2000, valid loss-2.2622, acc-0.2220, test loss-2.2546, acc-0.2309\n",
      "Iter-3810, train loss-2.2839, acc-0.1200, valid loss-2.2621, acc-0.2222, test loss-2.2545, acc-0.2312\n",
      "Iter-3820, train loss-2.2486, acc-0.2400, valid loss-2.2619, acc-0.2232, test loss-2.2544, acc-0.2311\n",
      "Iter-3830, train loss-2.2706, acc-0.1000, valid loss-2.2618, acc-0.2240, test loss-2.2542, acc-0.2312\n",
      "Iter-3840, train loss-2.2568, acc-0.2000, valid loss-2.2617, acc-0.2240, test loss-2.2541, acc-0.2313\n",
      "Iter-3850, train loss-2.2533, acc-0.2800, valid loss-2.2616, acc-0.2242, test loss-2.2540, acc-0.2312\n",
      "Iter-3860, train loss-2.2777, acc-0.2400, valid loss-2.2615, acc-0.2246, test loss-2.2539, acc-0.2315\n",
      "Iter-3870, train loss-2.2535, acc-0.2600, valid loss-2.2614, acc-0.2250, test loss-2.2538, acc-0.2316\n",
      "Iter-3880, train loss-2.2541, acc-0.1800, valid loss-2.2613, acc-0.2250, test loss-2.2537, acc-0.2319\n",
      "Iter-3890, train loss-2.2595, acc-0.2400, valid loss-2.2612, acc-0.2264, test loss-2.2536, acc-0.2320\n",
      "Iter-3900, train loss-2.2459, acc-0.2800, valid loss-2.2611, acc-0.2270, test loss-2.2534, acc-0.2323\n",
      "Iter-3910, train loss-2.2742, acc-0.2000, valid loss-2.2609, acc-0.2272, test loss-2.2533, acc-0.2327\n",
      "Iter-3920, train loss-2.2296, acc-0.2600, valid loss-2.2608, acc-0.2274, test loss-2.2532, acc-0.2326\n",
      "Iter-3930, train loss-2.2540, acc-0.2800, valid loss-2.2607, acc-0.2280, test loss-2.2531, acc-0.2328\n",
      "Iter-3940, train loss-2.2773, acc-0.2000, valid loss-2.2606, acc-0.2284, test loss-2.2530, acc-0.2332\n",
      "Iter-3950, train loss-2.2624, acc-0.2200, valid loss-2.2605, acc-0.2286, test loss-2.2529, acc-0.2331\n",
      "Iter-3960, train loss-2.2603, acc-0.2200, valid loss-2.2604, acc-0.2290, test loss-2.2528, acc-0.2338\n",
      "Iter-3970, train loss-2.2667, acc-0.2000, valid loss-2.2603, acc-0.2298, test loss-2.2527, acc-0.2339\n",
      "Iter-3980, train loss-2.2499, acc-0.2000, valid loss-2.2602, acc-0.2298, test loss-2.2526, acc-0.2342\n",
      "Iter-3990, train loss-2.2598, acc-0.1600, valid loss-2.2601, acc-0.2298, test loss-2.2525, acc-0.2346\n",
      "Iter-4000, train loss-2.2586, acc-0.2000, valid loss-2.2600, acc-0.2304, test loss-2.2524, acc-0.2348\n",
      "Iter-4010, train loss-2.2522, acc-0.2000, valid loss-2.2599, acc-0.2310, test loss-2.2523, acc-0.2350\n",
      "Iter-4020, train loss-2.2700, acc-0.1600, valid loss-2.2598, acc-0.2310, test loss-2.2522, acc-0.2353\n",
      "Iter-4030, train loss-2.2290, acc-0.3000, valid loss-2.2597, acc-0.2316, test loss-2.2520, acc-0.2354\n",
      "Iter-4040, train loss-2.2582, acc-0.2200, valid loss-2.2596, acc-0.2318, test loss-2.2519, acc-0.2355\n",
      "Iter-4050, train loss-2.2659, acc-0.1600, valid loss-2.2595, acc-0.2322, test loss-2.2518, acc-0.2360\n",
      "Iter-4060, train loss-2.2527, acc-0.2600, valid loss-2.2594, acc-0.2324, test loss-2.2517, acc-0.2363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-2.2522, acc-0.2200, valid loss-2.2592, acc-0.2324, test loss-2.2516, acc-0.2367\n",
      "Iter-4080, train loss-2.2093, acc-0.3600, valid loss-2.2591, acc-0.2322, test loss-2.2515, acc-0.2372\n",
      "Iter-4090, train loss-2.2635, acc-0.2200, valid loss-2.2590, acc-0.2324, test loss-2.2514, acc-0.2370\n",
      "Iter-4100, train loss-2.2456, acc-0.2400, valid loss-2.2589, acc-0.2322, test loss-2.2512, acc-0.2376\n",
      "Iter-4110, train loss-2.2796, acc-0.1800, valid loss-2.2588, acc-0.2322, test loss-2.2511, acc-0.2376\n",
      "Iter-4120, train loss-2.2422, acc-0.2600, valid loss-2.2587, acc-0.2330, test loss-2.2510, acc-0.2379\n",
      "Iter-4130, train loss-2.2622, acc-0.1400, valid loss-2.2586, acc-0.2328, test loss-2.2509, acc-0.2380\n",
      "Iter-4140, train loss-2.3024, acc-0.1200, valid loss-2.2585, acc-0.2330, test loss-2.2508, acc-0.2381\n",
      "Iter-4150, train loss-2.2469, acc-0.2400, valid loss-2.2584, acc-0.2332, test loss-2.2507, acc-0.2383\n",
      "Iter-4160, train loss-2.2684, acc-0.1400, valid loss-2.2583, acc-0.2338, test loss-2.2506, acc-0.2384\n",
      "Iter-4170, train loss-2.2532, acc-0.2400, valid loss-2.2582, acc-0.2340, test loss-2.2505, acc-0.2384\n",
      "Iter-4180, train loss-2.2283, acc-0.2400, valid loss-2.2581, acc-0.2340, test loss-2.2503, acc-0.2388\n",
      "Iter-4190, train loss-2.2590, acc-0.2600, valid loss-2.2579, acc-0.2342, test loss-2.2502, acc-0.2388\n",
      "Iter-4200, train loss-2.2250, acc-0.3000, valid loss-2.2578, acc-0.2348, test loss-2.2501, acc-0.2390\n",
      "Iter-4210, train loss-2.2631, acc-0.1600, valid loss-2.2577, acc-0.2348, test loss-2.2500, acc-0.2394\n",
      "Iter-4220, train loss-2.2553, acc-0.2400, valid loss-2.2576, acc-0.2350, test loss-2.2499, acc-0.2396\n",
      "Iter-4230, train loss-2.2436, acc-0.2800, valid loss-2.2575, acc-0.2352, test loss-2.2498, acc-0.2398\n",
      "Iter-4240, train loss-2.3102, acc-0.0800, valid loss-2.2574, acc-0.2354, test loss-2.2497, acc-0.2402\n",
      "Iter-4250, train loss-2.2320, acc-0.2600, valid loss-2.2573, acc-0.2356, test loss-2.2495, acc-0.2401\n",
      "Iter-4260, train loss-2.2786, acc-0.1800, valid loss-2.2572, acc-0.2360, test loss-2.2494, acc-0.2405\n",
      "Iter-4270, train loss-2.2406, acc-0.2400, valid loss-2.2571, acc-0.2366, test loss-2.2493, acc-0.2405\n",
      "Iter-4280, train loss-2.2855, acc-0.2000, valid loss-2.2570, acc-0.2368, test loss-2.2492, acc-0.2407\n",
      "Iter-4290, train loss-2.2546, acc-0.2800, valid loss-2.2569, acc-0.2368, test loss-2.2491, acc-0.2408\n",
      "Iter-4300, train loss-2.2494, acc-0.2400, valid loss-2.2567, acc-0.2370, test loss-2.2490, acc-0.2408\n",
      "Iter-4310, train loss-2.2515, acc-0.2600, valid loss-2.2566, acc-0.2370, test loss-2.2489, acc-0.2412\n",
      "Iter-4320, train loss-2.2241, acc-0.3000, valid loss-2.2565, acc-0.2366, test loss-2.2487, acc-0.2414\n",
      "Iter-4330, train loss-2.2491, acc-0.2000, valid loss-2.2564, acc-0.2372, test loss-2.2486, acc-0.2413\n",
      "Iter-4340, train loss-2.2381, acc-0.3000, valid loss-2.2563, acc-0.2374, test loss-2.2485, acc-0.2414\n",
      "Iter-4350, train loss-2.2512, acc-0.2200, valid loss-2.2562, acc-0.2376, test loss-2.2484, acc-0.2416\n",
      "Iter-4360, train loss-2.2619, acc-0.1400, valid loss-2.2561, acc-0.2380, test loss-2.2483, acc-0.2418\n",
      "Iter-4370, train loss-2.2377, acc-0.2800, valid loss-2.2560, acc-0.2380, test loss-2.2482, acc-0.2418\n",
      "Iter-4380, train loss-2.2234, acc-0.2000, valid loss-2.2559, acc-0.2380, test loss-2.2481, acc-0.2419\n",
      "Iter-4390, train loss-2.2796, acc-0.1400, valid loss-2.2558, acc-0.2382, test loss-2.2480, acc-0.2420\n",
      "Iter-4400, train loss-2.3049, acc-0.1000, valid loss-2.2557, acc-0.2382, test loss-2.2478, acc-0.2422\n",
      "Iter-4410, train loss-2.2513, acc-0.2600, valid loss-2.2555, acc-0.2392, test loss-2.2477, acc-0.2424\n",
      "Iter-4420, train loss-2.2714, acc-0.1600, valid loss-2.2554, acc-0.2394, test loss-2.2476, acc-0.2425\n",
      "Iter-4430, train loss-2.2527, acc-0.2800, valid loss-2.2553, acc-0.2398, test loss-2.2475, acc-0.2428\n",
      "Iter-4440, train loss-2.2119, acc-0.3000, valid loss-2.2552, acc-0.2404, test loss-2.2474, acc-0.2429\n",
      "Iter-4450, train loss-2.2931, acc-0.1400, valid loss-2.2551, acc-0.2400, test loss-2.2473, acc-0.2433\n",
      "Iter-4460, train loss-2.2265, acc-0.3600, valid loss-2.2550, acc-0.2402, test loss-2.2472, acc-0.2436\n",
      "Iter-4470, train loss-2.2536, acc-0.2400, valid loss-2.2549, acc-0.2406, test loss-2.2471, acc-0.2439\n",
      "Iter-4480, train loss-2.2466, acc-0.2800, valid loss-2.2548, acc-0.2402, test loss-2.2470, acc-0.2443\n",
      "Iter-4490, train loss-2.2407, acc-0.2400, valid loss-2.2547, acc-0.2404, test loss-2.2468, acc-0.2443\n",
      "Iter-4500, train loss-2.2817, acc-0.1200, valid loss-2.2546, acc-0.2408, test loss-2.2467, acc-0.2447\n",
      "Iter-4510, train loss-2.2424, acc-0.2400, valid loss-2.2545, acc-0.2412, test loss-2.2466, acc-0.2449\n",
      "Iter-4520, train loss-2.2549, acc-0.2000, valid loss-2.2544, acc-0.2414, test loss-2.2465, acc-0.2449\n",
      "Iter-4530, train loss-2.2968, acc-0.1800, valid loss-2.2543, acc-0.2414, test loss-2.2464, acc-0.2450\n",
      "Iter-4540, train loss-2.2306, acc-0.3400, valid loss-2.2542, acc-0.2416, test loss-2.2463, acc-0.2451\n",
      "Iter-4550, train loss-2.2774, acc-0.2000, valid loss-2.2540, acc-0.2420, test loss-2.2462, acc-0.2455\n",
      "Iter-4560, train loss-2.2599, acc-0.3200, valid loss-2.2539, acc-0.2422, test loss-2.2460, acc-0.2454\n",
      "Iter-4570, train loss-2.2456, acc-0.3000, valid loss-2.2538, acc-0.2424, test loss-2.2459, acc-0.2456\n",
      "Iter-4580, train loss-2.2161, acc-0.4000, valid loss-2.2537, acc-0.2426, test loss-2.2458, acc-0.2454\n",
      "Iter-4590, train loss-2.2024, acc-0.3800, valid loss-2.2536, acc-0.2428, test loss-2.2457, acc-0.2459\n",
      "Iter-4600, train loss-2.2545, acc-0.2400, valid loss-2.2535, acc-0.2428, test loss-2.2456, acc-0.2465\n",
      "Iter-4610, train loss-2.2613, acc-0.3000, valid loss-2.2534, acc-0.2430, test loss-2.2455, acc-0.2469\n",
      "Iter-4620, train loss-2.2515, acc-0.2400, valid loss-2.2533, acc-0.2428, test loss-2.2454, acc-0.2469\n",
      "Iter-4630, train loss-2.2639, acc-0.2600, valid loss-2.2532, acc-0.2430, test loss-2.2453, acc-0.2470\n",
      "Iter-4640, train loss-2.2488, acc-0.2800, valid loss-2.2531, acc-0.2434, test loss-2.2452, acc-0.2466\n",
      "Iter-4650, train loss-2.2404, acc-0.1800, valid loss-2.2530, acc-0.2440, test loss-2.2451, acc-0.2468\n",
      "Iter-4660, train loss-2.2094, acc-0.3800, valid loss-2.2529, acc-0.2442, test loss-2.2450, acc-0.2473\n",
      "Iter-4670, train loss-2.2792, acc-0.1800, valid loss-2.2528, acc-0.2444, test loss-2.2448, acc-0.2472\n",
      "Iter-4680, train loss-2.2774, acc-0.2000, valid loss-2.2526, acc-0.2444, test loss-2.2447, acc-0.2470\n",
      "Iter-4690, train loss-2.2358, acc-0.3000, valid loss-2.2525, acc-0.2438, test loss-2.2446, acc-0.2473\n",
      "Iter-4700, train loss-2.2706, acc-0.2400, valid loss-2.2524, acc-0.2436, test loss-2.2445, acc-0.2476\n",
      "Iter-4710, train loss-2.2305, acc-0.2800, valid loss-2.2523, acc-0.2442, test loss-2.2444, acc-0.2478\n",
      "Iter-4720, train loss-2.2667, acc-0.2400, valid loss-2.2522, acc-0.2446, test loss-2.2443, acc-0.2480\n",
      "Iter-4730, train loss-2.2408, acc-0.2000, valid loss-2.2521, acc-0.2452, test loss-2.2442, acc-0.2481\n",
      "Iter-4740, train loss-2.2624, acc-0.2000, valid loss-2.2520, acc-0.2458, test loss-2.2441, acc-0.2486\n",
      "Iter-4750, train loss-2.2115, acc-0.3400, valid loss-2.2519, acc-0.2462, test loss-2.2439, acc-0.2488\n",
      "Iter-4760, train loss-2.2249, acc-0.2600, valid loss-2.2518, acc-0.2460, test loss-2.2438, acc-0.2492\n",
      "Iter-4770, train loss-2.2360, acc-0.2800, valid loss-2.2517, acc-0.2468, test loss-2.2437, acc-0.2495\n",
      "Iter-4780, train loss-2.2547, acc-0.2000, valid loss-2.2516, acc-0.2460, test loss-2.2436, acc-0.2498\n",
      "Iter-4790, train loss-2.2495, acc-0.2600, valid loss-2.2515, acc-0.2462, test loss-2.2435, acc-0.2502\n",
      "Iter-4800, train loss-2.2463, acc-0.2800, valid loss-2.2514, acc-0.2460, test loss-2.2434, acc-0.2503\n",
      "Iter-4810, train loss-2.2548, acc-0.2000, valid loss-2.2513, acc-0.2462, test loss-2.2433, acc-0.2497\n",
      "Iter-4820, train loss-2.2890, acc-0.1600, valid loss-2.2511, acc-0.2462, test loss-2.2431, acc-0.2502\n",
      "Iter-4830, train loss-2.2398, acc-0.3000, valid loss-2.2510, acc-0.2460, test loss-2.2430, acc-0.2503\n",
      "Iter-4840, train loss-2.2384, acc-0.3200, valid loss-2.2509, acc-0.2462, test loss-2.2429, acc-0.2509\n",
      "Iter-4850, train loss-2.2128, acc-0.2800, valid loss-2.2508, acc-0.2466, test loss-2.2428, acc-0.2508\n",
      "Iter-4860, train loss-2.2592, acc-0.2200, valid loss-2.2507, acc-0.2464, test loss-2.2427, acc-0.2510\n",
      "Iter-4870, train loss-2.2155, acc-0.2600, valid loss-2.2506, acc-0.2466, test loss-2.2426, acc-0.2515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-2.2414, acc-0.2600, valid loss-2.2505, acc-0.2470, test loss-2.2425, acc-0.2517\n",
      "Iter-4890, train loss-2.2019, acc-0.3200, valid loss-2.2504, acc-0.2470, test loss-2.2424, acc-0.2518\n",
      "Iter-4900, train loss-2.2781, acc-0.1800, valid loss-2.2503, acc-0.2470, test loss-2.2422, acc-0.2522\n",
      "Iter-4910, train loss-2.2469, acc-0.2200, valid loss-2.2502, acc-0.2470, test loss-2.2421, acc-0.2525\n",
      "Iter-4920, train loss-2.2713, acc-0.2000, valid loss-2.2500, acc-0.2474, test loss-2.2420, acc-0.2526\n",
      "Iter-4930, train loss-2.2474, acc-0.2400, valid loss-2.2499, acc-0.2472, test loss-2.2419, acc-0.2529\n",
      "Iter-4940, train loss-2.2611, acc-0.2800, valid loss-2.2498, acc-0.2484, test loss-2.2418, acc-0.2534\n",
      "Iter-4950, train loss-2.2484, acc-0.2400, valid loss-2.2497, acc-0.2480, test loss-2.2417, acc-0.2535\n",
      "Iter-4960, train loss-2.2882, acc-0.2600, valid loss-2.2496, acc-0.2480, test loss-2.2416, acc-0.2536\n",
      "Iter-4970, train loss-2.2269, acc-0.2000, valid loss-2.2495, acc-0.2490, test loss-2.2415, acc-0.2538\n",
      "Iter-4980, train loss-2.2396, acc-0.2400, valid loss-2.2494, acc-0.2488, test loss-2.2414, acc-0.2540\n",
      "Iter-4990, train loss-2.2548, acc-0.1600, valid loss-2.2493, acc-0.2492, test loss-2.2413, acc-0.2544\n",
      "Iter-5000, train loss-2.2484, acc-0.2200, valid loss-2.2492, acc-0.2494, test loss-2.2412, acc-0.2547\n",
      "Iter-5010, train loss-2.2260, acc-0.2400, valid loss-2.2491, acc-0.2492, test loss-2.2410, acc-0.2550\n",
      "Iter-5020, train loss-2.2171, acc-0.3400, valid loss-2.2490, acc-0.2492, test loss-2.2409, acc-0.2548\n",
      "Iter-5030, train loss-2.2339, acc-0.3200, valid loss-2.2489, acc-0.2496, test loss-2.2408, acc-0.2547\n",
      "Iter-5040, train loss-2.2896, acc-0.1200, valid loss-2.2488, acc-0.2496, test loss-2.2407, acc-0.2547\n",
      "Iter-5050, train loss-2.2240, acc-0.3600, valid loss-2.2487, acc-0.2496, test loss-2.2406, acc-0.2548\n",
      "Iter-5060, train loss-2.2628, acc-0.2000, valid loss-2.2486, acc-0.2498, test loss-2.2405, acc-0.2550\n",
      "Iter-5070, train loss-2.2131, acc-0.3000, valid loss-2.2484, acc-0.2502, test loss-2.2403, acc-0.2552\n",
      "Iter-5080, train loss-2.2500, acc-0.2200, valid loss-2.2483, acc-0.2502, test loss-2.2402, acc-0.2552\n",
      "Iter-5090, train loss-2.2673, acc-0.2000, valid loss-2.2482, acc-0.2504, test loss-2.2401, acc-0.2553\n",
      "Iter-5100, train loss-2.2507, acc-0.2600, valid loss-2.2481, acc-0.2504, test loss-2.2400, acc-0.2553\n",
      "Iter-5110, train loss-2.2159, acc-0.3000, valid loss-2.2480, acc-0.2510, test loss-2.2399, acc-0.2551\n",
      "Iter-5120, train loss-2.2583, acc-0.2400, valid loss-2.2479, acc-0.2508, test loss-2.2398, acc-0.2551\n",
      "Iter-5130, train loss-2.2291, acc-0.3000, valid loss-2.2478, acc-0.2510, test loss-2.2397, acc-0.2551\n",
      "Iter-5140, train loss-2.2394, acc-0.3000, valid loss-2.2477, acc-0.2510, test loss-2.2396, acc-0.2549\n",
      "Iter-5150, train loss-2.2469, acc-0.2200, valid loss-2.2476, acc-0.2512, test loss-2.2395, acc-0.2550\n",
      "Iter-5160, train loss-2.2748, acc-0.2200, valid loss-2.2475, acc-0.2512, test loss-2.2393, acc-0.2551\n",
      "Iter-5170, train loss-2.2264, acc-0.3000, valid loss-2.2474, acc-0.2510, test loss-2.2392, acc-0.2554\n",
      "Iter-5180, train loss-2.2608, acc-0.1800, valid loss-2.2473, acc-0.2520, test loss-2.2391, acc-0.2560\n",
      "Iter-5190, train loss-2.2610, acc-0.2200, valid loss-2.2472, acc-0.2520, test loss-2.2390, acc-0.2560\n",
      "Iter-5200, train loss-2.2421, acc-0.2200, valid loss-2.2471, acc-0.2522, test loss-2.2389, acc-0.2561\n",
      "Iter-5210, train loss-2.2384, acc-0.2400, valid loss-2.2470, acc-0.2528, test loss-2.2388, acc-0.2561\n",
      "Iter-5220, train loss-2.2341, acc-0.2400, valid loss-2.2468, acc-0.2530, test loss-2.2387, acc-0.2563\n",
      "Iter-5230, train loss-2.2707, acc-0.1600, valid loss-2.2467, acc-0.2528, test loss-2.2386, acc-0.2564\n",
      "Iter-5240, train loss-2.2303, acc-0.3200, valid loss-2.2466, acc-0.2528, test loss-2.2385, acc-0.2566\n",
      "Iter-5250, train loss-2.2414, acc-0.2800, valid loss-2.2465, acc-0.2530, test loss-2.2384, acc-0.2564\n",
      "Iter-5260, train loss-2.2532, acc-0.2400, valid loss-2.2464, acc-0.2534, test loss-2.2382, acc-0.2566\n",
      "Iter-5270, train loss-2.2041, acc-0.3400, valid loss-2.2463, acc-0.2528, test loss-2.2381, acc-0.2569\n",
      "Iter-5280, train loss-2.2200, acc-0.2200, valid loss-2.2462, acc-0.2532, test loss-2.2380, acc-0.2573\n",
      "Iter-5290, train loss-2.2704, acc-0.2600, valid loss-2.2461, acc-0.2536, test loss-2.2379, acc-0.2572\n",
      "Iter-5300, train loss-2.2135, acc-0.2800, valid loss-2.2460, acc-0.2536, test loss-2.2378, acc-0.2576\n",
      "Iter-5310, train loss-2.2058, acc-0.3400, valid loss-2.2459, acc-0.2540, test loss-2.2377, acc-0.2579\n",
      "Iter-5320, train loss-2.2509, acc-0.2000, valid loss-2.2458, acc-0.2544, test loss-2.2376, acc-0.2579\n",
      "Iter-5330, train loss-2.2604, acc-0.1800, valid loss-2.2457, acc-0.2542, test loss-2.2375, acc-0.2587\n",
      "Iter-5340, train loss-2.2499, acc-0.2800, valid loss-2.2456, acc-0.2546, test loss-2.2374, acc-0.2589\n",
      "Iter-5350, train loss-2.2498, acc-0.2400, valid loss-2.2454, acc-0.2550, test loss-2.2372, acc-0.2590\n",
      "Iter-5360, train loss-2.2283, acc-0.3200, valid loss-2.2453, acc-0.2552, test loss-2.2371, acc-0.2593\n",
      "Iter-5370, train loss-2.2469, acc-0.3600, valid loss-2.2452, acc-0.2552, test loss-2.2370, acc-0.2591\n",
      "Iter-5380, train loss-2.2428, acc-0.2000, valid loss-2.2451, acc-0.2556, test loss-2.2369, acc-0.2594\n",
      "Iter-5390, train loss-2.2503, acc-0.2400, valid loss-2.2450, acc-0.2554, test loss-2.2368, acc-0.2597\n",
      "Iter-5400, train loss-2.2506, acc-0.2200, valid loss-2.2449, acc-0.2558, test loss-2.2367, acc-0.2599\n",
      "Iter-5410, train loss-2.2727, acc-0.2000, valid loss-2.2448, acc-0.2558, test loss-2.2366, acc-0.2600\n",
      "Iter-5420, train loss-2.2470, acc-0.3000, valid loss-2.2447, acc-0.2558, test loss-2.2365, acc-0.2601\n",
      "Iter-5430, train loss-2.2432, acc-0.2400, valid loss-2.2446, acc-0.2556, test loss-2.2363, acc-0.2602\n",
      "Iter-5440, train loss-2.2275, acc-0.2600, valid loss-2.2445, acc-0.2562, test loss-2.2362, acc-0.2606\n",
      "Iter-5450, train loss-2.2360, acc-0.2600, valid loss-2.2444, acc-0.2568, test loss-2.2361, acc-0.2605\n",
      "Iter-5460, train loss-2.2503, acc-0.3400, valid loss-2.2443, acc-0.2568, test loss-2.2360, acc-0.2608\n",
      "Iter-5470, train loss-2.1980, acc-0.4200, valid loss-2.2442, acc-0.2574, test loss-2.2359, acc-0.2609\n",
      "Iter-5480, train loss-2.2376, acc-0.2400, valid loss-2.2440, acc-0.2572, test loss-2.2358, acc-0.2611\n",
      "Iter-5490, train loss-2.2050, acc-0.4000, valid loss-2.2440, acc-0.2572, test loss-2.2357, acc-0.2610\n",
      "Iter-5500, train loss-2.2288, acc-0.3000, valid loss-2.2438, acc-0.2574, test loss-2.2356, acc-0.2614\n",
      "Iter-5510, train loss-2.2483, acc-0.2000, valid loss-2.2437, acc-0.2578, test loss-2.2354, acc-0.2615\n",
      "Iter-5520, train loss-2.2342, acc-0.2800, valid loss-2.2436, acc-0.2574, test loss-2.2353, acc-0.2616\n",
      "Iter-5530, train loss-2.2696, acc-0.1800, valid loss-2.2435, acc-0.2582, test loss-2.2352, acc-0.2616\n",
      "Iter-5540, train loss-2.2429, acc-0.1800, valid loss-2.2434, acc-0.2582, test loss-2.2351, acc-0.2616\n",
      "Iter-5550, train loss-2.2322, acc-0.2600, valid loss-2.2433, acc-0.2584, test loss-2.2350, acc-0.2616\n",
      "Iter-5560, train loss-2.2538, acc-0.2200, valid loss-2.2432, acc-0.2586, test loss-2.2349, acc-0.2618\n",
      "Iter-5570, train loss-2.2178, acc-0.2800, valid loss-2.2431, acc-0.2588, test loss-2.2348, acc-0.2618\n",
      "Iter-5580, train loss-2.2440, acc-0.2600, valid loss-2.2430, acc-0.2582, test loss-2.2347, acc-0.2619\n",
      "Iter-5590, train loss-2.2503, acc-0.2200, valid loss-2.2429, acc-0.2584, test loss-2.2346, acc-0.2618\n",
      "Iter-5600, train loss-2.2473, acc-0.2200, valid loss-2.2428, acc-0.2588, test loss-2.2345, acc-0.2618\n",
      "Iter-5610, train loss-2.2143, acc-0.3200, valid loss-2.2427, acc-0.2590, test loss-2.2344, acc-0.2620\n",
      "Iter-5620, train loss-2.2368, acc-0.2800, valid loss-2.2426, acc-0.2586, test loss-2.2343, acc-0.2619\n",
      "Iter-5630, train loss-2.2393, acc-0.2600, valid loss-2.2425, acc-0.2594, test loss-2.2342, acc-0.2623\n",
      "Iter-5640, train loss-2.2678, acc-0.1800, valid loss-2.2424, acc-0.2592, test loss-2.2341, acc-0.2625\n",
      "Iter-5650, train loss-2.2307, acc-0.2200, valid loss-2.2423, acc-0.2594, test loss-2.2340, acc-0.2627\n",
      "Iter-5660, train loss-2.2179, acc-0.3400, valid loss-2.2422, acc-0.2596, test loss-2.2339, acc-0.2629\n",
      "Iter-5670, train loss-2.2082, acc-0.3600, valid loss-2.2421, acc-0.2598, test loss-2.2337, acc-0.2626\n",
      "Iter-5680, train loss-2.2656, acc-0.1600, valid loss-2.2420, acc-0.2598, test loss-2.2336, acc-0.2630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-2.2186, acc-0.2800, valid loss-2.2419, acc-0.2598, test loss-2.2335, acc-0.2633\n",
      "Iter-5700, train loss-2.2561, acc-0.1400, valid loss-2.2418, acc-0.2602, test loss-2.2334, acc-0.2636\n",
      "Iter-5710, train loss-2.2008, acc-0.3200, valid loss-2.2416, acc-0.2602, test loss-2.2333, acc-0.2638\n",
      "Iter-5720, train loss-2.2433, acc-0.2000, valid loss-2.2415, acc-0.2608, test loss-2.2332, acc-0.2635\n",
      "Iter-5730, train loss-2.2616, acc-0.2200, valid loss-2.2414, acc-0.2610, test loss-2.2331, acc-0.2636\n",
      "Iter-5740, train loss-2.1989, acc-0.4200, valid loss-2.2413, acc-0.2612, test loss-2.2330, acc-0.2635\n",
      "Iter-5750, train loss-2.2099, acc-0.4000, valid loss-2.2412, acc-0.2608, test loss-2.2329, acc-0.2637\n",
      "Iter-5760, train loss-2.2603, acc-0.2600, valid loss-2.2411, acc-0.2604, test loss-2.2327, acc-0.2641\n",
      "Iter-5770, train loss-2.2655, acc-0.1800, valid loss-2.2410, acc-0.2608, test loss-2.2326, acc-0.2638\n",
      "Iter-5780, train loss-2.2616, acc-0.2800, valid loss-2.2409, acc-0.2610, test loss-2.2325, acc-0.2639\n",
      "Iter-5790, train loss-2.2556, acc-0.1800, valid loss-2.2408, acc-0.2612, test loss-2.2324, acc-0.2638\n",
      "Iter-5800, train loss-2.1925, acc-0.3600, valid loss-2.2407, acc-0.2612, test loss-2.2323, acc-0.2640\n",
      "Iter-5810, train loss-2.2318, acc-0.2200, valid loss-2.2406, acc-0.2618, test loss-2.2322, acc-0.2639\n",
      "Iter-5820, train loss-2.2330, acc-0.3000, valid loss-2.2405, acc-0.2618, test loss-2.2321, acc-0.2644\n",
      "Iter-5830, train loss-2.2243, acc-0.2000, valid loss-2.2403, acc-0.2622, test loss-2.2320, acc-0.2647\n",
      "Iter-5840, train loss-2.2669, acc-0.1600, valid loss-2.2403, acc-0.2622, test loss-2.2319, acc-0.2649\n",
      "Iter-5850, train loss-2.2442, acc-0.2600, valid loss-2.2401, acc-0.2626, test loss-2.2317, acc-0.2649\n",
      "Iter-5860, train loss-2.2408, acc-0.3600, valid loss-2.2400, acc-0.2628, test loss-2.2316, acc-0.2649\n",
      "Iter-5870, train loss-2.2253, acc-0.2600, valid loss-2.2399, acc-0.2630, test loss-2.2315, acc-0.2648\n",
      "Iter-5880, train loss-2.2440, acc-0.2400, valid loss-2.2398, acc-0.2634, test loss-2.2314, acc-0.2654\n",
      "Iter-5890, train loss-2.2474, acc-0.2400, valid loss-2.2397, acc-0.2632, test loss-2.2313, acc-0.2658\n",
      "Iter-5900, train loss-2.2000, acc-0.3400, valid loss-2.2396, acc-0.2632, test loss-2.2312, acc-0.2660\n",
      "Iter-5910, train loss-2.2465, acc-0.2000, valid loss-2.2395, acc-0.2634, test loss-2.2311, acc-0.2659\n",
      "Iter-5920, train loss-2.2413, acc-0.2000, valid loss-2.2394, acc-0.2636, test loss-2.2310, acc-0.2660\n",
      "Iter-5930, train loss-2.2386, acc-0.3000, valid loss-2.2393, acc-0.2640, test loss-2.2309, acc-0.2664\n",
      "Iter-5940, train loss-2.2340, acc-0.2400, valid loss-2.2392, acc-0.2642, test loss-2.2308, acc-0.2666\n",
      "Iter-5950, train loss-2.2525, acc-0.2000, valid loss-2.2391, acc-0.2642, test loss-2.2307, acc-0.2670\n",
      "Iter-5960, train loss-2.2389, acc-0.3600, valid loss-2.2390, acc-0.2644, test loss-2.2306, acc-0.2672\n",
      "Iter-5970, train loss-2.2675, acc-0.1200, valid loss-2.2389, acc-0.2648, test loss-2.2304, acc-0.2672\n",
      "Iter-5980, train loss-2.2512, acc-0.1200, valid loss-2.2388, acc-0.2646, test loss-2.2303, acc-0.2675\n",
      "Iter-5990, train loss-2.1962, acc-0.3600, valid loss-2.2387, acc-0.2652, test loss-2.2302, acc-0.2677\n",
      "Iter-6000, train loss-2.2526, acc-0.3000, valid loss-2.2386, acc-0.2654, test loss-2.2301, acc-0.2680\n",
      "Iter-6010, train loss-2.2459, acc-0.2000, valid loss-2.2384, acc-0.2652, test loss-2.2300, acc-0.2684\n",
      "Iter-6020, train loss-2.2566, acc-0.2000, valid loss-2.2383, acc-0.2652, test loss-2.2299, acc-0.2687\n",
      "Iter-6030, train loss-2.2430, acc-0.3000, valid loss-2.2382, acc-0.2654, test loss-2.2298, acc-0.2688\n",
      "Iter-6040, train loss-2.2164, acc-0.2800, valid loss-2.2381, acc-0.2652, test loss-2.2297, acc-0.2693\n",
      "Iter-6050, train loss-2.2319, acc-0.2400, valid loss-2.2380, acc-0.2656, test loss-2.2296, acc-0.2692\n",
      "Iter-6060, train loss-2.2360, acc-0.2400, valid loss-2.2379, acc-0.2660, test loss-2.2295, acc-0.2692\n",
      "Iter-6070, train loss-2.2241, acc-0.4000, valid loss-2.2378, acc-0.2662, test loss-2.2293, acc-0.2693\n",
      "Iter-6080, train loss-2.2397, acc-0.1600, valid loss-2.2377, acc-0.2664, test loss-2.2292, acc-0.2692\n",
      "Iter-6090, train loss-2.2455, acc-0.2400, valid loss-2.2376, acc-0.2668, test loss-2.2291, acc-0.2697\n",
      "Iter-6100, train loss-2.2633, acc-0.2800, valid loss-2.2375, acc-0.2672, test loss-2.2290, acc-0.2699\n",
      "Iter-6110, train loss-2.2302, acc-0.3000, valid loss-2.2374, acc-0.2676, test loss-2.2289, acc-0.2700\n",
      "Iter-6120, train loss-2.2267, acc-0.2400, valid loss-2.2373, acc-0.2672, test loss-2.2288, acc-0.2699\n",
      "Iter-6130, train loss-2.2244, acc-0.3200, valid loss-2.2372, acc-0.2672, test loss-2.2287, acc-0.2699\n",
      "Iter-6140, train loss-2.2386, acc-0.2600, valid loss-2.2371, acc-0.2668, test loss-2.2286, acc-0.2701\n",
      "Iter-6150, train loss-2.2361, acc-0.2600, valid loss-2.2370, acc-0.2668, test loss-2.2285, acc-0.2702\n",
      "Iter-6160, train loss-2.2510, acc-0.1800, valid loss-2.2369, acc-0.2672, test loss-2.2284, acc-0.2701\n",
      "Iter-6170, train loss-2.2331, acc-0.2800, valid loss-2.2368, acc-0.2672, test loss-2.2283, acc-0.2701\n",
      "Iter-6180, train loss-2.2613, acc-0.1800, valid loss-2.2367, acc-0.2672, test loss-2.2282, acc-0.2702\n",
      "Iter-6190, train loss-2.2218, acc-0.2600, valid loss-2.2366, acc-0.2680, test loss-2.2281, acc-0.2703\n",
      "Iter-6200, train loss-2.2470, acc-0.2600, valid loss-2.2365, acc-0.2682, test loss-2.2280, acc-0.2706\n",
      "Iter-6210, train loss-2.2298, acc-0.2000, valid loss-2.2364, acc-0.2684, test loss-2.2278, acc-0.2708\n",
      "Iter-6220, train loss-2.2246, acc-0.3200, valid loss-2.2363, acc-0.2684, test loss-2.2277, acc-0.2708\n",
      "Iter-6230, train loss-2.2175, acc-0.2600, valid loss-2.2362, acc-0.2690, test loss-2.2276, acc-0.2709\n",
      "Iter-6240, train loss-2.2170, acc-0.3400, valid loss-2.2361, acc-0.2690, test loss-2.2275, acc-0.2709\n",
      "Iter-6250, train loss-2.2112, acc-0.2600, valid loss-2.2360, acc-0.2690, test loss-2.2274, acc-0.2712\n",
      "Iter-6260, train loss-2.1982, acc-0.2800, valid loss-2.2358, acc-0.2692, test loss-2.2273, acc-0.2713\n",
      "Iter-6270, train loss-2.2211, acc-0.2600, valid loss-2.2357, acc-0.2690, test loss-2.2272, acc-0.2713\n",
      "Iter-6280, train loss-2.2434, acc-0.2000, valid loss-2.2356, acc-0.2690, test loss-2.2271, acc-0.2714\n",
      "Iter-6290, train loss-2.2240, acc-0.3800, valid loss-2.2355, acc-0.2690, test loss-2.2270, acc-0.2714\n",
      "Iter-6300, train loss-2.2533, acc-0.2600, valid loss-2.2354, acc-0.2694, test loss-2.2269, acc-0.2719\n",
      "Iter-6310, train loss-2.2369, acc-0.2600, valid loss-2.2353, acc-0.2694, test loss-2.2268, acc-0.2719\n",
      "Iter-6320, train loss-2.2111, acc-0.2800, valid loss-2.2352, acc-0.2692, test loss-2.2266, acc-0.2719\n",
      "Iter-6330, train loss-2.2661, acc-0.1800, valid loss-2.2351, acc-0.2688, test loss-2.2265, acc-0.2720\n",
      "Iter-6340, train loss-2.2262, acc-0.3400, valid loss-2.2350, acc-0.2684, test loss-2.2264, acc-0.2720\n",
      "Iter-6350, train loss-2.2339, acc-0.3000, valid loss-2.2349, acc-0.2684, test loss-2.2263, acc-0.2720\n",
      "Iter-6360, train loss-2.2178, acc-0.3800, valid loss-2.2348, acc-0.2684, test loss-2.2262, acc-0.2723\n",
      "Iter-6370, train loss-2.2206, acc-0.3000, valid loss-2.2347, acc-0.2686, test loss-2.2261, acc-0.2727\n",
      "Iter-6380, train loss-2.2780, acc-0.2000, valid loss-2.2346, acc-0.2690, test loss-2.2260, acc-0.2727\n",
      "Iter-6390, train loss-2.2588, acc-0.2400, valid loss-2.2345, acc-0.2692, test loss-2.2259, acc-0.2727\n",
      "Iter-6400, train loss-2.2094, acc-0.2800, valid loss-2.2343, acc-0.2694, test loss-2.2258, acc-0.2727\n",
      "Iter-6410, train loss-2.2428, acc-0.2200, valid loss-2.2342, acc-0.2694, test loss-2.2256, acc-0.2730\n",
      "Iter-6420, train loss-2.2007, acc-0.3000, valid loss-2.2341, acc-0.2696, test loss-2.2255, acc-0.2731\n",
      "Iter-6430, train loss-2.2149, acc-0.3000, valid loss-2.2340, acc-0.2700, test loss-2.2254, acc-0.2731\n",
      "Iter-6440, train loss-2.2586, acc-0.2000, valid loss-2.2339, acc-0.2698, test loss-2.2253, acc-0.2730\n",
      "Iter-6450, train loss-2.2496, acc-0.1600, valid loss-2.2338, acc-0.2702, test loss-2.2252, acc-0.2735\n",
      "Iter-6460, train loss-2.2049, acc-0.3600, valid loss-2.2337, acc-0.2700, test loss-2.2251, acc-0.2735\n",
      "Iter-6470, train loss-2.2124, acc-0.3000, valid loss-2.2336, acc-0.2698, test loss-2.2250, acc-0.2734\n",
      "Iter-6480, train loss-2.2213, acc-0.3000, valid loss-2.2335, acc-0.2702, test loss-2.2249, acc-0.2736\n",
      "Iter-6490, train loss-2.2218, acc-0.2600, valid loss-2.2334, acc-0.2698, test loss-2.2248, acc-0.2737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-2.2136, acc-0.3000, valid loss-2.2333, acc-0.2698, test loss-2.2247, acc-0.2734\n",
      "Iter-6510, train loss-2.2285, acc-0.2000, valid loss-2.2332, acc-0.2702, test loss-2.2246, acc-0.2736\n",
      "Iter-6520, train loss-2.2523, acc-0.2200, valid loss-2.2331, acc-0.2704, test loss-2.2245, acc-0.2738\n",
      "Iter-6530, train loss-2.2060, acc-0.3600, valid loss-2.2330, acc-0.2702, test loss-2.2244, acc-0.2740\n",
      "Iter-6540, train loss-2.2029, acc-0.4000, valid loss-2.2329, acc-0.2702, test loss-2.2242, acc-0.2741\n",
      "Iter-6550, train loss-2.2149, acc-0.2200, valid loss-2.2328, acc-0.2708, test loss-2.2241, acc-0.2745\n",
      "Iter-6560, train loss-2.2091, acc-0.3200, valid loss-2.2327, acc-0.2704, test loss-2.2240, acc-0.2742\n",
      "Iter-6570, train loss-2.2066, acc-0.2800, valid loss-2.2326, acc-0.2710, test loss-2.2239, acc-0.2744\n",
      "Iter-6580, train loss-2.2816, acc-0.1800, valid loss-2.2325, acc-0.2716, test loss-2.2238, acc-0.2743\n",
      "Iter-6590, train loss-2.2493, acc-0.2400, valid loss-2.2324, acc-0.2714, test loss-2.2237, acc-0.2747\n",
      "Iter-6600, train loss-2.2278, acc-0.2400, valid loss-2.2323, acc-0.2718, test loss-2.2236, acc-0.2745\n",
      "Iter-6610, train loss-2.2401, acc-0.1800, valid loss-2.2321, acc-0.2718, test loss-2.2235, acc-0.2748\n",
      "Iter-6620, train loss-2.2315, acc-0.2600, valid loss-2.2320, acc-0.2716, test loss-2.2234, acc-0.2749\n",
      "Iter-6630, train loss-2.2483, acc-0.2600, valid loss-2.2319, acc-0.2718, test loss-2.2232, acc-0.2750\n",
      "Iter-6640, train loss-2.2199, acc-0.2800, valid loss-2.2318, acc-0.2724, test loss-2.2231, acc-0.2752\n",
      "Iter-6650, train loss-2.2405, acc-0.2400, valid loss-2.2317, acc-0.2722, test loss-2.2230, acc-0.2751\n",
      "Iter-6660, train loss-2.2257, acc-0.3000, valid loss-2.2316, acc-0.2720, test loss-2.2229, acc-0.2752\n",
      "Iter-6670, train loss-2.2107, acc-0.2800, valid loss-2.2315, acc-0.2720, test loss-2.2228, acc-0.2757\n",
      "Iter-6680, train loss-2.2155, acc-0.3600, valid loss-2.2314, acc-0.2722, test loss-2.2227, acc-0.2758\n",
      "Iter-6690, train loss-2.2215, acc-0.3600, valid loss-2.2313, acc-0.2724, test loss-2.2226, acc-0.2757\n",
      "Iter-6700, train loss-2.2181, acc-0.3000, valid loss-2.2312, acc-0.2726, test loss-2.2225, acc-0.2756\n",
      "Iter-6710, train loss-2.2585, acc-0.2600, valid loss-2.2311, acc-0.2728, test loss-2.2224, acc-0.2755\n",
      "Iter-6720, train loss-2.2512, acc-0.1600, valid loss-2.2310, acc-0.2730, test loss-2.2223, acc-0.2754\n",
      "Iter-6730, train loss-2.1958, acc-0.2200, valid loss-2.2309, acc-0.2730, test loss-2.2222, acc-0.2755\n",
      "Iter-6740, train loss-2.2437, acc-0.2600, valid loss-2.2308, acc-0.2732, test loss-2.2220, acc-0.2757\n",
      "Iter-6750, train loss-2.2279, acc-0.2400, valid loss-2.2307, acc-0.2732, test loss-2.2219, acc-0.2760\n",
      "Iter-6760, train loss-2.2382, acc-0.2000, valid loss-2.2306, acc-0.2734, test loss-2.2218, acc-0.2759\n",
      "Iter-6770, train loss-2.2033, acc-0.3200, valid loss-2.2305, acc-0.2734, test loss-2.2217, acc-0.2758\n",
      "Iter-6780, train loss-2.2757, acc-0.1600, valid loss-2.2304, acc-0.2738, test loss-2.2216, acc-0.2758\n",
      "Iter-6790, train loss-2.2212, acc-0.3400, valid loss-2.2303, acc-0.2740, test loss-2.2215, acc-0.2760\n",
      "Iter-6800, train loss-2.2314, acc-0.3200, valid loss-2.2302, acc-0.2742, test loss-2.2214, acc-0.2761\n",
      "Iter-6810, train loss-2.2116, acc-0.2600, valid loss-2.2301, acc-0.2744, test loss-2.2213, acc-0.2764\n",
      "Iter-6820, train loss-2.1759, acc-0.4000, valid loss-2.2300, acc-0.2742, test loss-2.2212, acc-0.2765\n",
      "Iter-6830, train loss-2.1960, acc-0.3000, valid loss-2.2299, acc-0.2744, test loss-2.2211, acc-0.2763\n",
      "Iter-6840, train loss-2.2051, acc-0.3200, valid loss-2.2298, acc-0.2742, test loss-2.2210, acc-0.2766\n",
      "Iter-6850, train loss-2.2325, acc-0.2600, valid loss-2.2296, acc-0.2742, test loss-2.2209, acc-0.2768\n",
      "Iter-6860, train loss-2.2181, acc-0.3200, valid loss-2.2295, acc-0.2746, test loss-2.2207, acc-0.2771\n",
      "Iter-6870, train loss-2.2033, acc-0.2400, valid loss-2.2294, acc-0.2746, test loss-2.2206, acc-0.2767\n",
      "Iter-6880, train loss-2.2329, acc-0.2600, valid loss-2.2293, acc-0.2746, test loss-2.2205, acc-0.2765\n",
      "Iter-6890, train loss-2.2479, acc-0.2600, valid loss-2.2292, acc-0.2746, test loss-2.2204, acc-0.2766\n",
      "Iter-6900, train loss-2.2240, acc-0.3000, valid loss-2.2291, acc-0.2748, test loss-2.2203, acc-0.2767\n",
      "Iter-6910, train loss-2.1956, acc-0.3600, valid loss-2.2290, acc-0.2754, test loss-2.2202, acc-0.2769\n",
      "Iter-6920, train loss-2.2343, acc-0.2600, valid loss-2.2289, acc-0.2758, test loss-2.2201, acc-0.2772\n",
      "Iter-6930, train loss-2.2264, acc-0.2400, valid loss-2.2288, acc-0.2756, test loss-2.2200, acc-0.2774\n",
      "Iter-6940, train loss-2.2167, acc-0.2800, valid loss-2.2287, acc-0.2758, test loss-2.2199, acc-0.2772\n",
      "Iter-6950, train loss-2.2543, acc-0.3000, valid loss-2.2286, acc-0.2762, test loss-2.2198, acc-0.2772\n",
      "Iter-6960, train loss-2.2263, acc-0.1400, valid loss-2.2285, acc-0.2766, test loss-2.2197, acc-0.2775\n",
      "Iter-6970, train loss-2.2410, acc-0.2000, valid loss-2.2284, acc-0.2768, test loss-2.2196, acc-0.2777\n",
      "Iter-6980, train loss-2.2381, acc-0.2400, valid loss-2.2284, acc-0.2764, test loss-2.2195, acc-0.2775\n",
      "Iter-6990, train loss-2.2214, acc-0.3200, valid loss-2.2282, acc-0.2768, test loss-2.2194, acc-0.2776\n",
      "Iter-7000, train loss-2.2337, acc-0.2400, valid loss-2.2281, acc-0.2768, test loss-2.2193, acc-0.2778\n",
      "Iter-7010, train loss-2.2258, acc-0.3000, valid loss-2.2280, acc-0.2774, test loss-2.2192, acc-0.2778\n",
      "Iter-7020, train loss-2.2248, acc-0.2400, valid loss-2.2279, acc-0.2772, test loss-2.2191, acc-0.2780\n",
      "Iter-7030, train loss-2.2075, acc-0.2800, valid loss-2.2278, acc-0.2770, test loss-2.2190, acc-0.2780\n",
      "Iter-7040, train loss-2.1888, acc-0.3400, valid loss-2.2277, acc-0.2770, test loss-2.2189, acc-0.2783\n",
      "Iter-7050, train loss-2.2102, acc-0.2600, valid loss-2.2276, acc-0.2770, test loss-2.2187, acc-0.2786\n",
      "Iter-7060, train loss-2.2430, acc-0.3400, valid loss-2.2275, acc-0.2770, test loss-2.2186, acc-0.2785\n",
      "Iter-7070, train loss-2.2196, acc-0.2200, valid loss-2.2274, acc-0.2778, test loss-2.2185, acc-0.2787\n",
      "Iter-7080, train loss-2.2589, acc-0.2200, valid loss-2.2273, acc-0.2782, test loss-2.2184, acc-0.2790\n",
      "Iter-7090, train loss-2.1814, acc-0.3600, valid loss-2.2272, acc-0.2782, test loss-2.2183, acc-0.2794\n",
      "Iter-7100, train loss-2.2368, acc-0.2200, valid loss-2.2271, acc-0.2776, test loss-2.2182, acc-0.2790\n",
      "Iter-7110, train loss-2.1836, acc-0.4200, valid loss-2.2270, acc-0.2774, test loss-2.2181, acc-0.2793\n",
      "Iter-7120, train loss-2.1993, acc-0.4200, valid loss-2.2269, acc-0.2774, test loss-2.2180, acc-0.2796\n",
      "Iter-7130, train loss-2.2042, acc-0.4000, valid loss-2.2268, acc-0.2774, test loss-2.2179, acc-0.2799\n",
      "Iter-7140, train loss-2.2303, acc-0.2400, valid loss-2.2267, acc-0.2780, test loss-2.2178, acc-0.2800\n",
      "Iter-7150, train loss-2.2375, acc-0.2600, valid loss-2.2266, acc-0.2784, test loss-2.2177, acc-0.2801\n",
      "Iter-7160, train loss-2.2133, acc-0.3600, valid loss-2.2265, acc-0.2788, test loss-2.2176, acc-0.2801\n",
      "Iter-7170, train loss-2.1834, acc-0.4200, valid loss-2.2264, acc-0.2788, test loss-2.2175, acc-0.2801\n",
      "Iter-7180, train loss-2.2400, acc-0.3000, valid loss-2.2263, acc-0.2792, test loss-2.2174, acc-0.2807\n",
      "Iter-7190, train loss-2.2376, acc-0.1400, valid loss-2.2262, acc-0.2794, test loss-2.2173, acc-0.2807\n",
      "Iter-7200, train loss-2.2332, acc-0.3000, valid loss-2.2261, acc-0.2788, test loss-2.2172, acc-0.2810\n",
      "Iter-7210, train loss-2.2221, acc-0.2800, valid loss-2.2260, acc-0.2788, test loss-2.2171, acc-0.2810\n",
      "Iter-7220, train loss-2.2253, acc-0.2800, valid loss-2.2259, acc-0.2784, test loss-2.2170, acc-0.2812\n",
      "Iter-7230, train loss-2.2066, acc-0.3800, valid loss-2.2258, acc-0.2786, test loss-2.2169, acc-0.2812\n",
      "Iter-7240, train loss-2.1922, acc-0.3200, valid loss-2.2257, acc-0.2786, test loss-2.2168, acc-0.2814\n",
      "Iter-7250, train loss-2.1799, acc-0.3000, valid loss-2.2256, acc-0.2786, test loss-2.2166, acc-0.2814\n",
      "Iter-7260, train loss-2.2536, acc-0.1800, valid loss-2.2255, acc-0.2788, test loss-2.2165, acc-0.2818\n",
      "Iter-7270, train loss-2.2140, acc-0.3000, valid loss-2.2254, acc-0.2794, test loss-2.2164, acc-0.2816\n",
      "Iter-7280, train loss-2.2066, acc-0.2800, valid loss-2.2253, acc-0.2790, test loss-2.2163, acc-0.2817\n",
      "Iter-7290, train loss-2.2355, acc-0.2400, valid loss-2.2252, acc-0.2794, test loss-2.2162, acc-0.2818\n",
      "Iter-7300, train loss-2.1451, acc-0.5000, valid loss-2.2251, acc-0.2792, test loss-2.2161, acc-0.2819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-2.2201, acc-0.2400, valid loss-2.2250, acc-0.2792, test loss-2.2160, acc-0.2821\n",
      "Iter-7320, train loss-2.1524, acc-0.4400, valid loss-2.2249, acc-0.2792, test loss-2.2159, acc-0.2825\n",
      "Iter-7330, train loss-2.2483, acc-0.2000, valid loss-2.2248, acc-0.2798, test loss-2.2158, acc-0.2827\n",
      "Iter-7340, train loss-2.1997, acc-0.3600, valid loss-2.2247, acc-0.2800, test loss-2.2157, acc-0.2829\n",
      "Iter-7350, train loss-2.2232, acc-0.3000, valid loss-2.2246, acc-0.2800, test loss-2.2156, acc-0.2833\n",
      "Iter-7360, train loss-2.2235, acc-0.3400, valid loss-2.2245, acc-0.2800, test loss-2.2155, acc-0.2837\n",
      "Iter-7370, train loss-2.2246, acc-0.2400, valid loss-2.2244, acc-0.2800, test loss-2.2154, acc-0.2836\n",
      "Iter-7380, train loss-2.2081, acc-0.2600, valid loss-2.2243, acc-0.2802, test loss-2.2153, acc-0.2837\n",
      "Iter-7390, train loss-2.2263, acc-0.3000, valid loss-2.2242, acc-0.2804, test loss-2.2152, acc-0.2838\n",
      "Iter-7400, train loss-2.1590, acc-0.3600, valid loss-2.2241, acc-0.2806, test loss-2.2151, acc-0.2840\n",
      "Iter-7410, train loss-2.2795, acc-0.1000, valid loss-2.2240, acc-0.2808, test loss-2.2150, acc-0.2839\n",
      "Iter-7420, train loss-2.2386, acc-0.2000, valid loss-2.2238, acc-0.2808, test loss-2.2149, acc-0.2837\n",
      "Iter-7430, train loss-2.2139, acc-0.3200, valid loss-2.2237, acc-0.2806, test loss-2.2147, acc-0.2840\n",
      "Iter-7440, train loss-2.2068, acc-0.3200, valid loss-2.2236, acc-0.2806, test loss-2.2146, acc-0.2845\n",
      "Iter-7450, train loss-2.2269, acc-0.3400, valid loss-2.2235, acc-0.2810, test loss-2.2145, acc-0.2851\n",
      "Iter-7460, train loss-2.2235, acc-0.2400, valid loss-2.2234, acc-0.2810, test loss-2.2144, acc-0.2853\n",
      "Iter-7470, train loss-2.1856, acc-0.3600, valid loss-2.2233, acc-0.2814, test loss-2.2143, acc-0.2850\n",
      "Iter-7480, train loss-2.2260, acc-0.2000, valid loss-2.2232, acc-0.2818, test loss-2.2142, acc-0.2851\n",
      "Iter-7490, train loss-2.2165, acc-0.2400, valid loss-2.2231, acc-0.2816, test loss-2.2141, acc-0.2852\n",
      "Iter-7500, train loss-2.2406, acc-0.2400, valid loss-2.2230, acc-0.2816, test loss-2.2140, acc-0.2854\n",
      "Iter-7510, train loss-2.2149, acc-0.2600, valid loss-2.2229, acc-0.2816, test loss-2.2139, acc-0.2854\n",
      "Iter-7520, train loss-2.2241, acc-0.2600, valid loss-2.2228, acc-0.2818, test loss-2.2138, acc-0.2853\n",
      "Iter-7530, train loss-2.2374, acc-0.2800, valid loss-2.2227, acc-0.2820, test loss-2.2137, acc-0.2854\n",
      "Iter-7540, train loss-2.1865, acc-0.3800, valid loss-2.2226, acc-0.2822, test loss-2.2136, acc-0.2856\n",
      "Iter-7550, train loss-2.2227, acc-0.3200, valid loss-2.2225, acc-0.2824, test loss-2.2135, acc-0.2857\n",
      "Iter-7560, train loss-2.1869, acc-0.3400, valid loss-2.2224, acc-0.2824, test loss-2.2134, acc-0.2858\n",
      "Iter-7570, train loss-2.2324, acc-0.2200, valid loss-2.2223, acc-0.2828, test loss-2.2133, acc-0.2859\n",
      "Iter-7580, train loss-2.2148, acc-0.3400, valid loss-2.2222, acc-0.2830, test loss-2.2131, acc-0.2857\n",
      "Iter-7590, train loss-2.2129, acc-0.2800, valid loss-2.2221, acc-0.2832, test loss-2.2130, acc-0.2861\n",
      "Iter-7600, train loss-2.2037, acc-0.3200, valid loss-2.2220, acc-0.2830, test loss-2.2129, acc-0.2862\n",
      "Iter-7610, train loss-2.2039, acc-0.3400, valid loss-2.2219, acc-0.2832, test loss-2.2128, acc-0.2864\n",
      "Iter-7620, train loss-2.2080, acc-0.3200, valid loss-2.2218, acc-0.2830, test loss-2.2127, acc-0.2868\n",
      "Iter-7630, train loss-2.2220, acc-0.3000, valid loss-2.2217, acc-0.2828, test loss-2.2126, acc-0.2869\n",
      "Iter-7640, train loss-2.1948, acc-0.4200, valid loss-2.2216, acc-0.2826, test loss-2.2125, acc-0.2870\n",
      "Iter-7650, train loss-2.2223, acc-0.1600, valid loss-2.2215, acc-0.2832, test loss-2.2124, acc-0.2869\n",
      "Iter-7660, train loss-2.2388, acc-0.3000, valid loss-2.2214, acc-0.2830, test loss-2.2123, acc-0.2868\n",
      "Iter-7670, train loss-2.1922, acc-0.3200, valid loss-2.2213, acc-0.2832, test loss-2.2122, acc-0.2870\n",
      "Iter-7680, train loss-2.1986, acc-0.3000, valid loss-2.2212, acc-0.2834, test loss-2.2121, acc-0.2873\n",
      "Iter-7690, train loss-2.2003, acc-0.3200, valid loss-2.2211, acc-0.2834, test loss-2.2120, acc-0.2875\n",
      "Iter-7700, train loss-2.2134, acc-0.3200, valid loss-2.2210, acc-0.2834, test loss-2.2119, acc-0.2876\n",
      "Iter-7710, train loss-2.1893, acc-0.3000, valid loss-2.2209, acc-0.2834, test loss-2.2118, acc-0.2877\n",
      "Iter-7720, train loss-2.1957, acc-0.3800, valid loss-2.2208, acc-0.2834, test loss-2.2117, acc-0.2877\n",
      "Iter-7730, train loss-2.2151, acc-0.2000, valid loss-2.2207, acc-0.2838, test loss-2.2116, acc-0.2879\n",
      "Iter-7740, train loss-2.1947, acc-0.3400, valid loss-2.2206, acc-0.2838, test loss-2.2115, acc-0.2880\n",
      "Iter-7750, train loss-2.2436, acc-0.1800, valid loss-2.2205, acc-0.2842, test loss-2.2114, acc-0.2882\n",
      "Iter-7760, train loss-2.2514, acc-0.2200, valid loss-2.2204, acc-0.2846, test loss-2.2113, acc-0.2884\n",
      "Iter-7770, train loss-2.2365, acc-0.2200, valid loss-2.2203, acc-0.2846, test loss-2.2112, acc-0.2885\n",
      "Iter-7780, train loss-2.2494, acc-0.2200, valid loss-2.2202, acc-0.2848, test loss-2.2111, acc-0.2887\n",
      "Iter-7790, train loss-2.2221, acc-0.3000, valid loss-2.2201, acc-0.2848, test loss-2.2110, acc-0.2886\n",
      "Iter-7800, train loss-2.2186, acc-0.3000, valid loss-2.2200, acc-0.2850, test loss-2.2109, acc-0.2885\n",
      "Iter-7810, train loss-2.2327, acc-0.1800, valid loss-2.2199, acc-0.2850, test loss-2.2108, acc-0.2890\n",
      "Iter-7820, train loss-2.2018, acc-0.3400, valid loss-2.2198, acc-0.2848, test loss-2.2107, acc-0.2890\n",
      "Iter-7830, train loss-2.2300, acc-0.3400, valid loss-2.2197, acc-0.2850, test loss-2.2106, acc-0.2892\n",
      "Iter-7840, train loss-2.1562, acc-0.4200, valid loss-2.2196, acc-0.2858, test loss-2.2105, acc-0.2892\n",
      "Iter-7850, train loss-2.2098, acc-0.3600, valid loss-2.2195, acc-0.2862, test loss-2.2104, acc-0.2891\n",
      "Iter-7860, train loss-2.2386, acc-0.2200, valid loss-2.2194, acc-0.2864, test loss-2.2103, acc-0.2892\n",
      "Iter-7870, train loss-2.2320, acc-0.3000, valid loss-2.2193, acc-0.2862, test loss-2.2102, acc-0.2887\n",
      "Iter-7880, train loss-2.1971, acc-0.3000, valid loss-2.2192, acc-0.2860, test loss-2.2101, acc-0.2888\n",
      "Iter-7890, train loss-2.2100, acc-0.2800, valid loss-2.2191, acc-0.2858, test loss-2.2099, acc-0.2890\n",
      "Iter-7900, train loss-2.1857, acc-0.3200, valid loss-2.2190, acc-0.2862, test loss-2.2098, acc-0.2893\n",
      "Iter-7910, train loss-2.2188, acc-0.2800, valid loss-2.2189, acc-0.2860, test loss-2.2097, acc-0.2894\n",
      "Iter-7920, train loss-2.2028, acc-0.3000, valid loss-2.2188, acc-0.2860, test loss-2.2096, acc-0.2896\n",
      "Iter-7930, train loss-2.2240, acc-0.2600, valid loss-2.2187, acc-0.2864, test loss-2.2095, acc-0.2895\n",
      "Iter-7940, train loss-2.2127, acc-0.2600, valid loss-2.2186, acc-0.2864, test loss-2.2094, acc-0.2895\n",
      "Iter-7950, train loss-2.2074, acc-0.3200, valid loss-2.2185, acc-0.2868, test loss-2.2093, acc-0.2899\n",
      "Iter-7960, train loss-2.2053, acc-0.3000, valid loss-2.2184, acc-0.2868, test loss-2.2092, acc-0.2901\n",
      "Iter-7970, train loss-2.2032, acc-0.3400, valid loss-2.2183, acc-0.2870, test loss-2.2091, acc-0.2900\n",
      "Iter-7980, train loss-2.2154, acc-0.2800, valid loss-2.2182, acc-0.2870, test loss-2.2090, acc-0.2899\n",
      "Iter-7990, train loss-2.2422, acc-0.2000, valid loss-2.2181, acc-0.2874, test loss-2.2089, acc-0.2900\n",
      "Iter-8000, train loss-2.2552, acc-0.2800, valid loss-2.2180, acc-0.2874, test loss-2.2088, acc-0.2901\n",
      "Iter-8010, train loss-2.1933, acc-0.2800, valid loss-2.2179, acc-0.2874, test loss-2.2087, acc-0.2902\n",
      "Iter-8020, train loss-2.2119, acc-0.4000, valid loss-2.2178, acc-0.2878, test loss-2.2086, acc-0.2905\n",
      "Iter-8030, train loss-2.2074, acc-0.3000, valid loss-2.2177, acc-0.2878, test loss-2.2085, acc-0.2906\n",
      "Iter-8040, train loss-2.2604, acc-0.2000, valid loss-2.2176, acc-0.2878, test loss-2.2084, acc-0.2909\n",
      "Iter-8050, train loss-2.2432, acc-0.2000, valid loss-2.2175, acc-0.2874, test loss-2.2083, acc-0.2908\n",
      "Iter-8060, train loss-2.2097, acc-0.3200, valid loss-2.2174, acc-0.2876, test loss-2.2082, acc-0.2909\n",
      "Iter-8070, train loss-2.2034, acc-0.3200, valid loss-2.2172, acc-0.2880, test loss-2.2081, acc-0.2909\n",
      "Iter-8080, train loss-2.2025, acc-0.2400, valid loss-2.2171, acc-0.2878, test loss-2.2079, acc-0.2912\n",
      "Iter-8090, train loss-2.2252, acc-0.2400, valid loss-2.2170, acc-0.2876, test loss-2.2078, acc-0.2915\n",
      "Iter-8100, train loss-2.2474, acc-0.2600, valid loss-2.2169, acc-0.2878, test loss-2.2077, acc-0.2917\n",
      "Iter-8110, train loss-2.1941, acc-0.3000, valid loss-2.2168, acc-0.2882, test loss-2.2076, acc-0.2917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-2.2173, acc-0.2800, valid loss-2.2167, acc-0.2884, test loss-2.2075, acc-0.2918\n",
      "Iter-8130, train loss-2.2317, acc-0.1800, valid loss-2.2166, acc-0.2882, test loss-2.2074, acc-0.2918\n",
      "Iter-8140, train loss-2.1982, acc-0.2200, valid loss-2.2165, acc-0.2882, test loss-2.2073, acc-0.2918\n",
      "Iter-8150, train loss-2.1606, acc-0.3600, valid loss-2.2164, acc-0.2882, test loss-2.2072, acc-0.2918\n",
      "Iter-8160, train loss-2.2384, acc-0.2800, valid loss-2.2163, acc-0.2880, test loss-2.2071, acc-0.2920\n",
      "Iter-8170, train loss-2.1625, acc-0.3600, valid loss-2.2162, acc-0.2880, test loss-2.2070, acc-0.2919\n",
      "Iter-8180, train loss-2.1997, acc-0.3200, valid loss-2.2161, acc-0.2882, test loss-2.2069, acc-0.2920\n",
      "Iter-8190, train loss-2.1832, acc-0.3400, valid loss-2.2160, acc-0.2884, test loss-2.2068, acc-0.2925\n",
      "Iter-8200, train loss-2.2015, acc-0.3200, valid loss-2.2159, acc-0.2884, test loss-2.2067, acc-0.2925\n",
      "Iter-8210, train loss-2.1750, acc-0.4400, valid loss-2.2158, acc-0.2884, test loss-2.2066, acc-0.2928\n",
      "Iter-8220, train loss-2.2125, acc-0.2000, valid loss-2.2157, acc-0.2884, test loss-2.2065, acc-0.2930\n",
      "Iter-8230, train loss-2.2080, acc-0.3200, valid loss-2.2156, acc-0.2892, test loss-2.2064, acc-0.2932\n",
      "Iter-8240, train loss-2.2302, acc-0.1400, valid loss-2.2155, acc-0.2892, test loss-2.2063, acc-0.2936\n",
      "Iter-8250, train loss-2.2351, acc-0.2600, valid loss-2.2154, acc-0.2890, test loss-2.2061, acc-0.2938\n",
      "Iter-8260, train loss-2.1877, acc-0.4000, valid loss-2.2153, acc-0.2890, test loss-2.2060, acc-0.2936\n",
      "Iter-8270, train loss-2.2123, acc-0.3400, valid loss-2.2152, acc-0.2894, test loss-2.2059, acc-0.2939\n",
      "Iter-8280, train loss-2.1983, acc-0.2800, valid loss-2.2151, acc-0.2894, test loss-2.2058, acc-0.2941\n",
      "Iter-8290, train loss-2.2231, acc-0.2000, valid loss-2.2150, acc-0.2894, test loss-2.2057, acc-0.2941\n",
      "Iter-8300, train loss-2.2438, acc-0.2200, valid loss-2.2149, acc-0.2894, test loss-2.2056, acc-0.2941\n",
      "Iter-8310, train loss-2.2225, acc-0.2600, valid loss-2.2148, acc-0.2894, test loss-2.2055, acc-0.2943\n",
      "Iter-8320, train loss-2.1907, acc-0.3200, valid loss-2.2147, acc-0.2892, test loss-2.2054, acc-0.2944\n",
      "Iter-8330, train loss-2.2248, acc-0.3000, valid loss-2.2146, acc-0.2894, test loss-2.2053, acc-0.2946\n",
      "Iter-8340, train loss-2.2227, acc-0.3200, valid loss-2.2145, acc-0.2896, test loss-2.2052, acc-0.2944\n",
      "Iter-8350, train loss-2.1776, acc-0.3600, valid loss-2.2144, acc-0.2900, test loss-2.2051, acc-0.2943\n",
      "Iter-8360, train loss-2.1972, acc-0.3600, valid loss-2.2143, acc-0.2902, test loss-2.2050, acc-0.2944\n",
      "Iter-8370, train loss-2.2347, acc-0.2000, valid loss-2.2142, acc-0.2902, test loss-2.2049, acc-0.2945\n",
      "Iter-8380, train loss-2.2091, acc-0.2400, valid loss-2.2141, acc-0.2906, test loss-2.2048, acc-0.2944\n",
      "Iter-8390, train loss-2.2191, acc-0.2200, valid loss-2.2140, acc-0.2906, test loss-2.2047, acc-0.2945\n",
      "Iter-8400, train loss-2.2094, acc-0.3000, valid loss-2.2139, acc-0.2906, test loss-2.2046, acc-0.2946\n",
      "Iter-8410, train loss-2.2263, acc-0.2200, valid loss-2.2138, acc-0.2904, test loss-2.2045, acc-0.2950\n",
      "Iter-8420, train loss-2.2129, acc-0.2600, valid loss-2.2137, acc-0.2902, test loss-2.2044, acc-0.2949\n",
      "Iter-8430, train loss-2.2030, acc-0.3200, valid loss-2.2136, acc-0.2906, test loss-2.2043, acc-0.2949\n",
      "Iter-8440, train loss-2.2263, acc-0.1600, valid loss-2.2135, acc-0.2910, test loss-2.2042, acc-0.2949\n",
      "Iter-8450, train loss-2.1892, acc-0.4000, valid loss-2.2134, acc-0.2912, test loss-2.2040, acc-0.2949\n",
      "Iter-8460, train loss-2.1881, acc-0.3200, valid loss-2.2133, acc-0.2914, test loss-2.2039, acc-0.2953\n",
      "Iter-8470, train loss-2.1713, acc-0.4000, valid loss-2.2132, acc-0.2912, test loss-2.2038, acc-0.2952\n",
      "Iter-8480, train loss-2.2025, acc-0.3800, valid loss-2.2131, acc-0.2914, test loss-2.2037, acc-0.2953\n",
      "Iter-8490, train loss-2.2034, acc-0.3000, valid loss-2.2130, acc-0.2912, test loss-2.2036, acc-0.2954\n",
      "Iter-8500, train loss-2.2057, acc-0.3600, valid loss-2.2129, acc-0.2914, test loss-2.2035, acc-0.2952\n",
      "Iter-8510, train loss-2.2610, acc-0.2800, valid loss-2.2128, acc-0.2922, test loss-2.2034, acc-0.2953\n",
      "Iter-8520, train loss-2.2178, acc-0.2800, valid loss-2.2127, acc-0.2924, test loss-2.2033, acc-0.2957\n",
      "Iter-8530, train loss-2.1951, acc-0.3400, valid loss-2.2126, acc-0.2922, test loss-2.2032, acc-0.2956\n",
      "Iter-8540, train loss-2.2183, acc-0.2400, valid loss-2.2125, acc-0.2926, test loss-2.2031, acc-0.2956\n",
      "Iter-8550, train loss-2.1565, acc-0.3200, valid loss-2.2124, acc-0.2926, test loss-2.2030, acc-0.2956\n",
      "Iter-8560, train loss-2.1937, acc-0.2800, valid loss-2.2123, acc-0.2926, test loss-2.2029, acc-0.2956\n",
      "Iter-8570, train loss-2.1827, acc-0.3800, valid loss-2.2122, acc-0.2924, test loss-2.2028, acc-0.2959\n",
      "Iter-8580, train loss-2.2102, acc-0.2200, valid loss-2.2121, acc-0.2926, test loss-2.2027, acc-0.2958\n",
      "Iter-8590, train loss-2.2167, acc-0.2800, valid loss-2.2120, acc-0.2926, test loss-2.2026, acc-0.2961\n",
      "Iter-8600, train loss-2.2458, acc-0.1800, valid loss-2.2119, acc-0.2928, test loss-2.2025, acc-0.2963\n",
      "Iter-8610, train loss-2.1956, acc-0.3200, valid loss-2.2118, acc-0.2928, test loss-2.2024, acc-0.2961\n",
      "Iter-8620, train loss-2.2386, acc-0.2200, valid loss-2.2117, acc-0.2928, test loss-2.2023, acc-0.2964\n",
      "Iter-8630, train loss-2.2406, acc-0.2400, valid loss-2.2116, acc-0.2930, test loss-2.2022, acc-0.2966\n",
      "Iter-8640, train loss-2.2172, acc-0.3400, valid loss-2.2115, acc-0.2936, test loss-2.2021, acc-0.2970\n",
      "Iter-8650, train loss-2.1982, acc-0.2800, valid loss-2.2114, acc-0.2938, test loss-2.2020, acc-0.2971\n",
      "Iter-8660, train loss-2.1442, acc-0.4200, valid loss-2.2113, acc-0.2938, test loss-2.2019, acc-0.2970\n",
      "Iter-8670, train loss-2.2001, acc-0.3400, valid loss-2.2112, acc-0.2938, test loss-2.2018, acc-0.2970\n",
      "Iter-8680, train loss-2.2137, acc-0.2800, valid loss-2.2111, acc-0.2940, test loss-2.2017, acc-0.2973\n",
      "Iter-8690, train loss-2.1990, acc-0.3200, valid loss-2.2110, acc-0.2944, test loss-2.2016, acc-0.2977\n",
      "Iter-8700, train loss-2.1752, acc-0.3600, valid loss-2.2109, acc-0.2940, test loss-2.2015, acc-0.2976\n",
      "Iter-8710, train loss-2.1896, acc-0.2600, valid loss-2.2108, acc-0.2948, test loss-2.2014, acc-0.2982\n",
      "Iter-8720, train loss-2.2336, acc-0.2400, valid loss-2.2107, acc-0.2946, test loss-2.2013, acc-0.2980\n",
      "Iter-8730, train loss-2.1934, acc-0.3200, valid loss-2.2106, acc-0.2944, test loss-2.2012, acc-0.2982\n",
      "Iter-8740, train loss-2.1779, acc-0.3400, valid loss-2.2105, acc-0.2940, test loss-2.2011, acc-0.2984\n",
      "Iter-8750, train loss-2.1827, acc-0.4000, valid loss-2.2104, acc-0.2942, test loss-2.2009, acc-0.2982\n",
      "Iter-8760, train loss-2.2071, acc-0.3000, valid loss-2.2103, acc-0.2946, test loss-2.2008, acc-0.2986\n",
      "Iter-8770, train loss-2.1914, acc-0.1800, valid loss-2.2102, acc-0.2944, test loss-2.2007, acc-0.2987\n",
      "Iter-8780, train loss-2.2546, acc-0.1200, valid loss-2.2101, acc-0.2944, test loss-2.2006, acc-0.2988\n",
      "Iter-8790, train loss-2.2123, acc-0.3000, valid loss-2.2100, acc-0.2944, test loss-2.2005, acc-0.2987\n",
      "Iter-8800, train loss-2.2167, acc-0.2800, valid loss-2.2099, acc-0.2944, test loss-2.2004, acc-0.2987\n",
      "Iter-8810, train loss-2.2154, acc-0.3600, valid loss-2.2098, acc-0.2948, test loss-2.2003, acc-0.2989\n",
      "Iter-8820, train loss-2.1994, acc-0.3400, valid loss-2.2097, acc-0.2948, test loss-2.2002, acc-0.2989\n",
      "Iter-8830, train loss-2.1861, acc-0.3400, valid loss-2.2096, acc-0.2950, test loss-2.2001, acc-0.2990\n",
      "Iter-8840, train loss-2.1941, acc-0.3000, valid loss-2.2095, acc-0.2956, test loss-2.2000, acc-0.2992\n",
      "Iter-8850, train loss-2.1727, acc-0.4200, valid loss-2.2094, acc-0.2958, test loss-2.1999, acc-0.2992\n",
      "Iter-8860, train loss-2.2058, acc-0.3000, valid loss-2.2093, acc-0.2958, test loss-2.1998, acc-0.2994\n",
      "Iter-8870, train loss-2.1772, acc-0.4000, valid loss-2.2092, acc-0.2958, test loss-2.1997, acc-0.2997\n",
      "Iter-8880, train loss-2.2167, acc-0.2400, valid loss-2.2091, acc-0.2958, test loss-2.1996, acc-0.3000\n",
      "Iter-8890, train loss-2.1856, acc-0.3200, valid loss-2.2090, acc-0.2958, test loss-2.1995, acc-0.3000\n",
      "Iter-8900, train loss-2.1955, acc-0.3200, valid loss-2.2089, acc-0.2960, test loss-2.1994, acc-0.3000\n",
      "Iter-8910, train loss-2.1984, acc-0.2600, valid loss-2.2088, acc-0.2962, test loss-2.1993, acc-0.3003\n",
      "Iter-8920, train loss-2.2151, acc-0.2600, valid loss-2.2087, acc-0.2962, test loss-2.1992, acc-0.3005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-2.2100, acc-0.2600, valid loss-2.2086, acc-0.2960, test loss-2.1991, acc-0.3006\n",
      "Iter-8940, train loss-2.2084, acc-0.2600, valid loss-2.2085, acc-0.2958, test loss-2.1990, acc-0.3006\n",
      "Iter-8950, train loss-2.1764, acc-0.4200, valid loss-2.2084, acc-0.2956, test loss-2.1989, acc-0.3009\n",
      "Iter-8960, train loss-2.1943, acc-0.2200, valid loss-2.2083, acc-0.2958, test loss-2.1988, acc-0.3008\n",
      "Iter-8970, train loss-2.2043, acc-0.3000, valid loss-2.2082, acc-0.2964, test loss-2.1987, acc-0.3010\n",
      "Iter-8980, train loss-2.1681, acc-0.3800, valid loss-2.2081, acc-0.2964, test loss-2.1986, acc-0.3009\n",
      "Iter-8990, train loss-2.1680, acc-0.4000, valid loss-2.2080, acc-0.2964, test loss-2.1985, acc-0.3009\n",
      "Iter-9000, train loss-2.2365, acc-0.2600, valid loss-2.2080, acc-0.2964, test loss-2.1984, acc-0.3010\n",
      "Iter-9010, train loss-2.1436, acc-0.4000, valid loss-2.2078, acc-0.2962, test loss-2.1983, acc-0.3011\n",
      "Iter-9020, train loss-2.2081, acc-0.3000, valid loss-2.2078, acc-0.2964, test loss-2.1982, acc-0.3013\n",
      "Iter-9030, train loss-2.1788, acc-0.3800, valid loss-2.2076, acc-0.2970, test loss-2.1981, acc-0.3016\n",
      "Iter-9040, train loss-2.2033, acc-0.3200, valid loss-2.2075, acc-0.2970, test loss-2.1980, acc-0.3016\n",
      "Iter-9050, train loss-2.1947, acc-0.3400, valid loss-2.2075, acc-0.2968, test loss-2.1979, acc-0.3018\n",
      "Iter-9060, train loss-2.1951, acc-0.3600, valid loss-2.2074, acc-0.2968, test loss-2.1978, acc-0.3019\n",
      "Iter-9070, train loss-2.1824, acc-0.3600, valid loss-2.2073, acc-0.2970, test loss-2.1977, acc-0.3020\n",
      "Iter-9080, train loss-2.1845, acc-0.3600, valid loss-2.2072, acc-0.2974, test loss-2.1976, acc-0.3018\n",
      "Iter-9090, train loss-2.1661, acc-0.3800, valid loss-2.2070, acc-0.2974, test loss-2.1975, acc-0.3022\n",
      "Iter-9100, train loss-2.2003, acc-0.2400, valid loss-2.2069, acc-0.2972, test loss-2.1974, acc-0.3021\n",
      "Iter-9110, train loss-2.2047, acc-0.3200, valid loss-2.2069, acc-0.2974, test loss-2.1973, acc-0.3023\n",
      "Iter-9120, train loss-2.1816, acc-0.3400, valid loss-2.2068, acc-0.2978, test loss-2.1971, acc-0.3022\n",
      "Iter-9130, train loss-2.2003, acc-0.3000, valid loss-2.2067, acc-0.2982, test loss-2.1970, acc-0.3024\n",
      "Iter-9140, train loss-2.2012, acc-0.3400, valid loss-2.2066, acc-0.2980, test loss-2.1969, acc-0.3023\n",
      "Iter-9150, train loss-2.1878, acc-0.2800, valid loss-2.2065, acc-0.2984, test loss-2.1968, acc-0.3025\n",
      "Iter-9160, train loss-2.2198, acc-0.1800, valid loss-2.2064, acc-0.2984, test loss-2.1967, acc-0.3024\n",
      "Iter-9170, train loss-2.1536, acc-0.4000, valid loss-2.2063, acc-0.2984, test loss-2.1966, acc-0.3026\n",
      "Iter-9180, train loss-2.2143, acc-0.3000, valid loss-2.2062, acc-0.2984, test loss-2.1966, acc-0.3026\n",
      "Iter-9190, train loss-2.1891, acc-0.3600, valid loss-2.2061, acc-0.2984, test loss-2.1964, acc-0.3031\n",
      "Iter-9200, train loss-2.1974, acc-0.2600, valid loss-2.2060, acc-0.2984, test loss-2.1964, acc-0.3029\n",
      "Iter-9210, train loss-2.1637, acc-0.3200, valid loss-2.2059, acc-0.2986, test loss-2.1963, acc-0.3032\n",
      "Iter-9220, train loss-2.1792, acc-0.2600, valid loss-2.2058, acc-0.2988, test loss-2.1962, acc-0.3034\n",
      "Iter-9230, train loss-2.2062, acc-0.2600, valid loss-2.2057, acc-0.2988, test loss-2.1961, acc-0.3039\n",
      "Iter-9240, train loss-2.2175, acc-0.2800, valid loss-2.2056, acc-0.2990, test loss-2.1960, acc-0.3038\n",
      "Iter-9250, train loss-2.1856, acc-0.3000, valid loss-2.2055, acc-0.2988, test loss-2.1959, acc-0.3038\n",
      "Iter-9260, train loss-2.2143, acc-0.2400, valid loss-2.2054, acc-0.2986, test loss-2.1958, acc-0.3037\n",
      "Iter-9270, train loss-2.1719, acc-0.3400, valid loss-2.2053, acc-0.2986, test loss-2.1957, acc-0.3039\n",
      "Iter-9280, train loss-2.2141, acc-0.2200, valid loss-2.2052, acc-0.2986, test loss-2.1956, acc-0.3040\n",
      "Iter-9290, train loss-2.2181, acc-0.2200, valid loss-2.2051, acc-0.2990, test loss-2.1955, acc-0.3042\n",
      "Iter-9300, train loss-2.1947, acc-0.3400, valid loss-2.2050, acc-0.2990, test loss-2.1954, acc-0.3043\n",
      "Iter-9310, train loss-2.2026, acc-0.2400, valid loss-2.2049, acc-0.2992, test loss-2.1953, acc-0.3045\n",
      "Iter-9320, train loss-2.1917, acc-0.3600, valid loss-2.2048, acc-0.2988, test loss-2.1951, acc-0.3041\n",
      "Iter-9330, train loss-2.1974, acc-0.3000, valid loss-2.2047, acc-0.2994, test loss-2.1951, acc-0.3045\n",
      "Iter-9340, train loss-2.1955, acc-0.2800, valid loss-2.2046, acc-0.2994, test loss-2.1949, acc-0.3047\n",
      "Iter-9350, train loss-2.1925, acc-0.4200, valid loss-2.2045, acc-0.2998, test loss-2.1948, acc-0.3045\n",
      "Iter-9360, train loss-2.1653, acc-0.3800, valid loss-2.2044, acc-0.2998, test loss-2.1947, acc-0.3046\n",
      "Iter-9370, train loss-2.1820, acc-0.2800, valid loss-2.2043, acc-0.2998, test loss-2.1946, acc-0.3046\n",
      "Iter-9380, train loss-2.2434, acc-0.2400, valid loss-2.2042, acc-0.2998, test loss-2.1945, acc-0.3046\n",
      "Iter-9390, train loss-2.1913, acc-0.3400, valid loss-2.2041, acc-0.2996, test loss-2.1944, acc-0.3048\n",
      "Iter-9400, train loss-2.2236, acc-0.2400, valid loss-2.2040, acc-0.2992, test loss-2.1943, acc-0.3050\n",
      "Iter-9410, train loss-2.2274, acc-0.2800, valid loss-2.2039, acc-0.2994, test loss-2.1942, acc-0.3049\n",
      "Iter-9420, train loss-2.1899, acc-0.2400, valid loss-2.2038, acc-0.2996, test loss-2.1941, acc-0.3052\n",
      "Iter-9430, train loss-2.1787, acc-0.3600, valid loss-2.2037, acc-0.3002, test loss-2.1940, acc-0.3050\n",
      "Iter-9440, train loss-2.1992, acc-0.2800, valid loss-2.2036, acc-0.3002, test loss-2.1939, acc-0.3054\n",
      "Iter-9450, train loss-2.1722, acc-0.3000, valid loss-2.2035, acc-0.3006, test loss-2.1938, acc-0.3053\n",
      "Iter-9460, train loss-2.2274, acc-0.2000, valid loss-2.2034, acc-0.3006, test loss-2.1937, acc-0.3055\n",
      "Iter-9470, train loss-2.1974, acc-0.3600, valid loss-2.2033, acc-0.3006, test loss-2.1936, acc-0.3059\n",
      "Iter-9480, train loss-2.2054, acc-0.2800, valid loss-2.2032, acc-0.3010, test loss-2.1935, acc-0.3060\n",
      "Iter-9490, train loss-2.2026, acc-0.3400, valid loss-2.2031, acc-0.3018, test loss-2.1934, acc-0.3060\n",
      "Iter-9500, train loss-2.2487, acc-0.1400, valid loss-2.2031, acc-0.3018, test loss-2.1933, acc-0.3063\n",
      "Iter-9510, train loss-2.1379, acc-0.4600, valid loss-2.2030, acc-0.3018, test loss-2.1932, acc-0.3062\n",
      "Iter-9520, train loss-2.2012, acc-0.2600, valid loss-2.2029, acc-0.3020, test loss-2.1931, acc-0.3063\n",
      "Iter-9530, train loss-2.1829, acc-0.3000, valid loss-2.2028, acc-0.3020, test loss-2.1930, acc-0.3064\n",
      "Iter-9540, train loss-2.2253, acc-0.2800, valid loss-2.2027, acc-0.3020, test loss-2.1929, acc-0.3065\n",
      "Iter-9550, train loss-2.1740, acc-0.4400, valid loss-2.2026, acc-0.3018, test loss-2.1928, acc-0.3068\n",
      "Iter-9560, train loss-2.2310, acc-0.2600, valid loss-2.2025, acc-0.3020, test loss-2.1927, acc-0.3068\n",
      "Iter-9570, train loss-2.1998, acc-0.3400, valid loss-2.2024, acc-0.3022, test loss-2.1926, acc-0.3071\n",
      "Iter-9580, train loss-2.2191, acc-0.3200, valid loss-2.2023, acc-0.3020, test loss-2.1925, acc-0.3071\n",
      "Iter-9590, train loss-2.1583, acc-0.2800, valid loss-2.2022, acc-0.3020, test loss-2.1924, acc-0.3069\n",
      "Iter-9600, train loss-2.1589, acc-0.3600, valid loss-2.2021, acc-0.3020, test loss-2.1923, acc-0.3069\n",
      "Iter-9610, train loss-2.1768, acc-0.2800, valid loss-2.2020, acc-0.3020, test loss-2.1922, acc-0.3070\n",
      "Iter-9620, train loss-2.1269, acc-0.4800, valid loss-2.2019, acc-0.3022, test loss-2.1921, acc-0.3069\n",
      "Iter-9630, train loss-2.1963, acc-0.3000, valid loss-2.2018, acc-0.3022, test loss-2.1920, acc-0.3069\n",
      "Iter-9640, train loss-2.1925, acc-0.3000, valid loss-2.2017, acc-0.3022, test loss-2.1919, acc-0.3070\n",
      "Iter-9650, train loss-2.1980, acc-0.3400, valid loss-2.2016, acc-0.3022, test loss-2.1918, acc-0.3070\n",
      "Iter-9660, train loss-2.1758, acc-0.3200, valid loss-2.2015, acc-0.3024, test loss-2.1917, acc-0.3070\n",
      "Iter-9670, train loss-2.2118, acc-0.2600, valid loss-2.2014, acc-0.3028, test loss-2.1916, acc-0.3073\n",
      "Iter-9680, train loss-2.2074, acc-0.2600, valid loss-2.2013, acc-0.3026, test loss-2.1915, acc-0.3079\n",
      "Iter-9690, train loss-2.2142, acc-0.2400, valid loss-2.2012, acc-0.3028, test loss-2.1914, acc-0.3080\n",
      "Iter-9700, train loss-2.1710, acc-0.3600, valid loss-2.2011, acc-0.3030, test loss-2.1913, acc-0.3079\n",
      "Iter-9710, train loss-2.2044, acc-0.2600, valid loss-2.2010, acc-0.3028, test loss-2.1912, acc-0.3082\n",
      "Iter-9720, train loss-2.1689, acc-0.4600, valid loss-2.2009, acc-0.3030, test loss-2.1911, acc-0.3082\n",
      "Iter-9730, train loss-2.2278, acc-0.3000, valid loss-2.2008, acc-0.3026, test loss-2.1910, acc-0.3083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-2.2413, acc-0.1400, valid loss-2.2007, acc-0.3028, test loss-2.1909, acc-0.3086\n",
      "Iter-9750, train loss-2.1831, acc-0.2800, valid loss-2.2006, acc-0.3032, test loss-2.1908, acc-0.3084\n",
      "Iter-9760, train loss-2.1797, acc-0.3400, valid loss-2.2005, acc-0.3032, test loss-2.1907, acc-0.3087\n",
      "Iter-9770, train loss-2.2117, acc-0.3600, valid loss-2.2004, acc-0.3034, test loss-2.1906, acc-0.3086\n",
      "Iter-9780, train loss-2.2006, acc-0.3200, valid loss-2.2003, acc-0.3032, test loss-2.1905, acc-0.3084\n",
      "Iter-9790, train loss-2.2025, acc-0.2800, valid loss-2.2002, acc-0.3034, test loss-2.1904, acc-0.3085\n",
      "Iter-9800, train loss-2.1458, acc-0.4000, valid loss-2.2001, acc-0.3032, test loss-2.1903, acc-0.3085\n",
      "Iter-9810, train loss-2.2016, acc-0.3800, valid loss-2.2000, acc-0.3032, test loss-2.1902, acc-0.3086\n",
      "Iter-9820, train loss-2.1599, acc-0.3200, valid loss-2.1999, acc-0.3034, test loss-2.1901, acc-0.3088\n",
      "Iter-9830, train loss-2.2141, acc-0.2000, valid loss-2.1998, acc-0.3038, test loss-2.1900, acc-0.3087\n",
      "Iter-9840, train loss-2.1787, acc-0.3200, valid loss-2.1997, acc-0.3040, test loss-2.1899, acc-0.3086\n",
      "Iter-9850, train loss-2.1938, acc-0.2200, valid loss-2.1996, acc-0.3044, test loss-2.1898, acc-0.3087\n",
      "Iter-9860, train loss-2.1787, acc-0.3800, valid loss-2.1995, acc-0.3044, test loss-2.1897, acc-0.3089\n",
      "Iter-9870, train loss-2.1750, acc-0.4200, valid loss-2.1994, acc-0.3040, test loss-2.1896, acc-0.3088\n",
      "Iter-9880, train loss-2.1785, acc-0.3800, valid loss-2.1993, acc-0.3042, test loss-2.1895, acc-0.3090\n",
      "Iter-9890, train loss-2.1623, acc-0.2800, valid loss-2.1992, acc-0.3044, test loss-2.1894, acc-0.3090\n",
      "Iter-9900, train loss-2.1953, acc-0.3600, valid loss-2.1991, acc-0.3044, test loss-2.1893, acc-0.3093\n",
      "Iter-9910, train loss-2.1854, acc-0.3400, valid loss-2.1990, acc-0.3046, test loss-2.1892, acc-0.3096\n",
      "Iter-9920, train loss-2.1773, acc-0.3600, valid loss-2.1989, acc-0.3044, test loss-2.1891, acc-0.3096\n",
      "Iter-9930, train loss-2.1936, acc-0.2000, valid loss-2.1988, acc-0.3042, test loss-2.1890, acc-0.3097\n",
      "Iter-9940, train loss-2.2480, acc-0.2600, valid loss-2.1987, acc-0.3046, test loss-2.1889, acc-0.3096\n",
      "Iter-9950, train loss-2.1659, acc-0.3200, valid loss-2.1986, acc-0.3046, test loss-2.1888, acc-0.3097\n",
      "Iter-9960, train loss-2.2064, acc-0.2400, valid loss-2.1985, acc-0.3048, test loss-2.1887, acc-0.3105\n",
      "Iter-9970, train loss-2.2148, acc-0.3000, valid loss-2.1984, acc-0.3048, test loss-2.1886, acc-0.3105\n",
      "Iter-9980, train loss-2.2047, acc-0.3200, valid loss-2.1983, acc-0.3044, test loss-2.1885, acc-0.3105\n",
      "Iter-9990, train loss-2.1978, acc-0.2800, valid loss-2.1982, acc-0.3044, test loss-2.1884, acc-0.3110\n",
      "Iter-10000, train loss-2.1847, acc-0.3400, valid loss-2.1981, acc-0.3044, test loss-2.1883, acc-0.3107\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEACAYAAABYq7oeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEBGROgoOIiJSR8FBRETq/B9Ld7avEXvruAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11889f6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4FcXawH+ThA4JCTVA6IiIgHQQUIpcQJCiNJGiFFEs\nYLmAghQreK+Nj6uiIKIIqHhpUq8lIqASmvSOdKQkBEgg5eT9/ticnJJzkpPk1GR+z7PP7s7Mzry7\n5+z7zszOO6NEBI1Go9EUTIJ8LYBGo9FofIc2AhqNRlOA0UZAo9FoCjDaCGg0Gk0BRhsBjUajKcBo\nI6DRaDQFGJeMgFKqq1LqoFLqsFJqgoP4e5VSV5VSO9K3ye4XVaPRaDTuJiS7BEqpIGA20Ak4B8Qo\npVaIyEG7pBtFpKcHZNRoNBqNh3ClJdACOCIiJ0UkBVgC9HKQTrlVMo1Go9F4HFeMQGXgtNX5mfQw\ne1orpXYppVYrpe5wi3QajUaj8SjZdge5yHagqogkKqW6AcuB29yUt0aj0Wg8hCtG4CxQ1eq8SnpY\nBiJyw+p4rVLqQ6VUhIjEWqdTSumJijQajSYXiIhHutxd6Q6KAWorpaoppQoDA4GV1gmUUhWsjlsA\nyt4AmBERvYkwdepUn8vgL5t+FoH5LEA4etQ/ngUI0dG257//7vtn5IrcZcs6jnv0USPeSOc5sm0J\niIhJKfU0sAHDaMwTkQNKqdFGtHwC9FVKPQmkADeBAZ4UWqPRaDTuwaVvAiKyDqhrFzbH6vg/wH/c\nK5pGo9EUXDzcAMhAewz7iPbt2/taBL9BPwsL+llYyOmz8JbSzG9oI+Aj9MtuQT8LC4H2LDypeAPt\nWeQWXxsvdw0R1Wg0uaB69eqcPHnS12Lkmjp1fC2BhQ4dbM9btfKNHDnlyhVQ6eN+qlWrxl9//QV4\nzzhoI6DR+JCTJ096fPSHJnBQyvsTL2gjoNG4keHDITERlizxtSSaQCYqCs6c8U5Zypu1EKWU6FqP\nJj9TpAgkJ7velFdK6ZaAJgPz/8G6QSCSEe4zZzGNRqPR5FO0EdBo3Iiu1DsmLS2NUqVKccZbfRwa\nl9FGQKPRZKJUqVKEhoYSGhpKcHAwxYsXzwhbvHhxjvMLCgri+vXrVKlSxQPSavKC/jCs0Wgycf36\n9YzjmjVrMm/ePDrYj8G0wmQyERwc7A3RvE5+vjfQLQGNxq3kx+4gy2RnFl555RUGDhzIoEGDCAsL\n46uvvuL333+ndevWhIeHU7lyZcaOHYvJZAIMRRoUFMSpU6cAGDJkCGPHjuX+++8nNDSUNm3aOPWX\nEBH69etHZGQkERERdOzYkYMHLQsb3rx5k+eee45q1aoRHh5O+/btSUlJAWDjxo20bt2a0qVLU61a\nNb766isA2rVrxxdffJGRh7WRM8v60UcfUadOHerVqwfAM888Q1RUFKVLl6Zly5b89ttvGdebTCZe\ne+01ateuTVhYGC1atODChQs88cQTTJw40eZ+unfvzn/+4z+z7GgjoNFocsXy5csZPHgw8fHxDBgw\ngEKFCjFr1ixiY2PZvHkz69evZ86cjCnGMo2BX7x4MW+88QZxcXFERUXxyiuvOC3rgQce4NixY1y4\ncIE777yTIUOGZMSNGzeOvXv3EhMTQ2xsLG+++SZBQUGcOHGC7t278+KLLxIbG8vOnTtp0KCB0zLs\n5Vu1ahXbtm1jz549ALRq1Yq9e/cSGxtL37596devX4axefvtt/nvf//Lhg0biI+PZ+7cuRQtWpRh\nw4axxGq88MWLF/nll18YNGiQC0/YS3h32lREo8nPBAeL5ORvnt07YbQt8r7lherVq8uPP/5oEzZ5\n8mTp1KlTltf9+9//lv79+4uISGpqqiil5OTJkyIiMnjwYHnyyScz0q5cuVIaNGjgkjyXLl0SpZQk\nJiaKyWSSIkWKyIEDBzKle+211zLKt6dt27ayYMGCjPO5c+dKhw4dbGTdtGmTUxnS0tKkVKlSsn//\nfhERqVWrlqxdu9Zh2rp160p0dLSIiLz//vvSq1cvp/ma/w/2v116uEf0sm4JaAoMPXrA/ff7Woqc\n4S4z4AmioqJszg8dOkSPHj2IjIwkLCyMqVOncvnyZafXV6xYMeO4ePHi3Lhxw2G6tLQ0xo8fT61a\ntShdujR16tRBKcXly5f5+++/SUlJoWbNmpmuO336NLVq1crl3ZHpI/bbb79NvXr1CA8PJyIigsTE\nxIz7O336tEMZwOj6WrhwIQALFy60acX4A9oIaAoMq1fD2rW+liL/YN99Mnr0aBo0aMDx48eJj49n\n+vTpbnGE++KLL1i3bh3R0dFcvXqVo0ePZtRiK1SoQOHChTl27Fim66Kiojh69KjDPEuUKEFiYmLG\n+YULFzKlsb6/6Oho3nvvPZYtW0ZcXBxxcXGUKFEi4/6qVq3qUAYwjMCyZcvYtWsXx48f54EHHsjR\n/XsabQQ0GjeSHz8Mu8r169cJCwujWLFiHDhwwOZ7QF7zLVKkCOHh4SQkJPDyyy9nKOigoCAeffRR\nxo0bx99//01aWhpbtmzBZDIxePBg1q9fz7JlyzCZTFy5coXdu3cDcNddd/Hdd99x69YtDh8+zGef\nfZatDIUKFSIiIoLk5GSmTp1qY0RGjBjB5MmTOX78OAB//vknV69eBQwD0bBhQ4YNG0a/fv0oXLiw\nW56Lu9BGQKPRZImrk5q98847fP7554SGhvLkk08ycOBAp/nkZKK0xx57jMjISCpVqkSDBg1o27at\nTfy7775LvXr1aNq0KWXKlGHSpEmICNWrV2fVqlXMmDGDiIgImjZtyt69ewF48cUXAahQoQIjR47M\n1EVjL9/9999Pp06dqFOnDjVr1qR06dJERkZmxP/zn/+kd+/edOrUibCwMEaPHs2tW7cy4ocNG8be\nvXsZOnSoy/ftLfTcQZoCg/m99uRfMDgY0tL03EEaW37++WdGjhzptMvIjJ47SKMJcLQ+19iTnJzM\nBx98wOOPP+5rURyijYBGo9F4iL179xIREcHVq1d55plnfC2OQ3R3kKbAoLuDNP6O7g7SFAiSkgIj\nz9yg9bkmr6Q7IXsNbQQ0XqdoUXAyfDtXXLtm5KnR5AciIrxbnjYCGp8QF+e+vPylFQC6JaDJO04c\npz2GNgIajUZTgNFGQBPw6Nq3RpN7tBHQaDRu5+TJkwQFBZGWlgYYHrdffvmlS2k13kUbAY1Gk4lu\n3boxbdq0TOErVqwgMjLSJYVtPfXCmjVrspw9MyfTSGjcizYCmoBHdwe5n2HDhmVMf2yNeSrkoKCC\nozryux9HwfklNRqNy/Tu3ZsrV66wadOmjLCrV6/y/fffZ0yCtmbNGpo0aUJYWBjVqlVj+vTpTvPr\n0KFDxkydaWlpvPjii5QrV47atWuzevXqLGWZOXMmtWvXJjQ0lDvvvJPly5fbxH/66afccccdGfG7\ndu0C4MyZMzz00EOUL1+ecuXK8eyzzwIwffp0m1aJfXdUhw4dmDx5Mm3btqVEiRKcOHGCzz//PKOM\n2rVr88knn9jIsGLFCho3bkxYWBh16tRhw4YNLF26lGbNmtmke/fdd+nTp0+W9+t1PLVajaMNvbJY\nvuTKlZylB5GtW91X/oULrq2eZb/K1tWrIikpeS8/Lc3yDLJaycvRc/Lnd2LUqFEyatSojPOPP/5Y\nGjdunHH+yy+/yN69e0VEZM+ePVKxYkVZsWKFiIj89ddfEhQUJCaTSURE2rdvL/PmzRMRkY8++kjq\n1asnZ8+elbi4OOnQoYNNWnuWLl0qFy5cEBGRb775RkqUKGFzXqVKFdm+fbuIiBw7dkxOnTolJpNJ\nGjVqJC+88ILcvHlTkpKSZPPmzSIiMm3aNBkyZEhG/o5krVatmhw4cEBMJpOkpKTImjVr5MSJEyIi\nsnHjRilevLjs3LlTRET++OMPCQsLy1h97dy5c3Lo0CFJSkqSMmXKyMGDBzPKaty4sSxbtszpMwcy\nLQl09apnVxbTRkCTJ5KSXFPA1oBITIz7ZDh/PndGAEQmT857+cuWGXklJjo3AtHRjsOzeyeYhlu2\n3LBp0yYpXbq0JCUliYhImzZt5P3333eafty4cfL888+LSNZGoGPHjjJnzpyM6zZs2JClEbDnrrvu\nkpUrV4qISJcuXWTWrFmZ0vz2229Svnx5h3m6YgSmTp2apQy9e/fOKHf06NEZ923PmDFjZHL6n2zv\n3r0SEREhycnJTvN1ZATSOzxFPKSXQ3zVAtHkD0wmX0uQN/76K+95/P23sU9NNfaOvnFmscpilshU\n3/VHt2nThnLlyrF8+XKaNWtGTEwMy5Yty4jfunUrEydOZO/evSQnJ5OcnEy/fv2yzffcuXM2S1NW\nq1Yty/RffPEF7733Hn+l/1gJCQk2yzo6WkLy9OnTVKtWLdffLuyXzly7di2vvvoqhw8fJi0tjZs3\nb9KwYcOMsrp37+4wn6FDhzJo0CBee+01Fi5cSP/+/SlUqFCuZPIU+puARuNmJB99RxwyZAgLFixg\n4cKFdOnShXLlymXEDRo0iN69e3P27FmuXr3K6NGjzS3+LImMjOT06dMZ5ydPnnSa9tSpUzz++ON8\n+OGHGcs61q9fP6OcqKgop0tLnjp1yuEoJvulJc+fP58pjfVopeTkZPr27cv48eO5dOkScXFxdOvW\nLVsZAFq2bEnhwoX59ddfWbRokd+tLwzaCGg0miwYOnQoP/zwA3PnzmXYsGE2cTdu3CA8PJxChQqx\ndetWFi1aZBPvzCD079+fWbNmcfbsWeLi4pg5c6bT8hMSEggKCqJs2bKkpaUxf/78jNXBAEaOHMm/\n//1vduzYAcCxY8c4ffo0LVq0IDIykokTJ5KYmEhSUhJbtmwBjKUlN27cyOnTp4mPj2fGjBlZPgNz\nK6ds2bIEBQWxdu1aNmzYkBE/YsQI5s+fz88//4yIcO7cOQ4dOpQRP2TIEJ5++mkKFy7M3XffnWVZ\nvkAbAU2Bxh21dvvun/w05L1atWrcfffdJCYm0rNnT5u4Dz/8kFdeeYWwsDBef/11BgwYYBPvbDnJ\nUaNG0aVLFxo1akSzZs146KGHnJZfr149XnjhBVq1akXFihXZt2+fzfKSffv2ZdKkSQwaNIjQ0FD6\n9OlDbGwsQUFBrFq1iiNHjlC1alWioqL45ptvALjvvvsYMGAADRs2pHnz5pkWfrf3WShZsiSzZs2i\nX79+REREsGTJEnr16pUR37x5c+bPn8+4ceMICwujffv2nDp1KiN+yJAh7N271y9bAeDiegJKqa7A\n+xhGY56IODTdSqnmwBZggIj810G8uFKeJnC4eROKF8+ZMlUKYmLAbvRcrrlwASIjs5fBfj0BpeCR\nR8DBcPgc8cknMHq0MZtpaKiRr30vxHffQd++mWXU6wnkf27dukWFChXYsWOHw+8X1hgGyNH/wYfr\nCSilgoDZQBegPvCwUup2J+lmAOvdLaRGo9EEKh9++CHNmzfP1gD4CldGB7UAjojISQCl1BKgF3DQ\nLt0zwFKguVsl1Pg1/tD14W8VaUfPxB+ek8b71KhRAyCTg5s/4co3gcrAaavzM+lhGSilKgG9ReQj\nQP/dNTZYdY9mkBOlKOI4D3uuXnWezjx801He9ly/DrGxlvOEBMsQz507bed7T02Fc+eM42vXspcx\n0IfUanLGiRMnOHHiBI0aNfK1KE5xl5/A+8AEq3Onr7j1pFTt27enffv2bhJB44/s2gWNG+ettr55\nM7Rrl30eTZvC8eOGorUfHv7ee66Xd999cOCARak//DCsWmWU36SJEWaW5d13wTxbQrt22ef98cfw\n1FOuy6IpqESnb57HFSNwFqhqdV4lPcyaZsASZXzVKAt0U0qliMhK+8wczUyoyb8kJOQ9j+vXs443\nK+Tjx419Xp21/vrLtszTp50m5dIl2+uclW/myhXX5dAUZNqnb2acz8uUV1wxAjFAbaVUNeA8MBB4\n2DqBiNQ0Hyul5gOrHBkATcHDWe3dX/rx3dlXr1T29+Uv963RmMnWCIiISSn1NLAByxDRA0qp0Ua0\nfGJ/iQfk1Pgpgf7B0xNKOdCfiaZg4dI3ARFZB9S1C5vjJO1wN8il0biMuxV5Tn0ecpJOGwiNv6E9\nhjWabMiponeEtYOaRuNPaCOgyRPe6OMOlH50V2r7gXIvpUqVIjQ0lNDQUIKDgylevHhG2OLFi3Od\nb+vWrTPNMaTxLXoqaY1P8JcasbeVcqAYgetWw6Nq1qzJvHnz6NChgw8l8g4mk4ng4GBfi+FVdEsg\ngLl5E44cyf31e/Z4ViklJICTGXYB2L/f4sS1d29mWfbsMRyzrOf837MHUlKMcfwXLsDFi0aYNTt3\nQnw8fPMNREcbabLCZDJkMWMe9rlnjzHs1GrW4Qzi4+HkSVtjZr6XlBQjbuVK457sn7P5vgPFccy8\n+Ig1aWlpvPbaa9SqVYvy5cszZMgQrqU7ViQmJvLwww9TpkwZwsPDad26NfHx8bz44ovExMQwcuRI\nQkND+ec//5mpLJPJRN++falYsSIRERF06tSJw4cPZ8QnJiby7LPPUrVqVcLDw+nQoUPGdNHR0dG0\nbt2a0qVLU716dZYsWQJkbn3MmTOHzp07A5CUlERQUBAff/wxtWvXpkGDBgCMGTOGqKgowsLCaNWq\nFX/88YeNjNOnT6dWrVqEhYXRsmVLLl68yMiRI5k8ebLN/XTp0oU5cxx+PvUfPLVajaMNvbKYW5kw\nIeerelkDIrt25U2GW7ecyzBmjOOVtkBk2zZjP3euJWzNGksa8ypdw4bZ5gEigwYZ+9BQkapVHa3C\nZLtVqiQyfrxtPhcvGscPPyyyYIGtjM7ysY7r2dPYW+fraNuyxdjPm2fsp0839p98Yi7X/9+J6tWr\nZyydaGbGjBlyzz33yIULFyQpKUkee+wxGT58uIiIfPDBB9KvXz9JSkoSk8kk27Ztk8TERBERadWq\nlSxatMhpWampqfLll19KYmKiJCUlyZgxY6RVq1YZ8cOHD5cuXbrIxYsXJS0tTTZt2iRpaWly5MgR\nKVmypCxbtkxMJpNcvnxZdu/enVHmV199lZHHxx9/LJ07dxYRkVu3bolSSnr06CHx8fFy69YtERH5\n8ssvJT4+XlJTU+XNN9+UqKgoSU1NFRGRV199VZo0aSLHjx8XEZFdu3ZJfHy8bNy4UWrUqJFRzrlz\n56REiRISFxfn8rPGByuL6ZZAABMfn/c8kpPznoczrl7NPo11LfvWLcuxeRbOuLjM15gdrq5dy76W\nD0Ya+1ZGSopjGVzFelqJrDA/X3P55n1CgouOdEq5Z3Mzc+bMYcaMGVSoUIHChQvzyiuvZNS8CxUq\nxKVLlzhy5AhBQUE0bdqUYsWKZVwrWTQ/g4ODGTx4MMWKFcvId+vWrSQnJ5OamsqXX37J7NmzKVeu\nHEop2rRpg1KKhQsX0rNnT3r37k1QUBBlypTJqNW7wuTJkwkNDaVIkSIADB48OON7yMSJE7ly5QrH\n070R582bx8yZMzPmBWrUqBGhoaG0a9eOoKAgNm/eDMCiRYvo2rUrpUuXztnD9TLaCGj8Gkf6y91d\nWLlcgdA7ZN/QcW1zM6dPn+b+++8nIiKCiIgImqTPpxEbG8uIESO455576Nu3L1WrVmXSpElZKn5r\nTCYTL7zwArVq1aJ06dLUq1cPgCtXrnD+/HlMJhM1a9bMdJ2zZSZdpUqVKjbnb731Frfffjvh4eFE\nRESQlJSUsaTl2bNnHcoAxtoBC9PnJl+4cKHfriFgjT///TUFDEd6wlMfkK3LyksZub3WA3rZq1Sp\nUoWffvqJ2NhYYmNjiYuLIyEhgYiICAoXLsz06dM5cOAAGzdu5Ntvv81oJdgv2GLP/Pnz+fHHH/nl\nl1+4evUqBw8akxWLCJGRkYSEhDhdTvLo0aMO87RfTvLChQuZ0ljL9cMPPzB79mxWrFhBXFwcsbGx\nFC1aNMOQValSxelykkOHDmXp0qXs2LGDM2fOOF172J/QRiCAcYci8fUoHWf34E7nKkc9ItblerMl\n4C7j42tGjx7NhAkTOHPmDAAXL17k+++/B+DHH3/kwIEDiAglS5YkJCQkY8RNhQoVMrpVHHH9+nWK\nFi1KeHg4N27cYNKkSRlxISEhDB06lLFjx3Lx4kXS0tLYvHkzIsKQIUNYvXo1K1aswGQycfnyZfak\njxi46667WLp0KUlJSRw8eJDPP/88y3u7fv06hQsXpkyZMiQlJfHKK6+QlJSUET9ixAhefvllTpw4\nAcCuXbsyPorXqFGDevXq8dhjjzFgwABCQvx/AKY2Ahq/xh2KMjtjmZsy8mqAA6kl4Kj2PmHCBDp3\n7kzHjh0JCwujbdu27Ny5EzC6S3r16kVoaCgNGzakR48e9O/fH4DnnnuOBQsWUKZMGSZOnJgp3xEj\nRlC2bFkqVqxIo0aNuOeee2ziP/jgA2rVqkXjxo0pW7YsU6ZMQUSoVasWK1as4I033iAiIoLmzZuz\nP33I1/jx40lJSaF8+fI88cQTmbpo7O/vgQceoF27dtSqVYvatWtTvnx5ypUrlxE/ceJEunfvnnHv\nTz75pI2RGDZsGHv37mXo0KE5ecy+w1NfnB1tBMBIiEBi9GjbUS05BURiYvImQ1KScxnMo3gcjQ7a\nvt3Yf/CBJey77yxpEhKMsIceyjw65x//sIQVKZJ9h3hISObRQadOGccDB1pG7ljLl93ooDZtjL15\nhJazLTpaMkZBgcjUqcb+nXdEPvpIAmJ0kCZnbNiwQerUqZOra9GjgzQFGVe/CeS0Fp1dTT833UF5\nbaEEUktA4zrJycnMmjWL0aNH+1oUl9FGQJNBXJzhtOWMrVvBqtVrw6ZNxv7KFcMZynzuDPuF2AG2\nb888XDM7ZetMHmvMzltmoqMd982fPAlZzWjw0UeWY7OT3u7dWZdtLtfsjBYTY+yXLjUWy9HkH/78\n808iIiJISEhgzJgxvhbHdTzVxHC0oZu+buXxx93bHfTww1nnZ919Y8bcHQQiN26I9OhhOe/f33l3\n0KxZmbuDQGTKFOPc3B1knweIdO6cfRdQdttff1m6g774Iu/55X7T74TGAro7SONtrGvaN29mn97a\nycoR1jX5rGrxztb8tXde8/QIGpHAHqWj0eQVbQQ0HsOVfm9nabw5/742ApqCjDYCAYw/Ki9XZXL1\nw6inPIYdfRPQaAoi/u/JoHGKO5Shv8lgn5+nFbRSvp02okiRatl60WoKDkWKVHNpsIM70UZA49d4\netoIX38TSEr6y3eFa/wObxsA0N1BmhySXc3fWqHmppVgr5C9MXeQX08gp9F4GP33L+AEYk+Eu7ug\nAvEZaDTuQncHBQgrV0LPnsbxwYO5V1w3b8KkSTBzpnH+xRfG6l1VqsDy5ZZ0M2fCQw8ZTl0XL9qO\n1lm/3pjLv0MH21W99u93fbWsF16wHM+aZRu3ahXUqWMcHzpkCTevHfDjj66VkRXmWYdNJm0ENAUc\nTzkgONrQjjG5BkRu3rQch4SIjBplcaJyFWO+GpGlS22dUaKiMjtlgUixYrbp/v1vy/GQIVk7QvXr\nZ5un9b2Yt/fftz3Pai4ea8cxd22RkZmfhd705n8bIqKdxQo81jVWR9MuuIKI43BnNfisavbO8nKH\nPDmVJS/l65aApiCjjUABw6zw3KHAs/ugmpsyvK2Q3fEcNJpARhuBAMUfaq+OjIAnZ9f0hMLWLQFN\nQUcbgQKGO5W0O4ZWagWs0fgWbQQKKJ7qDsqpn0BO5NBdNxqN+9FGIIDwh1qztQyO5Mmrs5i3CQQZ\nNRpPoo1AAOFOheWOid687WnrqW8CGk1BRhuBXLJgAVy/njk8NdV2BSp7Ll2Cdu1cK+PkScNxyhFK\nWRTY+fPw3XfGcXw8fPklfPop3Lrl+DrIvCLWuXOWY1edsT77LHOY9TDOn3+2HCckGGW//rpt+uee\nsz3PqmxrZzZ3cfkyPPig+/PVaAIGTzkgONqM4vIHIPLZZ5nD9+834pzx1FNZx1tjdo4yl3frluU4\nJERk5Ejj+OmnLelmz7Y4mPzwQ+Y858zJmZNK4cK25//6V+6cXRYtcrfzjN70VpA2REQ7i2mcIOJ6\nWl99V8iJjBqNxntoI5AHcqPYcqKEteLUaDSeRhsBL5OXmrizkTnOjIU/jCYyow2aRpNz7uEXFjDU\no2W4NIuoUqor8D6G0ZgnIjPt4nsCrwFpQArwnIhsdrOsfkdulj50Z0vAUXx2+fuTYdBoCi5CU7ZT\nk+O05A/Kcpm/qE44cRTjJvewkbocBuAkVT0qSbZGQCkVBMwGOgHngBil1AoROWiV7AcRWZmevgHw\nDVDPA/L6FZ7uDsorWuFrNL5CqMlxmrKd2hylK+uIIJYUCpFCIRrxJ0VIZhNtiCOcwiRznkiKcosD\n1GMN9/M7rbhARUClb57BlZZAC+CIiJwEUEotAXoBGUZARBKt0pfEaBFo8oi9kXFHl0pODYN9mdqw\naDQQhIl6HKAXKyjHJcKIpyvr2M8dNGQ3RUgilOv8Slu20oJYIlhOb3bQhESKc4uiHOB2UikMRa9C\n+HFIC4Y7lkLlDRByE6pvhJvhcOBBWOm5e3HFCFQGTludn8EwDDYopXoDbwHlgO5ukS4f4k4l6s4Z\nQV0lt2XpbwKawEIoxXXKc5FmbKMmx6nKKcpzkXDi6EB0Rso93Ml3PMQm2rKa7tykGJtoyzkqQd2V\nUOYIhIRDiRNQdBcEJ0HJC4aSt+ZmOJxtDteqgEqDovFwtoWxMc9jd+q2lcVEZDmwXCnVFngd6Oyu\nvD3B888bq2cVKmQb/uefsG0bjBiR8zyTkuCf/8w6jVlxx8XB++/D9Omwaxfs2AHDhzu+ZtMmx/nY\nz68/axa89JLl/MUXYft2w4GtUCE4ciR7+exJSbE9z+n1ZpYsyd11Go0nCSGFMOKpxDmaE0NL/qAh\nu2nFH9yiCBeoyAHqsZuGJFGEGJqzgyZMZTq/Fm0Apc4bSv3BIbB7MBQ5BBX/hKB5EHYSyh6GmCcN\nBX8tCs41A1MRSCkGGydDkWtwqh0klM9G0lEefAbZcxZsvkxUSQ9ziIhsUkrVVEpFiEisffy0adMy\njtu3b0/K05ZYAAAgAElEQVT79u1dFtadvPeescRh5cq24ZMnw/ff584IHD0Ka9ZkncZsBDZsgFdf\nNYzASy/BunXOjYBZFvtWhNlj2VzLHjvWNn7HDmP/99/GfsoUw/j4gtWrfVOuRmOmCLdoTzRN2c4b\nTAbgFkVIoARBpHGaKD7nURYxiOPU5HzhEqSGXoKoLYBAcDI0/xAqvOy4gEYL4FAvqLPWOF+4Bk50\nNJR+jolO3zyPK0YgBqitlKoGnAcGAg9bJ1BK1RKRY+nHTYDCjgwA2BqBgoi7RgfpvnmNxuibr8op\nCpFCOHE8yuekUIjLlKUU16nOX4QRzz/4HwB/0IIrRLCoSDfmF+vNxtCaJCeXhboroMM0OLoBqr8E\niWUh1GoulVNt4FI9ONoNjnWBP56FpFBjK3oVbkZY0v4www131j59MzPdDXk6JlsjICImpdTTwAYs\nQ0QPKKVGG9HyCfCQUmookAzcBPp7TGJNBnn9yKvRBBZCeS7SkZ8Yz9s0ZldGTDyhpBFEOFfZRSMO\nUZcdQQ04UKEsZ8uU4D+3HmdN5TKkdnjLKr/0GvvFO0CCIaEs/PEM7Ehvel+qD1fqQFoIWY7OsTYA\nAYhL3wREZB1Q1y5sjtXx28Db7hXNN7jLmcsd+buqtHPjT6DR+CPFSaAmxwnGRB2OcDdbqMUx2rCZ\ncOIIQrhJUVbTnbV0433GcYlyUHEXdB1nKPPQM5C2G8p9Y8n4VijEV4Mf3oKdjxl98EpAtL+s2z4M\nF0T83U/An8rWaAAiuEJFLnAXu7ibLURynvJcpDDJtCAmI90potgTXIctReoTUzGCx8PGERt+jZS4\n26BQEjT4CoocgKD/QsgtKH3SUsjfd8LWp+FiAzjT0jAMjhD9QkABNwK+qCFntxCLNa76Cbjqpaxb\nBBpPE4SJwiRTiXOU5ipVOMMAvuZefiGEVCpwkcPUIZYIjlOTrbRgT42LlAk+R6E7Yvgm9RFuVD4E\nlbdhjEz/ybaAHcMhtSjE1YTtjxs1elMRuFERUor74pYDngJtBByRk9pybmrW7qqN64noNP5AECba\nE01DdlOWy0ziTQCSKMw+6nOOSqQpmCAz2FGsOmcjErlW8SR0HwOXb0CxaGOY5cl2UA04dA12PQbr\n34PTrZ3X4jVuQxuBPOBpb1pz/lqJa/yBwiRxF7toynaKk8g/+RcVuMgRarOFu0kKhs6lPia6/2xS\nK+2FG2eh5M70q9PHCJ9tZjhCHbkfYsbAldvgRgVIKeGz+yroFCgj8MMPxopazz9vnLuqtD/+GCpV\ngp49c1Zez57w3/9CSPpTHjkSli61LdtkgrXpgxQmToQZVqPLVqww9keOGPvNm+HAAUv8t98a+08+\ncS6DUjB+vHGsHbY0riNEcZreLKcJO6gZfIB7TFsB2KYas6tyEjHXy/Fmq6r8VqkwlFkHJS8CX1iy\n+HMo7OtvOEiVuAS3SoOpsG9uR+MUJV6sZiqlxJvl2dOyJWzdatSslYIzZzI7i/XqBStX2ta+lYLq\n1eHECduwuXNtncoOHIA77jCOzWXExkJ4uOUaM0uWwMCBcPUqlC5tCbcv15ouXWD9euO4aFHHy0dq\nNK6gSKMS53ia2fxFdVIJYQ6j2VW8MkW5RVFTGhFyFQlKY2dYOLPbx7KvHBwpa5VJXHU43wR2Doeb\nZYx++fgo3YXjERQinvmSXaBaAr7AfmoHjcbTBGGiKdspyQ36spQEStCd1ZyiKg3YQ2XOZbpmbzlI\njIefG53mSBmISezCrhqxyJ6hUOocbG9nOEphrqXokTX5hQJtBLzRKMnOCOT2Q7Qe7llwCcJEbY5S\nggTu5RfKc5HaHKU2RynJDepwlDNU5jRVaMBufipTlVl041REErdX3syBYjU5U+UMe4tXgKRwo+Ye\nuRO+XA+//SOb0vUfL79RoI2AI/Iy6sbRtc6MgFbimqxQpNGAPRnTFHdnNcOZzw1KEEIqwZjYzx1c\noCK/BzVlZfkoDhRpxfW6P3G9fBHOB5eHGj+n53YIrsfD0W6sLd3ccJb6tbPRfaMp8BQoI+ALxevO\nloAm/xJKPB35iRlMpC6HSSWY46o6p6Qq14umUT7oPBtLleLTu68TXR3OhAJqN7AbY0YXOy6nwuIV\ncLiH9orVZEmBMgLuxhUFnuZkeR097FNTims8xHcMZAld0hV5dHBLRtzXlJjm20kOOQYcMxJfaAiH\nn4V9rWBbuDGfzahWcOw+WPYF3Ij03Y1oAhptBOxwtxOWMyOgKRgUJokefE8j/iSOcCpxjjDi6cJ6\nqnGKtcWboorE0rR7GXbUvgJXz0PpU8bFs/cbUyJcr+x4vvlpuiahyTsF2gi4uyvG098E9Idh/yMI\nE3U5RASxPMxikilMcRIJIZW6HKItmwFYH9yeU6GKuKJwPCyBXi3jOREJ1xKvGouNxDwF33Uyhlpq\nNF4kYPwEGjaE33+H4unTg3z+OZw9C5MmwenTMGwY/GQ3zUi9ejB/vrHi1qJFtnPoKGVcV6WKvYyW\nNPZhAOfOGY5jAOXLw5NPGiuDLV/uWDHffbfh5GWfz+jRMGeO4Vewf78lvE8fY3GYMWNswzX+Qx/+\nSxfWU5xEhrAQgLNU4iC387tqRkLZU/xdvBBxJZPYVCuBuLDrpFb/DYLTl2lLKAcr58KptgE/DbHG\nW3jOTyBgjIBScPw41KhhnFeubChkEcNztn9/x6N1nnwSPvrIovghb0bg+++hR4/M8lnn7yjOPp+Q\nEGPZR0cMHKi9e/2JEtygK+u4i130YRn1MazzTMbzEx35oc5N0no+CZfrWY3IAZJKGitL7RgFfzc0\nHKkAPcxSk3O0s5jf4A3nL/3R2LtEcIV+fMvdbCEYE/U4wDkqcY1QBrE4I93flGdO0KM8EjWKP1tu\ngjvSl9G4GQ5BKXC4O3z/EVytAaZCaGWvCQQCygj4Qz+4u4xAVopeGwHPU5HzDOFLRjKX2zjCbhqw\nk8bcpBhbacEFKnKSanwa2YSN960mLa1Y+tqxbxtz1F+PhG+XwJlWcK2KnipBE7AElBHIDe5WqHoa\niMCkDJcZyVz6sIxgTDRjO2voxgu8wy/cy3VKQfErELkDKsVAp8m2GXy7BIKT4JvvjInQNJp8QkAZ\nAWejY7y5ILvuDgoUhDJc4Q7205vlPM97GTGTeY0urCe2zGUY2hlKXrB8tD3bzOi/3zUUDjwEJzpA\ncikjbt8AH9yHRuNZAsoI+AO6JeCflOAG9/IL97OGp/gwI3wv9VlXLoraA+GYefTlqXVQ9RXLxb89\nB9tHGStWXa3hXcE1Gh8TUEbA1dq/J/GGEdAOZq4RxSleZQqPsgCAbTQlITiE+8u+ydmOH7KvQhqm\n0vuAfZBYBq6Ew9mWsPsRiK1j9OWnFkF/wNUUZALKCOQGdxsLdylo3eXjOoVI5nYO0piddOZ/VOEM\ndTlEJBc4TG1GlnyVBfdvJbVwMtTeAPxhXPi/GdB5Iny12ljJSqPRZEZEvLYZxYmMHSvy3ntiw9Sp\nIiASHW0bHh4ucuWKEfd//yfywANGeKVKRtjEiSJff20cWzNsmBFm3kqWtBwbzgoi778vctttIvXr\nG+cDB1rS3HabSKdOIoMG2ebjbHvxRedxd9yROSwoyLV8C+IWTIrcQ7Qsp6f8l94ZEfuoJxu4T0Yy\nR1rdNkFCXkGYZrW1fVMoeU4ISvH5PehNb+7dEE/pZZ84iyllOGmdPm0dZ+zHj4eZM23Dd+2Cu+4y\nvG+3bDEeSZUqhscwwNdfw4AB6Y/KLj9HiBjxzZrBtm3uu7+cEBysvy9YE04sHzCWZmyjOn9RjFtc\npgxTmc7qoq05Wf8PY06desug7CGje2fLC3D8Poirqadb0ORztLOYJh9Sims8wCo+YzhFSCYNxfO8\ny0IGcyUiDtpPh4ZPgyhD0V+4C/YONNatvVQP3Zev0eQdvzMCXmyY+JSCcp+2CJU5S2+W8xYvUYob\n7OFOVvEA71Rqx+9hUVD1V2hdDlKKpiv+RrBsAfzdyNfCazT5Ep8ZgZwoQX/wFHY3BcMICKOZQzt+\npSG7acBem9j2hVbxy50XofW7UP47OPQAlD4BZ1rAFz9YxudrNBqP4Xctgdwo/IKhUAOHClxgG82o\nwtmMsM5sYDNtSKYwpqI3YGxNKPYAnGwHGycbXTx6BSyNxuv4nRHwpkL3ZQtDqfxnvOpwmGX0yZhl\n8z7+x490wqbvvtx+eKq+cbzsc/hzKLpvX6PxHX5nBLyJL41AfjEAijR6sYJ+fEsPvmcWz9Ka37hO\nqG3CsFPwXDXjePtIWPWp94XVaDSZ8DsjkJVidrfS9qUiDnQjEEo8w1jAS7xFJBd4k5doTgyHqWuX\n8Aw8/ACEnoXzjWHhOsdLJWo0Gp/g005YpYytTh1L2ObNxopd5nhrzIpz1izbuORkS/rhw7M3Fp+m\nV0JjYnIve0GkCLfoyI9soDPxlOY+fuBhFqNIYxJv2hqAkJvQeTw8HwWRu2D55/BJjDYAGo2f4Rct\ngaNHLce//ZZ5agZ7pf7xx7bnZiMAxnKS2fGp7onIEYo0WvIHv3E3f1OeJQzkTV4mmg52KcWYjbPh\nl9BhCgSZYO4WY8593e+v0bjEiBEwb573yguIIaLu7jrJj0NOPUEQJiYyg1F8SgSxvMVEXuYtu0Sp\nELkdBneDYnGW8N2PGLX/NL+oZ2g0AUOxYt4tz+/eUG98E9BGIHsqcIELRBJDM/rzDTE0x1KbF2j0\nJdy+HKpHG+fFrsLJtvDlBkj18r9Yo8lHePt7od8ZAVewV+JaqbsPRRrdWc0qerKSB+jNcsT+09H9\nT0OLDyF6KuwYCUe6obt7NJrAxKUPw0qprkqpg0qpw0qpCQ7iByml/kzfNimlGrhTSLOSd2YhtRFw\nB8I43uMA9VhFT75kML1YaWsASvwNfYYaBmDPwxA9LX2KZv0DaDSBSrYtAaVUEDAb6AScA2KUUitE\n5KBVsuPAPSISr5TqCnwKtMoq37wo9CDtWOpWxvEe7/E8AP35mv/yICbrv8Zd86HbWChy3Thfugj2\nPuwDSTWa/I+3K7WudAe1AI6IyEkApdQSoBeQYQRE5Her9L8DlXMrUG76w3J6jW45GMM9Z/M0g1hE\ncW4ygrl8xnBAQaEEqLkaKm+Fe940Ljj6D/hhBlxo7FO5NRqNe3HFCFQGrGb+5wyGYXDGSGBtdpnm\nRRGb1wPILQXbCAiDWMRXDAZgOlN4ncmkUsiILnYFJpSF6xUhuSSse9eY1+d6ru26RqPxY9z6YVgp\n1QF4DGjrPI2xP3fOcbx5oRVrRd2wobHfssXY79uXNzl/+y1v1wcqQZiYy0ge43NeYzLTmWrb7XPP\n69AxfQH2WUchpYRvBNVoNF7Dld71s0BVq/Mq6WE2KKUaAp8APUUkzj7ewjSrLdpFMTV5JZhUvmYA\nw1hAE7YzhdcwEQyFEqHdG/B8FcMA7OsL09K0AQhwevXytQQWune3PXd1nW5z5S8rEhKMvbMu4R49\nXCtr2TLo3995fG6+QxYtCi+/bBz37WvsRSAx0ZKmTBlLeMZCkgBE8+yz07DoSs/hSksgBqitlKoG\nnAcGAjZfBZVSVYHvgCEicizr7KblRs4sCfR5eDzNSD7lUx4nhqaUrLCJm51fgtJ/QdnDlkRHusKn\nf+huH43bye1ADle6bbNL4+uu39zrpvaMG9eeWbPM59PdI5ADsjUCImJSSj0NbMBoOcwTkQNKqdFG\ntHwCvAJEAB8qpRSQIiJZfTdwK9oIOKYCF1hKX9qymVnFBvHiC4tICWkD/5sBZ1pDxZ2w7UkwFfa1\nqJp8TG4VsSvXuevd96axsJbZH3SXS98ERGQd2E4PKSJzrI5HAaPcK5rr+MOD9Dfu4Rd+oT0nioQT\n9ng5rpVZZES8FQ9J6dM8n7zHdwJqCgy+dO50tSzbrhjfyeELAtJjWOOcO9jH18F9uNN0hAn3wfut\n4kiOfgu2Pw43I3wtnsZL+JPS8aQs/t4dZMbawPiLTGbyhRHQLQGD50Je51WmsrR+Gu26wtXlK+CH\nB9AevQWP/PBOuENZ5iQPTzyz7PLMSj5v/YbaCOQDBrCEN5hErdTjtBkOW5YfgZm1fS2WRgN49puA\nu8iurPysY/KFESiotOdnfqZjxnnHobDli0Q9i6dGk0M8peS9MQNCXskXs/A88YSvJfA+jzOHn+nI\nb8Vrc8cYUIO68/M3sdoAaAD/6nd2Jss9bhiXUKhQ1vE9e7qeV5cuzuPGjXM9n0ceMfbDhtnmXa+e\ncWyWOTgYmjZ1PV9PkS+MQEGiPH+zhzuZGfQcvQbC3S+e4MDOd2DR93Ar3NfiaTxIzZqOw+094K1r\nkidPGvvffycT1pUn62uqVs2cNiuyqrmKOPYTEIFffsk6X0fLy75lt6ZRSIilfOsVBs3pH3vM2L/2\nWub8LWPwDUaOhLAwx7K8+27moZ316xvHL78Mx45ZwhcuNPbWKyCOHAn799vKnJrq2BBmN2uyu9Hd\nQQHEPfzC10EPUjEtlogXIe5Md3jzW137LyAEQtdCIGFtZPyp5eRttBEIAMpxkZ/owJ3sZ0gvWMTD\npM36j675awD3eNb6I+6U2dW8cjOayB9G+OQF3R3kpwSTyj95G0FxkQoUCj1B5Auw8PDXpC1bpA1A\nASQ3StHbXQvuxFdevN683h9+F90S8DOaso3HmM9TfMi2ojV47l74pj6cOzwUPnxTO3xpMpHbmqg/\nKCB/w92GJxCesTYCfkAIKYziU8byATU4wR4a8GSRGcwZPxE5ej+8u9rXImr8mEDs6nEFR/eVlVL1\n9hojrnQHBQLaCPiQypxhHO/zHO9xnVL0ZCW/qeak3rUIeo2EEx1gkTYAmpwT6IoJ3HsPjoxHfnhG\n7kB/E/ABo/iEa5TiGLVozE6e4f+IKL2NX++NJnVUW8MA7BgOC370tagaPyIv63L7kkDwGM5rmZ64\nzltdSdoIeInSxPEc73KQunzCaJ5mNiVI4D5+4KN2cci4OtDoC9j1KLx+E1bOQ8/5499UqeJrCaBW\nrezTBAdDtWpZO0NlR5s2ub82O7p3h27doGVLS1hQkK2/QseOma/LDY89Zjlu1cpybB7zP3as4dR1\n553QtWvWeZmVdKdO2afxZ7QR8BhCd75nHV3YS31OUZVurOV53qV4mR18MXgRpmmFYFoQNJkL7x83\nlnTc+gykFvW18G4hJ16W2VGhQu6v/fRTYz9+vCWsQYOcv6BpabbXZOepbvYQdcSkSY7D16yxHPfp\nY+xDQ53nY31PjjA7JVWsCIMHZ53W2sPVzNy5Rh6bNmV9rZlVq7KWxdEzHzzYuO+hQ43zsDBjmVnz\nby4CP6Y3il2pcScnO/9tP/vM2MfGGt66Zmet2ulTbU2ZYjh17dkDa9dCZQdrLDVpYnvevn32Mvkz\n+puAm2nKNp7gY+5iF6FcYyYT+IOW/E0FLhcqDk0/gQ4D4a8OsHg5XGgM16qA5D977Ok+XX/HW90Z\nvpyq2RW8/ds5ktmR97E7CcT/pxltBNyCMJAlLGYQAG/wMhOZwRbu5ibFAYH2043tRgVY8Rns70t+\n7+7xlxfDXaM4cnp9VumdxTnyYnXXWP/cTGvsb98b/OU/BZ6XRU8bEQAUJok2bOZ/dCaYNM5QmShO\nY6Pcew+Du74wjn+eDr9M8YmsGu/jrY9+7lLUnhpBk9Npmv3J8PiT0fEU2gjkGGEKr/IPNtCGLQAs\n5SEeZjGphEDpk9B+KkT9BqXOQuFE2P0ILJ8PadlMeajxCI5eZG+83LlZYD2rFajcPTrIlWcQSF67\n3i7LlTwCwYhoI+ASQmN2MpiFPM97gDHMsw/LuER5KHINmvwfdHnB9rItz8OvL8PNMj6Q2ff4U40O\nvC9PbrqD3JUmN11I/qKwAmWqC3+Xz1W0EXCKUIlzfMZw2rKJK5ThKLV5lPks5mGSVSGosxoG9YTE\nMhBX01D6m8dDQh6GsmjyDQXxw7AvFWNevH49dU0gGAptBKxQpFGHI3RlHU/yEbdzCID+fM1/eRAT\nIVD/a6gxFprNsVz46R8Q58KAbU2Bwl1KyZeLqQeSw5a3CQQF7wr5b1xijhFu4xC7aEQawRzidlrz\nG+/xHMGkooKT+Lb3akwPPgrTFPQbCEGp8L+Z8EYCTBNtAJzgDy9z06aWl7VnT8Nhatw4eP75rK9r\n1sxwYMoKc/yDDzqOz+r+S5fOOm/z9VOmwNSpxrkIRFjNH9iyJbRrZziCVatme63ZKapcOUtY69a2\naQYMMPavvGLsH3kE+vWDl15y7R4AJk7MHDZyZNbnzujQwdhPyWLsRNeu2Y/LdyRzt26W/MuXh5Il\nXZNp0iTH9wi2RqBCBcc+BX36GM5wzrCWy2eIiNc2QCwuI77bqnNchrBA/sOTNhEf8IxAilD2gNBm\nhjCmvjANY3slRHisrVBun8/l9+Y2e7aIUtmnq13bcjxhgrF//nljP2ZM3uUoX17kiSdyfp2IyMcf\nW47tMaf79FPba0REtmzJHGa+5tFHM+f1zju2ZTdp4lym//zHcdzq1Zbjfv1sy6xa1TguVszx/TiS\n1Z5Fi1xLZ06zcKEl7NVXjbDbbjP233yTOf3KlSIxMa6VYb7uq68cxz39dNa/2++/24alphrhqanZ\nl2umXTvX5LQut3Fj47hWrZxd6yrm+z50yPq/gYh4Ri8XiO6gIEwMZiGDWUghUqjHAf6gJdtoxqPM\nZ0HxrnDvG9BwIRT7P+OiS7cbUzgc6Q6X6oEE+/QeAhERY+/rFoFZDm/nmdchop54bjl9Fv78TcCb\nZflj+e4i3xoBRRp9WMYLvEM4cdTjIDGFb+f/qjTlm6rNSSp2CyqthbKHIPhpiK0NG/4FJ+81jjUZ\n+Muf3V/k8CTW9+iKQ5kvyO538LV83iK//B/znRGowmnuZw2jmUMTdnKI25hZcgiLeq8nKeIcnFYQ\ndAZi68Cl+vDNd4YXr67p5wlHystflIG/yGGNr2Tyhtezt/B1+fmFgDcCijQa8ScjmEdX1lGbY5xV\nFZlZoyltGlfhVo2tEPIv2Po0LJ4KpsK+FjmgyM3arPY1JF+/rK7U2HJaq3OUPif3nZvuIHfUPN2R\nhz9Mf+wuciOvt+7RW+UEpBEIJpUurGc8b1OffZTlCosZyLBCHxLT9StSmn4Bp+Jg2xPw0/twtTr5\ncYK2gkagKZi8rAnsL3hDnkD7XfMbAWME2vMzPVlJV9ZRj4PcoggbuYe72cKRen8aH3Yrpk+YvnQx\n7B3oW4HzCe5QZL5WbL5SMnn1E/CHloAva/25zd9b/7f8Yrz82ggEk8pEZnAH+xnEYrbSnEPU5U1e\nZqEaAA8/CLfVhYRysHES7BgJKSV8LbYmHX/7NhBo+MNz8wcZnOHrykZ+MQIeGXfqbIPs/QQe4lsR\nkCQKSQrBco6K8hT/J1GcTE+TJjze1DJ+/77xQnBSnseh683xduyYSFRU9unMfgLdu4vs2mUcv/CC\nyJNP2o4bd7b94x8inTtbzuvWtY1fuNAYFx4Wlvnaxx4T6dvXcb4ixj307+94TPZrrxnpTpywvUZE\nJC4uc5iIyLhxIps2Zc7r0CGRXr1EOnQQadnSdsw/iLRta/hMmGWyjvvgA2P//ffG/vnnRTZutOT9\nzjsiixcbx/PnG34G9oBRblacPu34nuyZNctIc+aMJWzfPpFhw0Tq1DHirP0EqlY1ws6fF4mPF2nR\nQqRjx6zLEBHp00fk1CnHcU895VxOEPnjD9swk8kIN5myL9fM6tUiU6a4nv7110WWLTOOq1XL/jnm\nhpgY4725cUPk3nsNfxFDVXtIL3sqY4eF4dwItGWjrOMfco2S8h+elMqclmIkWNKEnRR6jBYmhBvK\nv8NkochVnytJf9iSkmzPR41yX94iIrffbhxbO0+ByPDhRjxYjIAZMIyAme3bM+c9daptOfbYy2Ef\nnpzs/LqICOf5OgNERo7MHKZUzvKxZsQIi7zPPWcbZzZA1nKuWpVzua1l3b8/+3T33pv7MkQsv7W1\nETD/lu7EG0YgL5gNnzfwpBHwaXdQIZJ5jveYieGXvZau1OQ4lykHFXfCPWNBpUH4cai427jok63G\nalxpft2T5VU8vWqSs2Z2Tspxt0yQ9VTN/tyN4Ws88VsURPLLc/SJJg0mlWf4P97jeY5Rk494grG8\nT0rr/0CX8paE+x+Eff3TR/cEw9kWvhBX4wL+NtFYILygnpDRG/ftrWcbCL9hfsDrRkDSV93aThNe\n5RWmMh2KxULfHlDrB/jfDNjzCCSU12P6/Yy8KF1H8Xl9yQOhtu+PMuZVJke/mz/ep6fJL0bKJSOg\nlOoKvI8x6+g8EZlpF18XmA80AV4WkXed5TWBGUSHtGBr2dJQex1EDoBaG+DQA/Baklb8bsBbf05f\nv/iBPl+/o3wDwTD6u/Lz9f8y0MjWCCilgoDZQCfgHBCjlFohIgetkl0BngF6Z5ff210uQOuOxsn2\nUXCiA/wwA+JqkN8XXvcU3hoql9XLn90i5d5WHFoReA5/MQK+/o395TnkFVdaAi2AIyJyEkAptQTo\nBWQYARG5DFxWSvXINrci12D2frhcL3cSa7LFWx+Gc0J+eWHygq+VlhlPdAcVRPLLc3DFCFQGTlud\nn8EwDLlj5bxcX6rxLXlpceSXF0ajf8v8hg9GB02zOm6fvmncifklveMO2L/fOB40CBYtsk1XrJix\nAtWHH2bOo1s3qJW+YNqsWXDihCWuenX46y/L+fDh8OijEBNjCZs2DQZazdxRvz6MGgV168KLLxph\nQ4fChQuGHDlh6lRYvdp5/GOPGfd7+HDO8n3zTehh15b99tush6Jmx7PPQs2a8PLLztO0aWN7PGZM\n7stzhTfegB07cn/9J58Yq5JZr4g1ZAikpORdNmueegoiIx3HjRxp/L+t8XZLa+5cuHzZM3lHR0cT\nHR3tmcztyc6RAGgFrLM6nwhMcJJ2KvB8Fnn53LHKU9uQIe7Ly+zMA4YnbXbpzSsqmbfhw439Rx9Z\nwh3SZEEAAAsySURBVCxOJyJly9o7omTeHPHbb5Y4czm5IasynMkV6IDhBWyN2VnMnWXs2+e+/AKN\ntDTjGaSl+VoS92Ooas84i7lSx4kBaiulqimlCgMDgZVZpPeTnk/vIm5sInu6zza3+ftLn7ZGo3Ef\n2XYHiYhJKfU0sAHLENEDSqnRRrR8opSqAGwDSgFpSqmxwB0icsOTwmu8i71x0UYhZ3hjFJf+TfQz\nyCkufRMQkXVAXbuwOVbHfwNR7hUtsHBnS8BduGO6B41Gk7/RK634IXldAlAref9E/y4af0QbgXyI\nWdm4u1ns6/nbNRqN+9FGwE3404dhjUajcRVtBPIhnmoJaPKG/j00/oielN8NvPkm9OtnOOAcPJh9\n+tzSsyfs3AmnT9uGm5XLggXwv/8ZDlkADz0Ejz9um3b9eihZ0jbs7bdh/HjLeZcujstv1Mi4V4DP\nP4d27XJ1G4wZk9nRxxkzZkDt2rkrx98ZMQIiItyX3+uvWxz8CipTp/paggDEUw4IjjYC0FnMvIRc\nVps1U6bYxlWtKvLEE46vK1PGuKZiRdvwTp0sx9bOYiLGalr2+ZidZG7dMtL861+W9GXLZpbRsTOK\n4/vxJSCyZo2vpXAPYLvSmkaTE/Cxs5gmB4iDsfTZdQOYTFnnkVcZApn8dC8ajT+ijYCHcaUfOC3N\n9fwcKUXd16zRaHKLNgJ+gH1LIK/TOujas3+ijbXGH9FGIBty+uLmRgHbG4GsKGiKRBs0jcazaCPg\nZhx9E8gOd7UEtMLUaDQ5RRsBL5DdHD45aQloNBqNO9FGwMO4+8OwRqPRuJOAcha791745ZecXzd4\nMISEGA5O2fHEE/Dxx5bz9euNFbOuXoWyZWHfPti+HdasgbFjoWVL2+snTDBWbgJjVamnnoLwcGN1\nqhEj4MYNY1WuO+6AokWNdDt2wOLFsGKFkb9S8Mgj0KIF9OkDGzdClSpG2pAQwzHt22/hH/8wHL3M\nODI4GzdCYqKrTyp3z9dTfP013Hefr6VwD2vWGL+nRuN3eMoBwdFGHp3FNm82HKzM5z162MZXqGB7\nHh5u7HftMjtcZHaKsncGs0/n3HlD5MwZx3FVquTO6cq8OlnnztmnBZEXX7Q9NzuLzZiR8/JBpFy5\nnF2j0Wi8A9pZzECpnH38LGgjafKKfl4aTcEj4IyAP+XpqdE4nl5eUqPRaMwEnBHIiYIsaDXbvN5v\nQXteGo0mAI1AVuRmjH4go1cU02g0eSVfGYG8pvc17pJXGwONRuMq2gjkgfymbAPNaGo0mrwTUEYg\nyE7aEDsvB2fdQVkpt8KF8y6Xu/IsVMjYu6qMnXUH2T8XV/HEs9BoNP6NT53FXnoJKlY0nKZKlIAr\nVwwHrJAQSE010uzYAaNHG6tLNW5suXb7doiKgilT4NdfoU0b6NbNiKtXDw4cMJTkhg3QoIERvnAh\nlCljSQfwww9w/Trs3m27gtWqVVC9unPZ16+3OHDZ8/PPkJCQ48fBv/4F8+bl/DqzPGbns6eeglat\ncnb9gQNQrFjuytZoNIGLEi/2aSilBISiReHWLWM5xKFD7dPAtGnG1q6d4fFqTdmyhrFwJHaFCnDx\nInTvDqtXQ2QknDuXOV2xYkb5zm5dKTh7FipVys1d5g2loGtXWLs2+3Tjx8PMmd6RS6PR+A6lFCLi\nkQ5bn3QH2Xfr5ISsukrMSt2VbiCNRqPR+MgIZKecs1ocxVsew9qAaDSagkDAtQRygj96A7tCbj8M\nazQaTU7xSyOQ2+6cguYsptFoNHnFp91BzpR0Vt1B3lLs2oBoNJqCgF9+Ewi0ctyN7g7SaDTewidG\noEwZY+/MOck8Xr1UqcxxpUs7z9ccZ77OmZIMD4ciRbKW0VvfLRxRsqRr6fS4fo1Gk1e87iewdatQ\nq5bhpPXQQxAcbJvmzz/h9tvh9GmIiDA2a/7+G5KSoGrVzPlfuADJyRAWZhiE6tXhxInM6c6fN9b1\ndebsFRMDzZvn6hbzzOHDhr9DWFjW6fbsgTp1LA5iGo0m/+JJPwGvGwFvlacU1KgBx497pTiNRqPx\nGPnOWcxb6D5zjUajyRptBDQajaYA45IRUEp1VUodVEodVkpNcJJmllLqiFJql1LqLveKmTu0EdBo\nNJqsydYIKKWCgNlAF6A+8LBS6na7NN2AWiJSBxgNfOwBWXOMPxuB6OhoX4vgN+hnYUE/Cwv6WXgH\nV1oCLYAjInJSRFKAJUAvuzS9gC8AROQPIEwpVcGtkuYCbQQCA/0sLOhnYUE/C+/gihGoDJy2Oj+T\nHpZVmrMO0ngdfzYCGo1G4w/k6w/DkZG+lkCj0Wj8m2z9BJRSrYBpItI1/XwiICIy0yrNx8DPIvJ1\n+vlB4F4R+dsur3y2Kq9Go9F4B0/5CbiyvGQMUFspVQ04DwwEHrZLsxJ4Cvg63WhctTcA4Lmb0Gg0\nGk3uyNYIiIhJKfU0sAGj+2ieiBxQSv1/e2cTYlUZxvHfP3W0NEddmNSUY4RIm2QkiyyCJiwMpE0U\nROmsok3hIlM3bWsR6aIWUpnZh5UV3kBIZFZB9oEOo+MgyhCZMiOhDdgiUJ4W73On0+THpc6c273v\n84ML73nuee89//898Jz3PO977nPpbdtuZvskrZF0Evgd6Jvaww6CIAjKoNLHRgRBEAT/LyorDDey\n4KyVkdQlqV/SkKQjkl7w+HxJ+yUdl/S1pM5Cn82+wG5Y0upCvEfSoHu1tRl6ykDSdZIOSar5dpZe\nSOqU9JlrG5J0T8ZebJB01HV8KKkjFy8kvSNpTNJgIVaadvdyt/f5VtJlHrN5Gcxsyl+kZHMSWAzM\nAAaAZVV8d1UvYBGw3NtzgOPAMuA1YKPHXwZe9fadwGHSLblu96c+MvsOuNvb+4BHmq3vX3qyAfgA\nqPl2ll4A7wF93p4OdOboBXAzMAJ0+PYnwLpcvADuB5YDg4VYadqB54G3vP0ksLuR46pqJNDIgrOW\nxsxGzWzA2xeAYaCLpHOn77YTeNzba0k/0kUz+wk4AayUtAi40cx+8P3eL/RpGSR1AWuAtwvh7LyQ\nNBd4wMx2ALjGcTL0wpkGzJY0HbietKYoCy/M7Bvg/KRwmdqLn7UH6G3kuKpKAo0sOGsbJHWTMv5B\n4CbzmVJmNgos9N2utMDuFpI/dVrVqzeAl4Bi0SlHL5YAv0ra4bfGtku6gQy9MLMzwOvAzyRd42Z2\ngAy9KLCwRO0TfczsEvCbpEn/yPJP2nqxWDOQNIeUhV/0EcHkynvbV+IlPQaM+cjoatOC294L0nC+\nB3jTzHpIs+c2ked5MY90tbqYdGtotqSnydCLq1Cm9oam5FeVBE4DxSJFl8faCh/i7gF2mdleD4/V\nn6PkQ7mzHj8N3FroXvfkSvFWYhWwVtII8DHwkKRdwGiGXvwCnDKzH337c1JSyPG8eBgYMbNzfqX6\nJXAfeXpRp0ztE+9JmgbMNbNz1zqAqpLAxIIzSR2kBWe1ir67St4FjpnZtkKsBqz39jpgbyH+lFf0\nlwB3AN/7kHBc0kpJAp4t9GkJzGyLmd1mZreTfut+M3sG+Ir8vBgDTkla6qFeYIgMzwvSbaB7Jc1y\nDb3AMfLyQvz9Cr1M7TX/DIAngP6GjqjCyvijpBkzJ4BNzajOT7G+VcAl0synw8Ah17wAOODa9wPz\nCn02k6r+w8DqQnwFcMS92tZsbf/Rlwf5a3ZQll4Ad5EuhAaAL0izg3L14hXXNUgqYs7IxQvgI+AM\n8AcpIfYB88vSDswEPvX4QaC7keOKxWJBEAQZE4XhIAiCjIkkEARBkDGRBIIgCDImkkAQBEHGRBII\ngiDImEgCQRAEGRNJIAiCIGMiCQRBEGTMnybYBbaBAvZoAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11889f240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
