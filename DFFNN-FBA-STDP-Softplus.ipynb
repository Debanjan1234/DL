{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "# X_train.shape, X_train.dtype, X_val.shape, X_val.dtype, X_test.shape, X_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # number of layers or depth\n",
    "        self.losses = {'train':[], 'valid':[], 'valid_acc':[]}\n",
    "        \n",
    "        # The adaptive/learnable/updatable random feedforward\n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.ys_prev = []\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        # Previous output layer\n",
    "        ys_prev_L = []\n",
    "        for _ in range(L):\n",
    "            ys_prev_L.append(0.0)\n",
    "        self.ys_prev.append(ys_prev_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Outout layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        # Previous output layer\n",
    "        self.ys_prev.append(0.0)\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        # dX = dout @ W.T # Backprop\n",
    "        dX = dout @ W_fixed.T # fb alignment\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "#         y, _ = l.selu_forward(X=y)\n",
    "#         y = l.elu_fwd(X=y)\n",
    "#         y = l.sigmoid(X=y)\n",
    "#         y = np.tanh(y)\n",
    "#         y, _ = l.relu_forward(X=y)\n",
    "        y, _ = l.softplus_forward(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[0]\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, ys_L, do_caches = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "#             y, _ = l.selu_forward(X=y)\n",
    "#             y = l.elu_fwd(X=y)\n",
    "#             y = l.sigmoid(X=y)\n",
    "#             y = np.tanh(y)\n",
    "#             y, _ = l.relu_forward(X=y)\n",
    "            y, _ = l.softplus_forward(X=y)\n",
    "            fc_caches.append(fc_cache)\n",
    "            ys_L.append(y) # ys[1][layer]\n",
    "            X = y.copy() # pass to next layer\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]\n",
    "        ys.append(ys_L) # ys[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        ys.append(y) # ys[2]\n",
    "\n",
    "        return ys, caches # for backpropating the error\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        \n",
    "        loss = cross_entropy(y, y_train) # softmax is included\n",
    "        dy = dcross_entropy(y, y_train) # dsoftmax is included\n",
    "        \n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads # initialized by Zero in every iteration/epoch\n",
    "        ys, ys_prev = self.ys, self.ys_prev # temporal diff instead of differentiable function\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy *= ys[1][layer] - ys_prev[1][layer] # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy *= ys[0] - ys_prev[0] # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        ys_logit, _ = self.train_forward(X, train=False)\n",
    "        y_logit = ys_logit[2] # last layer\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_prob = l.softmax(y_logit) # for accuracy == acc\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_logit\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            self.ys, caches = self.train_forward(X_mini, train=True)\n",
    "            loss, dy = self.loss_function(self.ys[2], y_mini)\n",
    "            _, grads = self.train_backward(dy, caches) # self.ys_prev is used here for dfunc/ diff\n",
    "            self.ys_prev = self.ys # for next iteration or epoch\n",
    "            self.losses['train'].append(loss)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "                \n",
    "            # Validate the updated model\n",
    "            y_pred, y_logit = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_logit, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} train loss: {:.4f} valid loss: {:.4f}, valid accuracy: {:.4f}'.format(\n",
    "                    iter, loss, valid_loss, valid_acc))\n",
    "\n",
    "        # Test the final model\n",
    "        y_pred, y_logit = nn.test(X_test)\n",
    "        loss, _ = self.loss_function(y_logit, y_test) # softmax is included in entropy loss function\n",
    "        acc = np.mean(y_pred == y_test)\n",
    "        print('Last iteration - Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.format(\n",
    "            acc.mean(), acc.std(), loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-100 train loss: 2.7380 valid loss: 2.6180, valid accuracy: 0.1100\n",
      "Iter-200 train loss: 2.4573 valid loss: 2.4858, valid accuracy: 0.1100\n",
      "Iter-300 train loss: 2.5434 valid loss: 2.4139, valid accuracy: 0.1100\n",
      "Iter-400 train loss: 2.4046 valid loss: 2.3706, valid accuracy: 0.1100\n",
      "Iter-500 train loss: 2.3673 valid loss: 2.3462, valid accuracy: 0.1100\n",
      "Iter-600 train loss: 2.3359 valid loss: 2.3302, valid accuracy: 0.1100\n",
      "Iter-700 train loss: 2.3859 valid loss: 2.3194, valid accuracy: 0.1100\n",
      "Iter-800 train loss: 2.3281 valid loss: 2.3132, valid accuracy: 0.1100\n",
      "Iter-900 train loss: 2.2849 valid loss: 2.3085, valid accuracy: 0.1100\n",
      "Iter-1000 train loss: 2.2875 valid loss: 2.3050, valid accuracy: 0.1100\n",
      "Iter-1100 train loss: 2.2907 valid loss: 2.3029, valid accuracy: 0.1092\n",
      "Iter-1200 train loss: 2.3093 valid loss: 2.3015, valid accuracy: 0.0936\n",
      "Iter-1300 train loss: 2.2892 valid loss: 2.3007, valid accuracy: 0.1154\n",
      "Iter-1400 train loss: 2.2897 valid loss: 2.3002, valid accuracy: 0.1154\n",
      "Iter-1500 train loss: 2.2986 valid loss: 2.2998, valid accuracy: 0.1138\n",
      "Iter-1600 train loss: 2.2879 valid loss: 2.2998, valid accuracy: 0.1126\n",
      "Iter-1700 train loss: 2.2911 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-1800 train loss: 2.3088 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-1900 train loss: 2.3059 valid loss: 2.2994, valid accuracy: 0.1138\n",
      "Iter-2000 train loss: 2.3198 valid loss: 2.2993, valid accuracy: 0.1136\n",
      "Iter-2100 train loss: 2.3054 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-2200 train loss: 2.2920 valid loss: 2.2997, valid accuracy: 0.1126\n",
      "Iter-2300 train loss: 2.2916 valid loss: 2.2996, valid accuracy: 0.1126\n",
      "Iter-2400 train loss: 2.2919 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-2500 train loss: 2.2859 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-2600 train loss: 2.3020 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-2700 train loss: 2.2923 valid loss: 2.2996, valid accuracy: 0.1126\n",
      "Iter-2800 train loss: 2.2826 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-2900 train loss: 2.2985 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-3000 train loss: 2.3002 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-3100 train loss: 2.3028 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-3200 train loss: 2.2854 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-3300 train loss: 2.2942 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-3400 train loss: 2.2977 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-3500 train loss: 2.2857 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-3600 train loss: 2.3009 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-3700 train loss: 2.2998 valid loss: 2.2991, valid accuracy: 0.1126\n",
      "Iter-3800 train loss: 2.2992 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-3900 train loss: 2.2965 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-4000 train loss: 2.2953 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-4100 train loss: 2.3076 valid loss: 2.2995, valid accuracy: 0.1126\n",
      "Iter-4200 train loss: 2.3134 valid loss: 2.2994, valid accuracy: 0.1128\n",
      "Iter-4300 train loss: 2.2998 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-4400 train loss: 2.2811 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-4500 train loss: 2.3031 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-4600 train loss: 2.3088 valid loss: 2.2994, valid accuracy: 0.1126\n",
      "Iter-4700 train loss: 2.3032 valid loss: 2.2992, valid accuracy: 0.1126\n",
      "Iter-4800 train loss: 2.2966 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-4900 train loss: 2.2973 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-5000 train loss: 2.3030 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-5100 train loss: 2.3100 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-5200 train loss: 2.3116 valid loss: 2.2993, valid accuracy: 0.1126\n",
      "Iter-5300 train loss: 2.3108 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-5400 train loss: 2.3145 valid loss: 2.2987, valid accuracy: 0.1126\n",
      "Iter-5500 train loss: 2.2994 valid loss: 2.2987, valid accuracy: 0.1126\n",
      "Iter-5600 train loss: 2.2992 valid loss: 2.2987, valid accuracy: 0.1126\n",
      "Iter-5700 train loss: 2.3007 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-5800 train loss: 2.3004 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-5900 train loss: 2.2957 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-6000 train loss: 2.3084 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-6100 train loss: 2.3061 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-6200 train loss: 2.3056 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-6300 train loss: 2.2904 valid loss: 2.2990, valid accuracy: 0.1126\n",
      "Iter-6400 train loss: 2.2973 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-6500 train loss: 2.2942 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-6600 train loss: 2.3031 valid loss: 2.2986, valid accuracy: 0.1126\n",
      "Iter-6700 train loss: 2.2999 valid loss: 2.2986, valid accuracy: 0.1126\n",
      "Iter-6800 train loss: 2.2952 valid loss: 2.2986, valid accuracy: 0.1126\n",
      "Iter-6900 train loss: 2.3063 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-7000 train loss: 2.2920 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-7100 train loss: 2.2981 valid loss: 2.2989, valid accuracy: 0.1126\n",
      "Iter-7200 train loss: 2.2990 valid loss: 2.2987, valid accuracy: 0.1126\n",
      "Iter-7300 train loss: 2.3005 valid loss: 2.2987, valid accuracy: 0.1126\n",
      "Iter-7400 train loss: 2.2834 valid loss: 2.2988, valid accuracy: 0.1126\n",
      "Iter-7500 train loss: 2.2878 valid loss: 2.2985, valid accuracy: 0.1126\n",
      "Iter-7600 train loss: 2.2989 valid loss: 2.2983, valid accuracy: 0.1126\n",
      "Iter-7700 train loss: 2.3156 valid loss: 2.2983, valid accuracy: 0.1126\n",
      "Iter-7800 train loss: 2.3038 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-7900 train loss: 2.3002 valid loss: 2.2984, valid accuracy: 0.1126\n",
      "Iter-8000 train loss: 2.3068 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-8100 train loss: 2.3106 valid loss: 2.2981, valid accuracy: 0.1126\n",
      "Iter-8200 train loss: 2.3004 valid loss: 2.2981, valid accuracy: 0.1126\n",
      "Iter-8300 train loss: 2.2966 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-8400 train loss: 2.3037 valid loss: 2.2981, valid accuracy: 0.1126\n",
      "Iter-8500 train loss: 2.3187 valid loss: 2.2980, valid accuracy: 0.1126\n",
      "Iter-8600 train loss: 2.3062 valid loss: 2.2983, valid accuracy: 0.1126\n",
      "Iter-8700 train loss: 2.2887 valid loss: 2.2986, valid accuracy: 0.1126\n",
      "Iter-8800 train loss: 2.2972 valid loss: 2.2985, valid accuracy: 0.1126\n",
      "Iter-8900 train loss: 2.2945 valid loss: 2.2980, valid accuracy: 0.1126\n",
      "Iter-9000 train loss: 2.2904 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-9100 train loss: 2.2947 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-9200 train loss: 2.3065 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-9300 train loss: 2.3076 valid loss: 2.2983, valid accuracy: 0.1126\n",
      "Iter-9400 train loss: 2.2838 valid loss: 2.2982, valid accuracy: 0.1126\n",
      "Iter-9500 train loss: 2.2958 valid loss: 2.2981, valid accuracy: 0.1126\n",
      "Iter-9600 train loss: 2.3029 valid loss: 2.2981, valid accuracy: 0.1126\n",
      "Iter-9700 train loss: 2.2860 valid loss: 2.2981, valid accuracy: 0.1126\n",
      "Iter-9800 train loss: 2.2963 valid loss: 2.2979, valid accuracy: 0.1126\n",
      "Iter-9900 train loss: 2.2863 valid loss: 2.2977, valid accuracy: 0.1126\n",
      "Iter-10000 train loss: 2.2997 valid loss: 2.2978, valid accuracy: 0.1126\n",
      "Last iteration - Test accuracy mean: 0.1135, std: 0.0000, loss: 2.2980\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 100 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZE8piAgIpLH/j9LTGGzxyS9RQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20ddaf1e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEACAYAAABPiSrXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20ddaf1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
