{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # vanilla Backprop\n",
    "#         dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.3335, acc-0.0200, valid loss-2.3323, acc-0.0604, test loss-2.3323, acc-0.0637\n",
      "Iter-20, train loss-2.3206, acc-0.0400, valid loss-2.3297, acc-0.0636, test loss-2.3295, acc-0.0660\n",
      "Iter-30, train loss-2.3172, acc-0.0400, valid loss-2.3269, acc-0.0664, test loss-2.3267, acc-0.0679\n",
      "Iter-40, train loss-2.3234, acc-0.0800, valid loss-2.3241, acc-0.0690, test loss-2.3237, acc-0.0708\n",
      "Iter-50, train loss-2.3217, acc-0.0600, valid loss-2.3214, acc-0.0708, test loss-2.3210, acc-0.0733\n",
      "Iter-60, train loss-2.3392, acc-0.0400, valid loss-2.3184, acc-0.0728, test loss-2.3179, acc-0.0768\n",
      "Iter-70, train loss-2.3430, acc-0.0000, valid loss-2.3158, acc-0.0756, test loss-2.3151, acc-0.0796\n",
      "Iter-80, train loss-2.3152, acc-0.0800, valid loss-2.3130, acc-0.0780, test loss-2.3122, acc-0.0833\n",
      "Iter-90, train loss-2.2648, acc-0.2200, valid loss-2.3103, acc-0.0810, test loss-2.3093, acc-0.0860\n",
      "Iter-100, train loss-2.3023, acc-0.0800, valid loss-2.3075, acc-0.0842, test loss-2.3065, acc-0.0888\n",
      "Iter-110, train loss-2.3114, acc-0.0800, valid loss-2.3047, acc-0.0862, test loss-2.3035, acc-0.0921\n",
      "Iter-120, train loss-2.3113, acc-0.1200, valid loss-2.3020, acc-0.0888, test loss-2.3007, acc-0.0947\n",
      "Iter-130, train loss-2.2808, acc-0.1400, valid loss-2.2993, acc-0.0910, test loss-2.2979, acc-0.0987\n",
      "Iter-140, train loss-2.2991, acc-0.0600, valid loss-2.2966, acc-0.0948, test loss-2.2951, acc-0.1023\n",
      "Iter-150, train loss-2.3008, acc-0.0600, valid loss-2.2939, acc-0.0968, test loss-2.2923, acc-0.1067\n",
      "Iter-160, train loss-2.2879, acc-0.0800, valid loss-2.2911, acc-0.1018, test loss-2.2894, acc-0.1110\n",
      "Iter-170, train loss-2.2813, acc-0.0400, valid loss-2.2885, acc-0.1044, test loss-2.2867, acc-0.1152\n",
      "Iter-180, train loss-2.2727, acc-0.1600, valid loss-2.2857, acc-0.1078, test loss-2.2838, acc-0.1192\n",
      "Iter-190, train loss-2.2622, acc-0.1400, valid loss-2.2831, acc-0.1122, test loss-2.2812, acc-0.1230\n",
      "Iter-200, train loss-2.2785, acc-0.0200, valid loss-2.2805, acc-0.1172, test loss-2.2785, acc-0.1275\n",
      "Iter-210, train loss-2.2791, acc-0.0800, valid loss-2.2781, acc-0.1208, test loss-2.2759, acc-0.1302\n",
      "Iter-220, train loss-2.2578, acc-0.0800, valid loss-2.2753, acc-0.1268, test loss-2.2730, acc-0.1344\n",
      "Iter-230, train loss-2.2328, acc-0.1000, valid loss-2.2726, acc-0.1312, test loss-2.2702, acc-0.1395\n",
      "Iter-240, train loss-2.2719, acc-0.1200, valid loss-2.2700, acc-0.1362, test loss-2.2675, acc-0.1448\n",
      "Iter-250, train loss-2.2644, acc-0.1400, valid loss-2.2673, acc-0.1388, test loss-2.2646, acc-0.1500\n",
      "Iter-260, train loss-2.2361, acc-0.2400, valid loss-2.2646, acc-0.1428, test loss-2.2620, acc-0.1536\n",
      "Iter-270, train loss-2.2722, acc-0.0800, valid loss-2.2621, acc-0.1476, test loss-2.2593, acc-0.1588\n",
      "Iter-280, train loss-2.2362, acc-0.2000, valid loss-2.2596, acc-0.1530, test loss-2.2567, acc-0.1640\n",
      "Iter-290, train loss-2.2451, acc-0.2800, valid loss-2.2569, acc-0.1594, test loss-2.2539, acc-0.1716\n",
      "Iter-300, train loss-2.2641, acc-0.1400, valid loss-2.2542, acc-0.1658, test loss-2.2511, acc-0.1781\n",
      "Iter-310, train loss-2.2213, acc-0.2200, valid loss-2.2516, acc-0.1706, test loss-2.2484, acc-0.1843\n",
      "Iter-320, train loss-2.2547, acc-0.2200, valid loss-2.2490, acc-0.1766, test loss-2.2456, acc-0.1916\n",
      "Iter-330, train loss-2.2369, acc-0.2200, valid loss-2.2463, acc-0.1806, test loss-2.2429, acc-0.1972\n",
      "Iter-340, train loss-2.2265, acc-0.2800, valid loss-2.2436, acc-0.1860, test loss-2.2401, acc-0.2042\n",
      "Iter-350, train loss-2.2417, acc-0.1400, valid loss-2.2409, acc-0.1952, test loss-2.2373, acc-0.2124\n",
      "Iter-360, train loss-2.2395, acc-0.1800, valid loss-2.2383, acc-0.2018, test loss-2.2345, acc-0.2208\n",
      "Iter-370, train loss-2.2432, acc-0.1600, valid loss-2.2355, acc-0.2102, test loss-2.2317, acc-0.2263\n",
      "Iter-380, train loss-2.2238, acc-0.2200, valid loss-2.2328, acc-0.2170, test loss-2.2289, acc-0.2344\n",
      "Iter-390, train loss-2.2321, acc-0.3400, valid loss-2.2300, acc-0.2238, test loss-2.2260, acc-0.2438\n",
      "Iter-400, train loss-2.2694, acc-0.1400, valid loss-2.2276, acc-0.2314, test loss-2.2234, acc-0.2517\n",
      "Iter-410, train loss-2.2249, acc-0.2400, valid loss-2.2250, acc-0.2394, test loss-2.2206, acc-0.2605\n",
      "Iter-420, train loss-2.2267, acc-0.2800, valid loss-2.2224, acc-0.2506, test loss-2.2180, acc-0.2701\n",
      "Iter-430, train loss-2.2509, acc-0.1800, valid loss-2.2198, acc-0.2588, test loss-2.2153, acc-0.2778\n",
      "Iter-440, train loss-2.1841, acc-0.4200, valid loss-2.2171, acc-0.2662, test loss-2.2125, acc-0.2884\n",
      "Iter-450, train loss-2.1995, acc-0.3600, valid loss-2.2146, acc-0.2726, test loss-2.2099, acc-0.2962\n",
      "Iter-460, train loss-2.2097, acc-0.3200, valid loss-2.2120, acc-0.2766, test loss-2.2072, acc-0.3041\n",
      "Iter-470, train loss-2.1888, acc-0.3200, valid loss-2.2095, acc-0.2840, test loss-2.2045, acc-0.3108\n",
      "Iter-480, train loss-2.2440, acc-0.2000, valid loss-2.2069, acc-0.2930, test loss-2.2019, acc-0.3178\n",
      "Iter-490, train loss-2.1882, acc-0.4000, valid loss-2.2044, acc-0.3000, test loss-2.1992, acc-0.3233\n",
      "Iter-500, train loss-2.1926, acc-0.3600, valid loss-2.2019, acc-0.3076, test loss-2.1966, acc-0.3296\n",
      "Iter-510, train loss-2.2047, acc-0.3200, valid loss-2.1994, acc-0.3156, test loss-2.1940, acc-0.3356\n",
      "Iter-520, train loss-2.1862, acc-0.3400, valid loss-2.1968, acc-0.3216, test loss-2.1913, acc-0.3420\n",
      "Iter-530, train loss-2.1916, acc-0.4000, valid loss-2.1942, acc-0.3278, test loss-2.1886, acc-0.3492\n",
      "Iter-540, train loss-2.1978, acc-0.3200, valid loss-2.1916, acc-0.3336, test loss-2.1860, acc-0.3546\n",
      "Iter-550, train loss-2.1901, acc-0.4000, valid loss-2.1889, acc-0.3386, test loss-2.1833, acc-0.3605\n",
      "Iter-560, train loss-2.1904, acc-0.3400, valid loss-2.1864, acc-0.3460, test loss-2.1806, acc-0.3685\n",
      "Iter-570, train loss-2.2059, acc-0.3200, valid loss-2.1839, acc-0.3500, test loss-2.1781, acc-0.3742\n",
      "Iter-580, train loss-2.1753, acc-0.4000, valid loss-2.1814, acc-0.3560, test loss-2.1755, acc-0.3789\n",
      "Iter-590, train loss-2.1893, acc-0.3600, valid loss-2.1788, acc-0.3592, test loss-2.1729, acc-0.3850\n",
      "Iter-600, train loss-2.1523, acc-0.4200, valid loss-2.1761, acc-0.3650, test loss-2.1701, acc-0.3910\n",
      "Iter-610, train loss-2.1731, acc-0.4400, valid loss-2.1736, acc-0.3710, test loss-2.1675, acc-0.3958\n",
      "Iter-620, train loss-2.1506, acc-0.4800, valid loss-2.1711, acc-0.3766, test loss-2.1649, acc-0.4009\n",
      "Iter-630, train loss-2.1737, acc-0.4000, valid loss-2.1686, acc-0.3812, test loss-2.1624, acc-0.4057\n",
      "Iter-640, train loss-2.1665, acc-0.3000, valid loss-2.1661, acc-0.3856, test loss-2.1597, acc-0.4114\n",
      "Iter-650, train loss-2.1571, acc-0.4400, valid loss-2.1636, acc-0.3896, test loss-2.1571, acc-0.4147\n",
      "Iter-660, train loss-2.1409, acc-0.6200, valid loss-2.1610, acc-0.3936, test loss-2.1545, acc-0.4190\n",
      "Iter-670, train loss-2.1616, acc-0.3600, valid loss-2.1587, acc-0.3962, test loss-2.1521, acc-0.4227\n",
      "Iter-680, train loss-2.1692, acc-0.3400, valid loss-2.1560, acc-0.4002, test loss-2.1493, acc-0.4273\n",
      "Iter-690, train loss-2.1690, acc-0.4600, valid loss-2.1535, acc-0.4036, test loss-2.1467, acc-0.4306\n",
      "Iter-700, train loss-2.1607, acc-0.4200, valid loss-2.1510, acc-0.4056, test loss-2.1442, acc-0.4345\n",
      "Iter-710, train loss-2.1708, acc-0.2800, valid loss-2.1487, acc-0.4110, test loss-2.1418, acc-0.4380\n",
      "Iter-720, train loss-2.1446, acc-0.3600, valid loss-2.1462, acc-0.4154, test loss-2.1392, acc-0.4417\n",
      "Iter-730, train loss-2.1222, acc-0.4400, valid loss-2.1436, acc-0.4204, test loss-2.1365, acc-0.4452\n",
      "Iter-740, train loss-2.1206, acc-0.5200, valid loss-2.1410, acc-0.4252, test loss-2.1339, acc-0.4494\n",
      "Iter-750, train loss-2.1613, acc-0.3600, valid loss-2.1385, acc-0.4282, test loss-2.1314, acc-0.4532\n",
      "Iter-760, train loss-2.1370, acc-0.4600, valid loss-2.1359, acc-0.4330, test loss-2.1286, acc-0.4567\n",
      "Iter-770, train loss-2.1493, acc-0.4600, valid loss-2.1333, acc-0.4358, test loss-2.1259, acc-0.4591\n",
      "Iter-780, train loss-2.1191, acc-0.4200, valid loss-2.1308, acc-0.4390, test loss-2.1233, acc-0.4611\n",
      "Iter-790, train loss-2.1149, acc-0.5000, valid loss-2.1283, acc-0.4412, test loss-2.1207, acc-0.4633\n",
      "Iter-800, train loss-2.1185, acc-0.4200, valid loss-2.1260, acc-0.4442, test loss-2.1183, acc-0.4656\n",
      "Iter-810, train loss-2.0819, acc-0.5000, valid loss-2.1232, acc-0.4482, test loss-2.1155, acc-0.4692\n",
      "Iter-820, train loss-2.1128, acc-0.4800, valid loss-2.1207, acc-0.4498, test loss-2.1128, acc-0.4717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.1227, acc-0.4600, valid loss-2.1180, acc-0.4524, test loss-2.1101, acc-0.4733\n",
      "Iter-840, train loss-2.1561, acc-0.4000, valid loss-2.1156, acc-0.4560, test loss-2.1076, acc-0.4750\n",
      "Iter-850, train loss-2.0905, acc-0.5000, valid loss-2.1130, acc-0.4588, test loss-2.1049, acc-0.4776\n",
      "Iter-860, train loss-2.0961, acc-0.5400, valid loss-2.1106, acc-0.4620, test loss-2.1024, acc-0.4798\n",
      "Iter-870, train loss-2.1269, acc-0.4400, valid loss-2.1080, acc-0.4652, test loss-2.0998, acc-0.4825\n",
      "Iter-880, train loss-2.1107, acc-0.4400, valid loss-2.1053, acc-0.4694, test loss-2.0971, acc-0.4846\n",
      "Iter-890, train loss-2.0969, acc-0.4400, valid loss-2.1028, acc-0.4710, test loss-2.0944, acc-0.4863\n",
      "Iter-900, train loss-2.1438, acc-0.3400, valid loss-2.1003, acc-0.4726, test loss-2.0918, acc-0.4886\n",
      "Iter-910, train loss-2.1117, acc-0.4000, valid loss-2.0978, acc-0.4762, test loss-2.0892, acc-0.4910\n",
      "Iter-920, train loss-2.0459, acc-0.5800, valid loss-2.0952, acc-0.4786, test loss-2.0865, acc-0.4932\n",
      "Iter-930, train loss-2.1061, acc-0.4600, valid loss-2.0925, acc-0.4812, test loss-2.0838, acc-0.4957\n",
      "Iter-940, train loss-2.0780, acc-0.4600, valid loss-2.0900, acc-0.4840, test loss-2.0812, acc-0.4972\n",
      "Iter-950, train loss-2.1220, acc-0.4400, valid loss-2.0875, acc-0.4884, test loss-2.0786, acc-0.4989\n",
      "Iter-960, train loss-2.0911, acc-0.5400, valid loss-2.0850, acc-0.4904, test loss-2.0762, acc-0.5006\n",
      "Iter-970, train loss-2.0760, acc-0.5200, valid loss-2.0825, acc-0.4928, test loss-2.0735, acc-0.5022\n",
      "Iter-980, train loss-2.0860, acc-0.5400, valid loss-2.0799, acc-0.4946, test loss-2.0710, acc-0.5032\n",
      "Iter-990, train loss-2.0929, acc-0.4400, valid loss-2.0773, acc-0.4962, test loss-2.0682, acc-0.5048\n",
      "Iter-1000, train loss-2.0774, acc-0.5200, valid loss-2.0747, acc-0.4974, test loss-2.0655, acc-0.5056\n",
      "Iter-1010, train loss-2.0633, acc-0.5400, valid loss-2.0721, acc-0.4998, test loss-2.0629, acc-0.5074\n",
      "Iter-1020, train loss-2.0321, acc-0.6200, valid loss-2.0696, acc-0.5022, test loss-2.0603, acc-0.5092\n",
      "Iter-1030, train loss-2.0634, acc-0.5800, valid loss-2.0671, acc-0.5036, test loss-2.0577, acc-0.5104\n",
      "Iter-1040, train loss-2.0275, acc-0.7000, valid loss-2.0645, acc-0.5042, test loss-2.0551, acc-0.5113\n",
      "Iter-1050, train loss-2.0869, acc-0.4600, valid loss-2.0619, acc-0.5054, test loss-2.0524, acc-0.5122\n",
      "Iter-1060, train loss-2.0791, acc-0.4200, valid loss-2.0592, acc-0.5064, test loss-2.0497, acc-0.5137\n",
      "Iter-1070, train loss-2.0942, acc-0.4800, valid loss-2.0567, acc-0.5084, test loss-2.0471, acc-0.5158\n",
      "Iter-1080, train loss-2.0734, acc-0.4400, valid loss-2.0541, acc-0.5096, test loss-2.0445, acc-0.5165\n",
      "Iter-1090, train loss-2.0552, acc-0.5400, valid loss-2.0517, acc-0.5128, test loss-2.0420, acc-0.5179\n",
      "Iter-1100, train loss-2.0512, acc-0.4800, valid loss-2.0492, acc-0.5128, test loss-2.0394, acc-0.5194\n",
      "Iter-1110, train loss-2.0851, acc-0.5000, valid loss-2.0467, acc-0.5132, test loss-2.0369, acc-0.5208\n",
      "Iter-1120, train loss-2.0491, acc-0.5400, valid loss-2.0440, acc-0.5148, test loss-2.0342, acc-0.5216\n",
      "Iter-1130, train loss-2.0262, acc-0.5600, valid loss-2.0414, acc-0.5162, test loss-2.0314, acc-0.5231\n",
      "Iter-1140, train loss-2.0333, acc-0.4600, valid loss-2.0388, acc-0.5162, test loss-2.0288, acc-0.5236\n",
      "Iter-1150, train loss-2.0437, acc-0.5400, valid loss-2.0362, acc-0.5188, test loss-2.0262, acc-0.5250\n",
      "Iter-1160, train loss-2.0858, acc-0.4600, valid loss-2.0337, acc-0.5202, test loss-2.0236, acc-0.5267\n",
      "Iter-1170, train loss-2.0353, acc-0.3800, valid loss-2.0311, acc-0.5218, test loss-2.0209, acc-0.5268\n",
      "Iter-1180, train loss-2.0652, acc-0.4000, valid loss-2.0286, acc-0.5240, test loss-2.0183, acc-0.5287\n",
      "Iter-1190, train loss-2.0966, acc-0.3800, valid loss-2.0260, acc-0.5246, test loss-2.0156, acc-0.5302\n",
      "Iter-1200, train loss-2.0504, acc-0.4600, valid loss-2.0234, acc-0.5250, test loss-2.0131, acc-0.5313\n",
      "Iter-1210, train loss-1.9738, acc-0.5800, valid loss-2.0208, acc-0.5260, test loss-2.0104, acc-0.5333\n",
      "Iter-1220, train loss-2.0031, acc-0.5800, valid loss-2.0183, acc-0.5266, test loss-2.0077, acc-0.5339\n",
      "Iter-1230, train loss-1.9985, acc-0.5200, valid loss-2.0158, acc-0.5270, test loss-2.0052, acc-0.5344\n",
      "Iter-1240, train loss-2.0218, acc-0.5200, valid loss-2.0132, acc-0.5284, test loss-2.0025, acc-0.5357\n",
      "Iter-1250, train loss-2.0236, acc-0.4600, valid loss-2.0106, acc-0.5286, test loss-1.9999, acc-0.5366\n",
      "Iter-1260, train loss-1.9691, acc-0.6000, valid loss-2.0082, acc-0.5302, test loss-1.9974, acc-0.5372\n",
      "Iter-1270, train loss-2.0621, acc-0.4600, valid loss-2.0056, acc-0.5316, test loss-1.9948, acc-0.5380\n",
      "Iter-1280, train loss-1.9671, acc-0.6200, valid loss-2.0029, acc-0.5312, test loss-1.9919, acc-0.5379\n",
      "Iter-1290, train loss-1.9782, acc-0.5600, valid loss-2.0003, acc-0.5324, test loss-1.9893, acc-0.5396\n",
      "Iter-1300, train loss-1.9316, acc-0.6200, valid loss-1.9978, acc-0.5338, test loss-1.9867, acc-0.5408\n",
      "Iter-1310, train loss-1.9547, acc-0.5800, valid loss-1.9951, acc-0.5358, test loss-1.9840, acc-0.5414\n",
      "Iter-1320, train loss-1.9854, acc-0.5600, valid loss-1.9925, acc-0.5354, test loss-1.9813, acc-0.5421\n",
      "Iter-1330, train loss-1.9970, acc-0.5200, valid loss-1.9900, acc-0.5362, test loss-1.9787, acc-0.5427\n",
      "Iter-1340, train loss-1.9651, acc-0.5600, valid loss-1.9874, acc-0.5366, test loss-1.9761, acc-0.5438\n",
      "Iter-1350, train loss-2.0308, acc-0.4800, valid loss-1.9849, acc-0.5386, test loss-1.9735, acc-0.5453\n",
      "Iter-1360, train loss-1.9572, acc-0.5000, valid loss-1.9824, acc-0.5390, test loss-1.9710, acc-0.5464\n",
      "Iter-1370, train loss-1.9055, acc-0.6800, valid loss-1.9796, acc-0.5384, test loss-1.9682, acc-0.5467\n",
      "Iter-1380, train loss-1.8267, acc-0.7800, valid loss-1.9770, acc-0.5390, test loss-1.9655, acc-0.5472\n",
      "Iter-1390, train loss-1.9036, acc-0.6800, valid loss-1.9743, acc-0.5386, test loss-1.9628, acc-0.5477\n",
      "Iter-1400, train loss-2.0139, acc-0.5200, valid loss-1.9718, acc-0.5388, test loss-1.9603, acc-0.5481\n",
      "Iter-1410, train loss-1.9538, acc-0.5600, valid loss-1.9693, acc-0.5408, test loss-1.9577, acc-0.5489\n",
      "Iter-1420, train loss-1.9298, acc-0.6000, valid loss-1.9666, acc-0.5400, test loss-1.9550, acc-0.5488\n",
      "Iter-1430, train loss-1.9126, acc-0.6800, valid loss-1.9639, acc-0.5402, test loss-1.9523, acc-0.5493\n",
      "Iter-1440, train loss-1.9807, acc-0.4600, valid loss-1.9613, acc-0.5404, test loss-1.9496, acc-0.5499\n",
      "Iter-1450, train loss-1.9937, acc-0.4400, valid loss-1.9589, acc-0.5414, test loss-1.9471, acc-0.5504\n",
      "Iter-1460, train loss-1.9573, acc-0.5400, valid loss-1.9565, acc-0.5414, test loss-1.9447, acc-0.5515\n",
      "Iter-1470, train loss-1.9252, acc-0.5400, valid loss-1.9538, acc-0.5424, test loss-1.9419, acc-0.5523\n",
      "Iter-1480, train loss-1.9501, acc-0.4800, valid loss-1.9512, acc-0.5424, test loss-1.9393, acc-0.5532\n",
      "Iter-1490, train loss-1.9601, acc-0.5800, valid loss-1.9486, acc-0.5428, test loss-1.9366, acc-0.5533\n",
      "Iter-1500, train loss-1.9675, acc-0.4800, valid loss-1.9460, acc-0.5436, test loss-1.9340, acc-0.5539\n",
      "Iter-1510, train loss-1.9215, acc-0.7000, valid loss-1.9436, acc-0.5444, test loss-1.9315, acc-0.5550\n",
      "Iter-1520, train loss-1.9557, acc-0.5200, valid loss-1.9410, acc-0.5452, test loss-1.9289, acc-0.5558\n",
      "Iter-1530, train loss-1.9106, acc-0.5200, valid loss-1.9385, acc-0.5454, test loss-1.9263, acc-0.5562\n",
      "Iter-1540, train loss-1.9491, acc-0.5600, valid loss-1.9357, acc-0.5462, test loss-1.9235, acc-0.5557\n",
      "Iter-1550, train loss-1.9280, acc-0.6000, valid loss-1.9332, acc-0.5472, test loss-1.9210, acc-0.5562\n",
      "Iter-1560, train loss-1.9190, acc-0.5600, valid loss-1.9306, acc-0.5480, test loss-1.9183, acc-0.5567\n",
      "Iter-1570, train loss-1.8905, acc-0.6200, valid loss-1.9280, acc-0.5486, test loss-1.9157, acc-0.5568\n",
      "Iter-1580, train loss-1.9135, acc-0.6400, valid loss-1.9254, acc-0.5496, test loss-1.9130, acc-0.5575\n",
      "Iter-1590, train loss-1.8725, acc-0.7200, valid loss-1.9228, acc-0.5502, test loss-1.9104, acc-0.5584\n",
      "Iter-1600, train loss-1.8875, acc-0.5800, valid loss-1.9201, acc-0.5502, test loss-1.9077, acc-0.5582\n",
      "Iter-1610, train loss-1.8634, acc-0.7200, valid loss-1.9174, acc-0.5500, test loss-1.9050, acc-0.5581\n",
      "Iter-1620, train loss-2.0666, acc-0.3200, valid loss-1.9149, acc-0.5502, test loss-1.9024, acc-0.5587\n",
      "Iter-1630, train loss-1.9723, acc-0.4800, valid loss-1.9125, acc-0.5524, test loss-1.8999, acc-0.5593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-1.8841, acc-0.5200, valid loss-1.9098, acc-0.5520, test loss-1.8973, acc-0.5595\n",
      "Iter-1650, train loss-1.9266, acc-0.5000, valid loss-1.9072, acc-0.5518, test loss-1.8947, acc-0.5604\n",
      "Iter-1660, train loss-1.8779, acc-0.5600, valid loss-1.9046, acc-0.5524, test loss-1.8921, acc-0.5606\n",
      "Iter-1670, train loss-1.8591, acc-0.5600, valid loss-1.9020, acc-0.5530, test loss-1.8894, acc-0.5616\n",
      "Iter-1680, train loss-1.8794, acc-0.5800, valid loss-1.8995, acc-0.5534, test loss-1.8868, acc-0.5619\n",
      "Iter-1690, train loss-1.9121, acc-0.5600, valid loss-1.8969, acc-0.5540, test loss-1.8843, acc-0.5617\n",
      "Iter-1700, train loss-1.8353, acc-0.6600, valid loss-1.8943, acc-0.5546, test loss-1.8816, acc-0.5621\n",
      "Iter-1710, train loss-1.8670, acc-0.5400, valid loss-1.8917, acc-0.5556, test loss-1.8790, acc-0.5625\n",
      "Iter-1720, train loss-1.9212, acc-0.4800, valid loss-1.8890, acc-0.5556, test loss-1.8764, acc-0.5625\n",
      "Iter-1730, train loss-1.8935, acc-0.5800, valid loss-1.8865, acc-0.5550, test loss-1.8738, acc-0.5626\n",
      "Iter-1740, train loss-1.9349, acc-0.5400, valid loss-1.8839, acc-0.5560, test loss-1.8712, acc-0.5631\n",
      "Iter-1750, train loss-1.8835, acc-0.6000, valid loss-1.8814, acc-0.5566, test loss-1.8687, acc-0.5636\n",
      "Iter-1760, train loss-1.9690, acc-0.3600, valid loss-1.8790, acc-0.5574, test loss-1.8662, acc-0.5637\n",
      "Iter-1770, train loss-1.9113, acc-0.5600, valid loss-1.8765, acc-0.5588, test loss-1.8638, acc-0.5649\n",
      "Iter-1780, train loss-1.8787, acc-0.6000, valid loss-1.8739, acc-0.5596, test loss-1.8611, acc-0.5655\n",
      "Iter-1790, train loss-1.7984, acc-0.6600, valid loss-1.8713, acc-0.5592, test loss-1.8584, acc-0.5657\n",
      "Iter-1800, train loss-1.8803, acc-0.5600, valid loss-1.8686, acc-0.5598, test loss-1.8558, acc-0.5664\n",
      "Iter-1810, train loss-1.9000, acc-0.5800, valid loss-1.8660, acc-0.5602, test loss-1.8531, acc-0.5667\n",
      "Iter-1820, train loss-1.8338, acc-0.4800, valid loss-1.8634, acc-0.5596, test loss-1.8506, acc-0.5666\n",
      "Iter-1830, train loss-1.8843, acc-0.5000, valid loss-1.8609, acc-0.5618, test loss-1.8481, acc-0.5676\n",
      "Iter-1840, train loss-1.7874, acc-0.6400, valid loss-1.8583, acc-0.5628, test loss-1.8454, acc-0.5686\n",
      "Iter-1850, train loss-1.8554, acc-0.6000, valid loss-1.8557, acc-0.5636, test loss-1.8428, acc-0.5688\n",
      "Iter-1860, train loss-1.8196, acc-0.6200, valid loss-1.8532, acc-0.5642, test loss-1.8403, acc-0.5705\n",
      "Iter-1870, train loss-1.8662, acc-0.6000, valid loss-1.8507, acc-0.5648, test loss-1.8377, acc-0.5709\n",
      "Iter-1880, train loss-1.8547, acc-0.6200, valid loss-1.8482, acc-0.5664, test loss-1.8353, acc-0.5713\n",
      "Iter-1890, train loss-1.8298, acc-0.6200, valid loss-1.8457, acc-0.5654, test loss-1.8327, acc-0.5712\n",
      "Iter-1900, train loss-1.8155, acc-0.6200, valid loss-1.8431, acc-0.5658, test loss-1.8301, acc-0.5711\n",
      "Iter-1910, train loss-1.8383, acc-0.6400, valid loss-1.8406, acc-0.5670, test loss-1.8276, acc-0.5713\n",
      "Iter-1920, train loss-1.8190, acc-0.6200, valid loss-1.8380, acc-0.5666, test loss-1.8250, acc-0.5714\n",
      "Iter-1930, train loss-1.7917, acc-0.6000, valid loss-1.8354, acc-0.5676, test loss-1.8225, acc-0.5713\n",
      "Iter-1940, train loss-1.7835, acc-0.5800, valid loss-1.8329, acc-0.5668, test loss-1.8199, acc-0.5706\n",
      "Iter-1950, train loss-1.7418, acc-0.6200, valid loss-1.8303, acc-0.5666, test loss-1.8173, acc-0.5713\n",
      "Iter-1960, train loss-1.7379, acc-0.6800, valid loss-1.8277, acc-0.5670, test loss-1.8147, acc-0.5714\n",
      "Iter-1970, train loss-1.8528, acc-0.5600, valid loss-1.8252, acc-0.5682, test loss-1.8121, acc-0.5724\n",
      "Iter-1980, train loss-1.7470, acc-0.6600, valid loss-1.8226, acc-0.5678, test loss-1.8095, acc-0.5726\n",
      "Iter-1990, train loss-1.7452, acc-0.7600, valid loss-1.8201, acc-0.5686, test loss-1.8070, acc-0.5735\n",
      "Iter-2000, train loss-1.8508, acc-0.5800, valid loss-1.8176, acc-0.5694, test loss-1.8045, acc-0.5737\n",
      "Iter-2010, train loss-1.8920, acc-0.5200, valid loss-1.8150, acc-0.5700, test loss-1.8019, acc-0.5736\n",
      "Iter-2020, train loss-1.8222, acc-0.5600, valid loss-1.8125, acc-0.5696, test loss-1.7994, acc-0.5735\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 100000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
