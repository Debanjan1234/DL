{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "        \n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        # q = 1-p_dropout\n",
    "        # u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "    \n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = (mask * h) + ((1 - mask) * alpha_p)\n",
    "        a = 1. / np.sqrt(q + (alpha_p ** 2 * q  * (1 - q)))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = (a * dropped) + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "    \n",
    "    def forward(self, X, h, m, train):\n",
    "        Wz, Wh, Wy = m['Wz'], m['Wh'], m['Wy']\n",
    "        bz, bh, by = m['bz'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        h = h_in + (hz * (hh - h_in))\n",
    "        # equal to\n",
    "        # h = (1.0 - hz) * h_in + hz * hh\n",
    "        # or\n",
    "        # h = h_in + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz\n",
    "        # or\n",
    "        # h = h_in + hh\n",
    "\n",
    "        # SELU + SELU-Dropout\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        y, y_selu_cache = self.selu_forward(y)\n",
    "        y = X_in + y\n",
    "        \n",
    "        if train: # with Dropout\n",
    "            y, y_do_cache = self.alpha_dropout_fwd(y, self.p_dropout)\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache, y_do_cache)\n",
    "        else: # no Dropout: testing or validation\n",
    "            cache = (h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache, y_do_cache = cache\n",
    "            dy = self.alpha_dropout_bwd(dout=dy, cache=y_do_cache)\n",
    "        else:\n",
    "            h_in, hz, hz_cache, hz_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache, y_selu_cache = cache\n",
    "        \n",
    "        dy_out = dy.copy()\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dy = self.selu_backward(dy, y_selu_cache)\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        \n",
    "        dh += dh_out\n",
    "        dh_in1 = dh * (1.0 - hz) # res\n",
    "\n",
    "        dhh =  dh * hz\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "        \n",
    "        dhz = dh * (hh - h_in)\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "        \n",
    "        dX = dXz + dXh\n",
    "        \n",
    "        dh_in2 = dX[:, :self.H]\n",
    "        dh = dh_in1 + dh_in2 # res cells\n",
    "        \n",
    "        dX = dX[:, self.H:]\n",
    "        dX += dy_out # res layers\n",
    "\n",
    "        grad = dict(Wz=dWz, Wh=dWh, Wy=dWy, bz=dbz, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t], train=True)                \n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.0\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        # for i in range(0, X.shape[0] - minibatch_size +1, 1):\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam_rnn(self, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "        M, R = [], []\n",
    "         \n",
    "        # Hidden layers\n",
    "        for layer in range(nn.L):\n",
    "            M.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            R.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "         \n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8\n",
    "        state = self.initial_state()\n",
    "        smooth_loss = 1.0\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "            # Minibacthes\n",
    "            for idx in range(len(minibatches)):\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                ys, caches = self.train_forward(X_mini, state)\n",
    "                loss, dys = self.loss_function(y_train=y_mini, ys=ys)\n",
    "                _, grads = self.train_backward(dys, caches)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "                for layer in range(nn.L):\n",
    "                    for key in grads[layer].keys(): #key, value: items\n",
    "                        M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                        R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Print loss and test sample\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "                sample = self.test(X_mini[0], state, size=100)\n",
    "                print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 106.7393\n",
      "e-nffhtEuktthnGlsroii dmrpaeienntt wne B2em ainaeils oi–c cpro elhee lrooiau r–ch rrodt.i mo,eno.n   \n",
      "Iter-20 loss: 112.8254\n",
      "eIrobr  p es niii  an  o opcsdde rittprgJtpt0aztcea nneic n oehn  nLeWsifpeuolknf  n ryngtapirs rlhcF\n",
      "Iter-30 loss: 113.0974\n",
      "e-yde wdnullaerSeeJ endtomvOui W mt ls imn p1J pn  aa y tnt  e  an,ee rlta. dtsOCotehcons o   fhnwhif\n",
      "Iter-40 loss: 100.4269\n",
      "encc v  ereuurott h p  degxevi-ne o, %e  rrnnsec e pemsnhwunae,r(lNIpNm,,r.C semih dt1e 8i t1a  plmn \n",
      "Iter-50 loss: 102.8677\n",
      "eePn   le d rrhe aie atffu acaDanm E\" dl yiib- toooi  Cl gOmstlodRe dn iM  rsusttpn  tidn ls i 'uei t\n",
      "Iter-60 loss: 105.5817\n",
      "erfz7'lnWr1es  db eblthtrent,zog PeouUlaPf ipoo fs lad rypond ooo slMttsy;–l't he 2e3 nn oeone  lliyc\n",
      "Iter-70 loss: 96.8412\n",
      "eug. dhn   sIno s rchhen nrsstin ,ttkuiis 6itee hka aoskacu iet eiiLm Namhdman  e本envorIWO  \n",
      "deeaaan \n",
      "Iter-80 loss: 100.3166\n",
      "eIibgI7mensan oo fdc\n",
      "%.gdeno l0Ocsoie uiatt s shesttipfetii:0bhinmrtcscnEuheyse  oumwdLJs,eliG at in \n",
      "Iter-90 loss: 98.0409\n",
      "ef.Cmsrs,pidornshaeece  s luoft Wons leciili0aceo2ed ppesa e lsi0pygti4en mBatGreiml i1 tth paructken\n",
      "Iter-100 loss: 104.9843\n",
      "es:S7P本anam' wao nn jicogirt,ts4 nnta oN;inn cantu oae  identsL fnis en bo日d bh  anh hin. asremal nth\n",
      "Iter-110 loss: 92.3266\n",
      "edCo4n strGRaos wpe iIt  oncte, pasi drvif8iLnf5han. orcord lta  au aarC,,0L aah pad orye rSozJR thev\n",
      "Iter-120 loss: 99.8601\n",
      "e)toCmhhefenriol Szushd eutha letes raes lhp anas MaolntsLcmy.. 1at2inllor dHss )oe本 Jt'mrCien lh1tee\n",
      "Iter-130 loss: 100.6263\n",
      "ea.ird eole iix yopkald he dJebdn tn oEtim,auu fldwiad Gsecsara anteskespan  tasee pef t1p nitOr r in\n",
      "Iter-140 loss: 89.8189\n",
      "e9jt cvnnsbH1com  mho本son\"vwysit0r  uepNhasr nin )o, -fras lhe EhCee 1ocom brBs ined af thd oo31 vrnr\n",
      "Iter-150 loss: 94.1803\n",
      "eg%f t-rven cmpanny see am Wy Nk ohhat n\" rmnr, an td s本plpeari2sy. beoot42p wEdni s dKd pepons  hlet\n",
      "Iter-160 loss: 89.0833\n",
      "eWy oicgiWa koann -nterwe'o) eotoe the oo tht \n",
      "aucdns the eerl das  ooptbeo fothaa ee) wha oo k),gyn \n",
      "Iter-170 loss: 86.5185\n",
      "ecd unetcatoglSorg Feest myeice ixrchens 1lehathe JJpan cee S cith omti  inithec 1perh  Kocthir(of Bi\n",
      "Iter-180 loss: 88.9193\n",
      "egesr 7Uprnreo1e, Jhatto4 tev insstsr 'st ipandmwict6u yfthe ul2pncen1d NaCiaryr Jpofotlilir hedesd t\n",
      "Iter-190 loss: 85.8650\n",
      "e5f pontior  iGlths etd Napgonre ha Nopan Ja2)u4Ro,,ksyald paptind filan e uetied ro nee ygs: U:en bb\n",
      "Iter-200 loss: 93.6493\n",
      "epanoR Jvonn bJ,ent lGPwUr ttLs onthe-t6 mnrs oo tUrtcccosd ande本erlany td sipor lNyis iodaU telaa4-d\n",
      "Iter-210 loss: 86.7878\n",
      "e–o\n",
      ";  o farec s8an the iod ith Earan l'prG1at uop Ele1de tu\n",
      "a ca efl, aChktuad l:rinas thes erePtM C\n",
      "Iter-220 loss: 89.1355\n",
      "eveithteo aH tat ulDid3it-my wevt,, unnt.  aspe tao loiyk Tcelioud conat skamcolet,ro -W-rrand si the\n",
      "Iter-230 loss: 90.6446\n",
      "eeTpwhe th pus, Tncd shlirn wheow7y  ue terH0cJap.rEss Nirtr awthh frulathB Emen aist ceenpe th capce\n",
      "Iter-240 loss: 91.6977\n",
      "ecveseiio wepinjveste rantas Ahash efestdcengavM os 6nstalante of the wurbo3  ema3ys f9kces97r7 Ctrge\n",
      "Iter-250 loss: 83.9083\n",
      "eEg papeccasgevUinbeb, the aels aa tering p7ten c)sJargith lind oElarall is8r\" mine 3l Japan., forind\n",
      "Iter-260 loss: 80.8460\n",
      "ebon  ufose eh sa,e60pen ind and whpei Iad an thm nas) in the tor 1evn lcittyes. Japan ss txefollud f\n",
      "Iter-270 loss: 86.7288\n",
      "ei5x pLad ind receFron the yeg cheteel. aa ee J,pan de pilis uioeis% oa tovLes, intW ino . indeepthe \n",
      "Iter-280 loss: 82.4798\n",
      "eprol N本uois M9iSoy te8eeG:i eTd sinnr8an lary :he Sonoang O8ktabe in  at,t.elitakd TopKra, gurate in\n",
      "Iter-290 loss: 82.7275\n",
      "euty The mopan jy chigese w(st p%liron. The fest an the of i2m teeed fo frclenion hasxt-thona \n",
      "x-uRo,\n",
      "Iter-300 loss: 87.8818\n",
      "ed io ald -ufaf Ts laplod an eoud rorHht 9al 1ssl spe6ntei h. San dy fopNthen, Eaae d in morho (orry \n",
      "Iter-310 loss: 77.8860\n",
      "eoAry Eh5 ance:cecdlrtd -istke 本uve pinled the aS in dhe tur wiin of Japen, ifgpats  oros the 3o33 In\n",
      "Iter-320 loss: 84.7134\n",
      "e. The imod本 Hokpmot e, iote  irint of cimaty IpcnThe Arrome Jspan tot endaR if is ihts itas aa of本h \n",
      "Iter-330 loss: 79.7294\n",
      "ey in cnathinttan tfen tC webiw Td saenlorHtp aate itov ghat  onponur- apano bes annd sn anL 'he B'ip\n",
      "Iter-340 loss: 82.6517\n",
      "e8GE carfstu Nanmey. Japon maP ongghan  Ccel–orls 'bhe as hhr Innthy re Gsis日d Nanlonred Japoness ien\n",
      "Iter-350 loss: 81.5853\n",
      "erithe War ss Wurvegi1d IBuWes meito inn cilotle3yS wsho nhe tho al ceur本–ah ind Japan of the ow of i\n",
      "Iter-360 loss: 80.9933\n",
      "eNo16 aT6 pore pere0hy fesl. Sxap a-pen,tys S 4nof 1su an ergesen iKtom\n",
      "lanhG inin chonn Ran\n",
      "srcnalan\n",
      "Iter-370 loss: 87.5228\n",
      "eO of 1rgokl roune rithe ar io th saeteg itaoh oh therpan eCporoceh wd catake an chelas medpor itety \n",
      "Iter-380 loss: 83.9612\n",
      "e litolegcenees a9uloroctrrron thin, intco ing chnlares 1hldo lopan iA, Japan se Eoperlyis se chases \n",
      "Iter-390 loss: 73.9891\n",
      "eFrac intreK  liito ya o \"1 inrostict o\n",
      " and 1f the aa Aoma and'sus1mlcCPanower torcans(iJarand Fromo\n",
      "Iter-400 loss: 73.3857\n",
      "e as ad andesty at ha ang of akelofullBuapan IisiM the laAly. The aalecwaset. Jast-eiwce Wex aas chwi\n",
      "Iter-410 loss: 75.1386\n",
      "er whith The auds an borle 1180htGvonaad s, Uosthe wfveref pount iiCtitis worte ase ofe1huNeance anej\n",
      "Iter-420 loss: 81.8712\n",
      "e; inlereed Nargasund bancer fo the, .flitalg artorad ss iuKeteif, it the comore comitor viond whe th\n",
      "Iter-430 loss: 71.2683\n",
      "exhicannc tounc fogth% ouran aid kals lure1. akGe, –p6e ,o on ofeje 19553. The inlenri Gs wourth t; 8\n",
      "Iter-440 loss: 77.0835\n",
      "e wortbo itg' inof 3ien codate werof as ioges whe in as dAned pesleres f Fres ar2or IBpamiii, Di日 tar\n",
      "Iter-450 loss: 70.0373\n",
      "ealgef 1Jgpaneeloramf iof eefle bof ialct iat the gintto an th' ane tipun re aor 4C aapav of7isese co\n",
      "Iter-460 loss: 80.2135\n",
      "esily. The askensed iod. 1s Srelcoustby wor3 is. The 1suste the Ezplllar timen, 1f the GhilessecArg k\n",
      "Iter-470 loss: 82.1403\n",
      "e6keg sr Tou)y unKin Wasterlilate an mecterty tes ant of  apor , ihisg and rhe tokfs rymhecy tiCksm-c\n",
      "Iter-480 loss: 74.5358\n",
      "ey bf mhety The he the and Sendurled Hinns Iopad 6i. Jepan th ke par\n",
      " ss Euputhe the the fuore cand i\n",
      "Iter-490 loss: 71.6035\n",
      "ebon yIf anli0a e ks an amd cara\" Lc war5 d ti6hel日 rinanar andok tht coundib latieFGurion th rhe mir\n",
      "Iter-500 loss: 77.8595\n",
      "e9E.g2h laud irapelithR vort2shary jf ehece fest1 Jasen in of fhe Poull6ane  aiasto f gae is ty chaed\n",
      "Iter-510 loss: 73.1300\n",
      "eithicend is IE Th an in\"o. JaDon checture tGre ind Wyst Irea wor chons i2ale IedG % ounto, Tof copor\n",
      "Iter-520 loss: 65.5089\n",
      "et rolle. Jippnos Gcevent eoGbe if  a eeth the wesld Jasan. is ma in thes the spdtion the JarWn's of \n",
      "Iter-530 loss: 64.2886\n",
      "e,, fosthecucjli\n",
      "zkurd ryleldmied a0 cathe . J7pan si %a moncthen4W bowth wwrth The noof'b tisnte hip\n",
      "Iter-540 loss: 69.8856\n",
      "e wist andes Wur1d, of T paK9Upen l N-Spatered es pest日 本erintom percomiti, werligita i g maliticiwor\n",
      "Iter-550 loss: 68.3943\n",
      "etas ustrin arc1st alithlthe wod riessian Rysst oomtt e–f\n",
      "uremy Japan'is wasit uopchareNzun hh, Hored\n",
      "Iter-560 loss: 66.0058\n",
      "esd ilath cented an Japan', cLecting wettle ary Antealind in 18–a  iss Swind i6pero s thithe Ind Nthe\n",
      "Iter-570 loss: 76.3434\n",
      "eBoun Wy9f ind the tar wind anded she toof be passes loumelion a3 eaped Noma6, on thor, pprit5-popnpp\n",
      "Iter-580 loss: 72.1981\n",
      "evtina ans rasularyios whothinO vlyxp. firs, dejtting iter as conl,ditareMeedith xfoha ty n matt in i\n",
      "Iter-590 loss: 66.5920\n",
      "eS ap, Japangsen Japan ingCrcnes. Japan hita paate ia ia Kesion s. 1uth, ofsbru, th ca and %a Ghen. u\n",
      "Iter-600 loss: 66.4285\n",
      "ectoa t, bwstreed. f oth abcedop oothy in the w–thr cedssterem iouR 9ea ofot te thesol pecealenit inc\n",
      "Iter-610 loss: 67.9495\n",
      "eag a5 movroand Sinapogitokez an 3ited.in Che pond istcenwore viyed legeponio moaptet ometEpe  er ane\n",
      "Iter-620 loss: 62.7889\n",
      "e onmthe Ease Whnt onLitisk ind wheoprus ind ione .es tou  akbe (f isel. L rldJapon is 1ok, chee%offe\n",
      "Iter-630 loss: 65.2571\n",
      "ed in the , fitar intouned in Re arth (is East al whicowed \"eds l rhe xou,,kK Wrnd fsettiy Oly vothe \n",
      "Iter-640 loss: 69.4493\n",
      "e4'prias mgeita yo nomyos ey ofGpmero  orlmiovesey pote as lerlIis theo urwind Rrdas ganthe Aostys--a\n",
      "Iter-650 loss: 62.4427\n",
      "e, lindBrwist, whith westergert milgror eP al cauy 19. AChase s)lateI somthc aades tha RuurNCyyxn the\n",
      "Iter-660 loss: 62.9116\n",
      "eEpal iob t  opga tik nimedes unt are terins zes fouudaby f inlond Aiches. wirth  indthe Kofmeseny Ja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 59.6129\n",
      "ew.lorH utreraorlyl ofela Japangas as epponet9 Au-g phase aWCont8c-rertor Che nousp Nionat ol thil 'a\n",
      "Iter-680 loss: 62.5010\n",
      "efun; fobens, 196, chFthe Eastt iinter an iceseEc; uopptit de tinncesi th popan tee本 woltty entoky o \n",
      "Iter-690 loss: 67.4505\n",
      "es enlorh \"h porjriat on whiof , Japan ci atil Giniond Japan'sesestileteecenPe. Werly if f as un,k9k.\n",
      "Iter-700 loss: 61.5341\n",
      "ebey as whithe finss-JapaneWen 1967 unpir wirod fiscUn;tion Nhekand ceand cann itla Iist i9G, mast th\n",
      "Iter-710 loss: 64.9094\n",
      "eg's torrest'r ioHs coreastlon日a, lhithe mith r9estor Othangas ff\"italaliSea Ses am eoperginn d eatur\n",
      "Iter-720 loss: 56.1720\n",
      "er whetGry unt -tathmys the torl 's kxpand Suloty 8S angss the wova a la,dong af in of Jalan%se Oacon\n",
      "Iter-730 loss: 63.8258\n",
      "eU \" th LSGI sld Japar-sa anEpomresty. Japan in apecedust chenetoly purisan the 3itet wisity Nof tivo\n",
      "Iter-740 loss: 63.1093\n",
      "e obJap,, iote an 1sthe tounAl. f moron tef and of tren ig matesea5come romntoby 3r Nhhind Rusist ovs\n",
      "Iter-750 loss: 53.8751\n",
      "embol mmeutury lang's the fite comamio . Shecomed innse ento mpeaty al e8,y o2ttobod dy intexcollofit\n",
      "Iter-760 loss: 67.2946\n",
      "e'l igicn the wor)d's gaats 8urered W;mlorylstol thelespented So the osata in. Japan ar maross aI the\n",
      "Iter-770 loss: 62.0411\n",
      "eB,Seat\" Taling . fponr atey wor at7 ary c%uryd cidd'y thing soofth margestion 6alet ir an xy mowor– \n",
      "Iter-780 loss: 62.0958\n",
      "er istte inded 'o des the woppArteremistsor dhc uuloky . As inCe dad d cotargest 本7eutannea is Wary s\n",
      "Iter-790 loss: 68.1285\n",
      "e porly,, isttiia  2hoowod -harapan pareallec the S07 a9mO in as d atd inctain purconnovy Sy loryy Em\n",
      "Iter-800 loss: 65.0389\n",
      "ey an 1vth as is aad inaal iant cind, lethectine rry be Naolector Ond paret. IEliteve int. Japan is a\n",
      "Iter-810 loss: 50.7787\n",
      "ev Bintg Sngest yfst-zory ;iConanGmevend Sis ar asino, mirirailyyat, watd an the ha tivhers pa –eD\" I\n",
      "Iter-820 loss: 52.5895\n",
      "e Japand sive aige G viconsth  attou, Japan's wathi 0son Aper zrosint,ix , iolthe Em4toa e in mive Rn\n",
      "Iter-830 loss: 49.3017\n",
      "evinaky wargpas aaf Japanoso  Deaty pore Bran the miol tor nesly mi8ttome Emth of thethe f–opurgese f\n",
      "Iter-840 loss: 48.9841\n",
      "es trok. –Se6 2c ynstturt Ru intoy f tretom f iftecentit in an muutt. A2st ona . Toref ia th restelel\n",
      "Iter-850 loss: 59.9182\n",
      "es OEan the  2urtel eu poke Kalind the wed dhilat9en ji the wast an bymktowithy Emer Japan Wos kLhe t\n",
      "Iter-860 loss: 50.5086\n",
      "eKetperton iped curesti le wor-rese wh aparesryac thith TR whactmiin mi:a, Korhhind Lhist the Sed an \n",
      "Iter-870 loss: 60.1473\n",
      "e3 Forllt%i t ar isg-r he\n",
      "a, coledelergLder no Japan, laecouner moktoly bito ed in ma iontu ind Kevea\n",
      "Iter-880 loss: 54.2500\n",
      "es oust- Lansext  frthel. 1hth thina aughy S.undenthes tures sistt of)i( Mopest. Japan in 194 e, loke\n",
      "Iter-890 loss: 60.0725\n",
      "ea lyriE, ji isttres tresterl Redan Axperuans uLhlyof mitlorygkekg ururory, Japan This mmmed ranestTo\n",
      "Iter-900 loss: 56.2199\n",
      "e-lmofertl nat\" %a ng )o pertes on moo ta fsin Counaa, Worggss the therea Rciand ient, iit e L0ka  p \n",
      "Iter-910 loss: 63.6254\n",
      "ed fion  Japang. f 17h cpentioneecy with incthe worthB in the Eist reae loryy n anteullledea a– se pe\n",
      "Iter-920 loss: 54.1354\n",
      "e xpurlesU. A;lory iso-torysist ofoth cokom Ind  ar6 u2l Isdid Dy thoun wCrghest ceand in the lfof W5\n",
      "Iter-930 loss: 46.0453\n",
      "e1 thhe hu ar 2sses t le pectol pemion , chath ev moubFdad frriB urlthk sldyula% whe5 mo-leringme ns \n",
      "Iter-940 loss: 45.4291\n",
      "extorccolloded xiolardes ofvthe tary s unat tec Inde 186, winthy Tetexlolltiets Japenecaliob lagesity\n",
      "Iter-950 loss: 58.6511\n",
      "eR ind Nispon th9 Fko thry onh Japonese papir an ion 1-me peebed ke the world's parithe widury in lii\n",
      "Iter-960 loss: 52.6424\n",
      "ex. Japan Chith eaji sht7iUn, \n",
      "8 by mind Japen pas G20Wand the Japan 199, exJaser hita, werthe Empert\n",
      "Iter-970 loss: 46.5190\n",
      "echarg .Pus regrins fmelofy themlirad  ural thl Gwhorth in the fouf Ghe War s cothe largest mivinerin\n",
      "Iter-980 loss: 51.0085\n",
      "es ind Japankt ko isthe ind 7io1s  istiontit ty aven intel not ed latg cento laris s for8a, Ial issti\n",
      "Iter-990 loss: 49.7012\n",
      "end pinte . Th of the Uolly d8 argist G ainaly J pannse Hins. Nhear msid ista distcanesit 7iche f1ope\n",
      "Iter-1000 loss: 56.1395\n",
      "e urod Japan War -%pof ope15vl anieg, E7sithecu.y mserolitil cithe a80y Won thi isl turlot. J7 anes e\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c0ac748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//100 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "nn = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, p_dropout=p_dropout, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "nn.adam_rnn(X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
