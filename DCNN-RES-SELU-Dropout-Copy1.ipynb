{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        self.bn_caches = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "\n",
    "\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = l.conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = self.selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = self.alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            if train:\n",
    "                h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "                dy = self.alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            else:\n",
    "                h_conv_cache, h_nl_cache = h_cache[layer]\n",
    "            dh = self.selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = l.conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8 # constant\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "        smooth_train = 1.0\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            for _ in range(10):\n",
    "                idx = np.random.randint(0, len(minibatches))\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                y, cache = self.forward(X_mini, train=True)\n",
    "                loss, dy = self.loss_function(y, y_mini)\n",
    "                _, grad = self.backward(dy, cache, train=True)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "                for key in grad[0]:\n",
    "                    M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                    R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "                for layer in range(self.L):\n",
    "                    for key in grad[1][layer]:\n",
    "                        M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                        R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "                for key in grad[2]:\n",
    "                    M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                    R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 2.0204 validation accuracy: 0.540000\n",
      "Iter-2 training loss: 2.0229 validation accuracy: 0.711200\n",
      "Iter-3 training loss: 1.0330 validation accuracy: 0.788600\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 256 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 10 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = nn.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 1.4883 validation accuracy: 0.614400\n",
      "Iter-2 training loss: 0.9199 validation accuracy: 0.783800\n",
      "Iter-3 training loss: 0.5832 validation accuracy: 0.837600\n",
      "Iter-4 training loss: 0.5398 validation accuracy: 0.853400\n",
      "Iter-5 training loss: 0.4412 validation accuracy: 0.868200\n",
      "Iter-6 training loss: 0.5503 validation accuracy: 0.873600\n",
      "Iter-7 training loss: 0.6234 validation accuracy: 0.882000\n",
      "Iter-8 training loss: 0.4168 validation accuracy: 0.888000\n",
      "Iter-9 training loss: 0.5614 validation accuracy: 0.890800\n",
      "Iter-10 training loss: 0.4590 validation accuracy: 0.896400\n",
      "Test Mean accuracy: 0.8895, std: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1183085f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Hyper-parameters\n",
    "# n_iter = 10 # number of epochs\n",
    "# alpha = 1e-4 # learning_rate\n",
    "# mb_size = 256 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "# num_layers = 1 # depth \n",
    "# print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "# num_hidden_units = 10\n",
    "# p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# # build the model/NN and learn it: running session.\n",
    "# nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "# nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "#            n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# y_pred = nn.test(X_test)\n",
    "# accs = np.mean(y_pred == y_test)\n",
    "# print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # # Display the learning curve and losses for training, validation, and testing\n",
    "# # %matplotlib inline\n",
    "# # %config InlineBackend.figure_format = 'retina'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(nn.losses['train'], label='Train loss')\n",
    "# # plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
