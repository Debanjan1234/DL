{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 1, 28, 28), (5000, 1, 28, 28), (10000, 1, 28, 28))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "# y_test.shape, y_val.shape, y_train.shape\n",
    "\n",
    "M, D, C = X_train.shape[0], X_train.shape[1], y_train.max() + 1\n",
    "# M, D, C\n",
    "\n",
    "X_train, X_val, X_test = l.prepro(X_train, X_val, X_test)\n",
    "# X_train.shape, X_val.shape, X_test.shape\n",
    "# if net_type == 'cnn':\n",
    "img_shape = (1, 28, 28)\n",
    "img_shape[:]\n",
    "# *img_shape\n",
    "# X_train = X_train.reshape(-1, img_shape[:])\n",
    "X_train = X_train.reshape(-1, *img_shape)\n",
    "# X_train = X_train.reshape(-1, img_shape[0], img_shape[1], img_shape[2])\n",
    "X_val = X_val.reshape(-1, *img_shape)\n",
    "X_test = X_test.reshape(-1, *img_shape)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L, p_dropout):\n",
    "        self.mode = 'classification'\n",
    "        self.L = L # number of layers or depth\n",
    "        self.p_dropout = p_dropout\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters: weights and biases\n",
    "        # Input layer of Conv\n",
    "        self.model = []\n",
    "        self.model.append(dict(\n",
    "            W1=np.random.randn(H, 1, 3, 3) / np.sqrt(H / 2.),\n",
    "            b1=np.zeros((H, 1)),\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of Conv-bn-relu-dropout\n",
    "        m = []\n",
    "        self.bn_caches = []\n",
    "        for _ in range(self.L):\n",
    "            m.append(dict(\n",
    "                    W2=np.random.randn(H, H, 3, 3) / np.sqrt(H / 2.),\n",
    "                    b2=np.zeros((H, 1)),\n",
    "            ))\n",
    "        self.model.append(m) # self.model[0][]\n",
    "        \n",
    "        # Output layer of FC to output\n",
    "        self.model.append(dict(\n",
    "            W3=np.random.randn(H*D, C) / np.sqrt(H*D / 2.),\n",
    "            b3=np.zeros((1, C))\n",
    "        ))\n",
    "\n",
    "    def selu_forward(self, X):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        out = scale * np.where(X>=0.0, X, alpha * (np.exp(X)-1))\n",
    "        cache = X\n",
    "        return out, cache\n",
    "\n",
    "    def selu_backward(self, dout, cache):\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        X = cache\n",
    "        dX_pos = dout.copy()\n",
    "        dX_pos[X<0] = 0\n",
    "        dX_neg = dout.copy()\n",
    "        dX_neg[X>0] = 0\n",
    "        dX = scale * np.where(X>=0.0, dX_pos, dX_neg * alpha * np.exp(X))\n",
    "        return dX\n",
    "\n",
    "    # p_dropout = keep_prob in this case! \n",
    "    # Is this true in other cases as well?\n",
    "    def alpha_dropout_fwd(self, h, q):\n",
    "        '''h is activation, q is keep probability: q=1-p, p=p_dropout, and q=keep_prob'''\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        alpha_p = -scale * alpha\n",
    "        mask = np.random.binomial(1, q, size=h.shape)\n",
    "        dropped = mask * h + (1 - mask) * alpha_p\n",
    "        a = 1. / np.sqrt(q + alpha_p ** 2 * q  * (1 - q))\n",
    "        b = -a * (1 - q) * alpha_p\n",
    "        out = a * dropped + b\n",
    "        cache = (a, mask)\n",
    "        return out, cache\n",
    "\n",
    "    def alpha_dropout_bwd(self, dout, cache):\n",
    "        a, mask = cache\n",
    "        d_dropped = dout * a\n",
    "        dh = d_dropped * mask\n",
    "        return dh\n",
    "\n",
    "    def forward(self, X, train):\n",
    "\n",
    "\n",
    "        # 1st layer - Input layer: X\n",
    "        X, X_conv_cache = l.conv_forward(X=X, W=self.model[0]['W1'], b=self.model[0]['b1'])\n",
    "        X_cache = X_conv_cache\n",
    "\n",
    "        # 2nd layers - Hidden layers: h\n",
    "        h_cache = []\n",
    "        for layer in range(self.L):\n",
    "            h, h_conv_cache = l.conv_forward(X=X, W=self.model[1][layer]['W2'], b=self.model[1][layer]['b2'])\n",
    "            h, h_nl_cache = self.selu_forward(X=h)\n",
    "            h += X # residual connection\n",
    "            if train: \n",
    "                h_do_cache = None # ERROR: referenced before assigned?\n",
    "                h, h_do_cache = self.alpha_dropout_fwd(h=h, q=self.p_dropout)\n",
    "                cache = (h_conv_cache, h_nl_cache, h_do_cache)\n",
    "            else:\n",
    "                cache = (h_conv_cache, h_nl_cache)\n",
    "            h_cache.append(cache)\n",
    "            \n",
    "        # 3rd layer - Output layer: y\n",
    "        y = h.reshape([X.shape[0], -1]) # flattening\n",
    "        y, y_fc_cache = l.fc_forward(X=y, W=self.model[2]['W3'], b=self.model[2]['b3'])\n",
    "        y_cache = X, y_fc_cache\n",
    "\n",
    "        cache = (X_cache, h_cache, y_cache)\n",
    "        \n",
    "        return y, cache\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y, y_train):\n",
    "        loss = self.cross_entropy(y, y_train)\n",
    "        dy = self.dcross_entropy(y, y_train)\n",
    "        return loss, dy\n",
    "    \n",
    "    def backward(self, dy, cache, train):\n",
    "        X_cache, h_cache, y_cache = cache\n",
    "\n",
    "        # 3rd layer: Ouput layer y\n",
    "        X, y_fc_cache = y_cache\n",
    "        dy, dw3, db3 = l.fc_backward(dout=dy, cache=y_fc_cache)\n",
    "        dy = dy.reshape([-1, *X.shape[1:4]])\n",
    "        \n",
    "        # 2nd layers: Hidden layers h\n",
    "        g = []\n",
    "        for layer in reversed(range(self.L)):\n",
    "            if train:\n",
    "                h_conv_cache, h_nl_cache, h_do_cache = h_cache[layer]\n",
    "                dy = self.alpha_dropout_bwd(dout=dy, cache=h_do_cache)\n",
    "            else:\n",
    "                h_conv_cache, h_nl_cache = h_cache[layer]\n",
    "            dh = self.selu_backward(dout=dy, cache=h_nl_cache)\n",
    "            dh, dw2, db2 = l.conv_backward(dout=dh, cache=h_conv_cache)\n",
    "            dh += dy\n",
    "            g.append(dict(\n",
    "                    W2=dw2,\n",
    "                    b2=db2\n",
    "                    ))\n",
    "            \n",
    "        # 1st layer: Input layer X\n",
    "        X_conv_cache = X_cache\n",
    "        dX, dw1, db1 = l.conv_backward(dout=dh, cache=X_conv_cache)\n",
    "\n",
    "        # grad for GD\n",
    "        grad = []\n",
    "        \n",
    "        # Input layer to conv layer\n",
    "        grad.append(dict(\n",
    "            W1=dw1, \n",
    "            b1=db1\n",
    "        ))\n",
    "        \n",
    "        # Hidden layers of conv-bn-nl/relu-dropout/do\n",
    "        grad.append(g)\n",
    "        \n",
    "        # Output later to FC layer\n",
    "        grad.append(dict(\n",
    "            W3=dw3, \n",
    "            b3=db3\n",
    "        ))\n",
    "        \n",
    "        return dX, grad\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_logit, cache = self.forward(X, train=False)\n",
    "        y_prob = l.softmax(y_logit)\n",
    "        if self.mode == 'classification':\n",
    "            return np.argmax(y_prob, axis=1)\n",
    "        else: # self.mode == 'regression'\n",
    "            return np.round(y_logit)\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def adam(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        if val_set:\n",
    "            X_val, y_val = val_set\n",
    "\n",
    "        M, R = [], []\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        M_, R_ = [], []\n",
    "        for layer in range(self.L):\n",
    "            M_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "            R_.append({key: np.zeros_like(val) for key, val in self.model[1][layer].items()})\n",
    "        M.append(M_)\n",
    "        R.append(R_)\n",
    "\n",
    "        M.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "\n",
    "        beta1 = .99\n",
    "        beta2 = .999\n",
    "        eps = 1e-8 # constant\n",
    "        minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "        smooth_train = 1.0\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            #         \"\"\"\n",
    "            #         Single training step over minibatch: forward, loss, backprop\n",
    "            #         \"\"\"\n",
    "            # Shuffle for each epochs/ stochasticity/ randomly choosing\n",
    "            #             for idx in range(len(minibatches)):\n",
    "            for _ in range(10):\n",
    "                idx = np.random.randint(0, len(minibatches))\n",
    "                X_mini, y_mini = minibatches[idx]\n",
    "                y, cache = self.forward(X_mini, train=True)\n",
    "                loss, dy = self.loss_function(y, y_mini)\n",
    "                _, grad = self.backward(dy, cache, train=True)\n",
    "                self.losses['train'].append(loss)\n",
    "                smooth_train = (0.999 * smooth_train) + (0.001 * loss)\n",
    "                self.losses['smooth train'].append(smooth_train)\n",
    "\n",
    "                for key in grad[0]:\n",
    "                    M[0][key] = l.exp_running_avg(M[0][key], grad[0][key], beta1)\n",
    "                    R[0][key] = l.exp_running_avg(R[0][key], grad[0][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[0][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[0][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[0][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "                for layer in range(self.L):\n",
    "                    for key in grad[1][layer]:\n",
    "                        M[1][layer][key] = l.exp_running_avg(M[1][layer][key], grad[1][layer][key], beta1)\n",
    "                        R[1][layer][key] = l.exp_running_avg(R[1][layer][key], grad[1][layer][key]**2, beta2)\n",
    "\n",
    "                        m_k_hat = M[1][layer][key] / (1. - (beta1**(iter)))\n",
    "                        r_k_hat = R[1][layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                        self.model[1][layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "                for key in grad[2]:\n",
    "                    M[2][key] = l.exp_running_avg(M[2][key], grad[2][key], beta1)\n",
    "                    R[2][key] = l.exp_running_avg(R[2][key], grad[2][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[2][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[2][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    self.model[2][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "            # Epochs\n",
    "            if iter % print_after == 0:\n",
    "                if val_set:\n",
    "                    val_acc = l.accuracy(y_val, self.test(X_val))\n",
    "                    print('Iter-{} training loss: {:.4f} validation accuracy: {:4f}'.format(iter, loss, val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2560,1,28,28) (256,10,28,28) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3e8951ad6d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n\u001b[0;32m---> 14\u001b[0;31m            n_iter=n_iter, print_after=print_after)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2a550a6d1d61>\u001b[0m in \u001b[0;36madam\u001b[0;34m(self, X_train, y_train, val_set, alpha, mb_size, n_iter, print_after)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0msmooth_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.999\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msmooth_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2a550a6d1d61>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dy, cache, train)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mh2_conv_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2_nl_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2_do_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mdh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_dropout_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mh2_do_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mh2_conv_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2_nl_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-2a550a6d1d61>\u001b[0m in \u001b[0;36malpha_dropout_bwd\u001b[0;34m(self, dout, cache)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0md_dropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_dropped\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2560,1,28,28) (256,10,28,28) "
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10 # number of epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "mb_size = 256 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "num_layers = 1 # depth \n",
    "print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 10\n",
    "p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# build the model/NN and learn it: running session.\n",
    "nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "y_pred = nn.test(X_test)\n",
    "accs = np.mean(y_pred == y_test)\n",
    "print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 training loss: 7.5030 validation accuracy: 0.142800\n",
      "Iter-2 training loss: 5.9763 validation accuracy: 0.208200\n",
      "Iter-3 training loss: 5.2537 validation accuracy: 0.280600\n",
      "Iter-4 training loss: 3.8231 validation accuracy: 0.384000\n",
      "Iter-5 training loss: 3.1221 validation accuracy: 0.500800\n",
      "Iter-6 training loss: 2.6168 validation accuracy: 0.573000\n",
      "Iter-7 training loss: 2.3891 validation accuracy: 0.619800\n",
      "Iter-8 training loss: 2.1013 validation accuracy: 0.651800\n",
      "Iter-9 training loss: 1.9732 validation accuracy: 0.680800\n",
      "Iter-10 training loss: 1.8920 validation accuracy: 0.705800\n",
      "Test Mean accuracy: 0.7066, std: 0.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLLgbY0wtZMHdGGNqof8HD3vNUz1FNJMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1186698d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Hyper-parameters\n",
    "# n_iter = 10 # number of epochs\n",
    "# alpha = 1e-4 # learning_rate\n",
    "# mb_size = 256 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "# num_layers = 10 # depth \n",
    "# print_after = 1 # n_iter//10 # print loss for train, valid, and test\n",
    "# num_hidden_units = 1\n",
    "# p_dropout = 0.95 #  keep_prob = 1.0 - p_dropout, q = 1-p, q=0.95, o=0.05\n",
    "\n",
    "# # build the model/NN and learn it: running session.\n",
    "# nn = CNN(C=C, D=D, H=num_hidden_units, p_dropout=p_dropout, L=num_layers)\n",
    "\n",
    "# nn.adam(X_train=X_train, y_train=y_train, val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "#            n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# y_pred = nn.test(X_test)\n",
    "# accs = np.mean(y_pred == y_test)\n",
    "# print('Test Mean accuracy: {:.4f}, std: {:.4f}'.format(accs.mean(), accs.std()))\n",
    "\n",
    "# # # Display the learning curve and losses for training, validation, and testing\n",
    "# # %matplotlib inline\n",
    "# # %config InlineBackend.figure_format = 'retina'\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(nn.losses['train'], label='Train loss')\n",
    "# plt.plot(nn.losses['smooth train'], label='Train smooth loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
