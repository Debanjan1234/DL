{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        self.W_fixed = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y, _ = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def onehot(self, labels):\n",
    "        # y = np.zeros([labels.size, np.max(labels) + 1])\n",
    "        y = np.zeros([labels.size, self.C])\n",
    "        y[range(labels.size), labels] = 1.\n",
    "        return y\n",
    "\n",
    "    def squared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        data_loss = 0.5 * np.sum((y_pred - self.onehot(y_train))**2)/ m\n",
    "        return data_loss\n",
    "\n",
    "    def dsquared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        grad_y = (y_pred - self.onehot(y_train))/ m\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.squared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        dy = self.dsquared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        \n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-0.4522, acc-0.0400, valid loss-0.4525, acc-0.0738, test loss-0.4521, acc-0.0679\n",
      "Iter-20, train loss-0.4528, acc-0.0300, valid loss-0.4519, acc-0.0816, test loss-0.4515, acc-0.0772\n",
      "Iter-30, train loss-0.4520, acc-0.0700, valid loss-0.4514, acc-0.0878, test loss-0.4511, acc-0.0866\n",
      "Iter-40, train loss-0.4508, acc-0.1300, valid loss-0.4509, acc-0.0960, test loss-0.4506, acc-0.0952\n",
      "Iter-50, train loss-0.4494, acc-0.1300, valid loss-0.4503, acc-0.1106, test loss-0.4499, acc-0.1079\n",
      "Iter-60, train loss-0.4494, acc-0.1800, valid loss-0.4497, acc-0.1162, test loss-0.4494, acc-0.1146\n",
      "Iter-70, train loss-0.4513, acc-0.0600, valid loss-0.4491, acc-0.1262, test loss-0.4487, acc-0.1244\n",
      "Iter-80, train loss-0.4483, acc-0.1400, valid loss-0.4484, acc-0.1358, test loss-0.4480, acc-0.1346\n",
      "Iter-90, train loss-0.4468, acc-0.1000, valid loss-0.4477, acc-0.1458, test loss-0.4473, acc-0.1472\n",
      "Iter-100, train loss-0.4474, acc-0.1900, valid loss-0.4468, acc-0.1600, test loss-0.4465, acc-0.1578\n",
      "Iter-110, train loss-0.4487, acc-0.1100, valid loss-0.4459, acc-0.1682, test loss-0.4456, acc-0.1670\n",
      "Iter-120, train loss-0.4434, acc-0.2100, valid loss-0.4451, acc-0.1770, test loss-0.4448, acc-0.1783\n",
      "Iter-130, train loss-0.4442, acc-0.1700, valid loss-0.4442, acc-0.1896, test loss-0.4439, acc-0.1918\n",
      "Iter-140, train loss-0.4429, acc-0.1800, valid loss-0.4432, acc-0.2022, test loss-0.4429, acc-0.2031\n",
      "Iter-150, train loss-0.4429, acc-0.2800, valid loss-0.4421, acc-0.2118, test loss-0.4420, acc-0.2125\n",
      "Iter-160, train loss-0.4427, acc-0.2000, valid loss-0.4413, acc-0.2162, test loss-0.4412, acc-0.2166\n",
      "Iter-170, train loss-0.4394, acc-0.2600, valid loss-0.4401, acc-0.2274, test loss-0.4400, acc-0.2289\n",
      "Iter-180, train loss-0.4386, acc-0.2100, valid loss-0.4390, acc-0.2370, test loss-0.4389, acc-0.2364\n",
      "Iter-190, train loss-0.4376, acc-0.2500, valid loss-0.4375, acc-0.2566, test loss-0.4374, acc-0.2507\n",
      "Iter-200, train loss-0.4351, acc-0.3100, valid loss-0.4361, acc-0.2742, test loss-0.4360, acc-0.2679\n",
      "Iter-210, train loss-0.4337, acc-0.3100, valid loss-0.4348, acc-0.2852, test loss-0.4348, acc-0.2742\n",
      "Iter-220, train loss-0.4348, acc-0.2900, valid loss-0.4332, acc-0.2962, test loss-0.4333, acc-0.2871\n",
      "Iter-230, train loss-0.4293, acc-0.3300, valid loss-0.4315, acc-0.3096, test loss-0.4317, acc-0.3000\n",
      "Iter-240, train loss-0.4297, acc-0.3000, valid loss-0.4299, acc-0.3272, test loss-0.4301, acc-0.3230\n",
      "Iter-250, train loss-0.4265, acc-0.3900, valid loss-0.4281, acc-0.3422, test loss-0.4283, acc-0.3420\n",
      "Iter-260, train loss-0.4298, acc-0.3400, valid loss-0.4265, acc-0.3528, test loss-0.4268, acc-0.3562\n",
      "Iter-270, train loss-0.4243, acc-0.4000, valid loss-0.4245, acc-0.3678, test loss-0.4249, acc-0.3761\n",
      "Iter-280, train loss-0.4236, acc-0.3400, valid loss-0.4224, acc-0.3824, test loss-0.4229, acc-0.3881\n",
      "Iter-290, train loss-0.4249, acc-0.3000, valid loss-0.4203, acc-0.4048, test loss-0.4208, acc-0.4048\n",
      "Iter-300, train loss-0.4155, acc-0.4400, valid loss-0.4183, acc-0.4174, test loss-0.4189, acc-0.4166\n",
      "Iter-310, train loss-0.4162, acc-0.4200, valid loss-0.4162, acc-0.4250, test loss-0.4169, acc-0.4209\n",
      "Iter-320, train loss-0.4122, acc-0.4800, valid loss-0.4139, acc-0.4316, test loss-0.4148, acc-0.4272\n",
      "Iter-330, train loss-0.4145, acc-0.4000, valid loss-0.4118, acc-0.4384, test loss-0.4127, acc-0.4321\n",
      "Iter-340, train loss-0.4071, acc-0.4800, valid loss-0.4094, acc-0.4460, test loss-0.4104, acc-0.4396\n",
      "Iter-350, train loss-0.4127, acc-0.5000, valid loss-0.4069, acc-0.4506, test loss-0.4081, acc-0.4405\n",
      "Iter-360, train loss-0.3987, acc-0.5000, valid loss-0.4042, acc-0.4596, test loss-0.4056, acc-0.4445\n",
      "Iter-370, train loss-0.4047, acc-0.4200, valid loss-0.4016, acc-0.4644, test loss-0.4031, acc-0.4485\n",
      "Iter-380, train loss-0.3956, acc-0.4800, valid loss-0.3989, acc-0.4768, test loss-0.4005, acc-0.4557\n",
      "Iter-390, train loss-0.4055, acc-0.4800, valid loss-0.3960, acc-0.4830, test loss-0.3977, acc-0.4606\n",
      "Iter-400, train loss-0.3899, acc-0.5200, valid loss-0.3930, acc-0.4906, test loss-0.3950, acc-0.4665\n",
      "Iter-410, train loss-0.3892, acc-0.5100, valid loss-0.3900, acc-0.4936, test loss-0.3921, acc-0.4690\n",
      "Iter-420, train loss-0.3867, acc-0.5100, valid loss-0.3868, acc-0.4996, test loss-0.3890, acc-0.4727\n",
      "Iter-430, train loss-0.3892, acc-0.4700, valid loss-0.3840, acc-0.4974, test loss-0.3864, acc-0.4748\n",
      "Iter-440, train loss-0.3937, acc-0.3900, valid loss-0.3810, acc-0.5024, test loss-0.3836, acc-0.4770\n",
      "Iter-450, train loss-0.3725, acc-0.5500, valid loss-0.3783, acc-0.5016, test loss-0.3811, acc-0.4745\n",
      "Iter-460, train loss-0.3715, acc-0.5400, valid loss-0.3760, acc-0.4980, test loss-0.3789, acc-0.4739\n",
      "Iter-470, train loss-0.3686, acc-0.5000, valid loss-0.3737, acc-0.5002, test loss-0.3769, acc-0.4733\n",
      "Iter-480, train loss-0.3809, acc-0.4500, valid loss-0.3710, acc-0.5036, test loss-0.3743, acc-0.4764\n",
      "Iter-490, train loss-0.3851, acc-0.3900, valid loss-0.3686, acc-0.5054, test loss-0.3721, acc-0.4770\n",
      "Iter-500, train loss-0.3801, acc-0.4500, valid loss-0.3661, acc-0.5074, test loss-0.3697, acc-0.4779\n",
      "Iter-510, train loss-0.3659, acc-0.5100, valid loss-0.3638, acc-0.5078, test loss-0.3676, acc-0.4772\n",
      "Iter-520, train loss-0.3653, acc-0.4600, valid loss-0.3613, acc-0.5110, test loss-0.3653, acc-0.4800\n",
      "Iter-530, train loss-0.3662, acc-0.4300, valid loss-0.3589, acc-0.5164, test loss-0.3628, acc-0.4868\n",
      "Iter-540, train loss-0.3432, acc-0.5700, valid loss-0.3563, acc-0.5176, test loss-0.3604, acc-0.4869\n",
      "Iter-550, train loss-0.3421, acc-0.5400, valid loss-0.3539, acc-0.5194, test loss-0.3581, acc-0.4907\n",
      "Iter-560, train loss-0.3577, acc-0.5200, valid loss-0.3509, acc-0.5228, test loss-0.3552, acc-0.4909\n",
      "Iter-570, train loss-0.3498, acc-0.5300, valid loss-0.3484, acc-0.5230, test loss-0.3528, acc-0.4921\n",
      "Iter-580, train loss-0.3601, acc-0.4300, valid loss-0.3457, acc-0.5262, test loss-0.3502, acc-0.4943\n",
      "Iter-590, train loss-0.3406, acc-0.5200, valid loss-0.3430, acc-0.5282, test loss-0.3476, acc-0.4969\n",
      "Iter-600, train loss-0.3691, acc-0.3400, valid loss-0.3407, acc-0.5304, test loss-0.3454, acc-0.4988\n",
      "Iter-610, train loss-0.3577, acc-0.4200, valid loss-0.3386, acc-0.5342, test loss-0.3433, acc-0.5006\n",
      "Iter-620, train loss-0.3397, acc-0.5100, valid loss-0.3360, acc-0.5390, test loss-0.3407, acc-0.5045\n",
      "Iter-630, train loss-0.3564, acc-0.4900, valid loss-0.3338, acc-0.5432, test loss-0.3386, acc-0.5090\n",
      "Iter-640, train loss-0.3518, acc-0.4900, valid loss-0.3318, acc-0.5440, test loss-0.3368, acc-0.5088\n",
      "Iter-650, train loss-0.3303, acc-0.5300, valid loss-0.3300, acc-0.5464, test loss-0.3351, acc-0.5111\n",
      "Iter-660, train loss-0.3370, acc-0.5300, valid loss-0.3277, acc-0.5558, test loss-0.3327, acc-0.5223\n",
      "Iter-670, train loss-0.3341, acc-0.5300, valid loss-0.3255, acc-0.5656, test loss-0.3304, acc-0.5300\n",
      "Iter-680, train loss-0.3201, acc-0.5900, valid loss-0.3235, acc-0.5700, test loss-0.3284, acc-0.5352\n",
      "Iter-690, train loss-0.3253, acc-0.5400, valid loss-0.3217, acc-0.5720, test loss-0.3265, acc-0.5370\n",
      "Iter-700, train loss-0.3314, acc-0.5400, valid loss-0.3192, acc-0.5798, test loss-0.3240, acc-0.5446\n",
      "Iter-710, train loss-0.3410, acc-0.5100, valid loss-0.3176, acc-0.5830, test loss-0.3226, acc-0.5468\n",
      "Iter-720, train loss-0.3242, acc-0.5200, valid loss-0.3161, acc-0.5852, test loss-0.3211, acc-0.5507\n",
      "Iter-730, train loss-0.3111, acc-0.6200, valid loss-0.3140, acc-0.5834, test loss-0.3190, acc-0.5503\n",
      "Iter-740, train loss-0.3241, acc-0.5500, valid loss-0.3114, acc-0.5902, test loss-0.3164, acc-0.5584\n",
      "Iter-750, train loss-0.3179, acc-0.5700, valid loss-0.3099, acc-0.5868, test loss-0.3151, acc-0.5547\n",
      "Iter-760, train loss-0.3059, acc-0.5800, valid loss-0.3085, acc-0.5858, test loss-0.3139, acc-0.5549\n",
      "Iter-770, train loss-0.3125, acc-0.5700, valid loss-0.3070, acc-0.5890, test loss-0.3125, acc-0.5595\n",
      "Iter-780, train loss-0.2972, acc-0.6000, valid loss-0.3054, acc-0.6012, test loss-0.3107, acc-0.5708\n",
      "Iter-790, train loss-0.3079, acc-0.5600, valid loss-0.3036, acc-0.6074, test loss-0.3089, acc-0.5770\n",
      "Iter-800, train loss-0.3131, acc-0.5600, valid loss-0.3024, acc-0.6100, test loss-0.3076, acc-0.5821\n",
      "Iter-810, train loss-0.3276, acc-0.4900, valid loss-0.3011, acc-0.6122, test loss-0.3064, acc-0.5835\n",
      "Iter-820, train loss-0.3061, acc-0.6000, valid loss-0.2996, acc-0.6134, test loss-0.3047, acc-0.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-0.3284, acc-0.5400, valid loss-0.2984, acc-0.6120, test loss-0.3037, acc-0.5832\n",
      "Iter-840, train loss-0.3164, acc-0.5900, valid loss-0.2973, acc-0.6122, test loss-0.3026, acc-0.5845\n",
      "Iter-850, train loss-0.3272, acc-0.5300, valid loss-0.2965, acc-0.6086, test loss-0.3018, acc-0.5823\n",
      "Iter-860, train loss-0.3130, acc-0.5800, valid loss-0.2953, acc-0.6146, test loss-0.3005, acc-0.5878\n",
      "Iter-870, train loss-0.2918, acc-0.5800, valid loss-0.2940, acc-0.6170, test loss-0.2992, acc-0.5932\n",
      "Iter-880, train loss-0.3038, acc-0.6000, valid loss-0.2931, acc-0.6182, test loss-0.2982, acc-0.5942\n",
      "Iter-890, train loss-0.2914, acc-0.6000, valid loss-0.2921, acc-0.6184, test loss-0.2972, acc-0.5944\n",
      "Iter-900, train loss-0.2856, acc-0.6500, valid loss-0.2911, acc-0.6224, test loss-0.2961, acc-0.5972\n",
      "Iter-910, train loss-0.2900, acc-0.5800, valid loss-0.2900, acc-0.6258, test loss-0.2950, acc-0.6024\n",
      "Iter-920, train loss-0.3027, acc-0.5700, valid loss-0.2892, acc-0.6272, test loss-0.2943, acc-0.6024\n",
      "Iter-930, train loss-0.3200, acc-0.5300, valid loss-0.2885, acc-0.6280, test loss-0.2936, acc-0.6040\n",
      "Iter-940, train loss-0.2690, acc-0.6400, valid loss-0.2882, acc-0.6206, test loss-0.2933, acc-0.5944\n",
      "Iter-950, train loss-0.3437, acc-0.4700, valid loss-0.2878, acc-0.6192, test loss-0.2930, acc-0.5946\n",
      "Iter-960, train loss-0.2733, acc-0.6600, valid loss-0.2865, acc-0.6224, test loss-0.2916, acc-0.5960\n",
      "Iter-970, train loss-0.2783, acc-0.6300, valid loss-0.2858, acc-0.6222, test loss-0.2909, acc-0.5972\n",
      "Iter-980, train loss-0.2940, acc-0.5900, valid loss-0.2849, acc-0.6292, test loss-0.2899, acc-0.6048\n",
      "Iter-990, train loss-0.2726, acc-0.6500, valid loss-0.2841, acc-0.6320, test loss-0.2890, acc-0.6097\n",
      "Iter-1000, train loss-0.2988, acc-0.6100, valid loss-0.2835, acc-0.6346, test loss-0.2884, acc-0.6131\n",
      "Iter-1010, train loss-0.2831, acc-0.6500, valid loss-0.2830, acc-0.6406, test loss-0.2878, acc-0.6211\n",
      "Iter-1020, train loss-0.2857, acc-0.6400, valid loss-0.2827, acc-0.6440, test loss-0.2874, acc-0.6238\n",
      "Iter-1030, train loss-0.3150, acc-0.5200, valid loss-0.2826, acc-0.6464, test loss-0.2875, acc-0.6233\n",
      "Iter-1040, train loss-0.2799, acc-0.6500, valid loss-0.2825, acc-0.6450, test loss-0.2875, acc-0.6209\n",
      "Iter-1050, train loss-0.2990, acc-0.6100, valid loss-0.2823, acc-0.6408, test loss-0.2875, acc-0.6175\n",
      "Iter-1060, train loss-0.3084, acc-0.5600, valid loss-0.2820, acc-0.6398, test loss-0.2871, acc-0.6146\n",
      "Iter-1070, train loss-0.2692, acc-0.6300, valid loss-0.2819, acc-0.6376, test loss-0.2870, acc-0.6122\n",
      "Iter-1080, train loss-0.2887, acc-0.6300, valid loss-0.2818, acc-0.6374, test loss-0.2870, acc-0.6122\n",
      "Iter-1090, train loss-0.2893, acc-0.6100, valid loss-0.2818, acc-0.6436, test loss-0.2869, acc-0.6195\n",
      "Iter-1100, train loss-0.2892, acc-0.6300, valid loss-0.2816, acc-0.6376, test loss-0.2867, acc-0.6132\n",
      "Iter-1110, train loss-0.3197, acc-0.5700, valid loss-0.2818, acc-0.6348, test loss-0.2868, acc-0.6095\n",
      "Iter-1120, train loss-0.2479, acc-0.6900, valid loss-0.2817, acc-0.6354, test loss-0.2867, acc-0.6108\n",
      "Iter-1130, train loss-0.3065, acc-0.5600, valid loss-0.2821, acc-0.6354, test loss-0.2872, acc-0.6114\n",
      "Iter-1140, train loss-0.2972, acc-0.6100, valid loss-0.2825, acc-0.6320, test loss-0.2876, acc-0.6062\n",
      "Iter-1150, train loss-0.3073, acc-0.5900, valid loss-0.2828, acc-0.6340, test loss-0.2878, acc-0.6096\n",
      "Iter-1160, train loss-0.2796, acc-0.6000, valid loss-0.2828, acc-0.6398, test loss-0.2879, acc-0.6178\n",
      "Iter-1170, train loss-0.3041, acc-0.6400, valid loss-0.2826, acc-0.6394, test loss-0.2877, acc-0.6174\n",
      "Iter-1180, train loss-0.3162, acc-0.6000, valid loss-0.2831, acc-0.6376, test loss-0.2884, acc-0.6159\n",
      "Iter-1190, train loss-0.2929, acc-0.5600, valid loss-0.2836, acc-0.6378, test loss-0.2890, acc-0.6144\n",
      "Iter-1200, train loss-0.2899, acc-0.6000, valid loss-0.2840, acc-0.6412, test loss-0.2894, acc-0.6183\n",
      "Iter-1210, train loss-0.2825, acc-0.6000, valid loss-0.2839, acc-0.6408, test loss-0.2892, acc-0.6187\n",
      "Iter-1220, train loss-0.3001, acc-0.5700, valid loss-0.2841, acc-0.6402, test loss-0.2894, acc-0.6189\n",
      "Iter-1230, train loss-0.2868, acc-0.6100, valid loss-0.2836, acc-0.6442, test loss-0.2888, acc-0.6232\n",
      "Iter-1240, train loss-0.2905, acc-0.6400, valid loss-0.2841, acc-0.6434, test loss-0.2894, acc-0.6229\n",
      "Iter-1250, train loss-0.2623, acc-0.6900, valid loss-0.2844, acc-0.6436, test loss-0.2897, acc-0.6238\n",
      "Iter-1260, train loss-0.2848, acc-0.6300, valid loss-0.2848, acc-0.6404, test loss-0.2903, acc-0.6196\n",
      "Iter-1270, train loss-0.2999, acc-0.6200, valid loss-0.2856, acc-0.6394, test loss-0.2911, acc-0.6180\n",
      "Iter-1280, train loss-0.2717, acc-0.6900, valid loss-0.2857, acc-0.6428, test loss-0.2911, acc-0.6214\n",
      "Iter-1290, train loss-0.2798, acc-0.6400, valid loss-0.2864, acc-0.6380, test loss-0.2918, acc-0.6166\n",
      "Iter-1300, train loss-0.2716, acc-0.7100, valid loss-0.2865, acc-0.6400, test loss-0.2918, acc-0.6180\n",
      "Iter-1310, train loss-0.2637, acc-0.6400, valid loss-0.2871, acc-0.6376, test loss-0.2926, acc-0.6157\n",
      "Iter-1320, train loss-0.2985, acc-0.5900, valid loss-0.2875, acc-0.6310, test loss-0.2930, acc-0.6108\n",
      "Iter-1330, train loss-0.3057, acc-0.6000, valid loss-0.2883, acc-0.6300, test loss-0.2938, acc-0.6075\n",
      "Iter-1340, train loss-0.2886, acc-0.6500, valid loss-0.2887, acc-0.6308, test loss-0.2942, acc-0.6091\n",
      "Iter-1350, train loss-0.2905, acc-0.5900, valid loss-0.2896, acc-0.6284, test loss-0.2953, acc-0.6067\n",
      "Iter-1360, train loss-0.2835, acc-0.6300, valid loss-0.2903, acc-0.6246, test loss-0.2960, acc-0.6024\n",
      "Iter-1370, train loss-0.2554, acc-0.7000, valid loss-0.2910, acc-0.6208, test loss-0.2969, acc-0.5965\n",
      "Iter-1380, train loss-0.3051, acc-0.6000, valid loss-0.2916, acc-0.6182, test loss-0.2974, acc-0.5973\n",
      "Iter-1390, train loss-0.3098, acc-0.6000, valid loss-0.2924, acc-0.6250, test loss-0.2980, acc-0.6029\n",
      "Iter-1400, train loss-0.3019, acc-0.6100, valid loss-0.2926, acc-0.6262, test loss-0.2981, acc-0.6052\n",
      "Iter-1410, train loss-0.3093, acc-0.5800, valid loss-0.2932, acc-0.6292, test loss-0.2987, acc-0.6090\n",
      "Iter-1420, train loss-0.3017, acc-0.5800, valid loss-0.2935, acc-0.6282, test loss-0.2990, acc-0.6104\n",
      "Iter-1430, train loss-0.3133, acc-0.5400, valid loss-0.2943, acc-0.6242, test loss-0.2998, acc-0.6055\n",
      "Iter-1440, train loss-0.3203, acc-0.5700, valid loss-0.2947, acc-0.6292, test loss-0.3001, acc-0.6109\n",
      "Iter-1450, train loss-0.3030, acc-0.6200, valid loss-0.2950, acc-0.6294, test loss-0.3003, acc-0.6117\n",
      "Iter-1460, train loss-0.3285, acc-0.5700, valid loss-0.2955, acc-0.6300, test loss-0.3008, acc-0.6152\n",
      "Iter-1470, train loss-0.3253, acc-0.5000, valid loss-0.2954, acc-0.6310, test loss-0.3007, acc-0.6164\n",
      "Iter-1480, train loss-0.2915, acc-0.6300, valid loss-0.2967, acc-0.6296, test loss-0.3020, acc-0.6146\n",
      "Iter-1490, train loss-0.3011, acc-0.5800, valid loss-0.2965, acc-0.6324, test loss-0.3016, acc-0.6156\n",
      "Iter-1500, train loss-0.3147, acc-0.6000, valid loss-0.2981, acc-0.6284, test loss-0.3031, acc-0.6146\n",
      "Iter-1510, train loss-0.2994, acc-0.6500, valid loss-0.2981, acc-0.6284, test loss-0.3031, acc-0.6141\n",
      "Iter-1520, train loss-0.3087, acc-0.6000, valid loss-0.2987, acc-0.6288, test loss-0.3036, acc-0.6128\n",
      "Iter-1530, train loss-0.3161, acc-0.5700, valid loss-0.2991, acc-0.6294, test loss-0.3040, acc-0.6140\n",
      "Iter-1540, train loss-0.3168, acc-0.5600, valid loss-0.2996, acc-0.6280, test loss-0.3044, acc-0.6159\n",
      "Iter-1550, train loss-0.2871, acc-0.6500, valid loss-0.3004, acc-0.6266, test loss-0.3052, acc-0.6159\n",
      "Iter-1560, train loss-0.3080, acc-0.6100, valid loss-0.3010, acc-0.6282, test loss-0.3056, acc-0.6148\n",
      "Iter-1570, train loss-0.3114, acc-0.5800, valid loss-0.3013, acc-0.6296, test loss-0.3057, acc-0.6168\n",
      "Iter-1580, train loss-0.3180, acc-0.6400, valid loss-0.3016, acc-0.6288, test loss-0.3060, acc-0.6159\n",
      "Iter-1590, train loss-0.3083, acc-0.5900, valid loss-0.3015, acc-0.6270, test loss-0.3058, acc-0.6161\n",
      "Iter-1600, train loss-0.3166, acc-0.5500, valid loss-0.3017, acc-0.6266, test loss-0.3059, acc-0.6146\n",
      "Iter-1610, train loss-0.3230, acc-0.6000, valid loss-0.3024, acc-0.6254, test loss-0.3065, acc-0.6144\n",
      "Iter-1620, train loss-0.3556, acc-0.5600, valid loss-0.3031, acc-0.6260, test loss-0.3069, acc-0.6166\n",
      "Iter-1630, train loss-0.3532, acc-0.4900, valid loss-0.3040, acc-0.6248, test loss-0.3078, acc-0.6155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.2929, acc-0.6800, valid loss-0.3043, acc-0.6252, test loss-0.3081, acc-0.6153\n",
      "Iter-1650, train loss-0.3122, acc-0.6500, valid loss-0.3043, acc-0.6240, test loss-0.3080, acc-0.6123\n",
      "Iter-1660, train loss-0.2935, acc-0.6800, valid loss-0.3047, acc-0.6210, test loss-0.3084, acc-0.6107\n",
      "Iter-1670, train loss-0.2963, acc-0.6600, valid loss-0.3042, acc-0.6212, test loss-0.3078, acc-0.6101\n",
      "Iter-1680, train loss-0.2884, acc-0.6400, valid loss-0.3037, acc-0.6236, test loss-0.3073, acc-0.6115\n",
      "Iter-1690, train loss-0.3027, acc-0.6400, valid loss-0.3044, acc-0.6226, test loss-0.3081, acc-0.6098\n",
      "Iter-1700, train loss-0.3284, acc-0.6200, valid loss-0.3044, acc-0.6252, test loss-0.3078, acc-0.6121\n",
      "Iter-1710, train loss-0.3195, acc-0.5900, valid loss-0.3048, acc-0.6196, test loss-0.3083, acc-0.6107\n",
      "Iter-1720, train loss-0.3052, acc-0.6700, valid loss-0.3056, acc-0.6202, test loss-0.3092, acc-0.6100\n",
      "Iter-1730, train loss-0.3005, acc-0.6600, valid loss-0.3056, acc-0.6236, test loss-0.3094, acc-0.6099\n",
      "Iter-1740, train loss-0.3541, acc-0.5500, valid loss-0.3054, acc-0.6184, test loss-0.3090, acc-0.6061\n",
      "Iter-1750, train loss-0.3063, acc-0.6100, valid loss-0.3063, acc-0.6130, test loss-0.3100, acc-0.5966\n",
      "Iter-1760, train loss-0.3050, acc-0.6000, valid loss-0.3056, acc-0.6198, test loss-0.3091, acc-0.6037\n",
      "Iter-1770, train loss-0.3600, acc-0.5100, valid loss-0.3051, acc-0.6224, test loss-0.3086, acc-0.6098\n",
      "Iter-1780, train loss-0.3296, acc-0.5500, valid loss-0.3049, acc-0.6228, test loss-0.3082, acc-0.6101\n",
      "Iter-1790, train loss-0.3021, acc-0.6200, valid loss-0.3052, acc-0.6204, test loss-0.3084, acc-0.6067\n",
      "Iter-1800, train loss-0.2834, acc-0.7000, valid loss-0.3055, acc-0.6246, test loss-0.3085, acc-0.6124\n",
      "Iter-1810, train loss-0.3071, acc-0.6400, valid loss-0.3054, acc-0.6204, test loss-0.3081, acc-0.6099\n",
      "Iter-1820, train loss-0.3331, acc-0.5900, valid loss-0.3050, acc-0.6190, test loss-0.3077, acc-0.6078\n",
      "Iter-1830, train loss-0.3371, acc-0.5700, valid loss-0.3052, acc-0.6186, test loss-0.3077, acc-0.6095\n",
      "Iter-1840, train loss-0.3309, acc-0.5700, valid loss-0.3049, acc-0.6124, test loss-0.3072, acc-0.6030\n",
      "Iter-1850, train loss-0.3165, acc-0.5800, valid loss-0.3052, acc-0.6168, test loss-0.3076, acc-0.6067\n",
      "Iter-1860, train loss-0.3030, acc-0.6300, valid loss-0.3056, acc-0.6168, test loss-0.3079, acc-0.6076\n",
      "Iter-1870, train loss-0.2882, acc-0.6200, valid loss-0.3057, acc-0.6110, test loss-0.3079, acc-0.6024\n",
      "Iter-1880, train loss-0.2894, acc-0.6500, valid loss-0.3051, acc-0.6172, test loss-0.3072, acc-0.6090\n",
      "Iter-1890, train loss-0.2538, acc-0.7100, valid loss-0.3046, acc-0.6186, test loss-0.3069, acc-0.6086\n",
      "Iter-1900, train loss-0.3050, acc-0.5800, valid loss-0.3039, acc-0.6194, test loss-0.3061, acc-0.6112\n",
      "Iter-1910, train loss-0.3133, acc-0.5600, valid loss-0.3038, acc-0.6182, test loss-0.3057, acc-0.6103\n",
      "Iter-1920, train loss-0.3292, acc-0.5300, valid loss-0.3041, acc-0.6196, test loss-0.3060, acc-0.6098\n",
      "Iter-1930, train loss-0.3000, acc-0.6400, valid loss-0.3042, acc-0.6130, test loss-0.3060, acc-0.6034\n",
      "Iter-1940, train loss-0.3623, acc-0.4800, valid loss-0.3049, acc-0.6118, test loss-0.3066, acc-0.5995\n",
      "Iter-1950, train loss-0.2976, acc-0.6100, valid loss-0.3049, acc-0.6114, test loss-0.3065, acc-0.5995\n",
      "Iter-1960, train loss-0.3202, acc-0.5700, valid loss-0.3046, acc-0.6074, test loss-0.3062, acc-0.5971\n",
      "Iter-1970, train loss-0.3213, acc-0.5500, valid loss-0.3049, acc-0.6118, test loss-0.3063, acc-0.6004\n",
      "Iter-1980, train loss-0.2866, acc-0.6400, valid loss-0.3052, acc-0.6088, test loss-0.3066, acc-0.5992\n",
      "Iter-1990, train loss-0.3296, acc-0.5300, valid loss-0.3050, acc-0.6064, test loss-0.3061, acc-0.5969\n",
      "Iter-2000, train loss-0.2937, acc-0.6100, valid loss-0.3049, acc-0.6146, test loss-0.3058, acc-0.6047\n",
      "Iter-2010, train loss-0.3121, acc-0.6000, valid loss-0.3049, acc-0.6002, test loss-0.3057, acc-0.5947\n",
      "Iter-2020, train loss-0.3434, acc-0.5400, valid loss-0.3045, acc-0.6064, test loss-0.3051, acc-0.5974\n",
      "Iter-2030, train loss-0.3414, acc-0.5400, valid loss-0.3042, acc-0.6064, test loss-0.3047, acc-0.5969\n",
      "Iter-2040, train loss-0.3234, acc-0.5800, valid loss-0.3046, acc-0.6010, test loss-0.3053, acc-0.5934\n",
      "Iter-2050, train loss-0.3237, acc-0.5500, valid loss-0.3042, acc-0.6014, test loss-0.3049, acc-0.5929\n",
      "Iter-2060, train loss-0.3060, acc-0.5700, valid loss-0.3037, acc-0.6048, test loss-0.3042, acc-0.5937\n",
      "Iter-2070, train loss-0.3050, acc-0.6000, valid loss-0.3040, acc-0.6036, test loss-0.3044, acc-0.5930\n",
      "Iter-2080, train loss-0.3224, acc-0.5700, valid loss-0.3042, acc-0.5956, test loss-0.3045, acc-0.5847\n",
      "Iter-2090, train loss-0.3124, acc-0.5300, valid loss-0.3042, acc-0.5884, test loss-0.3045, acc-0.5801\n",
      "Iter-2100, train loss-0.2917, acc-0.6100, valid loss-0.3046, acc-0.5902, test loss-0.3052, acc-0.5829\n",
      "Iter-2110, train loss-0.3357, acc-0.5300, valid loss-0.3040, acc-0.5846, test loss-0.3045, acc-0.5776\n",
      "Iter-2120, train loss-0.3201, acc-0.5900, valid loss-0.3043, acc-0.5836, test loss-0.3048, acc-0.5784\n",
      "Iter-2130, train loss-0.3053, acc-0.5500, valid loss-0.3048, acc-0.5888, test loss-0.3052, acc-0.5847\n",
      "Iter-2140, train loss-0.2975, acc-0.6200, valid loss-0.3050, acc-0.5860, test loss-0.3052, acc-0.5819\n",
      "Iter-2150, train loss-0.2943, acc-0.6100, valid loss-0.3044, acc-0.5888, test loss-0.3045, acc-0.5856\n",
      "Iter-2160, train loss-0.3336, acc-0.5200, valid loss-0.3041, acc-0.5880, test loss-0.3041, acc-0.5849\n",
      "Iter-2170, train loss-0.3213, acc-0.5600, valid loss-0.3038, acc-0.5984, test loss-0.3039, acc-0.5928\n",
      "Iter-2180, train loss-0.2827, acc-0.6000, valid loss-0.3037, acc-0.5914, test loss-0.3037, acc-0.5873\n",
      "Iter-2190, train loss-0.2961, acc-0.5500, valid loss-0.3038, acc-0.5918, test loss-0.3036, acc-0.5889\n",
      "Iter-2200, train loss-0.2914, acc-0.6100, valid loss-0.3037, acc-0.5834, test loss-0.3034, acc-0.5820\n",
      "Iter-2210, train loss-0.3129, acc-0.5200, valid loss-0.3033, acc-0.5856, test loss-0.3027, acc-0.5848\n",
      "Iter-2220, train loss-0.3067, acc-0.5600, valid loss-0.3039, acc-0.5784, test loss-0.3031, acc-0.5783\n",
      "Iter-2230, train loss-0.2824, acc-0.5900, valid loss-0.3041, acc-0.5802, test loss-0.3034, acc-0.5806\n",
      "Iter-2240, train loss-0.3176, acc-0.5400, valid loss-0.3040, acc-0.5856, test loss-0.3033, acc-0.5826\n",
      "Iter-2250, train loss-0.3288, acc-0.5200, valid loss-0.3040, acc-0.5824, test loss-0.3033, acc-0.5814\n",
      "Iter-2260, train loss-0.3322, acc-0.5700, valid loss-0.3036, acc-0.5788, test loss-0.3029, acc-0.5802\n",
      "Iter-2270, train loss-0.3147, acc-0.5900, valid loss-0.3034, acc-0.5914, test loss-0.3026, acc-0.5908\n",
      "Iter-2280, train loss-0.3280, acc-0.5200, valid loss-0.3037, acc-0.5908, test loss-0.3032, acc-0.5874\n",
      "Iter-2290, train loss-0.2990, acc-0.6400, valid loss-0.3038, acc-0.5998, test loss-0.3033, acc-0.5946\n",
      "Iter-2300, train loss-0.3111, acc-0.5300, valid loss-0.3038, acc-0.6038, test loss-0.3033, acc-0.5996\n",
      "Iter-2310, train loss-0.2809, acc-0.6000, valid loss-0.3043, acc-0.5970, test loss-0.3037, acc-0.5945\n",
      "Iter-2320, train loss-0.3302, acc-0.5300, valid loss-0.3042, acc-0.5916, test loss-0.3036, acc-0.5885\n",
      "Iter-2330, train loss-0.2932, acc-0.6500, valid loss-0.3051, acc-0.5780, test loss-0.3044, acc-0.5794\n",
      "Iter-2340, train loss-0.3030, acc-0.5400, valid loss-0.3050, acc-0.5818, test loss-0.3042, acc-0.5818\n",
      "Iter-2350, train loss-0.3528, acc-0.4400, valid loss-0.3044, acc-0.5878, test loss-0.3037, acc-0.5885\n",
      "Iter-2360, train loss-0.3002, acc-0.5900, valid loss-0.3049, acc-0.5880, test loss-0.3043, acc-0.5865\n",
      "Iter-2370, train loss-0.3094, acc-0.5800, valid loss-0.3044, acc-0.5730, test loss-0.3039, acc-0.5726\n",
      "Iter-2380, train loss-0.3122, acc-0.4900, valid loss-0.3045, acc-0.5654, test loss-0.3039, acc-0.5680\n",
      "Iter-2390, train loss-0.3018, acc-0.5800, valid loss-0.3044, acc-0.5744, test loss-0.3038, acc-0.5744\n",
      "Iter-2400, train loss-0.2779, acc-0.5800, valid loss-0.3045, acc-0.5864, test loss-0.3038, acc-0.5853\n",
      "Iter-2410, train loss-0.3516, acc-0.4900, valid loss-0.3043, acc-0.5862, test loss-0.3036, acc-0.5863\n",
      "Iter-2420, train loss-0.2645, acc-0.7100, valid loss-0.3038, acc-0.5826, test loss-0.3034, acc-0.5812\n",
      "Iter-2430, train loss-0.2980, acc-0.6100, valid loss-0.3032, acc-0.5918, test loss-0.3030, acc-0.5904\n",
      "Iter-2440, train loss-0.2983, acc-0.5500, valid loss-0.3027, acc-0.5860, test loss-0.3025, acc-0.5837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.3362, acc-0.4600, valid loss-0.3024, acc-0.5814, test loss-0.3020, acc-0.5794\n",
      "Iter-2460, train loss-0.3167, acc-0.5600, valid loss-0.3022, acc-0.5880, test loss-0.3016, acc-0.5855\n",
      "Iter-2470, train loss-0.3076, acc-0.5800, valid loss-0.3024, acc-0.5728, test loss-0.3019, acc-0.5739\n",
      "Iter-2480, train loss-0.3070, acc-0.4900, valid loss-0.3025, acc-0.5786, test loss-0.3020, acc-0.5782\n",
      "Iter-2490, train loss-0.2714, acc-0.6500, valid loss-0.3028, acc-0.5622, test loss-0.3025, acc-0.5636\n",
      "Iter-2500, train loss-0.2999, acc-0.5400, valid loss-0.3025, acc-0.5686, test loss-0.3021, acc-0.5709\n",
      "Iter-2510, train loss-0.3114, acc-0.5100, valid loss-0.3022, acc-0.5694, test loss-0.3017, acc-0.5737\n",
      "Iter-2520, train loss-0.3182, acc-0.5700, valid loss-0.3018, acc-0.5646, test loss-0.3014, acc-0.5679\n",
      "Iter-2530, train loss-0.3164, acc-0.6100, valid loss-0.3016, acc-0.5586, test loss-0.3011, acc-0.5599\n",
      "Iter-2540, train loss-0.2986, acc-0.5200, valid loss-0.3018, acc-0.5604, test loss-0.3014, acc-0.5605\n",
      "Iter-2550, train loss-0.3237, acc-0.5000, valid loss-0.3014, acc-0.5552, test loss-0.3011, acc-0.5565\n",
      "Iter-2560, train loss-0.3071, acc-0.5900, valid loss-0.3016, acc-0.5554, test loss-0.3014, acc-0.5572\n",
      "Iter-2570, train loss-0.2891, acc-0.6100, valid loss-0.3022, acc-0.5430, test loss-0.3019, acc-0.5473\n",
      "Iter-2580, train loss-0.2940, acc-0.5500, valid loss-0.3022, acc-0.5422, test loss-0.3019, acc-0.5449\n",
      "Iter-2590, train loss-0.3011, acc-0.6000, valid loss-0.3023, acc-0.5430, test loss-0.3017, acc-0.5481\n",
      "Iter-2600, train loss-0.2853, acc-0.5600, valid loss-0.3026, acc-0.5446, test loss-0.3018, acc-0.5487\n",
      "Iter-2610, train loss-0.3156, acc-0.5100, valid loss-0.3021, acc-0.5450, test loss-0.3013, acc-0.5491\n",
      "Iter-2620, train loss-0.3051, acc-0.5100, valid loss-0.3024, acc-0.5402, test loss-0.3017, acc-0.5444\n",
      "Iter-2630, train loss-0.2849, acc-0.5300, valid loss-0.3019, acc-0.5372, test loss-0.3012, acc-0.5410\n",
      "Iter-2640, train loss-0.3252, acc-0.4900, valid loss-0.3015, acc-0.5438, test loss-0.3009, acc-0.5480\n",
      "Iter-2650, train loss-0.2975, acc-0.5500, valid loss-0.3020, acc-0.5454, test loss-0.3012, acc-0.5484\n",
      "Iter-2660, train loss-0.3158, acc-0.5100, valid loss-0.3017, acc-0.5438, test loss-0.3011, acc-0.5475\n",
      "Iter-2670, train loss-0.3049, acc-0.5300, valid loss-0.3014, acc-0.5576, test loss-0.3010, acc-0.5597\n",
      "Iter-2680, train loss-0.3123, acc-0.5500, valid loss-0.3009, acc-0.5600, test loss-0.3002, acc-0.5612\n",
      "Iter-2690, train loss-0.3292, acc-0.5400, valid loss-0.3004, acc-0.5534, test loss-0.2999, acc-0.5562\n",
      "Iter-2700, train loss-0.3148, acc-0.5100, valid loss-0.2996, acc-0.5592, test loss-0.2990, acc-0.5604\n",
      "Iter-2710, train loss-0.2833, acc-0.5900, valid loss-0.2996, acc-0.5572, test loss-0.2990, acc-0.5592\n",
      "Iter-2720, train loss-0.2503, acc-0.6100, valid loss-0.2991, acc-0.5568, test loss-0.2988, acc-0.5587\n",
      "Iter-2730, train loss-0.2809, acc-0.6000, valid loss-0.2990, acc-0.5590, test loss-0.2986, acc-0.5615\n",
      "Iter-2740, train loss-0.3089, acc-0.5700, valid loss-0.2987, acc-0.5600, test loss-0.2985, acc-0.5634\n",
      "Iter-2750, train loss-0.2943, acc-0.6000, valid loss-0.2987, acc-0.5576, test loss-0.2983, acc-0.5615\n",
      "Iter-2760, train loss-0.3091, acc-0.6000, valid loss-0.2988, acc-0.5618, test loss-0.2984, acc-0.5635\n",
      "Iter-2770, train loss-0.3221, acc-0.5400, valid loss-0.2982, acc-0.5582, test loss-0.2979, acc-0.5612\n",
      "Iter-2780, train loss-0.3065, acc-0.5800, valid loss-0.2982, acc-0.5572, test loss-0.2979, acc-0.5629\n",
      "Iter-2790, train loss-0.3003, acc-0.5100, valid loss-0.2981, acc-0.5546, test loss-0.2977, acc-0.5579\n",
      "Iter-2800, train loss-0.2864, acc-0.5600, valid loss-0.2985, acc-0.5470, test loss-0.2982, acc-0.5489\n",
      "Iter-2810, train loss-0.3150, acc-0.5300, valid loss-0.2985, acc-0.5412, test loss-0.2983, acc-0.5465\n",
      "Iter-2820, train loss-0.3275, acc-0.4800, valid loss-0.2987, acc-0.5410, test loss-0.2984, acc-0.5429\n",
      "Iter-2830, train loss-0.3261, acc-0.4500, valid loss-0.2991, acc-0.5340, test loss-0.2989, acc-0.5362\n",
      "Iter-2840, train loss-0.3233, acc-0.4900, valid loss-0.2991, acc-0.5328, test loss-0.2988, acc-0.5347\n",
      "Iter-2850, train loss-0.2955, acc-0.5700, valid loss-0.2996, acc-0.5246, test loss-0.2994, acc-0.5271\n",
      "Iter-2860, train loss-0.3050, acc-0.5100, valid loss-0.2992, acc-0.5286, test loss-0.2991, acc-0.5293\n",
      "Iter-2870, train loss-0.2848, acc-0.5500, valid loss-0.2993, acc-0.5262, test loss-0.2992, acc-0.5282\n",
      "Iter-2880, train loss-0.2801, acc-0.5600, valid loss-0.2992, acc-0.5262, test loss-0.2991, acc-0.5263\n",
      "Iter-2890, train loss-0.2969, acc-0.5000, valid loss-0.2987, acc-0.5314, test loss-0.2986, acc-0.5339\n",
      "Iter-2900, train loss-0.3075, acc-0.5600, valid loss-0.2990, acc-0.5302, test loss-0.2989, acc-0.5337\n",
      "Iter-2910, train loss-0.2941, acc-0.5700, valid loss-0.2983, acc-0.5300, test loss-0.2985, acc-0.5336\n",
      "Iter-2920, train loss-0.2901, acc-0.5900, valid loss-0.2976, acc-0.5334, test loss-0.2979, acc-0.5368\n",
      "Iter-2930, train loss-0.2576, acc-0.6200, valid loss-0.2980, acc-0.5308, test loss-0.2983, acc-0.5346\n",
      "Iter-2940, train loss-0.2989, acc-0.5200, valid loss-0.2978, acc-0.5358, test loss-0.2982, acc-0.5369\n",
      "Iter-2950, train loss-0.3264, acc-0.4400, valid loss-0.2973, acc-0.5398, test loss-0.2975, acc-0.5421\n",
      "Iter-2960, train loss-0.3263, acc-0.5200, valid loss-0.2972, acc-0.5404, test loss-0.2973, acc-0.5429\n",
      "Iter-2970, train loss-0.3227, acc-0.4800, valid loss-0.2974, acc-0.5338, test loss-0.2976, acc-0.5371\n",
      "Iter-2980, train loss-0.3362, acc-0.4600, valid loss-0.2972, acc-0.5362, test loss-0.2976, acc-0.5385\n",
      "Iter-2990, train loss-0.3080, acc-0.5000, valid loss-0.2975, acc-0.5334, test loss-0.2979, acc-0.5373\n",
      "Iter-3000, train loss-0.2756, acc-0.5400, valid loss-0.2975, acc-0.5316, test loss-0.2980, acc-0.5369\n",
      "Iter-3010, train loss-0.3262, acc-0.5300, valid loss-0.2974, acc-0.5298, test loss-0.2978, acc-0.5349\n",
      "Iter-3020, train loss-0.2832, acc-0.5800, valid loss-0.2969, acc-0.5310, test loss-0.2976, acc-0.5376\n",
      "Iter-3030, train loss-0.3019, acc-0.6000, valid loss-0.2964, acc-0.5330, test loss-0.2971, acc-0.5375\n",
      "Iter-3040, train loss-0.2775, acc-0.6000, valid loss-0.2959, acc-0.5380, test loss-0.2967, acc-0.5442\n",
      "Iter-3050, train loss-0.2825, acc-0.6000, valid loss-0.2961, acc-0.5320, test loss-0.2969, acc-0.5382\n",
      "Iter-3060, train loss-0.3078, acc-0.5600, valid loss-0.2959, acc-0.5314, test loss-0.2966, acc-0.5347\n",
      "Iter-3070, train loss-0.2762, acc-0.5600, valid loss-0.2955, acc-0.5304, test loss-0.2962, acc-0.5341\n",
      "Iter-3080, train loss-0.3401, acc-0.4700, valid loss-0.2954, acc-0.5292, test loss-0.2962, acc-0.5312\n",
      "Iter-3090, train loss-0.3148, acc-0.4800, valid loss-0.2949, acc-0.5324, test loss-0.2956, acc-0.5346\n",
      "Iter-3100, train loss-0.3256, acc-0.4500, valid loss-0.2953, acc-0.5318, test loss-0.2960, acc-0.5340\n",
      "Iter-3110, train loss-0.3354, acc-0.4400, valid loss-0.2953, acc-0.5310, test loss-0.2960, acc-0.5334\n",
      "Iter-3120, train loss-0.2864, acc-0.5600, valid loss-0.2952, acc-0.5346, test loss-0.2959, acc-0.5370\n",
      "Iter-3130, train loss-0.2775, acc-0.5200, valid loss-0.2946, acc-0.5376, test loss-0.2954, acc-0.5409\n",
      "Iter-3140, train loss-0.3140, acc-0.5000, valid loss-0.2942, acc-0.5378, test loss-0.2951, acc-0.5392\n",
      "Iter-3150, train loss-0.3047, acc-0.4700, valid loss-0.2939, acc-0.5364, test loss-0.2950, acc-0.5384\n",
      "Iter-3160, train loss-0.2867, acc-0.5800, valid loss-0.2942, acc-0.5350, test loss-0.2953, acc-0.5343\n",
      "Iter-3170, train loss-0.2753, acc-0.5800, valid loss-0.2939, acc-0.5342, test loss-0.2951, acc-0.5375\n",
      "Iter-3180, train loss-0.2834, acc-0.5800, valid loss-0.2941, acc-0.5336, test loss-0.2951, acc-0.5372\n",
      "Iter-3190, train loss-0.3379, acc-0.4100, valid loss-0.2942, acc-0.5366, test loss-0.2952, acc-0.5384\n",
      "Iter-3200, train loss-0.3011, acc-0.4700, valid loss-0.2940, acc-0.5400, test loss-0.2950, acc-0.5415\n",
      "Iter-3210, train loss-0.3260, acc-0.4800, valid loss-0.2937, acc-0.5446, test loss-0.2949, acc-0.5462\n",
      "Iter-3220, train loss-0.2857, acc-0.6000, valid loss-0.2941, acc-0.5366, test loss-0.2954, acc-0.5414\n",
      "Iter-3230, train loss-0.3120, acc-0.4800, valid loss-0.2941, acc-0.5396, test loss-0.2956, acc-0.5426\n",
      "Iter-3240, train loss-0.3123, acc-0.5300, valid loss-0.2943, acc-0.5348, test loss-0.2958, acc-0.5388\n",
      "Iter-3250, train loss-0.2830, acc-0.5000, valid loss-0.2942, acc-0.5368, test loss-0.2957, acc-0.5399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.3163, acc-0.4800, valid loss-0.2937, acc-0.5438, test loss-0.2951, acc-0.5447\n",
      "Iter-3270, train loss-0.2992, acc-0.5200, valid loss-0.2940, acc-0.5450, test loss-0.2953, acc-0.5460\n",
      "Iter-3280, train loss-0.2934, acc-0.5800, valid loss-0.2939, acc-0.5498, test loss-0.2951, acc-0.5506\n",
      "Iter-3290, train loss-0.2796, acc-0.5800, valid loss-0.2938, acc-0.5518, test loss-0.2951, acc-0.5536\n",
      "Iter-3300, train loss-0.2815, acc-0.5500, valid loss-0.2937, acc-0.5598, test loss-0.2950, acc-0.5599\n",
      "Iter-3310, train loss-0.2750, acc-0.5900, valid loss-0.2937, acc-0.5564, test loss-0.2950, acc-0.5567\n",
      "Iter-3320, train loss-0.3100, acc-0.5000, valid loss-0.2932, acc-0.5596, test loss-0.2947, acc-0.5596\n",
      "Iter-3330, train loss-0.2932, acc-0.5400, valid loss-0.2932, acc-0.5616, test loss-0.2947, acc-0.5612\n",
      "Iter-3340, train loss-0.3021, acc-0.5800, valid loss-0.2928, acc-0.5546, test loss-0.2944, acc-0.5576\n",
      "Iter-3350, train loss-0.3244, acc-0.4800, valid loss-0.2923, acc-0.5654, test loss-0.2940, acc-0.5650\n",
      "Iter-3360, train loss-0.3119, acc-0.5300, valid loss-0.2921, acc-0.5626, test loss-0.2938, acc-0.5623\n",
      "Iter-3370, train loss-0.2947, acc-0.6100, valid loss-0.2920, acc-0.5556, test loss-0.2939, acc-0.5588\n",
      "Iter-3380, train loss-0.3176, acc-0.5000, valid loss-0.2920, acc-0.5536, test loss-0.2939, acc-0.5575\n",
      "Iter-3390, train loss-0.2818, acc-0.5900, valid loss-0.2918, acc-0.5552, test loss-0.2938, acc-0.5570\n",
      "Iter-3400, train loss-0.3032, acc-0.5400, valid loss-0.2916, acc-0.5548, test loss-0.2937, acc-0.5555\n",
      "Iter-3410, train loss-0.2766, acc-0.6100, valid loss-0.2914, acc-0.5554, test loss-0.2936, acc-0.5554\n",
      "Iter-3420, train loss-0.3306, acc-0.4900, valid loss-0.2913, acc-0.5606, test loss-0.2934, acc-0.5601\n",
      "Iter-3430, train loss-0.2954, acc-0.5500, valid loss-0.2910, acc-0.5552, test loss-0.2931, acc-0.5566\n",
      "Iter-3440, train loss-0.2751, acc-0.5500, valid loss-0.2907, acc-0.5542, test loss-0.2929, acc-0.5558\n",
      "Iter-3450, train loss-0.2836, acc-0.5900, valid loss-0.2909, acc-0.5462, test loss-0.2930, acc-0.5497\n",
      "Iter-3460, train loss-0.3066, acc-0.5600, valid loss-0.2910, acc-0.5498, test loss-0.2931, acc-0.5536\n",
      "Iter-3470, train loss-0.3035, acc-0.5000, valid loss-0.2904, acc-0.5538, test loss-0.2926, acc-0.5587\n",
      "Iter-3480, train loss-0.2573, acc-0.6300, valid loss-0.2904, acc-0.5528, test loss-0.2926, acc-0.5572\n",
      "Iter-3490, train loss-0.3133, acc-0.5400, valid loss-0.2902, acc-0.5542, test loss-0.2924, acc-0.5596\n",
      "Iter-3500, train loss-0.2750, acc-0.6300, valid loss-0.2901, acc-0.5510, test loss-0.2923, acc-0.5559\n",
      "Iter-3510, train loss-0.2813, acc-0.5500, valid loss-0.2898, acc-0.5534, test loss-0.2921, acc-0.5591\n",
      "Iter-3520, train loss-0.2995, acc-0.5300, valid loss-0.2899, acc-0.5524, test loss-0.2922, acc-0.5553\n",
      "Iter-3530, train loss-0.2951, acc-0.5700, valid loss-0.2897, acc-0.5544, test loss-0.2921, acc-0.5587\n",
      "Iter-3540, train loss-0.2958, acc-0.5400, valid loss-0.2896, acc-0.5556, test loss-0.2921, acc-0.5578\n",
      "Iter-3550, train loss-0.3091, acc-0.5300, valid loss-0.2893, acc-0.5604, test loss-0.2918, acc-0.5637\n",
      "Iter-3560, train loss-0.3024, acc-0.5700, valid loss-0.2891, acc-0.5646, test loss-0.2917, acc-0.5657\n",
      "Iter-3570, train loss-0.3008, acc-0.5100, valid loss-0.2889, acc-0.5726, test loss-0.2916, acc-0.5762\n",
      "Iter-3580, train loss-0.2985, acc-0.5500, valid loss-0.2888, acc-0.5590, test loss-0.2916, acc-0.5627\n",
      "Iter-3590, train loss-0.3001, acc-0.5600, valid loss-0.2889, acc-0.5596, test loss-0.2916, acc-0.5628\n",
      "Iter-3600, train loss-0.3165, acc-0.5000, valid loss-0.2891, acc-0.5586, test loss-0.2918, acc-0.5620\n",
      "Iter-3610, train loss-0.2608, acc-0.6200, valid loss-0.2892, acc-0.5554, test loss-0.2918, acc-0.5578\n",
      "Iter-3620, train loss-0.2814, acc-0.5800, valid loss-0.2893, acc-0.5542, test loss-0.2920, acc-0.5563\n",
      "Iter-3630, train loss-0.3506, acc-0.4000, valid loss-0.2895, acc-0.5470, test loss-0.2920, acc-0.5513\n",
      "Iter-3640, train loss-0.3243, acc-0.4700, valid loss-0.2893, acc-0.5526, test loss-0.2919, acc-0.5570\n",
      "Iter-3650, train loss-0.3238, acc-0.4800, valid loss-0.2894, acc-0.5448, test loss-0.2921, acc-0.5517\n",
      "Iter-3660, train loss-0.3112, acc-0.5400, valid loss-0.2891, acc-0.5556, test loss-0.2920, acc-0.5580\n",
      "Iter-3670, train loss-0.2970, acc-0.5300, valid loss-0.2890, acc-0.5532, test loss-0.2918, acc-0.5578\n",
      "Iter-3680, train loss-0.2494, acc-0.6300, valid loss-0.2887, acc-0.5840, test loss-0.2916, acc-0.5787\n",
      "Iter-3690, train loss-0.2901, acc-0.5700, valid loss-0.2888, acc-0.5582, test loss-0.2917, acc-0.5615\n",
      "Iter-3700, train loss-0.3205, acc-0.5700, valid loss-0.2884, acc-0.5890, test loss-0.2915, acc-0.5800\n",
      "Iter-3710, train loss-0.3395, acc-0.5000, valid loss-0.2879, acc-0.5966, test loss-0.2910, acc-0.5875\n",
      "Iter-3720, train loss-0.2995, acc-0.6200, valid loss-0.2878, acc-0.6002, test loss-0.2908, acc-0.5888\n",
      "Iter-3730, train loss-0.2861, acc-0.6500, valid loss-0.2878, acc-0.5924, test loss-0.2909, acc-0.5814\n",
      "Iter-3740, train loss-0.2870, acc-0.5600, valid loss-0.2876, acc-0.5928, test loss-0.2907, acc-0.5823\n",
      "Iter-3750, train loss-0.2948, acc-0.5900, valid loss-0.2875, acc-0.5974, test loss-0.2906, acc-0.5869\n",
      "Iter-3760, train loss-0.3164, acc-0.5900, valid loss-0.2877, acc-0.5966, test loss-0.2910, acc-0.5835\n",
      "Iter-3770, train loss-0.2980, acc-0.6300, valid loss-0.2877, acc-0.5636, test loss-0.2910, acc-0.5642\n",
      "Iter-3780, train loss-0.2959, acc-0.5200, valid loss-0.2875, acc-0.5938, test loss-0.2908, acc-0.5861\n",
      "Iter-3790, train loss-0.2988, acc-0.5800, valid loss-0.2872, acc-0.5890, test loss-0.2905, acc-0.5838\n",
      "Iter-3800, train loss-0.2762, acc-0.6200, valid loss-0.2873, acc-0.5582, test loss-0.2906, acc-0.5624\n",
      "Iter-3810, train loss-0.3271, acc-0.5400, valid loss-0.2869, acc-0.5636, test loss-0.2902, acc-0.5668\n",
      "Iter-3820, train loss-0.2647, acc-0.6100, valid loss-0.2868, acc-0.5624, test loss-0.2901, acc-0.5660\n",
      "Iter-3830, train loss-0.3022, acc-0.5400, valid loss-0.2864, acc-0.5614, test loss-0.2899, acc-0.5652\n",
      "Iter-3840, train loss-0.3033, acc-0.5000, valid loss-0.2862, acc-0.5792, test loss-0.2899, acc-0.5781\n",
      "Iter-3850, train loss-0.2895, acc-0.4900, valid loss-0.2859, acc-0.5936, test loss-0.2897, acc-0.5816\n",
      "Iter-3860, train loss-0.3060, acc-0.5500, valid loss-0.2859, acc-0.5980, test loss-0.2896, acc-0.5847\n",
      "Iter-3870, train loss-0.2716, acc-0.6200, valid loss-0.2857, acc-0.5860, test loss-0.2893, acc-0.5817\n",
      "Iter-3880, train loss-0.2596, acc-0.6600, valid loss-0.2854, acc-0.6040, test loss-0.2890, acc-0.5880\n",
      "Iter-3890, train loss-0.2791, acc-0.6400, valid loss-0.2857, acc-0.6024, test loss-0.2892, acc-0.5889\n",
      "Iter-3900, train loss-0.3087, acc-0.5600, valid loss-0.2853, acc-0.5690, test loss-0.2888, acc-0.5698\n",
      "Iter-3910, train loss-0.3008, acc-0.5400, valid loss-0.2852, acc-0.5940, test loss-0.2887, acc-0.5862\n",
      "Iter-3920, train loss-0.2605, acc-0.5500, valid loss-0.2851, acc-0.5734, test loss-0.2886, acc-0.5734\n",
      "Iter-3930, train loss-0.3174, acc-0.5000, valid loss-0.2848, acc-0.5758, test loss-0.2885, acc-0.5742\n",
      "Iter-3940, train loss-0.2946, acc-0.5800, valid loss-0.2848, acc-0.5736, test loss-0.2883, acc-0.5750\n",
      "Iter-3950, train loss-0.3042, acc-0.5900, valid loss-0.2846, acc-0.6046, test loss-0.2882, acc-0.5918\n",
      "Iter-3960, train loss-0.2680, acc-0.6200, valid loss-0.2847, acc-0.6034, test loss-0.2882, acc-0.5910\n",
      "Iter-3970, train loss-0.2928, acc-0.5400, valid loss-0.2845, acc-0.6050, test loss-0.2879, acc-0.5916\n",
      "Iter-3980, train loss-0.2744, acc-0.5700, valid loss-0.2847, acc-0.5974, test loss-0.2879, acc-0.5899\n",
      "Iter-3990, train loss-0.3022, acc-0.5700, valid loss-0.2842, acc-0.6100, test loss-0.2876, acc-0.5952\n",
      "Iter-4000, train loss-0.3030, acc-0.5800, valid loss-0.2843, acc-0.6082, test loss-0.2874, acc-0.5918\n",
      "Iter-4010, train loss-0.3073, acc-0.6000, valid loss-0.2839, acc-0.6090, test loss-0.2873, acc-0.5930\n",
      "Iter-4020, train loss-0.3178, acc-0.5300, valid loss-0.2836, acc-0.6144, test loss-0.2872, acc-0.5991\n",
      "Iter-4030, train loss-0.2733, acc-0.6400, valid loss-0.2836, acc-0.6162, test loss-0.2872, acc-0.6009\n",
      "Iter-4040, train loss-0.2620, acc-0.7500, valid loss-0.2834, acc-0.6228, test loss-0.2872, acc-0.6076\n",
      "Iter-4050, train loss-0.3072, acc-0.5700, valid loss-0.2830, acc-0.6162, test loss-0.2868, acc-0.6022\n",
      "Iter-4060, train loss-0.2569, acc-0.6400, valid loss-0.2827, acc-0.6254, test loss-0.2868, acc-0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.2791, acc-0.6200, valid loss-0.2827, acc-0.6220, test loss-0.2869, acc-0.6093\n",
      "Iter-4080, train loss-0.3130, acc-0.5700, valid loss-0.2828, acc-0.6264, test loss-0.2870, acc-0.6106\n",
      "Iter-4090, train loss-0.2906, acc-0.6100, valid loss-0.2827, acc-0.6240, test loss-0.2869, acc-0.6086\n",
      "Iter-4100, train loss-0.3032, acc-0.5800, valid loss-0.2825, acc-0.6258, test loss-0.2866, acc-0.6085\n",
      "Iter-4110, train loss-0.3404, acc-0.5300, valid loss-0.2826, acc-0.6276, test loss-0.2866, acc-0.6111\n",
      "Iter-4120, train loss-0.2976, acc-0.6300, valid loss-0.2822, acc-0.6280, test loss-0.2863, acc-0.6112\n",
      "Iter-4130, train loss-0.2958, acc-0.6300, valid loss-0.2818, acc-0.6294, test loss-0.2860, acc-0.6125\n",
      "Iter-4140, train loss-0.3241, acc-0.5400, valid loss-0.2820, acc-0.6216, test loss-0.2860, acc-0.6050\n",
      "Iter-4150, train loss-0.2736, acc-0.6600, valid loss-0.2812, acc-0.6272, test loss-0.2854, acc-0.6108\n",
      "Iter-4160, train loss-0.3132, acc-0.6200, valid loss-0.2809, acc-0.6238, test loss-0.2854, acc-0.6091\n",
      "Iter-4170, train loss-0.2993, acc-0.5900, valid loss-0.2806, acc-0.6254, test loss-0.2850, acc-0.6115\n",
      "Iter-4180, train loss-0.3038, acc-0.5900, valid loss-0.2804, acc-0.6252, test loss-0.2847, acc-0.6101\n",
      "Iter-4190, train loss-0.2609, acc-0.6700, valid loss-0.2803, acc-0.6272, test loss-0.2847, acc-0.6115\n",
      "Iter-4200, train loss-0.3240, acc-0.5800, valid loss-0.2801, acc-0.6292, test loss-0.2846, acc-0.6127\n",
      "Iter-4210, train loss-0.2967, acc-0.5900, valid loss-0.2800, acc-0.6296, test loss-0.2846, acc-0.6131\n",
      "Iter-4220, train loss-0.3048, acc-0.5400, valid loss-0.2797, acc-0.6288, test loss-0.2845, acc-0.6137\n",
      "Iter-4230, train loss-0.2880, acc-0.6000, valid loss-0.2797, acc-0.6274, test loss-0.2844, acc-0.6123\n",
      "Iter-4240, train loss-0.2951, acc-0.5400, valid loss-0.2797, acc-0.6246, test loss-0.2843, acc-0.6086\n",
      "Iter-4250, train loss-0.2924, acc-0.5900, valid loss-0.2793, acc-0.6316, test loss-0.2841, acc-0.6159\n",
      "Iter-4260, train loss-0.2673, acc-0.6700, valid loss-0.2793, acc-0.6318, test loss-0.2840, acc-0.6151\n",
      "Iter-4270, train loss-0.2721, acc-0.6000, valid loss-0.2788, acc-0.6332, test loss-0.2837, acc-0.6173\n",
      "Iter-4280, train loss-0.2710, acc-0.6700, valid loss-0.2791, acc-0.6308, test loss-0.2841, acc-0.6150\n",
      "Iter-4290, train loss-0.2814, acc-0.6600, valid loss-0.2793, acc-0.6266, test loss-0.2843, acc-0.6122\n",
      "Iter-4300, train loss-0.2620, acc-0.6700, valid loss-0.2792, acc-0.6296, test loss-0.2843, acc-0.6141\n",
      "Iter-4310, train loss-0.2612, acc-0.6800, valid loss-0.2789, acc-0.6274, test loss-0.2841, acc-0.6127\n",
      "Iter-4320, train loss-0.3293, acc-0.5300, valid loss-0.2786, acc-0.6292, test loss-0.2840, acc-0.6135\n",
      "Iter-4330, train loss-0.2735, acc-0.6500, valid loss-0.2782, acc-0.6278, test loss-0.2838, acc-0.6130\n",
      "Iter-4340, train loss-0.2976, acc-0.5900, valid loss-0.2783, acc-0.6274, test loss-0.2839, acc-0.6122\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 5000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
