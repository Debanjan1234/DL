{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((55000, 784), (5000, 784), (10000, 784))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import impl.layer as l\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.grads = []\n",
    "        self.W_fixed = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), \n",
    "                 b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                     b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), \n",
    "                 b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "\n",
    "        return dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches = []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b'])\n",
    "        y, _ = l.tanh_forward(X=y)\n",
    "        X = y.copy() # pass to the next layer\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches = []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, _ = l.tanh_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "        if train:\n",
    "            caches.append(fc_caches) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches\n",
    "\n",
    "    def onehot(self, labels):\n",
    "        # y = np.zeros([labels.size, np.max(labels) + 1])\n",
    "        y = np.zeros([labels.size, self.C])\n",
    "        y[range(labels.size), labels] = 1.\n",
    "        return y\n",
    "\n",
    "    def squared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        data_loss = 0.5 * np.sum((y_pred - self.onehot(y_train))**2)/ m\n",
    "        return data_loss\n",
    "\n",
    "    def dsquared_loss(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "        grad_y = (y_pred - self.onehot(y_train))/ m\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.squared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        dy = self.dsquared_loss(y_pred=y_prob, y_train=y_train)\n",
    "        \n",
    "        return loss, dy\n",
    "\n",
    "    def train_backward(self, dy, caches):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "        dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "        self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        dy =  dy @ self.W_fixed[2].T\n",
    "        dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "            dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer])\n",
    "            dy =  dy @ self.W_fixed[1][layer].T\n",
    "            dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache = caches[0]\n",
    "        dy = dy - dy_prev # temporal diff instead of differentiable function\n",
    "        dW, db = self.fc_backward(dout=dy, cache=fc_cache)\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            grads = self.train_backward(dy, caches)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "            # print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "            # format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-0.4522, acc-0.0400, valid loss-0.4525, acc-0.0738, test loss-0.4521, acc-0.0679\n",
      "Iter-20, train loss-0.4528, acc-0.0300, valid loss-0.4519, acc-0.0816, test loss-0.4515, acc-0.0772\n",
      "Iter-30, train loss-0.4520, acc-0.0700, valid loss-0.4514, acc-0.0878, test loss-0.4511, acc-0.0866\n",
      "Iter-40, train loss-0.4508, acc-0.1300, valid loss-0.4509, acc-0.0960, test loss-0.4506, acc-0.0952\n",
      "Iter-50, train loss-0.4494, acc-0.1300, valid loss-0.4503, acc-0.1106, test loss-0.4499, acc-0.1079\n",
      "Iter-60, train loss-0.4494, acc-0.1800, valid loss-0.4497, acc-0.1162, test loss-0.4494, acc-0.1146\n",
      "Iter-70, train loss-0.4513, acc-0.0600, valid loss-0.4491, acc-0.1262, test loss-0.4487, acc-0.1244\n",
      "Iter-80, train loss-0.4483, acc-0.1400, valid loss-0.4484, acc-0.1358, test loss-0.4480, acc-0.1346\n",
      "Iter-90, train loss-0.4468, acc-0.1000, valid loss-0.4477, acc-0.1458, test loss-0.4473, acc-0.1472\n",
      "Iter-100, train loss-0.4474, acc-0.1900, valid loss-0.4468, acc-0.1600, test loss-0.4465, acc-0.1578\n",
      "Iter-110, train loss-0.4487, acc-0.1100, valid loss-0.4459, acc-0.1682, test loss-0.4456, acc-0.1670\n",
      "Iter-120, train loss-0.4434, acc-0.2100, valid loss-0.4451, acc-0.1770, test loss-0.4448, acc-0.1783\n",
      "Iter-130, train loss-0.4442, acc-0.1700, valid loss-0.4442, acc-0.1896, test loss-0.4439, acc-0.1918\n",
      "Iter-140, train loss-0.4429, acc-0.1800, valid loss-0.4432, acc-0.2022, test loss-0.4429, acc-0.2031\n",
      "Iter-150, train loss-0.4429, acc-0.2800, valid loss-0.4421, acc-0.2118, test loss-0.4420, acc-0.2125\n",
      "Iter-160, train loss-0.4427, acc-0.2000, valid loss-0.4413, acc-0.2162, test loss-0.4412, acc-0.2166\n",
      "Iter-170, train loss-0.4394, acc-0.2600, valid loss-0.4401, acc-0.2274, test loss-0.4400, acc-0.2289\n",
      "Iter-180, train loss-0.4386, acc-0.2100, valid loss-0.4390, acc-0.2370, test loss-0.4389, acc-0.2364\n",
      "Iter-190, train loss-0.4376, acc-0.2500, valid loss-0.4375, acc-0.2566, test loss-0.4374, acc-0.2507\n",
      "Iter-200, train loss-0.4351, acc-0.3100, valid loss-0.4361, acc-0.2742, test loss-0.4360, acc-0.2679\n",
      "Iter-210, train loss-0.4337, acc-0.3100, valid loss-0.4348, acc-0.2852, test loss-0.4348, acc-0.2742\n",
      "Iter-220, train loss-0.4348, acc-0.2900, valid loss-0.4332, acc-0.2962, test loss-0.4333, acc-0.2871\n",
      "Iter-230, train loss-0.4293, acc-0.3300, valid loss-0.4315, acc-0.3096, test loss-0.4317, acc-0.3000\n",
      "Iter-240, train loss-0.4297, acc-0.3000, valid loss-0.4299, acc-0.3272, test loss-0.4301, acc-0.3230\n",
      "Iter-250, train loss-0.4265, acc-0.3900, valid loss-0.4281, acc-0.3422, test loss-0.4283, acc-0.3420\n",
      "Iter-260, train loss-0.4298, acc-0.3400, valid loss-0.4265, acc-0.3528, test loss-0.4268, acc-0.3562\n",
      "Iter-270, train loss-0.4243, acc-0.4000, valid loss-0.4245, acc-0.3678, test loss-0.4249, acc-0.3761\n",
      "Iter-280, train loss-0.4236, acc-0.3400, valid loss-0.4224, acc-0.3824, test loss-0.4229, acc-0.3881\n",
      "Iter-290, train loss-0.4249, acc-0.3000, valid loss-0.4203, acc-0.4048, test loss-0.4208, acc-0.4048\n",
      "Iter-300, train loss-0.4155, acc-0.4400, valid loss-0.4183, acc-0.4174, test loss-0.4189, acc-0.4166\n",
      "Iter-310, train loss-0.4162, acc-0.4200, valid loss-0.4162, acc-0.4250, test loss-0.4169, acc-0.4209\n",
      "Iter-320, train loss-0.4122, acc-0.4800, valid loss-0.4139, acc-0.4316, test loss-0.4148, acc-0.4272\n",
      "Iter-330, train loss-0.4145, acc-0.4000, valid loss-0.4118, acc-0.4384, test loss-0.4127, acc-0.4321\n",
      "Iter-340, train loss-0.4071, acc-0.4800, valid loss-0.4094, acc-0.4460, test loss-0.4104, acc-0.4396\n",
      "Iter-350, train loss-0.4127, acc-0.5000, valid loss-0.4069, acc-0.4506, test loss-0.4081, acc-0.4405\n",
      "Iter-360, train loss-0.3987, acc-0.5000, valid loss-0.4042, acc-0.4596, test loss-0.4056, acc-0.4445\n",
      "Iter-370, train loss-0.4047, acc-0.4200, valid loss-0.4016, acc-0.4644, test loss-0.4031, acc-0.4485\n",
      "Iter-380, train loss-0.3956, acc-0.4800, valid loss-0.3989, acc-0.4768, test loss-0.4005, acc-0.4557\n",
      "Iter-390, train loss-0.4055, acc-0.4800, valid loss-0.3960, acc-0.4830, test loss-0.3977, acc-0.4606\n",
      "Iter-400, train loss-0.3899, acc-0.5200, valid loss-0.3930, acc-0.4906, test loss-0.3950, acc-0.4665\n",
      "Iter-410, train loss-0.3892, acc-0.5100, valid loss-0.3900, acc-0.4936, test loss-0.3921, acc-0.4690\n",
      "Iter-420, train loss-0.3867, acc-0.5100, valid loss-0.3868, acc-0.4996, test loss-0.3890, acc-0.4727\n",
      "Iter-430, train loss-0.3892, acc-0.4700, valid loss-0.3840, acc-0.4974, test loss-0.3864, acc-0.4748\n",
      "Iter-440, train loss-0.3937, acc-0.3900, valid loss-0.3810, acc-0.5024, test loss-0.3836, acc-0.4770\n",
      "Iter-450, train loss-0.3725, acc-0.5500, valid loss-0.3783, acc-0.5016, test loss-0.3811, acc-0.4745\n",
      "Iter-460, train loss-0.3715, acc-0.5400, valid loss-0.3760, acc-0.4980, test loss-0.3789, acc-0.4739\n",
      "Iter-470, train loss-0.3686, acc-0.5000, valid loss-0.3737, acc-0.5002, test loss-0.3769, acc-0.4733\n",
      "Iter-480, train loss-0.3809, acc-0.4500, valid loss-0.3710, acc-0.5036, test loss-0.3743, acc-0.4764\n",
      "Iter-490, train loss-0.3851, acc-0.3900, valid loss-0.3686, acc-0.5054, test loss-0.3721, acc-0.4770\n",
      "Iter-500, train loss-0.3801, acc-0.4500, valid loss-0.3661, acc-0.5074, test loss-0.3697, acc-0.4779\n",
      "Iter-510, train loss-0.3659, acc-0.5100, valid loss-0.3638, acc-0.5078, test loss-0.3676, acc-0.4772\n",
      "Iter-520, train loss-0.3653, acc-0.4600, valid loss-0.3613, acc-0.5110, test loss-0.3653, acc-0.4800\n",
      "Iter-530, train loss-0.3662, acc-0.4300, valid loss-0.3589, acc-0.5164, test loss-0.3628, acc-0.4868\n",
      "Iter-540, train loss-0.3432, acc-0.5700, valid loss-0.3563, acc-0.5176, test loss-0.3604, acc-0.4869\n",
      "Iter-550, train loss-0.3421, acc-0.5400, valid loss-0.3539, acc-0.5194, test loss-0.3581, acc-0.4907\n",
      "Iter-560, train loss-0.3577, acc-0.5200, valid loss-0.3509, acc-0.5228, test loss-0.3552, acc-0.4909\n",
      "Iter-570, train loss-0.3498, acc-0.5300, valid loss-0.3484, acc-0.5230, test loss-0.3528, acc-0.4921\n",
      "Iter-580, train loss-0.3601, acc-0.4300, valid loss-0.3457, acc-0.5262, test loss-0.3502, acc-0.4943\n",
      "Iter-590, train loss-0.3406, acc-0.5200, valid loss-0.3430, acc-0.5282, test loss-0.3476, acc-0.4969\n",
      "Iter-600, train loss-0.3691, acc-0.3400, valid loss-0.3407, acc-0.5304, test loss-0.3454, acc-0.4988\n",
      "Iter-610, train loss-0.3577, acc-0.4200, valid loss-0.3386, acc-0.5342, test loss-0.3433, acc-0.5006\n",
      "Iter-620, train loss-0.3397, acc-0.5100, valid loss-0.3360, acc-0.5390, test loss-0.3407, acc-0.5045\n",
      "Iter-630, train loss-0.3564, acc-0.4900, valid loss-0.3338, acc-0.5432, test loss-0.3386, acc-0.5090\n",
      "Iter-640, train loss-0.3518, acc-0.4900, valid loss-0.3318, acc-0.5440, test loss-0.3368, acc-0.5088\n",
      "Iter-650, train loss-0.3303, acc-0.5300, valid loss-0.3300, acc-0.5464, test loss-0.3351, acc-0.5111\n",
      "Iter-660, train loss-0.3370, acc-0.5300, valid loss-0.3277, acc-0.5558, test loss-0.3327, acc-0.5223\n",
      "Iter-670, train loss-0.3341, acc-0.5300, valid loss-0.3255, acc-0.5656, test loss-0.3304, acc-0.5300\n",
      "Iter-680, train loss-0.3201, acc-0.5900, valid loss-0.3235, acc-0.5700, test loss-0.3284, acc-0.5352\n",
      "Iter-690, train loss-0.3253, acc-0.5400, valid loss-0.3217, acc-0.5720, test loss-0.3265, acc-0.5370\n",
      "Iter-700, train loss-0.3314, acc-0.5400, valid loss-0.3192, acc-0.5798, test loss-0.3240, acc-0.5446\n",
      "Iter-710, train loss-0.3410, acc-0.5100, valid loss-0.3176, acc-0.5830, test loss-0.3226, acc-0.5468\n",
      "Iter-720, train loss-0.3242, acc-0.5200, valid loss-0.3161, acc-0.5852, test loss-0.3211, acc-0.5507\n",
      "Iter-730, train loss-0.3111, acc-0.6200, valid loss-0.3140, acc-0.5834, test loss-0.3190, acc-0.5503\n",
      "Iter-740, train loss-0.3241, acc-0.5500, valid loss-0.3114, acc-0.5902, test loss-0.3164, acc-0.5584\n",
      "Iter-750, train loss-0.3179, acc-0.5700, valid loss-0.3099, acc-0.5868, test loss-0.3151, acc-0.5547\n",
      "Iter-760, train loss-0.3059, acc-0.5800, valid loss-0.3085, acc-0.5858, test loss-0.3139, acc-0.5549\n",
      "Iter-770, train loss-0.3125, acc-0.5700, valid loss-0.3070, acc-0.5890, test loss-0.3125, acc-0.5595\n",
      "Iter-780, train loss-0.2972, acc-0.6000, valid loss-0.3054, acc-0.6012, test loss-0.3107, acc-0.5708\n",
      "Iter-790, train loss-0.3079, acc-0.5600, valid loss-0.3036, acc-0.6074, test loss-0.3089, acc-0.5770\n",
      "Iter-800, train loss-0.3131, acc-0.5600, valid loss-0.3024, acc-0.6100, test loss-0.3076, acc-0.5821\n",
      "Iter-810, train loss-0.3276, acc-0.4900, valid loss-0.3011, acc-0.6122, test loss-0.3064, acc-0.5835\n",
      "Iter-820, train loss-0.3061, acc-0.6000, valid loss-0.2996, acc-0.6134, test loss-0.3047, acc-0.5846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-0.3284, acc-0.5400, valid loss-0.2984, acc-0.6120, test loss-0.3037, acc-0.5832\n",
      "Iter-840, train loss-0.3164, acc-0.5900, valid loss-0.2973, acc-0.6122, test loss-0.3026, acc-0.5845\n",
      "Iter-850, train loss-0.3272, acc-0.5300, valid loss-0.2965, acc-0.6086, test loss-0.3018, acc-0.5823\n",
      "Iter-860, train loss-0.3130, acc-0.5800, valid loss-0.2953, acc-0.6146, test loss-0.3005, acc-0.5878\n",
      "Iter-870, train loss-0.2918, acc-0.5800, valid loss-0.2940, acc-0.6170, test loss-0.2992, acc-0.5932\n",
      "Iter-880, train loss-0.3038, acc-0.6000, valid loss-0.2931, acc-0.6182, test loss-0.2982, acc-0.5942\n",
      "Iter-890, train loss-0.2914, acc-0.6000, valid loss-0.2921, acc-0.6184, test loss-0.2972, acc-0.5944\n",
      "Iter-900, train loss-0.2856, acc-0.6500, valid loss-0.2911, acc-0.6224, test loss-0.2961, acc-0.5972\n",
      "Iter-910, train loss-0.2900, acc-0.5800, valid loss-0.2900, acc-0.6258, test loss-0.2950, acc-0.6024\n",
      "Iter-920, train loss-0.3027, acc-0.5700, valid loss-0.2892, acc-0.6272, test loss-0.2943, acc-0.6024\n",
      "Iter-930, train loss-0.3200, acc-0.5300, valid loss-0.2885, acc-0.6280, test loss-0.2936, acc-0.6040\n",
      "Iter-940, train loss-0.2690, acc-0.6400, valid loss-0.2882, acc-0.6206, test loss-0.2933, acc-0.5944\n",
      "Iter-950, train loss-0.3437, acc-0.4700, valid loss-0.2878, acc-0.6192, test loss-0.2930, acc-0.5946\n",
      "Iter-960, train loss-0.2733, acc-0.6600, valid loss-0.2865, acc-0.6224, test loss-0.2916, acc-0.5960\n",
      "Iter-970, train loss-0.2783, acc-0.6300, valid loss-0.2858, acc-0.6222, test loss-0.2909, acc-0.5972\n",
      "Iter-980, train loss-0.2940, acc-0.5900, valid loss-0.2849, acc-0.6292, test loss-0.2899, acc-0.6048\n",
      "Iter-990, train loss-0.2726, acc-0.6500, valid loss-0.2841, acc-0.6320, test loss-0.2890, acc-0.6097\n",
      "Iter-1000, train loss-0.2988, acc-0.6100, valid loss-0.2835, acc-0.6346, test loss-0.2884, acc-0.6131\n",
      "Iter-1010, train loss-0.2831, acc-0.6500, valid loss-0.2830, acc-0.6406, test loss-0.2878, acc-0.6211\n",
      "Iter-1020, train loss-0.2857, acc-0.6400, valid loss-0.2827, acc-0.6440, test loss-0.2874, acc-0.6238\n",
      "Iter-1030, train loss-0.3150, acc-0.5200, valid loss-0.2826, acc-0.6464, test loss-0.2875, acc-0.6233\n",
      "Iter-1040, train loss-0.2799, acc-0.6500, valid loss-0.2825, acc-0.6450, test loss-0.2875, acc-0.6209\n",
      "Iter-1050, train loss-0.2990, acc-0.6100, valid loss-0.2823, acc-0.6408, test loss-0.2875, acc-0.6175\n",
      "Iter-1060, train loss-0.3084, acc-0.5600, valid loss-0.2820, acc-0.6398, test loss-0.2871, acc-0.6146\n",
      "Iter-1070, train loss-0.2692, acc-0.6300, valid loss-0.2819, acc-0.6376, test loss-0.2870, acc-0.6122\n",
      "Iter-1080, train loss-0.2887, acc-0.6300, valid loss-0.2818, acc-0.6374, test loss-0.2870, acc-0.6122\n",
      "Iter-1090, train loss-0.2893, acc-0.6100, valid loss-0.2818, acc-0.6436, test loss-0.2869, acc-0.6195\n",
      "Iter-1100, train loss-0.2892, acc-0.6300, valid loss-0.2816, acc-0.6376, test loss-0.2867, acc-0.6132\n",
      "Iter-1110, train loss-0.3197, acc-0.5700, valid loss-0.2818, acc-0.6348, test loss-0.2868, acc-0.6095\n",
      "Iter-1120, train loss-0.2479, acc-0.6900, valid loss-0.2817, acc-0.6354, test loss-0.2867, acc-0.6108\n",
      "Iter-1130, train loss-0.3065, acc-0.5600, valid loss-0.2821, acc-0.6354, test loss-0.2872, acc-0.6114\n",
      "Iter-1140, train loss-0.2972, acc-0.6100, valid loss-0.2825, acc-0.6320, test loss-0.2876, acc-0.6062\n",
      "Iter-1150, train loss-0.3073, acc-0.5900, valid loss-0.2828, acc-0.6340, test loss-0.2878, acc-0.6096\n",
      "Iter-1160, train loss-0.2796, acc-0.6000, valid loss-0.2828, acc-0.6398, test loss-0.2879, acc-0.6178\n",
      "Iter-1170, train loss-0.3041, acc-0.6400, valid loss-0.2826, acc-0.6394, test loss-0.2877, acc-0.6174\n",
      "Iter-1180, train loss-0.3162, acc-0.6000, valid loss-0.2831, acc-0.6376, test loss-0.2884, acc-0.6159\n",
      "Iter-1190, train loss-0.2929, acc-0.5600, valid loss-0.2836, acc-0.6378, test loss-0.2890, acc-0.6144\n",
      "Iter-1200, train loss-0.2899, acc-0.6000, valid loss-0.2840, acc-0.6412, test loss-0.2894, acc-0.6183\n",
      "Iter-1210, train loss-0.2825, acc-0.6000, valid loss-0.2839, acc-0.6408, test loss-0.2892, acc-0.6187\n",
      "Iter-1220, train loss-0.3001, acc-0.5700, valid loss-0.2841, acc-0.6402, test loss-0.2894, acc-0.6189\n",
      "Iter-1230, train loss-0.2868, acc-0.6100, valid loss-0.2836, acc-0.6442, test loss-0.2888, acc-0.6232\n",
      "Iter-1240, train loss-0.2905, acc-0.6400, valid loss-0.2841, acc-0.6434, test loss-0.2894, acc-0.6229\n",
      "Iter-1250, train loss-0.2623, acc-0.6900, valid loss-0.2844, acc-0.6436, test loss-0.2897, acc-0.6238\n",
      "Iter-1260, train loss-0.2848, acc-0.6300, valid loss-0.2848, acc-0.6404, test loss-0.2903, acc-0.6196\n",
      "Iter-1270, train loss-0.2999, acc-0.6200, valid loss-0.2856, acc-0.6394, test loss-0.2911, acc-0.6180\n",
      "Iter-1280, train loss-0.2717, acc-0.6900, valid loss-0.2857, acc-0.6428, test loss-0.2911, acc-0.6214\n",
      "Iter-1290, train loss-0.2798, acc-0.6400, valid loss-0.2864, acc-0.6380, test loss-0.2918, acc-0.6166\n",
      "Iter-1300, train loss-0.2716, acc-0.7100, valid loss-0.2865, acc-0.6400, test loss-0.2918, acc-0.6180\n",
      "Iter-1310, train loss-0.2637, acc-0.6400, valid loss-0.2871, acc-0.6376, test loss-0.2926, acc-0.6157\n",
      "Iter-1320, train loss-0.2985, acc-0.5900, valid loss-0.2875, acc-0.6310, test loss-0.2930, acc-0.6108\n",
      "Iter-1330, train loss-0.3057, acc-0.6000, valid loss-0.2883, acc-0.6300, test loss-0.2938, acc-0.6075\n",
      "Iter-1340, train loss-0.2886, acc-0.6500, valid loss-0.2887, acc-0.6308, test loss-0.2942, acc-0.6091\n",
      "Iter-1350, train loss-0.2905, acc-0.5900, valid loss-0.2896, acc-0.6284, test loss-0.2953, acc-0.6067\n",
      "Iter-1360, train loss-0.2835, acc-0.6300, valid loss-0.2903, acc-0.6246, test loss-0.2960, acc-0.6024\n",
      "Iter-1370, train loss-0.2554, acc-0.7000, valid loss-0.2910, acc-0.6208, test loss-0.2969, acc-0.5965\n",
      "Iter-1380, train loss-0.3051, acc-0.6000, valid loss-0.2916, acc-0.6182, test loss-0.2974, acc-0.5973\n",
      "Iter-1390, train loss-0.3098, acc-0.6000, valid loss-0.2924, acc-0.6250, test loss-0.2980, acc-0.6029\n",
      "Iter-1400, train loss-0.3019, acc-0.6100, valid loss-0.2926, acc-0.6262, test loss-0.2981, acc-0.6052\n",
      "Iter-1410, train loss-0.3093, acc-0.5800, valid loss-0.2932, acc-0.6292, test loss-0.2987, acc-0.6090\n",
      "Iter-1420, train loss-0.3017, acc-0.5800, valid loss-0.2935, acc-0.6282, test loss-0.2990, acc-0.6104\n",
      "Iter-1430, train loss-0.3133, acc-0.5400, valid loss-0.2943, acc-0.6242, test loss-0.2998, acc-0.6055\n",
      "Iter-1440, train loss-0.3203, acc-0.5700, valid loss-0.2947, acc-0.6292, test loss-0.3001, acc-0.6109\n",
      "Iter-1450, train loss-0.3030, acc-0.6200, valid loss-0.2950, acc-0.6294, test loss-0.3003, acc-0.6117\n",
      "Iter-1460, train loss-0.3285, acc-0.5700, valid loss-0.2955, acc-0.6300, test loss-0.3008, acc-0.6152\n",
      "Iter-1470, train loss-0.3253, acc-0.5000, valid loss-0.2954, acc-0.6310, test loss-0.3007, acc-0.6164\n",
      "Iter-1480, train loss-0.2915, acc-0.6300, valid loss-0.2967, acc-0.6296, test loss-0.3020, acc-0.6146\n",
      "Iter-1490, train loss-0.3011, acc-0.5800, valid loss-0.2965, acc-0.6324, test loss-0.3016, acc-0.6156\n",
      "Iter-1500, train loss-0.3147, acc-0.6000, valid loss-0.2981, acc-0.6284, test loss-0.3031, acc-0.6146\n",
      "Iter-1510, train loss-0.2994, acc-0.6500, valid loss-0.2981, acc-0.6284, test loss-0.3031, acc-0.6141\n",
      "Iter-1520, train loss-0.3087, acc-0.6000, valid loss-0.2987, acc-0.6288, test loss-0.3036, acc-0.6128\n",
      "Iter-1530, train loss-0.3161, acc-0.5700, valid loss-0.2991, acc-0.6294, test loss-0.3040, acc-0.6140\n",
      "Iter-1540, train loss-0.3168, acc-0.5600, valid loss-0.2996, acc-0.6280, test loss-0.3044, acc-0.6159\n",
      "Iter-1550, train loss-0.2871, acc-0.6500, valid loss-0.3004, acc-0.6266, test loss-0.3052, acc-0.6159\n",
      "Iter-1560, train loss-0.3080, acc-0.6100, valid loss-0.3010, acc-0.6282, test loss-0.3056, acc-0.6148\n",
      "Iter-1570, train loss-0.3114, acc-0.5800, valid loss-0.3013, acc-0.6296, test loss-0.3057, acc-0.6168\n",
      "Iter-1580, train loss-0.3180, acc-0.6400, valid loss-0.3016, acc-0.6288, test loss-0.3060, acc-0.6159\n",
      "Iter-1590, train loss-0.3083, acc-0.5900, valid loss-0.3015, acc-0.6270, test loss-0.3058, acc-0.6161\n",
      "Iter-1600, train loss-0.3166, acc-0.5500, valid loss-0.3017, acc-0.6266, test loss-0.3059, acc-0.6146\n",
      "Iter-1610, train loss-0.3230, acc-0.6000, valid loss-0.3024, acc-0.6254, test loss-0.3065, acc-0.6144\n",
      "Iter-1620, train loss-0.3556, acc-0.5600, valid loss-0.3031, acc-0.6260, test loss-0.3069, acc-0.6166\n",
      "Iter-1630, train loss-0.3532, acc-0.4900, valid loss-0.3040, acc-0.6248, test loss-0.3078, acc-0.6155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-0.2929, acc-0.6800, valid loss-0.3043, acc-0.6252, test loss-0.3081, acc-0.6153\n",
      "Iter-1650, train loss-0.3122, acc-0.6500, valid loss-0.3043, acc-0.6240, test loss-0.3080, acc-0.6123\n",
      "Iter-1660, train loss-0.2935, acc-0.6800, valid loss-0.3047, acc-0.6210, test loss-0.3084, acc-0.6107\n",
      "Iter-1670, train loss-0.2963, acc-0.6600, valid loss-0.3042, acc-0.6212, test loss-0.3078, acc-0.6101\n",
      "Iter-1680, train loss-0.2884, acc-0.6400, valid loss-0.3037, acc-0.6236, test loss-0.3073, acc-0.6115\n",
      "Iter-1690, train loss-0.3027, acc-0.6400, valid loss-0.3044, acc-0.6226, test loss-0.3081, acc-0.6098\n",
      "Iter-1700, train loss-0.3284, acc-0.6200, valid loss-0.3044, acc-0.6252, test loss-0.3078, acc-0.6121\n",
      "Iter-1710, train loss-0.3195, acc-0.5900, valid loss-0.3048, acc-0.6196, test loss-0.3083, acc-0.6107\n",
      "Iter-1720, train loss-0.3052, acc-0.6700, valid loss-0.3056, acc-0.6202, test loss-0.3092, acc-0.6100\n",
      "Iter-1730, train loss-0.3005, acc-0.6600, valid loss-0.3056, acc-0.6236, test loss-0.3094, acc-0.6099\n",
      "Iter-1740, train loss-0.3541, acc-0.5500, valid loss-0.3054, acc-0.6184, test loss-0.3090, acc-0.6061\n",
      "Iter-1750, train loss-0.3063, acc-0.6100, valid loss-0.3063, acc-0.6130, test loss-0.3100, acc-0.5966\n",
      "Iter-1760, train loss-0.3050, acc-0.6000, valid loss-0.3056, acc-0.6198, test loss-0.3091, acc-0.6037\n",
      "Iter-1770, train loss-0.3600, acc-0.5100, valid loss-0.3051, acc-0.6224, test loss-0.3086, acc-0.6098\n",
      "Iter-1780, train loss-0.3296, acc-0.5500, valid loss-0.3049, acc-0.6228, test loss-0.3082, acc-0.6101\n",
      "Iter-1790, train loss-0.3021, acc-0.6200, valid loss-0.3052, acc-0.6204, test loss-0.3084, acc-0.6067\n",
      "Iter-1800, train loss-0.2834, acc-0.7000, valid loss-0.3055, acc-0.6246, test loss-0.3085, acc-0.6124\n",
      "Iter-1810, train loss-0.3071, acc-0.6400, valid loss-0.3054, acc-0.6204, test loss-0.3081, acc-0.6099\n",
      "Iter-1820, train loss-0.3331, acc-0.5900, valid loss-0.3050, acc-0.6190, test loss-0.3077, acc-0.6078\n",
      "Iter-1830, train loss-0.3371, acc-0.5700, valid loss-0.3052, acc-0.6186, test loss-0.3077, acc-0.6095\n",
      "Iter-1840, train loss-0.3309, acc-0.5700, valid loss-0.3049, acc-0.6124, test loss-0.3072, acc-0.6030\n",
      "Iter-1850, train loss-0.3165, acc-0.5800, valid loss-0.3052, acc-0.6168, test loss-0.3076, acc-0.6067\n",
      "Iter-1860, train loss-0.3030, acc-0.6300, valid loss-0.3056, acc-0.6168, test loss-0.3079, acc-0.6076\n",
      "Iter-1870, train loss-0.2882, acc-0.6200, valid loss-0.3057, acc-0.6110, test loss-0.3079, acc-0.6024\n",
      "Iter-1880, train loss-0.2894, acc-0.6500, valid loss-0.3051, acc-0.6172, test loss-0.3072, acc-0.6090\n",
      "Iter-1890, train loss-0.2538, acc-0.7100, valid loss-0.3046, acc-0.6186, test loss-0.3069, acc-0.6086\n",
      "Iter-1900, train loss-0.3050, acc-0.5800, valid loss-0.3039, acc-0.6194, test loss-0.3061, acc-0.6112\n",
      "Iter-1910, train loss-0.3133, acc-0.5600, valid loss-0.3038, acc-0.6182, test loss-0.3057, acc-0.6103\n",
      "Iter-1920, train loss-0.3292, acc-0.5300, valid loss-0.3041, acc-0.6196, test loss-0.3060, acc-0.6098\n",
      "Iter-1930, train loss-0.3000, acc-0.6400, valid loss-0.3042, acc-0.6130, test loss-0.3060, acc-0.6034\n",
      "Iter-1940, train loss-0.3623, acc-0.4800, valid loss-0.3049, acc-0.6118, test loss-0.3066, acc-0.5995\n",
      "Iter-1950, train loss-0.2976, acc-0.6100, valid loss-0.3049, acc-0.6114, test loss-0.3065, acc-0.5995\n",
      "Iter-1960, train loss-0.3202, acc-0.5700, valid loss-0.3046, acc-0.6074, test loss-0.3062, acc-0.5971\n",
      "Iter-1970, train loss-0.3213, acc-0.5500, valid loss-0.3049, acc-0.6118, test loss-0.3063, acc-0.6004\n",
      "Iter-1980, train loss-0.2866, acc-0.6400, valid loss-0.3052, acc-0.6088, test loss-0.3066, acc-0.5992\n",
      "Iter-1990, train loss-0.3296, acc-0.5300, valid loss-0.3050, acc-0.6064, test loss-0.3061, acc-0.5969\n",
      "Iter-2000, train loss-0.2937, acc-0.6100, valid loss-0.3049, acc-0.6146, test loss-0.3058, acc-0.6047\n",
      "Iter-2010, train loss-0.3121, acc-0.6000, valid loss-0.3049, acc-0.6002, test loss-0.3057, acc-0.5947\n",
      "Iter-2020, train loss-0.3434, acc-0.5400, valid loss-0.3045, acc-0.6064, test loss-0.3051, acc-0.5974\n",
      "Iter-2030, train loss-0.3414, acc-0.5400, valid loss-0.3042, acc-0.6064, test loss-0.3047, acc-0.5969\n",
      "Iter-2040, train loss-0.3234, acc-0.5800, valid loss-0.3046, acc-0.6010, test loss-0.3053, acc-0.5934\n",
      "Iter-2050, train loss-0.3237, acc-0.5500, valid loss-0.3042, acc-0.6014, test loss-0.3049, acc-0.5929\n",
      "Iter-2060, train loss-0.3060, acc-0.5700, valid loss-0.3037, acc-0.6048, test loss-0.3042, acc-0.5937\n",
      "Iter-2070, train loss-0.3050, acc-0.6000, valid loss-0.3040, acc-0.6036, test loss-0.3044, acc-0.5930\n",
      "Iter-2080, train loss-0.3224, acc-0.5700, valid loss-0.3042, acc-0.5956, test loss-0.3045, acc-0.5847\n",
      "Iter-2090, train loss-0.3124, acc-0.5300, valid loss-0.3042, acc-0.5884, test loss-0.3045, acc-0.5801\n",
      "Iter-2100, train loss-0.2917, acc-0.6100, valid loss-0.3046, acc-0.5902, test loss-0.3052, acc-0.5829\n",
      "Iter-2110, train loss-0.3357, acc-0.5300, valid loss-0.3040, acc-0.5846, test loss-0.3045, acc-0.5776\n",
      "Iter-2120, train loss-0.3201, acc-0.5900, valid loss-0.3043, acc-0.5836, test loss-0.3048, acc-0.5784\n",
      "Iter-2130, train loss-0.3053, acc-0.5500, valid loss-0.3048, acc-0.5888, test loss-0.3052, acc-0.5847\n",
      "Iter-2140, train loss-0.2975, acc-0.6200, valid loss-0.3050, acc-0.5860, test loss-0.3052, acc-0.5819\n",
      "Iter-2150, train loss-0.2943, acc-0.6100, valid loss-0.3044, acc-0.5888, test loss-0.3045, acc-0.5856\n",
      "Iter-2160, train loss-0.3336, acc-0.5200, valid loss-0.3041, acc-0.5880, test loss-0.3041, acc-0.5849\n",
      "Iter-2170, train loss-0.3213, acc-0.5600, valid loss-0.3038, acc-0.5984, test loss-0.3039, acc-0.5928\n",
      "Iter-2180, train loss-0.2827, acc-0.6000, valid loss-0.3037, acc-0.5914, test loss-0.3037, acc-0.5873\n",
      "Iter-2190, train loss-0.2961, acc-0.5500, valid loss-0.3038, acc-0.5918, test loss-0.3036, acc-0.5889\n",
      "Iter-2200, train loss-0.2914, acc-0.6100, valid loss-0.3037, acc-0.5834, test loss-0.3034, acc-0.5820\n",
      "Iter-2210, train loss-0.3129, acc-0.5200, valid loss-0.3033, acc-0.5856, test loss-0.3027, acc-0.5848\n",
      "Iter-2220, train loss-0.3067, acc-0.5600, valid loss-0.3039, acc-0.5784, test loss-0.3031, acc-0.5783\n",
      "Iter-2230, train loss-0.2824, acc-0.5900, valid loss-0.3041, acc-0.5802, test loss-0.3034, acc-0.5806\n",
      "Iter-2240, train loss-0.3176, acc-0.5400, valid loss-0.3040, acc-0.5856, test loss-0.3033, acc-0.5826\n",
      "Iter-2250, train loss-0.3288, acc-0.5200, valid loss-0.3040, acc-0.5824, test loss-0.3033, acc-0.5814\n",
      "Iter-2260, train loss-0.3322, acc-0.5700, valid loss-0.3036, acc-0.5788, test loss-0.3029, acc-0.5802\n",
      "Iter-2270, train loss-0.3147, acc-0.5900, valid loss-0.3034, acc-0.5914, test loss-0.3026, acc-0.5908\n",
      "Iter-2280, train loss-0.3280, acc-0.5200, valid loss-0.3037, acc-0.5908, test loss-0.3032, acc-0.5874\n",
      "Iter-2290, train loss-0.2990, acc-0.6400, valid loss-0.3038, acc-0.5998, test loss-0.3033, acc-0.5946\n",
      "Iter-2300, train loss-0.3111, acc-0.5300, valid loss-0.3038, acc-0.6038, test loss-0.3033, acc-0.5996\n",
      "Iter-2310, train loss-0.2809, acc-0.6000, valid loss-0.3043, acc-0.5970, test loss-0.3037, acc-0.5945\n",
      "Iter-2320, train loss-0.3302, acc-0.5300, valid loss-0.3042, acc-0.5916, test loss-0.3036, acc-0.5885\n",
      "Iter-2330, train loss-0.2932, acc-0.6500, valid loss-0.3051, acc-0.5780, test loss-0.3044, acc-0.5794\n",
      "Iter-2340, train loss-0.3030, acc-0.5400, valid loss-0.3050, acc-0.5818, test loss-0.3042, acc-0.5818\n",
      "Iter-2350, train loss-0.3528, acc-0.4400, valid loss-0.3044, acc-0.5878, test loss-0.3037, acc-0.5885\n",
      "Iter-2360, train loss-0.3002, acc-0.5900, valid loss-0.3049, acc-0.5880, test loss-0.3043, acc-0.5865\n",
      "Iter-2370, train loss-0.3094, acc-0.5800, valid loss-0.3044, acc-0.5730, test loss-0.3039, acc-0.5726\n",
      "Iter-2380, train loss-0.3122, acc-0.4900, valid loss-0.3045, acc-0.5654, test loss-0.3039, acc-0.5680\n",
      "Iter-2390, train loss-0.3018, acc-0.5800, valid loss-0.3044, acc-0.5744, test loss-0.3038, acc-0.5744\n",
      "Iter-2400, train loss-0.2779, acc-0.5800, valid loss-0.3045, acc-0.5864, test loss-0.3038, acc-0.5853\n",
      "Iter-2410, train loss-0.3516, acc-0.4900, valid loss-0.3043, acc-0.5862, test loss-0.3036, acc-0.5863\n",
      "Iter-2420, train loss-0.2645, acc-0.7100, valid loss-0.3038, acc-0.5826, test loss-0.3034, acc-0.5812\n",
      "Iter-2430, train loss-0.2980, acc-0.6100, valid loss-0.3032, acc-0.5918, test loss-0.3030, acc-0.5904\n",
      "Iter-2440, train loss-0.2983, acc-0.5500, valid loss-0.3027, acc-0.5860, test loss-0.3025, acc-0.5837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-0.3362, acc-0.4600, valid loss-0.3024, acc-0.5814, test loss-0.3020, acc-0.5794\n",
      "Iter-2460, train loss-0.3167, acc-0.5600, valid loss-0.3022, acc-0.5880, test loss-0.3016, acc-0.5855\n",
      "Iter-2470, train loss-0.3076, acc-0.5800, valid loss-0.3024, acc-0.5728, test loss-0.3019, acc-0.5739\n",
      "Iter-2480, train loss-0.3070, acc-0.4900, valid loss-0.3025, acc-0.5786, test loss-0.3020, acc-0.5782\n",
      "Iter-2490, train loss-0.2714, acc-0.6500, valid loss-0.3028, acc-0.5622, test loss-0.3025, acc-0.5636\n",
      "Iter-2500, train loss-0.2999, acc-0.5400, valid loss-0.3025, acc-0.5686, test loss-0.3021, acc-0.5709\n",
      "Iter-2510, train loss-0.3114, acc-0.5100, valid loss-0.3022, acc-0.5694, test loss-0.3017, acc-0.5737\n",
      "Iter-2520, train loss-0.3182, acc-0.5700, valid loss-0.3018, acc-0.5646, test loss-0.3014, acc-0.5679\n",
      "Iter-2530, train loss-0.3164, acc-0.6100, valid loss-0.3016, acc-0.5586, test loss-0.3011, acc-0.5599\n",
      "Iter-2540, train loss-0.2986, acc-0.5200, valid loss-0.3018, acc-0.5604, test loss-0.3014, acc-0.5605\n",
      "Iter-2550, train loss-0.3237, acc-0.5000, valid loss-0.3014, acc-0.5552, test loss-0.3011, acc-0.5565\n",
      "Iter-2560, train loss-0.3071, acc-0.5900, valid loss-0.3016, acc-0.5554, test loss-0.3014, acc-0.5572\n",
      "Iter-2570, train loss-0.2891, acc-0.6100, valid loss-0.3022, acc-0.5430, test loss-0.3019, acc-0.5473\n",
      "Iter-2580, train loss-0.2940, acc-0.5500, valid loss-0.3022, acc-0.5422, test loss-0.3019, acc-0.5449\n",
      "Iter-2590, train loss-0.3011, acc-0.6000, valid loss-0.3023, acc-0.5430, test loss-0.3017, acc-0.5481\n",
      "Iter-2600, train loss-0.2853, acc-0.5600, valid loss-0.3026, acc-0.5446, test loss-0.3018, acc-0.5487\n",
      "Iter-2610, train loss-0.3156, acc-0.5100, valid loss-0.3021, acc-0.5450, test loss-0.3013, acc-0.5491\n",
      "Iter-2620, train loss-0.3051, acc-0.5100, valid loss-0.3024, acc-0.5402, test loss-0.3017, acc-0.5444\n",
      "Iter-2630, train loss-0.2849, acc-0.5300, valid loss-0.3019, acc-0.5372, test loss-0.3012, acc-0.5410\n",
      "Iter-2640, train loss-0.3252, acc-0.4900, valid loss-0.3015, acc-0.5438, test loss-0.3009, acc-0.5480\n",
      "Iter-2650, train loss-0.2975, acc-0.5500, valid loss-0.3020, acc-0.5454, test loss-0.3012, acc-0.5484\n",
      "Iter-2660, train loss-0.3158, acc-0.5100, valid loss-0.3017, acc-0.5438, test loss-0.3011, acc-0.5475\n",
      "Iter-2670, train loss-0.3049, acc-0.5300, valid loss-0.3014, acc-0.5576, test loss-0.3010, acc-0.5597\n",
      "Iter-2680, train loss-0.3123, acc-0.5500, valid loss-0.3009, acc-0.5600, test loss-0.3002, acc-0.5612\n",
      "Iter-2690, train loss-0.3292, acc-0.5400, valid loss-0.3004, acc-0.5534, test loss-0.2999, acc-0.5562\n",
      "Iter-2700, train loss-0.3148, acc-0.5100, valid loss-0.2996, acc-0.5592, test loss-0.2990, acc-0.5604\n",
      "Iter-2710, train loss-0.2833, acc-0.5900, valid loss-0.2996, acc-0.5572, test loss-0.2990, acc-0.5592\n",
      "Iter-2720, train loss-0.2503, acc-0.6100, valid loss-0.2991, acc-0.5568, test loss-0.2988, acc-0.5587\n",
      "Iter-2730, train loss-0.2809, acc-0.6000, valid loss-0.2990, acc-0.5590, test loss-0.2986, acc-0.5615\n",
      "Iter-2740, train loss-0.3089, acc-0.5700, valid loss-0.2987, acc-0.5600, test loss-0.2985, acc-0.5634\n",
      "Iter-2750, train loss-0.2943, acc-0.6000, valid loss-0.2987, acc-0.5576, test loss-0.2983, acc-0.5615\n",
      "Iter-2760, train loss-0.3091, acc-0.6000, valid loss-0.2988, acc-0.5618, test loss-0.2984, acc-0.5635\n",
      "Iter-2770, train loss-0.3221, acc-0.5400, valid loss-0.2982, acc-0.5582, test loss-0.2979, acc-0.5612\n",
      "Iter-2780, train loss-0.3065, acc-0.5800, valid loss-0.2982, acc-0.5572, test loss-0.2979, acc-0.5629\n",
      "Iter-2790, train loss-0.3003, acc-0.5100, valid loss-0.2981, acc-0.5546, test loss-0.2977, acc-0.5579\n",
      "Iter-2800, train loss-0.2864, acc-0.5600, valid loss-0.2985, acc-0.5470, test loss-0.2982, acc-0.5489\n",
      "Iter-2810, train loss-0.3150, acc-0.5300, valid loss-0.2985, acc-0.5412, test loss-0.2983, acc-0.5465\n",
      "Iter-2820, train loss-0.3275, acc-0.4800, valid loss-0.2987, acc-0.5410, test loss-0.2984, acc-0.5429\n",
      "Iter-2830, train loss-0.3261, acc-0.4500, valid loss-0.2991, acc-0.5340, test loss-0.2989, acc-0.5362\n",
      "Iter-2840, train loss-0.3233, acc-0.4900, valid loss-0.2991, acc-0.5328, test loss-0.2988, acc-0.5347\n",
      "Iter-2850, train loss-0.2955, acc-0.5700, valid loss-0.2996, acc-0.5246, test loss-0.2994, acc-0.5271\n",
      "Iter-2860, train loss-0.3050, acc-0.5100, valid loss-0.2992, acc-0.5286, test loss-0.2991, acc-0.5293\n",
      "Iter-2870, train loss-0.2848, acc-0.5500, valid loss-0.2993, acc-0.5262, test loss-0.2992, acc-0.5282\n",
      "Iter-2880, train loss-0.2801, acc-0.5600, valid loss-0.2992, acc-0.5262, test loss-0.2991, acc-0.5263\n",
      "Iter-2890, train loss-0.2969, acc-0.5000, valid loss-0.2987, acc-0.5314, test loss-0.2986, acc-0.5339\n",
      "Iter-2900, train loss-0.3075, acc-0.5600, valid loss-0.2990, acc-0.5302, test loss-0.2989, acc-0.5337\n",
      "Iter-2910, train loss-0.2941, acc-0.5700, valid loss-0.2983, acc-0.5300, test loss-0.2985, acc-0.5336\n",
      "Iter-2920, train loss-0.2901, acc-0.5900, valid loss-0.2976, acc-0.5334, test loss-0.2979, acc-0.5368\n",
      "Iter-2930, train loss-0.2576, acc-0.6200, valid loss-0.2980, acc-0.5308, test loss-0.2983, acc-0.5346\n",
      "Iter-2940, train loss-0.2989, acc-0.5200, valid loss-0.2978, acc-0.5358, test loss-0.2982, acc-0.5369\n",
      "Iter-2950, train loss-0.3264, acc-0.4400, valid loss-0.2973, acc-0.5398, test loss-0.2975, acc-0.5421\n",
      "Iter-2960, train loss-0.3263, acc-0.5200, valid loss-0.2972, acc-0.5404, test loss-0.2973, acc-0.5429\n",
      "Iter-2970, train loss-0.3227, acc-0.4800, valid loss-0.2974, acc-0.5338, test loss-0.2976, acc-0.5371\n",
      "Iter-2980, train loss-0.3362, acc-0.4600, valid loss-0.2972, acc-0.5362, test loss-0.2976, acc-0.5385\n",
      "Iter-2990, train loss-0.3080, acc-0.5000, valid loss-0.2975, acc-0.5334, test loss-0.2979, acc-0.5373\n",
      "Iter-3000, train loss-0.2756, acc-0.5400, valid loss-0.2975, acc-0.5316, test loss-0.2980, acc-0.5369\n",
      "Iter-3010, train loss-0.3262, acc-0.5300, valid loss-0.2974, acc-0.5298, test loss-0.2978, acc-0.5349\n",
      "Iter-3020, train loss-0.2832, acc-0.5800, valid loss-0.2969, acc-0.5310, test loss-0.2976, acc-0.5376\n",
      "Iter-3030, train loss-0.3019, acc-0.6000, valid loss-0.2964, acc-0.5330, test loss-0.2971, acc-0.5375\n",
      "Iter-3040, train loss-0.2775, acc-0.6000, valid loss-0.2959, acc-0.5380, test loss-0.2967, acc-0.5442\n",
      "Iter-3050, train loss-0.2825, acc-0.6000, valid loss-0.2961, acc-0.5320, test loss-0.2969, acc-0.5382\n",
      "Iter-3060, train loss-0.3078, acc-0.5600, valid loss-0.2959, acc-0.5314, test loss-0.2966, acc-0.5347\n",
      "Iter-3070, train loss-0.2762, acc-0.5600, valid loss-0.2955, acc-0.5304, test loss-0.2962, acc-0.5341\n",
      "Iter-3080, train loss-0.3401, acc-0.4700, valid loss-0.2954, acc-0.5292, test loss-0.2962, acc-0.5312\n",
      "Iter-3090, train loss-0.3148, acc-0.4800, valid loss-0.2949, acc-0.5324, test loss-0.2956, acc-0.5346\n",
      "Iter-3100, train loss-0.3256, acc-0.4500, valid loss-0.2953, acc-0.5318, test loss-0.2960, acc-0.5340\n",
      "Iter-3110, train loss-0.3354, acc-0.4400, valid loss-0.2953, acc-0.5310, test loss-0.2960, acc-0.5334\n",
      "Iter-3120, train loss-0.2864, acc-0.5600, valid loss-0.2952, acc-0.5346, test loss-0.2959, acc-0.5370\n",
      "Iter-3130, train loss-0.2775, acc-0.5200, valid loss-0.2946, acc-0.5376, test loss-0.2954, acc-0.5409\n",
      "Iter-3140, train loss-0.3140, acc-0.5000, valid loss-0.2942, acc-0.5378, test loss-0.2951, acc-0.5392\n",
      "Iter-3150, train loss-0.3047, acc-0.4700, valid loss-0.2939, acc-0.5364, test loss-0.2950, acc-0.5384\n",
      "Iter-3160, train loss-0.2867, acc-0.5800, valid loss-0.2942, acc-0.5350, test loss-0.2953, acc-0.5343\n",
      "Iter-3170, train loss-0.2753, acc-0.5800, valid loss-0.2939, acc-0.5342, test loss-0.2951, acc-0.5375\n",
      "Iter-3180, train loss-0.2834, acc-0.5800, valid loss-0.2941, acc-0.5336, test loss-0.2951, acc-0.5372\n",
      "Iter-3190, train loss-0.3379, acc-0.4100, valid loss-0.2942, acc-0.5366, test loss-0.2952, acc-0.5384\n",
      "Iter-3200, train loss-0.3011, acc-0.4700, valid loss-0.2940, acc-0.5400, test loss-0.2950, acc-0.5415\n",
      "Iter-3210, train loss-0.3260, acc-0.4800, valid loss-0.2937, acc-0.5446, test loss-0.2949, acc-0.5462\n",
      "Iter-3220, train loss-0.2857, acc-0.6000, valid loss-0.2941, acc-0.5366, test loss-0.2954, acc-0.5414\n",
      "Iter-3230, train loss-0.3120, acc-0.4800, valid loss-0.2941, acc-0.5396, test loss-0.2956, acc-0.5426\n",
      "Iter-3240, train loss-0.3123, acc-0.5300, valid loss-0.2943, acc-0.5348, test loss-0.2958, acc-0.5388\n",
      "Iter-3250, train loss-0.2830, acc-0.5000, valid loss-0.2942, acc-0.5368, test loss-0.2957, acc-0.5399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-0.3163, acc-0.4800, valid loss-0.2937, acc-0.5438, test loss-0.2951, acc-0.5447\n",
      "Iter-3270, train loss-0.2992, acc-0.5200, valid loss-0.2940, acc-0.5450, test loss-0.2953, acc-0.5460\n",
      "Iter-3280, train loss-0.2934, acc-0.5800, valid loss-0.2939, acc-0.5498, test loss-0.2951, acc-0.5506\n",
      "Iter-3290, train loss-0.2796, acc-0.5800, valid loss-0.2938, acc-0.5518, test loss-0.2951, acc-0.5536\n",
      "Iter-3300, train loss-0.2815, acc-0.5500, valid loss-0.2937, acc-0.5598, test loss-0.2950, acc-0.5599\n",
      "Iter-3310, train loss-0.2750, acc-0.5900, valid loss-0.2937, acc-0.5564, test loss-0.2950, acc-0.5567\n",
      "Iter-3320, train loss-0.3100, acc-0.5000, valid loss-0.2932, acc-0.5596, test loss-0.2947, acc-0.5596\n",
      "Iter-3330, train loss-0.2932, acc-0.5400, valid loss-0.2932, acc-0.5616, test loss-0.2947, acc-0.5612\n",
      "Iter-3340, train loss-0.3021, acc-0.5800, valid loss-0.2928, acc-0.5546, test loss-0.2944, acc-0.5576\n",
      "Iter-3350, train loss-0.3244, acc-0.4800, valid loss-0.2923, acc-0.5654, test loss-0.2940, acc-0.5650\n",
      "Iter-3360, train loss-0.3119, acc-0.5300, valid loss-0.2921, acc-0.5626, test loss-0.2938, acc-0.5623\n",
      "Iter-3370, train loss-0.2947, acc-0.6100, valid loss-0.2920, acc-0.5556, test loss-0.2939, acc-0.5588\n",
      "Iter-3380, train loss-0.3176, acc-0.5000, valid loss-0.2920, acc-0.5536, test loss-0.2939, acc-0.5575\n",
      "Iter-3390, train loss-0.2818, acc-0.5900, valid loss-0.2918, acc-0.5552, test loss-0.2938, acc-0.5570\n",
      "Iter-3400, train loss-0.3032, acc-0.5400, valid loss-0.2916, acc-0.5548, test loss-0.2937, acc-0.5555\n",
      "Iter-3410, train loss-0.2766, acc-0.6100, valid loss-0.2914, acc-0.5554, test loss-0.2936, acc-0.5554\n",
      "Iter-3420, train loss-0.3306, acc-0.4900, valid loss-0.2913, acc-0.5606, test loss-0.2934, acc-0.5601\n",
      "Iter-3430, train loss-0.2954, acc-0.5500, valid loss-0.2910, acc-0.5552, test loss-0.2931, acc-0.5566\n",
      "Iter-3440, train loss-0.2751, acc-0.5500, valid loss-0.2907, acc-0.5542, test loss-0.2929, acc-0.5558\n",
      "Iter-3450, train loss-0.2836, acc-0.5900, valid loss-0.2909, acc-0.5462, test loss-0.2930, acc-0.5497\n",
      "Iter-3460, train loss-0.3066, acc-0.5600, valid loss-0.2910, acc-0.5498, test loss-0.2931, acc-0.5536\n",
      "Iter-3470, train loss-0.3035, acc-0.5000, valid loss-0.2904, acc-0.5538, test loss-0.2926, acc-0.5587\n",
      "Iter-3480, train loss-0.2573, acc-0.6300, valid loss-0.2904, acc-0.5528, test loss-0.2926, acc-0.5572\n",
      "Iter-3490, train loss-0.3133, acc-0.5400, valid loss-0.2902, acc-0.5542, test loss-0.2924, acc-0.5596\n",
      "Iter-3500, train loss-0.2750, acc-0.6300, valid loss-0.2901, acc-0.5510, test loss-0.2923, acc-0.5559\n",
      "Iter-3510, train loss-0.2813, acc-0.5500, valid loss-0.2898, acc-0.5534, test loss-0.2921, acc-0.5591\n",
      "Iter-3520, train loss-0.2995, acc-0.5300, valid loss-0.2899, acc-0.5524, test loss-0.2922, acc-0.5553\n",
      "Iter-3530, train loss-0.2951, acc-0.5700, valid loss-0.2897, acc-0.5544, test loss-0.2921, acc-0.5587\n",
      "Iter-3540, train loss-0.2958, acc-0.5400, valid loss-0.2896, acc-0.5556, test loss-0.2921, acc-0.5578\n",
      "Iter-3550, train loss-0.3091, acc-0.5300, valid loss-0.2893, acc-0.5604, test loss-0.2918, acc-0.5637\n",
      "Iter-3560, train loss-0.3024, acc-0.5700, valid loss-0.2891, acc-0.5646, test loss-0.2917, acc-0.5657\n",
      "Iter-3570, train loss-0.3008, acc-0.5100, valid loss-0.2889, acc-0.5726, test loss-0.2916, acc-0.5762\n",
      "Iter-3580, train loss-0.2985, acc-0.5500, valid loss-0.2888, acc-0.5590, test loss-0.2916, acc-0.5627\n",
      "Iter-3590, train loss-0.3001, acc-0.5600, valid loss-0.2889, acc-0.5596, test loss-0.2916, acc-0.5628\n",
      "Iter-3600, train loss-0.3165, acc-0.5000, valid loss-0.2891, acc-0.5586, test loss-0.2918, acc-0.5620\n",
      "Iter-3610, train loss-0.2608, acc-0.6200, valid loss-0.2892, acc-0.5554, test loss-0.2918, acc-0.5578\n",
      "Iter-3620, train loss-0.2814, acc-0.5800, valid loss-0.2893, acc-0.5542, test loss-0.2920, acc-0.5563\n",
      "Iter-3630, train loss-0.3506, acc-0.4000, valid loss-0.2895, acc-0.5470, test loss-0.2920, acc-0.5513\n",
      "Iter-3640, train loss-0.3243, acc-0.4700, valid loss-0.2893, acc-0.5526, test loss-0.2919, acc-0.5570\n",
      "Iter-3650, train loss-0.3238, acc-0.4800, valid loss-0.2894, acc-0.5448, test loss-0.2921, acc-0.5517\n",
      "Iter-3660, train loss-0.3112, acc-0.5400, valid loss-0.2891, acc-0.5556, test loss-0.2920, acc-0.5580\n",
      "Iter-3670, train loss-0.2970, acc-0.5300, valid loss-0.2890, acc-0.5532, test loss-0.2918, acc-0.5578\n",
      "Iter-3680, train loss-0.2494, acc-0.6300, valid loss-0.2887, acc-0.5840, test loss-0.2916, acc-0.5787\n",
      "Iter-3690, train loss-0.2901, acc-0.5700, valid loss-0.2888, acc-0.5582, test loss-0.2917, acc-0.5615\n",
      "Iter-3700, train loss-0.3205, acc-0.5700, valid loss-0.2884, acc-0.5890, test loss-0.2915, acc-0.5800\n",
      "Iter-3710, train loss-0.3395, acc-0.5000, valid loss-0.2879, acc-0.5966, test loss-0.2910, acc-0.5875\n",
      "Iter-3720, train loss-0.2995, acc-0.6200, valid loss-0.2878, acc-0.6002, test loss-0.2908, acc-0.5888\n",
      "Iter-3730, train loss-0.2861, acc-0.6500, valid loss-0.2878, acc-0.5924, test loss-0.2909, acc-0.5814\n",
      "Iter-3740, train loss-0.2870, acc-0.5600, valid loss-0.2876, acc-0.5928, test loss-0.2907, acc-0.5823\n",
      "Iter-3750, train loss-0.2948, acc-0.5900, valid loss-0.2875, acc-0.5974, test loss-0.2906, acc-0.5869\n",
      "Iter-3760, train loss-0.3164, acc-0.5900, valid loss-0.2877, acc-0.5966, test loss-0.2910, acc-0.5835\n",
      "Iter-3770, train loss-0.2980, acc-0.6300, valid loss-0.2877, acc-0.5636, test loss-0.2910, acc-0.5642\n",
      "Iter-3780, train loss-0.2959, acc-0.5200, valid loss-0.2875, acc-0.5938, test loss-0.2908, acc-0.5861\n",
      "Iter-3790, train loss-0.2988, acc-0.5800, valid loss-0.2872, acc-0.5890, test loss-0.2905, acc-0.5838\n",
      "Iter-3800, train loss-0.2762, acc-0.6200, valid loss-0.2873, acc-0.5582, test loss-0.2906, acc-0.5624\n",
      "Iter-3810, train loss-0.3271, acc-0.5400, valid loss-0.2869, acc-0.5636, test loss-0.2902, acc-0.5668\n",
      "Iter-3820, train loss-0.2647, acc-0.6100, valid loss-0.2868, acc-0.5624, test loss-0.2901, acc-0.5660\n",
      "Iter-3830, train loss-0.3022, acc-0.5400, valid loss-0.2864, acc-0.5614, test loss-0.2899, acc-0.5652\n",
      "Iter-3840, train loss-0.3033, acc-0.5000, valid loss-0.2862, acc-0.5792, test loss-0.2899, acc-0.5781\n",
      "Iter-3850, train loss-0.2895, acc-0.4900, valid loss-0.2859, acc-0.5936, test loss-0.2897, acc-0.5816\n",
      "Iter-3860, train loss-0.3060, acc-0.5500, valid loss-0.2859, acc-0.5980, test loss-0.2896, acc-0.5847\n",
      "Iter-3870, train loss-0.2716, acc-0.6200, valid loss-0.2857, acc-0.5860, test loss-0.2893, acc-0.5817\n",
      "Iter-3880, train loss-0.2596, acc-0.6600, valid loss-0.2854, acc-0.6040, test loss-0.2890, acc-0.5880\n",
      "Iter-3890, train loss-0.2791, acc-0.6400, valid loss-0.2857, acc-0.6024, test loss-0.2892, acc-0.5889\n",
      "Iter-3900, train loss-0.3087, acc-0.5600, valid loss-0.2853, acc-0.5690, test loss-0.2888, acc-0.5698\n",
      "Iter-3910, train loss-0.3008, acc-0.5400, valid loss-0.2852, acc-0.5940, test loss-0.2887, acc-0.5862\n",
      "Iter-3920, train loss-0.2605, acc-0.5500, valid loss-0.2851, acc-0.5734, test loss-0.2886, acc-0.5734\n",
      "Iter-3930, train loss-0.3174, acc-0.5000, valid loss-0.2848, acc-0.5758, test loss-0.2885, acc-0.5742\n",
      "Iter-3940, train loss-0.2946, acc-0.5800, valid loss-0.2848, acc-0.5736, test loss-0.2883, acc-0.5750\n",
      "Iter-3950, train loss-0.3042, acc-0.5900, valid loss-0.2846, acc-0.6046, test loss-0.2882, acc-0.5918\n",
      "Iter-3960, train loss-0.2680, acc-0.6200, valid loss-0.2847, acc-0.6034, test loss-0.2882, acc-0.5910\n",
      "Iter-3970, train loss-0.2928, acc-0.5400, valid loss-0.2845, acc-0.6050, test loss-0.2879, acc-0.5916\n",
      "Iter-3980, train loss-0.2744, acc-0.5700, valid loss-0.2847, acc-0.5974, test loss-0.2879, acc-0.5899\n",
      "Iter-3990, train loss-0.3022, acc-0.5700, valid loss-0.2842, acc-0.6100, test loss-0.2876, acc-0.5952\n",
      "Iter-4000, train loss-0.3030, acc-0.5800, valid loss-0.2843, acc-0.6082, test loss-0.2874, acc-0.5918\n",
      "Iter-4010, train loss-0.3073, acc-0.6000, valid loss-0.2839, acc-0.6090, test loss-0.2873, acc-0.5930\n",
      "Iter-4020, train loss-0.3178, acc-0.5300, valid loss-0.2836, acc-0.6144, test loss-0.2872, acc-0.5991\n",
      "Iter-4030, train loss-0.2733, acc-0.6400, valid loss-0.2836, acc-0.6162, test loss-0.2872, acc-0.6009\n",
      "Iter-4040, train loss-0.2620, acc-0.7500, valid loss-0.2834, acc-0.6228, test loss-0.2872, acc-0.6076\n",
      "Iter-4050, train loss-0.3072, acc-0.5700, valid loss-0.2830, acc-0.6162, test loss-0.2868, acc-0.6022\n",
      "Iter-4060, train loss-0.2569, acc-0.6400, valid loss-0.2827, acc-0.6254, test loss-0.2868, acc-0.6104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-0.2791, acc-0.6200, valid loss-0.2827, acc-0.6220, test loss-0.2869, acc-0.6093\n",
      "Iter-4080, train loss-0.3130, acc-0.5700, valid loss-0.2828, acc-0.6264, test loss-0.2870, acc-0.6106\n",
      "Iter-4090, train loss-0.2906, acc-0.6100, valid loss-0.2827, acc-0.6240, test loss-0.2869, acc-0.6086\n",
      "Iter-4100, train loss-0.3032, acc-0.5800, valid loss-0.2825, acc-0.6258, test loss-0.2866, acc-0.6085\n",
      "Iter-4110, train loss-0.3404, acc-0.5300, valid loss-0.2826, acc-0.6276, test loss-0.2866, acc-0.6111\n",
      "Iter-4120, train loss-0.2976, acc-0.6300, valid loss-0.2822, acc-0.6280, test loss-0.2863, acc-0.6112\n",
      "Iter-4130, train loss-0.2958, acc-0.6300, valid loss-0.2818, acc-0.6294, test loss-0.2860, acc-0.6125\n",
      "Iter-4140, train loss-0.3241, acc-0.5400, valid loss-0.2820, acc-0.6216, test loss-0.2860, acc-0.6050\n",
      "Iter-4150, train loss-0.2736, acc-0.6600, valid loss-0.2812, acc-0.6272, test loss-0.2854, acc-0.6108\n",
      "Iter-4160, train loss-0.3132, acc-0.6200, valid loss-0.2809, acc-0.6238, test loss-0.2854, acc-0.6091\n",
      "Iter-4170, train loss-0.2993, acc-0.5900, valid loss-0.2806, acc-0.6254, test loss-0.2850, acc-0.6115\n",
      "Iter-4180, train loss-0.3038, acc-0.5900, valid loss-0.2804, acc-0.6252, test loss-0.2847, acc-0.6101\n",
      "Iter-4190, train loss-0.2609, acc-0.6700, valid loss-0.2803, acc-0.6272, test loss-0.2847, acc-0.6115\n",
      "Iter-4200, train loss-0.3240, acc-0.5800, valid loss-0.2801, acc-0.6292, test loss-0.2846, acc-0.6127\n",
      "Iter-4210, train loss-0.2967, acc-0.5900, valid loss-0.2800, acc-0.6296, test loss-0.2846, acc-0.6131\n",
      "Iter-4220, train loss-0.3048, acc-0.5400, valid loss-0.2797, acc-0.6288, test loss-0.2845, acc-0.6137\n",
      "Iter-4230, train loss-0.2880, acc-0.6000, valid loss-0.2797, acc-0.6274, test loss-0.2844, acc-0.6123\n",
      "Iter-4240, train loss-0.2951, acc-0.5400, valid loss-0.2797, acc-0.6246, test loss-0.2843, acc-0.6086\n",
      "Iter-4250, train loss-0.2924, acc-0.5900, valid loss-0.2793, acc-0.6316, test loss-0.2841, acc-0.6159\n",
      "Iter-4260, train loss-0.2673, acc-0.6700, valid loss-0.2793, acc-0.6318, test loss-0.2840, acc-0.6151\n",
      "Iter-4270, train loss-0.2721, acc-0.6000, valid loss-0.2788, acc-0.6332, test loss-0.2837, acc-0.6173\n",
      "Iter-4280, train loss-0.2710, acc-0.6700, valid loss-0.2791, acc-0.6308, test loss-0.2841, acc-0.6150\n",
      "Iter-4290, train loss-0.2814, acc-0.6600, valid loss-0.2793, acc-0.6266, test loss-0.2843, acc-0.6122\n",
      "Iter-4300, train loss-0.2620, acc-0.6700, valid loss-0.2792, acc-0.6296, test loss-0.2843, acc-0.6141\n",
      "Iter-4310, train loss-0.2612, acc-0.6800, valid loss-0.2789, acc-0.6274, test loss-0.2841, acc-0.6127\n",
      "Iter-4320, train loss-0.3293, acc-0.5300, valid loss-0.2786, acc-0.6292, test loss-0.2840, acc-0.6135\n",
      "Iter-4330, train loss-0.2735, acc-0.6500, valid loss-0.2782, acc-0.6278, test loss-0.2838, acc-0.6130\n",
      "Iter-4340, train loss-0.2976, acc-0.5900, valid loss-0.2783, acc-0.6274, test loss-0.2839, acc-0.6122\n",
      "Iter-4350, train loss-0.2807, acc-0.5800, valid loss-0.2781, acc-0.6300, test loss-0.2837, acc-0.6142\n",
      "Iter-4360, train loss-0.2748, acc-0.5900, valid loss-0.2779, acc-0.6314, test loss-0.2834, acc-0.6158\n",
      "Iter-4370, train loss-0.2781, acc-0.6200, valid loss-0.2779, acc-0.6320, test loss-0.2834, acc-0.6163\n",
      "Iter-4380, train loss-0.2760, acc-0.6100, valid loss-0.2776, acc-0.6308, test loss-0.2834, acc-0.6142\n",
      "Iter-4390, train loss-0.2754, acc-0.7100, valid loss-0.2771, acc-0.6324, test loss-0.2829, acc-0.6156\n",
      "Iter-4400, train loss-0.2946, acc-0.6000, valid loss-0.2768, acc-0.6330, test loss-0.2827, acc-0.6163\n",
      "Iter-4410, train loss-0.2495, acc-0.6300, valid loss-0.2768, acc-0.6288, test loss-0.2829, acc-0.6141\n",
      "Iter-4420, train loss-0.2931, acc-0.6100, valid loss-0.2764, acc-0.6308, test loss-0.2825, acc-0.6147\n",
      "Iter-4430, train loss-0.2900, acc-0.6200, valid loss-0.2763, acc-0.6250, test loss-0.2824, acc-0.6127\n",
      "Iter-4440, train loss-0.2489, acc-0.6500, valid loss-0.2762, acc-0.6290, test loss-0.2824, acc-0.6134\n",
      "Iter-4450, train loss-0.2663, acc-0.6700, valid loss-0.2757, acc-0.6298, test loss-0.2819, acc-0.6135\n",
      "Iter-4460, train loss-0.2927, acc-0.5400, valid loss-0.2756, acc-0.6300, test loss-0.2817, acc-0.6151\n",
      "Iter-4470, train loss-0.2873, acc-0.6100, valid loss-0.2751, acc-0.6308, test loss-0.2813, acc-0.6162\n",
      "Iter-4480, train loss-0.3042, acc-0.5900, valid loss-0.2748, acc-0.6302, test loss-0.2813, acc-0.6140\n",
      "Iter-4490, train loss-0.2809, acc-0.5400, valid loss-0.2747, acc-0.6294, test loss-0.2812, acc-0.6135\n",
      "Iter-4500, train loss-0.3114, acc-0.5200, valid loss-0.2748, acc-0.6310, test loss-0.2811, acc-0.6160\n",
      "Iter-4510, train loss-0.2676, acc-0.6500, valid loss-0.2749, acc-0.6288, test loss-0.2812, acc-0.6144\n",
      "Iter-4520, train loss-0.2898, acc-0.5900, valid loss-0.2751, acc-0.6314, test loss-0.2814, acc-0.6160\n",
      "Iter-4530, train loss-0.3070, acc-0.5300, valid loss-0.2747, acc-0.6336, test loss-0.2811, acc-0.6165\n",
      "Iter-4540, train loss-0.3179, acc-0.5400, valid loss-0.2748, acc-0.6328, test loss-0.2812, acc-0.6170\n",
      "Iter-4550, train loss-0.3033, acc-0.5100, valid loss-0.2747, acc-0.6336, test loss-0.2809, acc-0.6170\n",
      "Iter-4560, train loss-0.2897, acc-0.6200, valid loss-0.2746, acc-0.6336, test loss-0.2810, acc-0.6179\n",
      "Iter-4570, train loss-0.2795, acc-0.6100, valid loss-0.2747, acc-0.6348, test loss-0.2810, acc-0.6185\n",
      "Iter-4580, train loss-0.2670, acc-0.6300, valid loss-0.2747, acc-0.6342, test loss-0.2810, acc-0.6192\n",
      "Iter-4590, train loss-0.2842, acc-0.5600, valid loss-0.2744, acc-0.6354, test loss-0.2809, acc-0.6193\n",
      "Iter-4600, train loss-0.2976, acc-0.5400, valid loss-0.2741, acc-0.6368, test loss-0.2805, acc-0.6208\n",
      "Iter-4610, train loss-0.2573, acc-0.6300, valid loss-0.2741, acc-0.6368, test loss-0.2806, acc-0.6189\n",
      "Iter-4620, train loss-0.2845, acc-0.6700, valid loss-0.2738, acc-0.6382, test loss-0.2803, acc-0.6198\n",
      "Iter-4630, train loss-0.2962, acc-0.5700, valid loss-0.2736, acc-0.6356, test loss-0.2801, acc-0.6190\n",
      "Iter-4640, train loss-0.2648, acc-0.6400, valid loss-0.2739, acc-0.6382, test loss-0.2804, acc-0.6184\n",
      "Iter-4650, train loss-0.3068, acc-0.5900, valid loss-0.2734, acc-0.6390, test loss-0.2801, acc-0.6178\n",
      "Iter-4660, train loss-0.2702, acc-0.5900, valid loss-0.2732, acc-0.6388, test loss-0.2799, acc-0.6187\n",
      "Iter-4670, train loss-0.3107, acc-0.5600, valid loss-0.2731, acc-0.6374, test loss-0.2797, acc-0.6183\n",
      "Iter-4680, train loss-0.2975, acc-0.5800, valid loss-0.2729, acc-0.6366, test loss-0.2795, acc-0.6185\n",
      "Iter-4690, train loss-0.2928, acc-0.5700, valid loss-0.2731, acc-0.6376, test loss-0.2796, acc-0.6197\n",
      "Iter-4700, train loss-0.2959, acc-0.6000, valid loss-0.2729, acc-0.6366, test loss-0.2794, acc-0.6185\n",
      "Iter-4710, train loss-0.2439, acc-0.6700, valid loss-0.2731, acc-0.6320, test loss-0.2796, acc-0.6180\n",
      "Iter-4720, train loss-0.2781, acc-0.6300, valid loss-0.2728, acc-0.6360, test loss-0.2793, acc-0.6201\n",
      "Iter-4730, train loss-0.2805, acc-0.6000, valid loss-0.2729, acc-0.6330, test loss-0.2793, acc-0.6181\n",
      "Iter-4740, train loss-0.2752, acc-0.6200, valid loss-0.2726, acc-0.6318, test loss-0.2792, acc-0.6175\n",
      "Iter-4750, train loss-0.2874, acc-0.5700, valid loss-0.2730, acc-0.6322, test loss-0.2795, acc-0.6175\n",
      "Iter-4760, train loss-0.2725, acc-0.6800, valid loss-0.2725, acc-0.6318, test loss-0.2793, acc-0.6167\n",
      "Iter-4770, train loss-0.2719, acc-0.6000, valid loss-0.2724, acc-0.6346, test loss-0.2790, acc-0.6186\n",
      "Iter-4780, train loss-0.2703, acc-0.6300, valid loss-0.2715, acc-0.6354, test loss-0.2782, acc-0.6186\n",
      "Iter-4790, train loss-0.2789, acc-0.6400, valid loss-0.2711, acc-0.6350, test loss-0.2778, acc-0.6185\n",
      "Iter-4800, train loss-0.2937, acc-0.6000, valid loss-0.2711, acc-0.6336, test loss-0.2778, acc-0.6192\n",
      "Iter-4810, train loss-0.2873, acc-0.6100, valid loss-0.2711, acc-0.6350, test loss-0.2778, acc-0.6184\n",
      "Iter-4820, train loss-0.2853, acc-0.5700, valid loss-0.2706, acc-0.6342, test loss-0.2775, acc-0.6176\n",
      "Iter-4830, train loss-0.2564, acc-0.6300, valid loss-0.2703, acc-0.6340, test loss-0.2773, acc-0.6167\n",
      "Iter-4840, train loss-0.2838, acc-0.5800, valid loss-0.2699, acc-0.6346, test loss-0.2771, acc-0.6161\n",
      "Iter-4850, train loss-0.2705, acc-0.6800, valid loss-0.2697, acc-0.6346, test loss-0.2769, acc-0.6181\n",
      "Iter-4860, train loss-0.2606, acc-0.6500, valid loss-0.2696, acc-0.6342, test loss-0.2769, acc-0.6157\n",
      "Iter-4870, train loss-0.2540, acc-0.6800, valid loss-0.2699, acc-0.6312, test loss-0.2772, acc-0.6145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-0.2730, acc-0.5800, valid loss-0.2698, acc-0.6348, test loss-0.2772, acc-0.6164\n",
      "Iter-4890, train loss-0.2928, acc-0.5800, valid loss-0.2695, acc-0.6352, test loss-0.2772, acc-0.6136\n",
      "Iter-4900, train loss-0.2508, acc-0.6500, valid loss-0.2693, acc-0.6352, test loss-0.2769, acc-0.6164\n",
      "Iter-4910, train loss-0.3114, acc-0.5800, valid loss-0.2696, acc-0.6342, test loss-0.2771, acc-0.6158\n",
      "Iter-4920, train loss-0.2747, acc-0.5800, valid loss-0.2696, acc-0.6358, test loss-0.2772, acc-0.6159\n",
      "Iter-4930, train loss-0.2953, acc-0.6100, valid loss-0.2697, acc-0.6378, test loss-0.2771, acc-0.6183\n",
      "Iter-4940, train loss-0.2499, acc-0.6800, valid loss-0.2695, acc-0.6354, test loss-0.2770, acc-0.6159\n",
      "Iter-4950, train loss-0.2473, acc-0.7000, valid loss-0.2695, acc-0.6330, test loss-0.2770, acc-0.6138\n",
      "Iter-4960, train loss-0.2736, acc-0.5500, valid loss-0.2692, acc-0.6330, test loss-0.2768, acc-0.6137\n",
      "Iter-4970, train loss-0.3032, acc-0.5800, valid loss-0.2690, acc-0.6322, test loss-0.2767, acc-0.6135\n",
      "Iter-4980, train loss-0.2999, acc-0.6000, valid loss-0.2691, acc-0.6306, test loss-0.2769, acc-0.6122\n",
      "Iter-4990, train loss-0.2587, acc-0.7000, valid loss-0.2692, acc-0.6296, test loss-0.2769, acc-0.6129\n",
      "Iter-5000, train loss-0.2613, acc-0.6500, valid loss-0.2691, acc-0.6312, test loss-0.2768, acc-0.6120\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 5000 # number of epochs\n",
    "alpha = 1e-2 # learning_rate\n",
    "mb_size = 100 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, n_iter=n_iter, \n",
    "       print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd8FEX7wL+TRk/oHUJv0ouCCARBQFBBpYqIBYxgb4AF\nxVdU9NWf2EBsqIAg8CoiIKIIKhaK9N5bCNJ7SUjm98fk7vbu9u72Lne5SzLfz2c/tzs7bfd259l5\n5plnhJQSjUaj0Wiiwl0BjUaj0UQGWiBoNBqNBtACQaPRaDRZaIGg0Wg0GkALBI1Go9FkoQWCRqPR\naACLAkEI0U0IsVUIsV0IMdLkfAchxCkhxOqs7XmraTUajUYTGQhf8xCEEFHAdqATcAhYCfSXUm41\nxOkAPCmlvMXftBqNRqOJDKz0EK4Gdkgp90kp04EZQE+TeCIbaTUajUYTZqwIhErAAcPxwawwV9oI\nIdYKIeYLIRr4mVaj0Wg0YSYmSPn8A1SVUl4QQtwIzAHqBClvjUaj0eQAVgRCClDVcFw5K8yOlPKc\nYf8HIcQEIURJK2ltCCG0UyWNRqPxEymlmbo+IKyojFYCtYQQiUKIOKA/MNcYQQhRzrB/NWqw+oSV\ntEaklHqTkhdffDHsdYiETd8HfS/0vfC+BRufPQQpZYYQ4iFgEUqAfCql3CKESFan5UdAbyHEMCAd\nuAj085Y26Feh0Wg0mmxjaQxBSrkQqOsSNsmw/wHwgdW0Go1Go4k89EzlCCQpKSncVYgI9H1woO+F\nA30vQofPiWk5hRBCRkpdNBqNJjcghEAGcVA5WGanGo0mj1CtWjX27dsX7mpoDCQmJrJ3796Ql6N7\nCBqNxomsr85wV0NjwNN/Euwegh5D0Gg0Gg2gBYJGo9FostACQaPRaDSAFggajSafkpmZSbFixTh4\n8KDfaXft2kVUVN5rPvPeFWk0mjxJsWLFiI+PJz4+nujoaAoXLmwPmz59ut/5RUVFcfbsWSpXrhxQ\nfYQI2lhuxKDNTjUaTa7g7Nmz9v0aNWrw6aef0rFjR4/xMzIyiI6Ozomq5Rl0D0Gj0eQ6zJy7jR49\nmv79+3PHHXeQkJDAtGnT+Pvvv2nTpg0lSpSgUqVKPProo2RkZABKYERFRbF//34ABg0axKOPPkr3\n7t2Jj4+nbdu2ludjpKSkcPPNN1OqVCnq1q3L5MmT7eeWL19OixYtSEhIoEKFCowcqVYSvnjxIgMH\nDqR06dKUKFGC1q1bc+LEiWDcnoDRAkGj0eQZ5syZw5133snp06fp168fsbGxvPvuu5w4cYI//viD\nH3/8kUmT7G7Y3NQ+06dP55VXXuHkyZNUqVKF0aNHWyq3X79+1KxZk8OHDzNjxgxGjBjB77//DsDD\nDz/MiBEjOH36NDt37qR3794ATJ48mYsXL3Lo0CFOnDjBhAkTKFiwYJDuRGBogaDRaPxCiOBsoeC6\n666je/fuABQoUIAWLVrQqlUrhBBUq1aNoUOH8uuvv9rju/YyevfuTbNmzYiOjmbgwIGsXbvWZ5l7\n9uxh5cqVjBs3jtjYWJo1a8Y999zDlClTAIiLi2PHjh2cOHGCIkWK0KpVKwBiY2M5duwY27dvRwhB\n8+bNKVy4cLBuRUBogaDRaPxCyuBsoaBKlSpOx9u2beOmm26iQoUKJCQk8OKLL3Ls2DGP6cuXL2/f\nL1y4MOfOnfMY10ZqaiqlS5d2+rpPTEwkJUWtBTZ58mQ2bdpE3bp1ad26NT/88AMAd999N507d6Zv\n375UqVKFZ599lszMTL+uN9hogaDRaPIMriqg5ORkGjVqxO7duzl9+jQvvfRS0N1yVKxYkWPHjnHx\n4kV72P79+6lUSS0fX7t2baZPn87Ro0d54oknuP3220lLSyM2NpYXXniBzZs3s2zZMr755humTZsW\n1Lr5ixYIGo0mz3L27FkSEhIoVKgQW7ZscRo/yC42wVKtWjVatmzJs88+S1paGmvXrmXy5MkMGjQI\ngKlTp3L8+HEA4uPjiYqKIioqiiVLlrBp0yaklBQtWpTY2Niwz23QAkGj0eQ6rM4BeOutt/j888+J\nj49n2LBh9O/f32M+/s4rMMb/+uuv2b59O+XLl6dv376MGzeOdu3aAbBgwQLq169PQkICI0aMYObM\nmcTExHDo0CFuu+02EhISaNSoEV26dOGOO+7wqw7BRns71Wg0Tmhvp5GH9naq0Wg0mhxFCwSNRqPR\nAFogaDQajSYLLRA0Go1GA2iBoNFoNJostEDQaDQaDaAFgkaj0Wiy0AJBo9FoNIAWCBqNJp+wb98+\noqKi7A7kunfvbvdI6iuuK9WrV+eXX34JWV3DhRYIGo0mV3DjjTcyZswYt/DvvvuOChUqWPIUanQ3\nsWDBAru/IV9x8wuWBIIQopsQYqsQYrsQYqSXeK2EEOlCiNsMYXuFEOuEEGuEECuCUWmNRpP/GDx4\nMFOnTnULnzp1KoMGDQq7Y7i8gM87KISIAt4HugJXAQOEEPU8xBsH/OhyKhNIklI2k1Je7a0sC67H\nNRpNPqVXr14cP36cZcuW2cNOnTrFvHnzuOuuuwD11d+8eXMSEhJITEzkpZde8phfx44d+eyzzwDI\nzMzkqaeeokyZMtSqVYv58+dbrldaWhqPPfYYlSpVonLlyjz++OOkp6cDcPz4cW6++WZKlChBqVKl\n6NChgz3d66+/TuXKlYmPj6d+/fosWbLEr/sRCqyI1KuBHVLKfVLKdGAG0NMk3sPAbOCIS7iwWA7F\n4jN56im4dEmtqJSaaiWVRqPJDxQsWJA+ffrw5Zdf2sO+/vpr6tevT8OGDQEoWrQoU6ZM4fTp08yf\nP58PP/yQuXPn+sz7o48+YsGCBaxbt45Vq1Yxe/Zsy/UaO3YsK1asYP369axbt44VK1YwduxYQHlb\nrVKlCsePH+fIkSO8+uqrAGzfvp0PPviAf/75hzNnzvDjjz9SrVo1P+5GaIixEKcScMBwfBAlJOwI\nISoCvaSUHYUQrr0ACfwkhMgAPpJSfuypoDeuqsLrk2fz1lttADh8GCpUsFBDjUaTY4iXgqNbly/6\n71F18ODB3HTTTbz//vvExcUxZcoUBg8ebD/fvn17+37Dhg3p378/v/76K7fccovXfGfNmsVjjz1G\nxYoVAXjmmWecltr0xldffcUHH3xAqVKlAHjxxRd54IEHeOmll4iNjSU1NZU9e/ZQs2ZN2rZtC0B0\ndDRpaWls3LiRUqVKUbVqVb/uQ6iwIhCsMB4wji0Yn5i2UspUIUQZlGDYIqVchgnx/1Zn06W2PFKv\nLzO3fcnJk3GkpmqhoNFEEoE05MGibdu2lClThjlz5tCyZUtWrlzJt99+az+/YsUKRo0axcaNG0lL\nSyMtLY0+ffr4zPfQoUNOy28mJiZartOhQ4ecGvTExEQOHToEwNNPP82YMWPo0qULQgiGDh3KyJEj\nqVmzJuPHj2fMmDFs3ryZrl278tZbb1EhzI2dFVVOCmAUX5Wzwoy0BGYIIfYAvYEPhBC3AEgpU7N+\njwLf4tK7MPLA0c40T+9L6d3fMLJSKXr0WUfFihABqjWNRhMhDBo0iC+++IKpU6fStWtXypQpYz93\nxx130KtXL1JSUjh16hTJycmW1naoUKECBw44FCH79u2zXJ+KFSs6xd+3b5+9p1G0aFHefPNNdu3a\nxdy5c/m///s/+1hB//79+f333+1pR40a5bOspUuXMmbMGPsWbKwIhJVALSFEohAiDugPOCnlpJQ1\nsrbqqHGE4VLKuUKIwkKIogBCiCJAF2Cjp4IuXx5Di24z+CLtOE3P1+OPmBZUrj6Z66+HLIGr0Wjy\nOXfddRc///wzn3zyiZO6CODcuXOUKFGC2NhYVqxYwVdffeV03pNw6Nu3L++++y4pKSmcPHmS119/\n3XJ9BgwYwNixYzl27BjHjh3j5Zdftpuzzp8/n127dgFQrFgxYmJiiIqKYvv27SxZsoS0tDTi4uIo\nVKiQJSuppKSk8AoEKWUG8BCwCNgEzJBSbhFCJAsh7jdLYtgvBywTQqwB/ga+l1Iu8lRWXBzMnQvn\nZDF+6rWCmZeSWXJyCJWrzOCJJ/y6Lo1Gk0dJTEzk2muv5cKFC25jAxMmTGD06NEkJCQwduxY+vXr\n53Te05KZQ4cOpWvXrjRp0oSWLVty++23e62DMe3zzz9Py5Ytady4sT39c889B8COHTvo3LkzxYoV\no23btjz44IN06NCBy5cvM2rUKMqUKUPFihU5evQor732WsD3JFhE7BKaKSlQuTI8VuIhnkqfyE3l\nX+bZV5/FgjpQo9FkA72EZuSRU0toRqxAcIRD7/i3eP/KCLoVn8CpuGT27oX0dIgJ1pC4RqOxowVC\n5KHXVM6iQQOYfeZJhhd9nvmnhxN7/hsA3nknzBXTaDSaPEbE9xAAFi6EG2+E+8o/yDMXP6RV5lpO\nnm3ErFnQu3cOV1SjyePoHkLkoVVGJixcCJuHtKWB3Eb3Q4eRxBAh1ddo8gxaIEQeWmVkQteuMOLQ\nYopmXOaBxDsBWLkyzJXSaDSaPEKu6iGoeNCwxBwWX76NxmI1les1ZdWqHKigRpNP0D2EyEP3EDwg\nJUxe1IsPS7dnSnxHVv9zhXfeUQ7xNBqNRhM4ua6HAMoLapWKafxRpgSfxN7BJ4c+pnFjWLcuxJXU\naPIBuocQeegeghcqVICFP8XxQMx4Xj71GcWjD7F+ve4laDSa4LNt2zZiY2PDXY0cIVcKBIDOneFg\n+lDmlE9kTFk1ffny5TBXSqPRhIxixYoRHx9PfHw80dHRFC5c2B42ffr0gPNt06aNm88jV/LLcpq5\neq5v3bowevNXbLt4Lf8tuIHixRtpM1SNJo9y9uxZ+36NGjX49NNP6dixYxhrlPfItT0EgIwMOHay\nNZ9VbMSo0ncDMHRoeOuk0WhCj5TSTaeemZnJyy+/TM2aNSlbtiyDBg3izJkzAFy4cIEBAwZQqlQp\nSpQoQZs2bTh9+jRPPfUUK1euZMiQIcTHx/P000/7LPvAgQP06NGDUqVKUa9ePacV3P7880/7Ep4V\nK1a0O7nzVH7EYbux4d5UVfxj4kQpQcrSRdfII4WErFPoDxkX53c2Go3GQCDvYk5TrVo1uXjxYqew\ncePGyfbt28vDhw/Ly5cvy3vuuUfee++9Ukop33nnHdmnTx95+fJlmZGRIVetWiUvXLggpZSydevW\n8quvvvJY1tatW2VsbKz9+JprrpFPPvmkTE9Pl6tWrZIlS5aUf/75p5RSymbNmsnZs2dLKaU8d+6c\nXLFihc/yreDpP8kKD1o7nKt7CA88oMYNjp1ryttVWzE64X7S0sJdK40mjyNEcLYgM2nSJMaNG0e5\ncuWIi4tj9OjRzJgxA4DY2FiOHj3Kjh07iIqKokWLFhQqVMieVlrUNe/YsYP169fzyiuvEBMTQ4sW\nLRg8eDBTpkwBIC4uju3bt3PixAmKFClCq1atLJUfKeRqgQBqDYX58+H9w5/Q5cxm6sStRAg16KzR\naEKA6phnfwsyBw4coHv37pQsWZKSJUvSvHlzAE6cOMF9991H+/bt6d27N1WrVuW5554LyLQ2NTWV\nMmXKUKBAAXtYYmIiKSlqEckvvviCdevWUadOHdq0acOiRWr5l/vuu48OHTrYy3/++ecj07Q3mN2N\n7Gxks5sKUj7boKmcXLaZ/YnTaDT+k913MScwUxlVq1ZNrl692mfaPXv2yNq1a9vVRG3atJHTpk3z\nGN+oMtqxY4csVKiQvHTpkv38E088IYcNG+aUJjMzU06bNk0WKVJEpqeney3fCp7+E7TKyDOTjr5N\nr9NrKc4xALp0CXOFNBpNjpGcnMzIkSM5ePAgAEeOHGHevHkALF68mC1btiClpGjRosTExBAdHQ1A\nuXLl2L17t9e8ZdbXfK1atWjUqBHPP/88aWlprF69mi+//NK+ZOaUKVM4ceIEQgji4+OJiopCCGFa\nvpUlM3OayKtRNjh+NIl5lUtyXxllKfDTT2GukEajCQlm8wJGjhzJDTfcwPXXX09CQgLXXXcda9as\nASAlJYWePXsSHx9P48aNuemmm+jbty8Ajz/+OF988QWlSpXyuNC9sbxZs2axadMmypcvz4ABA3jz\nzTdp06YNAPPmzaNu3bokJCTw3HPPMWvWLKKjo03Ld13eMxLIla4rzNi8GTZsgLeee5mZh1+l1vmz\nZGj32BqN32jXFZGHdl3hJw0aQL9+sHLvSFLjM7gl/n0APvkkzBXTaDSaXEKeEQh2MuIYX7wXj8a8\nAcAbb8CSJWGuk0aj0eQC8ozKyJEPxBQ+yO7oqtyS9gtrLycBsG0bVK8O+cRHlUYTMFplFHlolVGA\n3H03/PpTZT6o0pRH45+yh9etCy++GL56aTQaTaSTq53bmTF5svq9+eRr7DzTnbIc5gjlAciyRtNo\nNBqNCXmuh2DjRGoXZtUsRnL88/awCDT71Wg0moghz/UQHAjei7uHH9I+4hUmkUl0KNynaDR5jsTE\nxHzj/z+3kJiYmCPl5GGBABt3PcWhEu/R/d85zMu8XQsEjcYCe/fuDXcVNGEibytRzlbivRr1GFb4\nZUCrjDQajcYbec7s1EbdurB9OxSr9zkpO4dQ4copzlOUXbugUiUwOCvUaDSaXElYzE6FEN2EEFuF\nENuFECO9xGslhEgXQtzmb9pgs3IlLFsGZ3f3ZllV6Bn9NQA1a8LIHKuFRqPR5B589hCEEFHAdqAT\ncAhYCfSXUm41ifcTcBH4TEr5jdW0WemD2kNw5At92jXh/tWZ3HB+gz08QjpGGo1GEzDh6CFcDeyQ\nUu6TUqYDM4CeJvEeBmYDRwJIG1LmnnqIpunbqMq+nC5ao9Focg1WBEIl4IDh+GBWmB0hREWgl5Ry\nIiD8SRtqPv4YLm/vx6yr4I7oyfZw3UPQaDQaZ4JldjoeyLZmfsyYMfb9pKQkkpKSspslQ4bA0KHx\nzCrVhNdjpzIuQ5WxbRvUq5ft7DUajSbHWLp0KUuXLg1Z/lbGEFoDY6SU3bKOR6GWbXvdEMe23JAA\nSgPngftR6iOvaQ15hGQMQeUNMc3f598NT9IwfQ+pVAR0L0Gj0eRuwjGGsBKoJYRIFELEAf2BucYI\nUsoaWVt11DjCcCnlXCtpc4LkZLiy41bm14Fe4n85XbxGo9HkCnwKBCllBvAQsAjYBMyQUm4RQiQL\nIe43S+IrbVBq7gcPPgicrcT/Klahd4HP7OHDh+d0TTQajSZyybMT04wcOqQmoxVs+yKHl4+j1pWD\nHKOM/XyE3AKNRqPxC70eQgBUrAjvvguXdvTmx+ox9GROuKuk0Wg0EUe+EAgAMTHAkYb8r2ZRbo/7\nPNzV0Wg0mogj3wiEe+4BECxI70PbjFUkcCrcVdJoNJqIIt8IhIIF4dpr4dzOPvxSpRA38739nHaL\nrdFoNPlIIABkZgIH2vK/qzLoHTPF6dyZM9CvX3jqpdFoNJFAvhIIGRlAZgzfy5vomPk7RTlrPzdx\nIsycGb66aTQaTbjJVwIhM1P9nt7Thz/KF6E7C+znRo0KU6U0Go0mQshXAiEjI2tnVxf+1+Qct0fN\nCGt9NBqNJpLInwIhrSjfFWxPV36kIBfDWieNRqOJFPKVQJg2DX74Qe0f29eX5aXjuR3t20ij0Wgg\nn7iucC8LKPIvPbvWYMQ3jWnLX/ZzEXI7NBqNxifadUWwOF+OefFNqRyzk6assQcLAQcOeEmnCStD\nhzqMAzQaTXDJvwIByFg7hI/rleJ+PnIK/+efMFVI45NPPoG0tHDXQqPJm+RLgfD551CoELC5D5+1\nT6UfMyjCOft5PXM5stFqPY0mNORLgTB4MHTvDqQV5dChW/mtRGX64zBBvXAhfHXTaDSacJEvBQIY\nJqKtu4tJrS+QzCT7uRdegHXrtGoiUtE9BI0mNORbgdCkSdbO3iQW1UmjbFQKzVgNqB5C06bwwQfh\nq59Go9HkNPlWIERHZ+3IKDI33MXHNWrYB5cPHVKnnnhC/U6dCufOueehCQ+6h6DRhIZ8KxCijFe+\n7i4+67Sdvsx0cnhnY9Ag+PbbnKubJndw5QocORLuWmg0wSPfCgSADRuydo7XJTWjBr8WqUdfzF2e\nRuXrOxVZREoP4bXXoFy5cNdCowke+bqZa9jQ0Lisu4svGkUziCmmcbUpqnfS0+HDD3OmrEgRCCkp\n4a5BeLH7BtPkGfK1QHBiYz9+uG49DdlAVfa5ndY9BO9s3AjDhoW2DJsgiBSBkJ9JSclap1yTp9DN\nnI2LpUjb35mvSzfmbj63B//5p/o9pZdg9kp2G2kpYenSnClLk330+5A30QIBaNUqa2fdXXzU9hT3\n8SkC5TDnk0/UqWHD4KuvwlO//MDGjdCxo3v40qUOdZ3uIUQO4fwPWreGWbN8x9u3Dy5fDn198hJa\nIADvvZe1s/NG1tdJ4XxUHK1YCcDkyY54AwfmfN1yC2YNxMSJ8MYb1tJ70kfv2GGtrNzK11/Df/5j\nLe7bb0eOtVs4/4Ply2HePN/xqlWDl18OeXXs5AUfaFogYPCemREHG/vzdZUqTq4sNIExYgSMHBm8\n/ILZQ/jlF1iyJDj1yQ5jxsCLL1qL+8QT6p5qrN/7EydCWw8bhw9Dy5Y5U1Yo0QIBx8OVmAisv5Np\nHXYwgOlEcyWs9crtGF/a334LTb6B0qkTdO4cWFrbxEVPHDsWOqs0T/nOmAGrVoWmzGAxb17etdbL\nKxZXWiAACQmGg5Sr2Vk0gf1xJenEYre4y5blXL0ilV27YO9e5zBfjXSHDsFbxyCc6opNm6BSJe9x\nTp+2nl+wdNwDBsDDDwcnr1CxZUu4a6DxhSWBIIToJoTYKoTYLoRwUwIIIW4RQqwTQqwRQqwQQrQ1\nnNtrPBfMygeLq65SXUt7g7W5D1NrlGUg09zimum08xu1akGzZs5hxkY6NRUOHnQ/5+3r0Jj+n3/M\nG49AVUZ79vgXH9TzcPSoe7g3FyYLFsClS/6VcyWXdkKDIZSrVQtcBZaXxpEiCZ8CQQgRBbwPdAWu\nAgYIIeq5RPtZStlEStkMuA/4xHAuE0iSUjaTUl4dpHoHnRIl1ABfpUqocYQOW7mFuU7rJGgcnDrl\n2W1DixZKaNjwtyFv2RKuv97zeX97GjVqqB6Na7feWz7XXQc1a7qHe7uGHj3UM+SJs2fdBYq/KhQr\n8du0gX//9S9ffwlGg7xvH/z+e/bz0QQPKz2Eq4EdUsp9Usp0YAbQ0xhBSmlcQaAoYHzVhMVywk6b\nNnDLLcCxehyJi+eXAs3dZi4/9hj88YdSm+R3li83Dz950lkVEqyvuUB6CGezXFPdfLPyYOuJVauc\nZx6npDjS+qqPVZo3VyaT2cGbQLCd+/tv5b49uwgRvEHZXr3g55+Dk5cmdFhpqCsBxlWGD2aFOSGE\n6CWE2AJ8D9xrOCWBn4QQK4UQQ7NT2ZzA7gV1ew8+q1WRO3CefHDmjPp67Ns35+sWKqT03Lh7IzkZ\n3n3XPD+zY08NaGIifPmleRqzBtCfhtg20Lpxo9o80aoV3HGH2l+0yLPqJzuDojt3qjGI3LQm9OHD\n/sVv2NBcrfbdd+q+uhJq1U9OqZYCeS7atYu8diRok8+llHOAOUKI64CxwA1Zp9pKKVOFEGVQgmGL\nlNJ0aHbMmDH2/aSkJJKSkoJVPcvYp+Nv7cWiLg/x+aZUqrKP/SQ6xcvN1hIpKVCyZNYyosDaterL\n1dvLc+YMFCvmfN2pqWrNiEcecaQ1WuCcP69+fb2U+/fD+PHm52xpDx9WdXbNTwh1PRUrei/DCrZ8\nu3b1HefcOThg+EzyZWWyc6dj//x5dS8DwUoPIRgEmtemTeq5qF07eHUxIy+MISxbBsWL+5dm6dKl\nLLU6pT8ArAiEFKCq4bhyVpgpUsplQogaQoiSUsoTUsrUrPCjQohvUSoonwIhXNh7CPvbkV70NLML\ndWTAxem8ziineLYX5sMPoU8fKFUqZ+tplfR0iI11DqtcGYYMgY8/VsdWBjYTEmDOHOjZ03s8Mwsc\n24Qd41f/9u3+NRrnzpkLBFBCyKpA2LABGjUyP+dPIzh0KCxcqPbPnoV33jGvmw1PE6QCbdgyMtR/\nW7BgcPO1irf8bfdx2TIYPhzWr/cdN7/i7//k+qH80ksvBbU+VlRGK4FaQohEIUQc0B+Ya4wghKhp\n2G8OxEkpTwghCgshimaFFwG6AF467uHHuHAO6wcy7apYN7UROB7kYcMid2W11ashLs78XCB+/Feu\n9KwO8kabNu5hW7daK9OKyshTHbZvd1fPNG7sX1meOHbMsR8fD6NHO45vv909/v79jn3XHo4/2OI/\n/bSjh2dGqATC33/7jiMETJmi1IkbNuixg9yET4EgpcwAHgIWAZuAGVLKLUKIZCHE/VnRbhdCbBRC\nrAbeA2yasXLAMiHEGuBv4HsppYkmMXKwCwSA9YP4o91SinOKRjh/5qxcCUWKqH2rM01zGm8TqAJp\nMF55xbPX11A1QMZ8PY1FeCq7bl3vVj+u+NM4e1IRCWE+oOupl+/LFDc93Tz+pk0+qxgUjPf29GmH\ncPfVQ7jrLoe/oRtu8Bw3t6p+tm41N0vO7Viy/pFSLpRS1pVS1pZSjssKmySl/Chr/w0pZUMpZXMp\nZVsp5V9Z4XuklE2zTE4b2dJGMk4N3tEGyLNVmFyiHUP52C3uhQtuQX6zYoWzv6RgYnzZvE2AsjUy\n8+ap8QAjp07B8eP+lZWdOK6sVC6lnMwo/cknVEufevrv/b0P3uKPG2few3v6afMBWqNwCUVDa3Uw\n3JdgXb9e9V6zQ7gFSf36kTcgHAxyhTloTuLm433NfXze5gT9mUEcwXed+OSTcO+9vuNlh8xMz7pm\nI+++a3D0l8V116kv7ZzG9sLbxjmuuw4efND5nI1rroHXX3ccP/ecQyVmpeE4eVL9BkOfHaj7iF27\n3Os6Z45j32YCKwR8840j3KiKyklc62q0RvJ1H1u1UnNVrMS1Wr4VMjM9L2qUmel/rysYH4T+lJUT\nvUItEFxwUhkBbOzH3sZ/sTGqLj2Y7zVt3bowe3bo6uYvVu32J03yfG7vXms9BH/q4w1vfvY//dRz\nPsZxnFecDPkrAAAgAElEQVRfteYN08ZNN6nfYAgEK+NJZmMItWo5N/Sgeo+gegNGk1ljPROdjd/s\nHDigLJuy67Ldyn+WmQkVKpjXz4xQ9mS8NdJTpyqDCjNmz1Yms/4QjOfF6vWPHet//QJBCwQXqlRx\nCbicAFt78kW1KtzPR17Tbt8Oi93dH3kllFYWrgLB08Pn+lWbmmq9DJsrj0DmMZhx1VW+41htpKzG\ntemCvf0XM2b4dmpnFaPqxVg/m3rrwAFnFylmcz3MMNY/OVlZcQ0cqHpLixf7P6fAKq73OLvP9Jw5\ngQuKIkWce1ZGvH3YXLzof1lCqJ5dWpr/af0lVKpPV7RAcGHwYNVAtG1rCFxzLzM6r6MRG9wGl22Y\nPcDBakCChWtDJARs3uy+AE3Hjmpugj989pnvOFZeciszY0M1XuGtIRswAF57zXHsjwCcO9f5+Phx\nZ19PNmx1rloVHn3UEf7nn45zgagNbJ5djV/x2eHMGedjm5rVis8qK9x6a/ac/gWiRnPTDHggNdUh\nWI4fVz27N9/0v7xIRQsEF4SA0qXV3AI7+9pzuUA678TfxijMx8XNvkoqVVJmd77KCzXeegi7drnX\nYds2d+d1Vsvwxpw5DrWPjbFjnY9tdfFmFmulLKtfw5cvO77Gff0XGRkwc6a1fI2Mc3lkunRRPdG/\n/vJcplEwul7v7t3+le9NDde6tXv9zEhJyXLrghrzMvsPrAoEY2Of3effJpxmzlQfN2a4zsR3FWhg\nfc30SpUcfrZsXm398W4bKDk1iK4FggeSk9WX2eDBAALW3sPEa8/TmZ+pzXa3+Lfdpn43bIDChR1q\niEAHns6d86+beP68+4CZVZWRaw8hVAwfribEGcv85BPnOFZezE2bVPr//tcRduCAsxngCy+oX7Nr\nMs4T8Ke7L4RatcxfPA3AXnutsxrDqLYwNpSuDbprfsa5AWYCy1uju3w5zPcyNGYr6/ffHa7fT57M\nnkAwS2OsjytmvaJVq1SP1+a6vl8/c3WjlGr8xehDyuydtCoQpHT/WMnOexNuaylXtEDwQMGCyuZ6\n4sSsgLWDOddkDu9FPcAIPK8L+ccf6sW2dVv9GWAz0rat+1f6mjWe9aNDhngeMPMmELzVLzPT4X7C\nG5cuWXuwCxTwHcfKrGnbbGlX18lWlzh1Hby1EYrempTu99B4jUY13vDhnvPxZvNumxtw6JB1o4aU\nFMf17typBuLN8NT42zCqRbOrMhoxwtzirmFDd0eDO3ZYb0xt99vbe2BVIPjiyy/VHAwzhHAYCkQq\nWiD4oFAhNRuZs5XgQBsm1SvD7fyPBLz0w004fty/ruXmzc7+b0A1+rfeah7fTMViZQLX3r2eX2Cr\nLlPS063ZqLsOVq9dq1wgG8mO7tgsrT9fYGb34eJFh828v19zf/6p5pi4qg2tCD1X53pW1gbevdva\n4vPgrJI6fFiZ6pp9OXfooCb3uV67zXrJ6KrENlEzUIHw3/861D6u4y6Zmaon9MwzvvPx9dyHUiB8\n/LGape2JbdsCy1erjCKRFQ9xtMPH/MQNpovnGLGpe2wPWvXqEKivPik9qzY++gg+/9w5bO1apSd1\n/SLKzHS3pnjrLedyjHTqZL2O2921aF45edL/cYpA8NVA2vTiYN6Q/fe/gdvMt23rW9/vSaUYTIsg\n47Ozd6/jelyxNeiuHDjgPGP65Elz1ZmtnGA4GuzXzz3spZesjXd4IhDHgCtXKrfl3rAZaNj2Q4EW\nCBHEoEFZOzu7QUYsE6vVZTgTAM//kq3xb9UKvv9edXmN3jFtWGlkoqI8q1uSk9VmpFkz58XtjYKh\nWzfnMOP+L7/4rosZP/7o/8pfVuz1/TVlFcL/HoavXpBRgE6Y4F/e4P+LvGBBcPIxYvz6XbnS+yxh\ns3JGjLCNpSnWrAm8Lp7ITs/Q6r0JRHX6yy/qevfuhZ9+8l2Gp4Ht55+3VEWfeJpYFyy0QLBAq1a2\nPQG/P8fSpKUIJB341VL6YcPUb7AmeJnh+kBfvmw+qGz2wGb368PKOIMrVhp7fxeT+fVX/90JGzFT\nG+T0oF+PHurX9f/MjqsHY16+3C1kZPg/lyYYmE0kDFUPcu5cd19UZg4JjXTrpqzDjJgJEdusd1de\necV6/bzhaZwwWGiBYAEndxbbb4Iy23i/cF8ew4MTfxe8SXVPXyZm4f6qLMx0p64WK0JEnqVDdvB3\nTWMj2V2Mx4zcdm83bVJzFnKK3bvVPerd2/2ccS7Mzp0Od+M2rDg7NJsVPny4Gt+x4c00PFBV0KxZ\nKq2rcHV9xqS0ZomoVUYRhv0PyYiD9XfyeasLXMuf1MWiH2fXfEw4f95zl9OV7793HzswIoTDYsPm\nGE5Kd9WOENlfwSu3NXreMLq1huxfmz+zvoORzgx/PiTMJsyFkqNHHQ4MvWFmGWa112QmOIz3xF+v\npVaeCVtPzMy3VenSDuusc+c8j92EAy0QAmHNvVxs8RUTeIBHsOhXIAvjw/TYY7BkieP4+ee9u26w\nPcRCqMHQe+5Rx2lp5uv/2sJslknGht9mex6MHkJeWeRECChTxjksu/fG17rM4cLTf/bHHzlbDwjM\nbQQ47q3rf3T+vJo34Y127QK3+HHF32fk+HFn1yRW8s+pZVe1QAiEIw3hRE1mNIihPzP8MkE1Pjy2\nVbYAnnjCswMzs7SueFvC0dOLYyVfK+SVHoLZPbdqxplXMLrnyCmsfFCYxfHkMuLtt6F9e/fwX12G\n/L78UjXOZs/v6NHKW4E31a3RJbsnsvOxtGePGtB+8UXvDiiDiRYIgbLwHbZ1/4DfuI7xPGY5mZTK\nnNPVrvzttx0rSxnVOlatd7w9eDZh4cm6Iq806NnF7B66zpPwl0C/fvMTVmaLmz2jtjEF43gAuL8z\nNtNf13ivvqrMW4159+yp1GZjx3qe5Of68eXNZ9nu3YGbEHfpokxebUvQ5gRaIPiBk5Q+3BSONOTu\ner24iXnUwnof8JdfPM+WBahZ07E/Y4Zj35+vDaOzOW+eP4Uw9+3iD3lJZRRsPJmR5iShNlXMLv/3\nf9lLb9YbMOLNsurMGede0dy55uM3xnk2rmMO06d7/qiaNEm5KLHhzzMWjvdKCwQ/sPnhsfPP/Zy+\nZirv8CijsTCVFGtf40ZvjfY5EHhffMX28JjNJbAJBE860+wusuLqQiK3EsyBXI11vPnsMo6bWcXo\nEcD4seNqMGDDdVA7Nta9/OxgZm4eqJos1GiB4AdRUS4N+tZbocwm3i99PT35jiL49kYXLBfBnjCb\nXWz7QjR+qQQTb55JcxN794a7BjmHP4OaocZtlUITAlVr+kqXmek+8G+cBGr1PbVav99+sxYPgudO\nwx+0QMgOGXHwyyuc6pvMn7ThNrzogbKwPTihmO0ZCJHUMISbnFqEJBKweeeNBMLR8Nkw088XKuR/\nPt7G+ozC4iPva2w5EahjzOygBUJ2WX0fZMTyfoU2PMNrFML7LBOrXxI5sQqTxpmcXCNX48DKzOic\nXHs5kDShWO/YJihzchxKC4QAaNfOeCTgrydZcMNvHKEsn+A60OBMqCx68srArkZjhuu6GVYJZOa6\npwls3rC5HPGVn40OHXznqccQcgluesBNfaDCGpIL/4c7mE4SS0zTgRYIGk0gWLH5N8PVjXY48FcV\n+cUXyhxWjyHkIs6fhwYNsg4yCsDWXmxr/hd3MI1xjPKYTtv8azSRTSA9hGDy+edq5TstEHIRhQtD\nyZKGgL8fg9Zv83XBG4jnDL341jRd0aKhqY+/rqJDRUM2MIgvkQiakQ0XnRpNmDAKBOM6EDlFOHv7\nWiBkA6ev/X8bw+Y+ZHYZxQjeYDyPUZC8O001hnQasZ6v6Uscl7mVb5AINtCYL1HO81fTAolAIhjI\nVNT6ERJBJrHoUXNNZGJ8r43riuQEa9c6/JvpMYRchpvDqcWvQK2FzKtchuVcY3myWm5jHY1JJ471\nNKEvs7hMQb7B4VD+Gv6mDEd4iyfsYVMZhCQKSRSZRJNGASSCDiy1xymJmsETzRUEOeTNS6NxIZxq\nXaN/s3CgBUI2iItzCbicAItfg65PMILXeYAPaW9xEZ1Iph8z+Jey9q/9xmzgKKWpwn6KcI5XeYZm\nrCa6cAqi26OsGNOaY2PK8tQj3yHEFUSlv6lZfBG7qAHApMK9uPX6+gAspaM93+OURiK4QiyZRLOJ\nBrzKM1zNcirgxWGMRpPLMS5la8NbDyFkxinSQs5CiG7AeJQA+VRK+brL+VuAl4FMIB14XEr5h5W0\nhjyklbpEEocPQ8OGLlPTo67AsMaw5D/035zO0/yXq1lBBhamY0YYxTjDGRLcwtvwJ3+XLAN15kG3\nx2Hux9DxBSjmp+8HCU8urMyby5UT/gd6wLUH4GIs3LgDqpr4WNpYoAoNL7uvRdqm/Af8fThraToE\nlNkEQsKF0nCxJBQ4A4WOI27vx8P713E2Ds7HwR8FruJQhcOUFMc5/t23sL2Xf9egyXNs3Qr16uV8\nuVIql/be1jkBNa4RHW0bdBZIKYOmXPIpEIQQUcB2oBNwCFgJ9JdSbjXEKSylvJC13wiYKaWsbyWt\nIY9cJxAArr/eeU0DAGr+CDcNgwnr2JnelAsUpjFelmWKMJqxmtU4VmK/he/4HsNq9A/XgVImU5wz\no2DKIthzvTqOyoAbH1YuPo42gL63q3WpN/WFY/Wh5z3Q9Ev3fC4Xg0MtoMwWEk7Fc/2ZHdQ4CW9m\nrWn7V2VoY1jIZUdJqH3CcfxZU7h3LewsAfsTYF4dGLUMylqYePZazWY8u38ppMdDVDqi4DHkhQq+\nE2o02URKtajV5Mne4z35pHLo17MnBFsgIKX0ugGtgR8Mx6OAkV7itwE2+ZtWVSX30bGjlOqvdNlu\nvVPS9XFZnV3yAgXlWJ6VkGkeN0K2WC47BbzDwzKadHXY9DNJ5xGSMajt1kGO6ylyWBJ3NsByMyVR\naRbSZ0rKr5aIDNPzUQk75LQqVZwCN1JfXiTOKWwcI2Q8p2RJjsmCXJCVOCDv4VPZg+/lktiWHivQ\neRCyyS3N5MhOyMNFkCsrID9phvzPtXGyT48Skrs6yaiBnSS150f8/6y3yNx27ZLyrrv8TYeU0nsb\n7s/mOwLcDnxkOL4TeNckXi9gC3AMuMaftFnnQtVmh5SkJA9/VOGjkserSOrOkfXZJI9RUt7Dp2F/\n6Fy3aNJlP6bLndSwBy6nlYzlsjqsP9shBGxbxZVhr7c/W1HO+BH3tLxMjD3g8Zj/+F1gr35IEnZJ\nS4JBXAn7/dFbbt6QUgZPIARNsS2lnAPMEUJcB4wFbvA3jzFjxtj3k5KSSEpKClb1cp4LpWHO59Dz\nXrbsW0v7S7+xlCT+og1bqR/u2gFQh21sw1lZWpV9HKAqRKdBq/FqjABg5izYbLMkyl3Tos9RzI+4\n8RTAYHx+Bd5mNLXZziUKcpjypKOsCWJJI4YrlOUIM+nLPUzmpoJT+fbrcUBN1paDB+t2ZXXadVza\n3xVSW4AUUPAUdeu8yeO8TfJ6ZZq8slwMh2JKM/DfdZy/UjaYl6/JUyzN2kKEL4mBUvssNBx7VRll\nxdkFlPQnrapK7sOjysi23TxE0vd2CZnyPj6WW6grixCoeiU4W2X2y5W0sAeU5bBznFvuc/QG2v8n\nAr6CctdWllSPJ/fFu4edizWPuzyurhxb9G45r2BruSa2ptxHJVmGf8N+fXqLpA0pZc6qjKKBnUAi\nEAesBeq7xKlp2G8OHLCa1pAuhM126PApEGLPSR5oosYUSJc/coOUIItxOscfniiuyEd52x4wiaEO\n1RBSUvCEs2oo5mIEPPC5e0vgpCzPIflRzEApQc4qX1M+Ev26bChWu8cXGXJ4w2vlngTnE6sqIJdV\ninLLfDa3ydIckU/xhqzEAa/1KMgFqcc28uKGlDIHBYKUEqAbsA3YAYzKCksG7s/aHwFsBFYDfwBt\nvKX1UEYIm+3Q4VMgICWFj0ieKSbp9IyMJl2uo5GcTj/nxjgHtmkMkBLkIL5wPldhleS5gg5BkPRi\nBDzoeqPoIUmcYfyj5fvyquQY+XEz7wl/4zr5Db18FvATnWQPvg//deotGxtSyhwWCDmx5VaB0K2b\n8x+0aJGHP674bsnTZSRVf5fFOC3PUkSmEy3v4VMZRWgHFqO4Ym8guvKDoU57nHsE/W+RFDgVAQ+5\n3rxu4oqk0nIphjaXte8rIaOSG8oajyDThYpwuAjyvVbIvyup468aIkuMQI67upBphhcoKGdzm/yJ\nTnIPifINnpLt+FUKMqTA3KpLb5GyIaUMXjtsaWJaTpBb5yHceSdMm+Y4vnQJChb0ELnmIrj1Lvh4\nOYVPl2Yqd3Irc5hJH/oxMyT1u4FFLKIrALfyDXO4FZBwzXtw46Mq0vg9cKpaSMrX5CDVf4E2b0Hc\nefjnfii/BspugrMVYf1AqLYUSm2HRjMgvRDEXiQ6A278rTkDN0jKny5AUsbfbKAhjdjolPUvdCSF\nShThPJlE8QrPsY4mSLuzA0k0GVQglQf5gMV0YjGdss5ohwihI4cnpuUUeUUgSOnDKdXV78H1o2Hi\nOjidSHFOcpKSvMMjPM7bQXp5JKtoSYssb6P/0Jxr+ZM0CkD5tfBAMxXtQBv44he44kmCqYl3v/wS\nhCppIpMym6DhDOgw1u1Uk99u5Ny52nTbfYXmJ87QOHMLLVFrTh6iAhVJZS+JxHCFyqR4LeZLBjGb\n3lRjL1eI4QQlWUMzdlKLTKJDcmn5Ay0QIopBg2DqVMexT4EA0GM4VFwFU36ESyVoyhqmMZD/cTsv\nBOAQryTHGcB0lpJEDXYzl54AfMq9vMWTbCFr4YZG0+D2O+FseZi4QZnG+qBTJ2tLHEY6a9fCkCGw\nalW4axKhRF2BmItQ4CzU+xbqzoVai+BKAYi57B5/4ZvUXJ1E4bRo4rhCtLjMlvhinC15FFpNJGrV\nfdTYXYtKpDCUjxnIV/akc+hJL75zym4TDbiKzQznAz7kAd2rsEwOz1TOqU1VJfdx8KCUCxY4dHpS\nSvnzz1Lef78XvZ/IkHR7ROntOz0jiT0ny5Eqt1BXfso9siIHveoNo0mXddgq2/CHTKWcW4R9VJEx\npBmCMh2mpB2f9zjb12x79VX3sORk87gpKc7Hw4fnhA7V+9apk+N/ufvu8NcnV24iQ1Jil6TVB5Ln\nC6jn6N5rHWNPI0u4T140bsnNJPH73Z67OC7JR3lbLqSLfIeH7SfO4xjr+Jo+8hWekd/QS9Zmm2Pm\nvN6yNqSUegwh4nn9dRjleeE0RdXf4bZBSp87YRPF5Dk+Zij9mMluqlODPWyiAZlE8TOd+Z6bGcB0\nhuK8wGwLVrGaFjRlDbuoyVniHScrroL7W6n9H96B5Y/4dR0ffQT33+8cJqV5L8g1fOJEGDbMPZ6N\nmBi4csWv6vjN0KHw8ceqbnffrZYn1ASJqCtQNBVK7oJLCXCyhvL4C1DwJNReAFe/D1X+9pzHmYrw\n7ZdwrB6crQRIKpDKAKbTlR/5m9YksZT2/G5PcpaiLOcaDlOetTSlBCc5Qllqs4PxPMZ+qtonD+Z9\ndA8hVzBunEUJH5UmGdZQTV7L8udTly2yDX/IJ3hTvs9weZwSUoLcwFVSgkxmoizGadmAjdKjbXmv\nwY4vtGEN1RyDAL5Azp93D5PSPK5r+IQJ3vOuU8c8/NZbrdcvOVnKvXs9n3/jDUfdBg/2//r1FqQt\n+pKkxk+Sa8YrM+db7pU8WcG9NzHkGknLCZJmn0iq/+zUqyjCWZnIHjmOEXIZ18qxPCuX0l6mUk5O\nQc3zOEZJKUEeoJJ8hldkInvCf+0h3ZBSBq8dzn0+mXMJllc7yoyFT5bDzUPhzq4wczbbzil3En9x\nLQAP8YFp0s1cpXaKpkK9ObDzRqg9H3o8pMLXDoY/RihPowFSuHDASWnVChIS4PRp5/AhQ+CTT7Ie\nZxMeegi+NV+B1M7996veS0ICJCaax6lWTbkJ1kQAGQVgd2e1Acz91HEu6grEH4THqkN6YWgwG2q4\nWzKcP16b8wdbM2pfO1j7CmQ6N1+DUIN53ZnPUD6mN7N5lecAWE0zYrjCWpqyhI4cozT7SKQH86lE\nClO5k6OUIZoMdlIr345haIGQAyxYoCyRjNZITqQXhm+nQPuxMKyJch+94kFlCnqmMm6+g0ptg6s/\nUG6i27/qnt+ObqobfqFMUOp/5YpS7xipVAlSUqB5c1htsnRyw4bQsiWcOuUuHNu1UwLBE1aEqW0B\nctvvxYtQqJBznMqVnevtSQAFg6++gjvuUGopX/7sNS5kxqhnfYzrHySh9g/KVLbeHCU4iqVAq4lw\nS5Ye81RVWH8nnCuv1E6nqrHgVFcWZPYAoDyp9GIOZylGIvuIIpOb+Z5SHOdqVrCJq7hEQa5jGU1Z\nZy/5CtEsogtTuZP9VGUFV+cLNZQWCDlAfDyUL+8jkoyCX1+AA9fCwBuh8t9QfF/WOQHbeyiroGaf\nO6c7VVWtQXC8LhQ+CpeKq15HEDH7yrY1rv/8Y96Ar3O8W5w8CSVKOI4HDYI+faBJk8DrZBMEtrJd\nBRZAhw4Q6+FW/PQT3OC3+0XPxGcN24wf759AeOQRePfd4NUjbyFgR3e1/f2YIVxCtV8hKh3693J8\nFB2rC6W3OaJNm8/hXTfwYaaXgSwXorlCMpOox1ZasZJhTKQlqyjEJY5Qht3UYAxj2EFtynKE6uzh\nd9pxkMpEkZnrTWi1QAgRAS+QvbszvJzlbbPwUaj/rZpgVOAsJByAbTerFcrOlwMkTr2HIPUIrFCr\nFpw75/m88fqLF4err4YVKxznChVyCBUpneNbuXe2ON4EQnKyWtXOJtDq1nWc69zZdxn+UKCA+k1w\nX2DOK2W1Y9MAELA3Se2+et7lnISWk6DBLBioegmseFCZz56sAZv7wPkyePLYm0EME3jQLbww53mI\n9xnKxyzkRq+1O0cRFtOJ3dQglnQmkcwhKnKK4hEvMLRAyAECVlVcKKNmnHoktG6o1671fG7BgtBb\nCHkjKQk+MB9acaJVK0c9R42C557zHv/XX1XPwhOvvgrPPuse3qmTuerMEytXqrppgo2AVQ+oLfYC\n1P1OzdautRBaj3eMrwH8/ShUXwynqsM/Q5WqNr0wZu/VBYrwBiN5g5FZIc4fY+U4TDxniCaDG/mB\n4pyiHb9ThQM8wIfEkGFa20XcwDO85rRCYTjRAiFEuH7lhlJ/nV2aNFEqnocegvffdw73RJEi3vMM\nuIdkkvbxx+Htt53Devf2v5woC+OEDRt6Px+XpUaeORP69nWECwHNmnlOV7EiHDrkODbr0ZjRvj38\n9pvvOqWlWcsvX5FeGDYOUPu/ZM3EjrkElVYoNx6FjsORhhB3Dgb0VGtw2zhXDmbPcPRE3HB+8P6l\nPP+i9MJm652UJ5UmrKMyB2nDX1zLn0zmHopyjoV04yQlmE8PlpLEZhqwi5qU4SjFOEsqFSjHv6QT\ny0GqZO+e+EALhBDTvLn7gt0PPwzvvRee+phx661KIGRmhq4MM4FoRUi2aOG9kc6O4DFidWF1W3n+\nWi/ZhFHZsnDkiPV0VQzvf6NGsMFlae5p06BjRyVwNBa4UhD2tVebK0X+BRkNPYapMYq7O0JaYdjW\nEw43VeN7+9sSSM/8MBU4jFqb+1OGOJ37iPt5ijepzEEe4ENasZLSHAeU+qko57lIQQpxyZ7mIJWo\nTErQdQRaIISIQYNUI/DEE+rY2HC9+WZkCQSbaWl2BMLrr3s/708PKRy9qzp1rMf96SelVhLCet1s\n8VwFiT/XVqqUeb7GAXtNNjhfTv3OmqV+o9JVT6L+N8rI44YsddG6O6Hov1BuPRxqqYTElYJKmByt\nD3s7upnEeiOFyjzOePuxIJPGrGcDjcgkmjguk0YBYkkjnjMU4TzxnCGWdAiyqkkLhBBRrpxDGIDz\nix8X5z6QGk5s9ShaVKmCWrSAJ58MbhmJidb9CBnvy7XXup/fZjAkiTOxBPzmG7jtNs+N7cMPey/T\nF7YB6cxMVX56uvf4Rmw9BauCwFe9qlf3bEnliRYtlHWYxgeZsbD7BrUBIJVH2ZuT1ezsKwWg4Cnl\nMLKhi7fi82UgrYgSDns6Ko+zx+opwXGxZFYk8z9XEsU6mtqP01AWC+nEcZzSHMe3D7JA0QIhl1Oj\nBuzenf18tm5V6onXXlONlhV9uxFfDdyUKTBpkrNQsNIovvsubN6sXIpfyuoxG7/mzSbO+TJn9Vau\n7Vy1arB3r/v5QIW4Ld/ixdX8DWN+nsryxTffmAtMI2+/rcZgjARLNZhXHB9aR8CeTvDuTvdTs7/O\nipIJlZYrIXHVTDU/ov1YKL3dPMsrcTDtBzWgHQFogZBD2BqSqlUdYfPnQ48ezvEGDVKNp1W+/x6u\nuir79TOaZIaCQoXU1rWr77iujW6DBmrimZXGuGVLJSQDxdZwlysXWCPtiTfegNRU9f+WL+8slEaO\ndPh82rlTmfQGC7N7dtnEealVXnwRXnpJ7X/3nepRRkpPNyKQUXCwjdpWZf2pi18FkaEm1kWnQakd\nyvNwags16W5wJ/i3IexvB8sfhmPug9I5hRYIOYStAdi3zxFm9iVrs2e3SjBexu7ds59Hdnn+eRjr\n7pLfiblz3SeTuX7tZ/d+GOdGANx7r1rz4vrrs5f/wIGO/W+/haYOjYDTNVQJ0IikWzdYuNBa3Pbt\nVa8rECpVcuxrQeAHMhoyopULj9TmarPxz/1wzTtQfK8SDlLAkUZwsjog1DhFuXVQ5Iia0b2vvZqI\nGnMJjga3mloghBGzSUz+vmS+1C61aqmvTm80sOjqqFgx2LXLPXzePOWOwl+MdX/5Zdi0STWWri4o\nbNx8c+D5B5qmdGnzcYrs0KuXY79cOWeTUX/+f2Nd582zbso6caLaSpRQrkUCxayuEyYoa6hAnod8\ny9mK8HOWVYbIgJo/qfkTl4orFVSjrwAJxVKVYOnwH+XFOPoKjAluVbRACCNFiyp9svGrKxCWL4dr\nrnZRYw8AABBJSURBVDE/d9NNyp1CsChjMhnaqPYy01lbxdbAtWgBbdvCv//6n4evBnXJEmjcWO2b\n9UqMjewrr6i5Bv7Ww6rBwJEjynLoww8dYd7S3XILfPop1KzpXtdAnPgdP+6c7sMP4YEHvKepXNl3\nXa+7zv+6aLKQ0bCzm9p8R4YgO+HLny79chFWvvquvtrzuZzu1j/2WOBmosbBzp9/hvXrfafxt6yk\nJCiZZeRhZkllzO/ZZ1UPyxgWyP0cMsQ8vEwZNXjvKX/XcZ3kZPURESxcDQeSk33fz9atYfLk4NUh\nP+Ft4mJgBP/l1gIhzHhrYJo1C32D7o/FSbDr4tr4GI8LFvSsOrJx/fXQpYtzmLGR89W4mVlS+bKu\nuu0252NfZfzf/6kFerzhSSBs3eocTwjn8+GY/V6okMPMNS+MIYwc6TuOv3hSbfo7PhgOtEAIM7YG\n6NZb3c8VLJj9/K/3Yc0WSS+1v+aQixe7D8zbrmfuXM/rJNiIj3fugW3caO5sztbw/vyz/4O+9f00\nGLHVf9Ik/9J5y69Pn+zlkZqqfk+edH4mbXX1JUTnzHHsm6kcQ4UV02l/nRFmh9ywNocWCDmEJ7cI\n5copXzU2vzjGeN665lb0tIcOqTEEgJ491QxpG0lJFlxyuxBq4fHaa8Fb4vLmm63V1ygwfZnvdupk\nvfyNG63HtaqSCvT+W3HJ4Q3X58TV06y3SYwNGjiEbExM6M2bjVgRhME2GADP/1MkfXx5QguEHGLI\nEM8OyNq1czQKjz+uLG5AvTy2h+izz5zTPP207zIrVHDsP/KIs6njkiWOL79IoVEjuOuu7OXh70tn\nG6ANlJdfdtjlG/Fnbkigqp9A0i1f7h7Wtq0aG/CFP27JbRgH2IOt4nJ1eGijZUv3ME8fP2XLOg/q\nBwNP16kFgsaOEN5dDNx8s9I1CwHPPOMwB7Q9RK4umW1fNmYPWbt27i+FEKo3EklEggfYd95xX+Kz\nXj3nnkPNmp6XEh01Cl54IXt18HYf+vRxLtvfMQTX58PMAOGVV+Cvv/zPK5AGzmqa0aN9x/E0xmRT\naxnNqb256khO9t854DDra+7Y0QJBY5miRR3WKNHR7rpNTy+/LdyoS1+8GP780z1uw4bWeha5GX9f\nuthYx2pnNrZsUeorGxUrwnnXdViCyO23u4fZ9N+33OIoO9wCNFCBEMhAuJm/Kdfzd99tLjhs97NJ\nExg+XO17+hhznYhoFZulmj9ogaDJNsnJarasL4ubtWuVTxxQD7/xBYiLc7hD8NcRmo2PPgp+1zrY\nvPSS7wVwIhGjbT/AokVqdrQvXBuxr75yuAq3NT7lyvluiKw0VA884FgDw3UMwZ/G1F/vsJ5o2VJZ\n7Qwa5Aiz3cfHDKtt2vLx5BE2FELWkzDLDQLB0sQ0IUQ3YDxKgHwqpXzd5fwdYF9K6CwwXEq5Puvc\nXuA0kAmkSym9WM1rXHnnHcf+nj3KuyU4Gn8rD5nRd02gfnKGDg0snTeC/TJmV3UTCgJpBDyt9exr\nwtuAAWrhno0blcXW8ePqS9aXmwrXPAsUcPd3NHGi73RW8ndNs2+fqqvtubZh9dmoXVv9jhmjZuRP\nnWqej6/5PLZ4zZrBmjXWyvaEcazOSG4QCD57CEKIKOB9oCtwFTBACOFqt7AbaC+lbAKMBT4ynMsE\nkqSUzbQwyB62MYDz5x2WI/5Omrr7bofX0HCTmBh4jyUv4s/6yqNHexYcNkqVCqwR2rRJ/frr8dYM\n4/PpKtCKFlXOHm0fN0Z83QtXa6W2bc2v1ZdgcVUZZbfRNq446EqeEAjA1cAOKeU+KWU6MAPoaYwg\npfxbSmkbmvsbMDpjEBbL0fjA9oJm50UVInImyMyfH3mWTpHKH384Dy7/5z/memxvjc7cuebhrmlq\n1lQNpKc5AzZBlN0xBG9uyoWANm0cx67uXVxdtVgxmDCb5e1JYBw/7js/M2rV8vx+BlsghGKRLStN\nSyXggOH4IM4NvitDgB8MxxL4SQixUggRAsVD/sFMIOSGrw5PFCtmvgqYxh3bugfZsezxNIO2WDH/\n8nNdT9uTBZYNY8PvT4/WeN7bBL/Ll5XJspHevd1NaQ8cwCOuPQRfg8Y2AxCj0LJRqpS5eW+w39Wk\npODmB0H+chdCdATuwTGeANBWStkc6A48KITQrq8CJBg9BE3OUaBA8CdiBbtRSU31vaCQK65f1aNG\nefc7ZTOR9jYGkpHhvcwJE6zlb2PWLNVrMIYVL+5uXhqolZFNzVW1qvlHjZl5b6D/3fTpgaULBCuD\nyimAYVkXKmeFOSGEaIwaO+gmpTxpC5dSpmb9HhVCfItSQS0zK2jMmDH2/aSkJJJCIQJzMVog5C5C\nMVZTpIj/M8y9EYy8ChVy/0I/cULZ/vszLuKKrQE9eFCpjGyrA7bIxjLC//yjBtlts85tgqBOHTh6\n1PMAe6tWsHKleZ6//goXLigh4E2w+CsQ/vtfZSbev78yGFAszdq8C8lAsSIQVgK1hBCJQCrQHxhg\njCCEqAr8DxgkpdxlCC8MREkpzwkhigBdAJN5nQqjQNC4I4TypZ+b1USa7BEXlzvGXUqUcKw9bcST\nADI+066LEdnGD2zH775rvR6uDXT58sonkyuLFilh++absN2w2uWUKcrPWEyM50Frq7PS/X1vXT/8\nlGv5JCAJgAcfhIkTPTanAeHzW1NKmQE8BCwCNgEzpJRbhBDJQoj7s6KNBkoCE4QQa4QQK7LCywHL\nhBBrUIPN30spFwX1CvIZ336rBUJepFat7H352jB7NgJ9Xjyly465sLHH4Gl2sC8fRKX9WGPeV11t\n3mtt4yBt2ribWFtdeMgMo8trTz17T0u+tmzpLGz69YMVK5yX4Q02li5VSrkQqOsSNsmwPxRwGzCW\nUu4BPFjlajQaG5s2RZ6g9+Rt15upsK9r6N8fjh1TX/nGwWwzSyTXvO67T6l86tQxz9us8ffl2M91\nwlqw/4OWLdU61MaefcuWanxj/nx1/MEHahxm3TrntNWquTtJbNVKzd1wdYUeLLQ2Og8RaQ2Kxjpx\ncZE3J+O339zXZABV10CXJ23XDr7+WjWIrtZJ99zjfNyhg7O78Weegdmz/SvziSeyN5aT3SVtmzVz\nDxs/3jGjfN48pVobNUqt4Oep7BtucB+81gJB48QPPzh/AYXb140mMgl0XY0qVfy3kvJ3/QcjNo++\ntuf45Zdh//7A8wP/5t089JB/6qFixbx7iZXS2QmezUtA27aOsB49VJn9+zu8HNswNviLFoXGVbcr\nek3lXEw3K8uuavI9jzzie6GkYBCsD5Kccgdu8yhsw9+JXmfO+Bf/zTcdjX4gnmpzAt1DyENolZHG\n7BkoWNB8jYBIJZjrRnsjJ1dLA2fPup07+7ZOCsf7rHsIeQhPHh01mnDjbeKakW3bsr9oUW7ghht8\nr6oXDC+1/qIFQh5i4cLQ+u3XaALFdeKaJzxZEOUWgqE2E8K3Z1tbvGCjVUZ5iNKlfS8sr9HkVYwW\nSd7IaeeO/giJ779Xa6GDHkPQaDSagHn5ZTXHwRc1aijVlBWMjfL58+6O/Vwxa/z9MSe+6SbHxDvd\nQ9BoNNkiPxsWxMZa955rVTVlbOALF1ZzKPzlxhvNl7T1he4haDSabJGdeQCa0BAdbe4m2xOuS5Tm\nJFogaDR5iDFj4OLFcNci7+DaKPu7dkR2ytQqI41Gky2iogKfmazxTbt2sHev5/PBMP0OZw9Bm51q\nNBqNCa+95u7CWwjPlnzHjgV3BcBw9BCEjBAHOEIIGSl10Wg0mnAihFq3oXhxz+d37oRatQRSyqCJ\nBq0y0mg0Gg2gBYJGo9FEJOFQmGiBoNFoNBpACwSNRqOJSLTZqUaj0WgsEYo5EdrsVKPRaHIZGRlq\nzkmw0T0EjUajyWWEQhiAFggajUYTkWhfRhqNRqMBwuOCRI8haDQaTYQRLqcNuoeg0Wg0GkALBI1G\no9FkoQWC5v/bu5sQq8o4juPfn5WS2YtSKjj5Ei4kyaxoCJTeIB0KtKUG9gJCmyhalFpELVrkqoRo\nIRmYUS6KcoKiMWQWLnwBZxqpyabEXsyZWoQhQZj9W5znNoerNnds7j0zPr8PXOY5/3vOcJ7fnHsf\nzrnnmWtmBnhAMDOzpKEBQVKHpK8lfSNpwzmef0jSF+mxV9KSRrc1M7PxYcQBQdIk4HVgJbAYWCtp\nUd1qR4E7I+Jm4GVg6yi2tTrd3d1V78K44ByGOYthzqJ5GjlDaAcGIuL7iDgN7ARWl1eIiH0RcTIt\n7gPmNLqtnc0HfME5DHMWw5xF8zQyIMwBfiwt/8TwG/65rAc+vcBtzcysImM6MU3SPcBjwPKx/L1m\nZtZ8I36nsqQ7gJcioiMtbwQiIjbXrbcE+ADoiIjvRrNtes5fqGxmNkpj+Z3KjZwhHAQWSpoHnADW\nAGvLK0iaSzEYrKsNBo1uWzOWnTIzs9EbcUCIiDOSngC6KD5z2BYR/ZIeL56OrcALwAzgDUkCTkdE\n+/m2bVpvzMzsgo14ycjMzPJQ+UzlHCauSdomaUhSX6k2XVKXpCOSPpN0dem5TZIGJPVLWlGq3yqp\nL2X1Wqv78X9JapO0R9KXkg5LejLVc8xiiqT9knpSFi+menZZ1EiaJOmQpM60nGUWko6lSb49kg6k\nWmuyiIjKHhQD0rfAPOAyoBdYVOU+Namfy4GlQF+pthl4NrU3AK+k9o1AD8XlvPkpn9qZ3H7g9tT+\nBFhZdd9GmcNsYGlqTwOOAItyzCLt99T08xKK+TvtuWaR9v1p4B2gMy1nmQXFRN/pdbWWZFH1GUIW\nE9ciYi/wW115NbA9tbcDD6b2KmBnRPwVEceAAaBd0mzgyog4mNZ7u7TNhBARgxHRm9qngH6gjQyz\nAIiIP1JzCsULOsg0C0ltwP3Am6VyllkA4uyrNy3JouoBIeeJazMjYgiKN0pgZqrXZ3I81eZQ5FMz\nobOSNJ/irGkfMCvHLNIlkh5gENidXrxZZgG8CjxDMSjW5JpFALslHZS0PtVakoW/MW38yObTfUnT\ngPeBpyLi1DnmoGSRRUT8Ddwi6SrgQ0mLObvvF30Wkh4AhiKiV9Ld/7HqRZ9FsiwiTki6DuiSdIQW\nHRdVnyEcB+aWlttSLQdDkmYBpNO7X1L9OHB9ab1aJuerTyiSLqUYDHZExK5UzjKLmoj4HegGOsgz\ni2XAKklHgfeAeyXtAAYzzIKIOJF+/gp8RHFpvSXHRdUDwr8T1yRNppi41lnxPjWL0qOmE3g0tR8B\ndpXqayRNlrQAWAgcSKeJJyW1SxLwcGmbieQt4KuI2FKqZZeFpGtrd4pIuhy4j+IzleyyiIjnImJu\nRNxA8R6wJyLWAR+TWRaSpqYzaCRdAawADtOq42IcfKLeQXG3yQCwser9aVIf3wV+Bv4EfqD4f0/T\ngc9T37uAa0rrb6K4W6AfWFGq35YOjgFgS9X9uoAclgFnKO4m6wEOpb//jAyzuCn1vxfoA55P9eyy\nqMvlLobvMsouC2BB6fVxuPae2KosPDHNzMyA6i8ZmZnZOOEBwczMAA8IZmaWeEAwMzPAA4KZmSUe\nEMzMDPCAYGZmiQcEMzMD4B+NsW4VLW58IgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118b9c6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmXQ3AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1184b23c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
