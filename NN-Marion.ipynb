{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import impl.layer as l\n",
    "\n",
    "# Dataset preparation and pre-processing\n",
    "mnist = input_data.read_data_sets('data/MNIST_data/', one_hot=False)\n",
    "\n",
    "X_train, y_train = mnist.train.images, mnist.train.labels\n",
    "X_val, y_val = mnist.validation.images, mnist.validation.labels\n",
    "X_test, y_test = mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-processing: normalizing\n",
    "def normalize(X):\n",
    "    # max scale for images 255= 2**8= 8 bit grayscale for each channel\n",
    "    return (X - X.mean(axis=0)) #/ X.std(axis=0)\n",
    "\n",
    "X_train, X_val, X_test = normalize(X=X_train), normalize(X=X_val), normalize(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l # or from impl.layer import *\n",
    "from impl.loss import * # import all functions from impl.loss file # import impl.loss as loss_func\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "class FFNN:\n",
    "\n",
    "    def __init__(self, D, C, H, L):\n",
    "        self.L = L # layers\n",
    "        self.C = C # classes\n",
    "        self.losses = {'train':[], 'train_acc':[], \n",
    "                       'valid':[], 'valid_acc':[], \n",
    "                       'test':[], 'test_acc':[]}\n",
    "        \n",
    "        self.model = []\n",
    "        self.W_fixed = []\n",
    "        self.grads = []\n",
    "        self.dy_prev = np.zeros((1, C))\n",
    "        self.y_prev = np.zeros((1, C))\n",
    "        low, high = -1, 1\n",
    "        \n",
    "        # Input layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.), b=np.zeros((1, H)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(D, H), low=low, high=high) / np.sqrt(D / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Input layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[0].items()})\n",
    "\n",
    "        # Hidden layers: weights/ biases\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = dict(W=np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, H)))\n",
    "            m_L.append(m)\n",
    "        self.model.append(m_L)\n",
    "        # Fixed feedback weight\n",
    "        m_L = []\n",
    "        for _ in range(L):\n",
    "            m = np.random.uniform(size=(H, H), low=low, high=high) / np.sqrt(H / 2.)\n",
    "            m_L.append(m)\n",
    "        self.W_fixed.append(m_L)\n",
    "        # Hidden layer: gradients\n",
    "        grad_L = []\n",
    "        for _ in range(L):\n",
    "            grad_L.append({key: np.zeros_like(val) for key, val in self.model[1][0].items()})\n",
    "        self.grads.append(grad_L)\n",
    "        \n",
    "        # Output layer: weights/ biases\n",
    "        m = dict(W=np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.), b=np.zeros((1, C)))\n",
    "        self.model.append(m)\n",
    "        # Fixed feedback weight\n",
    "        m = np.random.uniform(size=(H, C), low=low, high=high) / np.sqrt(H / 2.)\n",
    "        self.W_fixed.append(m)\n",
    "        # Output layer: gradients\n",
    "        self.grads.append({key: np.zeros_like(val) for key, val in self.model[2].items()})\n",
    "        \n",
    "    def fc_forward(self, X, W, b):\n",
    "        out = (X @ W) + b\n",
    "        cache = (W, X)\n",
    "        return out, cache\n",
    "\n",
    "    def fc_backward(self, dout, cache, W_fixed):\n",
    "        W, X = cache\n",
    "\n",
    "        dW = X.T @ dout\n",
    "        db = np.sum(dout, axis=0).reshape(1, -1) # db_1xn\n",
    "        \n",
    "        dX = dout @ W.T # vanilla Backprop\n",
    "#         dX = dout @ W_fixed.T # fba backprop\n",
    "\n",
    "        return dX, dW, db\n",
    "\n",
    "    def train_forward(self, X, train):\n",
    "        caches, ys = [], []\n",
    "        \n",
    "        # Input layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[0]['W'], b=self.model[0]['b']) # X_1xD, y_1xc\n",
    "        y, nl_cache = l.tanh_forward(X=y)\n",
    "#         y, nl_cache = l.sigmoid_forward(X=y)\n",
    "        if train:\n",
    "            caches.append((fc_cache, nl_cache))\n",
    "        X = y.copy() # pass to the next layer\n",
    "        \n",
    "        # Hidden layers\n",
    "        fc_caches, nl_caches = [], []\n",
    "        for layer in range(self.L):\n",
    "            y, fc_cache = self.fc_forward(X=X, W=self.model[1][layer]['W'], b=self.model[1][layer]['b'])\n",
    "            y, nl_cache = l.tanh_forward(X=y)\n",
    "#             y, nl_cache = l.sigmoid_forward(X=y)\n",
    "            X = y.copy() # pass to next layer\n",
    "            if train:\n",
    "                fc_caches.append(fc_cache)\n",
    "                nl_caches.append(nl_cache)\n",
    "        if train:\n",
    "            caches.append((fc_caches, nl_caches)) # caches[1]            \n",
    "        \n",
    "        # Output layer\n",
    "        y, fc_cache = self.fc_forward(X=X, W=self.model[2]['W'], b=self.model[2]['b'])\n",
    "        y_prob = l.softmax(X=y)\n",
    "        if train:\n",
    "            caches.append(fc_cache)\n",
    "\n",
    "        return y_prob, caches # for backpropating the error\n",
    "\n",
    "    def cross_entropy(self, y_prob, y_train):\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(y_prob[range(m), y_train] + l.eps) # to avoid the devision by zero\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_prob, y_train): # this is equal for both since the reg_loss (noise) derivative is ZERO.\n",
    "        m = y_prob.shape[0]\n",
    "\n",
    "        #         grad_y = l.softmax(y_pred)\n",
    "        grad_y = y_prob\n",
    "        grad_y[range(m), y_train] -= 1.\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "\n",
    "    def loss_function(self, y_prob, y_train):\n",
    "        \n",
    "        loss = self.cross_entropy(y_prob, y_train) # softmax is included\n",
    "        dy = self.dcross_entropy(y_prob, y_train) # dsoftmax is included\n",
    "\n",
    "        return loss, dy\n",
    "        \n",
    "    def train_backward(self, dy, caches, y):\n",
    "        grads = self.grads.copy() # initialized by Zero in every iteration/epoch\n",
    "#         dy_prev = self.dy_prev.copy() # for temporal differencing\n",
    "#         self.dy_prev = dy.copy() # next iteration/ epoch\n",
    "#         y_prev = self.y_prev.copy() # for temporal differencing\n",
    "#         self.y_prev = y.copy() # next iteration/ epoch\n",
    "        \n",
    "        # Output layer\n",
    "        fc_cache = caches[2]\n",
    "        # softmax_backward is included in dcross_entropy.\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[2])\n",
    "        dy = dX.copy()\n",
    "# #         dy =  dy @ self.W_fixed[2].T # done\n",
    "#         dy_prev =  dy_prev @ self.W_fixed[2].T\n",
    "#         y =  y @ self.W_fixed[2].T # done\n",
    "#         y_prev =  y_prev @ self.W_fixed[2].T\n",
    "        grads[2]['W'] = dW\n",
    "        grads[2]['b'] = db\n",
    "\n",
    "        # Hidden layer\n",
    "        fc_caches, nl_caches = caches[1]\n",
    "        for layer in reversed(range(self.L)):\n",
    "            dy = l.tanh_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#             dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#             dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "            dX, dW, db = self.fc_backward(dout=dy, cache=fc_caches[layer], W_fixed=self.W_fixed[1][layer])\n",
    "            dy = dX.copy()\n",
    "# #             dy =  dy @ self.W_fixed[2].T # done\n",
    "#             dy_prev =  dy_prev @ self.W_fixed[1][layer].T\n",
    "#             y =  y @ self.W_fixed[1][layer].T # done\n",
    "#             y_prev =  y_prev @ self.W_fixed[1][layer].T\n",
    "            grads[1][layer]['W'] = dW\n",
    "            grads[1][layer]['b'] = db\n",
    "        \n",
    "        # Input layer\n",
    "        fc_cache, nl_cache = caches[0]\n",
    "        dy = l.tanh_backward(cache=nl_cache, dout=dy) # diffable function\n",
    "#         dy = l.sigmoid_backward(cache=nl_caches[layer], dout=dy) # diffable function\n",
    "#         dy *= dy - dy_prev # temporal diff instead of differentiable function\n",
    "#         dy *= y - y_prev # temporal diff instead of differentiable function\n",
    "        dX, dW, db = self.fc_backward(dout=dy, cache=fc_cache, W_fixed=self.W_fixed[0])\n",
    "        grads[0]['W'] = dW\n",
    "        grads[0]['b'] = db\n",
    "\n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X):\n",
    "        y_prob, _ = self.train_forward(X, train=False)\n",
    "        \n",
    "        # if self.mode == 'classification':\n",
    "        y_pred = np.argmax(y_prob, axis=1) # for loss ==err\n",
    "        \n",
    "        return y_pred, y_prob\n",
    "        \n",
    "    def get_minibatch(self, X, y, minibatch_size, shuffle):\n",
    "        minibatches = []\n",
    "\n",
    "        if shuffle:\n",
    "            X, y = skshuffle(X, y)\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            X_mini = X[i:i + minibatch_size]\n",
    "            y_mini = y[i:i + minibatch_size]\n",
    "            minibatches.append((X_mini, y_mini))\n",
    "\n",
    "        return minibatches\n",
    "\n",
    "    def sgd(self, train_set, val_set, alpha, mb_size, n_iter, print_after):\n",
    "        X_train, y_train = train_set\n",
    "        X_val, y_val = val_set\n",
    "\n",
    "        # Epochs\n",
    "        for iter in range(1, n_iter + 1):\n",
    "\n",
    "            # Minibatches\n",
    "            minibatches = self.get_minibatch(X_train, y_train, mb_size, shuffle=True)\n",
    "            idx = np.random.randint(0, len(minibatches))\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            \n",
    "            # Train the model\n",
    "            y_prob, caches = self.train_forward(X_mini, train=True)\n",
    "            _, dy = self.loss_function(y_prob, y_mini)\n",
    "            _, grads = self.train_backward(dy, caches, y_prob)\n",
    "            \n",
    "            # Update the model for input layer\n",
    "            for key in grads[0].keys():\n",
    "                self.model[0][key] -= alpha * grads[0][key]\n",
    "\n",
    "            # Update the model for the hidden layers\n",
    "            for layer in range(self.L):\n",
    "                for key in grads[1][layer].keys():\n",
    "                    self.model[1][layer][key] -= alpha * grads[1][layer][key]\n",
    "\n",
    "            # Update the model for output layer\n",
    "            for key in grads[2].keys():\n",
    "                self.model[2][key] -= alpha * grads[2][key]\n",
    "            \n",
    "            # Training accuracy\n",
    "            y_pred, y_prob = self.test(X_mini)\n",
    "            loss, _ = self.loss_function(y_prob, y_mini) # softmax is included in entropy loss function\n",
    "            self.losses['train'].append(loss)\n",
    "            acc = np.mean(y_pred == y_mini) # confusion matrix\n",
    "            self.losses['train_acc'].append(acc)\n",
    "\n",
    "            # Validate the updated model\n",
    "            y_pred, y_prob = self.test(X_val)\n",
    "            valid_loss, _ = self.loss_function(y_prob, y_val) # softmax is included in entropy loss function\n",
    "            self.losses['valid'].append(valid_loss)\n",
    "            valid_acc = np.mean(y_pred == y_val) # confusion matrix\n",
    "            self.losses['valid_acc'].append(valid_acc)\n",
    "            \n",
    "            # Test the final model\n",
    "            y_pred, y_prob = nn.test(X_test)\n",
    "            test_loss, _ = self.loss_function(y_prob, y_test) # softmax is included in entropy loss function\n",
    "            self.losses['test'].append(test_loss)\n",
    "            test_acc = np.mean(y_pred == y_test)\n",
    "            self.losses['test_acc'].append(test_acc)\n",
    "#             print('Test accuracy mean: {:.4f}, std: {:.4f}, loss: {:.4f}'.\n",
    "#             format(acc.mean(), acc.std(), loss))\n",
    "            \n",
    "            # Print the model info: loss & accuracy or err & acc\n",
    "            if iter % print_after == 0:\n",
    "                print('Iter-{}, train loss-{:.4f}, acc-{:.4f}, valid loss-{:.4f}, acc-{:.4f}, test loss-{:.4f}, acc-{:.4f}'.format(\n",
    "                   iter, loss, acc, valid_loss, valid_acc, test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10, train loss-2.2883, acc-0.1000, valid loss-2.3073, acc-0.0882, test loss-2.3053, acc-0.0914\n",
      "Iter-20, train loss-2.3026, acc-0.1000, valid loss-2.3047, acc-0.0930, test loss-2.3028, acc-0.0957\n",
      "Iter-30, train loss-2.3111, acc-0.1200, valid loss-2.3019, acc-0.0972, test loss-2.3000, acc-0.1007\n",
      "Iter-40, train loss-2.2832, acc-0.1400, valid loss-2.2992, acc-0.1006, test loss-2.2972, acc-0.1059\n",
      "Iter-50, train loss-2.2902, acc-0.0600, valid loss-2.2963, acc-0.1060, test loss-2.2943, acc-0.1093\n",
      "Iter-60, train loss-2.3093, acc-0.0800, valid loss-2.2936, acc-0.1108, test loss-2.2916, acc-0.1149\n",
      "Iter-70, train loss-2.2783, acc-0.1600, valid loss-2.2910, acc-0.1172, test loss-2.2890, acc-0.1198\n",
      "Iter-80, train loss-2.2944, acc-0.0600, valid loss-2.2884, acc-0.1216, test loss-2.2864, acc-0.1251\n",
      "Iter-90, train loss-2.2814, acc-0.1000, valid loss-2.2857, acc-0.1254, test loss-2.2837, acc-0.1286\n",
      "Iter-100, train loss-2.2866, acc-0.1000, valid loss-2.2831, acc-0.1296, test loss-2.2811, acc-0.1343\n",
      "Iter-110, train loss-2.2929, acc-0.1600, valid loss-2.2804, acc-0.1332, test loss-2.2784, acc-0.1396\n",
      "Iter-120, train loss-2.2904, acc-0.0800, valid loss-2.2778, acc-0.1398, test loss-2.2757, acc-0.1437\n",
      "Iter-130, train loss-2.2798, acc-0.1600, valid loss-2.2751, acc-0.1440, test loss-2.2731, acc-0.1487\n",
      "Iter-140, train loss-2.2808, acc-0.0800, valid loss-2.2723, acc-0.1500, test loss-2.2703, acc-0.1531\n",
      "Iter-150, train loss-2.2684, acc-0.1600, valid loss-2.2696, acc-0.1562, test loss-2.2677, acc-0.1596\n",
      "Iter-160, train loss-2.2681, acc-0.2000, valid loss-2.2670, acc-0.1628, test loss-2.2650, acc-0.1654\n",
      "Iter-170, train loss-2.2653, acc-0.1400, valid loss-2.2642, acc-0.1672, test loss-2.2623, acc-0.1724\n",
      "Iter-180, train loss-2.2822, acc-0.2000, valid loss-2.2616, acc-0.1708, test loss-2.2597, acc-0.1794\n",
      "Iter-190, train loss-2.2712, acc-0.1800, valid loss-2.2589, acc-0.1762, test loss-2.2570, acc-0.1855\n",
      "Iter-200, train loss-2.2510, acc-0.2400, valid loss-2.2564, acc-0.1822, test loss-2.2545, acc-0.1915\n",
      "Iter-210, train loss-2.2355, acc-0.1600, valid loss-2.2537, acc-0.1904, test loss-2.2518, acc-0.1985\n",
      "Iter-220, train loss-2.2328, acc-0.1600, valid loss-2.2510, acc-0.1950, test loss-2.2492, acc-0.2048\n",
      "Iter-230, train loss-2.2408, acc-0.2200, valid loss-2.2484, acc-0.2016, test loss-2.2465, acc-0.2109\n",
      "Iter-240, train loss-2.2477, acc-0.2600, valid loss-2.2458, acc-0.2098, test loss-2.2440, acc-0.2171\n",
      "Iter-250, train loss-2.2179, acc-0.2800, valid loss-2.2433, acc-0.2150, test loss-2.2415, acc-0.2228\n",
      "Iter-260, train loss-2.2231, acc-0.3000, valid loss-2.2407, acc-0.2240, test loss-2.2388, acc-0.2278\n",
      "Iter-270, train loss-2.2325, acc-0.2400, valid loss-2.2379, acc-0.2336, test loss-2.2361, acc-0.2324\n",
      "Iter-280, train loss-2.2211, acc-0.2600, valid loss-2.2353, acc-0.2388, test loss-2.2334, acc-0.2406\n",
      "Iter-290, train loss-2.2588, acc-0.1800, valid loss-2.2326, acc-0.2438, test loss-2.2308, acc-0.2465\n",
      "Iter-300, train loss-2.2378, acc-0.2400, valid loss-2.2302, acc-0.2488, test loss-2.2284, acc-0.2531\n",
      "Iter-310, train loss-2.2528, acc-0.1600, valid loss-2.2277, acc-0.2552, test loss-2.2258, acc-0.2598\n",
      "Iter-320, train loss-2.2311, acc-0.2200, valid loss-2.2250, acc-0.2624, test loss-2.2232, acc-0.2663\n",
      "Iter-330, train loss-2.1892, acc-0.3600, valid loss-2.2223, acc-0.2700, test loss-2.2205, acc-0.2733\n",
      "Iter-340, train loss-2.1993, acc-0.3200, valid loss-2.2196, acc-0.2788, test loss-2.2178, acc-0.2797\n",
      "Iter-350, train loss-2.2175, acc-0.3000, valid loss-2.2170, acc-0.2856, test loss-2.2152, acc-0.2865\n",
      "Iter-360, train loss-2.1825, acc-0.3800, valid loss-2.2145, acc-0.2920, test loss-2.2127, acc-0.2922\n",
      "Iter-370, train loss-2.2345, acc-0.1600, valid loss-2.2119, acc-0.2984, test loss-2.2102, acc-0.2983\n",
      "Iter-380, train loss-2.2096, acc-0.4000, valid loss-2.2095, acc-0.3074, test loss-2.2077, acc-0.3049\n",
      "Iter-390, train loss-2.2123, acc-0.3400, valid loss-2.2068, acc-0.3132, test loss-2.2050, acc-0.3113\n",
      "Iter-400, train loss-2.2215, acc-0.2600, valid loss-2.2042, acc-0.3182, test loss-2.2024, acc-0.3168\n",
      "Iter-410, train loss-2.1983, acc-0.3000, valid loss-2.2017, acc-0.3250, test loss-2.2000, acc-0.3216\n",
      "Iter-420, train loss-2.2003, acc-0.4800, valid loss-2.1992, acc-0.3310, test loss-2.1975, acc-0.3280\n",
      "Iter-430, train loss-2.2112, acc-0.2600, valid loss-2.1966, acc-0.3332, test loss-2.1949, acc-0.3331\n",
      "Iter-440, train loss-2.1740, acc-0.3400, valid loss-2.1941, acc-0.3388, test loss-2.1923, acc-0.3396\n",
      "Iter-450, train loss-2.1756, acc-0.3600, valid loss-2.1915, acc-0.3458, test loss-2.1898, acc-0.3461\n",
      "Iter-460, train loss-2.1926, acc-0.3600, valid loss-2.1890, acc-0.3508, test loss-2.1873, acc-0.3525\n",
      "Iter-470, train loss-2.1726, acc-0.3800, valid loss-2.1863, acc-0.3592, test loss-2.1846, acc-0.3607\n",
      "Iter-480, train loss-2.2240, acc-0.3400, valid loss-2.1838, acc-0.3658, test loss-2.1821, acc-0.3678\n",
      "Iter-490, train loss-2.1809, acc-0.3000, valid loss-2.1813, acc-0.3726, test loss-2.1795, acc-0.3733\n",
      "Iter-500, train loss-2.1909, acc-0.3000, valid loss-2.1787, acc-0.3768, test loss-2.1769, acc-0.3780\n",
      "Iter-510, train loss-2.1684, acc-0.4000, valid loss-2.1761, acc-0.3850, test loss-2.1744, acc-0.3810\n",
      "Iter-520, train loss-2.1750, acc-0.3600, valid loss-2.1737, acc-0.3896, test loss-2.1719, acc-0.3857\n",
      "Iter-530, train loss-2.1692, acc-0.3600, valid loss-2.1712, acc-0.3944, test loss-2.1694, acc-0.3910\n",
      "Iter-540, train loss-2.1531, acc-0.4600, valid loss-2.1686, acc-0.4000, test loss-2.1668, acc-0.3964\n",
      "Iter-550, train loss-2.1452, acc-0.5000, valid loss-2.1661, acc-0.4040, test loss-2.1644, acc-0.4008\n",
      "Iter-560, train loss-2.1626, acc-0.5000, valid loss-2.1636, acc-0.4086, test loss-2.1618, acc-0.4052\n",
      "Iter-570, train loss-2.1614, acc-0.3600, valid loss-2.1610, acc-0.4128, test loss-2.1592, acc-0.4096\n",
      "Iter-580, train loss-2.1518, acc-0.4000, valid loss-2.1586, acc-0.4176, test loss-2.1568, acc-0.4150\n",
      "Iter-590, train loss-2.1505, acc-0.3400, valid loss-2.1561, acc-0.4210, test loss-2.1543, acc-0.4186\n",
      "Iter-600, train loss-2.1460, acc-0.4000, valid loss-2.1536, acc-0.4242, test loss-2.1518, acc-0.4226\n",
      "Iter-610, train loss-2.1229, acc-0.4000, valid loss-2.1510, acc-0.4274, test loss-2.1493, acc-0.4256\n",
      "Iter-620, train loss-2.1198, acc-0.5400, valid loss-2.1485, acc-0.4298, test loss-2.1468, acc-0.4299\n",
      "Iter-630, train loss-2.1351, acc-0.4000, valid loss-2.1459, acc-0.4348, test loss-2.1442, acc-0.4334\n",
      "Iter-640, train loss-2.1505, acc-0.5400, valid loss-2.1433, acc-0.4396, test loss-2.1416, acc-0.4371\n",
      "Iter-650, train loss-2.1257, acc-0.4400, valid loss-2.1408, acc-0.4438, test loss-2.1391, acc-0.4418\n",
      "Iter-660, train loss-2.1159, acc-0.5200, valid loss-2.1382, acc-0.4450, test loss-2.1365, acc-0.4451\n",
      "Iter-670, train loss-2.1318, acc-0.4400, valid loss-2.1357, acc-0.4478, test loss-2.1340, acc-0.4482\n",
      "Iter-680, train loss-2.1080, acc-0.5400, valid loss-2.1331, acc-0.4494, test loss-2.1314, acc-0.4519\n",
      "Iter-690, train loss-2.1504, acc-0.2800, valid loss-2.1305, acc-0.4514, test loss-2.1289, acc-0.4546\n",
      "Iter-700, train loss-2.1001, acc-0.5600, valid loss-2.1279, acc-0.4554, test loss-2.1262, acc-0.4563\n",
      "Iter-710, train loss-2.1520, acc-0.3800, valid loss-2.1253, acc-0.4588, test loss-2.1237, acc-0.4596\n",
      "Iter-720, train loss-2.1142, acc-0.4400, valid loss-2.1228, acc-0.4628, test loss-2.1212, acc-0.4637\n",
      "Iter-730, train loss-2.1247, acc-0.4200, valid loss-2.1203, acc-0.4676, test loss-2.1186, acc-0.4669\n",
      "Iter-740, train loss-2.1222, acc-0.5000, valid loss-2.1177, acc-0.4700, test loss-2.1161, acc-0.4708\n",
      "Iter-750, train loss-2.0829, acc-0.6000, valid loss-2.1151, acc-0.4702, test loss-2.1135, acc-0.4720\n",
      "Iter-760, train loss-2.1119, acc-0.4200, valid loss-2.1126, acc-0.4738, test loss-2.1109, acc-0.4755\n",
      "Iter-770, train loss-2.1280, acc-0.4600, valid loss-2.1100, acc-0.4752, test loss-2.1083, acc-0.4777\n",
      "Iter-780, train loss-2.1074, acc-0.5200, valid loss-2.1076, acc-0.4796, test loss-2.1059, acc-0.4810\n",
      "Iter-790, train loss-2.0675, acc-0.5800, valid loss-2.1050, acc-0.4830, test loss-2.1033, acc-0.4836\n",
      "Iter-800, train loss-2.1214, acc-0.4200, valid loss-2.1025, acc-0.4838, test loss-2.1008, acc-0.4861\n",
      "Iter-810, train loss-2.0388, acc-0.6400, valid loss-2.0999, acc-0.4866, test loss-2.0982, acc-0.4880\n",
      "Iter-820, train loss-2.0498, acc-0.6400, valid loss-2.0973, acc-0.4884, test loss-2.0956, acc-0.4906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-830, train loss-2.1142, acc-0.4600, valid loss-2.0949, acc-0.4920, test loss-2.0932, acc-0.4930\n",
      "Iter-840, train loss-2.1350, acc-0.4000, valid loss-2.0922, acc-0.4940, test loss-2.0905, acc-0.4947\n",
      "Iter-850, train loss-2.0984, acc-0.4200, valid loss-2.0898, acc-0.4984, test loss-2.0881, acc-0.4972\n",
      "Iter-860, train loss-2.0378, acc-0.5600, valid loss-2.0872, acc-0.5006, test loss-2.0855, acc-0.4989\n",
      "Iter-870, train loss-2.0727, acc-0.5000, valid loss-2.0845, acc-0.5026, test loss-2.0828, acc-0.5011\n",
      "Iter-880, train loss-2.1090, acc-0.4600, valid loss-2.0820, acc-0.5032, test loss-2.0803, acc-0.5022\n",
      "Iter-890, train loss-2.0541, acc-0.6000, valid loss-2.0793, acc-0.5066, test loss-2.0777, acc-0.5045\n",
      "Iter-900, train loss-2.0880, acc-0.5200, valid loss-2.0767, acc-0.5098, test loss-2.0751, acc-0.5066\n",
      "Iter-910, train loss-2.0480, acc-0.5800, valid loss-2.0741, acc-0.5120, test loss-2.0725, acc-0.5084\n",
      "Iter-920, train loss-2.0452, acc-0.5800, valid loss-2.0715, acc-0.5130, test loss-2.0699, acc-0.5092\n",
      "Iter-930, train loss-2.0761, acc-0.4400, valid loss-2.0689, acc-0.5136, test loss-2.0673, acc-0.5108\n",
      "Iter-940, train loss-2.0581, acc-0.5400, valid loss-2.0663, acc-0.5160, test loss-2.0647, acc-0.5117\n",
      "Iter-950, train loss-2.0558, acc-0.6600, valid loss-2.0636, acc-0.5166, test loss-2.0620, acc-0.5123\n",
      "Iter-960, train loss-2.1107, acc-0.5200, valid loss-2.0610, acc-0.5182, test loss-2.0594, acc-0.5151\n",
      "Iter-970, train loss-2.0373, acc-0.4400, valid loss-2.0583, acc-0.5198, test loss-2.0567, acc-0.5169\n",
      "Iter-980, train loss-2.0399, acc-0.5800, valid loss-2.0558, acc-0.5214, test loss-2.0542, acc-0.5196\n",
      "Iter-990, train loss-2.0651, acc-0.5600, valid loss-2.0532, acc-0.5222, test loss-2.0517, acc-0.5213\n",
      "Iter-1000, train loss-2.0396, acc-0.6200, valid loss-2.0509, acc-0.5240, test loss-2.0493, acc-0.5233\n",
      "Iter-1010, train loss-2.0408, acc-0.6200, valid loss-2.0483, acc-0.5254, test loss-2.0468, acc-0.5251\n",
      "Iter-1020, train loss-2.0516, acc-0.5400, valid loss-2.0457, acc-0.5264, test loss-2.0442, acc-0.5267\n",
      "Iter-1030, train loss-2.0382, acc-0.5000, valid loss-2.0431, acc-0.5296, test loss-2.0416, acc-0.5274\n",
      "Iter-1040, train loss-1.9933, acc-0.6400, valid loss-2.0407, acc-0.5306, test loss-2.0392, acc-0.5284\n",
      "Iter-1050, train loss-2.0436, acc-0.5600, valid loss-2.0381, acc-0.5318, test loss-2.0366, acc-0.5294\n",
      "Iter-1060, train loss-2.0348, acc-0.5800, valid loss-2.0357, acc-0.5336, test loss-2.0341, acc-0.5306\n",
      "Iter-1070, train loss-2.0516, acc-0.5000, valid loss-2.0332, acc-0.5342, test loss-2.0317, acc-0.5317\n",
      "Iter-1080, train loss-2.0555, acc-0.4000, valid loss-2.0306, acc-0.5348, test loss-2.0291, acc-0.5338\n",
      "Iter-1090, train loss-1.9782, acc-0.6600, valid loss-2.0280, acc-0.5358, test loss-2.0265, acc-0.5346\n",
      "Iter-1100, train loss-1.9609, acc-0.5800, valid loss-2.0254, acc-0.5370, test loss-2.0239, acc-0.5356\n",
      "Iter-1110, train loss-2.0277, acc-0.5600, valid loss-2.0228, acc-0.5376, test loss-2.0213, acc-0.5373\n",
      "Iter-1120, train loss-2.0273, acc-0.5400, valid loss-2.0202, acc-0.5386, test loss-2.0187, acc-0.5377\n",
      "Iter-1130, train loss-2.0700, acc-0.4000, valid loss-2.0177, acc-0.5396, test loss-2.0162, acc-0.5381\n",
      "Iter-1140, train loss-2.0543, acc-0.4800, valid loss-2.0152, acc-0.5410, test loss-2.0137, acc-0.5395\n",
      "Iter-1150, train loss-2.0305, acc-0.5200, valid loss-2.0126, acc-0.5424, test loss-2.0111, acc-0.5411\n",
      "Iter-1160, train loss-2.0275, acc-0.4400, valid loss-2.0101, acc-0.5438, test loss-2.0085, acc-0.5429\n",
      "Iter-1170, train loss-1.9858, acc-0.5600, valid loss-2.0076, acc-0.5452, test loss-2.0060, acc-0.5447\n",
      "Iter-1180, train loss-2.0119, acc-0.5200, valid loss-2.0050, acc-0.5472, test loss-2.0035, acc-0.5459\n",
      "Iter-1190, train loss-2.0163, acc-0.5200, valid loss-2.0025, acc-0.5486, test loss-2.0009, acc-0.5478\n",
      "Iter-1200, train loss-2.0116, acc-0.4200, valid loss-1.9999, acc-0.5502, test loss-1.9983, acc-0.5490\n",
      "Iter-1210, train loss-2.0458, acc-0.4400, valid loss-1.9973, acc-0.5516, test loss-1.9957, acc-0.5508\n",
      "Iter-1220, train loss-2.0243, acc-0.4400, valid loss-1.9946, acc-0.5514, test loss-1.9930, acc-0.5524\n",
      "Iter-1230, train loss-1.9962, acc-0.4800, valid loss-1.9920, acc-0.5544, test loss-1.9904, acc-0.5537\n",
      "Iter-1240, train loss-1.9542, acc-0.5400, valid loss-1.9893, acc-0.5562, test loss-1.9877, acc-0.5545\n",
      "Iter-1250, train loss-1.9738, acc-0.4800, valid loss-1.9866, acc-0.5552, test loss-1.9850, acc-0.5547\n",
      "Iter-1260, train loss-2.0420, acc-0.5000, valid loss-1.9840, acc-0.5556, test loss-1.9824, acc-0.5552\n",
      "Iter-1270, train loss-1.9137, acc-0.6000, valid loss-1.9813, acc-0.5568, test loss-1.9797, acc-0.5567\n",
      "Iter-1280, train loss-1.9993, acc-0.5400, valid loss-1.9788, acc-0.5574, test loss-1.9772, acc-0.5581\n",
      "Iter-1290, train loss-1.8948, acc-0.6400, valid loss-1.9760, acc-0.5558, test loss-1.9745, acc-0.5584\n",
      "Iter-1300, train loss-1.9911, acc-0.6400, valid loss-1.9734, acc-0.5564, test loss-1.9719, acc-0.5589\n",
      "Iter-1310, train loss-1.9859, acc-0.5200, valid loss-1.9708, acc-0.5576, test loss-1.9693, acc-0.5590\n",
      "Iter-1320, train loss-1.9877, acc-0.5000, valid loss-1.9682, acc-0.5564, test loss-1.9666, acc-0.5593\n",
      "Iter-1330, train loss-1.9937, acc-0.5200, valid loss-1.9656, acc-0.5582, test loss-1.9640, acc-0.5599\n",
      "Iter-1340, train loss-1.9071, acc-0.6800, valid loss-1.9627, acc-0.5552, test loss-1.9611, acc-0.5596\n",
      "Iter-1350, train loss-1.9459, acc-0.5400, valid loss-1.9601, acc-0.5568, test loss-1.9585, acc-0.5598\n",
      "Iter-1360, train loss-2.0288, acc-0.5600, valid loss-1.9575, acc-0.5584, test loss-1.9559, acc-0.5606\n",
      "Iter-1370, train loss-1.9358, acc-0.6000, valid loss-1.9549, acc-0.5582, test loss-1.9532, acc-0.5618\n",
      "Iter-1380, train loss-1.9859, acc-0.5400, valid loss-1.9523, acc-0.5600, test loss-1.9507, acc-0.5639\n",
      "Iter-1390, train loss-1.9431, acc-0.7000, valid loss-1.9497, acc-0.5602, test loss-1.9481, acc-0.5630\n",
      "Iter-1400, train loss-1.9094, acc-0.6200, valid loss-1.9471, acc-0.5604, test loss-1.9455, acc-0.5636\n",
      "Iter-1410, train loss-1.9458, acc-0.5400, valid loss-1.9447, acc-0.5632, test loss-1.9430, acc-0.5650\n",
      "Iter-1420, train loss-1.9632, acc-0.5400, valid loss-1.9420, acc-0.5642, test loss-1.9403, acc-0.5661\n",
      "Iter-1430, train loss-1.9283, acc-0.6600, valid loss-1.9393, acc-0.5650, test loss-1.9376, acc-0.5666\n",
      "Iter-1440, train loss-1.9608, acc-0.5600, valid loss-1.9367, acc-0.5656, test loss-1.9349, acc-0.5676\n",
      "Iter-1450, train loss-1.9342, acc-0.6200, valid loss-1.9340, acc-0.5654, test loss-1.9323, acc-0.5672\n",
      "Iter-1460, train loss-1.9421, acc-0.6600, valid loss-1.9314, acc-0.5660, test loss-1.9297, acc-0.5675\n",
      "Iter-1470, train loss-1.9241, acc-0.6200, valid loss-1.9287, acc-0.5654, test loss-1.9270, acc-0.5679\n",
      "Iter-1480, train loss-1.8852, acc-0.6600, valid loss-1.9262, acc-0.5666, test loss-1.9244, acc-0.5686\n",
      "Iter-1490, train loss-1.8813, acc-0.7200, valid loss-1.9236, acc-0.5682, test loss-1.9219, acc-0.5698\n",
      "Iter-1500, train loss-1.9275, acc-0.5600, valid loss-1.9211, acc-0.5692, test loss-1.9193, acc-0.5703\n",
      "Iter-1510, train loss-1.8800, acc-0.6400, valid loss-1.9185, acc-0.5702, test loss-1.9168, acc-0.5708\n",
      "Iter-1520, train loss-1.9795, acc-0.5000, valid loss-1.9158, acc-0.5710, test loss-1.9141, acc-0.5710\n",
      "Iter-1530, train loss-1.8959, acc-0.5400, valid loss-1.9130, acc-0.5694, test loss-1.9113, acc-0.5709\n",
      "Iter-1540, train loss-1.9115, acc-0.5600, valid loss-1.9105, acc-0.5708, test loss-1.9087, acc-0.5707\n",
      "Iter-1550, train loss-1.8808, acc-0.5600, valid loss-1.9078, acc-0.5706, test loss-1.9061, acc-0.5705\n",
      "Iter-1560, train loss-1.8855, acc-0.5000, valid loss-1.9052, acc-0.5718, test loss-1.9035, acc-0.5703\n",
      "Iter-1570, train loss-1.9093, acc-0.6000, valid loss-1.9025, acc-0.5720, test loss-1.9008, acc-0.5712\n",
      "Iter-1580, train loss-1.9413, acc-0.4400, valid loss-1.9000, acc-0.5716, test loss-1.8983, acc-0.5719\n",
      "Iter-1590, train loss-1.8569, acc-0.6600, valid loss-1.8974, acc-0.5724, test loss-1.8957, acc-0.5726\n",
      "Iter-1600, train loss-1.9085, acc-0.5200, valid loss-1.8947, acc-0.5718, test loss-1.8930, acc-0.5719\n",
      "Iter-1610, train loss-1.8936, acc-0.4400, valid loss-1.8921, acc-0.5716, test loss-1.8904, acc-0.5719\n",
      "Iter-1620, train loss-1.9594, acc-0.5400, valid loss-1.8894, acc-0.5718, test loss-1.8877, acc-0.5722\n",
      "Iter-1630, train loss-1.8817, acc-0.6600, valid loss-1.8867, acc-0.5716, test loss-1.8851, acc-0.5717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1640, train loss-1.9045, acc-0.4800, valid loss-1.8842, acc-0.5724, test loss-1.8825, acc-0.5723\n",
      "Iter-1650, train loss-1.8929, acc-0.5400, valid loss-1.8816, acc-0.5726, test loss-1.8800, acc-0.5732\n",
      "Iter-1660, train loss-1.9554, acc-0.5600, valid loss-1.8790, acc-0.5724, test loss-1.8774, acc-0.5729\n",
      "Iter-1670, train loss-1.8729, acc-0.6000, valid loss-1.8763, acc-0.5722, test loss-1.8747, acc-0.5737\n",
      "Iter-1680, train loss-1.9087, acc-0.5600, valid loss-1.8737, acc-0.5728, test loss-1.8720, acc-0.5746\n",
      "Iter-1690, train loss-1.8958, acc-0.5400, valid loss-1.8712, acc-0.5742, test loss-1.8696, acc-0.5767\n",
      "Iter-1700, train loss-1.9038, acc-0.5400, valid loss-1.8685, acc-0.5736, test loss-1.8669, acc-0.5773\n",
      "Iter-1710, train loss-1.7941, acc-0.6600, valid loss-1.8659, acc-0.5746, test loss-1.8642, acc-0.5778\n",
      "Iter-1720, train loss-1.8725, acc-0.5400, valid loss-1.8632, acc-0.5754, test loss-1.8616, acc-0.5775\n",
      "Iter-1730, train loss-1.8550, acc-0.5600, valid loss-1.8606, acc-0.5764, test loss-1.8589, acc-0.5787\n",
      "Iter-1740, train loss-1.8604, acc-0.5000, valid loss-1.8578, acc-0.5762, test loss-1.8562, acc-0.5784\n",
      "Iter-1750, train loss-1.8116, acc-0.6400, valid loss-1.8553, acc-0.5774, test loss-1.8536, acc-0.5795\n",
      "Iter-1760, train loss-1.8324, acc-0.5600, valid loss-1.8526, acc-0.5766, test loss-1.8510, acc-0.5796\n",
      "Iter-1770, train loss-1.8096, acc-0.5400, valid loss-1.8499, acc-0.5778, test loss-1.8483, acc-0.5804\n",
      "Iter-1780, train loss-1.8615, acc-0.5600, valid loss-1.8473, acc-0.5788, test loss-1.8456, acc-0.5808\n",
      "Iter-1790, train loss-1.8355, acc-0.6200, valid loss-1.8446, acc-0.5790, test loss-1.8430, acc-0.5806\n",
      "Iter-1800, train loss-1.8444, acc-0.4400, valid loss-1.8420, acc-0.5798, test loss-1.8403, acc-0.5807\n",
      "Iter-1810, train loss-1.7733, acc-0.7600, valid loss-1.8392, acc-0.5816, test loss-1.8376, acc-0.5817\n",
      "Iter-1820, train loss-1.8809, acc-0.6200, valid loss-1.8366, acc-0.5822, test loss-1.8349, acc-0.5811\n",
      "Iter-1830, train loss-1.8089, acc-0.5600, valid loss-1.8340, acc-0.5828, test loss-1.8323, acc-0.5819\n",
      "Iter-1840, train loss-1.9209, acc-0.5200, valid loss-1.8315, acc-0.5836, test loss-1.8299, acc-0.5818\n",
      "Iter-1850, train loss-1.8416, acc-0.6200, valid loss-1.8290, acc-0.5836, test loss-1.8273, acc-0.5831\n",
      "Iter-1860, train loss-1.8536, acc-0.4000, valid loss-1.8264, acc-0.5842, test loss-1.8248, acc-0.5839\n",
      "Iter-1870, train loss-1.8541, acc-0.6000, valid loss-1.8238, acc-0.5858, test loss-1.8222, acc-0.5849\n",
      "Iter-1880, train loss-1.8344, acc-0.4800, valid loss-1.8211, acc-0.5864, test loss-1.8195, acc-0.5855\n",
      "Iter-1890, train loss-1.8246, acc-0.5800, valid loss-1.8186, acc-0.5876, test loss-1.8170, acc-0.5867\n",
      "Iter-1900, train loss-1.7709, acc-0.6800, valid loss-1.8160, acc-0.5884, test loss-1.8144, acc-0.5876\n",
      "Iter-1910, train loss-1.8046, acc-0.5800, valid loss-1.8135, acc-0.5886, test loss-1.8119, acc-0.5879\n",
      "Iter-1920, train loss-1.7767, acc-0.6400, valid loss-1.8110, acc-0.5898, test loss-1.8094, acc-0.5886\n",
      "Iter-1930, train loss-1.8221, acc-0.5200, valid loss-1.8084, acc-0.5896, test loss-1.8067, acc-0.5888\n",
      "Iter-1940, train loss-1.7824, acc-0.5600, valid loss-1.8058, acc-0.5894, test loss-1.8042, acc-0.5892\n",
      "Iter-1950, train loss-1.8005, acc-0.5800, valid loss-1.8031, acc-0.5902, test loss-1.8015, acc-0.5897\n",
      "Iter-1960, train loss-1.8258, acc-0.5000, valid loss-1.8005, acc-0.5912, test loss-1.7989, acc-0.5911\n",
      "Iter-1970, train loss-1.8574, acc-0.5400, valid loss-1.7978, acc-0.5914, test loss-1.7962, acc-0.5913\n",
      "Iter-1980, train loss-1.7686, acc-0.5600, valid loss-1.7951, acc-0.5924, test loss-1.7935, acc-0.5915\n",
      "Iter-1990, train loss-1.7812, acc-0.6200, valid loss-1.7925, acc-0.5930, test loss-1.7909, acc-0.5916\n",
      "Iter-2000, train loss-1.8296, acc-0.5400, valid loss-1.7899, acc-0.5936, test loss-1.7883, acc-0.5924\n",
      "Iter-2010, train loss-1.8142, acc-0.5000, valid loss-1.7873, acc-0.5944, test loss-1.7857, acc-0.5928\n",
      "Iter-2020, train loss-1.8085, acc-0.6200, valid loss-1.7847, acc-0.5944, test loss-1.7832, acc-0.5928\n",
      "Iter-2030, train loss-1.8296, acc-0.6400, valid loss-1.7822, acc-0.5950, test loss-1.7807, acc-0.5936\n",
      "Iter-2040, train loss-1.8246, acc-0.4400, valid loss-1.7797, acc-0.5952, test loss-1.7782, acc-0.5951\n",
      "Iter-2050, train loss-1.6762, acc-0.6600, valid loss-1.7772, acc-0.5964, test loss-1.7757, acc-0.5953\n",
      "Iter-2060, train loss-1.7382, acc-0.6600, valid loss-1.7747, acc-0.5962, test loss-1.7732, acc-0.5952\n",
      "Iter-2070, train loss-1.7796, acc-0.5600, valid loss-1.7721, acc-0.5962, test loss-1.7706, acc-0.5959\n",
      "Iter-2080, train loss-1.7513, acc-0.5600, valid loss-1.7695, acc-0.5966, test loss-1.7679, acc-0.5973\n",
      "Iter-2090, train loss-1.7925, acc-0.6200, valid loss-1.7668, acc-0.5966, test loss-1.7653, acc-0.5980\n",
      "Iter-2100, train loss-1.8265, acc-0.5600, valid loss-1.7643, acc-0.5970, test loss-1.7628, acc-0.5986\n",
      "Iter-2110, train loss-1.7749, acc-0.6200, valid loss-1.7617, acc-0.5974, test loss-1.7603, acc-0.5996\n",
      "Iter-2120, train loss-1.7077, acc-0.6800, valid loss-1.7591, acc-0.5976, test loss-1.7576, acc-0.6002\n",
      "Iter-2130, train loss-1.7569, acc-0.6800, valid loss-1.7565, acc-0.5984, test loss-1.7550, acc-0.5997\n",
      "Iter-2140, train loss-1.8253, acc-0.5200, valid loss-1.7540, acc-0.5986, test loss-1.7525, acc-0.6007\n",
      "Iter-2150, train loss-1.8067, acc-0.6000, valid loss-1.7515, acc-0.5994, test loss-1.7500, acc-0.6017\n",
      "Iter-2160, train loss-1.7251, acc-0.6000, valid loss-1.7489, acc-0.5998, test loss-1.7474, acc-0.6030\n",
      "Iter-2170, train loss-1.7670, acc-0.6200, valid loss-1.7463, acc-0.6002, test loss-1.7449, acc-0.6038\n",
      "Iter-2180, train loss-1.7250, acc-0.5800, valid loss-1.7437, acc-0.6024, test loss-1.7422, acc-0.6053\n",
      "Iter-2190, train loss-1.6858, acc-0.6800, valid loss-1.7411, acc-0.6020, test loss-1.7396, acc-0.6060\n",
      "Iter-2200, train loss-1.7680, acc-0.5400, valid loss-1.7385, acc-0.6034, test loss-1.7371, acc-0.6064\n",
      "Iter-2210, train loss-1.7611, acc-0.5200, valid loss-1.7359, acc-0.6038, test loss-1.7345, acc-0.6072\n",
      "Iter-2220, train loss-1.6635, acc-0.6600, valid loss-1.7334, acc-0.6042, test loss-1.7320, acc-0.6082\n",
      "Iter-2230, train loss-1.7046, acc-0.6600, valid loss-1.7308, acc-0.6040, test loss-1.7294, acc-0.6085\n",
      "Iter-2240, train loss-1.7615, acc-0.5000, valid loss-1.7282, acc-0.6028, test loss-1.7268, acc-0.6086\n",
      "Iter-2250, train loss-1.6856, acc-0.5600, valid loss-1.7257, acc-0.6042, test loss-1.7243, acc-0.6085\n",
      "Iter-2260, train loss-1.7829, acc-0.6600, valid loss-1.7230, acc-0.6050, test loss-1.7216, acc-0.6089\n",
      "Iter-2270, train loss-1.7828, acc-0.5800, valid loss-1.7204, acc-0.6058, test loss-1.7190, acc-0.6097\n",
      "Iter-2280, train loss-1.6791, acc-0.6600, valid loss-1.7179, acc-0.6076, test loss-1.7164, acc-0.6109\n",
      "Iter-2290, train loss-1.7434, acc-0.6200, valid loss-1.7154, acc-0.6082, test loss-1.7140, acc-0.6110\n",
      "Iter-2300, train loss-1.6546, acc-0.6000, valid loss-1.7128, acc-0.6104, test loss-1.7114, acc-0.6123\n",
      "Iter-2310, train loss-1.7356, acc-0.5600, valid loss-1.7104, acc-0.6104, test loss-1.7089, acc-0.6125\n",
      "Iter-2320, train loss-1.6780, acc-0.6200, valid loss-1.7078, acc-0.6118, test loss-1.7064, acc-0.6128\n",
      "Iter-2330, train loss-1.6465, acc-0.6600, valid loss-1.7052, acc-0.6114, test loss-1.7037, acc-0.6130\n",
      "Iter-2340, train loss-1.7885, acc-0.5200, valid loss-1.7025, acc-0.6116, test loss-1.7011, acc-0.6143\n",
      "Iter-2350, train loss-1.7805, acc-0.5200, valid loss-1.7000, acc-0.6128, test loss-1.6986, acc-0.6151\n",
      "Iter-2360, train loss-1.6020, acc-0.8000, valid loss-1.6974, acc-0.6126, test loss-1.6960, acc-0.6152\n",
      "Iter-2370, train loss-1.7204, acc-0.7800, valid loss-1.6949, acc-0.6124, test loss-1.6935, acc-0.6153\n",
      "Iter-2380, train loss-1.6883, acc-0.6200, valid loss-1.6923, acc-0.6130, test loss-1.6909, acc-0.6156\n",
      "Iter-2390, train loss-1.6330, acc-0.6600, valid loss-1.6898, acc-0.6138, test loss-1.6884, acc-0.6155\n",
      "Iter-2400, train loss-1.6984, acc-0.5600, valid loss-1.6872, acc-0.6144, test loss-1.6859, acc-0.6158\n",
      "Iter-2410, train loss-1.6495, acc-0.5400, valid loss-1.6847, acc-0.6158, test loss-1.6834, acc-0.6169\n",
      "Iter-2420, train loss-1.6745, acc-0.6800, valid loss-1.6822, acc-0.6162, test loss-1.6808, acc-0.6168\n",
      "Iter-2430, train loss-1.6486, acc-0.6800, valid loss-1.6795, acc-0.6172, test loss-1.6781, acc-0.6167\n",
      "Iter-2440, train loss-1.7568, acc-0.5200, valid loss-1.6771, acc-0.6182, test loss-1.6757, acc-0.6169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2450, train loss-1.6667, acc-0.6800, valid loss-1.6745, acc-0.6182, test loss-1.6731, acc-0.6176\n",
      "Iter-2460, train loss-1.7160, acc-0.5800, valid loss-1.6719, acc-0.6186, test loss-1.6706, acc-0.6176\n",
      "Iter-2470, train loss-1.6749, acc-0.6600, valid loss-1.6694, acc-0.6192, test loss-1.6681, acc-0.6182\n",
      "Iter-2480, train loss-1.7293, acc-0.5800, valid loss-1.6668, acc-0.6198, test loss-1.6655, acc-0.6192\n",
      "Iter-2490, train loss-1.6249, acc-0.6400, valid loss-1.6643, acc-0.6204, test loss-1.6630, acc-0.6189\n",
      "Iter-2500, train loss-1.7588, acc-0.5400, valid loss-1.6618, acc-0.6214, test loss-1.6605, acc-0.6191\n",
      "Iter-2510, train loss-1.6540, acc-0.6600, valid loss-1.6592, acc-0.6216, test loss-1.6579, acc-0.6195\n",
      "Iter-2520, train loss-1.6222, acc-0.7200, valid loss-1.6567, acc-0.6222, test loss-1.6554, acc-0.6191\n",
      "Iter-2530, train loss-1.6437, acc-0.6400, valid loss-1.6543, acc-0.6226, test loss-1.6530, acc-0.6193\n",
      "Iter-2540, train loss-1.6892, acc-0.5400, valid loss-1.6518, acc-0.6224, test loss-1.6506, acc-0.6197\n",
      "Iter-2550, train loss-1.6844, acc-0.6000, valid loss-1.6493, acc-0.6228, test loss-1.6480, acc-0.6195\n",
      "Iter-2560, train loss-1.6139, acc-0.6600, valid loss-1.6466, acc-0.6232, test loss-1.6454, acc-0.6198\n",
      "Iter-2570, train loss-1.6797, acc-0.6000, valid loss-1.6441, acc-0.6242, test loss-1.6429, acc-0.6204\n",
      "Iter-2580, train loss-1.6780, acc-0.5800, valid loss-1.6417, acc-0.6266, test loss-1.6405, acc-0.6204\n",
      "Iter-2590, train loss-1.7270, acc-0.5200, valid loss-1.6393, acc-0.6274, test loss-1.6381, acc-0.6215\n",
      "Iter-2600, train loss-1.6791, acc-0.5200, valid loss-1.6369, acc-0.6282, test loss-1.6357, acc-0.6226\n",
      "Iter-2610, train loss-1.6529, acc-0.5400, valid loss-1.6344, acc-0.6284, test loss-1.6333, acc-0.6237\n",
      "Iter-2620, train loss-1.7247, acc-0.5200, valid loss-1.6321, acc-0.6312, test loss-1.6309, acc-0.6249\n",
      "Iter-2630, train loss-1.6599, acc-0.6400, valid loss-1.6295, acc-0.6298, test loss-1.6284, acc-0.6248\n",
      "Iter-2640, train loss-1.6262, acc-0.6800, valid loss-1.6270, acc-0.6304, test loss-1.6260, acc-0.6255\n",
      "Iter-2650, train loss-1.6181, acc-0.6800, valid loss-1.6246, acc-0.6322, test loss-1.6235, acc-0.6267\n",
      "Iter-2660, train loss-1.4983, acc-0.7800, valid loss-1.6219, acc-0.6336, test loss-1.6210, acc-0.6268\n",
      "Iter-2670, train loss-1.6429, acc-0.6400, valid loss-1.6195, acc-0.6366, test loss-1.6185, acc-0.6287\n",
      "Iter-2680, train loss-1.6345, acc-0.6200, valid loss-1.6170, acc-0.6378, test loss-1.6161, acc-0.6289\n",
      "Iter-2690, train loss-1.6543, acc-0.6200, valid loss-1.6146, acc-0.6390, test loss-1.6136, acc-0.6297\n",
      "Iter-2700, train loss-1.6000, acc-0.7000, valid loss-1.6121, acc-0.6392, test loss-1.6111, acc-0.6300\n",
      "Iter-2710, train loss-1.6199, acc-0.6200, valid loss-1.6096, acc-0.6394, test loss-1.6087, acc-0.6305\n",
      "Iter-2720, train loss-1.5788, acc-0.6800, valid loss-1.6072, acc-0.6398, test loss-1.6063, acc-0.6301\n",
      "Iter-2730, train loss-1.6879, acc-0.5600, valid loss-1.6048, acc-0.6420, test loss-1.6039, acc-0.6319\n",
      "Iter-2740, train loss-1.5798, acc-0.6200, valid loss-1.6023, acc-0.6438, test loss-1.6014, acc-0.6329\n",
      "Iter-2750, train loss-1.6334, acc-0.6600, valid loss-1.5999, acc-0.6452, test loss-1.5990, acc-0.6336\n",
      "Iter-2760, train loss-1.5239, acc-0.7400, valid loss-1.5974, acc-0.6462, test loss-1.5965, acc-0.6345\n",
      "Iter-2770, train loss-1.5355, acc-0.6800, valid loss-1.5950, acc-0.6482, test loss-1.5941, acc-0.6362\n",
      "Iter-2780, train loss-1.5837, acc-0.6400, valid loss-1.5926, acc-0.6484, test loss-1.5917, acc-0.6370\n",
      "Iter-2790, train loss-1.6109, acc-0.6000, valid loss-1.5902, acc-0.6500, test loss-1.5893, acc-0.6382\n",
      "Iter-2800, train loss-1.6437, acc-0.5400, valid loss-1.5878, acc-0.6506, test loss-1.5869, acc-0.6390\n",
      "Iter-2810, train loss-1.6353, acc-0.6200, valid loss-1.5854, acc-0.6500, test loss-1.5846, acc-0.6390\n",
      "Iter-2820, train loss-1.5713, acc-0.6400, valid loss-1.5829, acc-0.6506, test loss-1.5821, acc-0.6394\n",
      "Iter-2830, train loss-1.6854, acc-0.6000, valid loss-1.5805, acc-0.6524, test loss-1.5797, acc-0.6401\n",
      "Iter-2840, train loss-1.6341, acc-0.5600, valid loss-1.5781, acc-0.6530, test loss-1.5774, acc-0.6405\n",
      "Iter-2850, train loss-1.6577, acc-0.6000, valid loss-1.5757, acc-0.6542, test loss-1.5751, acc-0.6415\n",
      "Iter-2860, train loss-1.5754, acc-0.5600, valid loss-1.5733, acc-0.6556, test loss-1.5727, acc-0.6420\n",
      "Iter-2870, train loss-1.6636, acc-0.6400, valid loss-1.5710, acc-0.6558, test loss-1.5704, acc-0.6423\n",
      "Iter-2880, train loss-1.5808, acc-0.6200, valid loss-1.5686, acc-0.6564, test loss-1.5680, acc-0.6432\n",
      "Iter-2890, train loss-1.5764, acc-0.6600, valid loss-1.5662, acc-0.6582, test loss-1.5656, acc-0.6439\n",
      "Iter-2900, train loss-1.6605, acc-0.5800, valid loss-1.5637, acc-0.6592, test loss-1.5631, acc-0.6449\n",
      "Iter-2910, train loss-1.5737, acc-0.6800, valid loss-1.5613, acc-0.6600, test loss-1.5608, acc-0.6451\n",
      "Iter-2920, train loss-1.5621, acc-0.6200, valid loss-1.5589, acc-0.6602, test loss-1.5583, acc-0.6453\n",
      "Iter-2930, train loss-1.4962, acc-0.6600, valid loss-1.5565, acc-0.6606, test loss-1.5560, acc-0.6455\n",
      "Iter-2940, train loss-1.5190, acc-0.7000, valid loss-1.5542, acc-0.6618, test loss-1.5537, acc-0.6466\n",
      "Iter-2950, train loss-1.6890, acc-0.5600, valid loss-1.5519, acc-0.6620, test loss-1.5514, acc-0.6465\n",
      "Iter-2960, train loss-1.5632, acc-0.7200, valid loss-1.5495, acc-0.6630, test loss-1.5490, acc-0.6479\n",
      "Iter-2970, train loss-1.4992, acc-0.7000, valid loss-1.5470, acc-0.6634, test loss-1.5466, acc-0.6476\n",
      "Iter-2980, train loss-1.5650, acc-0.7000, valid loss-1.5446, acc-0.6644, test loss-1.5443, acc-0.6483\n",
      "Iter-2990, train loss-1.5969, acc-0.7000, valid loss-1.5423, acc-0.6644, test loss-1.5420, acc-0.6484\n",
      "Iter-3000, train loss-1.5739, acc-0.6000, valid loss-1.5399, acc-0.6642, test loss-1.5397, acc-0.6486\n",
      "Iter-3010, train loss-1.5728, acc-0.6600, valid loss-1.5377, acc-0.6650, test loss-1.5374, acc-0.6490\n",
      "Iter-3020, train loss-1.6192, acc-0.6000, valid loss-1.5353, acc-0.6666, test loss-1.5351, acc-0.6507\n",
      "Iter-3030, train loss-1.4518, acc-0.7600, valid loss-1.5328, acc-0.6668, test loss-1.5327, acc-0.6508\n",
      "Iter-3040, train loss-1.5380, acc-0.5000, valid loss-1.5305, acc-0.6674, test loss-1.5304, acc-0.6520\n",
      "Iter-3050, train loss-1.5234, acc-0.6800, valid loss-1.5282, acc-0.6666, test loss-1.5281, acc-0.6519\n",
      "Iter-3060, train loss-1.4884, acc-0.6800, valid loss-1.5258, acc-0.6688, test loss-1.5258, acc-0.6528\n",
      "Iter-3070, train loss-1.6404, acc-0.5400, valid loss-1.5234, acc-0.6700, test loss-1.5235, acc-0.6537\n",
      "Iter-3080, train loss-1.4242, acc-0.7800, valid loss-1.5210, acc-0.6704, test loss-1.5211, acc-0.6537\n",
      "Iter-3090, train loss-1.5697, acc-0.6000, valid loss-1.5187, acc-0.6708, test loss-1.5188, acc-0.6547\n",
      "Iter-3100, train loss-1.6145, acc-0.6000, valid loss-1.5164, acc-0.6718, test loss-1.5166, acc-0.6556\n",
      "Iter-3110, train loss-1.5427, acc-0.5800, valid loss-1.5141, acc-0.6728, test loss-1.5144, acc-0.6576\n",
      "Iter-3120, train loss-1.5780, acc-0.6400, valid loss-1.5118, acc-0.6738, test loss-1.5121, acc-0.6581\n",
      "Iter-3130, train loss-1.3863, acc-0.7200, valid loss-1.5095, acc-0.6752, test loss-1.5098, acc-0.6593\n",
      "Iter-3140, train loss-1.5704, acc-0.6000, valid loss-1.5071, acc-0.6754, test loss-1.5074, acc-0.6591\n",
      "Iter-3150, train loss-1.4575, acc-0.7200, valid loss-1.5046, acc-0.6754, test loss-1.5049, acc-0.6593\n",
      "Iter-3160, train loss-1.6912, acc-0.4600, valid loss-1.5024, acc-0.6776, test loss-1.5027, acc-0.6608\n",
      "Iter-3170, train loss-1.5435, acc-0.6200, valid loss-1.5001, acc-0.6772, test loss-1.5005, acc-0.6606\n",
      "Iter-3180, train loss-1.4620, acc-0.6800, valid loss-1.4978, acc-0.6778, test loss-1.4982, acc-0.6612\n",
      "Iter-3190, train loss-1.4336, acc-0.7200, valid loss-1.4955, acc-0.6790, test loss-1.4958, acc-0.6619\n",
      "Iter-3200, train loss-1.4935, acc-0.5800, valid loss-1.4932, acc-0.6798, test loss-1.4936, acc-0.6625\n",
      "Iter-3210, train loss-1.4968, acc-0.7600, valid loss-1.4908, acc-0.6798, test loss-1.4912, acc-0.6633\n",
      "Iter-3220, train loss-1.5138, acc-0.6400, valid loss-1.4884, acc-0.6804, test loss-1.4889, acc-0.6636\n",
      "Iter-3230, train loss-1.5303, acc-0.5800, valid loss-1.4861, acc-0.6816, test loss-1.4866, acc-0.6646\n",
      "Iter-3240, train loss-1.4507, acc-0.6600, valid loss-1.4838, acc-0.6820, test loss-1.4843, acc-0.6649\n",
      "Iter-3250, train loss-1.4840, acc-0.6600, valid loss-1.4815, acc-0.6834, test loss-1.4820, acc-0.6661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-3260, train loss-1.5185, acc-0.6600, valid loss-1.4792, acc-0.6834, test loss-1.4798, acc-0.6661\n",
      "Iter-3270, train loss-1.4931, acc-0.6000, valid loss-1.4770, acc-0.6850, test loss-1.4776, acc-0.6666\n",
      "Iter-3280, train loss-1.4347, acc-0.7400, valid loss-1.4748, acc-0.6856, test loss-1.4754, acc-0.6674\n",
      "Iter-3290, train loss-1.5035, acc-0.6400, valid loss-1.4727, acc-0.6862, test loss-1.4732, acc-0.6681\n",
      "Iter-3300, train loss-1.5652, acc-0.6000, valid loss-1.4704, acc-0.6866, test loss-1.4710, acc-0.6686\n",
      "Iter-3310, train loss-1.4441, acc-0.6600, valid loss-1.4680, acc-0.6872, test loss-1.4687, acc-0.6691\n",
      "Iter-3320, train loss-1.5187, acc-0.6600, valid loss-1.4658, acc-0.6876, test loss-1.4665, acc-0.6698\n",
      "Iter-3330, train loss-1.4217, acc-0.7200, valid loss-1.4635, acc-0.6890, test loss-1.4642, acc-0.6703\n",
      "Iter-3340, train loss-1.5400, acc-0.5600, valid loss-1.4613, acc-0.6886, test loss-1.4620, acc-0.6714\n",
      "Iter-3350, train loss-1.4056, acc-0.7000, valid loss-1.4591, acc-0.6892, test loss-1.4598, acc-0.6715\n",
      "Iter-3360, train loss-1.3278, acc-0.7600, valid loss-1.4568, acc-0.6898, test loss-1.4574, acc-0.6728\n",
      "Iter-3370, train loss-1.3953, acc-0.8000, valid loss-1.4545, acc-0.6908, test loss-1.4553, acc-0.6742\n",
      "Iter-3380, train loss-1.3925, acc-0.6800, valid loss-1.4522, acc-0.6912, test loss-1.4530, acc-0.6755\n",
      "Iter-3390, train loss-1.5997, acc-0.5600, valid loss-1.4501, acc-0.6916, test loss-1.4508, acc-0.6770\n",
      "Iter-3400, train loss-1.5072, acc-0.5800, valid loss-1.4479, acc-0.6922, test loss-1.4485, acc-0.6778\n",
      "Iter-3410, train loss-1.5835, acc-0.6000, valid loss-1.4457, acc-0.6924, test loss-1.4464, acc-0.6785\n",
      "Iter-3420, train loss-1.4479, acc-0.6600, valid loss-1.4435, acc-0.6928, test loss-1.4443, acc-0.6788\n",
      "Iter-3430, train loss-1.5147, acc-0.6800, valid loss-1.4413, acc-0.6934, test loss-1.4421, acc-0.6795\n",
      "Iter-3440, train loss-1.3797, acc-0.7600, valid loss-1.4390, acc-0.6938, test loss-1.4399, acc-0.6811\n",
      "Iter-3450, train loss-1.4960, acc-0.6800, valid loss-1.4367, acc-0.6938, test loss-1.4376, acc-0.6804\n",
      "Iter-3460, train loss-1.3928, acc-0.7800, valid loss-1.4347, acc-0.6942, test loss-1.4355, acc-0.6813\n",
      "Iter-3470, train loss-1.4606, acc-0.7000, valid loss-1.4324, acc-0.6954, test loss-1.4333, acc-0.6827\n",
      "Iter-3480, train loss-1.3890, acc-0.6800, valid loss-1.4302, acc-0.6962, test loss-1.4312, acc-0.6837\n",
      "Iter-3490, train loss-1.4035, acc-0.7200, valid loss-1.4280, acc-0.6956, test loss-1.4291, acc-0.6825\n",
      "Iter-3500, train loss-1.4891, acc-0.6400, valid loss-1.4258, acc-0.6966, test loss-1.4270, acc-0.6830\n",
      "Iter-3510, train loss-1.4483, acc-0.6400, valid loss-1.4235, acc-0.6966, test loss-1.4248, acc-0.6830\n",
      "Iter-3520, train loss-1.5008, acc-0.6200, valid loss-1.4214, acc-0.6972, test loss-1.4226, acc-0.6834\n",
      "Iter-3530, train loss-1.4540, acc-0.6000, valid loss-1.4192, acc-0.6978, test loss-1.4205, acc-0.6853\n",
      "Iter-3540, train loss-1.3396, acc-0.7400, valid loss-1.4169, acc-0.6980, test loss-1.4182, acc-0.6854\n",
      "Iter-3550, train loss-1.4718, acc-0.6600, valid loss-1.4147, acc-0.6976, test loss-1.4161, acc-0.6856\n",
      "Iter-3560, train loss-1.3927, acc-0.7200, valid loss-1.4126, acc-0.6980, test loss-1.4140, acc-0.6863\n",
      "Iter-3570, train loss-1.4090, acc-0.7800, valid loss-1.4103, acc-0.6984, test loss-1.4117, acc-0.6867\n",
      "Iter-3580, train loss-1.3167, acc-0.7800, valid loss-1.4081, acc-0.6986, test loss-1.4096, acc-0.6871\n",
      "Iter-3590, train loss-1.3091, acc-0.7200, valid loss-1.4060, acc-0.6982, test loss-1.4075, acc-0.6872\n",
      "Iter-3600, train loss-1.4195, acc-0.6800, valid loss-1.4038, acc-0.6996, test loss-1.4053, acc-0.6882\n",
      "Iter-3610, train loss-1.4344, acc-0.6800, valid loss-1.4017, acc-0.6992, test loss-1.4032, acc-0.6893\n",
      "Iter-3620, train loss-1.3658, acc-0.7000, valid loss-1.3995, acc-0.6992, test loss-1.4010, acc-0.6902\n",
      "Iter-3630, train loss-1.3198, acc-0.7200, valid loss-1.3974, acc-0.6990, test loss-1.3988, acc-0.6913\n",
      "Iter-3640, train loss-1.3493, acc-0.7400, valid loss-1.3952, acc-0.7000, test loss-1.3967, acc-0.6915\n",
      "Iter-3650, train loss-1.4811, acc-0.6400, valid loss-1.3931, acc-0.7010, test loss-1.3946, acc-0.6925\n",
      "Iter-3660, train loss-1.4353, acc-0.6000, valid loss-1.3910, acc-0.7018, test loss-1.3925, acc-0.6922\n",
      "Iter-3670, train loss-1.4507, acc-0.6800, valid loss-1.3889, acc-0.7024, test loss-1.3905, acc-0.6933\n",
      "Iter-3680, train loss-1.3351, acc-0.7800, valid loss-1.3868, acc-0.7032, test loss-1.3883, acc-0.6940\n",
      "Iter-3690, train loss-1.4089, acc-0.6800, valid loss-1.3847, acc-0.7032, test loss-1.3862, acc-0.6945\n",
      "Iter-3700, train loss-1.2884, acc-0.7200, valid loss-1.3826, acc-0.7038, test loss-1.3841, acc-0.6955\n",
      "Iter-3710, train loss-1.3341, acc-0.6400, valid loss-1.3806, acc-0.7062, test loss-1.3821, acc-0.6975\n",
      "Iter-3720, train loss-1.3817, acc-0.7600, valid loss-1.3785, acc-0.7068, test loss-1.3800, acc-0.6980\n",
      "Iter-3730, train loss-1.3288, acc-0.7600, valid loss-1.3763, acc-0.7074, test loss-1.3779, acc-0.6993\n",
      "Iter-3740, train loss-1.3706, acc-0.7600, valid loss-1.3742, acc-0.7076, test loss-1.3759, acc-0.6992\n",
      "Iter-3750, train loss-1.3239, acc-0.7400, valid loss-1.3721, acc-0.7076, test loss-1.3738, acc-0.6997\n",
      "Iter-3760, train loss-1.4449, acc-0.6400, valid loss-1.3701, acc-0.7088, test loss-1.3717, acc-0.7006\n",
      "Iter-3770, train loss-1.3332, acc-0.7200, valid loss-1.3681, acc-0.7100, test loss-1.3697, acc-0.7016\n",
      "Iter-3780, train loss-1.4380, acc-0.6400, valid loss-1.3661, acc-0.7110, test loss-1.3677, acc-0.7041\n",
      "Iter-3790, train loss-1.4677, acc-0.7400, valid loss-1.3640, acc-0.7114, test loss-1.3656, acc-0.7053\n",
      "Iter-3800, train loss-1.4594, acc-0.7200, valid loss-1.3618, acc-0.7116, test loss-1.3635, acc-0.7048\n",
      "Iter-3810, train loss-1.3833, acc-0.7400, valid loss-1.3599, acc-0.7118, test loss-1.3615, acc-0.7045\n",
      "Iter-3820, train loss-1.3346, acc-0.7000, valid loss-1.3578, acc-0.7126, test loss-1.3595, acc-0.7061\n",
      "Iter-3830, train loss-1.2743, acc-0.8000, valid loss-1.3557, acc-0.7124, test loss-1.3575, acc-0.7063\n",
      "Iter-3840, train loss-1.3698, acc-0.7000, valid loss-1.3537, acc-0.7124, test loss-1.3555, acc-0.7065\n",
      "Iter-3850, train loss-1.4819, acc-0.5600, valid loss-1.3517, acc-0.7124, test loss-1.3535, acc-0.7072\n",
      "Iter-3860, train loss-1.4446, acc-0.6800, valid loss-1.3497, acc-0.7128, test loss-1.3515, acc-0.7071\n",
      "Iter-3870, train loss-1.3583, acc-0.7000, valid loss-1.3476, acc-0.7142, test loss-1.3495, acc-0.7084\n",
      "Iter-3880, train loss-1.2567, acc-0.8000, valid loss-1.3455, acc-0.7152, test loss-1.3474, acc-0.7089\n",
      "Iter-3890, train loss-1.4788, acc-0.7400, valid loss-1.3435, acc-0.7156, test loss-1.3454, acc-0.7096\n",
      "Iter-3900, train loss-1.4244, acc-0.7200, valid loss-1.3415, acc-0.7166, test loss-1.3434, acc-0.7104\n",
      "Iter-3910, train loss-1.3173, acc-0.7000, valid loss-1.3395, acc-0.7168, test loss-1.3414, acc-0.7112\n",
      "Iter-3920, train loss-1.3129, acc-0.6600, valid loss-1.3374, acc-0.7168, test loss-1.3394, acc-0.7115\n",
      "Iter-3930, train loss-1.4901, acc-0.6600, valid loss-1.3354, acc-0.7176, test loss-1.3374, acc-0.7130\n",
      "Iter-3940, train loss-1.4471, acc-0.5200, valid loss-1.3334, acc-0.7170, test loss-1.3354, acc-0.7129\n",
      "Iter-3950, train loss-1.2453, acc-0.7800, valid loss-1.3315, acc-0.7182, test loss-1.3335, acc-0.7137\n",
      "Iter-3960, train loss-1.1713, acc-0.8200, valid loss-1.3294, acc-0.7194, test loss-1.3315, acc-0.7146\n",
      "Iter-3970, train loss-1.2062, acc-0.8600, valid loss-1.3274, acc-0.7200, test loss-1.3295, acc-0.7154\n",
      "Iter-3980, train loss-1.2842, acc-0.7400, valid loss-1.3254, acc-0.7200, test loss-1.3275, acc-0.7169\n",
      "Iter-3990, train loss-1.5077, acc-0.6400, valid loss-1.3234, acc-0.7206, test loss-1.3255, acc-0.7174\n",
      "Iter-4000, train loss-1.3124, acc-0.7200, valid loss-1.3214, acc-0.7204, test loss-1.3235, acc-0.7179\n",
      "Iter-4010, train loss-1.3418, acc-0.7400, valid loss-1.3194, acc-0.7204, test loss-1.3216, acc-0.7183\n",
      "Iter-4020, train loss-1.2159, acc-0.8000, valid loss-1.3175, acc-0.7206, test loss-1.3197, acc-0.7178\n",
      "Iter-4030, train loss-1.2945, acc-0.7200, valid loss-1.3155, acc-0.7210, test loss-1.3177, acc-0.7185\n",
      "Iter-4040, train loss-1.3542, acc-0.6400, valid loss-1.3135, acc-0.7218, test loss-1.3157, acc-0.7187\n",
      "Iter-4050, train loss-1.3601, acc-0.6000, valid loss-1.3116, acc-0.7218, test loss-1.3139, acc-0.7195\n",
      "Iter-4060, train loss-1.3309, acc-0.7200, valid loss-1.3096, acc-0.7218, test loss-1.3119, acc-0.7198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4070, train loss-1.4712, acc-0.6600, valid loss-1.3077, acc-0.7222, test loss-1.3101, acc-0.7201\n",
      "Iter-4080, train loss-1.3308, acc-0.6400, valid loss-1.3058, acc-0.7224, test loss-1.3082, acc-0.7195\n",
      "Iter-4090, train loss-1.4593, acc-0.6200, valid loss-1.3038, acc-0.7224, test loss-1.3062, acc-0.7194\n",
      "Iter-4100, train loss-1.4940, acc-0.6200, valid loss-1.3019, acc-0.7230, test loss-1.3043, acc-0.7194\n",
      "Iter-4110, train loss-1.2628, acc-0.7200, valid loss-1.2999, acc-0.7222, test loss-1.3024, acc-0.7192\n",
      "Iter-4120, train loss-1.2938, acc-0.7600, valid loss-1.2980, acc-0.7218, test loss-1.3005, acc-0.7185\n",
      "Iter-4130, train loss-1.3580, acc-0.6400, valid loss-1.2960, acc-0.7230, test loss-1.2986, acc-0.7191\n",
      "Iter-4140, train loss-1.3543, acc-0.6600, valid loss-1.2942, acc-0.7238, test loss-1.2968, acc-0.7201\n",
      "Iter-4150, train loss-1.3983, acc-0.7800, valid loss-1.2922, acc-0.7244, test loss-1.2949, acc-0.7209\n",
      "Iter-4160, train loss-1.2720, acc-0.7600, valid loss-1.2902, acc-0.7248, test loss-1.2929, acc-0.7212\n",
      "Iter-4170, train loss-1.3294, acc-0.7200, valid loss-1.2883, acc-0.7252, test loss-1.2910, acc-0.7217\n",
      "Iter-4180, train loss-1.1700, acc-0.8600, valid loss-1.2863, acc-0.7256, test loss-1.2891, acc-0.7215\n",
      "Iter-4190, train loss-1.4319, acc-0.6000, valid loss-1.2844, acc-0.7262, test loss-1.2872, acc-0.7218\n",
      "Iter-4200, train loss-1.2861, acc-0.7800, valid loss-1.2825, acc-0.7268, test loss-1.2853, acc-0.7219\n",
      "Iter-4210, train loss-1.1626, acc-0.7800, valid loss-1.2806, acc-0.7272, test loss-1.2834, acc-0.7217\n",
      "Iter-4220, train loss-1.3002, acc-0.7200, valid loss-1.2787, acc-0.7280, test loss-1.2815, acc-0.7224\n",
      "Iter-4230, train loss-1.2893, acc-0.7200, valid loss-1.2768, acc-0.7284, test loss-1.2796, acc-0.7239\n",
      "Iter-4240, train loss-1.2634, acc-0.7400, valid loss-1.2749, acc-0.7290, test loss-1.2778, acc-0.7241\n",
      "Iter-4250, train loss-1.3125, acc-0.7400, valid loss-1.2731, acc-0.7294, test loss-1.2760, acc-0.7251\n",
      "Iter-4260, train loss-1.3007, acc-0.6800, valid loss-1.2712, acc-0.7294, test loss-1.2742, acc-0.7259\n",
      "Iter-4270, train loss-1.3297, acc-0.7800, valid loss-1.2694, acc-0.7300, test loss-1.2724, acc-0.7266\n",
      "Iter-4280, train loss-1.3314, acc-0.6600, valid loss-1.2676, acc-0.7302, test loss-1.2707, acc-0.7273\n",
      "Iter-4290, train loss-1.3714, acc-0.6400, valid loss-1.2658, acc-0.7314, test loss-1.2689, acc-0.7284\n",
      "Iter-4300, train loss-1.3734, acc-0.6000, valid loss-1.2639, acc-0.7328, test loss-1.2670, acc-0.7296\n",
      "Iter-4310, train loss-1.1273, acc-0.8600, valid loss-1.2620, acc-0.7328, test loss-1.2651, acc-0.7303\n",
      "Iter-4320, train loss-1.2012, acc-0.7600, valid loss-1.2601, acc-0.7336, test loss-1.2633, acc-0.7315\n",
      "Iter-4330, train loss-1.1502, acc-0.7800, valid loss-1.2582, acc-0.7338, test loss-1.2614, acc-0.7333\n",
      "Iter-4340, train loss-1.3327, acc-0.6600, valid loss-1.2564, acc-0.7346, test loss-1.2595, acc-0.7345\n",
      "Iter-4350, train loss-1.1629, acc-0.7400, valid loss-1.2546, acc-0.7354, test loss-1.2577, acc-0.7351\n",
      "Iter-4360, train loss-1.3186, acc-0.8000, valid loss-1.2528, acc-0.7362, test loss-1.2559, acc-0.7361\n",
      "Iter-4370, train loss-1.3324, acc-0.7400, valid loss-1.2509, acc-0.7376, test loss-1.2540, acc-0.7367\n",
      "Iter-4380, train loss-1.1459, acc-0.8800, valid loss-1.2491, acc-0.7382, test loss-1.2523, acc-0.7366\n",
      "Iter-4390, train loss-1.2737, acc-0.7400, valid loss-1.2473, acc-0.7372, test loss-1.2504, acc-0.7374\n",
      "Iter-4400, train loss-1.3279, acc-0.6600, valid loss-1.2455, acc-0.7380, test loss-1.2486, acc-0.7383\n",
      "Iter-4410, train loss-1.2078, acc-0.7200, valid loss-1.2437, acc-0.7390, test loss-1.2468, acc-0.7385\n",
      "Iter-4420, train loss-1.3377, acc-0.6600, valid loss-1.2419, acc-0.7400, test loss-1.2451, acc-0.7389\n",
      "Iter-4430, train loss-1.1849, acc-0.8000, valid loss-1.2401, acc-0.7408, test loss-1.2434, acc-0.7393\n",
      "Iter-4440, train loss-1.4509, acc-0.6800, valid loss-1.2383, acc-0.7418, test loss-1.2416, acc-0.7401\n",
      "Iter-4450, train loss-1.3320, acc-0.7000, valid loss-1.2365, acc-0.7416, test loss-1.2398, acc-0.7407\n",
      "Iter-4460, train loss-1.2465, acc-0.7600, valid loss-1.2347, acc-0.7420, test loss-1.2381, acc-0.7409\n",
      "Iter-4470, train loss-1.2123, acc-0.7800, valid loss-1.2329, acc-0.7428, test loss-1.2363, acc-0.7421\n",
      "Iter-4480, train loss-1.3535, acc-0.6800, valid loss-1.2311, acc-0.7428, test loss-1.2346, acc-0.7413\n",
      "Iter-4490, train loss-1.2450, acc-0.6600, valid loss-1.2293, acc-0.7430, test loss-1.2328, acc-0.7418\n",
      "Iter-4500, train loss-1.2464, acc-0.7200, valid loss-1.2274, acc-0.7430, test loss-1.2310, acc-0.7420\n",
      "Iter-4510, train loss-1.4008, acc-0.6600, valid loss-1.2257, acc-0.7434, test loss-1.2293, acc-0.7422\n",
      "Iter-4520, train loss-1.2764, acc-0.7400, valid loss-1.2239, acc-0.7430, test loss-1.2275, acc-0.7422\n",
      "Iter-4530, train loss-1.2843, acc-0.7400, valid loss-1.2222, acc-0.7434, test loss-1.2259, acc-0.7430\n",
      "Iter-4540, train loss-1.3834, acc-0.6600, valid loss-1.2205, acc-0.7436, test loss-1.2242, acc-0.7432\n",
      "Iter-4550, train loss-1.2963, acc-0.7000, valid loss-1.2188, acc-0.7448, test loss-1.2225, acc-0.7439\n",
      "Iter-4560, train loss-1.2930, acc-0.6600, valid loss-1.2171, acc-0.7446, test loss-1.2208, acc-0.7438\n",
      "Iter-4570, train loss-1.2567, acc-0.7200, valid loss-1.2154, acc-0.7458, test loss-1.2191, acc-0.7448\n",
      "Iter-4580, train loss-1.2558, acc-0.6800, valid loss-1.2136, acc-0.7460, test loss-1.2174, acc-0.7455\n",
      "Iter-4590, train loss-1.3012, acc-0.7400, valid loss-1.2119, acc-0.7456, test loss-1.2157, acc-0.7451\n",
      "Iter-4600, train loss-1.2131, acc-0.7800, valid loss-1.2102, acc-0.7460, test loss-1.2139, acc-0.7455\n",
      "Iter-4610, train loss-1.2610, acc-0.7200, valid loss-1.2084, acc-0.7458, test loss-1.2121, acc-0.7466\n",
      "Iter-4620, train loss-1.1701, acc-0.7800, valid loss-1.2066, acc-0.7466, test loss-1.2104, acc-0.7468\n",
      "Iter-4630, train loss-1.2635, acc-0.6800, valid loss-1.2048, acc-0.7472, test loss-1.2087, acc-0.7479\n",
      "Iter-4640, train loss-1.2438, acc-0.7200, valid loss-1.2032, acc-0.7470, test loss-1.2070, acc-0.7478\n",
      "Iter-4650, train loss-1.1659, acc-0.7800, valid loss-1.2014, acc-0.7480, test loss-1.2054, acc-0.7488\n",
      "Iter-4660, train loss-1.0176, acc-0.9000, valid loss-1.1997, acc-0.7486, test loss-1.2037, acc-0.7495\n",
      "Iter-4670, train loss-1.2551, acc-0.7400, valid loss-1.1980, acc-0.7478, test loss-1.2020, acc-0.7488\n",
      "Iter-4680, train loss-1.2476, acc-0.7200, valid loss-1.1962, acc-0.7482, test loss-1.2003, acc-0.7494\n",
      "Iter-4690, train loss-1.2161, acc-0.6600, valid loss-1.1946, acc-0.7484, test loss-1.1987, acc-0.7494\n",
      "Iter-4700, train loss-1.2553, acc-0.7200, valid loss-1.1929, acc-0.7486, test loss-1.1970, acc-0.7500\n",
      "Iter-4710, train loss-1.1950, acc-0.8000, valid loss-1.1911, acc-0.7492, test loss-1.1953, acc-0.7506\n",
      "Iter-4720, train loss-1.2477, acc-0.7600, valid loss-1.1894, acc-0.7502, test loss-1.1937, acc-0.7510\n",
      "Iter-4730, train loss-1.2516, acc-0.7600, valid loss-1.1878, acc-0.7504, test loss-1.1920, acc-0.7514\n",
      "Iter-4740, train loss-1.2565, acc-0.7200, valid loss-1.1862, acc-0.7512, test loss-1.1904, acc-0.7516\n",
      "Iter-4750, train loss-1.2114, acc-0.7400, valid loss-1.1844, acc-0.7522, test loss-1.1886, acc-0.7528\n",
      "Iter-4760, train loss-1.1454, acc-0.7800, valid loss-1.1828, acc-0.7522, test loss-1.1870, acc-0.7522\n",
      "Iter-4770, train loss-1.1578, acc-0.8400, valid loss-1.1811, acc-0.7522, test loss-1.1854, acc-0.7519\n",
      "Iter-4780, train loss-1.2117, acc-0.7200, valid loss-1.1794, acc-0.7530, test loss-1.1837, acc-0.7521\n",
      "Iter-4790, train loss-1.1853, acc-0.7800, valid loss-1.1778, acc-0.7530, test loss-1.1821, acc-0.7522\n",
      "Iter-4800, train loss-1.1466, acc-0.8400, valid loss-1.1762, acc-0.7538, test loss-1.1806, acc-0.7527\n",
      "Iter-4810, train loss-1.1587, acc-0.7400, valid loss-1.1747, acc-0.7542, test loss-1.1790, acc-0.7531\n",
      "Iter-4820, train loss-1.1446, acc-0.7800, valid loss-1.1731, acc-0.7542, test loss-1.1774, acc-0.7531\n",
      "Iter-4830, train loss-1.2313, acc-0.7000, valid loss-1.1714, acc-0.7544, test loss-1.1758, acc-0.7533\n",
      "Iter-4840, train loss-1.1215, acc-0.8200, valid loss-1.1697, acc-0.7550, test loss-1.1742, acc-0.7538\n",
      "Iter-4850, train loss-1.3169, acc-0.7000, valid loss-1.1682, acc-0.7548, test loss-1.1725, acc-0.7548\n",
      "Iter-4860, train loss-1.2155, acc-0.6800, valid loss-1.1666, acc-0.7550, test loss-1.1710, acc-0.7550\n",
      "Iter-4870, train loss-1.2028, acc-0.7000, valid loss-1.1650, acc-0.7546, test loss-1.1694, acc-0.7547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-4880, train loss-1.2003, acc-0.7600, valid loss-1.1633, acc-0.7554, test loss-1.1678, acc-0.7552\n",
      "Iter-4890, train loss-1.0813, acc-0.8200, valid loss-1.1617, acc-0.7556, test loss-1.1662, acc-0.7560\n",
      "Iter-4900, train loss-1.0821, acc-0.7800, valid loss-1.1601, acc-0.7564, test loss-1.1645, acc-0.7563\n",
      "Iter-4910, train loss-1.1415, acc-0.7400, valid loss-1.1585, acc-0.7572, test loss-1.1630, acc-0.7566\n",
      "Iter-4920, train loss-1.1403, acc-0.7400, valid loss-1.1568, acc-0.7570, test loss-1.1614, acc-0.7563\n",
      "Iter-4930, train loss-1.1124, acc-0.7800, valid loss-1.1552, acc-0.7570, test loss-1.1598, acc-0.7565\n",
      "Iter-4940, train loss-1.3750, acc-0.6200, valid loss-1.1536, acc-0.7572, test loss-1.1583, acc-0.7573\n",
      "Iter-4950, train loss-1.1057, acc-0.8400, valid loss-1.1520, acc-0.7576, test loss-1.1567, acc-0.7573\n",
      "Iter-4960, train loss-1.1180, acc-0.7200, valid loss-1.1504, acc-0.7580, test loss-1.1551, acc-0.7579\n",
      "Iter-4970, train loss-1.1439, acc-0.8200, valid loss-1.1488, acc-0.7584, test loss-1.1535, acc-0.7580\n",
      "Iter-4980, train loss-1.2608, acc-0.6400, valid loss-1.1473, acc-0.7596, test loss-1.1520, acc-0.7586\n",
      "Iter-4990, train loss-1.1078, acc-0.7600, valid loss-1.1457, acc-0.7598, test loss-1.1505, acc-0.7597\n",
      "Iter-5000, train loss-1.0120, acc-0.7800, valid loss-1.1442, acc-0.7600, test loss-1.1489, acc-0.7595\n",
      "Iter-5010, train loss-1.0962, acc-0.8400, valid loss-1.1427, acc-0.7602, test loss-1.1474, acc-0.7596\n",
      "Iter-5020, train loss-1.1808, acc-0.6800, valid loss-1.1411, acc-0.7608, test loss-1.1459, acc-0.7604\n",
      "Iter-5030, train loss-1.1407, acc-0.7200, valid loss-1.1396, acc-0.7622, test loss-1.1444, acc-0.7605\n",
      "Iter-5040, train loss-1.1161, acc-0.7400, valid loss-1.1380, acc-0.7620, test loss-1.1428, acc-0.7604\n",
      "Iter-5050, train loss-1.2085, acc-0.6600, valid loss-1.1364, acc-0.7632, test loss-1.1412, acc-0.7611\n",
      "Iter-5060, train loss-1.0567, acc-0.7800, valid loss-1.1349, acc-0.7632, test loss-1.1397, acc-0.7608\n",
      "Iter-5070, train loss-1.2033, acc-0.6800, valid loss-1.1334, acc-0.7642, test loss-1.1382, acc-0.7615\n",
      "Iter-5080, train loss-1.0244, acc-0.8200, valid loss-1.1318, acc-0.7640, test loss-1.1366, acc-0.7614\n",
      "Iter-5090, train loss-1.1413, acc-0.7800, valid loss-1.1302, acc-0.7654, test loss-1.1351, acc-0.7617\n",
      "Iter-5100, train loss-1.1270, acc-0.7200, valid loss-1.1287, acc-0.7656, test loss-1.1336, acc-0.7622\n",
      "Iter-5110, train loss-1.2194, acc-0.7400, valid loss-1.1272, acc-0.7652, test loss-1.1321, acc-0.7628\n",
      "Iter-5120, train loss-0.9947, acc-0.7600, valid loss-1.1256, acc-0.7652, test loss-1.1307, acc-0.7633\n",
      "Iter-5130, train loss-0.9571, acc-0.9000, valid loss-1.1241, acc-0.7658, test loss-1.1291, acc-0.7630\n",
      "Iter-5140, train loss-1.1504, acc-0.7200, valid loss-1.1225, acc-0.7668, test loss-1.1276, acc-0.7634\n",
      "Iter-5150, train loss-1.0451, acc-0.7800, valid loss-1.1210, acc-0.7660, test loss-1.1260, acc-0.7638\n",
      "Iter-5160, train loss-1.1870, acc-0.7400, valid loss-1.1195, acc-0.7664, test loss-1.1246, acc-0.7640\n",
      "Iter-5170, train loss-1.0618, acc-0.8400, valid loss-1.1180, acc-0.7662, test loss-1.1231, acc-0.7644\n",
      "Iter-5180, train loss-0.9537, acc-0.8200, valid loss-1.1165, acc-0.7662, test loss-1.1216, acc-0.7646\n",
      "Iter-5190, train loss-1.1957, acc-0.7400, valid loss-1.1149, acc-0.7660, test loss-1.1201, acc-0.7642\n",
      "Iter-5200, train loss-1.2314, acc-0.6400, valid loss-1.1134, acc-0.7672, test loss-1.1186, acc-0.7647\n",
      "Iter-5210, train loss-1.0636, acc-0.7800, valid loss-1.1119, acc-0.7670, test loss-1.1172, acc-0.7650\n",
      "Iter-5220, train loss-1.0932, acc-0.7800, valid loss-1.1105, acc-0.7676, test loss-1.1158, acc-0.7652\n",
      "Iter-5230, train loss-1.0743, acc-0.7400, valid loss-1.1090, acc-0.7676, test loss-1.1143, acc-0.7659\n",
      "Iter-5240, train loss-1.0763, acc-0.8000, valid loss-1.1075, acc-0.7688, test loss-1.1128, acc-0.7661\n",
      "Iter-5250, train loss-1.1021, acc-0.7000, valid loss-1.1060, acc-0.7684, test loss-1.1114, acc-0.7662\n",
      "Iter-5260, train loss-1.1394, acc-0.7200, valid loss-1.1045, acc-0.7688, test loss-1.1099, acc-0.7662\n",
      "Iter-5270, train loss-1.2066, acc-0.6600, valid loss-1.1030, acc-0.7692, test loss-1.1084, acc-0.7663\n",
      "Iter-5280, train loss-1.1365, acc-0.7800, valid loss-1.1015, acc-0.7700, test loss-1.1069, acc-0.7663\n",
      "Iter-5290, train loss-1.1037, acc-0.7800, valid loss-1.1001, acc-0.7700, test loss-1.1056, acc-0.7666\n",
      "Iter-5300, train loss-0.9822, acc-0.8200, valid loss-1.0986, acc-0.7706, test loss-1.1041, acc-0.7673\n",
      "Iter-5310, train loss-1.0415, acc-0.7800, valid loss-1.0972, acc-0.7706, test loss-1.1027, acc-0.7672\n",
      "Iter-5320, train loss-1.3551, acc-0.5600, valid loss-1.0957, acc-0.7710, test loss-1.1013, acc-0.7684\n",
      "Iter-5330, train loss-1.1868, acc-0.6600, valid loss-1.0944, acc-0.7714, test loss-1.0999, acc-0.7687\n",
      "Iter-5340, train loss-1.2406, acc-0.6200, valid loss-1.0929, acc-0.7720, test loss-1.0984, acc-0.7691\n",
      "Iter-5350, train loss-1.0428, acc-0.6600, valid loss-1.0915, acc-0.7720, test loss-1.0970, acc-0.7695\n",
      "Iter-5360, train loss-1.0049, acc-0.8200, valid loss-1.0901, acc-0.7722, test loss-1.0956, acc-0.7697\n",
      "Iter-5370, train loss-1.1485, acc-0.7800, valid loss-1.0887, acc-0.7724, test loss-1.0942, acc-0.7697\n",
      "Iter-5380, train loss-1.2535, acc-0.7200, valid loss-1.0874, acc-0.7728, test loss-1.0929, acc-0.7699\n",
      "Iter-5390, train loss-0.9928, acc-0.8200, valid loss-1.0859, acc-0.7734, test loss-1.0915, acc-0.7704\n",
      "Iter-5400, train loss-0.9654, acc-0.8800, valid loss-1.0844, acc-0.7728, test loss-1.0900, acc-0.7706\n",
      "Iter-5410, train loss-0.9701, acc-0.9000, valid loss-1.0830, acc-0.7740, test loss-1.0887, acc-0.7704\n",
      "Iter-5420, train loss-1.0246, acc-0.8200, valid loss-1.0817, acc-0.7736, test loss-1.0873, acc-0.7702\n",
      "Iter-5430, train loss-1.1597, acc-0.7000, valid loss-1.0802, acc-0.7744, test loss-1.0859, acc-0.7701\n",
      "Iter-5440, train loss-1.1509, acc-0.7000, valid loss-1.0788, acc-0.7748, test loss-1.0845, acc-0.7706\n",
      "Iter-5450, train loss-1.0473, acc-0.8200, valid loss-1.0775, acc-0.7750, test loss-1.0832, acc-0.7706\n",
      "Iter-5460, train loss-0.9684, acc-0.8400, valid loss-1.0761, acc-0.7744, test loss-1.0819, acc-0.7708\n",
      "Iter-5470, train loss-1.0584, acc-0.8600, valid loss-1.0746, acc-0.7746, test loss-1.0804, acc-0.7712\n",
      "Iter-5480, train loss-1.0661, acc-0.8000, valid loss-1.0732, acc-0.7760, test loss-1.0790, acc-0.7716\n",
      "Iter-5490, train loss-1.0771, acc-0.8000, valid loss-1.0718, acc-0.7760, test loss-1.0777, acc-0.7716\n",
      "Iter-5500, train loss-1.0666, acc-0.7200, valid loss-1.0704, acc-0.7760, test loss-1.0763, acc-0.7718\n",
      "Iter-5510, train loss-0.9817, acc-0.8400, valid loss-1.0690, acc-0.7760, test loss-1.0750, acc-0.7721\n",
      "Iter-5520, train loss-1.0651, acc-0.7800, valid loss-1.0677, acc-0.7760, test loss-1.0736, acc-0.7724\n",
      "Iter-5530, train loss-1.1463, acc-0.6800, valid loss-1.0664, acc-0.7762, test loss-1.0723, acc-0.7728\n",
      "Iter-5540, train loss-1.0320, acc-0.8400, valid loss-1.0649, acc-0.7768, test loss-1.0709, acc-0.7731\n",
      "Iter-5550, train loss-1.2031, acc-0.7200, valid loss-1.0636, acc-0.7780, test loss-1.0696, acc-0.7735\n",
      "Iter-5560, train loss-1.0397, acc-0.8400, valid loss-1.0622, acc-0.7784, test loss-1.0682, acc-0.7736\n",
      "Iter-5570, train loss-1.1548, acc-0.7600, valid loss-1.0609, acc-0.7782, test loss-1.0669, acc-0.7746\n",
      "Iter-5580, train loss-1.1741, acc-0.6800, valid loss-1.0596, acc-0.7788, test loss-1.0656, acc-0.7754\n",
      "Iter-5590, train loss-1.0142, acc-0.7800, valid loss-1.0583, acc-0.7784, test loss-1.0643, acc-0.7758\n",
      "Iter-5600, train loss-1.1920, acc-0.6800, valid loss-1.0569, acc-0.7792, test loss-1.0628, acc-0.7758\n",
      "Iter-5610, train loss-1.0828, acc-0.7600, valid loss-1.0556, acc-0.7796, test loss-1.0615, acc-0.7762\n",
      "Iter-5620, train loss-0.9622, acc-0.8000, valid loss-1.0542, acc-0.7800, test loss-1.0602, acc-0.7770\n",
      "Iter-5630, train loss-1.0328, acc-0.8000, valid loss-1.0529, acc-0.7806, test loss-1.0588, acc-0.7769\n",
      "Iter-5640, train loss-1.0737, acc-0.7400, valid loss-1.0515, acc-0.7806, test loss-1.0575, acc-0.7770\n",
      "Iter-5650, train loss-1.0356, acc-0.7800, valid loss-1.0502, acc-0.7806, test loss-1.0562, acc-0.7772\n",
      "Iter-5660, train loss-0.9823, acc-0.8600, valid loss-1.0489, acc-0.7804, test loss-1.0548, acc-0.7782\n",
      "Iter-5670, train loss-1.1297, acc-0.8200, valid loss-1.0475, acc-0.7814, test loss-1.0535, acc-0.7787\n",
      "Iter-5680, train loss-1.1306, acc-0.6800, valid loss-1.0462, acc-0.7818, test loss-1.0521, acc-0.7788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-5690, train loss-1.1882, acc-0.6400, valid loss-1.0449, acc-0.7820, test loss-1.0508, acc-0.7787\n",
      "Iter-5700, train loss-1.0720, acc-0.7400, valid loss-1.0436, acc-0.7826, test loss-1.0495, acc-0.7792\n",
      "Iter-5710, train loss-1.1782, acc-0.7800, valid loss-1.0422, acc-0.7826, test loss-1.0482, acc-0.7792\n",
      "Iter-5720, train loss-1.1509, acc-0.7800, valid loss-1.0410, acc-0.7828, test loss-1.0470, acc-0.7796\n",
      "Iter-5730, train loss-0.9452, acc-0.8400, valid loss-1.0396, acc-0.7828, test loss-1.0456, acc-0.7802\n",
      "Iter-5740, train loss-1.0314, acc-0.7800, valid loss-1.0384, acc-0.7830, test loss-1.0443, acc-0.7807\n",
      "Iter-5750, train loss-0.8605, acc-0.9000, valid loss-1.0371, acc-0.7828, test loss-1.0430, acc-0.7809\n",
      "Iter-5760, train loss-1.0165, acc-0.7200, valid loss-1.0359, acc-0.7826, test loss-1.0418, acc-0.7808\n",
      "Iter-5770, train loss-1.1373, acc-0.7000, valid loss-1.0346, acc-0.7822, test loss-1.0406, acc-0.7812\n",
      "Iter-5780, train loss-1.0394, acc-0.8400, valid loss-1.0333, acc-0.7828, test loss-1.0393, acc-0.7813\n",
      "Iter-5790, train loss-1.0665, acc-0.7200, valid loss-1.0320, acc-0.7828, test loss-1.0381, acc-0.7819\n",
      "Iter-5800, train loss-1.1161, acc-0.7600, valid loss-1.0306, acc-0.7834, test loss-1.0368, acc-0.7820\n",
      "Iter-5810, train loss-1.1201, acc-0.7600, valid loss-1.0294, acc-0.7838, test loss-1.0356, acc-0.7821\n",
      "Iter-5820, train loss-1.0455, acc-0.8000, valid loss-1.0282, acc-0.7844, test loss-1.0344, acc-0.7825\n",
      "Iter-5830, train loss-0.8508, acc-0.9600, valid loss-1.0268, acc-0.7840, test loss-1.0331, acc-0.7826\n",
      "Iter-5840, train loss-1.0013, acc-0.8400, valid loss-1.0255, acc-0.7850, test loss-1.0318, acc-0.7828\n",
      "Iter-5850, train loss-1.1433, acc-0.7600, valid loss-1.0242, acc-0.7852, test loss-1.0305, acc-0.7827\n",
      "Iter-5860, train loss-1.0823, acc-0.7600, valid loss-1.0229, acc-0.7862, test loss-1.0293, acc-0.7842\n",
      "Iter-5870, train loss-1.0337, acc-0.8000, valid loss-1.0217, acc-0.7862, test loss-1.0281, acc-0.7843\n",
      "Iter-5880, train loss-0.9499, acc-0.8000, valid loss-1.0205, acc-0.7864, test loss-1.0268, acc-0.7843\n",
      "Iter-5890, train loss-1.0234, acc-0.8200, valid loss-1.0192, acc-0.7870, test loss-1.0256, acc-0.7848\n",
      "Iter-5900, train loss-0.9993, acc-0.7800, valid loss-1.0179, acc-0.7878, test loss-1.0243, acc-0.7853\n",
      "Iter-5910, train loss-1.0755, acc-0.7600, valid loss-1.0167, acc-0.7882, test loss-1.0230, acc-0.7854\n",
      "Iter-5920, train loss-1.1291, acc-0.6800, valid loss-1.0155, acc-0.7884, test loss-1.0217, acc-0.7861\n",
      "Iter-5930, train loss-1.1873, acc-0.6800, valid loss-1.0144, acc-0.7886, test loss-1.0206, acc-0.7863\n",
      "Iter-5940, train loss-1.1067, acc-0.7000, valid loss-1.0131, acc-0.7894, test loss-1.0194, acc-0.7863\n",
      "Iter-5950, train loss-1.0278, acc-0.7200, valid loss-1.0119, acc-0.7894, test loss-1.0182, acc-0.7867\n",
      "Iter-5960, train loss-0.8986, acc-0.8600, valid loss-1.0106, acc-0.7890, test loss-1.0169, acc-0.7868\n",
      "Iter-5970, train loss-1.1462, acc-0.7600, valid loss-1.0093, acc-0.7888, test loss-1.0157, acc-0.7866\n",
      "Iter-5980, train loss-0.9276, acc-0.8000, valid loss-1.0081, acc-0.7894, test loss-1.0145, acc-0.7873\n",
      "Iter-5990, train loss-1.1433, acc-0.6800, valid loss-1.0069, acc-0.7892, test loss-1.0133, acc-0.7873\n",
      "Iter-6000, train loss-1.2166, acc-0.7200, valid loss-1.0057, acc-0.7894, test loss-1.0120, acc-0.7875\n",
      "Iter-6010, train loss-0.9840, acc-0.8600, valid loss-1.0044, acc-0.7896, test loss-1.0108, acc-0.7877\n",
      "Iter-6020, train loss-0.9653, acc-0.8800, valid loss-1.0033, acc-0.7898, test loss-1.0096, acc-0.7879\n",
      "Iter-6030, train loss-0.9309, acc-0.8200, valid loss-1.0021, acc-0.7900, test loss-1.0084, acc-0.7881\n",
      "Iter-6040, train loss-0.9405, acc-0.8600, valid loss-1.0009, acc-0.7900, test loss-1.0072, acc-0.7887\n",
      "Iter-6050, train loss-0.8685, acc-0.9200, valid loss-0.9997, acc-0.7908, test loss-1.0060, acc-0.7886\n",
      "Iter-6060, train loss-0.9688, acc-0.8400, valid loss-0.9984, acc-0.7914, test loss-1.0048, acc-0.7892\n",
      "Iter-6070, train loss-0.9999, acc-0.8000, valid loss-0.9972, acc-0.7918, test loss-1.0036, acc-0.7903\n",
      "Iter-6080, train loss-1.0090, acc-0.7400, valid loss-0.9960, acc-0.7924, test loss-1.0024, acc-0.7901\n",
      "Iter-6090, train loss-1.0305, acc-0.7600, valid loss-0.9948, acc-0.7928, test loss-1.0011, acc-0.7902\n",
      "Iter-6100, train loss-0.9771, acc-0.8200, valid loss-0.9936, acc-0.7938, test loss-1.0000, acc-0.7906\n",
      "Iter-6110, train loss-1.0178, acc-0.7800, valid loss-0.9924, acc-0.7940, test loss-0.9988, acc-0.7906\n",
      "Iter-6120, train loss-1.1486, acc-0.7200, valid loss-0.9913, acc-0.7940, test loss-0.9976, acc-0.7912\n",
      "Iter-6130, train loss-1.1476, acc-0.7600, valid loss-0.9900, acc-0.7936, test loss-0.9965, acc-0.7912\n",
      "Iter-6140, train loss-0.9041, acc-0.8200, valid loss-0.9889, acc-0.7944, test loss-0.9953, acc-0.7917\n",
      "Iter-6150, train loss-0.7567, acc-0.8800, valid loss-0.9878, acc-0.7942, test loss-0.9942, acc-0.7916\n",
      "Iter-6160, train loss-0.9652, acc-0.7400, valid loss-0.9866, acc-0.7938, test loss-0.9930, acc-0.7918\n",
      "Iter-6170, train loss-1.0507, acc-0.7600, valid loss-0.9853, acc-0.7942, test loss-0.9918, acc-0.7919\n",
      "Iter-6180, train loss-0.9850, acc-0.7800, valid loss-0.9842, acc-0.7948, test loss-0.9907, acc-0.7923\n",
      "Iter-6190, train loss-0.9215, acc-0.8400, valid loss-0.9831, acc-0.7952, test loss-0.9896, acc-0.7926\n",
      "Iter-6200, train loss-1.0908, acc-0.7600, valid loss-0.9819, acc-0.7954, test loss-0.9883, acc-0.7929\n",
      "Iter-6210, train loss-0.9821, acc-0.7600, valid loss-0.9808, acc-0.7960, test loss-0.9872, acc-0.7933\n",
      "Iter-6220, train loss-0.9191, acc-0.8800, valid loss-0.9796, acc-0.7958, test loss-0.9861, acc-0.7938\n",
      "Iter-6230, train loss-0.8659, acc-0.7400, valid loss-0.9785, acc-0.7960, test loss-0.9849, acc-0.7938\n",
      "Iter-6240, train loss-0.9345, acc-0.7600, valid loss-0.9773, acc-0.7958, test loss-0.9837, acc-0.7935\n",
      "Iter-6250, train loss-1.1494, acc-0.7600, valid loss-0.9761, acc-0.7958, test loss-0.9825, acc-0.7938\n",
      "Iter-6260, train loss-0.9444, acc-0.7800, valid loss-0.9748, acc-0.7958, test loss-0.9813, acc-0.7941\n",
      "Iter-6270, train loss-0.9747, acc-0.8200, valid loss-0.9737, acc-0.7964, test loss-0.9802, acc-0.7946\n",
      "Iter-6280, train loss-0.9644, acc-0.7600, valid loss-0.9725, acc-0.7962, test loss-0.9791, acc-0.7954\n",
      "Iter-6290, train loss-0.9368, acc-0.8600, valid loss-0.9713, acc-0.7968, test loss-0.9779, acc-0.7958\n",
      "Iter-6300, train loss-0.9825, acc-0.7800, valid loss-0.9703, acc-0.7974, test loss-0.9768, acc-0.7954\n",
      "Iter-6310, train loss-1.1247, acc-0.7200, valid loss-0.9692, acc-0.7972, test loss-0.9757, acc-0.7956\n",
      "Iter-6320, train loss-0.8842, acc-0.8600, valid loss-0.9681, acc-0.7974, test loss-0.9745, acc-0.7958\n",
      "Iter-6330, train loss-1.0222, acc-0.7400, valid loss-0.9669, acc-0.7972, test loss-0.9734, acc-0.7961\n",
      "Iter-6340, train loss-0.8172, acc-0.8400, valid loss-0.9658, acc-0.7976, test loss-0.9723, acc-0.7958\n",
      "Iter-6350, train loss-0.8300, acc-0.9000, valid loss-0.9647, acc-0.7978, test loss-0.9711, acc-0.7960\n",
      "Iter-6360, train loss-1.0370, acc-0.8400, valid loss-0.9636, acc-0.7982, test loss-0.9699, acc-0.7962\n",
      "Iter-6370, train loss-0.8884, acc-0.8200, valid loss-0.9625, acc-0.7990, test loss-0.9688, acc-0.7966\n",
      "Iter-6380, train loss-0.9088, acc-0.7800, valid loss-0.9614, acc-0.7990, test loss-0.9677, acc-0.7968\n",
      "Iter-6390, train loss-0.8618, acc-0.8800, valid loss-0.9602, acc-0.8000, test loss-0.9666, acc-0.7972\n",
      "Iter-6400, train loss-0.9218, acc-0.8800, valid loss-0.9591, acc-0.8000, test loss-0.9655, acc-0.7976\n",
      "Iter-6410, train loss-0.8861, acc-0.8200, valid loss-0.9580, acc-0.8004, test loss-0.9644, acc-0.7975\n",
      "Iter-6420, train loss-0.9427, acc-0.7600, valid loss-0.9569, acc-0.8008, test loss-0.9633, acc-0.7979\n",
      "Iter-6430, train loss-0.9885, acc-0.7400, valid loss-0.9558, acc-0.8010, test loss-0.9622, acc-0.7982\n",
      "Iter-6440, train loss-0.8835, acc-0.8800, valid loss-0.9547, acc-0.8014, test loss-0.9610, acc-0.7983\n",
      "Iter-6450, train loss-0.8688, acc-0.8800, valid loss-0.9536, acc-0.8016, test loss-0.9599, acc-0.7989\n",
      "Iter-6460, train loss-0.9777, acc-0.7800, valid loss-0.9525, acc-0.8022, test loss-0.9589, acc-0.7994\n",
      "Iter-6470, train loss-1.0551, acc-0.7800, valid loss-0.9515, acc-0.8024, test loss-0.9578, acc-0.7997\n",
      "Iter-6480, train loss-0.9478, acc-0.7600, valid loss-0.9504, acc-0.8026, test loss-0.9567, acc-0.7998\n",
      "Iter-6490, train loss-0.8027, acc-0.8600, valid loss-0.9493, acc-0.8030, test loss-0.9557, acc-0.7999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-6500, train loss-1.0165, acc-0.7600, valid loss-0.9483, acc-0.8026, test loss-0.9546, acc-0.8000\n",
      "Iter-6510, train loss-0.8835, acc-0.8200, valid loss-0.9472, acc-0.8038, test loss-0.9536, acc-0.7999\n",
      "Iter-6520, train loss-0.9566, acc-0.8000, valid loss-0.9462, acc-0.8042, test loss-0.9525, acc-0.8006\n",
      "Iter-6530, train loss-0.9519, acc-0.8200, valid loss-0.9451, acc-0.8042, test loss-0.9515, acc-0.8006\n",
      "Iter-6540, train loss-1.1157, acc-0.7800, valid loss-0.9441, acc-0.8032, test loss-0.9504, acc-0.8008\n",
      "Iter-6550, train loss-1.0606, acc-0.8200, valid loss-0.9431, acc-0.8038, test loss-0.9494, acc-0.8014\n",
      "Iter-6560, train loss-0.8909, acc-0.8000, valid loss-0.9420, acc-0.8042, test loss-0.9483, acc-0.8013\n",
      "Iter-6570, train loss-0.8424, acc-0.9000, valid loss-0.9409, acc-0.8044, test loss-0.9472, acc-0.8016\n",
      "Iter-6580, train loss-0.9992, acc-0.8400, valid loss-0.9399, acc-0.8044, test loss-0.9462, acc-0.8019\n",
      "Iter-6590, train loss-0.9252, acc-0.7800, valid loss-0.9389, acc-0.8048, test loss-0.9451, acc-0.8019\n",
      "Iter-6600, train loss-0.9128, acc-0.8200, valid loss-0.9378, acc-0.8048, test loss-0.9441, acc-0.8021\n",
      "Iter-6610, train loss-0.8858, acc-0.8600, valid loss-0.9367, acc-0.8050, test loss-0.9430, acc-0.8019\n",
      "Iter-6620, train loss-0.9269, acc-0.7600, valid loss-0.9357, acc-0.8056, test loss-0.9419, acc-0.8023\n",
      "Iter-6630, train loss-0.9827, acc-0.7800, valid loss-0.9346, acc-0.8056, test loss-0.9409, acc-0.8028\n",
      "Iter-6640, train loss-0.8448, acc-0.8400, valid loss-0.9336, acc-0.8054, test loss-0.9398, acc-0.8028\n",
      "Iter-6650, train loss-0.8701, acc-0.8200, valid loss-0.9325, acc-0.8050, test loss-0.9388, acc-0.8031\n",
      "Iter-6660, train loss-0.8872, acc-0.8400, valid loss-0.9315, acc-0.8054, test loss-0.9378, acc-0.8034\n",
      "Iter-6670, train loss-0.9114, acc-0.8200, valid loss-0.9305, acc-0.8052, test loss-0.9368, acc-0.8039\n",
      "Iter-6680, train loss-1.1086, acc-0.7000, valid loss-0.9295, acc-0.8058, test loss-0.9358, acc-0.8039\n",
      "Iter-6690, train loss-0.8878, acc-0.8400, valid loss-0.9284, acc-0.8064, test loss-0.9348, acc-0.8038\n",
      "Iter-6700, train loss-1.0547, acc-0.8000, valid loss-0.9273, acc-0.8066, test loss-0.9337, acc-0.8044\n",
      "Iter-6710, train loss-0.9246, acc-0.7800, valid loss-0.9263, acc-0.8064, test loss-0.9327, acc-0.8045\n",
      "Iter-6720, train loss-0.9994, acc-0.7600, valid loss-0.9253, acc-0.8068, test loss-0.9317, acc-0.8046\n",
      "Iter-6730, train loss-1.0106, acc-0.7800, valid loss-0.9243, acc-0.8070, test loss-0.9307, acc-0.8049\n",
      "Iter-6740, train loss-0.8448, acc-0.8400, valid loss-0.9233, acc-0.8072, test loss-0.9298, acc-0.8048\n",
      "Iter-6750, train loss-0.9184, acc-0.7800, valid loss-0.9223, acc-0.8076, test loss-0.9288, acc-0.8052\n",
      "Iter-6760, train loss-0.8272, acc-0.8600, valid loss-0.9212, acc-0.8078, test loss-0.9277, acc-0.8054\n",
      "Iter-6770, train loss-1.0421, acc-0.7600, valid loss-0.9202, acc-0.8082, test loss-0.9267, acc-0.8056\n",
      "Iter-6780, train loss-1.0757, acc-0.7200, valid loss-0.9192, acc-0.8084, test loss-0.9257, acc-0.8058\n",
      "Iter-6790, train loss-0.9304, acc-0.8400, valid loss-0.9182, acc-0.8094, test loss-0.9247, acc-0.8058\n",
      "Iter-6800, train loss-0.8864, acc-0.8200, valid loss-0.9172, acc-0.8104, test loss-0.9238, acc-0.8064\n",
      "Iter-6810, train loss-1.0845, acc-0.8200, valid loss-0.9162, acc-0.8106, test loss-0.9228, acc-0.8066\n",
      "Iter-6820, train loss-1.0126, acc-0.7600, valid loss-0.9152, acc-0.8106, test loss-0.9218, acc-0.8064\n",
      "Iter-6830, train loss-0.9616, acc-0.7400, valid loss-0.9142, acc-0.8102, test loss-0.9209, acc-0.8066\n",
      "Iter-6840, train loss-0.8974, acc-0.8200, valid loss-0.9132, acc-0.8110, test loss-0.9199, acc-0.8067\n",
      "Iter-6850, train loss-1.0068, acc-0.7400, valid loss-0.9122, acc-0.8110, test loss-0.9189, acc-0.8067\n",
      "Iter-6860, train loss-0.7897, acc-0.8400, valid loss-0.9111, acc-0.8110, test loss-0.9179, acc-0.8073\n",
      "Iter-6870, train loss-0.9127, acc-0.8200, valid loss-0.9101, acc-0.8110, test loss-0.9169, acc-0.8075\n",
      "Iter-6880, train loss-0.9972, acc-0.7400, valid loss-0.9092, acc-0.8116, test loss-0.9159, acc-0.8075\n",
      "Iter-6890, train loss-0.8324, acc-0.8800, valid loss-0.9082, acc-0.8116, test loss-0.9149, acc-0.8075\n",
      "Iter-6900, train loss-0.9576, acc-0.7400, valid loss-0.9072, acc-0.8118, test loss-0.9139, acc-0.8078\n",
      "Iter-6910, train loss-0.7596, acc-0.9400, valid loss-0.9062, acc-0.8118, test loss-0.9129, acc-0.8083\n",
      "Iter-6920, train loss-0.9570, acc-0.7800, valid loss-0.9052, acc-0.8122, test loss-0.9119, acc-0.8081\n",
      "Iter-6930, train loss-0.9119, acc-0.8200, valid loss-0.9043, acc-0.8126, test loss-0.9109, acc-0.8083\n",
      "Iter-6940, train loss-0.9996, acc-0.7200, valid loss-0.9033, acc-0.8128, test loss-0.9100, acc-0.8089\n",
      "Iter-6950, train loss-0.9959, acc-0.7800, valid loss-0.9023, acc-0.8126, test loss-0.9090, acc-0.8097\n",
      "Iter-6960, train loss-0.9611, acc-0.7600, valid loss-0.9013, acc-0.8128, test loss-0.9080, acc-0.8101\n",
      "Iter-6970, train loss-0.9271, acc-0.8400, valid loss-0.9004, acc-0.8130, test loss-0.9070, acc-0.8095\n",
      "Iter-6980, train loss-0.9249, acc-0.7600, valid loss-0.8994, acc-0.8132, test loss-0.9061, acc-0.8099\n",
      "Iter-6990, train loss-0.9037, acc-0.7600, valid loss-0.8985, acc-0.8134, test loss-0.9051, acc-0.8104\n",
      "Iter-7000, train loss-0.8559, acc-0.8400, valid loss-0.8975, acc-0.8134, test loss-0.9041, acc-0.8108\n",
      "Iter-7010, train loss-1.0300, acc-0.7200, valid loss-0.8966, acc-0.8138, test loss-0.9032, acc-0.8107\n",
      "Iter-7020, train loss-1.0122, acc-0.7600, valid loss-0.8956, acc-0.8138, test loss-0.9023, acc-0.8109\n",
      "Iter-7030, train loss-0.8383, acc-0.8800, valid loss-0.8946, acc-0.8134, test loss-0.9014, acc-0.8118\n",
      "Iter-7040, train loss-0.8347, acc-0.8400, valid loss-0.8937, acc-0.8136, test loss-0.9005, acc-0.8118\n",
      "Iter-7050, train loss-0.8415, acc-0.9000, valid loss-0.8927, acc-0.8138, test loss-0.8995, acc-0.8121\n",
      "Iter-7060, train loss-0.8512, acc-0.8200, valid loss-0.8918, acc-0.8140, test loss-0.8986, acc-0.8123\n",
      "Iter-7070, train loss-1.0353, acc-0.7400, valid loss-0.8909, acc-0.8144, test loss-0.8977, acc-0.8123\n",
      "Iter-7080, train loss-0.8642, acc-0.8600, valid loss-0.8900, acc-0.8146, test loss-0.8968, acc-0.8127\n",
      "Iter-7090, train loss-0.9960, acc-0.7400, valid loss-0.8891, acc-0.8148, test loss-0.8959, acc-0.8128\n",
      "Iter-7100, train loss-0.8702, acc-0.8200, valid loss-0.8882, acc-0.8146, test loss-0.8949, acc-0.8132\n",
      "Iter-7110, train loss-0.9540, acc-0.7400, valid loss-0.8873, acc-0.8146, test loss-0.8940, acc-0.8137\n",
      "Iter-7120, train loss-0.9281, acc-0.7600, valid loss-0.8863, acc-0.8156, test loss-0.8931, acc-0.8137\n",
      "Iter-7130, train loss-0.8669, acc-0.8000, valid loss-0.8854, acc-0.8158, test loss-0.8922, acc-0.8138\n",
      "Iter-7140, train loss-0.9961, acc-0.7800, valid loss-0.8845, acc-0.8164, test loss-0.8913, acc-0.8140\n",
      "Iter-7150, train loss-0.9248, acc-0.7200, valid loss-0.8836, acc-0.8162, test loss-0.8904, acc-0.8138\n",
      "Iter-7160, train loss-0.9362, acc-0.7800, valid loss-0.8826, acc-0.8162, test loss-0.8895, acc-0.8139\n",
      "Iter-7170, train loss-0.9993, acc-0.7200, valid loss-0.8817, acc-0.8164, test loss-0.8886, acc-0.8140\n",
      "Iter-7180, train loss-0.8663, acc-0.8000, valid loss-0.8808, acc-0.8164, test loss-0.8877, acc-0.8142\n",
      "Iter-7190, train loss-0.7713, acc-0.8800, valid loss-0.8798, acc-0.8168, test loss-0.8867, acc-0.8143\n",
      "Iter-7200, train loss-0.9309, acc-0.7600, valid loss-0.8789, acc-0.8176, test loss-0.8858, acc-0.8147\n",
      "Iter-7210, train loss-0.9677, acc-0.7400, valid loss-0.8780, acc-0.8172, test loss-0.8849, acc-0.8149\n",
      "Iter-7220, train loss-0.9618, acc-0.7200, valid loss-0.8771, acc-0.8176, test loss-0.8840, acc-0.8150\n",
      "Iter-7230, train loss-0.9221, acc-0.7800, valid loss-0.8762, acc-0.8172, test loss-0.8830, acc-0.8151\n",
      "Iter-7240, train loss-0.9356, acc-0.8000, valid loss-0.8753, acc-0.8176, test loss-0.8822, acc-0.8152\n",
      "Iter-7250, train loss-0.8537, acc-0.9000, valid loss-0.8744, acc-0.8180, test loss-0.8813, acc-0.8148\n",
      "Iter-7260, train loss-1.0127, acc-0.7800, valid loss-0.8735, acc-0.8180, test loss-0.8804, acc-0.8156\n",
      "Iter-7270, train loss-0.9303, acc-0.8200, valid loss-0.8725, acc-0.8184, test loss-0.8795, acc-0.8159\n",
      "Iter-7280, train loss-0.8201, acc-0.8400, valid loss-0.8717, acc-0.8186, test loss-0.8786, acc-0.8165\n",
      "Iter-7290, train loss-0.7602, acc-0.8800, valid loss-0.8708, acc-0.8188, test loss-0.8777, acc-0.8163\n",
      "Iter-7300, train loss-0.9436, acc-0.7600, valid loss-0.8699, acc-0.8188, test loss-0.8768, acc-0.8162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-7310, train loss-0.8253, acc-0.8600, valid loss-0.8689, acc-0.8192, test loss-0.8759, acc-0.8168\n",
      "Iter-7320, train loss-0.9199, acc-0.7600, valid loss-0.8680, acc-0.8194, test loss-0.8749, acc-0.8170\n",
      "Iter-7330, train loss-0.8710, acc-0.8200, valid loss-0.8671, acc-0.8194, test loss-0.8740, acc-0.8176\n",
      "Iter-7340, train loss-1.0477, acc-0.6800, valid loss-0.8662, acc-0.8200, test loss-0.8730, acc-0.8176\n",
      "Iter-7350, train loss-0.8088, acc-0.8000, valid loss-0.8653, acc-0.8200, test loss-0.8721, acc-0.8175\n",
      "Iter-7360, train loss-0.8233, acc-0.8400, valid loss-0.8645, acc-0.8198, test loss-0.8713, acc-0.8174\n",
      "Iter-7370, train loss-0.9900, acc-0.7000, valid loss-0.8635, acc-0.8198, test loss-0.8704, acc-0.8177\n",
      "Iter-7380, train loss-0.9129, acc-0.7400, valid loss-0.8627, acc-0.8200, test loss-0.8695, acc-0.8182\n",
      "Iter-7390, train loss-1.0709, acc-0.7200, valid loss-0.8619, acc-0.8202, test loss-0.8687, acc-0.8183\n",
      "Iter-7400, train loss-0.8595, acc-0.8600, valid loss-0.8610, acc-0.8200, test loss-0.8678, acc-0.8177\n",
      "Iter-7410, train loss-0.9652, acc-0.8600, valid loss-0.8601, acc-0.8204, test loss-0.8670, acc-0.8181\n",
      "Iter-7420, train loss-0.9017, acc-0.8000, valid loss-0.8592, acc-0.8210, test loss-0.8661, acc-0.8183\n",
      "Iter-7430, train loss-0.8851, acc-0.8600, valid loss-0.8583, acc-0.8210, test loss-0.8652, acc-0.8187\n",
      "Iter-7440, train loss-0.7321, acc-0.9600, valid loss-0.8575, acc-0.8212, test loss-0.8644, acc-0.8191\n",
      "Iter-7450, train loss-0.8695, acc-0.8400, valid loss-0.8566, acc-0.8212, test loss-0.8635, acc-0.8189\n",
      "Iter-7460, train loss-0.9159, acc-0.8000, valid loss-0.8557, acc-0.8216, test loss-0.8626, acc-0.8191\n",
      "Iter-7470, train loss-0.7595, acc-0.7800, valid loss-0.8549, acc-0.8218, test loss-0.8618, acc-0.8190\n",
      "Iter-7480, train loss-0.8238, acc-0.8200, valid loss-0.8540, acc-0.8222, test loss-0.8609, acc-0.8190\n",
      "Iter-7490, train loss-0.8611, acc-0.8400, valid loss-0.8531, acc-0.8218, test loss-0.8600, acc-0.8195\n",
      "Iter-7500, train loss-0.7751, acc-0.8400, valid loss-0.8523, acc-0.8224, test loss-0.8592, acc-0.8199\n",
      "Iter-7510, train loss-0.7299, acc-0.8800, valid loss-0.8515, acc-0.8224, test loss-0.8584, acc-0.8195\n",
      "Iter-7520, train loss-0.9519, acc-0.8200, valid loss-0.8506, acc-0.8222, test loss-0.8574, acc-0.8195\n",
      "Iter-7530, train loss-0.9661, acc-0.7800, valid loss-0.8497, acc-0.8226, test loss-0.8565, acc-0.8200\n",
      "Iter-7540, train loss-0.7538, acc-0.8600, valid loss-0.8489, acc-0.8226, test loss-0.8557, acc-0.8202\n",
      "Iter-7550, train loss-0.8839, acc-0.7600, valid loss-0.8480, acc-0.8224, test loss-0.8548, acc-0.8209\n",
      "Iter-7560, train loss-0.8072, acc-0.8400, valid loss-0.8472, acc-0.8222, test loss-0.8540, acc-0.8213\n",
      "Iter-7570, train loss-0.9669, acc-0.7400, valid loss-0.8463, acc-0.8224, test loss-0.8532, acc-0.8214\n",
      "Iter-7580, train loss-0.8880, acc-0.7600, valid loss-0.8455, acc-0.8230, test loss-0.8524, acc-0.8219\n",
      "Iter-7590, train loss-0.9563, acc-0.8000, valid loss-0.8447, acc-0.8232, test loss-0.8515, acc-0.8223\n",
      "Iter-7600, train loss-0.8503, acc-0.8200, valid loss-0.8439, acc-0.8234, test loss-0.8506, acc-0.8226\n",
      "Iter-7610, train loss-0.8963, acc-0.8600, valid loss-0.8430, acc-0.8238, test loss-0.8498, acc-0.8225\n",
      "Iter-7620, train loss-0.7404, acc-0.9400, valid loss-0.8422, acc-0.8238, test loss-0.8490, acc-0.8226\n",
      "Iter-7630, train loss-0.9333, acc-0.7400, valid loss-0.8414, acc-0.8240, test loss-0.8481, acc-0.8226\n",
      "Iter-7640, train loss-0.9742, acc-0.7800, valid loss-0.8406, acc-0.8242, test loss-0.8473, acc-0.8227\n",
      "Iter-7650, train loss-0.8715, acc-0.8400, valid loss-0.8397, acc-0.8244, test loss-0.8465, acc-0.8228\n",
      "Iter-7660, train loss-0.7199, acc-0.9000, valid loss-0.8389, acc-0.8248, test loss-0.8456, acc-0.8233\n",
      "Iter-7670, train loss-0.8164, acc-0.8000, valid loss-0.8380, acc-0.8250, test loss-0.8448, acc-0.8232\n",
      "Iter-7680, train loss-1.0080, acc-0.6400, valid loss-0.8372, acc-0.8250, test loss-0.8440, acc-0.8234\n",
      "Iter-7690, train loss-0.7922, acc-0.8800, valid loss-0.8365, acc-0.8252, test loss-0.8432, acc-0.8238\n",
      "Iter-7700, train loss-0.9129, acc-0.7600, valid loss-0.8356, acc-0.8252, test loss-0.8424, acc-0.8241\n",
      "Iter-7710, train loss-0.7107, acc-0.9000, valid loss-0.8349, acc-0.8258, test loss-0.8416, acc-0.8241\n",
      "Iter-7720, train loss-0.8172, acc-0.7600, valid loss-0.8341, acc-0.8258, test loss-0.8408, acc-0.8245\n",
      "Iter-7730, train loss-0.7800, acc-0.8800, valid loss-0.8332, acc-0.8256, test loss-0.8400, acc-0.8242\n",
      "Iter-7740, train loss-1.1160, acc-0.7000, valid loss-0.8324, acc-0.8260, test loss-0.8392, acc-0.8247\n",
      "Iter-7750, train loss-0.8789, acc-0.7200, valid loss-0.8316, acc-0.8260, test loss-0.8384, acc-0.8252\n",
      "Iter-7760, train loss-0.7817, acc-0.8600, valid loss-0.8308, acc-0.8266, test loss-0.8376, acc-0.8253\n",
      "Iter-7770, train loss-0.8102, acc-0.8600, valid loss-0.8300, acc-0.8264, test loss-0.8368, acc-0.8250\n",
      "Iter-7780, train loss-0.8783, acc-0.8000, valid loss-0.8292, acc-0.8262, test loss-0.8360, acc-0.8250\n",
      "Iter-7790, train loss-0.9026, acc-0.8200, valid loss-0.8285, acc-0.8268, test loss-0.8352, acc-0.8253\n",
      "Iter-7800, train loss-0.9527, acc-0.7600, valid loss-0.8277, acc-0.8268, test loss-0.8344, acc-0.8255\n",
      "Iter-7810, train loss-0.7300, acc-0.8400, valid loss-0.8270, acc-0.8266, test loss-0.8336, acc-0.8257\n",
      "Iter-7820, train loss-0.9171, acc-0.8200, valid loss-0.8262, acc-0.8270, test loss-0.8328, acc-0.8260\n",
      "Iter-7830, train loss-0.8304, acc-0.7800, valid loss-0.8254, acc-0.8264, test loss-0.8320, acc-0.8260\n",
      "Iter-7840, train loss-0.9433, acc-0.7600, valid loss-0.8246, acc-0.8266, test loss-0.8312, acc-0.8261\n",
      "Iter-7850, train loss-0.7847, acc-0.8800, valid loss-0.8239, acc-0.8268, test loss-0.8304, acc-0.8260\n",
      "Iter-7860, train loss-0.7682, acc-0.8600, valid loss-0.8230, acc-0.8270, test loss-0.8297, acc-0.8264\n",
      "Iter-7870, train loss-0.6745, acc-0.9200, valid loss-0.8222, acc-0.8264, test loss-0.8288, acc-0.8264\n",
      "Iter-7880, train loss-0.8231, acc-0.8400, valid loss-0.8214, acc-0.8270, test loss-0.8281, acc-0.8268\n",
      "Iter-7890, train loss-0.7460, acc-0.8600, valid loss-0.8206, acc-0.8264, test loss-0.8272, acc-0.8271\n",
      "Iter-7900, train loss-0.8944, acc-0.8200, valid loss-0.8198, acc-0.8268, test loss-0.8265, acc-0.8269\n",
      "Iter-7910, train loss-0.8147, acc-0.8600, valid loss-0.8191, acc-0.8270, test loss-0.8257, acc-0.8272\n",
      "Iter-7920, train loss-0.8740, acc-0.8200, valid loss-0.8183, acc-0.8272, test loss-0.8249, acc-0.8272\n",
      "Iter-7930, train loss-0.8690, acc-0.8400, valid loss-0.8175, acc-0.8276, test loss-0.8241, acc-0.8273\n",
      "Iter-7940, train loss-0.7770, acc-0.8600, valid loss-0.8166, acc-0.8280, test loss-0.8233, acc-0.8276\n",
      "Iter-7950, train loss-0.8687, acc-0.8200, valid loss-0.8159, acc-0.8280, test loss-0.8225, acc-0.8280\n",
      "Iter-7960, train loss-0.7292, acc-0.8600, valid loss-0.8151, acc-0.8278, test loss-0.8218, acc-0.8278\n",
      "Iter-7970, train loss-0.9424, acc-0.7400, valid loss-0.8144, acc-0.8282, test loss-0.8211, acc-0.8280\n",
      "Iter-7980, train loss-0.7839, acc-0.8400, valid loss-0.8136, acc-0.8282, test loss-0.8203, acc-0.8284\n",
      "Iter-7990, train loss-0.9335, acc-0.7800, valid loss-0.8128, acc-0.8284, test loss-0.8196, acc-0.8283\n",
      "Iter-8000, train loss-0.9015, acc-0.7600, valid loss-0.8121, acc-0.8284, test loss-0.8189, acc-0.8286\n",
      "Iter-8010, train loss-0.7409, acc-0.9000, valid loss-0.8113, acc-0.8288, test loss-0.8180, acc-0.8290\n",
      "Iter-8020, train loss-0.8295, acc-0.8200, valid loss-0.8106, acc-0.8290, test loss-0.8173, acc-0.8290\n",
      "Iter-8030, train loss-0.8069, acc-0.8000, valid loss-0.8098, acc-0.8294, test loss-0.8165, acc-0.8292\n",
      "Iter-8040, train loss-0.7852, acc-0.7800, valid loss-0.8090, acc-0.8290, test loss-0.8158, acc-0.8294\n",
      "Iter-8050, train loss-0.7897, acc-0.8400, valid loss-0.8083, acc-0.8296, test loss-0.8150, acc-0.8293\n",
      "Iter-8060, train loss-1.0104, acc-0.7200, valid loss-0.8075, acc-0.8300, test loss-0.8142, acc-0.8297\n",
      "Iter-8070, train loss-0.8286, acc-0.8400, valid loss-0.8068, acc-0.8298, test loss-0.8135, acc-0.8297\n",
      "Iter-8080, train loss-0.9901, acc-0.8200, valid loss-0.8060, acc-0.8298, test loss-0.8127, acc-0.8303\n",
      "Iter-8090, train loss-0.7871, acc-0.8400, valid loss-0.8053, acc-0.8304, test loss-0.8120, acc-0.8300\n",
      "Iter-8100, train loss-0.7730, acc-0.8800, valid loss-0.8047, acc-0.8296, test loss-0.8113, acc-0.8298\n",
      "Iter-8110, train loss-0.7441, acc-0.8400, valid loss-0.8039, acc-0.8302, test loss-0.8106, acc-0.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8120, train loss-0.6728, acc-0.9400, valid loss-0.8031, acc-0.8308, test loss-0.8099, acc-0.8304\n",
      "Iter-8130, train loss-0.8874, acc-0.8200, valid loss-0.8024, acc-0.8304, test loss-0.8091, acc-0.8304\n",
      "Iter-8140, train loss-1.0689, acc-0.6800, valid loss-0.8017, acc-0.8310, test loss-0.8084, acc-0.8307\n",
      "Iter-8150, train loss-0.8661, acc-0.8200, valid loss-0.8009, acc-0.8314, test loss-0.8076, acc-0.8310\n",
      "Iter-8160, train loss-0.9360, acc-0.7800, valid loss-0.8001, acc-0.8320, test loss-0.8068, acc-0.8312\n",
      "Iter-8170, train loss-0.7775, acc-0.8600, valid loss-0.7994, acc-0.8324, test loss-0.8061, acc-0.8314\n",
      "Iter-8180, train loss-0.6585, acc-0.9200, valid loss-0.7988, acc-0.8328, test loss-0.8053, acc-0.8318\n",
      "Iter-8190, train loss-0.9286, acc-0.7000, valid loss-0.7981, acc-0.8334, test loss-0.8045, acc-0.8318\n",
      "Iter-8200, train loss-0.7872, acc-0.8200, valid loss-0.7974, acc-0.8336, test loss-0.8038, acc-0.8323\n",
      "Iter-8210, train loss-0.8931, acc-0.7600, valid loss-0.7967, acc-0.8334, test loss-0.8030, acc-0.8324\n",
      "Iter-8220, train loss-0.8093, acc-0.8200, valid loss-0.7960, acc-0.8336, test loss-0.8023, acc-0.8327\n",
      "Iter-8230, train loss-0.7751, acc-0.7800, valid loss-0.7953, acc-0.8340, test loss-0.8016, acc-0.8328\n",
      "Iter-8240, train loss-0.7991, acc-0.8400, valid loss-0.7946, acc-0.8348, test loss-0.8009, acc-0.8329\n",
      "Iter-8250, train loss-0.7570, acc-0.8200, valid loss-0.7939, acc-0.8344, test loss-0.8002, acc-0.8329\n",
      "Iter-8260, train loss-0.6606, acc-0.8800, valid loss-0.7932, acc-0.8346, test loss-0.7995, acc-0.8326\n",
      "Iter-8270, train loss-0.7561, acc-0.8000, valid loss-0.7924, acc-0.8354, test loss-0.7988, acc-0.8325\n",
      "Iter-8280, train loss-0.6880, acc-0.8800, valid loss-0.7917, acc-0.8350, test loss-0.7981, acc-0.8329\n",
      "Iter-8290, train loss-0.7723, acc-0.8200, valid loss-0.7910, acc-0.8354, test loss-0.7974, acc-0.8329\n",
      "Iter-8300, train loss-0.7267, acc-0.8400, valid loss-0.7904, acc-0.8346, test loss-0.7967, acc-0.8336\n",
      "Iter-8310, train loss-0.7602, acc-0.8800, valid loss-0.7897, acc-0.8348, test loss-0.7960, acc-0.8336\n",
      "Iter-8320, train loss-0.8121, acc-0.7400, valid loss-0.7890, acc-0.8354, test loss-0.7953, acc-0.8340\n",
      "Iter-8330, train loss-0.8383, acc-0.8600, valid loss-0.7883, acc-0.8356, test loss-0.7946, acc-0.8341\n",
      "Iter-8340, train loss-0.8435, acc-0.8200, valid loss-0.7876, acc-0.8358, test loss-0.7940, acc-0.8343\n",
      "Iter-8350, train loss-0.8215, acc-0.7600, valid loss-0.7868, acc-0.8364, test loss-0.7933, acc-0.8342\n",
      "Iter-8360, train loss-0.6816, acc-0.9000, valid loss-0.7861, acc-0.8366, test loss-0.7926, acc-0.8341\n",
      "Iter-8370, train loss-1.0775, acc-0.6800, valid loss-0.7854, acc-0.8362, test loss-0.7919, acc-0.8343\n",
      "Iter-8380, train loss-0.8326, acc-0.7400, valid loss-0.7847, acc-0.8360, test loss-0.7913, acc-0.8343\n",
      "Iter-8390, train loss-0.7966, acc-0.8600, valid loss-0.7840, acc-0.8356, test loss-0.7905, acc-0.8347\n",
      "Iter-8400, train loss-0.9421, acc-0.7800, valid loss-0.7833, acc-0.8356, test loss-0.7899, acc-0.8346\n",
      "Iter-8410, train loss-0.8144, acc-0.8000, valid loss-0.7825, acc-0.8362, test loss-0.7892, acc-0.8347\n",
      "Iter-8420, train loss-0.7238, acc-0.8600, valid loss-0.7818, acc-0.8362, test loss-0.7884, acc-0.8345\n",
      "Iter-8430, train loss-0.6959, acc-0.8600, valid loss-0.7811, acc-0.8360, test loss-0.7878, acc-0.8345\n",
      "Iter-8440, train loss-0.9295, acc-0.7600, valid loss-0.7804, acc-0.8366, test loss-0.7871, acc-0.8349\n",
      "Iter-8450, train loss-0.8851, acc-0.7600, valid loss-0.7796, acc-0.8372, test loss-0.7864, acc-0.8351\n",
      "Iter-8460, train loss-0.9125, acc-0.7000, valid loss-0.7789, acc-0.8380, test loss-0.7857, acc-0.8352\n",
      "Iter-8470, train loss-0.9463, acc-0.7400, valid loss-0.7782, acc-0.8380, test loss-0.7851, acc-0.8353\n",
      "Iter-8480, train loss-0.8364, acc-0.8000, valid loss-0.7776, acc-0.8380, test loss-0.7844, acc-0.8350\n",
      "Iter-8490, train loss-0.7233, acc-0.8400, valid loss-0.7768, acc-0.8380, test loss-0.7837, acc-0.8351\n",
      "Iter-8500, train loss-0.7760, acc-0.8600, valid loss-0.7762, acc-0.8384, test loss-0.7830, acc-0.8351\n",
      "Iter-8510, train loss-0.8008, acc-0.8000, valid loss-0.7755, acc-0.8386, test loss-0.7823, acc-0.8348\n",
      "Iter-8520, train loss-0.7914, acc-0.7600, valid loss-0.7748, acc-0.8388, test loss-0.7816, acc-0.8349\n",
      "Iter-8530, train loss-0.7245, acc-0.8000, valid loss-0.7742, acc-0.8386, test loss-0.7809, acc-0.8347\n",
      "Iter-8540, train loss-0.7925, acc-0.8000, valid loss-0.7734, acc-0.8388, test loss-0.7802, acc-0.8349\n",
      "Iter-8550, train loss-0.7817, acc-0.8200, valid loss-0.7727, acc-0.8386, test loss-0.7796, acc-0.8352\n",
      "Iter-8560, train loss-0.6625, acc-0.8600, valid loss-0.7720, acc-0.8394, test loss-0.7789, acc-0.8352\n",
      "Iter-8570, train loss-0.8330, acc-0.8600, valid loss-0.7714, acc-0.8396, test loss-0.7783, acc-0.8352\n",
      "Iter-8580, train loss-0.7159, acc-0.8800, valid loss-0.7707, acc-0.8404, test loss-0.7776, acc-0.8353\n",
      "Iter-8590, train loss-0.7680, acc-0.8400, valid loss-0.7700, acc-0.8400, test loss-0.7769, acc-0.8353\n",
      "Iter-8600, train loss-0.7392, acc-0.8400, valid loss-0.7693, acc-0.8400, test loss-0.7762, acc-0.8355\n",
      "Iter-8610, train loss-0.7833, acc-0.8400, valid loss-0.7687, acc-0.8400, test loss-0.7755, acc-0.8360\n",
      "Iter-8620, train loss-0.8588, acc-0.6800, valid loss-0.7680, acc-0.8406, test loss-0.7748, acc-0.8362\n",
      "Iter-8630, train loss-0.7295, acc-0.8200, valid loss-0.7673, acc-0.8406, test loss-0.7741, acc-0.8366\n",
      "Iter-8640, train loss-0.7588, acc-0.7800, valid loss-0.7667, acc-0.8408, test loss-0.7735, acc-0.8368\n",
      "Iter-8650, train loss-0.8453, acc-0.8000, valid loss-0.7659, acc-0.8412, test loss-0.7728, acc-0.8368\n",
      "Iter-8660, train loss-0.7537, acc-0.8200, valid loss-0.7653, acc-0.8412, test loss-0.7721, acc-0.8366\n",
      "Iter-8670, train loss-0.7840, acc-0.8200, valid loss-0.7646, acc-0.8416, test loss-0.7714, acc-0.8367\n",
      "Iter-8680, train loss-0.9395, acc-0.8200, valid loss-0.7640, acc-0.8412, test loss-0.7708, acc-0.8368\n",
      "Iter-8690, train loss-0.7508, acc-0.8200, valid loss-0.7633, acc-0.8414, test loss-0.7702, acc-0.8367\n",
      "Iter-8700, train loss-0.6170, acc-0.9200, valid loss-0.7626, acc-0.8416, test loss-0.7695, acc-0.8369\n",
      "Iter-8710, train loss-0.8043, acc-0.8200, valid loss-0.7620, acc-0.8416, test loss-0.7688, acc-0.8374\n",
      "Iter-8720, train loss-0.6348, acc-0.8800, valid loss-0.7613, acc-0.8422, test loss-0.7680, acc-0.8371\n",
      "Iter-8730, train loss-0.6859, acc-0.9200, valid loss-0.7607, acc-0.8420, test loss-0.7674, acc-0.8375\n",
      "Iter-8740, train loss-0.7723, acc-0.8600, valid loss-0.7600, acc-0.8420, test loss-0.7668, acc-0.8376\n",
      "Iter-8750, train loss-0.7809, acc-0.7800, valid loss-0.7593, acc-0.8420, test loss-0.7661, acc-0.8375\n",
      "Iter-8760, train loss-0.8465, acc-0.7800, valid loss-0.7586, acc-0.8422, test loss-0.7654, acc-0.8375\n",
      "Iter-8770, train loss-0.8125, acc-0.8400, valid loss-0.7580, acc-0.8420, test loss-0.7647, acc-0.8372\n",
      "Iter-8780, train loss-0.7286, acc-0.8400, valid loss-0.7574, acc-0.8424, test loss-0.7641, acc-0.8373\n",
      "Iter-8790, train loss-0.6637, acc-0.9000, valid loss-0.7567, acc-0.8422, test loss-0.7634, acc-0.8372\n",
      "Iter-8800, train loss-0.6483, acc-0.8800, valid loss-0.7561, acc-0.8422, test loss-0.7628, acc-0.8376\n",
      "Iter-8810, train loss-0.6992, acc-0.8200, valid loss-0.7554, acc-0.8420, test loss-0.7621, acc-0.8375\n",
      "Iter-8820, train loss-0.7749, acc-0.8200, valid loss-0.7548, acc-0.8422, test loss-0.7614, acc-0.8378\n",
      "Iter-8830, train loss-0.6890, acc-0.9000, valid loss-0.7541, acc-0.8422, test loss-0.7607, acc-0.8381\n",
      "Iter-8840, train loss-0.8122, acc-0.8600, valid loss-0.7535, acc-0.8430, test loss-0.7601, acc-0.8381\n",
      "Iter-8850, train loss-0.9018, acc-0.7400, valid loss-0.7528, acc-0.8432, test loss-0.7595, acc-0.8382\n",
      "Iter-8860, train loss-0.7504, acc-0.8200, valid loss-0.7522, acc-0.8428, test loss-0.7589, acc-0.8380\n",
      "Iter-8870, train loss-0.6467, acc-0.9000, valid loss-0.7515, acc-0.8426, test loss-0.7582, acc-0.8383\n",
      "Iter-8880, train loss-0.6930, acc-0.8600, valid loss-0.7509, acc-0.8434, test loss-0.7576, acc-0.8381\n",
      "Iter-8890, train loss-0.8598, acc-0.8200, valid loss-0.7503, acc-0.8430, test loss-0.7570, acc-0.8382\n",
      "Iter-8900, train loss-0.6657, acc-0.8600, valid loss-0.7496, acc-0.8432, test loss-0.7563, acc-0.8383\n",
      "Iter-8910, train loss-0.6613, acc-0.9600, valid loss-0.7489, acc-0.8434, test loss-0.7556, acc-0.8388\n",
      "Iter-8920, train loss-0.6979, acc-0.8800, valid loss-0.7484, acc-0.8436, test loss-0.7550, acc-0.8388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-8930, train loss-0.7610, acc-0.7600, valid loss-0.7478, acc-0.8434, test loss-0.7544, acc-0.8385\n",
      "Iter-8940, train loss-0.7816, acc-0.8000, valid loss-0.7471, acc-0.8440, test loss-0.7538, acc-0.8384\n",
      "Iter-8950, train loss-0.7383, acc-0.8800, valid loss-0.7466, acc-0.8438, test loss-0.7532, acc-0.8385\n",
      "Iter-8960, train loss-0.7084, acc-0.9000, valid loss-0.7459, acc-0.8438, test loss-0.7526, acc-0.8383\n",
      "Iter-8970, train loss-0.7554, acc-0.8600, valid loss-0.7452, acc-0.8440, test loss-0.7519, acc-0.8388\n",
      "Iter-8980, train loss-0.9486, acc-0.8200, valid loss-0.7446, acc-0.8442, test loss-0.7513, acc-0.8387\n",
      "Iter-8990, train loss-0.5906, acc-0.9000, valid loss-0.7440, acc-0.8440, test loss-0.7507, acc-0.8386\n",
      "Iter-9000, train loss-0.6254, acc-0.8800, valid loss-0.7434, acc-0.8442, test loss-0.7501, acc-0.8389\n",
      "Iter-9010, train loss-0.7562, acc-0.8000, valid loss-0.7427, acc-0.8442, test loss-0.7496, acc-0.8390\n",
      "Iter-9020, train loss-0.6574, acc-0.8800, valid loss-0.7422, acc-0.8444, test loss-0.7489, acc-0.8391\n",
      "Iter-9030, train loss-0.8549, acc-0.7800, valid loss-0.7415, acc-0.8448, test loss-0.7483, acc-0.8392\n",
      "Iter-9040, train loss-0.7600, acc-0.8400, valid loss-0.7409, acc-0.8448, test loss-0.7477, acc-0.8393\n",
      "Iter-9050, train loss-0.7284, acc-0.8600, valid loss-0.7403, acc-0.8446, test loss-0.7471, acc-0.8395\n",
      "Iter-9060, train loss-0.6992, acc-0.8800, valid loss-0.7397, acc-0.8450, test loss-0.7465, acc-0.8391\n",
      "Iter-9070, train loss-0.7312, acc-0.8400, valid loss-0.7391, acc-0.8450, test loss-0.7459, acc-0.8394\n",
      "Iter-9080, train loss-0.8442, acc-0.8000, valid loss-0.7385, acc-0.8450, test loss-0.7453, acc-0.8396\n",
      "Iter-9090, train loss-0.9331, acc-0.7200, valid loss-0.7379, acc-0.8462, test loss-0.7447, acc-0.8398\n",
      "Iter-9100, train loss-0.7776, acc-0.8200, valid loss-0.7373, acc-0.8456, test loss-0.7441, acc-0.8399\n",
      "Iter-9110, train loss-0.6685, acc-0.9000, valid loss-0.7367, acc-0.8454, test loss-0.7434, acc-0.8397\n",
      "Iter-9120, train loss-0.8122, acc-0.7800, valid loss-0.7361, acc-0.8454, test loss-0.7428, acc-0.8395\n",
      "Iter-9130, train loss-0.8169, acc-0.8000, valid loss-0.7355, acc-0.8456, test loss-0.7422, acc-0.8397\n",
      "Iter-9140, train loss-0.6631, acc-0.8600, valid loss-0.7349, acc-0.8466, test loss-0.7416, acc-0.8401\n",
      "Iter-9150, train loss-0.7386, acc-0.8200, valid loss-0.7343, acc-0.8470, test loss-0.7409, acc-0.8402\n",
      "Iter-9160, train loss-0.7191, acc-0.7800, valid loss-0.7337, acc-0.8470, test loss-0.7403, acc-0.8404\n",
      "Iter-9170, train loss-0.7310, acc-0.8200, valid loss-0.7330, acc-0.8474, test loss-0.7397, acc-0.8406\n",
      "Iter-9180, train loss-0.8187, acc-0.8200, valid loss-0.7324, acc-0.8474, test loss-0.7391, acc-0.8407\n",
      "Iter-9190, train loss-0.6612, acc-0.8800, valid loss-0.7318, acc-0.8474, test loss-0.7385, acc-0.8407\n",
      "Iter-9200, train loss-0.5593, acc-0.9000, valid loss-0.7312, acc-0.8478, test loss-0.7379, acc-0.8409\n",
      "Iter-9210, train loss-0.7081, acc-0.8000, valid loss-0.7306, acc-0.8478, test loss-0.7372, acc-0.8412\n",
      "Iter-9220, train loss-0.6521, acc-0.8800, valid loss-0.7300, acc-0.8478, test loss-0.7366, acc-0.8414\n",
      "Iter-9230, train loss-0.7291, acc-0.8400, valid loss-0.7295, acc-0.8488, test loss-0.7360, acc-0.8418\n",
      "Iter-9240, train loss-0.6409, acc-0.9000, valid loss-0.7290, acc-0.8484, test loss-0.7354, acc-0.8413\n",
      "Iter-9250, train loss-0.7321, acc-0.8400, valid loss-0.7284, acc-0.8484, test loss-0.7347, acc-0.8417\n",
      "Iter-9260, train loss-0.7916, acc-0.8400, valid loss-0.7278, acc-0.8480, test loss-0.7341, acc-0.8421\n",
      "Iter-9270, train loss-0.7665, acc-0.8000, valid loss-0.7272, acc-0.8482, test loss-0.7335, acc-0.8427\n",
      "Iter-9280, train loss-0.7405, acc-0.9000, valid loss-0.7266, acc-0.8486, test loss-0.7329, acc-0.8424\n",
      "Iter-9290, train loss-0.8939, acc-0.7000, valid loss-0.7260, acc-0.8494, test loss-0.7323, acc-0.8425\n",
      "Iter-9300, train loss-0.7504, acc-0.8200, valid loss-0.7254, acc-0.8496, test loss-0.7318, acc-0.8425\n",
      "Iter-9310, train loss-0.6115, acc-0.9000, valid loss-0.7248, acc-0.8494, test loss-0.7312, acc-0.8425\n",
      "Iter-9320, train loss-0.8335, acc-0.8200, valid loss-0.7242, acc-0.8496, test loss-0.7307, acc-0.8426\n",
      "Iter-9330, train loss-0.9103, acc-0.7800, valid loss-0.7236, acc-0.8494, test loss-0.7301, acc-0.8429\n",
      "Iter-9340, train loss-0.8297, acc-0.7800, valid loss-0.7230, acc-0.8498, test loss-0.7295, acc-0.8435\n",
      "Iter-9350, train loss-0.6323, acc-0.8800, valid loss-0.7225, acc-0.8504, test loss-0.7289, acc-0.8437\n",
      "Iter-9360, train loss-0.8466, acc-0.7400, valid loss-0.7219, acc-0.8504, test loss-0.7284, acc-0.8437\n",
      "Iter-9370, train loss-0.6022, acc-0.9600, valid loss-0.7213, acc-0.8506, test loss-0.7278, acc-0.8441\n",
      "Iter-9380, train loss-0.5619, acc-0.9200, valid loss-0.7207, acc-0.8506, test loss-0.7272, acc-0.8444\n",
      "Iter-9390, train loss-0.7732, acc-0.8400, valid loss-0.7202, acc-0.8508, test loss-0.7267, acc-0.8449\n",
      "Iter-9400, train loss-0.7458, acc-0.7800, valid loss-0.7195, acc-0.8516, test loss-0.7261, acc-0.8450\n",
      "Iter-9410, train loss-0.6559, acc-0.9000, valid loss-0.7189, acc-0.8518, test loss-0.7256, acc-0.8448\n",
      "Iter-9420, train loss-0.7040, acc-0.8600, valid loss-0.7184, acc-0.8508, test loss-0.7250, acc-0.8445\n",
      "Iter-9430, train loss-0.8379, acc-0.8200, valid loss-0.7178, acc-0.8510, test loss-0.7245, acc-0.8444\n",
      "Iter-9440, train loss-0.6987, acc-0.9000, valid loss-0.7173, acc-0.8506, test loss-0.7239, acc-0.8445\n",
      "Iter-9450, train loss-0.7705, acc-0.8200, valid loss-0.7168, acc-0.8510, test loss-0.7234, acc-0.8447\n",
      "Iter-9460, train loss-0.6918, acc-0.8800, valid loss-0.7162, acc-0.8508, test loss-0.7228, acc-0.8447\n",
      "Iter-9470, train loss-0.7431, acc-0.8200, valid loss-0.7156, acc-0.8508, test loss-0.7223, acc-0.8445\n",
      "Iter-9480, train loss-0.6683, acc-0.8600, valid loss-0.7150, acc-0.8504, test loss-0.7217, acc-0.8445\n",
      "Iter-9490, train loss-0.6764, acc-0.8600, valid loss-0.7145, acc-0.8508, test loss-0.7211, acc-0.8450\n",
      "Iter-9500, train loss-0.7508, acc-0.7800, valid loss-0.7139, acc-0.8510, test loss-0.7206, acc-0.8449\n",
      "Iter-9510, train loss-0.8062, acc-0.7400, valid loss-0.7134, acc-0.8514, test loss-0.7200, acc-0.8450\n",
      "Iter-9520, train loss-0.6332, acc-0.9400, valid loss-0.7128, acc-0.8508, test loss-0.7194, acc-0.8450\n",
      "Iter-9530, train loss-0.6554, acc-0.8600, valid loss-0.7123, acc-0.8506, test loss-0.7189, acc-0.8451\n",
      "Iter-9540, train loss-0.7812, acc-0.8000, valid loss-0.7117, acc-0.8504, test loss-0.7183, acc-0.8450\n",
      "Iter-9550, train loss-0.8792, acc-0.7800, valid loss-0.7112, acc-0.8504, test loss-0.7178, acc-0.8453\n",
      "Iter-9560, train loss-0.6553, acc-0.8200, valid loss-0.7107, acc-0.8504, test loss-0.7173, acc-0.8453\n",
      "Iter-9570, train loss-0.8094, acc-0.8200, valid loss-0.7101, acc-0.8504, test loss-0.7167, acc-0.8451\n",
      "Iter-9580, train loss-0.6941, acc-0.8800, valid loss-0.7096, acc-0.8508, test loss-0.7161, acc-0.8457\n",
      "Iter-9590, train loss-0.7499, acc-0.8000, valid loss-0.7090, acc-0.8512, test loss-0.7156, acc-0.8457\n",
      "Iter-9600, train loss-0.6782, acc-0.9200, valid loss-0.7084, acc-0.8510, test loss-0.7150, acc-0.8458\n",
      "Iter-9610, train loss-0.6796, acc-0.8600, valid loss-0.7078, acc-0.8512, test loss-0.7145, acc-0.8460\n",
      "Iter-9620, train loss-0.7062, acc-0.8600, valid loss-0.7073, acc-0.8514, test loss-0.7139, acc-0.8457\n",
      "Iter-9630, train loss-0.7459, acc-0.8600, valid loss-0.7068, acc-0.8514, test loss-0.7134, acc-0.8461\n",
      "Iter-9640, train loss-0.6160, acc-0.9400, valid loss-0.7063, acc-0.8512, test loss-0.7129, acc-0.8458\n",
      "Iter-9650, train loss-0.6617, acc-0.8400, valid loss-0.7057, acc-0.8512, test loss-0.7124, acc-0.8459\n",
      "Iter-9660, train loss-0.6587, acc-0.8800, valid loss-0.7051, acc-0.8516, test loss-0.7119, acc-0.8461\n",
      "Iter-9670, train loss-0.7572, acc-0.8400, valid loss-0.7046, acc-0.8518, test loss-0.7114, acc-0.8462\n",
      "Iter-9680, train loss-0.6753, acc-0.8400, valid loss-0.7041, acc-0.8520, test loss-0.7109, acc-0.8464\n",
      "Iter-9690, train loss-0.6110, acc-0.8800, valid loss-0.7035, acc-0.8522, test loss-0.7103, acc-0.8466\n",
      "Iter-9700, train loss-0.7927, acc-0.7800, valid loss-0.7031, acc-0.8520, test loss-0.7098, acc-0.8467\n",
      "Iter-9710, train loss-0.9018, acc-0.7800, valid loss-0.7025, acc-0.8524, test loss-0.7093, acc-0.8466\n",
      "Iter-9720, train loss-0.7702, acc-0.8200, valid loss-0.7021, acc-0.8524, test loss-0.7087, acc-0.8465\n",
      "Iter-9730, train loss-0.6778, acc-0.8600, valid loss-0.7015, acc-0.8526, test loss-0.7081, acc-0.8467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-9740, train loss-0.6983, acc-0.8400, valid loss-0.7010, acc-0.8526, test loss-0.7076, acc-0.8467\n",
      "Iter-9750, train loss-0.6537, acc-0.9000, valid loss-0.7004, acc-0.8526, test loss-0.7070, acc-0.8469\n",
      "Iter-9760, train loss-0.7177, acc-0.8800, valid loss-0.6999, acc-0.8528, test loss-0.7065, acc-0.8471\n",
      "Iter-9770, train loss-0.5728, acc-0.9000, valid loss-0.6993, acc-0.8526, test loss-0.7059, acc-0.8468\n",
      "Iter-9780, train loss-0.8296, acc-0.7800, valid loss-0.6988, acc-0.8530, test loss-0.7054, acc-0.8470\n",
      "Iter-9790, train loss-0.7892, acc-0.8400, valid loss-0.6983, acc-0.8538, test loss-0.7049, acc-0.8472\n",
      "Iter-9800, train loss-0.7347, acc-0.8400, valid loss-0.6977, acc-0.8538, test loss-0.7043, acc-0.8471\n",
      "Iter-9810, train loss-0.8348, acc-0.7400, valid loss-0.6972, acc-0.8536, test loss-0.7038, acc-0.8472\n",
      "Iter-9820, train loss-0.7345, acc-0.8400, valid loss-0.6967, acc-0.8536, test loss-0.7033, acc-0.8469\n",
      "Iter-9830, train loss-0.8791, acc-0.7600, valid loss-0.6962, acc-0.8538, test loss-0.7028, acc-0.8472\n",
      "Iter-9840, train loss-0.8493, acc-0.8600, valid loss-0.6957, acc-0.8536, test loss-0.7023, acc-0.8473\n",
      "Iter-9850, train loss-0.8847, acc-0.8000, valid loss-0.6952, acc-0.8530, test loss-0.7018, acc-0.8474\n",
      "Iter-9860, train loss-0.8288, acc-0.8000, valid loss-0.6947, acc-0.8530, test loss-0.7013, acc-0.8474\n",
      "Iter-9870, train loss-0.7202, acc-0.8200, valid loss-0.6943, acc-0.8538, test loss-0.7008, acc-0.8474\n",
      "Iter-9880, train loss-0.7390, acc-0.8200, valid loss-0.6937, acc-0.8536, test loss-0.7004, acc-0.8473\n",
      "Iter-9890, train loss-0.6010, acc-0.9400, valid loss-0.6932, acc-0.8538, test loss-0.6998, acc-0.8474\n",
      "Iter-9900, train loss-0.7154, acc-0.8400, valid loss-0.6927, acc-0.8532, test loss-0.6993, acc-0.8477\n",
      "Iter-9910, train loss-0.9151, acc-0.7400, valid loss-0.6922, acc-0.8528, test loss-0.6988, acc-0.8476\n",
      "Iter-9920, train loss-0.7325, acc-0.9000, valid loss-0.6917, acc-0.8532, test loss-0.6983, acc-0.8480\n",
      "Iter-9930, train loss-0.7151, acc-0.8600, valid loss-0.6911, acc-0.8540, test loss-0.6977, acc-0.8478\n",
      "Iter-9940, train loss-0.7331, acc-0.7800, valid loss-0.6906, acc-0.8538, test loss-0.6972, acc-0.8482\n",
      "Iter-9950, train loss-0.7497, acc-0.7800, valid loss-0.6900, acc-0.8542, test loss-0.6966, acc-0.8482\n",
      "Iter-9960, train loss-0.7773, acc-0.8400, valid loss-0.6895, acc-0.8538, test loss-0.6961, acc-0.8481\n",
      "Iter-9970, train loss-0.7150, acc-0.8200, valid loss-0.6890, acc-0.8544, test loss-0.6957, acc-0.8482\n",
      "Iter-9980, train loss-0.9142, acc-0.7600, valid loss-0.6885, acc-0.8538, test loss-0.6951, acc-0.8481\n",
      "Iter-9990, train loss-0.6412, acc-0.8800, valid loss-0.6880, acc-0.8544, test loss-0.6946, acc-0.8485\n",
      "Iter-10000, train loss-0.8178, acc-0.7800, valid loss-0.6875, acc-0.8550, test loss-0.6940, acc-0.8485\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "n_iter = 10000 # number of epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "mb_size = 50 # 2**10==1024 # width, timestep for sequential data or minibatch size\n",
    "print_after = 10 # n_iter//10 # print loss for train, valid, and test\n",
    "num_hidden_units = 32 # number of kernels/ filters in each layer\n",
    "num_input_units = X_train.shape[1] # noise added at the input lavel as input noise we can use dX or for more improvement\n",
    "num_output_units = y_train.max() + 1 # number of classes in this classification problem\n",
    "num_layers = 2 # depth \n",
    "\n",
    "# Build the model/NN and learn it: running session.\n",
    "nn = FFNN(C=num_output_units, D=num_input_units, H=num_hidden_units, L=num_layers)\n",
    "\n",
    "nn.sgd(train_set=(X_train, y_train), val_set=(X_val, y_val), mb_size=mb_size, alpha=alpha, \n",
    "           n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJAgIIUQA+3+cMmKnHTrBAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11945ab38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(nn.losses['train'], label='Train loss')\n",
    "plt.plot(nn.losses['valid'], label='Valid loss')\n",
    "plt.plot(nn.losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwhEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119414588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(nn.losses['train_acc'], label='Train accuracy')\n",
    "plt.plot(nn.losses['valid_acc'], label='Valid accuracy')\n",
    "plt.plot(nn.losses['test_acc'], label='Test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
