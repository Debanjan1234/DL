{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        for layer in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    idx = 0\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1 # -np.log(1.0 / len(set(X_train)))\n",
    "    eps = 1e-8\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 104.8406\n",
      "ehJ7KtTat'yeiSwytnrla\"ehi otstetnohdetsreu5ultn irntn tiond pttehs ,iei aen0sEoilpm upn rl  hatpneiNa\n",
      "Iter-20 loss: 101.7449\n",
      "estigfba ita icf se nnspmthpsesgss eeenO roieotder ou ipaaren er iteaWee aitC ,kmitk9yinkogmGWTeptdus\n",
      "Iter-30 loss: 99.2245\n",
      "etc\"n ae s Say2nt Oorfayol rsei chT eIonf ane rt hphd  asanti srfsAotyUo nlP 7Halen, rcyuyrt ,heysoal\n",
      "Iter-40 loss: 96.7669\n",
      "eoe to wrld he oge hagin ong yte Dflarchulh dn8doenahJ1 ftnl EpIyfswprf.ogo RatnaGre ayitftwis ead,pe\n",
      "Iter-50 loss: 94.6718\n",
      "er,se Tuaurh rree 4an ayliu an y pnh  aembiTond日l6Fan egt tle fos,trsceflars naraUtSpathftstnxt-in ts\n",
      "Iter-60 loss: 92.9186\n",
      "egrs4t Jatit8wy aef2Ttn ltn os pnrlermCswirl y Ciwd conapa iph olpfntsn nnmacasg, paed in xty n11tRmt\n",
      "Iter-70 loss: 91.4135\n",
      "eTl8oNas uiwtuihd aaEic aogane an 'lcnciche eiiii thc fr.haktd ftry athoy geifLtgipsld fywaort ut iNn\n",
      "Iter-80 loss: 90.0977\n",
      "eO OPao pnec mpfoapow dTelthra- p:c'e The ine in arn o yb. Ane oyhm teirueris ioned ntos wEprutgbtan \n",
      "Iter-90 loss: 88.9343\n",
      "eSan–NC5Hahi se \"e phmkiktRe tha and marne GSar geres eaearon ldineuathf Sor lapsrNith3,1pTo fndtity \n",
      "Iter-100 loss: 87.8944\n",
      "es9ak k\"日dI  ilJ pamrt lehekpldaipau prdalltre hsseTo 5oonde  Bo.ranr e,1auloonc yos d6 eetono, .k 2S\n",
      "Iter-110 loss: 86.9561\n",
      "e:n -ed c ,narsh sturathe . panawa iou ketstte ouu Ses tl t7la9 uhe iath tjks ipste Aonkciusmes 1ired\n",
      "Iter-120 loss: 86.1042\n",
      "ep tEHeisntvyacingopardds,. an Japou ian iperol iprars arlty 1umcres wof ele pscaty na in in e perthe\n",
      "Iter-130 loss: 85.3282\n",
      "e7,d iestioio7 .in Hache Tom dGlite SPeaatyyE etwerwethed AnditlftDein the 7oml.ey TApso ahmiin anare\n",
      "Iter-140 loss: 84.6209\n",
      "ea日1 istIoihen o. theg Uure y cieaoe tan au kbldes ty Tor eapa, iog redduvpan tofiit wi iet ilJpisrtt\n",
      "Iter-150 loss: 83.9759\n",
      "egkA'd ontse to cewin fhk hhs Topth the 5EWan aisesd Oald tiss ins ,af. .t,g pfef ind in girxis Norse\n",
      "Iter-160 loss: 83.3871\n",
      "e;er khas oOlsisess Aon pl oan eallsflhors oEopsapef an pang ta lar., S2cs. ,ote toNa1. Danta Il pori\n",
      "Iter-170 loss: 82.8476\n",
      "ezwanM pan in t.s asta op8rnd wirtish sovitapwestoe s. rorgh\" Eal\"you Oapan an KTupah cnsth6-cWrufl3E\n",
      "Iter-180 loss: 82.3504\n",
      "eCI1y. Cahud p3leocanad tof the haneT. shs apme ih cando So antes f pankh LoflterJa7atledss. both -rr\n",
      "Iter-190 loss: 81.8881\n",
      "e\"ustgis timhety anacised ans intter., En0uns, thl Waptne Japandowaritt ind and seae orle oan auclima\n",
      "Iter-200 loss: 81.4541\n",
      "ea19 Slhiu hisn uoLocerea ly cneitiin in'uy ovld Inde euprydjs Whe Gfona y–, wasres hg1hest koeard. J\n",
      "Iter-210 loss: 81.0428\n",
      "ese lanse timea, wopen Palan th  amly ped id Ghe theuth本 1SJapan rst'r iliea irkg in in the \"apded mt\n",
      "Iter-220 loss: 80.6496\n",
      "eslNston pan Ia in Tipandtch tan tiom WouJaualfLOaectieomy pali1u-ef rtema6 an io the cigg pty. HGveh\n",
      "Iter-230 loss: 80.2710\n",
      "eCih ghse tepety po uarod HonEo-the tor worigrsy Nhhan Esseros chedd pfinutioe Core the whengurrse Ae\n",
      "Iter-240 loss: 79.9044\n",
      "ejla. . baxurthabis lased iol KoAnde omate EDal ds ion daan kut. Japantin Ilg oinhe iad philateoI shi\n",
      "Iter-250 loss: 79.5482\n",
      "eiva ebpestecy th  aithe 'ines  ipen icisisn in angl pofths 1unlaet inara Otf vekutlerwirest sised i–\n",
      "Iter-260 loss: 79.2007\n",
      "er caadon6 Mhenthirh the and 1SClslcingipora bSptofliveswarasmc ean and Is the aMeelbisa popehtinse r\n",
      "Iter-270 loss: 78.8607\n",
      "end Japlornd Jipene  thede Te rfe Cmpingesi bonthic d2rald wolysi– iist  rial wak an .oG. tonitou the\n",
      "Iter-280 loss: 78.5268\n",
      "esipao s, 8ianst 0o. x7orlDusous e. inHpa inl ersoorg ana Sxrgergest whand Rade inothiin porse. wr1 t\n",
      "Iter-290 loss: 78.1973\n",
      "e-4t's Gtryetit the mare Aymla. Japan -sohedG irorisntte of wN,thi led aau\"e finns Rist of unoyy siPa\n",
      "Iter-300 loss: 77.8705\n",
      "esdopertsicarfrs th chipthe SAthekev chlitamas tu,ly misn-6achelarum ry simhe2 an Oano本SSFd ga aldsou\n",
      "Iter-310 loss: 77.5445\n",
      "ect rerPeleremurat an 日at yiritenex bid. Gon (nKea lys besi lopand Td tho mrrthus of Jappod a, , and \n",
      "Iter-320 loss: 77.2171\n",
      "e.sth -aecestreer1xlid mins ance meerled ar trclarity in Cminst oa \"oflo, an anth tely. Sica eHtye jh\n",
      "Iter-330 loss: 76.8860\n",
      "ewes shpm-,inaped Japan 9Uin on, fs-the 9amf the Ward A91te jist g9ca, wove forek Amisnkeoust.ly tec2\n",
      "Iter-340 loss: 76.5488\n",
      "exMd ng8iiolg \"hfs cheredt-lirly Ii hncinapof w\n",
      "rse icrlarnoigd iaok8 Eocby dara-f8本 Japren tUpinala \n",
      "Iter-350 loss: 76.2027\n",
      "eatg pancomgea9uuicioscowared fofmsentoclatithe Urcectiondal Nnsitated ons and's aed soprnatu nary ce\n",
      "Iter-360 loss: 75.8450\n",
      "e pepeand ssclaruliy Gn 1ver imatececrkte it un at in if okCend Warss Afpan cocinof w2r ir ial-rarper\n",
      "Iter-370 loss: 75.4726\n",
      "eEdtar Ir ertho, rakand ind wacldtherid Sisa far Japane of the 1atl if thi in's moprecpardd's locpeny\n",
      "Iter-380 loss: 75.0824\n",
      "ea an tae oit en, manse 1lakS ine Warlectac ou and copanithi, the Maing is an of of Is the eaged chec\n",
      "Iter-390 loss: 74.6713\n",
      "e)d argith moan is instr and an es,s eacctesthy AafaFd\"f ceanalirithun aid the undirar Jaarte a and t\n",
      "Iter-400 loss: 74.2363\n",
      "eaMise fiu tooltheg ther Nipan tou Japandiplcoitinid 's bourarser in t8oj) whe olea, parth wirit ledi\n",
      "Iter-410 loss: 73.7747\n",
      "e日dsog and caca cres dInca end miif rWiso Asand are aest end maas y\n",
      "'s cheapares alus enporerl. 20rla\n",
      "Iter-420 loss: 73.2841\n",
      "es the Exciterousturud tet tist fgest o2 and owel of CSjintr andisa ises JapandCa ang \"apan estof fop\n",
      "Iter-430 loss: 72.7625\n",
      "e parsy dupred in lanf Thj nxpenmed is olsth th-in ino's and anitimece perolddif obol ae endtr iteta \n",
      "Iter-440 loss: 72.2084\n",
      "er roroce,a ia the Mopel wes chitesiman os the The oas intholaru本 G8Hunarit tho llohl Somportol the E\n",
      "Iter-450 loss: 71.6208\n",
      "edlito ridofsl wand panje wouthing gicana, 5is Japan rna forg. in totudinttol 186M asecead wouu and s\n",
      "Iter-460 loss: 70.9995\n",
      "el ou Nixeesed int Ltfetee of Japan in 194mbsolyl. the feprilamury Negere porvextat emthi and certest\n",
      "Iter-470 loss: 70.3444\n",
      "ejJapandea, Tol oso ur thoun int ien\"'d San. Whe ar. Aad perteoGurliwirgestcenpurere Gionced inso ist\n",
      "Iter-480 loss: 69.6558\n",
      "e, lapa es mempore bete thi. Aseapan Dhestrla esth Japan paked sof th mGresaris the Rrancins-.\n",
      ". ant \n",
      "Iter-490 loss: 68.9342\n",
      "euxcinclaturlonl whus Th lasthis an fornthiw lineke Si9 prdlded as hogorttitd which insticonsur, Wort\n",
      "Iter-500 loss: 68.1798\n",
      "eegDJapan's Kicolapist loby is the 120 chac abd ion,e Japan sea Cobaomiol o本 183Tiglmite oo ands. War\n",
      "Iter-510 loss: 67.3927\n",
      "e', alatlans faud v0tero1d', coritedes. Thichitht erja hel popmpored Wovntorea ir the Ufe Resed count\n",
      "Iter-520 loss: 66.5728\n",
      "ene mmensin an, G7thilar J6irnesr Iag Cedeat ed al the Japan sea Ltlomend in the Roby with the f roka\n",
      "Iter-530 loss: 65.7199\n",
      "e tha Uupgese popul Ind oupar ics andhtKGue thy whe terlof whikirkef inld unps5 th the G7sese which a\n",
      "Iter-540 loss: 64.8334\n",
      "ex17 Lpan in E8–y EHis arhed itt ainEon large, miloomcthilokywat ingita, To, wongext ane tay torxtite\n",
      "Iter-550 loss: 63.9128\n",
      "erthe Oord of Ozome topth cegtory milat om biof an milmiowtry endect reponuty Ef the wai or any Asice\n",
      "Iter-560 loss: 62.9576\n",
      "ecunhicored Wa \"ang papesof Aseanl iTly Seslil pin E5s hist ryiscted In Wup f-dand Tanghcnas lacitat \n",
      "Iter-570 loss: 61.9679\n",
      "ed wase in the. Jopan omethind togn ov inethe, The Waugand ctoloted Nithe borl. Japan in SoutheJ\"pan \n",
      "Iter-580 loss: 60.9454\n",
      "ed and dith  lesthey sith . Thanam ard rsed sory ofo f0untila, Intera. In'sy Thiceate and Japan's 20p\n",
      "Iter-590 loss: 59.8949\n",
      "es nanjD and the wound 0alital s urhexial Isith lase-ne, colaty in the Ef toun. worll-Eanped the thr \n",
      "Iter-600 loss: 58.8251\n",
      "ed Wero2 ALpardethinstiellangewitert mmand'o ind Japan's. Thi of Japar od Japand Japar's a and's the \n",
      "Iter-610 loss: 57.7498\n",
      "esturth iJaian y日sicowam boco. Japanoct pprina, mand anded raisha tosm-rterite. Arthe Ufd sana in the\n",
      "Iter-620 loss: 56.6881\n",
      "e, chellofed the the dirldis the R;viof han (Japanes igaled rfhand stur-andeiwld mith rary of bmaod a\n",
      "Iter-630 loss: 55.6615\n",
      "etho O8\". makle. var and the Gy wan of Japan is Sir, dar Hitthe wired Japan ex unesecores racldang st\n",
      "Iter-640 loss: 54.6906\n",
      "eand 19UKofme the  filntely Wef Pokah ald Gythe gocoforo morover intei mparese Aofith can\"eds rechint\n",
      "Iter-650 loss: 53.7894\n",
      "ery the esol of Japan is a pertea, Chald aopmititar in 1945 Wt tiol cedert ousa- (opme panestu lommir\n",
      "Iter-660 loss: 52.9619\n",
      "ethina wont. Sunury. Jipan easin Tfe coped riand-nathe GSlatay . aal tecr usind ixgalamied as ofeatrl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 52.2023\n",
      "exto yoroty Of pomsectoridas in. Naom ald Japar hist8. Ofurs lilerg n. Japan tre ,iphr atiolof minto \n",
      "Iter-680 loss: 51.4999\n",
      "est thi the Ease fourl turce Srysed istt anvecear is the whind Nxpan onted is redtered carnoky if pfo\n",
      "Iter-690 loss: 50.8443\n",
      "eAje arded Eaake  omKrl Iocnraits ange itry war In the womld'  norare wicturis naofins if thoriteed J\n",
      "Iter-700 loss: 50.2269\n",
      "e of Wonter Oopbm boge eopunexd the  icofanl chang the (aHtthichad it atko Japan \"y ande sexrry werod\n",
      "Iter-710 loss: 49.6414\n",
      "ed intoy mo eheofiea gnt rofmoun 1937 Iicgaorte- aod ana teme o foins, Ohexte FNipe porl pyupa fiat c\n",
      "Iter-720 loss: 49.0827\n",
      "eny eured Japan vesthe Sexpold%tutity of hinth of preserrdesero ghon Hhisad the Wupmol mand worlcouns\n",
      "Iter-730 loss: 48.5470\n",
      "es Japan ss. The GhiStdkulome A9th in ahe th  aod\". Japan hs acondCithilg of thipea, Roroky wso th le\n",
      "Iter-740 loss: 48.0303\n",
      "eEJapmonti, an paresor Eofa honaty. Thilaviy deof of anetexis dhea im hace Snceapo Ti tool Dsing ofli\n",
      "Iter-750 loss: 47.5281\n",
      "ec. farl darg cofountrly rlgoonter and psinsCilare who fry torte ALsiomJanan's the hawan me ange pria\n",
      "Iter-760 loss: 47.0356\n",
      "eofoup.-larleca an the G7ofolithe wort or sfertertur in the wirld's countle-amde, lipare the wirto-Ja\n",
      "Iter-770 loss: 46.5485\n",
      "ecestu anly Koppme operlrom ir arde permity os the coun or. OA0ek amdest coutories in U20 foush carle\n",
      "Iter-780 loss: 46.0460\n",
      "exth of the G8, darl sisid whs atlod as irteatuklo, tol gurnh is and 94in,s mebtolyofg. Japan es thec\n",
      "Iter-790 loss: 45.6357\n",
      "eUthe Eamg ce aren\"D. Kkrla,ed chint of Japanicaty, batd centub om auticones. Suron LEapo2r and 9arg \n",
      "Iter-800 loss: 45.1873\n",
      "elara gesthe prpored 18,inak Imiel,rof of Chunasisst rathest meitollei nurthe Un wort er aret bo Eu,h\n",
      "Iter-810 loss: 44.8135\n",
      "est thecn riwan  Stalas angece pppen in tro nis. Wan As, chish wacg Hominedes ipth tiry in Ihet. parl\n",
      "Iter-820 loss: 44.3838\n",
      "ed mith en, (Pianth therad exth  and RSond ud ptore-terc wimisor amiker zed Japan is thh Emparoe whe \n",
      "Iter-830 loss: 44.0427\n",
      "esto unth intkyd sofh. Fhen a tomeon wo the Erpeborl Japan in ue, Nigscatorons. Tho Souriceal pupre I\n",
      "Iter-840 loss: 43.6821\n",
      "e, worth pirceate Sin, Khech pedtin and Cicean of. The amdhthin cefof enciben of Asearil Worle 126 bo\n",
      "Iter-850 loss: 43.3188\n",
      "e the fourth-largest Cheha iast in ther :0 Japan Was n parry sivith eicorers  arde maictaaliod ans th\n",
      "Iter-860 loss: 42.9682\n",
      "es thect is War is the Werth- im bofevingict ipsedin 186L ankry-Hof Chand pesoper ans. The Rsted-larg\n",
      "Iter-870 loss: 42.5460\n",
      "exrr hen, Nictutored is t8eat inasioct an istir,. 2apotla as inas Sinto the Wor dea te fort titlout o\n",
      "Iter-880 loss: 42.2539\n",
      "es redstrer. Japan toked livhreestured eseat cigaounde if Japan 58 and the Eagcect licstropere wo. Ko\n",
      "Iter-890 loss: 41.9733\n",
      "ejR sinong on of romperie the 2ountercantr y. whom ceeso. The woprtulaprisy in Ihan Pyticl omy higotr\n",
      "Iter-900 loss: 41.6787\n",
      "eld rsiof Apenory. The G8ean bith-y sf fperlicates of and Snamest te 19th destore Sinj Sighartho or a\n",
      "Iter-910 loss: 41.2987\n",
      "enter ary win in the torld urle Wart Ti, or ivelamchiand beifounter anf gingect ient.o monlouJapanase\n",
      "Iter-920 loss: 41.0466\n",
      "exfo-ral op. AAt orinnduvidese Riotse ind ixonttro ac earco-aree \"litedos dith of arde ander. Torlati\n",
      "Iter-930 loss: 40.7870\n",
      "exa fhuth-lirgestith igntede sity ty 1nvilate sine \"55 dembar ins 20Ihas red is flurth-yoporshu are c\n",
      "Iter-940 loss: 40.3953\n",
      "ed Japan ha, eate  hlpora apde fiar lass f the Gkomurkeis. furll Japppekteriog in  hellas ins 120hech\n",
      "Iter-950 loss: 40.2354\n",
      "esty ffror and tertound pare mperear eper of atd and the World Ar iheth-largest oread reatity Wy  fou\n",
      "Iter-960 loss: 39.8633\n",
      "e. The Gopno lareasi lamlith. A5: Asint, Japan's Japan rasitat ou thica an elocl ofe an tre andy nime\n",
      "Iter-970 loss: 39.6382\n",
      "elacendc who \n",
      "lle morelain restory pmontilatie forgltirne site Ar ioll fios in Ruke1 Db Japan,y Sinsc\n",
      "Iter-980 loss: 39.4746\n",
      "esex Japan, d alola. wottory Tre Istallcomy. whr lalor Nogurlkosei, the ason 17ian Japan's laao ad is\n",
      "Iter-990 loss: 39.1021\n",
      "e fourth iat icm of bored al ifhic cestiro, Ciecand cictertu monsin mhicnarisat molati in of Japan's \n",
      "Iter-1000 loss: 38.8257\n",
      "ed Bhurlen. Fros war t me leritoly. An laliort logular estity resturofory borc. Kpore-aans duthe Ru, \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxngsIRhjjAEsIRhjjPFYQjDGGAPA/wN+nEXeGT6PTQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c122e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
