{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "# with open('data/text_data/anna.txt', 'r') as f:\n",
    "\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model or Network\n",
    "import impl.layer as l\n",
    "from impl.loss import *\n",
    "\n",
    "class GRU:\n",
    "    def __init__(self, D, H, L, char2idx, idx2char):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        \n",
    "        # Model params: output and input sequence\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "        )\n",
    "\n",
    "        self.model = []\n",
    "        \n",
    "        self.model.append([])\n",
    "        for _ in range(0, self.L - 1, 1):\n",
    "            self.model[0].append(m)\n",
    "\n",
    "        self.model.append([])\n",
    "        for _ in range(self.L):\n",
    "            self.model[1].append(m)\n",
    "\n",
    "        # Input sequence - last layer\n",
    "        m = dict(\n",
    "            Wz=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wr=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wh=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            bz=np.zeros((1, H)),\n",
    "            br=np.zeros((1, H)),\n",
    "            bh=np.zeros((1, H)),\n",
    "        )\n",
    "        for _ in range(self.L - 1, self.L, 1):\n",
    "            self.model[0].append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wz, Wr, Wh, Wy = m['Wz'], m['Wr'], m['Wh'], m['Wy']\n",
    "        bz, br, bh, by = m['bz'], m['br'], m['bh'], m['by']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache, y_cache = cache\n",
    "        \n",
    "        dh_out = dh.copy()\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def forward_(self, X, h, m):\n",
    "        Wz, Wr, Wh = m['Wz'], m['Wr'], m['Wh']\n",
    "        bz, br, bh = m['bz'], m['br'], m['bh']\n",
    "\n",
    "        X_in = X.copy()\n",
    "        h_in = h.copy()\n",
    "\n",
    "        X = np.column_stack((h_in, X_in))\n",
    "\n",
    "        hz, hz_cache = l.fc_forward(X, Wz, bz)\n",
    "        hz, hz_sigm_cache = l.sigmoid_forward(hz)\n",
    "\n",
    "        hr, hr_cache = l.fc_forward(X, Wr, br)\n",
    "        hr, hr_sigm_cache = l.sigmoid_forward(hr)\n",
    "\n",
    "        X = np.column_stack((hr * h_in, X_in))\n",
    "        \n",
    "        hh, hh_cache = l.fc_forward(X, Wh, bh)\n",
    "        hh, hh_tanh_cache = l.tanh_forward(hh)\n",
    "\n",
    "        # h = (1. - hz) * h_old + hz * hh\n",
    "        # or\n",
    "        h = ((1. - hz) * h_in) + (hz * hh)\n",
    "        # or\n",
    "        # h = h_in + hz (hh - h_in)\n",
    "\n",
    "        cache = (h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache)\n",
    "\n",
    "        return h, cache\n",
    "\n",
    "    def backward_(self, dh, cache):\n",
    "        h_in, hz, hz_cache, hz_sigm_cache, hr, hr_cache, hr_sigm_cache, hh, hh_cache, hh_tanh_cache = cache\n",
    "        \n",
    "        dh_in1 = (1. - hz) * dh\n",
    "        dhh = hz * dh\n",
    "        dhz = (hh * dh) - (h_in * dh)\n",
    "        # or\n",
    "        # dhz = (hh - h_in) * dh\n",
    "\n",
    "        dhh = l.tanh_backward(dhh, hh_tanh_cache)\n",
    "        dXh, dWh, dbh = l.fc_backward(dhh, hh_cache)\n",
    "\n",
    "        dh = dXh[:, :self.H]\n",
    "        dX_in2 = dXh[:, self.H:]\n",
    "        dh_in2 = hr * dh\n",
    "\n",
    "        dhr = h_in * dh\n",
    "        dhr = l.sigmoid_backward(dhr, hr_sigm_cache)\n",
    "        dXr, dWr, dbr = l.fc_backward(dhr, hr_cache)\n",
    "\n",
    "        dhz = l.sigmoid_backward(dhz, hz_sigm_cache)\n",
    "        dXz, dWz, dbz = l.fc_backward(dhz, hz_cache)\n",
    "\n",
    "        dX = dXr + dXz\n",
    "        dh_in3 = dX[:, :self.H]\n",
    "        dX_in1 = dX[:, self.H:]\n",
    "\n",
    "        dh = dh_in1 + dh_in2 + dh_in3\n",
    "        dX = dX_in1 + dX_in2\n",
    "\n",
    "        grad = dict(Wz=dWz, Wr=dWr, Wh=dWh, Wy=dWy, bz=dbz, br=dbr, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "    \n",
    "    def train_forward(self, XY_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        # number of sequences: in & out\n",
    "        for _ in range(2):\n",
    "            caches.append([])\n",
    "        \n",
    "        # number of layers\n",
    "        for _ in range(self.L):\n",
    "            caches[0].append([])\n",
    "            caches[1].append([])\n",
    "            \n",
    "        # in and out sequences\n",
    "        X_train, Y_train = XY_train\n",
    "\n",
    "        # Input sequence\n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(0, self.L - 1):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[0][layer])\n",
    "                caches[0][layer].append(cache)\n",
    "                X = y.copy()\n",
    "            # Last layer for the input sequence\n",
    "            for layer in range(self.L - 1, self.L):\n",
    "                h[layer], cache = self.forward_(X, h[layer], self.model[0][layer])\n",
    "                caches[0][layer].append(cache)\n",
    "\n",
    "        # Output sequence\n",
    "        for X in Y_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[1][layer])\n",
    "                caches[1][layer].append(cache)\n",
    "                X = y.copy()\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += cross_entropy(y_pred, y)\n",
    "            dy = dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "    \n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for k in grad[layer].keys():\n",
    "                    grads[layer][k] += grad[layer][k]\n",
    "                dy = dX.copy()\n",
    "            dXs.append(dX)\n",
    "                \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    # for i in range(0, X.shape[0], minibatch_size):\n",
    "    for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        R.append({k: np.zeros_like(v) for k, v in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "    \n",
    "    for iter in range(1, n_iter + 1):\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for k in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][k] = l.exp_running_avg(M[layer][k], grads[layer][k], beta1)\n",
    "                    R[layer][k] = l.exp_running_avg(R[layer][k], grads[layer][k]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][k] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][k] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][k] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + l.eps)\n",
    "\n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, 100)\n",
    "            print(sample)\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1 loss: 41.5361\n",
      "in uurOy th lugse civt tare thd wSran fn tn eatromaran sh rits unac un Takrg wf'n wanp mes ankd Wstli\n",
      "Iter-2 loss: 41.3454\n",
      "initged f2eurlaD tery, Japan usunth The 0has in Tal Aaanst-e fithe the ofsist If iol Te innted round \n",
      "Iter-3 loss: 41.1848\n",
      "ind fsmrof cixce thel wortith whien, io the pheass nc ali1ujthe wored lirx me rokthg oftthe 1 6perndt\n",
      "Iter-4 loss: 40.8326\n",
      "iponlowist, ealomle's d. Horttrld, P ubofles andind as im tres, Japan of rale\"dict rhand WErst en wis\n",
      "Iter-5 loss: 40.2764\n",
      "igens and TNe Siufs. Japan setith'l Sim, ungurct rulerigecece ic the an the fortobe whe to unstion ro\n",
      "Iter-6 loss: 39.5957\n",
      "itl the wi1lolb-Emesind,ititnol CJapan ionced aid whh the world's land Naianerlest winsthily, Nopinty\n",
      "Iter-7 loss: 38.8887\n",
      "it iontiry . Ixponf coth tad in lrbankivesea Japat , wombict and and mbeverod Njech the Githe Sed the\n",
      "Iter-8 loss: 38.2029\n",
      "inan bede frcotainol inan e tored of by fxrxts in the 1nx, furcowust and Aand S anshureepeaisy bUc in\n",
      "Iter-9 loss: 37.5340\n",
      "ishisionobg pentitiving and lurmidgss and Sexeargese dulcoullibe 1501 cates fiurth. Japan 1's and isl\n",
      "Iter-10 loss: 36.8653\n",
      "inad Fionty liratest onditiosea istthe 10hila, wat ensevest The whish ComlocumJapan eag eatrih, nhlig\n",
      "Iter-11 loss: 36.1940\n",
      "in 1 s thich iand maintare in Atich ferst Wor at of Japan the hishat of the G)2th sevedaliconana mele\n",
      "Iter-12 loss: 35.5254\n",
      "ir. is the ceathe comitry and 20taty nakiou, Konbl's andsimeloved largest and celfoucounteieed divint\n",
      "Iter-13 loss: 34.8662\n",
      "iot paanes and Tegescounan DaJapan sevoke pare panted in Asto and itte lokuagand devcidwa t he outhis\n",
      "Iter-14 loss: 34.2205\n",
      "ish lcof leged fiits feurts in semilaritopan whe pith the ra west urijs acan in a 4Nka an eaprogsure \n",
      "Iter-15 loss: 33.5878\n",
      "inntry Sinal Diveng precored if 20952ky insthe Seaters in orlacint Silpofnopud souttin world's larges\n",
      "Iter-16 loss: 32.9666\n",
      "iy 1947keveno-cong lanked Wax Nomperios and lareed. Japan East is and Komber, largest is of colstante\n",
      "Iter-17 loss: 32.3545\n",
      "ist cokbolalgsal asomunceurd's it of ondec mare War -Japan Japanese GOgopo-to-Japanese Pappertery Ind\n",
      "Iter-18 loss: 31.7493\n",
      "i3 19nd Japan's ts insero ercoun 19ana Siveijt It the Japan war melariilitary Fits in the ntaresmarid\n",
      "Iter-19 loss: 31.1500\n",
      "ivind the comper and War ald's f-largedel banked forld-lowirg cates the worst paraly. The ser, the wo\n",
      "Iter-20 loss: 30.5572\n",
      "is ansh okughss erored as furcomediny. The counth-largectaened Stmar achifo, was ess it mies, the Emp\n",
      "Iter-21 loss: 29.9720\n",
      "ies, Japan was, countriom an eforted, asieg ind. The Arihoro eata Emperad is ted of 19ma esulimation.\n",
      "Iter-22 loss: 29.3937\n",
      "int-y dech fesouting and of isiagest Japan iola, the E2pir severcoup tie 19th a geth test redionter a\n",
      "Iter-23 loss: 28.8224\n",
      "ind's the Japan wast In,tured chama mectea dengiss. ixplcine Coby in Asia unthion chine tse R-has fic\n",
      "Iter-24 loss: 28.2592\n",
      "inal Mithe Asur of lititanesy profered the ralion in the Rippon, uridamc andediceatiry and the highes\n",
      "Iter-25 loss: 27.7070\n",
      "imal mal CDinal sithe Gu, as efounthy Asiag preatly Japanese poputet. Japan is cededivelod centered J\n",
      "Iter-26 loss: 27.1707\n",
      "inted chinest ry the 6panesida anged cousslicinese pureti2uthit of Japan, was redich was arlary sumbe\n",
      "Iter-27 loss: 26.6533\n",
      "inetho world's Napan maleopte th cent morlation.-Japan was in lighed insumr aIpolcean, islaticy piect\n",
      "Iter-28 loss: 26.1580\n",
      "in is the cerectarize worco, and fourth-lagusol bopon tase highe hise of Hith late io the World War c\n",
      "Iter-29 loss: 25.6876\n",
      "irter Unsed istondalaiged, urth oworghs. Japan is a Chineseapreajenes, Warly Index, which panged coms\n",
      "Iter-30 loss: 25.2434\n",
      "ith intury pepto wotan seacokJapane b. Uhilly leve end Napanase G20 Natao, atinmy the warby cots thes\n",
      "Iter-31 loss: 24.8253\n",
      "igest. Japan ts hests. Japan was the 126 millies infcin. Arecountry5 highssku, which pepina byuth the\n",
      "Iter-32 loss: 24.4326\n",
      "in regis..IThe world Som, eally untly in the deriod Waldor untereccenesure pios and, and Kand vinges \n",
      "Iter-33 loss: 24.0642\n",
      "in s acedevuntse an pare war, the to the Sea pioflargest inthic the worlet. Inded lare dures as a sev\n",
      "Iter-34 loss: 23.7188\n",
      "ian, whicad cint incounortes of liChont Asua comithin to sountry is of coutarichasu Wer. The rankeasu\n",
      "Iter-35 loss: 23.3947\n",
      "istaligd is infestico and in menaly in the thic sare to the Weriods frexionth-ser and larmepel 1868, \n",
      "Iter-36 loss: 23.0907\n",
      "is lage sumber, larcesiceast exporter, the Gleand Hiotsy, the ralion an earigy. Nhichesided colleciun\n",
      "Iter-37 loss: 22.8050\n",
      "ing Nippona coumeligg officinty, indtre tory. Arnh-kokud piet. Japan eso ato uthan early expanded Ris\n",
      "Iter-38 loss: 22.5360\n",
      "ind a Asia, Japanesev camis Is the fourth-loku op esomblase world'st Aseare congulan Devilatarly from\n",
      "Iter-39 loss: 22.2820\n",
      "its a mpin in the cexporter undicteroled the Gxports forl wor Nihan, wits a incendese bovin the Sicia\n",
      "Iter-40 loss: 22.0415\n",
      "itationdo. The first in the nEmmerropedceoco-Japanese up ofi1lit. Ebolanke sure tioth largest ring In\n",
      "Iter-41 loss: 21.8130\n",
      "iby, a devings mimital Iy orecce cligitary hest Japan is of to has fourored islatict a is as nagisaa \n",
      "Iter-42 loss: 21.5952\n",
      "inal maty of 17th-lered Japan hase Sino foursh-largest. rilioa an the Simper an's ligh larceles inter\n",
      "Iter-43 loss: 21.3869\n",
      "ipolatareace ppon a and ito arlald's firsourld if tomesl orored laegest rigitan yI the  iflatecinin i\n",
      "Iter-44 loss: 21.1869\n",
      "is tat or Nihan, high canded inssiventures in is of heshogudalabase world.TImithines and. y hered ant\n",
      "Iter-45 loss: 20.9942\n",
      "ith. bith in the world's oreand Rupss uror and Whekea, which ferctures inceuasing meofletion. Abolaec\n",
      "Iter-46 loss: 20.8077\n",
      "ia in 1, and in the UJapanese War, the Napeets-largest menesure couthitesivengrchiad cheaodex. Japan \n",
      "Iter-47 loss: 20.6267\n",
      "isla, eas exppre nombily520 and it asa wires. This oflopee, tas inte peppet, wasd Japan in the Arigis\n",
      "Iter-48 loss: 20.4503\n",
      "in ceatory b-largest milutar. The coraticiletion ic pare Sexowatest-largest eferth the Japan was the \n",
      "Iter-49 loss: 20.2778\n",
      "inha melitaries. Japan io Asia. Japan was inh Ctunegesont. The considerg bulconkumarcy inclurgest inc\n",
      "Iter-50 loss: 20.1085\n",
      "inte forrat ih levesem istered cety ctunt C7ipan is and and NiPpan est the narihy a divinese peot. Ja\n",
      "Iter-51 loss: 19.9420\n",
      "ih. Gan, the Simiomy ch the to th ted ins. Inatien the nombinal Piages, wat bbld's fourth-larmest ict\n",
      "Iter-52 loss: 19.7777\n",
      "is a C0la and the G7, the G8, and the world's largest urtiolloreds. The Sifpontury in East Acintuvirc\n",
      "Iter-53 loss: 19.6152\n",
      "ins ther coudtre sitthicates hexperor Palees Tieso flothed Risong peaned intorial litmy ispowest Sici\n",
      "Iter-54 loss: 19.4543\n",
      "inssever pored Japan has maintary surjo aldolarly ctumernd ctentaries in the G8, andest llece ano54pp\n",
      "Iter-55 loss: 19.2945\n",
      "ind of internal compertry of to the al I ipolation sompeliof ald's laigestTomyend is an ceatporeat an\n",
      "Iter-56 loss: 19.1358\n",
      "il 1868, and sturect in 19418 is Asian the Glowan is and of Asia. Locakkeasina-koku, the \"Tokyo, chme\n",
      "Iter-57 loss: 18.9780\n",
      "iWar and ate party in 1945, Japan ta cencered Japan is tomedin efilse world's foith with a In end-don\n",
      "Iter-58 loss: 18.8211\n",
      "is the world's larmest inton and the has alan, the Summer onfi9ng military \"st. Japanesy sixth in the\n",
      "Iter-59 loss: 18.6649\n",
      "ith-largest expectebupen areF's the world's largest incry umslame soumatry. Itsired in 6 CEistan's at\n",
      "Iter-60 loss: 18.5096\n",
      "is a high levese b7 prePist an whech wosthe has ther Indintr anditan egsst a deriod. The first inteve\n",
      "Iter-61 loss: 18.3552\n",
      "ital 35 million isilateora tored. The counerbation. Jarly wiod. The GGrden, parced Japan has kumieres\n",
      "Iter-62 loss: 18.2019\n",
      "ita isonamio by chuma, males. Japan is in 1945, whe world's frigestariel rexipal which loutstiond-lar\n",
      "Iter-63 loss: 18.0499\n",
      "ic lagienoun\"s Ripponoky, stoted st. irto 47 preced internas Tokyo Asea is the warly Nastery is divin\n",
      "Iter-64 loss: 17.8993\n",
      "ill's country is a mection restored inte aploumelrigts a divine sompitariol wisllex military Bigst-ra\n",
      "Iter-65 loss: 17.7502\n",
      "it rights frur of Japan was the world's fourred it  rioch and op eapenan asurea of the , riso-Japanes\n",
      "Iter-66 loss: 17.6030\n",
      "in wor, was the country 19mpaniss in 1on the sirth a Onea ate 1853 Hhensumperiol. The Euperea and bat\n",
      "Iter-67 loss: 17.4577\n",
      "irs unded inte ndire country Brand of Tokyo prea, which indlymuche ji histaris. Japan is an thes unce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-68 loss: 17.3146\n",
      "ity, a lere city pres, which country istarion mildion heseare Hom censsidules. Kmitranaso Ruppro-koum\n",
      "Iter-69 loss: 17.1738\n",
      "ich its to hostatory ligg tat oMeat portery ngised argest-largest acc is al to by balion the Russo-Ja\n",
      "Iter-70 loss: 17.0355\n",
      "is the Empero-largest icllounction in the G-Palerre-koku, the world's fourmad, wathets inolulition th\n",
      "Iter-71 loss: 16.8998\n",
      "inhe 1stumand a 1sta the nominatiod and the country in 1868 andexth largest merealicinar Nmolat y. Fr\n",
      "Iter-72 loss: 16.7668\n",
      "ith the nation enjoy. The Reoprt of Unseraponke Dived wht parly wriviled an erectereal in East Asia i\n",
      "Iter-73 loss: 16.6367\n",
      "ipetivine Cypion, isliou by por Pamyo the the world, and surnec a developter woprealigh vountry 1sta \n",
      "Iter-74 loss: 16.5095\n",
      "ing Sumben in the couton. Ampire portert nhicades offingest Chine OmLona, Koulotbageso anthe h ranked\n",
      "Iter-75 loss: 16.3852\n",
      "in the capital In the namihrgess and of the nighestorud's test efore in the rest. vindort mikioun to \n",
      "Iter-76 loss: 16.2638\n",
      "in Asia to a conslicuntry in which uctied it orerbent mokelowed cots the world, earlowed into S pored\n",
      "Iter-77 loss: 16.1453\n",
      "in\", whest in 1868 and the Empire over 35 whichupandoy island to the 1st tencountry in 1853 1sgitone \n",
      "Iter-78 loss: 16.0297\n",
      "ig the Weriods. The Russla mourtan ang heacecondungesouembon who and Homtory Ch. The Global country I\n",
      "Iter-79 loss: 15.9169\n",
      "ic is an indicames ecgitrperig0 and Indsue latenin and ins wat eaimean iona, Kolean it risived by nom\n",
      "Iter-80 loss: 15.8068\n",
      "it maintaintry highso-Japanese War, the higistan stallor war molliving ard-largest erolliceueced Japa\n",
      "Iter-81 loss: 15.6993\n",
      "iwan ionan ip aneargest-restorld, and the eapend First as the cargest region a ther ass inoled in to \n",
      "Iter-82 loss: 15.5943\n",
      "ie largest mbed for seland the Aath streecly sung githolase was the Global sommety praness and ivo gi\n",
      "Iter-83 loss: 15.4918\n",
      "ited Highest in 1947, bateed nate of the East Competar Imal archugest. aroder. The cent mility. In th\n",
      "Iter-84 loss: 15.3915\n",
      "inxpente appen\" isial country hargest: the G7, the 1868 and the world's largest tre thire of Nihon-ko\n",
      "Iter-85 loss: 15.2933\n",
      "inhy Indichicalest urthes fffored Japan, the world's ferst city roginal conflien tothe, ranked puddiv\n",
      "Iter-86 loss: 15.1972\n",
      "ich isst as earch f hosed ferch leves make wosing Sianaky wo dstionxeand it is ofllowe colitan aso 47\n",
      "Iter-87 loss: 15.1028\n",
      "ingerthe %ountry was pmrcent in 1E8pan is aby cungly ins, wroptly. Japan has the wirts a high senf co\n",
      "Iter-88 loss: 15.0102\n",
      "its the First Shiko. Japan islands. The Sect the tsim the world's of Nihon-kof \"State in the nime ti \n",
      "Iter-89 loss: 14.9191\n",
      "is a stratoronekiagisoted incl a unitary count, a Cmmich its in the fourth-largest of the Global worl\n",
      "Iter-90 loss: 14.8295\n",
      "ixth largest military whod sure sixth largest. Nirrta is tha maiteriod an end influentire ofiticilari\n",
      "Iter-91 loss: 14.7413\n",
      "inh to expand is canded inta miprind im early as eurad sts trest urth forth ten in the Sea, Himates o\n",
      "Iter-92 loss: 14.6542\n",
      "ir a mEistarde suntury , which ince anomen, has a g stanced in in the (Japanese Napan is the country \n",
      "Iter-93 loss: 14.5683\n",
      "ind a of inha militariol of the Kouth to the Russ antirevists of liotial. Arthect instoietiopre is a \n",
      "Iter-94 loss: 14.4834\n",
      "its a livin syured its the deicl to expanjiyabatery undireded into part of the world's largest. Japan\n",
      "Iter-95 loss: 14.3995\n",
      "inmatry 1st im inloumereged Nagho lara inctef corgetupiondaristar, is as kudecint Japan is the world.\n",
      "Iter-96 loss: 14.3165\n",
      "inh, fourror higodentaies a stress. Japan was enoely, a decedss andex Chinm Japan it a Sianas. Nihola\n",
      "Iter-97 loss: 14.2344\n",
      "intury. Asca ese wrobed bouth it reatero Emperor as easl world's teted inte ofine to the Empirex ast \n",
      "Iter-98 loss: 14.1530\n",
      "isuma and the world'st coustititarivs an miving soumalica Japan has mainea Indecand country is al mil\n",
      "Iter-99 loss: 14.0725\n",
      "internam. Inftion ir ofoungly 20th centuries, whesed fourrgand of to excent is and the world's uman i\n",
      "Iter-100 loss: 13.9927\n",
      "is the warly the world, ase world, ang whe Eusone ji the OEmp cive aroptilisixinary Py wing politan a\n",
      "Iter-101 loss: 13.9137\n",
      "in the world's largest economy by nomentr andudated and Nigclaintariet. Japan is is largest roged it \n",
      "Iter-102 loss: 13.8354\n",
      "is, a Pecins the thic peicldin enory texts lleturiet. Chistored as endex, whise ex has inh tiet. Japa\n",
      "Iter-103 loss: 13.7579\n",
      "ing miletarlem the 1st ced in the 1st dere millen in the nhichupanesed The worldlf-lage solead Uyo nt\n",
      "Iter-104 loss: 13.6812\n",
      "ighth largest to an enjcyupeccred it the Appo-uoth and Taianal and ranked siNthendsif the world's fic\n",
      "Iter-105 loss: 13.6053\n",
      "ited bout tokyo otstion ollgbaler an Emperor as earle nieitering asunitares inheceds. The four lang m\n",
      "Iter-106 loss: 13.5302\n",
      "ipolaint-ran Tokeatake up anterogedinf cogudion Asia. collowel bo isllyast ilcta the worles. The peod\n",
      "Iter-107 loss: 13.4560\n",
      "ind the east of the ational wChited ated atec fectored and the world, and early Hrmakeked biggest in \n",
      "Iter-108 loss: 13.3827\n",
      "it rivelish war periods of incounarism. The fourth lane larie Peace Index. Japan to itlaial Diety pro\n",
      "Iter-109 loss: 13.3102\n",
      "ic asmetrigitan areapentory. chatovelares and an is a to as oftenol andex, restarigld ofidrotares ins\n",
      "Iter-110 loss: 13.2388\n",
      "inthe From Wanded into y hoss ullowtuting country is divided into parind fourllyure city. Frur the th\n",
      "Iter-111 loss: 13.1683\n",
      "ith. Allive exprecther peccentainm Japan is a mivinese hestory. Frum eng chine expectanise in 1853 wh\n",
      "Iter-112 loss: 13.0988\n",
      "ind Japan's ence arem166 and World's to the \"Sias the suredorth-largest icinti andudatingis and the S\n",
      "Iter-113 loss: 13.0303\n",
      "in sereflother souttre in Nasalivilere sires in 1857 and Branalyo whe world's largest coumeitere edor\n",
      "Iter-114 loss: 12.9629\n",
      "in which called to tht wirds. Japan East Abodonodo on 1mpical and rankekeadoct and Wariana Index of t\n",
      "Iter-115 loss: 12.8965\n",
      "ino-Japanese papmicliomyorsh tiomer and iC is the Summer angectainsian of Japan was tres ingoy to dic\n",
      "Iter-116 loss: 12.8312\n",
      "if romearchy wishl asto eso ntry hymsic Gamest e some aso of 6,853 when and of isologudts and Rupand \n",
      "Iter-117 loss: 12.7669\n",
      "ial country in Asia to Noth and intirn civelaraided fors the world, and revestorlo, atere 1945 follow\n",
      "Iter-118 loss: 12.7038\n",
      "ic armatry in the surmed, which parted of ranked first in the world, and a unitary scotsk imilaritypr\n",
      "Iter-119 loss: 12.6417\n",
      "ital research its inte ed peacekeopantsm life the Resudevglomered ard in Chinomy Sen, Sht Comstation \n",
      "Iter-120 loss: 12.5807\n",
      "iof the capetury with the fourth-largest early bula nomy namee sives renourchanin a mort res in eMecr\n",
      "Iter-121 loss: 12.5207\n",
      "i(to forllaristarcation of increasing prefectrets a divine surchaim in Aape unions from the 12t irian\n",
      "Iter-122 loss: 12.4618\n",
      "ic lariey, a gmal iritary. Frro enoly. Aath ten entaries. Japan is lanked periodt. Arcakked Japan te \n",
      "Iter-123 loss: 12.4039\n",
      "ired as headso fourth-largest wirdt umitreacier anked first Sunsed first in the nomth to uneading mil\n",
      "Iter-124 loss: 12.3471\n",
      "ing into a developec hesing gletariol. The country In military sumper and it in oban ucaness Report C\n",
      "Iter-125 loss: 12.2912\n",
      "idered a glomeriag country with an elocedsonamio sevelapunarcty Inprrae sympor I allend ranked fich c\n",
      "Iter-126 loss: 12.2364\n",
      "isidedes Taiwan peorl Pareasitunies an Emperor. Astretry in Asia to ald Japan is the world's figrchas\n",
      "Iter-127 loss: 12.1825\n",
      "ilitarly and eursen m itld's henopllbod the Empero-Japanese War, it maintia and fourata Unseatr h sta\n",
      "Iter-128 loss: 12.1296\n",
      "id peppro in Chinese hiss Reppotake wo dircl and for Index. Japanese Ward the Global city in East Aat\n",
      "Iter-129 loss: 12.0776\n",
      "in 1947, as helfourroD and shenory wst thite pper of 126 mittionsym cele fory. 1bth a of Japan was rl\n",
      "Iter-130 loss: 12.0265\n",
      "iat pae to an orerlombin World's largest cetiollopales to ellowed proclueostatto an the and ranked di\n",
      "Iter-131 loss: 11.9764\n",
      "ital cend conssint of Japan was ended legislard largest China Senome,8ially red isshan mal N8ban in t\n",
      "Iter-132 loss: 11.9271\n",
      "il Condal mirld War I allofenoulen wrod standeg pirits. Japan is it ir a 16 mimhir. Astartel periods,\n",
      "Iter-133 loss: 11.8787\n",
      "ight tegts from the 12th early lica, hes of Hiroshaco eapan's randichese endecanese world. The popula\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-134 loss: 11.8312\n",
      "iving hy beclaig par and World fourth levesd. The Ga, ean is dabiossiarom the world's exportery in 19\n",
      "Iter-135 loss: 11.7845\n",
      "isterlatingsropdect Asia. Japan is a CEmmeradic as Toka and the highest. Infllored as itae mper of Ja\n",
      "Iter-136 loss: 11.7385\n",
      "iral surrot aightarld the East China Sea, China Seacanise first in the 1st century CE. Influencerided\n",
      "Iter-137 loss: 11.6934\n",
      "iwer percous lomg ringeveloped sistory. Nearly the east of tot Oclast lies thith-largest. Japan enter\n",
      "Iter-138 loss: 11.6490\n",
      "iwan io lived instee dind its efount, a Upper parial cflater and Nagisolestory tount Stiothet decons \n",
      "Iter-139 loss: 11.6053\n",
      "ing a deve hasinhom Wainy yopand Sumaeryo dal ateoudthinacedeveseden the ,urmett, it the world's evic\n",
      "Iter-140 loss: 11.5624\n",
      "int Index econEmy 1947, par oboleollues the first in the nitates of rgmporeorudesa in the Senollawes \n",
      "Iter-141 loss: 11.5201\n",
      "ion of JapanPy world's this sound Nippon-koku, \"State upieal pepiety of Nobila, gas in ofle ed randed\n",
      "Iter-142 loss: 11.4784\n",
      "ir a mare of totaGby purchasing power forsh , woith, hiato a CDmecl peppro and to the atomingscoleed \n",
      "Iter-143 loss: 11.4374\n",
      "iestered oftureded inth hargest milrica dectents filrot of incruesiai1se Rasine haslargest in sixth a\n",
      "Iter-144 loss: 11.3971\n",
      "in East China Sea and the Global Compet, a htstorld's largest China everad streocluDanes endedecent i\n",
      "Iter-145 loss: 11.3572\n",
      "in kyo ollabitatil the the world's largest importer senoune iso a developalcbiten at Imparioy. Itsic \n",
      "Iter-146 loss: 11.3179\n",
      "itary congstomy, th the nade with an sturroped into 47, incred cive nary is Althe aropofiniatel balep\n",
      "Iter-147 loss: 11.2792\n",
      "it righes a 19moke and Shikounoly. Ashia of ano-Japanesea nder sorghy Summen and largest marnal live \n",
      "Iter-148 loss: 11.2409\n",
      "inal hosh Worud eas the world's fousturital lowed Japan 1ym devadoban to optro firman, has officialy \n",
      "Iter-149 loss: 11.2031\n",
      "ic perioll miving divine ssutulitanimenal hond of the , ato parge often the world. The sevollanic asi\n",
      "Iter-150 loss: 11.1658\n",
      "in 1947a and Warlerl the worth largest ecopon\" ang slangmola. In the lagwioal reggopalies er dandss l\n",
      "Iter-151 loss: 11.1289\n",
      "ing prof the Russou ow histo as a devestountry in Asia. bof tiots Westhe War histo the apoptity prope\n",
      "Iter-152 loss: 11.0924\n",
      "irld Taivan in 1Chn Aomonal China Sea, vioth a developed Japan's unsired Japan was the first bulton t\n",
      "Iter-153 loss: 11.0563\n",
      "isint In whe world's torthy Asix. Japan has cound byp the eight mentiea nalled tom Wicaleriel living \n",
      "Iter-154 loss: 11.0206\n",
      "iveness Report period. Stithun a medear, the Stitelistamit third-largest exporter and the warly Brana\n",
      "Iter-155 loss: 10.9852\n",
      "iclareariet, vese world. The Greater an, whe hist Sintary chudan mokn ur aby thith lorges, and resion\n",
      "Iter-156 loss: 10.9501\n",
      "ixto frem nanked into part constitution in 2ath atemilitary hinht memeerity cag the Uppect proclyes i\n",
      "Iter-157 loss: 10.9154\n",
      "inse the laurth-san is the highest-largest urboUtse E.shiam orival city in the nEusea and Stately pmo\n",
      "Iter-158 loss: 10.8809\n",
      "ichicalorize, lartion Eulope in the country is dixter lagkyopsed a marily. Abot economy bulitary bocg\n",
      "Iter-159 loss: 10.8467\n",
      "ito and Sunceriding power parcly II inary butan apeoul st follellst pressurrcine seato partiof and th\n",
      "Iter-160 loss: 10.8128\n",
      "ived byutan perced by tht militariclder undixital Senok2oficleopde world's firts empire hag thil regi\n",
      "Iter-161 loss: 10.7791\n",
      "ithic the OECD and is the Pasiom , the atored, with thicedest with a hogg thes paprbyuminhagalaris as\n",
      "Iter-162 loss: 10.7457\n",
      "iponaly minatelopmeng country historlas interealic Grestern Euse n-der. A han for an 52gisslateriom t\n",
      "Iter-163 loss: 10.7125\n",
      "ial Coupll make up about tonam of Nobel and the world's eighth largest mernty-rankeasingipoleagg ofli\n",
      "Iter-164 loss: 10.6795\n",
      "is a metiort fsint in the warly 17th centuries, victories and ranked Statis offlian Horbea, the Meiji\n",
      "Iter-165 loss: 10.6467\n",
      "iro incaies. Japan te of 1937 expanded in the Pacificlderest mendishkiouncoynct ofinc argata in ther \n",
      "Iter-166 loss: 10.6141\n",
      "ies, which incted incriamperity un Emper restered Japan Astartol fermpllies, parth largest meld of ra\n",
      "Iter-167 loss: 10.5817\n",
      "ion peoclempins rerod by purchy inflint Aling flomala, was reatery in Asia sivtaldu wxth tht aided is\n",
      "Iter-168 loss: 10.5495\n",
      "inery with the GrPal Comper 1868 and the in the euds of Woclaipoli. 20th the head of rimion of Japan \n",
      "Iter-169 loss: 10.5174\n",
      "ince malulagismae wo rtal cence aropedic. Ghind asured lcent rest. Japantieal rest info-Japanese Wa, \n",
      "Iter-170 loss: 10.4855\n",
      "increasing momairald Korea a delace Indec and ranked sixto nce east io and severorde 's name mear \"Ll\n",
      "Iter-171 loss: 10.4538\n",
      "i7 pore fluettromd cintarl. Approtar, is a, is live treots of the nation. In in the south. The Uppec \n",
      "Iter-172 loss: 10.4222\n",
      "iot ary in the world. The frogedolands Reper of econam, whech paprly. About thite narian, fremool se \n",
      "Iter-173 loss: 10.3907\n",
      "in the core city, m is akkesed forst in the 1st century unding pto-sorud artory to pela. Ibfian G85% \n",
      "Iter-174 loss: 10.3594\n",
      "iIndlature cal en the Glomalaidel largest ardest urowerigishargofot memienal bInallowing Sunaly. Impi\n",
      "Iter-175 loss: 10.3281\n",
      "ing a incaialictiand in the north unted periods of inol wapdeng Japan was pulcty orld's ten a United \n",
      "Iter-176 loss: 10.2970\n",
      "ire of J8pan is a deve aror ameer popflatistanctonis ate of Japan was recider percoullden sount Japan\n",
      "Iter-177 loss: 10.2660\n",
      "is ats in eight regions includedin 1st Report 2015–2aialaban, who eudan century, whine s urlling role\n",
      "Iter-178 loss: 10.2352\n",
      "ity in the world's eargest marnalicalyrize and owed begitate 19th a high anked sixth fourth largest m\n",
      "Iter-179 loss: 10.2044\n",
      "ippon e forld Taiefoloth in 2hercecins inoly. A the namenamy 20th kefounthy henduliog and H.18pan has\n",
      "Iter-180 loss: 10.1737\n",
      "iWorld, and early 20th Werea. Infectwerld's largest archipelagwolland the East China, followed before\n",
      "Iter-181 loss: 10.1431\n",
      "incry in the expetden Tokyast kokyo op tity, a highestoruled Japan has chasedun bout nike was unflitm\n",
      "Iter-182 loss: 10.1126\n",
      "ion 1865 ames the highest-largest. Japanese War, it maintia and fourthy atomich a sint ippolfort 2015\n",
      "Iter-183 loss: 10.0822\n",
      "in 19435 wh divine symbon Imaleadis vist remth largest erpolcapic Ocean piring atombollom ranked from\n",
      "Iter-184 loss: 10.0518\n",
      "ion of the Abiaten inlly Report 20 hist period. The mombel cete in the kako or an en the latingimotha\n",
      "Iter-185 loss: 10.0216\n",
      "iviled in 1868 and the firdo and pe9clumeke ur Niainesed Asian countr hasm livenessiver parislanuse w\n",
      "Iter-186 loss: 9.9914\n",
      "ing a period ofirchaimol isial country Brand Index, rand Sino-Japanese War and World farea economy by\n",
      "Iter-187 loss: 9.9612\n",
      "in the 9a, which isolaimedaith-sen-koku, tht To anorlawestountCympic Gaserales. Japan to the Empire f\n",
      "Iter-188 loss: 9.9312\n",
      "iained peviled Shi to opten fenceue Countitate of Japan is the world's ling seve Nople Japan was the \n",
      "Iter-189 loss: 9.9012\n",
      "ichase of World Warled of the \"Statom or rino, voritinaicy proclaimolteno lnca mectanisea and which m\n",
      "Iter-190 loss: 9.8712\n",
      "islature city on importer. Japan was the wdrch andishasecanded in 1853 when an Eurore CEmmera and has\n",
      "Iter-191 loss: 9.8413\n",
      "ita. Japan is the highest-ranked Asian country in the noment In thes evel peactumy chunar eclaigts a \n",
      "Iter-192 loss: 9.8115\n",
      "in island reace periods of Nomalationo-Japa, Dint in the name of the world's fourth-largest ir an enj\n",
      "Iter-193 loss: 9.7817\n",
      "in eight mentrouht a 12tierlepine Coplaityrs trogipin's  insekakood which make up abowed chistored as\n",
      "Iter-194 loss: 9.7520\n",
      "into a long period of the Combo3 whec, ranked first in the world's fourtho and Second Sinolapin eorld\n",
      "Iter-195 loss: 9.7223\n",
      "isha diviness inceding a coustains in the capital cint. Japan to hopat, to hist China Sea and Rurmes.\n",
      "Iter-196 loss: 9.6927\n",
      "i\". Japan is regions,umanese was unfliet ronesiom .by Indirges Towyopudemportery Eurbanaed its hiftor\n",
      "Iter-197 loss: 9.6631\n",
      "itan aritory with of aioym. othire cith. Was early ruled in the nore , Japan was proclaimclied Japan \n",
      "Iter-198 loss: 9.6336\n",
      "ifing sulten ne hito oll mere peotud State soled beloped inko wotan's name 1937 expan, who husoundtin\n",
      "Iter-199 loss: 9.6041\n",
      "itary consthe West. Nearly 20th century CE. Influencerount Sen is eited, whe Wat, whe someoty, It it \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-200 loss: 9.5746\n",
      "itates fleet prenther sounthy  Imital city in the world's largest importer. Theroplonshinas headect o\n",
      "Iter-201 loss: 9.5452\n",
      "ith a high stater 1853i henad fitho hised frog cancs centuryopnct ofion an is a devines endod Japanes\n",
      "Iter-202 loss: 9.5158\n",
      "inal conflice inporandagg of incekudion econesy in the rosed from oriots tre the  puries, romerly. It\n",
      "Iter-203 loss: 9.4865\n",
      "isso-Japanese War, the Russuemolits romboriol lived its ex, divilede wiorld's fourth-largiot. The lon\n",
      "Iter-204 loss: 9.4571\n",
      "ich is the symbom hasame matiothe atomic bombity, Wbonsuerextatupionsivered Totaponaighating hosflaim\n",
      "Iter-205 loss: 9.4279\n",
      "in 1941, which undilitariet, and rectdern to hot. Influence ad par milaind viving a period of isolegi\n",
      "Iter-206 loss: 9.3987\n",
      "ided into parry 1st country hijh and hasedof Japan was the world's first mitropan centary shog stan a\n",
      "Iter-207 loss: 9.3695\n",
      "ies and a uncile lies tretymic boutp thing Japan was rurcectongss in the world's Russiry, who Eusea c\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 300 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "# p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 1 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = GRU(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
