{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         q = 1-p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "        h_ = (h, c)\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        \n",
    "        if train:\n",
    "            y, do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache\n",
    "            )\n",
    "        else: # train=False\n",
    "            cache = (\n",
    "                X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "                ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache\n",
    "            )\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache, train):\n",
    "        if train:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache, do_cache = cache\n",
    "            dy = self.dropout_backward(dout=dy, cache=do_cache)\n",
    "        else:\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t], train=True)\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer], train=False)\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 90.6763\n",
      "ekenm e ghiso the unth ip9. morol lasn xn my paran .patt The misSie SpethedConnfe. shet wot rJarcon(p\n",
      "Iter-26 loss: 82.4098\n",
      "e: whed uO, wofsusly maesto Gpast Japelid thisy m orery courcond the Ies mres omtontlore patitan ind \n",
      "Iter-39 loss: 83.2661\n",
      "et. rs tht JapancoHofalaty ury mpuretor ponrok fsol tho te9omd ssit Scexthe de thi G8te merth and is \n",
      "Iter-52 loss: 76.6612\n",
      "e hicerinxist ryun rom pory eatd in tarte ixttreth In Jatin i scoat poplaceteoudesuthe Astacarcesed t\n",
      "Iter-65 loss: 77.0350\n",
      "e;is pan, is toces oba n'se the ann opand wrond whe ap an the UCs and cor at ann beson aidd's yrncoGe\n",
      "Iter-78 loss: 67.0331\n",
      "erybo, histou the of the hily. Wor Wte esity lort tho 182. wares of inchase tobonse edinabe toked wer\n",
      "Iter-91 loss: 67.9321\n",
      "e porlitoChe is saryt Semoulorelygr fithe usincoitign. Thet ompllour corgopatedtino anmion 4mperkun c\n",
      "Iter-104 loss: 68.3487\n",
      "ed denowthitasian Japa1 \"ss arly whriro, tary whisa in. The Filed's wollectes-and winth in cand, A  o\n",
      "Iter-117 loss: 70.9720\n",
      "e unatery. umpanly is theodicielagithisa fomyerlargelsthe of inthasulstallryacthilasion Sopanad'ss ir\n",
      "Iter-130 loss: 82.2560\n",
      "e thereasan \"a dh histres, and glerenjEfses S thi wh igony. mesedenI hesuthunsonalealichiryeto. wr rl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM at 0x7fc2a1567630>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2dc511630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
