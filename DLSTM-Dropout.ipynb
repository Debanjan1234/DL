{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "\n",
    "    X = np.array([char_to_idx[x] for x in txt])\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters wights and biases\n",
    "        Z = H + D\n",
    "        m = dict(\n",
    "            Wf=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wi=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wc=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wo=np.random.randn(Z, H) / np.sqrt(Z / 2.),\n",
    "            Wy=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bf=np.zeros((1, H)),\n",
    "            bi=np.zeros((1, H)),\n",
    "            bc=np.zeros((1, H)),\n",
    "            bo=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D)))\n",
    "\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "\n",
    "    def initial_state(self):\n",
    "        return (np.zeros((1, self.H)), np.zeros((1, self.H)))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wf, Wi, Wc, Wo, Wy = m['Wf'], m['Wi'], m['Wc'], m['Wo'], m['Wy']\n",
    "        bf, bi, bc, bo, by = m['bf'], m['bi'], m['bc'], m['bo'], m['by']\n",
    "\n",
    "        h_old, c_old = h\n",
    "        X_one_hot = X.copy()\n",
    "\n",
    "        X = np.column_stack((h_old, X_one_hot))\n",
    "\n",
    "        hf, hf_cache = l.fc_forward(X, Wf, bf)\n",
    "        hf, hf_sigm_cache = l.sigmoid_forward(hf)\n",
    "\n",
    "        hi, hi_cache = l.fc_forward(X, Wi, bi)\n",
    "        hi, hi_sigm_cache = l.sigmoid_forward(hi)\n",
    "\n",
    "        ho, ho_cache = l.fc_forward(X, Wo, bo)\n",
    "        ho, ho_sigm_cache = l.sigmoid_forward(ho)\n",
    "\n",
    "        hc, hc_cache = l.fc_forward(X, Wc, bc)\n",
    "        hc, hc_tanh_cache = l.tanh_forward(hc)\n",
    "\n",
    "        c = hf * c_old + hi * hc\n",
    "        c, c_tanh_cache = l.tanh_forward(c)\n",
    "\n",
    "        h = ho * c\n",
    "\n",
    "        y, y_cache = l.fc_forward(h, Wy, by)\n",
    "        h_ = (h, c)\n",
    "\n",
    "        cache = (\n",
    "            X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache,\n",
    "            ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache\n",
    "        )\n",
    "\n",
    "        return y, h_, cache\n",
    "\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, hf, hi, ho, hc, hf_cache, hf_sigm_cache, hi_cache, hi_sigm_cache, ho_cache, ho_sigm_cache, hc_cache, hc_tanh_cache, c_old, c, c_tanh_cache, y_cache = cache\n",
    "        dh_next, dc_next = dh\n",
    "\n",
    "        dh, dWy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "\n",
    "        dho = c * dh\n",
    "        dho = l.sigmoid_backward(dho, ho_sigm_cache)\n",
    "\n",
    "        dc = ho * dh\n",
    "        dc = l.tanh_backward(dc, c_tanh_cache)\n",
    "        dc = dc + dc_next\n",
    "\n",
    "        dhf = c_old * dc\n",
    "        dhf = l.sigmoid_backward(dhf, hf_sigm_cache)\n",
    "\n",
    "        dhi = hc * dc\n",
    "        dhi = l.sigmoid_backward(dhi, hi_sigm_cache)\n",
    "\n",
    "        dhc = hi * dc\n",
    "        dhc = l.tanh_backward(dhc, hc_tanh_cache)\n",
    "\n",
    "        dXo, dWo, dbo = l.fc_backward(dho, ho_cache)\n",
    "        dXc, dWc, dbc = l.fc_backward(dhc, hc_cache)\n",
    "        dXi, dWi, dbi = l.fc_backward(dhi, hi_cache)\n",
    "        dXf, dWf, dbf = l.fc_backward(dhf, hf_cache)\n",
    "\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "        dh_next = dX[:, :self.H]\n",
    "        dc_next = hf * dc\n",
    "\n",
    "        dX = dX[:, self.H:]\n",
    "        dh = (dh_next, dc_next)\n",
    "\n",
    "        grad = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "\n",
    "        return dX, dh, grad\n",
    "            \n",
    "    def train_forward(self, X_train, h_):\n",
    "        ys, caches = [], []\n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append((np.zeros((1, self.H)), np.zeros((1, self.H))))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h_, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        #         h_init = h.copy()\n",
    "        h, c = h_\n",
    "        h_init = (h.copy(), c.copy())\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init)\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8 # const epsillon\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No batches or only one\n",
    "        # Minibatches\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            # Updating the model parameters\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items, dict={}\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "                \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=mb_size) # time_step=mb_size\n",
    "            print(sample)\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-13 loss: 2.9776\n",
      "epanthed Wiog  os as cou aunx ke centuer Janecrtangd oar thcde ans mictons Ne coor an awrf xalind  an\n",
      "Iter-26 loss: 2.7111\n",
      "ee i9 Catin uoncof ietses itonand Werte sertrcangain, Aof wouvilean, Japandin an't1\n",
      "i8nd cospgpang Ja\n",
      "Iter-39 loss: 2.5762\n",
      "e, con838. Iny the The tarles cokeat pale Eisturty sh 1lsstr'n bee intor an mamuem pestre peree lar, \n",
      "Iter-52 loss: 2.4703\n",
      "ean, tistestee Apiby thes uhes\"tr arake the 1a8 47 iont8) e it ontar hoced's who Chens and Japaeandis\n",
      "Iter-65 loss: 2.6293\n",
      "epa an -erned tho Japan oufar Japa ahkartary Cha aucaratecal lars rhust eann liche sinal unaeves menr\n",
      "Iter-78 loss: 2.4875\n",
      "ey Japancht the with thu ghestala, of 1kPe conicond Nerla ofte fartthist gapit ine of the of cowhint.\n",
      "Iter-91 loss: 2.3760\n",
      "e paperek, The Japaneng cpeesion of thy A3f fif wiche panare the Chian-larec Seation if tount moprlgs\n",
      "Iter-104 loss: 2.2974\n",
      "es Aapente in peate opel Friisty Ss murrourlyen world is cecomerthe mapen. Japan in nar aled ji-weren\n",
      "Iter-117 loss: 2.2525\n",
      "ed wokud whist. Japan t iiganpbo latiy in leand wo th coukhictd in tokem in thk O7ttricured (rothe fo\n",
      "Iter-130 loss: 2.2234\n",
      "en'g forllyur restho cowas prwere-Icpaigeshentue Toe watin hawins is the the Uf'd flduring the shunt \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.LSTM at 0x7fe4d1bd7390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 130 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = LSTM(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nE0JIgKDSJ4SQAPH/AdGTLckx9/J7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe496a3f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
