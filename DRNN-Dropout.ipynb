{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)\n",
    "\n",
    "# # Data exploration\n",
    "# X.shape, y.shape, X, y, txt.split()[:2], \n",
    "# # set(txt), \n",
    "# # for val, key in enumerate(set(txt)):\n",
    "# #     print(val, key)\n",
    "# val2char = {val: key for val, key in enumerate(set(txt))}\n",
    "# # val2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for l in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "\n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "    \n",
    "        h = X @ Wxh + hprev @ Whh + bh\n",
    "        h, h_cache = l.tanh_forward(h)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        cache = X, Wxh, hprev, Whh, h_cache, y_cache\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X, Wxh, hprev, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        # tanh\n",
    "        dh = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        # Hidden gradient\n",
    "        dbh = dh\n",
    "        dWhh = hprev.T @ dh\n",
    "        dWxh = X.T @ dh\n",
    "        \n",
    "        dX = dh @ Wxh.T\n",
    "        dh = dh @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(y, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                \n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss # + reg_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            #             loss += loss_fun.cross_entropy(self.model[0], y_pred, y, lam=0)/ y_train.shape[0]\n",
    "            loss += self.cross_entropy(y_pred, y) #/ y_train.shape[0]\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            \n",
    "        for t in reversed(range(len(dys))):\n",
    "            dX = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dX, dh[layer], caches[layer][t])\n",
    "                for key in grad[0].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                \n",
    "        return dX, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            y = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(y, h[layer], self.model[layer])\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle as skshuffle\n",
    "\n",
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    if shuffle:\n",
    "        X, y = skshuffle(X, y)\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        #         if idx >= len(minibatches):\n",
    "        #             idx = 0\n",
    "        #             state = nn.initial_state()\n",
    "        #         X_mini, y_mini = minibatches[idx]\n",
    "        #         idx += 1\n",
    "        minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            dX, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-130 loss: 4.9230\n",
      "canemian inh ord lepged,. flital Gsenaty Waris-kevit as ounest einbin reand anksl aistybort th th iny\n",
      "Iter-260 loss: 3.7493\n",
      "catitn, Chvwan mofturtht Emperon .5 tity eati ombin 1868, ctpon-redla, was resc exety. Sh tokym ind i\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1300 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJyJKIf8fs2fcxYRJqcUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff687f08940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
