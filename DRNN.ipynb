{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "    \n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "        X_one_hot = X.copy()\n",
    "    \n",
    "        X = (X_one_hot @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(X)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        # Dropout for training\n",
    "        if train:\n",
    "            y, y_do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X_one_hot, Wxh, hprev, Whh, h_cache, y_cache, y_do_cache)\n",
    "        else:\n",
    "            cache = (X_one_hot, Wxh, hprev, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        #         if train: \n",
    "        # Backward is performed for learning/training and not testing or validation.\n",
    "        X_one_hot, Wxh, hprev, Whh, h_cache, y_cache, y_do_cache = cache\n",
    "        dy = self.dropout_backward(dout=dy, cache=y_do_cache)\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        # tanh\n",
    "        dX_one_hot = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        # Hidden gradient\n",
    "        dbh = dX_one_hot * 1.0\n",
    "        dWhh = hprev.T @ dX_one_hot\n",
    "        dWxh = X_one_hot.T @ dX_one_hot\n",
    "        \n",
    "        dX = dX_one_hot @ Wxh.T\n",
    "        dh = dX_one_hot @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy() # the output for the previous layer is the input for the next layer\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t]) # train=True\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy() # The input for the next layer is the output for the previous layer\n",
    "            dXs.append(dX)\n",
    "            \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No full batch or files\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1000 loss: 4.8588\n",
      "craite tand noevo ntittlit tericentiss rym in mstreg. rcoworusko-mamandit res anountiyelSrg lurekt us\n",
      "Iter-2000 loss: 6.7332\n",
      "casi edean  ee the Eu iat and eses ury \"9 ha as map G8, and on simenathineilo1935 aotvekt is an iast \n",
      "Iter-3000 loss: 2.8752\n",
      "casi ee amist ans thx t ietounhuo us aur d T8. itithan erectreon putrrcgsothe nixsta enati iterthi Ec\n",
      "Iter-4000 loss: 2.6558\n",
      "ccite nertert inf neritd wurut  ims redthe fircon mama Gre tevelipcrcelied, ws sS art terctsinch hat \n",
      "Iter-5000 loss: 2.5364\n",
      "cins iod t rld ant e ren eo penou an日, the wteste wertt cin tiei s theto narinstary irghtr emPted n t\n",
      "Iter-6000 loss: 2.5909\n",
      "constitution ingcoe ecoloma, wot 日本 Nh lo ncry, the et hul iampat  ivins Jacan paleed endEd stied ih \n",
      "Iter-7000 loss: 2.4690\n",
      "ch includes rhentean Olimprop endite ihi ontu Tiegsel ogen, ane ha mee the icl teetrly in torporthe W\n",
      "Iter-8000 loss: 2.5145\n",
      "creclit ligite es erelirinh oast e pin ingren Estvrostare Uarloc ane oh io peotha, share emrcinita ie\n",
      "Iter-9000 loss: 2.4730\n",
      "cted legisl ame l rheli te the5s elant I trelenes\" an iop Fimin ropth ime tevt en 1. Japan is tieotht\n",
      "Iter-10000 loss: 2.4348\n",
      "city proper ihtan ietreas ha mper ored shen ure Brtnainy ins it extai3 hest m. The fieatiliteti imgen\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4P8DZSG/1OfRN/4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10631c0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
