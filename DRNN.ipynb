{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        h_in = h.copy()\n",
    "        X_in = X.copy()\n",
    "    \n",
    "        X = (X_in @ Wxh) + (h_in @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(X)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        \n",
    "        cache = (X_in, Wxh, h_in, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X_in, Wxh, h_in, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dX = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        dbh = dX * 1.0\n",
    "        dWhh = h_in.T @ dX\n",
    "        dWxh = X_in.T @ dX\n",
    "        \n",
    "        dX_in = dX @ Wxh.T\n",
    "        dh_in = dX @ Whh.T\n",
    "        \n",
    "        dX = dX_in\n",
    "        dh = dh_in\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy() # Output of previous layer == input of next layer\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy() # The input for the next layer is the output for the previous layer\n",
    "            dXs.append(dX)\n",
    "            \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 96.4800\n",
      "et in b ei in'tnph  5i twoecwrdpef  g t o  fdooee fdriJWtso phe8tA.rns ryhitdneuet teNsaCoe ree ies R\n",
      "Iter-20 loss: 85.9331\n",
      "e8suntes ind inrtd ice waasriat rale orrboi tau cDere1tpe woee coegtkc bas if pas the ponbtfe oIet2 \"\n",
      "Iter-30 loss: 79.9708\n",
      "eipaion ion angesthTmlabpes inCGSnone ad ilatre knban c9apth oc abtipe the ion eslyB5rod war en ansce\n",
      "Iter-40 loss: 76.6933\n",
      "e tovand Japan 1ip hivn my ururat Hhinssldcoelat as ang the ry iany h, Soand an't vithe Japan und,. S\n",
      "Iter-50 loss: 73.8736\n",
      "e3; oictupaod of ias tures ioeaiveethr 18C, opdrisdowthe worl. Japhe,rof the Ayore, bofwor dixthe wer\n",
      "Iter-60 loss: 71.5793\n",
      "esaco wan iisotry wveit outhe Oicone O7eart of faty a Son, whK rfe1che Gnuleec-liol, The an The with \n",
      "Iter-70 loss: 69.7502\n",
      "estint. arelee icetoriladd Luben ke pestiwha a ara'iffGamalef ioicin smarte ry sopersa, wrmalested la\n",
      "Iter-80 loss: 67.9511\n",
      "e: The world in exies afd an's estoroily. inpewod the Oorlof an nas of tis 1imu Japanth coukoc Wins N\n",
      "Iter-90 loss: 65.9842\n",
      "en inoti\"d ig the Memel inlte roml-bovenory linithe wared, want The wan 18. heimeled iodes the macand\n",
      "Iter-100 loss: 63.2548\n",
      "exderonte of Japan 29 lithed Gmintronsof Treocoflyjh ingthof itrsy. whomul ou Eas chmind ion, Ch lagm\n",
      "Iter-110 loss: 61.7068\n",
      "ena catas ind Torias ra andexevrld incl parenc2I Eemel conon the Eand Japan ton tiha r, assoleaturea \n",
      "Iter-120 loss: 57.7694\n",
      "ec urgho, th  Raked Hinat iof asg thi sopenthe estrot ind Emllegrt, mamecand muleded Aiin, 17th tho E\n",
      "Iter-130 loss: 54.4914\n",
      "el ofolthed fof the horlath icocthigasl ina, wod e kis ted the fiminnd f, roSti, the Rivere 3iconale \n",
      "Iter-140 loss: 53.5411\n",
      "eke turt hidas wove igo Nithereditx iauthe the wor ist inony 20ty ofont intcof Sim. A\n",
      "pstate asded be\n",
      "Iter-150 loss: 51.5567\n",
      "ed ourthia ferpoxatxiss. mekede tin the tentcr ing of is 1h4) Worl-'ar top 126s wodeon la ind e ony L\n",
      "Iter-160 loss: 48.5703\n",
      "ecopeand Ts irc\"labes evelory. 17in porpgrctle mife eme oures ingestiriand on toe es ing Was warll ar\n",
      "Iter-170 loss: 48.2979\n",
      "es of the Ufredged pare and thi eityes med aino Eunntredwev Dromll\".an. Japand on-tofopomalicalicand'\n",
      "Iter-180 loss: 45.6362\n",
      "ed feacen. S an1, which Lat l-kokh gEmbith an igh obobasgint e pectly, bagest Phlatgeg, the f Srontty\n",
      "Iter-190 loss: 44.9669\n",
      "ec2udmpincokamd Nfrand, a xurla, ome Iaby I, tho Empitino, whith  ven.ip pporle encemal torld imd of \n",
      "Iter-200 loss: 43.3688\n",
      "ed onuthecoryo, , uron meke fitity, vedirig ce sexen bombof)Gs atriok\"o wen ehe Guritco iso aa este t\n",
      "Iter-210 loss: 45.5378\n",
      "en an ias lomity insty Woparisinand ofsexead pantedis mintseve oro Da, the Oapan's nonut erowtect in \n",
      "Iter-220 loss: 40.9023\n",
      "eUnisns ouwor lard cinit rombol Nigitsu, woxnu hecturin . Shed coron's arenlig mimenoku Eu, wod on th\n",
      "Iter-230 loss: 41.1174\n",
      "ed Japan suvice, thec. Abop og ha llallihigs \"lomokyo W日s S2a fop the Imaliry a devito the wisint ive\n",
      "Iter-240 loss: 39.3898\n",
      "en, fomin p oumelod ssreh ha treglity Ny Japan hon Locanopen malesite as ofomalloe emprriand callate \n",
      "Iter-250 loss: 38.7852\n",
      "ed Arian Embort-torld conind Seveleang ducthesangicate pevelef isto-lary.t. thechessanoenth. Sh peran\n",
      "Iter-260 loss: 38.3967\n",
      "es ofp'st oleengest Phestiolland merofount. porland ination 1818se teon Empirse fowes, la, the counti\n",
      "Iter-270 loss: 37.2389\n",
      "ek ire ap mithe 19te tio ty. Wan the bopllino fesint Japan in an reatith anded hesen isa dvo torte pe\n",
      "Iter-280 loss: 38.6338\n",
      "e Ristowliwisu areand firrecion owgesta churectmon mopeter\"senjuden lourtwel aid catee porloworgics i\n",
      "Iter-290 loss: 36.3330\n",
      "ex. Neapror as anal ina lyI Pechof 18th  Saina, 6sumalicandsinto  momethin aivina Sea ahe wom lare of\n",
      "Iter-300 loss: 35.4972\n",
      "ed ftrot ent rfon tit gd forot Elgiss ereanis and Tared whi wisily Srana ireCSich. chenarge the Gamem\n",
      "Iter-310 loss: 35.8829\n",
      "e: 日本 Sha ina ivikeoy ing an tirntand meceapecmbits. Toe and is peo tu, as unf ferdon tigin ky b en d\n",
      "Iter-320 loss: 34.4756\n",
      "ec upurcelyald, the sorlo an, balito ly OnChithicCteness istaltes ended Japands, isereaik on es and w\n",
      "Iter-330 loss: 32.9981\n",
      "e Aupanji ssof terllgita e of anith he counl pombtu counan ma ify, ontsilas of the Empiry an thir ari\n",
      "Iter-340 loss: 35.2415\n",
      "e copen, picctrelalal wirs ian Em, wcone it rrrand m inler, rsmate promlicy, Cha in mokeod the Glaras\n",
      "Iter-350 loss: 33.2537\n",
      "ed fophocaligithit Horat GLalimalu, of ihe 9s19th lictof eseal langentircon, Japan hassevithis  are f\n",
      "Iter-360 loss: 30.7567\n",
      "ed lerongind S and t minecougss raxtarlad of tho Efomitc-taly and the Gfoussturial. Se parcobount,y, \n",
      "Iter-370 loss: 34.5568\n",
      "es ofistare and fevinokd vedece oe tar kevedof 1har and finto and Sudin migetom lare sihwhich. 17thy \n",
      "Iter-380 loss: 30.6361\n",
      "ef efonsus of the portd, the G8malalat the purt -iostubity angh and chudecflior as isolodalititles wh\n",
      "Iter-390 loss: 28.3946\n",
      "ec leallig grsiowcolith a d of is8,8625 haitopan Emparient. The porth laome war ory. whin thim pare t\n",
      "Iter-400 loss: 30.7750\n",
      "ed fousthe Olorten tc puris indGr aintt ind She Empere oth oustestin ly bous ow. Tolyo cmpmritis as i\n",
      "Iter-410 loss: 27.4676\n",
      "ed fturth laky. 17th iall ain tixthe worl. Im in tislopoungis eanes frperithe fithe aLvedecturea, \"s \n",
      "Iter-420 loss: 26.9519\n",
      "ed first inat ald an uncendin tintsingsored Neake Up mber wid sy conotuke , vared wo he foure Astioww\n",
      "Iter-430 loss: 29.2096\n",
      "ed Aromotpenpirt in mity muly ihica. Japan hase with larlowo, csrlong, the of Nohlla, its iof.rehith.\n",
      "Iter-440 loss: 25.1794\n",
      "e, matnlad War, Japan ha wsol toresed, th  hegestist rasedest Indeiction in tory Chint Inatisity ists\n",
      "Iter-450 loss: 25.6725\n",
      "e Paping an the nomitr. ma eavily as the Sion. Iules ristar al Olotnt Sis and ramith mek ou lar on th\n",
      "Iter-460 loss: 25.5928\n",
      "es porld and bargest ane gsy ese ive ep ry coty ty ioflacenisea mekyo, whichlian a. Dioftertiol Napen\n",
      "Iter-470 loss: 24.3982\n",
      "e wort Cho esedexpacend, thec moky. Aroh mat Japan Sla ind iondsbif perto iont. Wertinand Seake boly.\n",
      "Iter-480 loss: 23.4364\n",
      "en tom. comptrese, theld Oismichesalfearc higEn\", conod courto . The wofld's tey. Hutsou es ecoloky b\n",
      "Iter-490 loss: 28.0061\n",
      "er agd the G20thl Oistla area, westligesturc torld ate ory port en\"u ar forecl. Tbidsthe GmmbmekD rag\n",
      "Iter-500 loss: 22.5134\n",
      "e ourgh mekeare 2u hethed las war ion, pante Gr agd the firstsinted istorcanden moleragisd mulerealdo\n",
      "Iter-510 loss: 21.6994\n",
      "e turth-lagmetce iso-levelion the counsic an wove easin pame eseon-lagins of pered, andtomuked popio \n",
      "Iter-520 loss: 22.9645\n",
      "ed furso-Japa of-turtoin. Japan's landeccedrsonom eapan es ontred surithe ceopecilitand Smurearly 17t\n",
      "Iter-530 loss: 19.5554\n",
      "e Rusto thigage tienG highla, withila, minthe Warl Chulinged, the G mothese. Japanes of tor en liof 1\n",
      "Iter-540 loss: 20.6402\n",
      "er aid the Emperol ung testepren s vereoper Ocfore is iongas of then. D eratist expored fint rigit pa\n",
      "Iter-550 loss: 19.9033\n",
      "es of the Emperiok implaper ard and reatorlacchis-larg on leatola, DS andents mered mo eawit pand She\n",
      "Iter-560 loss: 18.9611\n",
      "ed fouro a, Wartha Rirmat inC. Thinand tint of Torlowe the Seall, andesered f the t Tilard seacemime \n",
      "Iter-570 loss: 18.8578\n",
      "ed ofokhe hasmlacony, was ina, Chine, shame feron Gr Nigin-kuricowe ene or and d Oceand cturerialin t\n",
      "Iter-580 loss: 18.5960\n",
      "en tomlligits The ofed the sodmitits d pron Ea tintir and Toky. Acon Embaly. Thi end omeld frus finse\n",
      "Iter-590 loss: 17.4623\n",
      "e cerind is to tre pero-torldite the Ealtiect olower, ss areciaky 9 tin Gxmbokulad an makye Alde the \n",
      "Iter-600 loss: 17.1973\n",
      "en urtand redith the fou, calledist ratit p of te plop of Nserod the wict of ofolatits . reard gintir\n",
      "Iter-610 loss: 23.1861\n",
      "e coroky. Asia tory. F oro\"lagiof or 1to the \"atund rndis in and War of inal tomuthe pofl. Wex, stity\n",
      "Iter-620 loss: 17.1216\n",
      "e, ffr,owas enfecien to isy ffAcof. Jadin citinkio sunceneas en the Gouthe serto lobol ind 9as Pichen\n",
      "Iter-630 loss: 15.3177\n",
      "e turticlapiol of Japan calesa mrlades the warl the Gambe pore troun\", ssdihes ofkosu ho EIsade Chisc\n",
      "Iter-640 loss: 17.0563\n",
      "ece ouste dint conkaly as oforigcg, an tom teoko uland miledith an hionala Sea, Jaden ion. Ty iont po\n",
      "Iter-650 loss: 14.7583\n",
      "es of the Nigost alea. citstake War and Naping fon ontary i, the Parleadmithictir wsoul war milldisg \n",
      "Iter-660 loss: 15.5757\n",
      "es of torllam east Cha gro5 tha wopld he Eant wos the Noflsalcearid t eisto iok of which baky. 19tind\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 17.9075\n",
      "er and War has mitetok oun\". The narltow in lhin-lart as ictery pormetop ofod Is es makd Na ushih- Pa\n",
      "Iter-680 loss: 13.6109\n",
      "e tortn tivilaky ry iest ox.mboly. Ase Chon-lam me omom 1947, parly I an polteint  hokaky hith the Eu\n",
      "Iter-690 loss: 13.1119\n",
      "es oppanieas ceate of Westurle Wart aseviremper en in the couns, wro, thegestoun Emperorlo adGD foper\n",
      "Iter-700 loss: 15.9606\n",
      "e ofuthe, perpon eapitx. Japans of isstho wec limperion 9by natledirsd siand Chinas the fored's minic\n",
      "Iter-710 loss: 14.7969\n",
      "ed and the of Japan's largeg\"tureainto , Jadan ca ingist of 1sth tered colitan is laranded Noponterd \n",
      "Iter-720 loss: 12.3758\n",
      "ed feichlicillaits d Japan tos tolod Nistnst histirl insin tionte tec liod NLathen isg astand whis to\n",
      "Iter-730 loss: 13.1916\n",
      "el dira, Askad s8,20whithead the Emperec Empirsoute popues of inteitand RisWailand hirlad sxpertor en\n",
      "Iter-740 loss: 18.0034\n",
      "es of torledin nd Japan tof ie par and Suder.in-kee ker llamete porto tog 18, meneiry an colits ry Ho\n",
      "Iter-750 loss: 13.7036\n",
      "e, aul cimbte tun ex insly bopal Diof. Thessured an the narla. The roputsy ist en in 194. Toglound bo\n",
      "Iter-760 loss: 11.8570\n",
      "en urtace rrowain im riand is tely ind colomit tor of the nwap re aictuns fircona, ching fres foest p\n",
      "Iter-770 loss: 11.1679\n",
      "ed chedecalio E-Japanit f9pary, which wedald an thinMiin isinas mupored chuman estentin th torld'g fi\n",
      "Iter-780 loss: 13.5696\n",
      "es opprcelde destest rowoth is and leake u, the warl o858, \"S an rowas, waste tarid caldes ichasGat a\n",
      "Iter-790 loss: 18.1100\n",
      "ed figstut ared in the regund Shiry 17t nof urea devedere turing on un end colegigud th larled lipind\n",
      "Iter-800 loss: 11.4734\n",
      "ed an mok6 amm ins, venturithe carled in the cound ia the angest oloregly, since East int of parly 17\n",
      "Iter-810 loss: 10.2744\n",
      "ex. Japan ca sexpecand y Chenthe Rurol-larinop huratin kioy as esea an he and the Emperor as a Siny. \n",
      "Iter-820 loss: 10.6449\n",
      "ed and ohitand te tee, pectien 9s ropaiciln, whith d Homatly. The 1umbea ury 185s acea. phesenpent of\n",
      "Iter-830 loss: 10.5731\n",
      "ex. Japan is hand, as and in the nfurestevinalita iny Im in Gighgs and coletre es whist in the Rssent\n",
      "Iter-840 loss: 10.7377\n",
      "es oputbe pont ro-kok3Nimingr westerof irsea, the Warllaicene aingh seatd porren th mekfory. Allad ch\n",
      "Iter-850 loss: 8.9294\n",
      "e ofetstiry lagestin is anded stoperba wan livin-lariona, China, voned an the East China Sey int rigs\n",
      "Iter-860 loss: 11.0516\n",
      "ed first in the counten, pe porld thitallig onfereatar as ofe copltie ise. The combtiy sun the forost\n",
      "Iter-870 loss: 11.0960\n",
      "es oppecelion th inaty rsucent ryvint. The ferjo anchunUk orod cal ofetsulit. inC. Thigargy weved ort\n",
      "Iter-880 loss: 9.3276\n",
      "e Seren, Huminl ivkee archand. China, Ky, huthece fopon cicitx. Inled inf for ore the worldas ofobe7,\n",
      "Iter-890 loss: 10.8768\n",
      "ed and the nat on-largins mekere Eestryrtho anded ty is esuricy, China whist anes. The first I ared c\n",
      "Iter-900 loss: 11.3514\n",
      "ed fmurh-tarlgr cins Shicheseligg the Russtilicand whis terigist ad canked pith abaly ig toke upic py\n",
      "Iter-910 loss: 8.6180\n",
      "es oppecelion mi inst ander ind seaculation-largsta an parl was leateo tulad ludicc iwforo the witht \n",
      "Iter-920 loss: 8.2259\n",
      "ed an humbke und wailatie pectiog an the eatin-, 6smaldithe Niangentiry sturesy fito-laritity in Asia\n",
      "Iter-930 loss: 15.3785\n",
      "e wer dis in the naml omatly, chech oventies endist readelid withe and war, as cofetred whis as ingec\n",
      "Iter-940 loss: 8.4860\n",
      "er and Worle, onm. hisssreatititita es fors. Whi and forla, and the counte ty h mecelig tar churedion\n",
      "Iter-950 loss: 7.7211\n",
      "ed foure Adicturco smomathe Comlit the nound Tokiod 9a, is poredict engist Sunk-kuku, which canes are\n",
      "Iter-960 loss: 7.5933\n",
      "es oppored on en the Uppercaling iste win in an Chine, ond chureaterign ciunt pyored ia la soxthisen \n",
      "Iter-970 loss: 12.5132\n",
      "es opplled inter histled its endofel ait oI ofos ontrlcgint eivin Ga nadlitin-,evener hisaldge cincen\n",
      "Iter-980 loss: 13.6423\n",
      "ed first in the Eare, and the Emperou tin-,8v3 Suread mulede cimind syvith ind le inge torlowed lipet\n",
      "Iter-990 loss: 7.6897\n",
      "e first in en en in 194. Frowco inghis arches urod caley. Weiala  ivgest rlfolo thxmecanse wort lat e\n",
      "Iter-1000 loss: 6.7928\n",
      "er and Wintor OEmatec lof. The Fiout 98. The Ces, pal ad howhish. The country, Japan tas earla mekede\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCGEEIAkBCGEEC6SEIQQQgDw/ySimC+F0SOlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f588fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
