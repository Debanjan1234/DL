{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        h_in = h.copy()\n",
    "        X_in = X.copy()\n",
    "    \n",
    "        X = (X_in @ Wxh) + (h_in @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(X)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        \n",
    "        cache = (X_in, Wxh, h_in, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X_in, Wxh, h_in, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dX = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        dbh = dX * 1.0\n",
    "        dWhh = h_in.T @ dX\n",
    "        dWxh = X_in.T @ dX\n",
    "        \n",
    "        dX_in = dX @ Wxh.T\n",
    "        dh_in = dX @ Whh.T\n",
    "        \n",
    "        dX = dX_in\n",
    "        dh = dh_in\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy() # Output of previous layer == input of next layer\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy() # The input for the next layer is the output for the previous layer\n",
    "            dXs.append(dX)\n",
    "            \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 35.0278\n",
      "cad, fooiuwred m rSuharse thl limelds mIsoasin rlon p loheye to-d arinepemtsr te shrh eisa.tiil Atrae\n",
      "Iter-20 loss: 33.7989\n",
      "c2ettan ihd hea, TiutE iimy-s mhesee leded kf rapl;as rc peptrdpowesmn, ipian5  hile wlelo oss esaf p\n",
      "Iter-30 loss: 32.4412\n",
      "ced rheosit ropaol loom eoth ma a3 txd po.os in rotte ces dipe. wfes an n' Phe eoxc ovckwan res's mit\n",
      "Iter-40 loss: 30.9318\n",
      "cany, th lheftiHge wbrd oipasln. fiturye laklas mne ba anaA teo Incn fPtarisl y west on bop a\"d Ja en\n",
      "Iter-50 loss: 29.3712\n",
      "cinos p5timoc xIcaprnos dand Jary  43emaithe nhen Jap.Co Site mp-rdes inssy iplasup2ac rpen iont, Nht\n",
      "Iter-60 loss: 27.8610\n",
      "c2cun al pion bor apmbis toboksenle whis 'n che lorlressi 1n ard nd 8ornoTalasd impar npiraryn mapald\n",
      "Iter-70 loss: 26.5123\n",
      "c7apd idekoten mivcd gesi rorldic tug.o. Jakin kes 4d fo Jntad Toose 7aplre ys r2orl 1res en ofland E\n",
      "Iter-80 loss: 25.2324\n",
      "csrontenxJant ewele sines isd Jaup, Ced me sokh in t9ey t9estat as prpese ra and. The Sinseleag cl th\n",
      "Iter-90 loss: 23.9550\n",
      "chd ato- ipan mistis le Aitithein pophl, ta3 uthesRh, ti codNerpapaces Side. iftedica ns\"o. 2arang ip\n",
      "Iter-100 loss: 22.7390\n",
      "casexes at wriss fcan, iy 2, Jh. uner waon wed Jakinesyl phest ry,isa coms. ch Nsinkesst imd nbcsi ip\n",
      "Iter-110 loss: 21.6451\n",
      "carceiW rf istarde8al pand Jarends and s, meurdnnw, tporsencbh 4W pren tesel one War myuth id corted \n",
      "Iter-120 loss: 20.7005\n",
      "cted an eeannrLines an rromean Joweo Natpct eOnele Ce f finb, man,i ano ntia co nnes5'mcltgke om benr\n",
      "Iter-130 loss: 19.8656\n",
      "c-kesd onemoircdofsilarmef o8 'n Wauist eic tasmneuto ugy Jithe waaled andancesived nowkdoftden ic ca\n",
      "Iter-140 loss: 19.0853\n",
      "chean tany, indixtas ne ed bho pitalans est, cht om en bop aion m, ty -co0lbches oped ile-te an ledil\n",
      "Iter-150 loss: 18.3322\n",
      "c. Japan estun elipal an zies lemgl Rapar's ,tr rsorlebon anan Japtrendede ia and Il iin kuruciOo,y w\n",
      "Iter-160 loss: 17.5977\n",
      "corentiet on-no. Tap7llo. Japden, ea as Woapen engos Chs atar os im ly ped an thesa an tid sisthe van\n",
      "Iter-170 loss: 16.8524\n",
      "ceate toed sg spronso akdess mopor Ek trMy askeit pe, ma torD5as theg. instdona dcat pedico6sanns ees\n",
      "Iter-180 loss: 16.1540\n",
      "cy oultry iconsintrges on Japeo edyw, ntinge ta  siten mkh. %aom, ias 's 1wos Inped, 日 and,hinlcona p\n",
      "Iter-190 loss: 15.5810\n",
      "cousthe the 本ivepCocaka an the ne lor dised'ne ja sr oriorl Misi engRim 5 desilo, th stuator nd Japap\n",
      "Iter-200 loss: 15.1243\n",
      "cotye barar nes andt mop eacarglgit weon io btindegiiren 日C uc. papar' 日f ar sixerog an. Alaa leona n\n",
      "Iter-210 loss: 14.6988\n",
      "coGanin t8 l pitoo lft ofurapanes Iuppor wosl5'n trar oininke We 'a aa trr an witho garesearan se \"sr\n",
      "Iter-220 loss: 14.2333\n",
      "chusicf-er is ers ofironiilyaEnd and 9i Pjnhi 1u f Emhi h7 mol apuntof naks coseres ane e; fomntran. \n",
      "Iter-230 loss: 13.7410\n",
      "ceasi larn country lomult of thesthege warang2 Wau ana inssilasd the sd vith t the 1ar, it den 2nepel\n",
      "Iter-240 loss: 13.1860\n",
      "coryt the wistory Nis onsb, an es the wovsd lse -ner issivatan wou minldy  on ef 1re ei 2nes 9hd go6h\n",
      "Iter-250 loss: 12.6636\n",
      "comilinde wanesc9tarese ia nedbale th Ch Ocg asorthe w2xs par-te in ie Japas 9amumec aslo an ond ficg\n",
      "Iter-260 loss: 12.1566\n",
      "ch-stiter id ses fefiJyban  alitgeynco, th tith mal,ri Wpos loGg pfe-uro, Jytso Sapan woh es prugost \n",
      "Iter-270 loss: 11.6565\n",
      "calalic ofibynevea fess fanen In iastimigketre eity Japaln5lcan ,eJapat f maken Ga9unsan Lisc Anrinoe\n",
      "Iter-280 loss: 11.1883\n",
      "cbas c8 me jr ilem if phestountio al 2omnce akkent rhapectary simeto  ia led. Jase ere oi8 asyttoun C\n",
      "Iter-290 loss: 10.7670\n",
      "cy pe es ine  opu patio  Rald an ipdbpan, e eaumntr agpeasicerol ky xrontiteyn. wo Ercle8,  of hgumar\n",
      "Iter-300 loss: 10.3817\n",
      "conaninct onosona3g iobu naumse, f wst bud Alu8len\"slang iits namese im. Sfo aobed hath an thes chef \n",
      "Iter-310 loss: 9.9637\n",
      "conomy bre conlycc eoulorgebinaog co0an teltor tirg tarros ax 1rtho undesix arlcg T8 Seo af thesTaina\n",
      "Iter-320 loss: 9.5881\n",
      "cceanes  a sid soutod kidese eatlok -s shest a8  sortaleys asis  csth rh fomtah cith isst 2ren c wirt\n",
      "Iter-330 loss: 9.2702\n",
      "clediinebun Gu. l8ur htis len of Wap lte pesity nsirisimin perchesthin  ficldgict, tireacs en t. taxa\n",
      "Iter-340 loss: 9.0124\n",
      "co, thetu tar0 ldes K\"st ent en ,ywhane Shan t isunll me as teapschao tie wa bhe s kary  it  cundes N\n",
      "Iter-350 loss: 8.8446\n",
      "c t2e n-) ema, Badtis th esea gempdon kh i1he ea nal ly af nntelete i1urse. tetchtUecu apalit wothes \n",
      "Iter-360 loss: 8.8016\n",
      "cy Fa vent ia aser arla ocy an thouneon meked in ecehat n\"site Jasl ar on Japyg acami te . , the tonc\n",
      "Iter-370 loss: 8.8730\n",
      "cd sti. li tirtan in malicti le \" anrege os aNpre\" kee Re aprcithe gert an 1h ifinht mla nes. Ja wa a\n",
      "Iter-380 loss: 8.9865\n",
      "cemengbaraclisosa atg su atl dicasgsiFtratekri as 2e r pithare, im mimt e. kol bId arac anis norerti \n",
      "Iter-390 loss: 8.9804\n",
      "camenct an tistledgif rSllt o. wastr haticangietor ts piref ast one il anditio, en chiinjr Waresth a9\n",
      "Iter-400 loss: 8.9649\n",
      "ch inlmesexta mogelan it in led lles  iuungi malopeoml,woum at bed ar romparinhincowupa pLals lorid 1\n",
      "Iter-410 loss: 9.2646\n",
      "chestited ar, it eoggsterinlenec aod anor ald codopldest anarilcSimrt takintd as ins In imeead Ftf NW\n",
      "Iter-420 loss: 9.1704\n",
      "ctes cofatlan, eainodeaitan' tigi mseorf on pina r. ois iif. Jopati of thes edean  e pardesteo ic ap3\n",
      "Iter-430 loss: 9.0054\n",
      "co, the c\"f Japgn-sos iprof if Pnatw Waan.. Jal io Santoweg cabarin erisivel e ofdouler oIlaof Sureat\n",
      "Iter-440 loss: 8.5866\n",
      "chmscoforett-Jn, Is  aempal\"a golc enpimha  Senof lhe Ch na 9k679y prcele ..wios rh mhe iH ne. 1na al\n",
      "Iter-450 loss: 8.1661\n",
      "ccenne earind und mon Wr or ssbhlineslays lean Japyr  th li e on  f Jeram. rr-se wetiandsrelat, ac em\n",
      "Iter-460 loss: 7.7687\n",
      "camekl . Itarnesigot rfeatunt toplatirgshit,  cuntre ef itdaphke Dlse eapst offitaleg4) ortyanjbinn r\n",
      "Iter-470 loss: 7.4165\n",
      "cy woul ar. ststheag fes f ar' l, Ja fpep hegedi ha6 mbdd Sfd an Jarane. argsist Arof intu 19una amal\n",
      "Iter-480 loss: 7.1574\n",
      "cams leonoth iasln's lalo ex by at elli simonven i, wmititaa d sed lo Cmim latityr ange ete sis b, eh\n",
      "Iter-490 loss: 6.9903\n",
      "cte, is laisd ea , thin ic 0os obs then it. in ade aoprtd rh xe Gan., Weficn\"n s. Jan . ihon Kh papno\n",
      "Iter-500 loss: 6.8312\n",
      "conotyt aitin perta an chsunt' eagaskd:xgarted watyt rou \"c ma. parys  crshxs mo thistuourr3ath  msth\n",
      "Iter-510 loss: 6.5513\n",
      "ciDLact rivyr map perelcnhr Jupang, Tor ois fentinar nf Sist, thh wei hi, al hau at, TWealosamelthr a\n",
      "Iter-520 loss: 6.2789\n",
      "che Dorte teencipanes l a as rhh ctigar tpky ti a Ciutyr. Pn Sis aroslal ef toin the exal ngeJss 2ns.\n",
      "Iter-530 loss: 6.0451\n",
      "cy, the thint gatid viners ryC panar r8 o, ture to fd camikin alintuean . JD if uirne W, As thorhe c5\n",
      "Iter-540 loss: 6.5501\n",
      "constitution the ald s sotro nemtapr2actdiserhare l es ind uf 1.-nd, co, tiftt fing capan ea trthrs l\n",
      "Iter-550 loss: 5.9483\n",
      "ccesseve\"st Amplef an, unru, Injiats eses mt dsesonj houseae noudtt rD), sarge cs,. s eSds dh the or \n",
      "Iter-560 loss: 5.3260\n",
      "ch includestas aen otud  ae or.g ana, ca nattdhu, ef apes of tss tor. . targe tor Tlco Node Wapand'sg\n",
      "Iter-570 loss: 5.1396\n",
      "cam, it tt turLanteinln meara d, ery ay PfmpkreminfP an ke an5 rtheunan kearla msa nor -til, eigestpe\n",
      "Iter-580 loss: 5.1369\n",
      "citunon eiets  wercde , par, tapakgenne, is ka .dilalannt 6hsisgryulathin m Gna andkend1umaln e. ind \n",
      "Iter-590 loss: 5.2807\n",
      "cy, the thind  ljelele1nlathe-se. angedelidesinh-re on ana g, wic as 6ad, a CDe th wogakt uruty. Jad \n",
      "Iter-600 loss: 5.2092\n",
      "ct ond xippl-npylintencmre tat selSy, th midthe yhin-ce 9ystal Sokyd sorest rant SUg ti ea aoc the so\n",
      "Iter-610 loss: 5.2019\n",
      "cand cn o1ktha corlness fhas, tha is rt the th ther ar  sisn, lhe athtu aau wicsvur ecexpl and War\"pp\n",
      "Iter-620 loss: 5.0101\n",
      "conomy by eas apalisanis  Aheios uricSuned, Ss galaecoisdh t.  he, idesul21f hs, ix 19totuneondi. an \n",
      "Iter-630 loss: 5.3432\n",
      "ce ufit. Jf rhesshhes, rr atintor0ef arnokhec. fhe ocgesa an 2haningest rssoal wi4pathea il tebomn.  \n",
      "Iter-640 loss: 4.9611\n",
      "cccunetd padlneseosurrta and in unla, 1asta ed mo, sokoimomon eatolee vfthen ang Int owendadinlucenar\n",
      "Iter-650 loss: 5.0617\n",
      "constituteona tic an tist oitu, rth hana IW crd And hes st r rex anntiond's hestlint 'n iri ge st map\n",
      "Iter-660 loss: 4.6906\n",
      "conomy by pon aedistaen shh uper laen\" nemevbig ase uheg an Whigtst of.expale as bh2, th of aasttheu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 4.6696\n",
      "coneedthe. dt.pr20an er haldSisa ake anyst anis  uxpapare uiden itared fonturh of ofs mo6na) rercrdh \n",
      "Iter-680 loss: 4.7904\n",
      "constitulag, ted. oisthe watlhorct,on Waen Etgent kogos peathespdic. Hhkss -m clrss Nam8lg nasea.onor\n",
      "Iter-690 loss: 5.1304\n",
      "che 12, taltenemy5ea as4 WmpehenkMene\n",
      "i' kh0iatatarsstyapumhe w5 2 o. onsturo t8mstrhwmhWily ap, E er\n",
      "Iter-700 loss: 4.8911\n",
      "ch. Nhelayamis ghe te cdestined spokawalanneiNpen isinofginoka acoka chal mp rured wo kral ges cly., \n",
      "Iter-710 loss: 5.2575\n",
      "cy, tfeec an, Che akcanongoulak cf mhintlhuint, ithoner -fopiov0 arowof'n Rings tpeutarspy Tu sex as'\n",
      "Iter-720 loss: 4.9164\n",
      "constitutions Nesta Js rs aas eec\"ane195a ins f8.ndt lorn hi eaigtet en rimine. N aonbis penas, perea\n",
      "Iter-730 loss: 4.9538\n",
      "constitalangs esisin talanest nd ma'nt, ophos tao zouint,h mym Rba Nd eecd amagokar,s ban esy  men 9e\n",
      "Iter-740 loss: 4.5274\n",
      "cevinesturyJEab, ngiisenas and vd strn ti eat amald, Ti6la., uhugtytri aCki at.kutracercu, auleal dy \n",
      "Iter-750 loss: 4.7212\n",
      "constit tanestlrr.acotheon kexthentegf toky ukeo, \"er, bemowo .-fhared 日k hipedei. Japn\" ntofntra. J9\n",
      "Iter-760 loss: 4.6850\n",
      "cofith inel en tedipawg frre ic 5c. apalteotatchex atl.1sgoirlaiedistryrlase mhel 4s ht eal,odstrresw\n",
      "Iter-770 loss: 4.4738\n",
      "cy, 1uIandsdeIntal ngl me-rosloJala ne ka ms-tiNp6-andaciav f anebc Toperlo t,outh, sc eations a kefp\n",
      "Iter-780 loss: 5.1124\n",
      "co, totmae ds0amilli.ten th sf-nele. wal a apa ltichtue,f29evespares, wasfsurtf miDotag eop Eas rel p\n",
      "Iter-790 loss: 4.7710\n",
      "cy, the farnet dievirstrrHmyguti,  Nr amap pukeolt Sis iisE hygari,  cNrapkes eo 1 rerch in fr aximht\n",
      "Iter-800 loss: 4.9877\n",
      "chinc-fo, ceulellowed, r, axis ungdy lordcstnt, 1han wt Res ant rf sethe Jmeand afatde eak acsld-umen\n",
      "Iter-810 loss: 5.8361\n",
      "cevomp.num Aitu ald, Wapljshynd crrert rhl as lamger osl enet anish5lsiduatean re vo2ntrep ahan-n tol\n",
      "Iter-820 loss: 5.0535\n",
      "came to un bami, sol tha Anden\"s eddigid he Wlagu, apmtardosk, ma. HroWan feakesind eld SEkc  he, eh \n",
      "Iter-830 loss: 5.0322\n",
      "ccpuneol 'nt al tethhte ysalion ceropones Sent-issthe caly aroI whist. Hme th ufo anpp9tsstr aalat R,\n",
      "Iter-840 loss: 5.3451\n",
      "came to m, whrsgedcal esthimamirhefitgunian,seyr0spatalymplanapap whe, thungef ioth milupSpat uowlde \n",
      "Iter-850 loss: 5.4405\n",
      "cy, the GbUpy mnd aliowlteatld fisy opar cehos WoshiJopepelancen aveacea aoicttima maxha en ly iy tar\n",
      "Iter-860 loss: 5.7361\n",
      "capimive  re cel 1ralenrxylowsit uarn Pe3d rr dt m,s of hh. the or mervoop rI ofumhasas , whes  \"aple\n",
      "Iter-870 loss: 5.5420\n",
      "conowarounhor iowakatnsesyunsd ist oS.lotaralden inal ntarn hosaly, .s at mceakt rerco arse ts Sie an\n",
      "Iter-880 loss: 5.1528\n",
      "came th and pounapesparev stut asgt he n. J0ale ffsa mon,or If iper n ani oi ses sntg Anth hisler Oim\n",
      "Iter-890 loss: 5.2374\n",
      "chestorurthe cyu nertfst 8n s rer ereN eon es poa0 onhihis r impeh1b thest of s oJ951ssiaopaeneetioyr\n",
      "Iter-900 loss: 5.1522\n",
      "ch incl aikor Skehed is stef r9bs ial pI stan si nol erograted  ad x rsinde arustrSalaowegalst eanexo\n",
      "Iter-910 loss: 5.0532\n",
      "capitic Jpal ploweril tatld s fenol d pr, expspl ar os th taparin to al veNemoep,emorgine cine 19tt a\n",
      "Iter-920 loss: 5.2984\n",
      "che thesdiLinyr an Jsptron on–turkasnrcWwa an the en  orsdslllC  rapar cyn anave eideape, corur riste\n",
      "Iter-930 loss: 4.7976\n",
      "city bopke fnuslsbyfpalmwexurtkg sao ystr Clac wrutsta nes1ur e ud, th, torhaintumarg nrsc rendNer op\n",
      "Iter-940 loss: 5.1517\n",
      "ccJapan fwes tol, fowntrertt an of. cst mp hesturichilne the e 1fhtheseNp, ,  uromnevo6amit, aclon. t\n",
      "Iter-950 loss: 4.8459\n",
      "ch includ oil loco soes lnmlcicokdestl rcme mamSaa honfy, WGaefenirbym\"fth-slares ixtge tacn, hory au\n",
      "Iter-960 loss: 4.8315\n",
      "camipiy cosil ri, Gnssalamut to,gk. iGb eoEns mct,f lhic0 foIu lief Dp6 eneaunndes moghttary t miog p\n",
      "Iter-970 loss: 6.0741\n",
      "ch inclldesel Inweg ftsthenn Sist,esounosindo ndesupthulge ed alindec4 ofur timha. d matl Janan-mau p\n",
      "Iter-980 loss: 4.8946\n",
      "cted legh.dert  a ll, w  f mnd cIf nomhel0 afWtG heraite eaaN 1un-r anibhor sa le, ia li, -fhufon As \n",
      "Iter-990 loss: 5.0298\n",
      "ch incmuf trlo tidin;rcd tsdth  Hilomanety  watess1h6ekes ono, the 1a ese fus in ates metaxdictiof ae\n",
      "Iter-1000 loss: 5.1492\n",
      "capital cobod coOdig inet o. lodatlain es Woandir. lIe nPmh ardi la skesn in rcurime1ftmend oertapen \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEACAYAAABVtcpZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x106119da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 10 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
