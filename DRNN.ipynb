{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "    \n",
    "    def dropout_forward(self, X, p_dropout):\n",
    "        u = np.random.binomial(1, p_dropout, size=X.shape) / p_dropout\n",
    "        #         u = np.random.binomial(1, q, size=X.shape)\n",
    "        out = X * u\n",
    "        cache = u\n",
    "        return out, cache\n",
    "\n",
    "    def dropout_backward(self, dout, cache):\n",
    "        dX = dout * cache\n",
    "        return dX\n",
    "\n",
    "    def forward(self, X, h, m, train):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        hprev = h.copy()\n",
    "        X_one_hot = X.copy()\n",
    "    \n",
    "        X = (X_one_hot @ Wxh) + (hprev @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(X)\n",
    "        \n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "\n",
    "        # Dropout for training\n",
    "        if train:\n",
    "            y, y_do_cache = self.dropout_forward(X=y, p_dropout=self.p_dropout)\n",
    "            cache = (X_one_hot, Wxh, hprev, Whh, h_cache, y_cache, y_do_cache)\n",
    "        else:\n",
    "            cache = (X_one_hot, Wxh, hprev, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        #         if train: \n",
    "        # Backward is performed for learning/training and not testing or validation.\n",
    "        X_one_hot, Wxh, hprev, Whh, h_cache, y_cache, y_do_cache = cache\n",
    "        dy = self.dropout_backward(dout=dy, cache=y_do_cache)\n",
    "\n",
    "        dh_next = dh.copy()\n",
    "        \n",
    "        # Hidden to output gradient\n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_next\n",
    "        dby = dby.reshape((1, -1))\n",
    "\n",
    "        # tanh\n",
    "        dX_one_hot = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        # Hidden gradient\n",
    "        dbh = dX_one_hot * 1.0\n",
    "        dWhh = hprev.T @ dX_one_hot\n",
    "        dWxh = X_one_hot.T @ dX_one_hot\n",
    "        \n",
    "        dX = dX_one_hot @ Wxh.T\n",
    "        dh = dX_one_hot @ Whh.T\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer], train=True)\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy() # the output for the previous layer is the input for the next layer\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t]) # train=True\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy() # The input for the next layer is the output for the previous layer\n",
    "            dXs.append(dX)\n",
    "            \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer], train=False)\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "\n",
    "        # No full batch or files\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 124.0747\n",
      "e(cgBbHjSe,lm本KbC9MU2WlcmnwNti;cx:%日RiW\"e–iw)e本ntcf2EeaW1iLa本kDEiubMMNtd5js7;oL6FtD本4s)hN7duasryS–elt\n",
      "Iter-20 loss: 106.2611\n",
      "e日NA3\"yTd7sy8rra.NeUout 9cw nwes wfst–PMftrihPhrtrafahota  dO neelews–iheai1 ttafIkb Dtw   ieh a\" aeT\n",
      "Iter-30 loss: 108.0775\n",
      "ePthmMr BhntbR  Ugk ara i uh2 sbeAuoolehs airttDuncsnt  akprti- nd brdet yspgoodicE8a stf rprclSn 2sa\n",
      "Iter-40 loss: 106.0726\n",
      "eE1vOill i-l 8iis iel oiyus  iide 'fa Cedto n eliWelre-thn pidcMi sWnwri8lCh dw g Cil oi  kh 6lstkoef\n",
      "Iter-50 loss: 104.1235\n",
      "eoThs t ttaahs mtagpbse smnom aa var rtoyr dp a-tip sfhhise tlaaldo nisttadd z lic'a.eii  itlS  i Sh \n",
      "Iter-60 loss: 104.0994\n",
      "e.(vra xuusrJ nttese iyr tn vperhwrdiracredaia1 plgnfoandvi  paiaterearooewemdandroe  ri ners eaa,an \n",
      "Iter-70 loss: 99.3134\n",
      "eBs1ni1uhts1 rerfhn erf Ir7f etnt tJchwgl5 ia  odotcdnierh inmeohiehntta6 he ilhta na ieor,ag's cpg h\n",
      "Iter-80 loss: 100.9485\n",
      "e\"RDtf 's gpl\n",
      "dBioyxte.aaotheortnteai diat.e ia  oe i日  iemT sot  ofynd etr  ar1Jvesaea \"e  )cvaftsut\n",
      "Iter-90 loss: 98.1138\n",
      "es9Asrdc aap nsf e日nle wsn  ringN faooanaonpatNtocpoagednpsRen ieeogrgJ klieyitipa ner  iurd ta  tegn\n",
      "Iter-100 loss: 99.2155\n",
      "el本 totkdterl   aJ8'  ncr\"s w oeWyasam dahkoeau .dpta  clamTytaiTns tth iPadthu ih  oe ybeAuhi4 d,she\n",
      "Iter-110 loss: 101.5353\n",
      "eSWehntotk ondgoo4as ar i1n vencci  frl\n",
      "ffsgr otalanaf ihe ohrsktaCd :td  Ae5on tr aniilr hae wh   hr\n",
      "Iter-120 loss: 98.6910\n",
      "ecD4S ksromsp easaok   yr ,a,t oiole Hwmehe.mdlcaee o日 hoatir dRitdaCd lhe  jhkot warec,Ti. eixnh Jn \n",
      "Iter-130 loss: 99.9249\n",
      "ecncxuen WEnnceesNon  yoi sh7ahAddhnJi cJrdet,edttaigkriel  s cWjppu4tsi日J ooR nroedana3t aa dokela i\n",
      "Iter-140 loss: 92.6343\n",
      "enpipmipbmi2 na's ogHineohstdinne,lWurrc r aoncts IteT tou -nsfier doJwuof ,admd,h7endit oir trlirutk\n",
      "Iter-150 loss: 102.2552\n",
      "eae to ttfrcpksahe thasa hr d4im rNy umi .h  Woiltd w tit ddnthe\"alad ehai.Jie te  Eea ilif skb.h srs\n",
      "Iter-160 loss: 93.2293\n",
      "edc8n skeol2s.in Jarr en ded thtia and fhre soeHpp J disnge Eoei oir4eia anye tsldyed2a. the  elmrtan\n",
      "Iter-170 loss: 91.9704\n",
      "e22utdu Golselulnltg2tWRie sniandinand  esDth  psstos boold et,n. gar c eh8tacconhssek ui  niJ. ahm h\n",
      "Iter-180 loss: 96.0904\n",
      "eduI.teeteumheoCehooe,loydbs 1n  ci7 mr%oin 4pzy'tp,cpadnia1 hn ..vspa 5ju rorkr tu ropmc ndinsttSpnt\n",
      "Iter-190 loss: 90.5668\n",
      "eChannie slel1cinlernithe thgllo%.eoaRv;njwlau 2ertd'e \"at aegenGhaole ielEiknduFisnrangatho Uh eot K\n",
      "Iter-200 loss: 89.9863\n",
      "e.80pif t  ontyy ir fier9t-r yfe,dthehico1aret iJnte1 mondesou tate Ja-u-eostsnred Joe toe Eaid sooiu\n",
      "Iter-210 loss: 90.0478\n",
      "eFdindtat ned or4e e  an'ad th 7lrtt s ut s  hsovkvtatw an opmoaltoLeiae  fhe oimc% tits  iocSat W oh\n",
      "Iter-220 loss: 85.5187\n",
      "eheh  oesiso oc tyeo, u mandthh  doe9d dovi6ait the Eeumism Pr–apeellafsg,s rs.of.r rf an  r  ihtly l\n",
      "Iter-230 loss: 86.1443\n",
      "eI,f–fa nherog8loodndhe. o-. ele1m2scCen yaan re lnWx  hiylapnonsit lh apeaeceit eiplnaCcea an e-scvx\n",
      "Iter-240 loss: 89.5155\n",
      "e to2rc eatanhdnre eathedmepd1imob tid aa, sed oo roee 2dr'ded  are 1aaaasrC and ts'd oa nus 5f Tnisn\n",
      "Iter-250 loss: 88.0829\n",
      "e%oup i8ltd本ent plyto:le omopdpuret cfhwmoo t 'adzis liolth icunsos psp lhtai as fot he mreuf 0h. Ie \n",
      "Iter-260 loss: 80.1775\n",
      "e\"iho iolcsowudetuhe ionsttyS orltho srurin Jd Jhvisf euoysereis aiedcint to 1nstdtc ochky wheuUsiliv\n",
      "Iter-270 loss: 85.7088\n",
      "en2ahmhoec and Gs4nkhie binlyhof Dy is ohr f p0Mrcstnts9 Etse fyCmnNtteliard po\"iw wiwAytodxan onsu. \n",
      "Iter-280 loss: 78.9874\n",
      "escheloo prsy icbdegor sPxoabond 1aea. unJapeikaB focoyttolarde ofotin Ealohus Jagyf thoccm  fBtth. S\n",
      "Iter-290 loss: 81.7616\n",
      "eocnnAao  iegsdraft 1focreo, oos cWh4s v8ed EaRddeinH tonrt cjccH1la Rfd6f is whpaoe\"JaWanota y yf Th\n",
      "Iter-300 loss: 83.4443\n",
      "eiAlpunbry u-m pfrtss lak l ryJ poryO ARdtdpSnatt t ingti  bhlaeswiti6  tHusykntaninggerd Eiao,T–B2in\n",
      "Iter-310 loss: 75.9270\n",
      "edJcIene. 1uruLnilae maparorerisorne exudN mostdin salanhetoa dy Toe Witarhn irlaodi, incaas covxae e\n",
      "Iter-320 loss: 76.5914\n",
      "epunabp.n is porid \"aegxn t inddru5fo on,.timdfapewbf CaanapaJ e oormn the turlanjnthh To lta  indein\n",
      "Iter-330 loss: 79.7424\n",
      "eublyTsexube theueurk.0eG,uand be to. Lard '2 S \"f u, 1odld in. hifalr eheC ihel oTnt ve potutano in \n",
      "Iter-340 loss: 78.6619\n",
      "ew0oo asrb5r\"a als meapa iagsocwif erlkiven.iihe pasl-ars Ii parufnheGss aalan the Uag he3  ChA Es ko\n",
      "Iter-350 loss: 76.7171\n",
      "e\"glasts  proto we il 4itTg naemthe fod te dlmmlettm rlleos Csii.s wcm w fime, 1re sfJceoeapise tf is\n",
      "Iter-360 loss: 74.9615\n",
      "erd chuR Nivthi hr rint pe lridgd dofl, rhiea,eKoh  8llchane eb ind wersk5rthac Jror8Uf 8iDthrname f本\n",
      "Iter-370 loss: 75.6980\n",
      "e4nthtin sgwe8yir whH EleUtcgaDty onlprenr Iar.d G chp5plkov .it itr cminest thereeiiFyd Otdphontrwns\n",
      "Iter-380 loss: 72.0677\n",
      "eEAp. tre Gfsiasnd an e lhekod Sidet-ofljecuicrtg cot ,ostt inhed, norwrryroeisthen aiahe S.0Is ;clmf\n",
      "Iter-390 loss: 71.1232\n",
      "e8ala na  Eognd,this arJeo plreernhiblolothex,Apands inathe 8aCota4 nand's  mtotre Amotakle ane chea.\n",
      "Iter-400 loss: 75.0147\n",
      "eA1a日 Iort ie sa-ind Oery ofurolld ke4 rodedefon the therT Tile OirjvOr sbondtta  ttu wPolini i(nd Do\n",
      "Iter-410 loss: 70.0129\n",
      "e0San La ccmpasgbrhd iags the PorirlseJaar wmcir noreikyLd5toeIn ri gnt SorlhCsk). peflergo5 t2l RtsE\n",
      "Iter-420 loss: 72.6781\n",
      "eith riegto tlirgad i1nt e itees uo %T1abxeneiry  Uagd dene2 oygefxgead drwgr Tht oseslre-.a . mye vh\n",
      "Iter-430 loss: 72.6259\n",
      "eRsof. tu Walm inp terin mmntmtm tbrm codaf to6onoinm teorbn tae duy wititi ne ewoaar WHKk9 a7 lapde \n",
      "Iter-440 loss: 68.6085\n",
      "e, mak g. h. Ga: no Jdirothe Swmeo6aponere Jeed-an iT. a26ntill thns ihliPente tny Pejgieg au, her'xa\n",
      "Iter-450 loss: 68.6604\n",
      "e Gar aii Iain ogte, fherinri1 Snte aog mor theucorgles Sn, gasth sh \"pure io rd Ohes3 8atas whrat ih\n",
      "Iter-460 loss: 70.1034\n",
      "eSwony usrldjh sn io th \"olarheke thoind ae tas and ongerttof in txel mogmtry wn8liuh sornkincmhillt \n",
      "Iter-470 loss: 67.5791\n",
      "eto tilstio  he weoox ab Osre oondtL5tec anb and J's\". JNpakst cur tie the IwdmangeKthe Cousth. af yr\n",
      "Iter-480 loss: 69.2115\n",
      "erore ca0ry arvni.vesth Jsfapdista, jhiom 'k anycrih7laynsshcrceotobeon evea do, che puntbott r-iEz 6\n",
      "Iter-490 loss: 65.8044\n",
      "ejvkor. tftuithD mhe morsca darod iand cre. hp pt4  ane an  ahe th. Iualut y mertaranoe as fnpeltgogt\n",
      "Iter-500 loss: 73.0006\n",
      "ean or Ea ceatld 5aban caedeeW 9xaH onot nicins 1nu hocbled wiylhAvlif nsche. tr oorthe prthw scref c\n",
      "Iter-510 loss: 64.1505\n",
      "espokyo wht 1 gkt icge in in m,.en of jhe elepikn asd hesd \n",
      "vonkl-inirnd Eests Jopa, taeasd of tItalc\n",
      "Iter-520 loss: 66.1035\n",
      "ej2fsus, cinapgede  whthe Gmre tf calg',ne ofgmilinEand Ja'a wutle oaniggh.pepndav uote coulasdea9 n \n",
      "Iter-530 loss: 64.2427\n",
      "enmpaneri tloemocCinien 9oN.keabooI a2d ent ofplyored he iame coin mivttio lfreaphe tatitos iaus. Jap\n",
      "Iter-540 loss: 64.9107\n",
      "e:0oL0sjTikevinteTwed Iorase,lHfiwar ot9ox. , toul me ylopeg thu lm il lim wanarin ogny Jaben, thr he\n",
      "Iter-550 loss: 63.2007\n",
      "e-LyA fkras, Sit ocon tey homg ,ht Wise ond imntry Kxtha jomest macsan pprSerkn oiavdith1lard tee wou\n",
      "Iter-560 loss: 68.4387\n",
      "el Comarl, iss fe日t te thcntre sered oa, Sopda ta anek-l phi r 'h Jamlvstithois .he foptin tods,any a\n",
      "Iter-570 loss: 62.9061\n",
      "ee bemit f pEomle2iis tnn:sinctaeid t EWxcwal y NoHerhd d-peletoe  Tud NypKn Jate Soslacheseopxsat, a\n",
      "Iter-580 loss: 64.5374\n",
      "esGhet.ollCs8npli,gy hi o tadtin Tuz i-s tosea an ine fi4ltan sh ardrsn, thelgeloli. -ai he ia dorev.\n",
      "Iter-590 loss: 62.7487\n",
      "eglatd th  iecd fuisy ouse Gnp JaAl7\"s. mocontgiraa, CownIy why or0,ss fitaa  Ise, tht Photo Aioo, io\n",
      "Iter-600 loss: 66.5108\n",
      "eObkndesi caulSoo crprestaeidaot eseldemifblctoile Japin weyt ora leaoged. os thr Jmpeo'. iurg,s ias \n",
      "Iter-610 loss: 59.7170\n",
      "ex ar Aoti tis wir thiu ferldlnivete anea. ofTanapofelaoiped  aios. Tikand ire tmceamalbes oe mithe c\n",
      "Iter-620 loss: 60.2527\n",
      "excpntctntar .om wast mevrat oJ Gau'hestmaed y De sf mint tapin8gou on Che Rrmly 1tre cGmndtvinure ti\n",
      "Iter-630 loss: 59.6922\n",
      "eneor wea mapifo ar onstcsliage capdog2 a. Jase liReded So'h thi an ith3 Je bonit co frest fhpanervs \n",
      "Iter-640 loss: 65.4482\n",
      "emChi endlaen ry kntse alrChtyC ielomuneou ors nideReKWiilauoea licgistroinrcareme wor .  Jikan inyur\n",
      "Iter-650 loss: 59.4524\n",
      "ekf pwokre thu lires r9 tokwd Wo prsur eyw vet ptante  mf peomnd be th and der O8e wrrlDignlaf in enr\n",
      "Iter-660 loss: 60.9184\n",
      "e–0meatesP indbcrrdp a in 1ndan on aK phs 9hred Hmpinnlikitit  afe .oranat unalote oWr, lolis ia s- h\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 65.2916\n",
      "eelhiventhe oi9gkof wer se nioyts. eabl JheaSestcwofit condd, the 1agte ralegeat in Rasincewor sicnie\n",
      "Iter-680 loss: 57.5149\n",
      "e'n fasd aane congert wipan eatae Winecinrm taro wan ala aus fboas ,the, Tatcalee. thr Rnsfeirtr ws b\n",
      "Iter-690 loss: 64.6545\n",
      "ecGar  file lople matr es aatdosinas an 18etn fhr .yprt warad tard tf arrtey aso oid ,ond ,hys  arcok\n",
      "Iter-700 loss: 64.6340\n",
      "e, aod Eurso%f rh terrir tan and ioNin Oyg\n",
      "n.Mpartiatc nsich' ro4n, tht 1r. phr Tsi5he RoC,lae  acato\n",
      "Iter-710 loss: 62.6590\n",
      "eb2Ty Javooes aiutd r94tran coI pob8 bi an fedand it the Rlfmiehtinb eree oNit ,oft itegd opothi ithn\n",
      "Iter-720 loss: 58.3846\n",
      "enIree. ropJongd1so angisoal theyRh lomd th isN ily lomiss ano1desLesine cerld rBthi fNLsxin1 gad fnP\n",
      "Iter-730 loss: 59.9207\n",
      "e the Serrhxdiby anenIeilgcred 3eaead i1un avgstumise ifgr. guveuch 6 pupgrkti teinyd colenciand O\"va\n",
      "Iter-740 loss: 59.3005\n",
      "e9oce ooktdnl ohise pan faperuth  Eevelcho Uert- iolrd Ry )i5. andena9e at eapan ra, toredf cultstmin\n",
      "Iter-750 loss: 60.2747\n",
      "es ana sesths mapeo- orlmy,sTy-oCt0ntpofar aedeor it ld Nar5Cyu0ch camy usuols UWr5b1nbDu olthwa. wis\n",
      "Iter-760 loss: 61.8158\n",
      "e Uh\" Wor dist i0oh gevis lomDoLt Cx ofe rlclaiE thiy tpelioyd the Gcllcesered th uhelooryd f8bed in \n",
      "Iter-770 loss: 55.0888\n",
      "ed aokin liclata cinrthe en fhe sitxis d8er and anyr–20in  Srod thioncrilmelwunerTo ngid, The husties\n",
      "Iter-780 loss: 58.1365\n",
      "e The Wartou narentaperertt 1JCn oalC2i\"Gvinne oiledeclbom zyrtice-tureeve o7ol JaKan tee lopoOsesuad\n",
      "Iter-790 loss: 56.1095\n",
      "evcalre tm mpustrccacithw yls wiab, shr p'am ase in the Gnoch ereapirlg gered an at \"n whe  ouontti c\n",
      "Iter-800 loss: 54.4471\n",
      "e0oan thu ehuiheo1-eanafy7ercumpan taclan whe -latolls 2oren ause Cis 1it loulye easocinngilltos wapu\n",
      "Iter-810 loss: 59.5876\n",
      "e21 Na o8st ttilsPameld ios,urntr.d atge tiond th cirges arankith on8a-o es f8reb.. Ilroged. \"i.)h ca\n",
      "Iter-820 loss: 60.0053\n",
      "e–s ba f .the ioms. The Gacta, tor mrya dudowy  ipredth iylr te the Derla.d Wk,peoiss uudoJutatanecea\n",
      "Iter-830 loss: 52.8158\n",
      "eti: the ianthe the Emrmd'a aimits Somy boundod A\".epetald ored sic orllimety ioveunaa an cinate Japa\n",
      "Iter-840 loss: 56.4075\n",
      "eU hity as orsasiche toula0d \"2xxs anati ler wWextodtin Wae Ssn'upennset Hns lamerlDornd Ware es ard \n",
      "Iter-850 loss: 59.4577\n",
      "estalaN, phnryr. woky Ws f Japaxghabexnorge tolghtai f aoda  in Cheacd Glisandiceniuliofy iy RusSr7y \n",
      "Iter-860 loss: 52.6492\n",
      "ed toe the 1e ur,S, 6h pf ary ouxma, tocAsrat Cowopstaeator anl the eopDti afar,anasd if nargowiuanan\n",
      "Iter-870 loss: 55.7938\n",
      "e6f reta maegvinlndeiCrered ountrt Gs an. T'e A2kta tonsrgeltiise orp\"UMntt anrired nf Sutyh raveanow\n",
      "Iter-880 loss: 55.1392\n",
      "ean, thi alleperat -ase of-lapueAta anofils aoklseon the Olper t io日 B,pan ty Aikhy Japoterof piot is\n",
      "Iter-890 loss: 51.9127\n",
      "e-datceith 1omi an cuo\"n. xtoy palisela ins insts caute worto chpouosar, th ho- den w9ura f Win the l\n",
      "Iter-900 loss: 57.6832\n",
      "e:Asoasd fe ttbes ar hed Japan Aa, dmelec inrscu poo  os lithI NJxuan Asi orest The -lina red in hinc\n",
      "Iter-910 loss: 54.0785\n",
      "eDpan aydy, oheclrusts Jo6h milaCos thi, tre dlan an Hikan Th oplrer illmhe 1efnxceolonbing oin of th\n",
      "Iter-920 loss: 56.5687\n",
      "ehe Eino Cor ssheatD, LarleCwearJape ,t a, Eored ore1micand pinat ent ranthe illthe bhmlhe 1isherelf \n",
      "Iter-930 loss: 56.8193\n",
      "eFthitin icec phd minatin ENllpomoth kurhf Nohto-7e, Jonas iopas St asdar gf ay pevcd Janae 's d uuy \n",
      "Iter-940 loss: 53.3386\n",
      "e war 2's  alioel wf ofdsscf in 1i6wrvilgior an eh Dadeta 9elytmest. the eoniee Solo tof ans1 eath. P\n",
      "Iter-950 loss: 49.9016\n",
      "e papinatis lv gisntirider nr Eap1oW\n",
      ", Sinmrl Hirt mo5d orpestlat1gn in 1he'O uooTiinta and durett 1s\n",
      "Iter-960 loss: 50.0446\n",
      "eryalicon radte suuprs tapate Hof tonlang sn imit tarcin io in mire. Tad on in th islld ine iorist  f\n",
      "Iter-970 loss: 51.7578\n",
      "ek. Arek-d perobe ite Hhxstxt on my ankc ofee lo tic Tase wistg onths an mie Hirat Inald wisttinhe ap\n",
      "Iter-980 loss: 60.5939\n",
      "eJ pap'i tinn am ol aimy inxti bald ogkthkectonl ete oivst. panchan ar Ehi woped iiked to aobldes 9um\n",
      "Iter-990 loss: 51.0715\n",
      "ecrandsith omlpern re kisstikea an isn oens rexchilotok oLdis hulilglaby in af incen and, he thiralac\n",
      "Iter-1000 loss: 51.5441\n",
      "ed Whe Cyupirglace of paKind ded hes the Olane thithe yarlc aist ha thwomy erevof of ,\n",
      "hescoraed ans \n",
      "Iter-1010 loss: 49.8465\n",
      "eghe thicy- eriithr Sss enored clas 12f oamik5m mcwinn mty werr cunasd sh  fogoum. fhrmat thy wfrla'd\n",
      "Iter-1020 loss: 49.5163\n",
      "e hi, cidltimnnditaw y onseuntopinare-sanano the tovwin .csthestar oug5x2 Ast0in mitrta in iomlas ins\n",
      "Iter-1030 loss: 48.6460\n",
      "eryoro oe irag lhe ophe tupanaJ Courideti, esvlftyitor Nesehichd rimae, Thired ik wise uhtgOnatur wec\n",
      "Iter-1040 loss: 48.8512\n",
      "e1olh as consdurtn Lie psnheithd , wored iot rilhycongurr thd auger, Asunisang wopured Japon'e . piol\n",
      "Iter-1050 loss: 48.1674\n",
      "ed dom Doredelc.n fe unore aconitntreeJapor,td Japev'x andeety iopote is ntap, pouth the Warge te io \n",
      "Iter-1060 loss: 47.5592\n",
      "e Lir tedcFikst ti-cheld ron iune uand Snver ald Ss. WstoNotO Jatl 47 oleini6 th iEep, mistr-us cofur\n",
      "Iter-1070 loss: 52.8252\n",
      "e Sarenlavdisly Wet to5 iclthe SArpon the Uoolti apmncmaoly iot if mhea lpmirst ofkof puran Sd thenta\n",
      "Iter-1080 loss: 46.5704\n",
      "ed the am ry an peosloxiiny mpconscrt ed enterin, thi Enges iesix hedivuteo cexfolatid co aiked th so\n",
      "Iter-1090 loss: 46.1683\n",
      "ed of ostwhi atlh t9toblle ointho7g an used sh inasinn am omoultorin lepitht ats bovlleroust\" fex6d a\n",
      "Iter-1100 loss: 46.2487\n",
      "eveateeNpa-outh wist ond ar or 19m\n",
      ". Gar af on 1241 3the Cwimve fagit. ta andse phcmellit. aa and yar\n",
      "Iter-1110 loss: 49.7800\n",
      "ed an rs tists pinortcarin. moty an oastarwin Japones anataei. th ineiwttor unleomistlouann g-iGt ry \n",
      "Iter-1120 loss: 57.1836\n",
      "elear Wexuna wert eokJ\n",
      " inu1,tancai, soredodn thigEnse y mirlatanissl-mpectoerea, oo本i7, cof bj bnxrS\n",
      "Iter-1130 loss: 49.6931\n",
      "esingtsid ing poteripea. Fobel oftd1son cy. esooland pasas tho st99ls8 ufborAseacand ca,atiras angedc\n",
      "Iter-1140 loss: 49.7150\n",
      "e the Gure th hi lmrydti colvellound Sonth ifcldeedt ofe nound toperlanges. lovesta and wiin are tomp\n",
      "Iter-1150 loss: 56.1538\n",
      "es acdesexd yopen rate wiry wst tiro ere tanky Imotat nive Errmtiwkan ehe cesrius 8-N1,C9- eally 1ik-\n",
      "Iter-1160 loss: 51.7515\n",
      "expan para of iin ho ticy hf higlare Na u\n",
      "d Gwis thr-celebrl5 2iLt Popre athedin bies lerest wolorest\n",
      "Iter-1170 loss: 53.9076\n",
      "e Aolgof , pan Fouyaeanihe thd wopes9hex, f-ilntorigin f pepes eas  afdt iigald i it t i7n cwanor7 lh\n",
      "Iter-1180 loss: 51.5466\n",
      "etc onorisntan vEesthlsslmautlo mereaudin ea and We th majrud i, mar al sasea, Hn Nn btd y-6uLante np\n",
      "Iter-1190 loss: 49.9884\n",
      "eU-oumd2 GoIiagy cedeasort5 topgokhe 1 wnkos ias nounbo8g la, he  frokC nupporetctuncrorlst ourexrag \n",
      "Iter-1200 loss: 45.8414\n",
      "eothe, criner mi. thekcourte thd 194ble -Iigelucolstho an isithest rextaPgana int inghilatgese cfury \n",
      "Iter-1210 loss: 46.1593\n",
      "espaokedJarantaC Eman miritouredvIngesm mescceritco faily wes dsd fees fish. Jo4a, Janay yp Emxconons\n",
      "Iter-1220 loss: 44.7797\n",
      "edrpero at u, thex aplhes xeuto 9f dgunatast expgebligicny 'stlocerlase Wc anst anandcWiot irgisin, y\n",
      "Iter-1230 loss: 53.5134\n",
      "ekeouth irea wan 1947 Jnpan linrian, iist mee thm thoigl tompixo olate fi0a  wmzed ninese, ponperlame\n",
      "Iter-1240 loss: 43.0794\n",
      "e Tomrootind os dxe th walat In  bolereand Japan Nm lDtins io oi lthycz 3isii utjy pLborasys Jake uf2\n",
      "Iter-1250 loss: 45.4255\n",
      "e6d hi a louO5n iisu, Ja.inlhidpare y iofwica lausesatic ff dnd ity inwecean on ih mhlano0s  fameatae\n",
      "Iter-1260 loss: 47.5660\n",
      "eddine kovedin ths whpst roded he 1 ro loco8ns Ipal cmres Rusts N fotanse oht ompered hos Jap fexthe \n",
      "Iter-1270 loss: 43.2711\n",
      "e pard inh ceantarise  ioth Taryd 7th whel tf inothe orpore ckedushJa apet is matd .s puryrstxoInd na\n",
      "Iter-1280 loss: 59.8547\n",
      "e tha ceutcer, popuryonxthi inbcresn ri dk\"n sy int. iselmpertredt. mherDonddertuly- abondeJete balst\n",
      "Iter-1290 loss: 42.5414\n",
      "e, Wouchudl fean Japen'2 a Nf foumbou nd Sa, thiiyl ored Japan eivepmintas peronr and Huras sorolist \n",
      "Iter-1300 loss: 48.7259\n",
      "esJatandes naricty mr af oure Jaend raref red ar uko. Janroria, wsk Bemter arth, Eal hf4) cch0erea of\n",
      "Iter-1310 loss: 51.8901\n",
      "e2of Navandais th 19myxlara., pa aorte tam eeso mobrt Cin the N9rlanes itho unpalitira a aod orlar eh\n",
      "Iter-1320 loss: 46.0058\n",
      "e JapbnJr ar pore tid Eus ceredeWsic nired, ariky \"s ar o本sy Jaioncin tife fouloen peredder inoticong\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1330 loss: 42.2477\n",
      "ew,ius JapeO, eika  of liven ald Ghe berthe the cofli's Icumore lut3h.5J-panc mrrex and Japan insea a\n",
      "Iter-1340 loss: 47.2673\n",
      "eJ phn ef ahd R IngeHllohok. Jaiancipanusm agdtid in oito, mhe fopetrrasticeOtobowil Aored War of Eap\n",
      "Iter-1350 loss: 45.6910\n",
      "e War of Napk本ats ingloea. thw seuAnt5 ine an 1yr h othithe 18jc wrmpupceataly erlloesturgusofiy eala\n",
      "Iter-1360 loss: 41.8005\n",
      "eNp8rusth ioald TotcouAst and chnclfbusy in o-ingond ri, th. s. fira anoT. an Se tho f oey.Rr, istar \n",
      "Iter-1370 loss: 44.9484\n",
      "e6. and coand argise ar onne9ebt bf Japioes xi tienter uratrgs fhes tand is 18Skidsy pandeoche Wigg a\n",
      "Iter-1380 loss: 51.6137\n",
      "e:\"bopnd Ie mpegt8ry Ashac an CR whe tares fof thin liag Kt the worrored Jipateitis torldes ap.nil ci\n",
      "Iter-1390 loss: 47.3942\n",
      "e Nof on khd urpis inUNso\"oke –her of onteela wit bin Ciglerooat. baly pived eoery If Gho ffrstd tse \n",
      "Iter-1400 loss: 43.1758\n",
      "e. The s2antod ahd the the hio, the Garit, oapet on ioly, As toperlxeanginstarnisgelWoy sived idg. ea\n",
      "Iter-1410 loss: 50.4838\n",
      "e ard anay  inesiclnCoH tyrmalaed Chesthy anamies ingescnpaowaled hed Gmleaga,iin gh cf4\n",
      " sr ostd. mf\n",
      "Iter-1420 loss: 47.6215\n",
      "expanteite easo. the boredopiks aroIicexe2nodemthe an the Poptr te ea lderis coustarlanPin et The din\n",
      "Iter-1430 loss: 43.4235\n",
      "e the thu Soureanith, frrbn a0ea ofke the kea ehe pureo ha re this lfey war of torgUxty . fhea eamaed\n",
      "Iter-1440 loss: 45.1718\n",
      "e WoreoNn Lolaldonte cheitilagisy as otn ta centees,aroind lurerti ylirgencemitht Clatees and of Jypl\n",
      "Iter-1450 loss: 39.5871\n",
      "e fapan, thin as uminy 0scp oureu, sicarinscnarea, in of ofoihy in the Glocynat icenlasared tareed he\n",
      "Iter-1460 loss: 48.2414\n",
      "esblod O8poncedyppeseaisan an 1nocednit ts ankedish ss sot, to-ld.anist tirecinusad it Wcre tired is \n",
      "Iter-1470 loss: 49.6322\n",
      "en bois 6hd Sennoultirin cuty siche conliks acainsas anouros- asthisan,elist ery-,upan sn the faiunth\n",
      "Iter-1480 loss: 44.4243\n",
      "ed the torldis sf Thean 183 and ais tar velcomlet2 fovid on 1hes afdes ce ontht on 68,an\"nhes resdicn\n",
      "Iter-1490 loss: 46.2965\n",
      "ebt1 ti pirissta leciplleper Suland ube towland ta anoet cin limyerthe if the anasn whesthe woponutt.\n",
      "Iter-1500 loss: 39.3445\n",
      "e the Gwry Japan we Worin 1alh s. Japand(d anrtiyloc andeiend Rurou Sante nalytenisr rhush torhe amDi\n",
      "Iter-1510 loss: 39.0578\n",
      "e wary er and of dndiest ridgherc mfnpoturan hichalmcIuod ofvinsin lhed Wm pedur5 of ba d eswly Ion r\n",
      "Iter-1520 loss: 49.4217\n",
      "e war gist of 3ith or win Nh8us oorirtopkicuAowhiba land tin wae tap6)' -wanre tt sintrion ia the Nhe\n",
      "Iter-1530 loss: 41.9878\n",
      "e6, ho th oingrdcin the Grre taf EireaFd W. hesinn the torlat on mhe Gous sod bconx Shiurk. Japa. j:d\n",
      "Iter-1540 loss: 42.3662\n",
      "eRusir-tagane aleank7 he Eaprous topls pes leaIwyd Inaing mowls loraren hi yoseso: f-ian at fonf Jith\n",
      "Iter-1550 loss: 41.2678\n",
      "e Nopenote ta g2As dabed cichas img arlmeiny Japanuth tolferea anpsto lamies apen args onlote feeua d\n",
      "Iter-1560 loss: 50.2697\n",
      "ed of ircy-oany poryr Ktus - pof-se Wist Lhe pandeor. Fcoml7riono. Sritn  futin, ab. Japan,rjhe olice\n",
      "Iter-1570 loss: 40.5527\n",
      "e the sorlaliogist esjad inry 27hhibhe d Wes :20okt本 p2 fh-ocuand wivet rwxthe Olgeon exItomiin ispl \n",
      "Iter-1580 loss: 42.9470\n",
      "eKoumand Japen tore Wa, on-\"avlasean, so ared ingilat bnke mrer insu8mithiy lebyust The vomlotin the \n",
      "Iter-1590 loss: 46.8780\n",
      "elabicht mloure Sind rndexfanoee sopontsisic al cost ba lte\" at tn anjy sintithans ist-Japapesese par\n",
      "Iter-1600 loss: 38.5980\n",
      "eawar seandeonhise hoko, pof trofed orgnth intim alty isand mertyr rtheanlsIwepbin oIlve.naoe \"ana. J\n",
      "Iter-1610 loss: 44.3890\n",
      "eonpanitht doftiere oonDae toealardest in ingepenuruston tal cedecou,nho and ped wire to esof Japan, \n",
      "Iter-1620 loss: 43.7833\n",
      "eoInd onases unbt wnctclans loredslavitllofedthe wast worl. Iflrooky sf centhe wars th thestit laccin\n",
      "Iter-1630 loss: 37.5453\n",
      "ecpant aie sorneod pho fe h8Wsnkin Japand.e ans is liolapan,ed. pfrldeslarasthe Wer ionear\"u\"7kst-De \n",
      "Iter-1640 loss: 35.4685\n",
      "et the tured werre d ist Prgstarine Gh s9el iegtor mitmes as bo. Forcht:gankaes he ty cerodedHaneal u\n",
      "Iter-1650 loss: 49.2897\n",
      "es Jorad Et85n4poncea my anabiit  amvens6 lm.ges xif wircd hase Japanelt as 18e6 LEmramo. Tis thi lig\n",
      "Iter-1660 loss: 41.0548\n",
      "e torghe oonitin oredeany con lobg st nOty-oEladgel o2 tr paheigs ofesthe chot. se auns wontuelan ere\n",
      "Iter-1670 loss: 47.6030\n",
      "ed of Dovf mylseadertwon ifurop ial ofwabcal GgolCtunity iameluhe Csd cof uri hopht Domht Snat os a, \n",
      "Iter-1680 loss: 43.3050\n",
      "ese orfina psed his the Worth nf pinowor ssimit turealleliche the kpoth anthe paresd Japan Gr asInis \n",
      "Iter-1690 loss: 36.7557\n",
      "e. The wo-lo popmd by Japarwe crineand Wsican. lat er oum tovin cty ural, Sha oIniWs and N20os uits c\n",
      "Iter-1700 loss: 42.0258\n",
      "ed ar niop ouror dadae PalyD ind tirea a des sarlatg co.dlerunstod ax latgedes and ikee miban Codxean\n",
      "Iter-1710 loss: 46.4973\n",
      "eed he the souter Thil eurl mina . Lsticed a inithe Pw mrrec khbss unDof Nihe tap Natin oEesttl as pr\n",
      "Iter-1720 loss: 43.1696\n",
      "e kfountin che grmawir of kov0iec, Astand aog artoreing oalistcocEmDd warithe. Tse Meejaletity re ofr\n",
      "Iter-1730 loss: 45.9046\n",
      "expar eata ten eseltho ciumind Irlesr he a-Uo, 19, th che fepepereits and Wire rgurth :ncPulpor the J\n",
      "Iter-1740 loss: 48.3794\n",
      "eata inmpeIwiz d werin an hediwort pathi wared , winr isg Holhes avin, giWgn, 日orAn\"r angest Wored ma\n",
      "Iter-1750 loss: 37.3088\n",
      "e the A8red and dex1at rWm ced pur th -he pmgttowins malute in if ontes aas an ert houyts cerobynd si\n",
      "Iter-1760 loss: 39.1977\n",
      "e Japan's f. Eaty Japen ias mathe en liec.ar Kost  ppents the wopbou. NapenIesion of2'2 J2pesoit e. J\n",
      "Iter-1770 loss: 38.6324\n",
      "etanien ingsis wur onnoenks ufeanin ba f eomhOrery  ftyr as the Hmpety ssy 1in ixe high mowhe Easd Wu\n",
      "Iter-1780 loss: 41.4806\n",
      "etpSoald ar at cocwdesc hne Rured canith te tomigastty ismemelithr ebdess af hikg Iild S\"n)(ks iniold\n",
      "Iter-1790 loss: 38.9364\n",
      "e 1omind tupstorast. f20sor instregtios andgou. Tutrute alatyen ar omsolount7 lobhel Janj wheb ly-Jap\n",
      "Iter-1800 loss: 38.3371\n",
      "e Amery ind fevesogur. Tst mirey wibo, horantem oumelalerion 2. Nshthe Jathy Fkpoth the Sigtounestknd\n",
      "Iter-1810 loss: 39.1425\n",
      "er.arina, is aily Nac unatd it wof wectoro an five winhes thitslligite as 1oth sil ty cean miounta, s\n",
      "Iter-1820 loss: 40.9595\n",
      "evingthe anctopn\"rvinean mestlenciabees, ihe Ruts fviny Iithd TuEmfwlen ufmonter15liolds and uumbo0ss\n",
      "Iter-1830 loss: 41.8490\n",
      "e Japan's aa Dog lereai tho hio UNki h8 Sinints coptore In mopangr, tar oiges:un, winperere th cP Ee:\n",
      "Iter-1840 loss: 42.4842\n",
      "echapenatn te cerliry in sikedsJivenkal est Wareatshk Pered ur is ialthe er und Ch'ucEmopboulat wired\n",
      "Iter-1850 loss: 34.9953\n",
      "e:Japand( inanes lerese ar oHD7UaStio, taust. iy pioingsansila itit wyulotot ytpevelothe Warliogticon\n",
      "Iter-1860 loss: 40.6951\n",
      "e Japan's as fared on C9ithy an the erulies d Seith- asd indic and wietin oletaragerth if lopored \"ch\n",
      "Iter-1870 loss: 40.6057\n",
      "e paryeutiod 1hes omlie, \"iMe conbees fupun rn PlaUde, Sind Japancsogedsl ynJapencsasiondsn an cf omt\n",
      "Iter-1880 loss: 36.4624\n",
      "e8 Jhp日n witr es area, iotlald Dean ot, whe Ssrche ifu if Jape wir dorldiJapynntceanlorentouotal, whi\n",
      "Iter-1890 loss: 34.1313\n",
      "extin in cakuto\"ts of mat ondster ar y wend ltred atea seawad onmex,ansriy Incly.sst marean opegn st \n",
      "Iter-1900 loss: 37.1023\n",
      "es fopunthe asithe OreDtar and SesisurropinChiceresthea, 日hity gpry ictoukes sikl -fekboumikto Hom-in\n",
      "Iter-1910 loss: 35.9617\n",
      "e2. Ts oFupbt oipllerin e phe torrea. Jata 3e togeoro whes, the Eorte is ing Hopilotad atgy an coun a\n",
      "Iter-1920 loss: 33.4663\n",
      "e Aoped Japanjenoasdiold to ma noulan ins S2Rhtve an-,enor. t redopeonsivea, esto ts apmy. Axin tonol\n",
      "Iter-1930 loss: 34.1607\n",
      "elraken molate the dopnou. Iy es ind invlok. so anead thy Nixth. Tstan Ni, lapist\"s apiand Iires lhD1\n",
      "Iter-1940 loss: 41.2071\n",
      "e3thex anrivit in oy nitid Whe S mf3bon colfexd in,stotins, an, innte wopplaiend the Ungmsith- f poro\n",
      "Iter-1950 loss: 36.5656\n",
      "ecJapen is S,fuvin rla H本u, intind ae, piithomor ha eaed 1ngcveterialg pigustticinl rorederthe and If\n",
      "Iter-1960 loss: 33.3265\n",
      "er aiion ith canlomiser Ta, Japan's fLinor mentike opury. Kior filthetil it Canigotu mif chpsr whinhe\n",
      "Iter-1970 loss: 39.2745\n",
      "e S lhoks an and the G,uth. OroDten inster onst mankCos hoobape y. 6fEFbom tit ayc anani1y, po the un\n",
      "Iter-1980 loss: 37.5767\n",
      "evealay evexdond sd tom esiucth panceagety fr icoab, hs whestupextl-conxpurti. the Ertut 17,niand ie \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-1990 loss: 37.0352\n",
      "eveapyins diuly'cud Jupes thia tise Gn the bEmbh mouly. ro fn itere, anst-. omllreat the. Theld Sin r\n",
      "Iter-2000 loss: 34.3061\n",
      "eg ioty chan, wor he free in Naso\"ok.  9ina. pat t\"s h, Emituton Natt opel wis  ard Nhesoupery ssclfe\n",
      "Iter-2010 loss: 32.8434\n",
      "e6 worla, tho veumbed Gig thopeguman so 9hic afrt eocan, Titshis tha Ersturcen.appen, wareI il Gapiyg\n",
      "Iter-2020 loss: 32.3857\n",
      "e Soplobe ts ofOlyPlaxe thon Paland le 19ve 1mmoweol mios rofroen mpreomarldan Sty- aslaceanet2 ss of\n",
      "Iter-2030 loss: 33.2760\n",
      "e Jaran' Iyveseriond niry if uurereanctesfldsind in es lurar glliiert cond rudss folopand if the into\n",
      "Iter-2040 loss: 40.6070\n",
      "e Solrod desta calae tures inliosouriut in 1861847Japan'se and an iwhe Japand maresicin tur on ichan \n",
      "Iter-2050 loss: 35.6714\n",
      "erlasistr is the tomines imed, wosulup-on A86ryn isrtucopol cuneanin, baccof minlag ft. carycertae an\n",
      "Iter-2060 loss: 42.1363\n",
      "e the ocxmalgooftmeh sh fh sr urto-. Wo lory. \"h. 6fl9roit lhceceanaro te Sie tf otPlagesfhG Beaplete\n",
      "Iter-2070 loss: 37.2335\n",
      "e mare tivintauty onc9llrousithas lpctor cored iceg fm inolmeerod dsy parese iath kn AWry or eace-d u\n",
      "Iter-2080 loss: 35.9739\n",
      "eIrombnan, wog talitiran ed Ee trom d ropes iove bar  es hxpasurar Jopanard earag ein of 19ze: 1937 h\n",
      "Iter-2090 loss: 35.8717\n",
      "ekJapan's fo0atl olde fmurtv tcand uosunoi andes ofreat then, ans compmeltoin mad es anky Japand is t\n",
      "Iter-2100 loss: 35.2915\n",
      "expan eath Che tor hic1nold re the Ginesid sico. itho, Lakcy-puporalie tand in Gale west Japand1C\" he\n",
      "Iter-2110 loss: 35.6176\n",
      "e5s rrced crdd. rest ,utirest lehe Garg Hithe Indey, Freatoon anay.esturche whe ouphoch-siryn ty 1sth\n",
      "Iter-2120 loss: 40.4146\n",
      "es tonista cen exkyy Japon shousroundeolnomidlor ghoin med ang Win shi stocnathe womli's sou Win isil\n",
      "Iter-2130 loss: 30.8440\n",
      "e copky, mit y osteicln, of 1n8w9L0JapInans anerwhity pecereal tocunditheic pmcthed lf wintiiy is lir\n",
      "Iter-2140 loss: 40.1164\n",
      "e mare fires. mairy iolhop2ase thare fapin mak pecolerod connpan ensiod okpas, my cednat lterist Wnd \n",
      "Iter-2150 loss: 34.5656\n",
      "e wor t. coftyry ao ho Horian, panke wore Jh e-eangepetur miret pedarige teussef iont, Jakan rra eith\n",
      "Iter-2160 loss: 42.0959\n",
      "e pfrbetc Ty teleathe Eist Japsn sta osstureetoncturtro, tho eroulled.onteruas and fevtres covocanicy\n",
      "Iter-2170 loss: 35.3383\n",
      "e the historya ing chcaolery mouthn lDpen chopehelatd inapdicha aritht olimalleoelteropedtpy. mseana \n",
      "Iter-2180 loss: 30.7635\n",
      "ecnaresr and Iand R ofelavese th mEatt 2r6stedes cer. The counthe Wo menteeina, mligntag Nsrupiborh T\n",
      "Iter-2190 loss: 34.2919\n",
      "el1m3intry incopithd candies f. Dsealiy the Irleliwni Shith talaelehe at r ainter aroel anesy andestg\n",
      "Iter-2200 loss: 30.8779\n",
      "ere, tory reunain the forla in, Sin oroly ofabe G. khed,u\"onar  fapes fopionoaresth an loped hinat ti\n",
      "Iter-2210 loss: 34.0673\n",
      "e Sorld hisId.npeon mits hicha. The fwurt indeltin Wx beath  angenitie, Th wory encaband Igatho ed oh\n",
      "Iter-2220 loss: 38.7017\n",
      "e palate tid Woftiacl ft.dconrty tare 1omhe corury the Geliced To therclbed cerof Japanest es ledintb\n",
      "Iter-2230 loss: 32.9017\n",
      "ef irper, the carin ty esthe in th ceoresica-y. (;je Eape To akr9os. ppandgyg.dertselWint of th a, wo\n",
      "Iter-2240 loss: 32.2853\n",
      "e Warl inis toeandepcan es bopnnPleter ite ifed Thecy bmbeita ints. pat iory ihesinced lisulary.o, Ja\n",
      "Iter-2250 loss: 34.0189\n",
      "ed an kio lo lhivy 6opod wist chice talates ho loroInd intho lation in 1941 35wanl esusho chboons and\n",
      "Iter-2260 loss: 34.6502\n",
      "e, the alg and bulas fico-labde t. sr ecred Nipgnd t8 constie tue the iogthe capan, S. whin the G2ut \n",
      "Iter-2270 loss: 30.5951\n",
      "ectareis ldgese oos ex bo, min tome ane rictadase ind conorod Japen Eas fenit tirl er asicen oalleden\n",
      "Iter-2280 loss: 41.8594\n",
      "ed the thins and Sings olmeita aicanrel topiwan omestoe the Rifigutorags iolgsend higisith tomlemioll\n",
      "Iter-2290 loss: 34.5820\n",
      "etecof-padeldye eand ge tominth warl, which whter fopunpleight Guse:tf pelleroInt's wally onsess frio\n",
      "Iter-2300 loss: 34.7195\n",
      "ed the Wara ea, Iriky 193isl-oInaleitho mintar aldileterter. Afty tec ofe imlid Jivines sthd solored \n",
      "Iter-2310 loss: 28.1071\n",
      "eBrun of vito ghpiont tertar en ingmar thi ang torasarean shd Stst kedupoferil tarithl Tf the sth soc\n",
      "Iter-2320 loss: 41.4938\n",
      "e Iored and SoandTg th 1gereand Jf onourea insturlerg phomp mod bere cf-lapPeasitho ilan, Fhoky pR to\n",
      "Iter-2330 loss: 32.5003\n",
      "eTo, thicy meitar anst-Japancte iorler istr uns mperor cen The Gnr d.\n",
      "pso0rs fa esd rlyN20n Nf9本ss iJ\n",
      "Iter-2340 loss: 37.8751\n",
      "emebere coponilexiorborc mfexto EPand1r as,as ald ofipol eat Ciand Indebe mre snfeurst ine Soup20iss-\n",
      "Iter-2350 loss: 34.1587\n",
      "e Sorlnsop-stpentle ans ionare myoream oARnhPiatwor d, pokty to onargestred in iog anounoniopin EoI a\n",
      "Iter-2360 loss: 42.2651\n",
      "expar who opanceasiy In chs anrea. canate ig 1s6l hig mary. Japan  whr DTredic Ce pareio hiv ropbaulo\n",
      "Iter-2370 loss: 31.0420\n",
      "ee0pur efsexeand ar tyr ht Iictaone tan Espepgitth a, wof red af cofEm Warl in sHupoouke if th insthe\n",
      "Iter-2380 loss: 28.3672\n",
      "e Iof te merand If 194l y ind ou th istto entora ly wis gury -f dopang ming er asin. As and tor Chegt\n",
      "Iter-2390 loss: 34.9638\n",
      "elecrantoliteo lared. Sinthesnaf piv del hes bo theg Th latenta coalith laopen on the SLusi, perth ta\n",
      "Iter-2400 loss: 29.2966\n",
      "e Aaity ind p resocuronolivithe asgoniNaipin. co's canor Lte oko0 Kowas dast anarean she hi t as the \n",
      "Iter-2410 loss: 27.6141\n",
      "e Incthe7, poppetrix Lorld. Thes\" ankinstad ectrlary Sochobe caf oust. Japonas upeduthe f proziolits \n",
      "Iter-2420 loss: 34.1119\n",
      "ed fp tte is the ais Hom \"hel Japan os thers otios lome WErbe Nisthica leases iontite bpulmeuty fomei\n",
      "Iter-2430 loss: 27.0610\n",
      "ef Japan wopid tiven are momarean Nsvkukub). Jhpan tuly iss xithe FapUcd Wfibith c9mye femits Japan w\n",
      "Iter-2440 loss: 26.9759\n",
      "eina consertot tirecea, wiury of oiconocla. foraty pity isttr an lomy permaitan ye Napurery  hr isl o\n",
      "Iter-2450 loss: 30.2129\n",
      "e Japan's  oualitboked bopsd JaDists ial thr toe mirt the wprldeg sirloceanoy Jopar ct 5f the Wand In\n",
      "Iter-2460 loss: 26.9530\n",
      "evin, Japendit la the ont Selasd orta lare lioghe Eupctutory liwhd S rrog Jhtunkhs iemm-toend'icoporl\n",
      "Iter-2470 loss: 32.2993\n",
      "e mpretri f carsa imiian C. pare turon imapeso mInaloulhe Hokhe Wares andecindcut ergerbonsty be to g\n",
      "Iter-2480 loss: 38.9551\n",
      "e Ghe a, the tured., Anslrtate narcond crmeit. ureatinhe h sed ins Japan's  analite and the ao mgesol\n",
      "Iter-2490 loss: 30.1167\n",
      "e Japancedin tor ter and of stuc an ese romhd \n",
      ". ingJstunas late.ssa an talgesin is 1u45 2020Ls ffiin\n",
      "Iter-2500 loss: 32.4424\n",
      "e) \"iond r-d dured whro ostom ibtn of gesorexthe 14liily curhisn fa ith tar Cian an odg tho iogthe as\n",
      "Iter-2510 loss: 25.7814\n",
      "e Japanke Japand Wanchagesturs. fiint. Ts5:.pift andiIlyist whe tof panerith iomns andectontse ing mi\n",
      "Iter-2520 loss: 30.5054\n",
      "e paroeltinis and Naithowelatand Eivelechinand is 1 mirgest looniry onten nary sscopanteo ind as tive\n",
      "Iter-2530 loss: 26.9181\n",
      "en \"op the aeatan, thtweliclmer thicht lopeke enity oelaC. Warcy. Ts asa inorlate Atias ingsse who d,\n",
      "Iter-2540 loss: 26.4466\n",
      "ed the Gald insex andes sis thd Nmetgelirot eang rilecongerhLNes\"opor'ich intIcledeina ,ywoJatan ansy\n",
      "Iter-2550 loss: 29.7662\n",
      "ekicotheri onlirhd can, ih. lpbon h. lat cetourcofiolo and Indes chboa ded hext meitur penclored con \n",
      "Iter-2560 loss: 30.5063\n",
      "es countthy fexsal tho lorli's fapat rlalgeEs pana ke Ta Wof pestomy proured aid H. FruWr fs. LN0apa本\n",
      "Iter-2570 loss: 29.7035\n",
      "eu, thhic ponite wur he darty onolisingmy an cwef bolesInnd Ingrle fecranicwol pised pakcourlame Arli\n",
      "Iter-2580 loss: 31.9790\n",
      "er. Japan eas cfpimllopakcoured migite uiritle iw themingsa, maily umprut itho Ungusy  srid toand ta \n",
      "Iter-2590 loss: 29.9086\n",
      "ef Japan Ease ooth lat es af Sindespadola an\"g realotrean Gig por8g nomy seadisiths ondestwe ch. Asit\n",
      "Iter-2600 loss: 26.4416\n",
      "e Nagront Nacy-Ofcourthto the mprexga inis. mopeans iodn flaceored if Japan\" wiatD in ingghes in th r\n",
      "Iter-2610 loss: 29.2294\n",
      "e world'ic as chisn miity. Glg arrdyr sh Japanase 19Was Eminabaus worlite y othely omimrter idetur ar\n",
      "Iter-2620 loss: 29.7441\n",
      "ed Aoweilo ontarese masbon WamleNo. bSeas rod sereanctofevitlte 19tan anstirss arsea, Csiledinad Ithe\n",
      "Iter-2630 loss: 26.0676\n",
      "estar eicglmed an Sokto uraod M93e7 Eslermerctinthi wmert-inpanolasin e as 1owh. Japin ias ata-de tin\n",
      "Iter-2640 loss: 24.7408\n",
      "en toustore to-lapa.led an Aita ion in the worl his erekUdTrexcaudirh. Warcy. us inh boundin encanite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-2650 loss: 28.6133\n",
      "e worrd in msepoof ixur Inieseano th fEusbce eoflien itg eceperurb io sn iss ko0本 \", pares war dinay \n",
      "Iter-2660 loss: 31.3776\n",
      "erJ-panky paree i the Riston \"ofe er regeamion. n8ry, whi obal Donnthe angoy tyrestat on t9elilila, K\n",
      "Iter-2670 loss: 25.8980\n",
      "e: ropsr fiola, inter andend is the nwmrtaenas the almeoke oniagin turevty ingtist isto yaly sith- an\n",
      "Iter-2680 loss: 27.9598\n",
      "e, whr h atginly ppacanes wirnd cures s sod fven od Ing bor d, hh comDd itig Win ortd f\"Runce-taml1bn\n",
      "Iter-2690 loss: 35.9091\n",
      "ed che Word sn riea, The constury. Tis c:ugny rive Japan th whes sur trokad the Gfury. Aseandestor wa\n",
      "Iter-2700 loss: 27.9956\n",
      "e Sof ist  smargins a aed wistd wevelichin thity-lereot wof Hropantile woploun he Nthe Nlate  irthe f\n",
      "Iter-2710 loss: 35.8363\n",
      "e Sirgns cofboundiand D anrei olme in rur UChirin wom 1stcynpext roulhopan, wakeb, ghe fds af The Pow\n",
      "Iter-2720 loss: 46.2626\n",
      "e Japan'in andpcroutrd s8vy fO, pan tarins andeIon bed bofedin thevelWog the Esperess rrcorxUss ans. \n",
      "Iter-2730 loss: 31.7245\n",
      "en pornd iis thigan eliin 18Th andse merean ed Japan iou anasd ing the wofed logins ipse fo-bt,-d ith\n",
      "Iter-2740 loss: 28.2936\n",
      "ed cho thisan Sed apecof tig th gurthy Wacthe e Ward \". Japan esta destom ity rumitho flropeacd watyt\n",
      "Iter-2750 loss: 33.1621\n",
      "erintoce pur est palawiold in the exppored ird fored Im Wory 25i falitd Ram eaten in the wofbia. Japa\n",
      "Iter-2760 loss: 27.9342\n",
      "experorth lorons rered ith colateos lf ifithe prxusunith i7 levever of tho edind Sed it llories tudex\n",
      "Iter-2770 loss: 26.9323\n",
      "ed ar at et oldealasith inompeand Sia, Japan so tiece laed en i7thIdaa Eas anithe iopmichoup ouros la\n",
      "Iter-2780 loss: 23.8067\n",
      "eccan re fhe6nUandepapas thi an ee forts limalacrithion sof insthe GlWro'g totes arla. Ahe trulacabd \n",
      "Iter-2790 loss: 35.0779\n",
      "eccantir the sisclaledein of oupkof J pancit thu, inthy Wo- ana an a. thona ceowat iand sh fpupbed au\n",
      "Iter-2800 loss: 30.7865\n",
      "e War of Japares cocldevcnloginac fme5bolacandes ch ture as aC TOke94l cWollesm iounare tulmety tures\n",
      "Iter-2810 loss: 33.6293\n",
      "e the chultod shon Wast sokea, fhis. Asthe chf ontorha roDd by raty NapoIndex andian areativ. mhe of \n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 1 # depth\n",
    "n_iter = 10000 # epochs\n",
    "alpha = 1e-4 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
