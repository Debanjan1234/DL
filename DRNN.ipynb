{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "with open('data/text_data/japan.txt', 'r') as f:\n",
    "    txt = f.read()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    char_to_idx = {char: i for i, char in enumerate(set(txt))}\n",
    "    idx_to_char = {i: char for i, char in enumerate(set(txt))}\n",
    "    \n",
    "    X = [char_to_idx[x] for x in txt]\n",
    "    X = np.array(X)\n",
    "    y = [char_to_idx[x] for x in txt[1:]]\n",
    "    y.append(char_to_idx['.'])\n",
    "    y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "import impl.layer as l\n",
    "\n",
    "class RNN:\n",
    "\n",
    "    def __init__(self, D, H, L, char2idx, idx2char, p_dropout):\n",
    "        self.D = D\n",
    "        self.H = H\n",
    "        self.L = L\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.losses = {'train':[], 'smooth train':[]}\n",
    "        self.p_dropout = p_dropout\n",
    "        \n",
    "        # Model parameters\n",
    "        m = dict(\n",
    "            Wxh=np.random.randn(D, H) / np.sqrt(D / 2.),\n",
    "            Whh=np.random.randn(H, H) / np.sqrt(H / 2.),\n",
    "            Why=np.random.randn(H, D) / np.sqrt(H / 2.),\n",
    "            bh=np.zeros((1, H)),\n",
    "            by=np.zeros((1, D))\n",
    "            )\n",
    "        self.model = []\n",
    "        for _ in range(self.L):\n",
    "            self.model.append(m)\n",
    "            \n",
    "    def initial_state(self):\n",
    "        return np.zeros((1, self.H))\n",
    "    \n",
    "    def forward(self, X, h, m):\n",
    "        Wxh, Whh, Why = m['Wxh'], m['Whh'], m['Why']\n",
    "        bh, by = m['bh'], m['by']\n",
    "\n",
    "        h_in = h.copy()\n",
    "        X_in = X.copy()\n",
    "    \n",
    "        X = (X_in @ Wxh) + (h_in @ Whh) + bh\n",
    "        h, h_cache = l.tanh_forward(X)\n",
    "        y, y_cache = l.fc_forward(h, Why, by)\n",
    "        \n",
    "        cache = (X_in, Wxh, h_in, Whh, h_cache, y_cache)\n",
    "\n",
    "        return y, h, cache\n",
    "\n",
    "    def backward(self, dy, dh, cache):\n",
    "        X_in, Wxh, h_in, Whh, h_cache, y_cache = cache\n",
    "\n",
    "        dh_out = dh.copy()\n",
    "        \n",
    "        dh, dWhy, dby = l.fc_backward(dy, y_cache)\n",
    "        dh += dh_out\n",
    "\n",
    "        dX = l.tanh_backward(dh, h_cache)\n",
    "\n",
    "        dbh = dX * 1.0\n",
    "        dWhh = h_in.T @ dX\n",
    "        dWxh = X_in.T @ dX\n",
    "        \n",
    "        dX_in = dX @ Wxh.T\n",
    "        dh_in = dX @ Whh.T\n",
    "        \n",
    "        dX = dX_in\n",
    "        dh = dh_in\n",
    "\n",
    "        grad = dict(Wxh=dWxh, Whh=dWhh, Why=dWhy, bh=dbh, by=dby)\n",
    "        \n",
    "        return dX, dh, grad\n",
    "\n",
    "    def train_forward(self, X_train, h):\n",
    "        ys, caches = [], []\n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "            caches.append([])\n",
    "            \n",
    "        for X in X_train:\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], cache = self.forward(X, h[layer], self.model[layer])\n",
    "                caches[layer].append(cache)\n",
    "                X = y.copy() # Output of previous layer == input of next layer\n",
    "            ys.append(y)\n",
    "            \n",
    "        return ys, caches\n",
    "\n",
    "    def cross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        prob = l.softmax(y_pred)\n",
    "        log_like = -np.log(prob[range(m), y_train])\n",
    "        data_loss = np.sum(log_like) / m\n",
    "\n",
    "        return data_loss\n",
    "\n",
    "    def dcross_entropy(self, y_pred, y_train):\n",
    "        m = y_pred.shape[0]\n",
    "\n",
    "        grad_y = l.softmax(y_pred)\n",
    "        grad_y[range(m), y_train] -= 1.0\n",
    "        grad_y /= m\n",
    "\n",
    "        return grad_y\n",
    "    \n",
    "    def loss_function(self, y_train, ys):\n",
    "        loss, dys = 0.0, []\n",
    "\n",
    "        for y_pred, y in zip(ys, y_train):\n",
    "            loss += self.cross_entropy(y_pred, y)\n",
    "            dy = self.dcross_entropy(y_pred, y)\n",
    "            dys.append(dy)\n",
    "            \n",
    "        return loss, dys\n",
    "\n",
    "    def train_backward(self, dys, caches):\n",
    "        dh, grad, grads = [], [], []\n",
    "        for layer in range(self.L):\n",
    "            dh.append(np.zeros((1, self.H)))\n",
    "            grad.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "            grads.append({key: np.zeros_like(val) for key, val in self.model[layer].items()})\n",
    "        \n",
    "        dXs = []\n",
    "        for t in reversed(range(len(dys))):\n",
    "            dy = dys[t]\n",
    "            for layer in reversed(range(self.L)):\n",
    "                dX, dh[layer], grad[layer] = self.backward(dy, dh[layer], caches[layer][t])\n",
    "                for key in grad[layer].keys():\n",
    "                    grads[layer][key] += grad[layer][key]\n",
    "                dy = dX.copy() # The input for the next layer is the output for the previous layer\n",
    "            dXs.append(dX)\n",
    "            \n",
    "        return dXs, grads\n",
    "    \n",
    "    def test(self, X_seed, h, size):\n",
    "        chars = [self.idx2char[X_seed]]\n",
    "        idx_list = list(range(self.vocab_size))\n",
    "        X = X_seed\n",
    "        \n",
    "        h_init = h.copy()\n",
    "        h = []\n",
    "        for _ in range(self.L):\n",
    "            h.append(h_init.copy())\n",
    "\n",
    "        for _ in range(size):\n",
    "            X_one_hot = np.zeros(self.D)\n",
    "            X_one_hot[X] = 1.\n",
    "            X = X_one_hot.reshape(1, -1)\n",
    "            for layer in range(self.L):\n",
    "                y, h[layer], _ = self.forward(X, h[layer], self.model[layer])\n",
    "                X = y.copy()\n",
    "                \n",
    "            prob = l.softmax(y)\n",
    "            idx = np.random.choice(idx_list, p=prob.ravel())\n",
    "            chars.append(self.idx2char[idx])\n",
    "            X = idx\n",
    "\n",
    "        return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(X, y, minibatch_size, shuffle):\n",
    "    minibatches = []\n",
    "\n",
    "    for i in range(0, X.shape[0], minibatch_size):\n",
    "    # for i in range(0, X.shape[0] - minibatch_size + 1, 1):\n",
    "        X_mini = X[i:i + minibatch_size]\n",
    "        y_mini = y[i:i + minibatch_size]\n",
    "        minibatches.append((X_mini, y_mini))\n",
    "\n",
    "    return minibatches\n",
    "\n",
    "def adam_rnn(nn, X_train, y_train, alpha, mb_size, n_iter, print_after):\n",
    "    M, R = [], []\n",
    "    for layer in range(nn.L):\n",
    "        M.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        R.append({key: np.zeros_like(val) for key, val in nn.model[layer].items()})\n",
    "        \n",
    "    beta1 = .99\n",
    "    beta2 = .999\n",
    "    state = nn.initial_state()\n",
    "    eps = 1e-8\n",
    "    smooth_loss = 1.\n",
    "    minibatches = get_minibatch(X_train, y_train, mb_size, shuffle=False)\n",
    "\n",
    "    # Epochs\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        # Minibatches\n",
    "        for idx in range(len(minibatches)):\n",
    "            X_mini, y_mini = minibatches[idx]\n",
    "            ys, caches = nn.train_forward(X_mini, state)\n",
    "            loss, dys = nn.loss_function(y_mini, ys)\n",
    "            _, grads = nn.train_backward(dys, caches)\n",
    "            nn.losses['train'].append(loss)\n",
    "            smooth_loss = (0.999 * smooth_loss) + (0.001 * loss)\n",
    "            nn.losses['smooth train'].append(smooth_loss)\n",
    "\n",
    "            for layer in range(nn.L):\n",
    "                for key in grads[layer].keys(): #key, value: items\n",
    "                    M[layer][key] = l.exp_running_avg(M[layer][key], grads[layer][key], beta1)\n",
    "                    R[layer][key] = l.exp_running_avg(R[layer][key], grads[layer][key]**2, beta2)\n",
    "\n",
    "                    m_k_hat = M[layer][key] / (1. - (beta1**(iter)))\n",
    "                    r_k_hat = R[layer][key] / (1. - (beta2**(iter)))\n",
    "\n",
    "                    nn.model[layer][key] -= alpha * m_k_hat / (np.sqrt(r_k_hat) + eps)\n",
    "    \n",
    "        # Print loss and test sample\n",
    "        if iter % print_after == 0:\n",
    "            print('Iter-{} loss: {:.4f}'.format(iter, loss))\n",
    "            sample = nn.test(X_mini[0], state, size=100)\n",
    "            print(sample)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-10 loss: 104.0063\n",
      "edite obA r h chrat t9.abodeca mw,'hna eth JyytvtEdmpm ta   l iehaeonthihla fl thsl  aiwkn  i t du1e \n",
      "Iter-20 loss: 91.4188\n",
      "ee ifmacesaCtuoron mT \"lctd I4danneg tonkiJ Nne Conedyarxrrr an .apolxrfegoil nun t8e fhp wacans aaaa\n",
      "Iter-30 loss: 83.8331\n",
      "e oatlxee Jepesnl rasl R8Hin eeopany ase ondteks a2,adedey 3oswoten cCtoran tira ane er ard phidoph S\n",
      "Iter-40 loss: 79.3704\n",
      "e8 Th af a9lha) al6ed Irxohons yunird was gapiWiTrgep atintt w oonmOas ar ase osmeglaner Cninds an Ao\n",
      "Iter-50 loss: 76.2756\n",
      "ed rathis ota% ras fhan aaol Winnetaer pered aonh licger, chiscunte sf an pist aintadlaldi-pal eskad \n",
      "Iter-60 loss: 74.0596\n",
      "e fhons Ip Se-cer-m fity Sindicy wsmed opbires oJ the to 5r wi on ices ine bath lased al the sulint a\n",
      "Iter-70 loss: 70.0485\n",
      "ed fekmune, Ar, )heru om. aseeg riof en fiotisensushir innd 1upatered rist oso2seato y Daricll.exthe \n",
      "Iter-80 loss: 68.4188\n",
      "ed pimh eaparly of if miansy prop Nedestih my anth then o-nh to sichithh worndas fintrtri1 Someand a \n",
      "Iter-90 loss: 67.6075\n",
      "e Rinlns alopur eard 47 Japaniong's pal tee 4n in lig p, h fy on thy earcodla and of mye Havomed lala\n",
      "Iter-100 loss: 66.1216\n",
      "e Nan2scinlnel 1frgemlouth mar, Japerild 1f of 3n ia. Japaned 1ad thy the Ky in. Khotal i7. r-verotto\n",
      "Iter-110 loss: 64.5125\n",
      "e morlof e Japan ch in re42 Sured Japan inibu, ind ron tiof of The Eonnises inld Neaod is raduntiin e\n",
      "Iter-120 loss: 61.3731\n",
      "enwiond Sea lt\"%bth8 1srand 1amle Eatithe Gcoperoty my whed 1fropin of wechinited 日anciawolzerea, The\n",
      "Iter-130 loss: 59.5026\n",
      "e 2onintec unallis. Warl pata und m lori5g Checild in of a, agaid cenmur tanyliog pur pety is srrexwo\n",
      "Iter-140 loss: 57.2542\n",
      "e Sestan'n ma. Apmnr fand chi1de6 onnkiste percond nolicttis 29eomina. patizy os if Japanl, syurgted \n",
      "Iter-150 loss: 55.9255\n",
      "ed tyobak co nhed Ha and fitle Dyurac alace of is and is craid of the Gu acaly whesian Du hares btmer\n",
      "Iter-160 loss: 53.4886\n",
      "e fored Japan ty ledumoct a gounte moctdectary the Repertite6 Jardev1 \"aud in tho of Aharcerke uatiry\n",
      "Iter-170 loss: 51.2358\n",
      "e worlni. enpbce pore gitiett otiow ki, Japan is o9wh chope Aporic, phar onjid inveris eald chion of \n",
      "Iter-180 loss: 50.7528\n",
      "e lorto5ge. The ciuntexe larD the isule cisbenh ropungeof Japan iag paroryd isto freumon loreg fto Na\n",
      "Iter-190 loss: 49.1974\n",
      "e ear%ted Dev Aharly wonky ce the lok. my wir. paenko firld Japan ked dy th menshis Japaned-mar, tte \n",
      "Iter-200 loss: 49.4245\n",
      "e eald fou and thexpoppen. and Afer. aald is ando, cofhis, chounth acln-, wos-wion on isme mifecensii\n",
      "Iter-210 loss: 45.2618\n",
      "e warabed lasm Gumite wese N8\n",
      "lilatur ourld ctelito of 19sh if tsecond sonid is, ease en, Tho an Chic\n",
      "Iter-220 loss: 45.7720\n",
      "e adthine Paceth-larwon, farlaterucbaruro, ther ona, rour, is d Japan CoTey porlinsy55– isgery xbeut \n",
      "Iter-230 loss: 46.9295\n",
      "er onven moko, the seargen regole tomite tion is wenter. ThUrured Wor, Ahan laricp hrgit%cund Japar's\n",
      "Iter-240 loss: 42.7727\n",
      "expalatusto Un th-larns tneceatar5d G2opren is to is itesrof areas the Gberter O2 certi anipr. untaky\n",
      "Iter-250 loss: 41.0005\n",
      "er eathso perpelity in; a Etre ioliitd on expe unmto Un 1adreatce 1adanwy in linsexing ran enst bith \n",
      "Iter-260 loss: 44.1720\n",
      "ered of insres andecins fs ofer, Japan thare miceulo sin7beine Eant at ion, O本 Ataih miceso pose the \n",
      "Iter-270 loss: 43.4611\n",
      "e eTpiotjn area, chimar, ta fres an Wonlislen ered Japan is asGaliof cethe am and KinEins fd Jard iis\n",
      "Iter-280 loss: 38.0710\n",
      "er allth of Nihesed firld esror buls am 1 Han hisemegatiind ny palicas and in 12, larla2, third cons \n",
      "Iter-290 loss: 37.7868\n",
      "erlost licithic hilatesu bilo in 1947,ux,h fountt fy Nacith in thar Centoxmee, futhe Sed amion in the\n",
      "Iter-300 loss: 42.4906\n",
      "er thind miENseop is chalg regoftitiinsex anditar tithe Earale mrerpangumyion ond Nelectin' emelomler\n",
      "Iter-310 loss: 42.5648\n",
      "e country wouth m purto from elpthe to an o piopbin und lontir highurte to fror and hararuntin to and\n",
      "Iter-320 loss: 35.1107\n",
      "er alling risulor. Japan wln puulocl an Wolld War Japaka, a wert the fp\"litaly in tenturyir, l9unt of\n",
      "Iter-330 loss: 37.4548\n",
      "e tored Gulociluda, eamest are 15g pakh couth-2e androl5d East isle acl. mald. Apped ots anke De pntr\n",
      "Iter-340 loss: 33.5833\n",
      "er sigh bee, Nye as Horowit\", rantion is mlerurto and. Rutarye 1tollintke Uanth-capanited has the 3ro\n",
      "Iter-350 loss: 42.0579\n",
      "er and Nhacupan 12015–Dea 2n SNoban thime Singe momur ofs. Japan davontse OD\n",
      "hhas in Aperon ts funco \n",
      "Iter-360 loss: 40.6647\n",
      "er8on Hinited Won and earko cevinder. Lanainmiint oreso nnee int an biltorin mexen teg per. The World\n",
      "Iter-370 loss: 35.6680\n",
      "e norgest bopolatexDexmanest aroperper, in 12civiny. Japanes5 petter rmo, bofcemmeiced is, and Inko o\n",
      "Iter-380 loss: 33.7252\n",
      "er Totion in the GLe at and bopher ta froped ih 1868 d and has vicgest peafer and fo chos and peverop\n",
      "Iter-390 loss: 35.2118\n",
      "e world Wonlle Chares into sea, ko hariontury 4n Jandenion in, fsuApe eule cimh ea of War al, of of o\n",
      "Iter-400 loss: 33.2138\n",
      "e torited lorldn the mefiolo an Asolan in exran in the world's Wonland if the south to the wares mboD\n",
      "Iter-410 loss: 36.9436\n",
      "ered 1sce four Inde56 Apal ving the witt and Chosh-lasis the Emperie tatg in the 19fst Was charur a m\n",
      "Iter-420 loss: 38.1288\n",
      "e fouthese shorteth aunted Niolicture mether rads. The astertictent in darly as the tedme moress and \n",
      "Iter-430 loss: 33.1498\n",
      "es ofdigan Decharcitito its. The fembo9 ee Gnd oplan learla Euph macing ris hurth teobewed wurl bopal\n",
      "Iter-440 loss: 28.7261\n",
      "er and ftichicath cowar G7 frUcter, in has Japan tes of Japan, whinh Rrsin in 19\"D of in ic eard's te\n",
      "Iter-450 loss: 35.8194\n",
      "e nhounoke werling in the the a. Fromithe nourth' if the fontr o pallen, Wosland Sinof reveu in dixal\n",
      "Iter-460 loss: 32.8141\n",
      "er futhsexeango, and ourler rath tn, xroulat. unJapan 1y bre borod-locoper popudritat-lhecturye ly Ni\n",
      "Iter-470 loss: 32.4966\n",
      "ew oteowad an prripicestenticheto aa a Bpobcorty bvermhare und Aroplanes arls1 G7reve the sirit-ku. t\n",
      "Iter-480 loss: 28.6570\n",
      "e world Warlim, the fea, Sintier anomed Im in 1ugation. Approxd pented alU,inn cturroso Inkbope the O\n",
      "Iter-490 loss: 31.0217\n",
      "er hichunes vecmerod HirEteaty of Japan the couth. Tored tte an ofte expinsowe tecceapentiom Japane d\n",
      "Iter-500 loss: 31.2489\n",
      "e tored in and wakg rinth four ing in 1947, rhas Asund is thas Amplen is fromion in the sucthiur oper\n",
      "Iter-510 loss: 30.1330\n",
      "e ton is troped anuki6 glles ristent bea, as and Natare with o8 opper in Cons thc Numwhico wonth oama\n",
      "Iter-520 loss: 32.7060\n",
      "e Sya ritigesran in the earld Winty. Japan ta wes srvered bured a-khof the war, the GLe ot mien evelt\n",
      "Iter-530 loss: 28.2736\n",
      "e torodofegnd-lomallcinhs apled as mimh Sincand un ats rigiths fcl6 tencesirslef Tokud Chincaugoly, i\n",
      "Iter-540 loss: 27.5684\n",
      "eg rea decheg sighict rast-largest exportisisoun med Ristiom wity of Japan in tith ciles arcedeconter\n",
      "Iter-550 loss: 31.3827\n",
      "e world's vexwonty, The ctuuthi Srand The Emperoinsect emper ol6keot\n",
      " 4t bofoufos and cond mirion 17w\n",
      "Iter-560 loss: 30.7000\n",
      "ed first the furmest as an lirto somlainy, f8. Hitaruvef Noled and camanns f mbor Toky, the sumth lor\n",
      "Iter-570 loss: 46.6766\n",
      "ef ol1, which of fhe and Asrod Japan hichaod of the 9ald the higita tounita 17uld ban pobeseanklintin\n",
      "Iter-580 loss: 37.1188\n",
      "e tored and lond forlalopolecturesy thorcand perpacl the G2 Noatn to and the World Wonth and in olal \n",
      "Iter-590 loss: 28.5772\n",
      "e world's lonteke won anly fromiod of insyceadye6e-dewu tenn1d opwer in thilak. ra mpon the world's f\n",
      "Iter-600 loss: 30.6191\n",
      "er r of. Japanese poboping in the Empertest Rmper on thind in 1947, Japan hu lola.evith to whe natite\n",
      "Iter-610 loss: 41.6825\n",
      "ed ates Sane. Srarir, ricite parit. Japan to ixten heverof 19sbe Pof and countr n texconod ban Japdle\n",
      "Iter-620 loss: 35.2792\n",
      "er hidath laursila, the \"spnir the an 1In, which iala whime fsirity iagarot is and chint p ry porian \n",
      "Iter-630 loss: 25.2436\n",
      "er8ake folla. Lountorespe9mHr we the lath l56ed CstereOchursko Oo oprk b8d maita wed, is lard World l\n",
      "Iter-640 loss: 22.1612\n",
      "ed I\"nhes eved las. Tf itrur. malalan in the nrme med Panomol f malatited Warled intlbenomyate expala\n",
      "Iter-650 loss: 23.7870\n",
      "ered ins also ons, to 3h cally withe nandeased as ir 1nG. Japan tes T. Hal3e expareslewed, the East C\n",
      "Iter-660 loss: 32.7508\n",
      "ed ate, in the G7bal Emal perthitary righony byolle a-ligrod. Narase cicl the Mand oukske peopal .eme\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-670 loss: 37.2381\n",
      "ered into reg poresease cegorion sting th lurleuf of en8. last an lontistion of Japan tited is tro, a\n",
      "Iter-680 loss: 23.3519\n",
      "e wosg profof Le courlity. Aplre 19ghite eastoring the sighan 15–20ed, Ty ced has pentens flgrsilita,\n",
      "Iter-690 loss: 23.3475\n",
      "ed hess and from the gan6\"stereso hasban mented and peacunal ly and Wand Taly wige intlorof inteing A\n",
      "Iter-700 loss: 29.7127\n",
      "e world's as the Gmeto as who aast an lamand is and-bal rigest-la fourest Wonofs. Kolal, in Asis, thi\n",
      "Iter-710 loss: 31.7488\n",
      "e tosed il lard Japan so Oandexenmek7keopop Chin, fn 920s an any Ca wa and ixman I aigity lealled feu\n",
      "Iter-720 loss: 23.2216\n",
      "er aunlitary The For, rogeto in a nighily pare itoly linto se as fhimeChin's hinn pexthated naty of J\n",
      "Iter-730 loss: 19.9716\n",
      "e colaistu 9asd lakekud aed the Gighedd Japan th ne Asta Eas tre katy Suswing is fored and centh call\n",
      "Iter-740 loss: 20.6436\n",
      "e wotur temcorion is and Rision. Wa and wition is figh as Gs, wats the couthitewiplhe: lakestets telc\n",
      "Iter-750 loss: 18.6603\n",
      "er atly Cond, Ol winto aankespopmalloto was perto 1945 Denoto makailo aales of Neandopurrol. Tilaiotu\n",
      "Iter-760 loss: 27.7402\n",
      "ered of the four a fegiots and fiud tha noud is texten, somed mixtigest eltr ove  asuryin 1y atosules\n",
      "Iter-770 loss: 27.9618\n",
      "e world's vevol en in the Ubeite. Ae-pila. The dlmole curs th cy with oven thicster ind Walied Nipuri\n",
      "Iter-780 loss: 31.8945\n",
      "erea. The wighust opkhinst rames olk. manrirs myopur il. the Eupilor wurityt ly Inyo end Wan in tules\n",
      "Iter-790 loss: 18.9282\n",
      "er aticl. mand an \"s pespuns th I Japan was rectal Gm p8rle suntoreasing Sreovel aropor of the Empore\n",
      "Iter-800 loss: 23.2675\n",
      "e norl, to and Toolitate cestorion of Nespand Sh Japaner as the foures thin teom purat best-sicten ta\n",
      "Iter-810 loss: 18.9797\n",
      "e  af chando Eany consed Ritaitat, rixth incland. Japan's as urobest the Sudoto, lharof iscesowha whe\n",
      "Iter-820 loss: 18.7471\n",
      "e wosld's feurme-ch wikh the G2nfenjt or Japanesicnlial peanes ons, Winglonogukharve pirl warges. The\n",
      "Iter-830 loss: 24.2893\n",
      "ed anctes secchan 17u lerized was prof the Sexpan heac Empero. Japan, which islald pmor ana, wst er p\n",
      "Iter-840 loss: 27.1797\n",
      "e nolal peoplosuredoland country ino-Jandour pint as crmestory Sea an Drated corhed land chuntry wan \n",
      "Iter-850 loss: 20.2628\n",
      "ered inr an Crin. rarthi an of the Emperor tith id fhar Japan was restory in Athite pintion of Japan \n",
      "Iter-860 loss: 22.2442\n",
      "e norstio tarisingitat ar adoJwighog Nbeas makined a thercest romiing andest surio, whico hics the wo\n",
      "Iter-870 loss: 15.6148\n",
      "er a, untites and fn 9 che dalate ciloita mako malis name memter G8, and is 1uft of Japan iand ito-pa\n",
      "Iter-880 loss: 27.7341\n",
      "e wostere Blontithitest lentent chiditat, ncopar Indorate easl baritions. Japan co the Emphan inteto \n",
      "Iter-890 loss: 23.3058\n",
      "e noron.. IIstioreanit cilElicalicand Seecerth 20ensed iEd.olalyy ing the Glalor inty in 1830eamperie\n",
      "Iter-900 loss: 17.4246\n",
      "ed fiwod und Indextener of Japan'd fe, dect operine Aurler thited lard of are9 ans.-Jasb of Okhopnh i\n",
      "Iter-910 loss: 30.2780\n",
      "e Japan Opero-Japan Ololles as 194Ding perionty In tead pistr rea an meve 9ry, powod Su hirsece-Japan\n",
      "Iter-920 loss: 20.0518\n",
      "er8ty of-raplercosith intumake popion o. Japan is hasirl. The compory rarid acands Sugeaturaunsed hir\n",
      "Iter-930 loss: 43.0249\n",
      "e  thites and fro, the Glaters the nonionsiion segirheswen witeto and fis, the wost axio in the Earan\n",
      "Iter-940 loss: 28.6469\n",
      "e Jwoth le and r. marenor aI G7, Japwhe ty as frl tencedofo. The ligse to fnd elny fhectertoco lith t\n",
      "Iter-950 loss: 23.7234\n",
      "e world'dulon1, mekidestio the wennio the Russ. The folalliop Decturiod ry wened War, the storer and \n",
      "Iter-960 loss: 13.6855\n",
      "e world, wigens r Un-iands rentur of 12akur wint an lore. N8ror whiin maly fo-9alaleg cilat cath in t\n",
      "Iter-970 loss: 12.5357\n",
      "e woths Toril. gheet pal Emperowod feo, the sisth ciility. Arthe The Indomerporlargest ecoperies in e\n",
      "Iter-980 loss: 12.2578\n",
      "ed fitions a the cest reased illlest sistu 1asler d country in is rist ry his the UDpe sonots mykfeta\n",
      "Iter-990 loss: 27.1583\n",
      "e world'd largest lath inceadined and thised eumth buritekyo andes Tlothol. Japanest the firstion ts \n",
      "Iter-1000 loss: 32.0993\n",
      "e JOnatese ed and a 4t proporer issions and, wiged-largestu ithe Gl aces and issuthions isod io linti\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkq5aEJQSikFwP8DdNW42wtV9yMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1023821d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "time_step = 100 # width, minibatch size and test sample size as well\n",
    "num_layers = 2 # depth\n",
    "n_iter = 1000 # epochs\n",
    "alpha = 1e-3 # learning_rate\n",
    "p_dropout = 0.95 # q=1-p, q=keep_prob and p=dropout.\n",
    "print_after = 10 # n_iter//10 # print training loss, valid, and test\n",
    "num_hidden_units = 64 # num_hidden_units in hidden layer\n",
    "num_input_units = len(char_to_idx) # vocab_size = len(char_to_idx)\n",
    "\n",
    "# Build the network and learning it or optimizing it using SGD\n",
    "net = RNN(D=num_input_units, H=num_hidden_units, L=num_layers, char2idx=char_to_idx, idx2char=idx_to_char, \n",
    "          p_dropout=p_dropout)\n",
    "\n",
    "# Start learning using BP-SGD-ADAM\n",
    "adam_rnn(nn=net, X_train=X, y_train=y, alpha=alpha, mb_size=time_step, n_iter=n_iter, print_after=print_after)\n",
    "\n",
    "# # Display the learning curve and losses for training, validation, and testing\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(net.losses['train'], label='Train loss')\n",
    "plt.plot(net.losses['smooth train'], label='Train smooth loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
